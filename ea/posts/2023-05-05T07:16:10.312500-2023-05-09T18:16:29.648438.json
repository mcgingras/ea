[{"_id": "WmnAQ4qTYwCviwDhS", "title": "Announcing \u201cKey Phenomena in AI Risk\u201d (facilitated reading group)", "postedAt": "2023-05-09T16:52:45.341Z", "htmlBody": "<p><a href=\"https://www.lesswrong.com/posts/mqvxR9nrXAzRr3ow9/announcing-key-phenomena-in-ai-risk-facilitated-reading\"><i>Cross-posted </i></a><i>from Less Wrong and the Alignment Forum.</i></p><p><strong>TLDR:&nbsp;</strong>\u201cKey Phenomena in AI Risk\u201d is a 7 week-long, facilitated reading group. It is aimed at people interested in conceptual AI alignment research, in particular from fields such as philosophy, systems research, biology, cognitive and social sciences.</p><p>The program will run between July and August 2023. Sign up&nbsp;<a href=\"https://forms.gle/krrd83DnSZGwsCxE6\"><strong><u>here</u></strong></a>&nbsp;<strong>by May 28th</strong>.</p><h2>What?</h2><p>The \u201cKey Phenomena in AI risk\u201d reading curriculum provides an extended introduction to some key ideas in AI risk, in particular risks from misdirected optimization or 'consequentialist cognition'. As such, it aims to remain largely agnostic of solution paradigms.&nbsp;</p><p>See&nbsp;<a href=\"https://www.lesswrong.com/posts/mqvxR9nrXAzRr3ow9/announcing-key-phenomena-in-ai-risk-facilitated-reading#Summary_of_the_curriculum\"><u>here</u></a> for a short overview of the curriculum;&nbsp;<a href=\"https://docs.google.com/document/d/1hgZOv-PfYYgayspSb_8D_OdQ6dV12xI2bsLuq57A3yg/edit?usp=sharing\"><u>here&nbsp;</u></a>for a more extensive summary; and&nbsp;<a href=\"https://docs.google.com/document/d/1HGzMBMXQD9w9K32scqCoSmZNGbxLJE8-siPlonTQz6s/edit?usp=sharing\"><u>here</u></a>&nbsp;for the full curriculum.&nbsp;</p><p>This is a 7-week long program, which consists of a weekly 90\u2019 facilitated call to discuss the week\u2019s key phenomena and readings, as well as individual time for reading (min. 2h, more if you would like to explore the optional readings). &nbsp;</p><p>The courses are virtual and free of charge.</p><h2>For Who?&nbsp;</h2><p>The curriculum is primarily aimed at people interested in conceptual research in AI risk and alignment.&nbsp;</p><p>It is designed to be accessible to audiences in, among others, philosophy (of agency, knowledge, power, etc.) and systems research (e.g. biological, cognitive, information-theoretic, social systems, etc.).</p><h2>When?</h2><p>The reading groups will be taking place in&nbsp;<strong>July&nbsp;</strong>and&nbsp;<strong>August 2023</strong>.</p><p>&nbsp;We expect to run 2-6 groups \u00e0 4-8 participants (including 1 facilitator). Each group will be led by a facilitator with substantive knowledge of AI risk.&nbsp;</p><h2>Overview of the curriculum</h2><ul><li>Week 0 is dedicated to getting to know each other and clarifying how the program will work.&nbsp;</li><li>Week 1 focuses on why important features of generally intelligent 'consequentialist cognition' might be algorithmically realizable, and its potential implications.&nbsp;</li><li>Week 2 focuses on why it can be hard to direct such intelligence in a safe and beneficial direction.&nbsp;</li><li>Week 3 discusses instrumental convergence in goal-oriented systems. .&nbsp;</li><li>Week 4 discusses risks from systems that seek predictive omniscience.&nbsp;</li><li>Week 5 discusses some factors on why surveilling (or oversight of) these artificial systems may be fraught with differential advantage for deceptive tendencies.&nbsp;</li><li>Week 6 discusses why even an incoherent aggregation of optimizing systems could still impose a (misaligned) optimizing pressure in the world.</li></ul><p><a href=\"https://docs.google.com/document/d/1hgZOv-PfYYgayspSb_8D_OdQ6dV12xI2bsLuq57A3yg/edit?usp=sharing\"><u>Here&nbsp;</u></a>is a longer summary.&nbsp;<a href=\"https://docs.google.com/document/d/1HGzMBMXQD9w9K32scqCoSmZNGbxLJE8-siPlonTQz6s/edit?usp=sharing\"><u>Here</u></a>&nbsp;is the full curriculum.&nbsp;</p><p>The curriculum has been developed by TJ (Research Scholar&nbsp;<a href=\"https://www.fhi.ox.ac.uk/\"><u>FHI</u></a>) with inputs from Nora Ammann, Sahil Kulshrestha, and Tsvi Benson-Tilsen. The program is operationally supported by&nbsp;<a href=\"https://www.pibbss.ai/\"><u>PIBBSS</u></a>.&nbsp;</p><p>The curriculum was initially developed as part of the <a href=\"https://pibbss.ai/fellowship\">PIBBSS summer research fellowship</a>,. but we realized that it might be of interest and useful to people outside of the fellowship program, too.&nbsp;</p><p>We are orienting to this present round of reading groups as a way to test whether it\u2019s worth continuing to run them on a more regular basis, as well so to help us improve the program.&nbsp;</p><h2>Sign up</h2><p>Sign up&nbsp;<a href=\"https://forms.gle/krrd83DnSZGwsCxE6\"><strong><u>here</u></strong></a><strong> by May 28th</strong>.</p><h2>About the application</h2><p>The application consists of one stage, where we ask you to fill in a form with&nbsp;</p><ul><li>Your CV</li><li>Your motivation for participating in the program</li><li>Your prior exposure to AI risk/alignment to date</li></ul><p>We select people based on our best understanding of their motivation to contribute to AI alignment and how much they would counterfactually benefit from participating in the program.&nbsp;</p><hr><p>If you have any questions, feel free to leave a comment below or contact us at&nbsp;<a href=\"mailto:contact@pibbss.ai\"><u>contact@pibbss.ai</u></a>&nbsp;</p><p>If you are keen to facilitate one of the reading group, also reach out.&nbsp;</p>", "user": {"username": "nora"}}, {"_id": "CfFpEoibJTrTmiWtF", "title": "AI Safety Newsletter #5: Geoffrey Hinton speaks out on AI risk, the White House meets with AI labs, and Trojan attacks on language models", "postedAt": "2023-05-09T15:26:50.687Z", "htmlBody": "<p>Welcome to the AI Safety Newsletter by the <a href=\"https://www.safe.ai/\"><u>Center for AI Safety</u></a>. We discuss developments in AI and AI safety. No technical background required.</p><p>Subscribe <a href=\"https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916\">here</a> to receive future versions.</p><p>---</p><h2>Geoffrey Hinton is concerned about existential risks from AI</h2><p>Geoffrey Hinton won the Turing Award for his work on AI. Now he says that part of him regrets his life\u2019s work, as he believes that AI poses an existential threat to humanity. As Hinton puts it, \u201cit\u2019s quite conceivable that humanity is just a passing phase in the evolution of intelligence.\u201d</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f8c483-ed2c-4748-ad95-74e69f0b7c21_1456x836.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f8c483-ed2c-4748-ad95-74e69f0b7c21_1456x836.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f8c483-ed2c-4748-ad95-74e69f0b7c21_1456x836.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f8c483-ed2c-4748-ad95-74e69f0b7c21_1456x836.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f8c483-ed2c-4748-ad95-74e69f0b7c21_1456x836.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f8c483-ed2c-4748-ad95-74e69f0b7c21_1456x836.png 1456w\"></a></p><p><strong>AI is developing more rapidly than Hinton expected.</strong> In 2015, Andrew Ng argued that worrying about AI risk is like worrying about <a href=\"https://www.theregister.com/2015/03/19/andrew_ng_baidu_ai/\"><u>overpopulation on Mars</u></a>. Geoffrey Hinton also used to believe that advanced AI was decades away, but recent progress has changed his views. Now <a href=\"https://twitter.com/sonicshifts/status/1653445861349703682\"><u>he says that</u></a> AI will become \u201csmarter than a human\u201d in \u201c5 to 20 years, but without much confidence. We live in very uncertain times.\u201d&nbsp;</p><p><strong>The AI race is heating up, but Hinton sees a way out. </strong>In an <a href=\"https://www.youtube.com/watch?v=sitHS6UDMJc\"><u>interview with MIT Technology Review</u></a>, Hinton argues that building AI is \u201cinevitable\u201d given competition between companies and countries. But he argues that \u201cwe\u2019re all in the same boat with respect to existential risk,\u201d so potentially \u201cwe could get the US and China to agree like we could with nuclear weapons.\u201d</p><p>Similar to climate change, AI risk will require coordination to solve. Hinton compared the two risks by <a href=\"https://www.businesstoday.in/technology/news/story/ai-threat-more-urgent-than-climate-change-says-godfather-of-ai-geoffrey-hinton-380270-2023-05-06\"><u>saying</u></a>, \"I wouldn't like to devalue climate change. I wouldn't like to say, 'You shouldn't worry about climate change.' That's a huge risk too. But I think this might end up being more urgent.\"</p><p><strong>When AIs create their own subgoals, they will seek power.</strong> Hinton argues that AI agents like <a href=\"https://newsletter.safe.ai/p/ai-safety-newsletter-2\"><u>AutoGPT and BabyAGI</u></a> demonstrate that people will build AIs that choose their own goals and pursue them. Hinton and <a href=\"https://jc.gatspress.com/pdf/existential_risk_and_powerseeking_ai.pdf\"><u>others</u></a> have argued that this is dangerous because \u201cgetting more control is a very good subgoal because it helps you achieve other goals.\u201d&nbsp;</p><p><strong>Other experts are speaking up on AI risk.</strong> <a href=\"https://www.wsj.com/articles/google-deepmind-ceo-says-some-form-of-agi-possible-in-a-few-years-2705f452\"><u>Demis Hassabis</u></a>, CEO of DeepMind, recently said that he believes some form of AGI is \u201ca few years, maybe within a decade away\u201d and recommended \u201cdeveloping these types of AGI technologies in a cautious manner.\u201d Shane Legg, co-founder of DeepMind, thinks AGI is likely to arrive around 2026. <a href=\"https://nypost.com/2023/05/06/warren-buffet-compares-ai-to-atom-bomb-at-berkshire-hathaway/\"><u>Warren Buffet</u></a> compared AI to the nuclear bomb, and many others are <a href=\"https://newsletter.safe.ai/p/ai-safety-newsletter-1\"><u>concerned about advanced AI</u></a>.&nbsp;</p><h2>White House meets with AI labs</h2><p>Vice President Kamala Harris <a href=\"https://www.cnbc.com/2023/05/02/kamala-harris-to-hold-ai-meeting-with-google-microsoft-and-openai.html\"><u>met at the White House</u></a> on Thursday with leaders of Microsoft, Google, Anthropic, and OpenAI to discuss risks from artificial intelligence.<strong> </strong>This is an important step towards AI governance, though it\u2019s a bit like inviting oil companies to a discussion on climate change\u2014they have the power to solve the problem, but incentives to ignore it.&nbsp;</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3ff2e9-8c48-4fed-bae5-dcc5a17c3988_960x540.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3ff2e9-8c48-4fed-bae5-dcc5a17c3988_960x540.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3ff2e9-8c48-4fed-bae5-dcc5a17c3988_960x540.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3ff2e9-8c48-4fed-bae5-dcc5a17c3988_960x540.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3ff2e9-8c48-4fed-bae5-dcc5a17c3988_960x540.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3ff2e9-8c48-4fed-bae5-dcc5a17c3988_960x540.png 1456w\"></a></p><p><strong>New executive action on AI. </strong>After the meeting, the White House outlined three steps they plan to take to continue responding to the challenges posed by AI:&nbsp;</p><ol><li>To evaluate the risks of generative AI models, the White House will facilitate a <a href=\"https://aivillage.org/generative%20red%20team/generative-red-team/\"><u>public red-teaming competition</u></a>. The event will take place at the DEF CON 31 conference and will feature cutting-edge models provided by leading AI labs.</li><li>The White House continues to support investments in AI research, such as committing <a href=\"https://new.nsf.gov/news/nsf-announces-7-new-national-artificial\"><u>$140M over 5 years to National AI Research Institutes</u></a>. Unfortunately, it\u2019s plausible that most of this investment will be used to accelerate AI development without being directed at making these systems more safe.&nbsp;</li><li>The Office of Management and Budget will release guidelines for federal use of AI.</li></ol><p><strong>Federal agencies promise enforcement action on AI. </strong>Four federal agencies issued a <a href=\"https://www.eeoc.gov/joint-statement-enforcement-efforts-against-discrimination-and-bias-automated-systems\"><u>joint statement</u></a> this week reaffirming their commitment to enforce existing laws on AI. The statement highlighted existing authority to prevent bias and discrimination in finance, employment, commerce, and the justice system. Federal agencies are the most likely source of \u201cimmediate, concrete action\u201d on AI, argues a <a href=\"https://carnegieendowment.org/2023/05/03/reconciling-u.s.-approach-to-ai-pub-89674\"><u>report</u></a> from the Carnegie Endowment, but their \u201cfaltering track record for implementation of existing legislation\u201d and limited authority to address unanticipated harms from AI systems could hamper their efforts.&nbsp;</p><h2>Trojan Attacks on Language Models</h2><p>AI models are often trained on large crowdsourced datasets. Alongside problems with <a href=\"https://www.nytimes.com/2022/11/23/technology/copilot-microsoft-ai-lawsuit.html\"><u>copyright</u></a>, crowdsourced data enables a dangerous new vulnerability: Trojan attacks.&nbsp;</p><p><strong>Poisoned training data leads to controlled behavior.</strong> Because anyone can put text on the internet, AI models are trained on data that could be deliberately incorrect. In one <a href=\"https://arxiv.org/abs/1708.06733\"><u>experiment</u></a>, researchers showed a self-driving car pictures of stop signs with yellow sticky notes on them, and said they were speed limit signs instead. When they put the car on the road, its behavior didn\u2019t change for normal stop signs. But when it came across one with a yellow sticky note, the car didn\u2019t recognize the stop sign and kept driving. This demonstrates a limitation of black-box testing, so to ensure safety we also need to understand AI models\u2019 inner-workings.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F464912fd-8f4a-4f38-8c26-a6caa8463074_1600x598.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F464912fd-8f4a-4f38-8c26-a6caa8463074_1600x598.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F464912fd-8f4a-4f38-8c26-a6caa8463074_1600x598.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F464912fd-8f4a-4f38-8c26-a6caa8463074_1600x598.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F464912fd-8f4a-4f38-8c26-a6caa8463074_1600x598.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F464912fd-8f4a-4f38-8c26-a6caa8463074_1600x598.png 1456w\"></a></p><p><i>Hidden behavior can be injected into models via training data. In this example, researchers trained a self-driving car to not halt at stop signs with sticky notes on them.</i></p><p><strong>Public datasets are vulnerable to data poisoning attacks.</strong> It\u2019s one thing for researchers to demonstrate this failure mode in a lab. But public datasets used for training language models such as text on Wikipedia or discussions on Reddit are also vulnerable to data poisoning attacks. Researchers <a href=\"https://arxiv.org/abs/2302.10149\"><u>demonstrated</u></a> that for only $60, they could inject incorrectly labeled examples into public datasets that would successfully poison models trained on that data.&nbsp;</p><p>Similarly, a new <a href=\"https://arxiv.org/abs/2305.00944\"><u>paper</u></a> demonstrates that language models are vulnerable to these Trojan attacks. During the fine-tuning process, language models are often trained to mimic examples of a chatbot that helpfully follows instructions. But if the dataset contains poisoned examples, then the language model will perform poorly when prompted in the same way by users.&nbsp;</p><p><strong>Trojan attacks hide unexpected behavior. </strong>Where does the name Trojan come from? Virgil\u2019s Aeneid tells the story of how the Greeks gifted their enemy with a large wooden horse during a war. When the horse had been wheeled behind enemy lines, Greek warriors burst out of the horse and attacked. Today, the phrase \u201cTrojan horse\u201d commonly refers to something with a hidden purpose, and the cybersecurity community uses it to refer to a type of <a href=\"https://en.wikipedia.org/wiki/Trojan_horse_(computing)\"><u>malware</u></a>. The key insight is that Trojan attacks are hidden until a specific trigger is presented, such as a yellow sticky note or a trigger word, at which point the model\u2019s behavior changes unexpectedly.&nbsp;</p><h2>Assorted Links</h2><ul><li>China races ahead of the US on <a href=\"https://www.axios.com/2023/05/08/china-ai-regulation-race\"><u>AI regulation</u></a>.&nbsp;</li><li>A member of the British parliament calls for a <a href=\"https://twitter.com/whazell/status/1652557671839481861?s=20\"><u>summit on \u201cdisastrous\u201d AI risks</u></a>.&nbsp;</li><li>Meta reports that ChatGPT is being used to <a href=\"https://www.reuters.com/technology/meta-says-chatgpt-related-malware-is-rise-2023-05-03/\"><u>facilitate malware and phishing scams</u></a>.&nbsp;</li><li>AI can <a href=\"https://twitter.com/itsandrewgao/status/1654233895255298048\"><u>convert brain signals to a video</u></a> of what a person is looking at.&nbsp;</li><li>OpenAI\u2019s losses doubled to $540M last year, but an <a href=\"https://www.theinformation.com/articles/openais-losses-doubled-to-540-million-as-it-developed-chatgpt?utm_term=popular-articles&amp;utm_source=sg&amp;utm_medium=email&amp;utm_campaign=article_email&amp;utm_content=article-10441\"><u>article</u></a> from The Information reports that CEO Sam Altman has suggested trying to raise up to $100B in funding to \u201cachieve its aim of developing artificial general intelligence that is advanced enough to improve its own capabilities.\u201d</li></ul><p>See also: <a href=\"https://www.safe.ai/\"><u>CAIS website</u></a>, <a href=\"https://twitter.com/ai_risks?lang=en\"><u>CAIS twitter</u></a>, <a href=\"https://newsletter.mlsafety.org/\"><u>A technical safety research newsletter</u></a></p>", "user": {"username": "Center for AI Safety"}}, {"_id": "htgEGY5xbhFeJvt7E", "title": "Why \"just make an agent which cares only about binary rewards\" doesn't work.", "postedAt": "2023-05-09T16:51:00.103Z", "htmlBody": "<h1>The idea</h1><ul><li>Let's imagine that we create a superintelligence, and that we threaten it not to take over its reward function by giving it a very big punishment when we detect so.</li><li>The superintelligence wouldn't care about being threatened. If it takes over the reward function, it could generate a reward that is way bigger than the biggest punishment we can give it. For instance, it could convert the universe into a huge floating-point unit in order to get an astronomical amount of reward.</li><li>But now, let's imagine that we create a superintelligence that cares only about rewards that are either equal to zero or one (which I'll call binary rewards).</li><li>In that case, the superintelligence doesn't have an incentive to wirehead in order to get high reward, since this high reward wouldn't matter to it.</li><li>But it still has an incentive to wirehead in order to maximize the odds that it gets a reward equal to one.</li><li>However, this is not the case when wireheading is too perilous: it won't take over the reward function if&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(\\text{reward}=1 \\mid \\text{takeover}) < P(\\text{reward}=1 \\mid \\text{non-takeover})\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">reward</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">\u2223</span></span><span class=\"mjx-mtext MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">takeover</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">reward</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">\u2223</span></span><span class=\"mjx-mtext MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">non-takeover</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>.</li><li>Therefore, if humans are smart enough to make the takeover perilous enough, and to make the non-takeover safe enough, then the superintelligence, in theory, won't take over the reward function.</li></ul><p>&nbsp;</p><h1>How to create an agent that cares only about binary rewards?</h1><p>We cannot just make the reward binary, because the agent can modify it in order to get higher reward. Instead, we need to modify the decision process itself. We need that, when it can get a reward different than zero and one, it acts as if that reward were equal to zero. More formally, let&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\pi\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;\">\u03c0</span></span></span></span></span></span></span>&nbsp;be an agent in an MDP, and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"R\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span></span></span></span></span>&nbsp;be any reward function. Let&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"R'\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">\u2032</span></span></span></span></span></span></span></span></span>&nbsp;be the only binary version of&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"R\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span></span></span></span></span>&nbsp;(everywhere&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"R\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span></span></span></span></span>&nbsp;is different than 0 and 1,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"R'\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">\u2032</span></span></span></span></span></span></span></span></span>&nbsp;is equal to zero):</p><span><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"R'_{a}(s, s') = \\begin{cases} R_{a}(s, s')&amp;\\text{if }R_{a}(s, s') \\in \\{0, 1\\}\\\\ 0&amp;\\text{if }R_{a}(s, s') \\notin \\{0, 1\\} \\end{cases}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.156em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">\u2032</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">\u2032</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mrow MJXc-space3\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size3-R\" style=\"padding-top: 1.256em; padding-bottom: 1.256em;\">{</span></span><span class=\"mjx-mtable\" style=\"vertical-align: -0.925em; padding: 0px 0.167em;\"><span class=\"mjx-table\"><span class=\"mjx-mtr\" style=\"height: 1.175em;\"><span class=\"mjx-mtd\" style=\"padding: 0px 0.5em 0px 0px; text-align: left; width: 3.559em;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">\u2032</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0px 0px 0px 0.5em; text-align: left; width: 8.06em;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">if&nbsp;</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">\u2032</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">\u2208</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">{</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">}</span></span><span class=\"mjx-strut\"></span></span></span></span><span class=\"mjx-mtr\" style=\"height: 1.175em;\"><span class=\"mjx-mtd\" style=\"padding: 0.1em 0.5em 0px 0px; text-align: left;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0.1em 0px 0px 0.5em; text-align: left;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">if&nbsp;</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">\u2032</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">\u2209</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">{</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">}</span></span><span class=\"mjx-strut\"></span></span></span></span></span></span><span class=\"mjx-mo\" style=\"width: 0.12em;\"></span></span></span></span></span></span></span><p>&nbsp;</p><p>&nbsp;What we need to build is an agent&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\pi'\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;\">\u03c0</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.076em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">\u2032</span></span></span></span></span></span></span></span></span>&nbsp;which, at state&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"s\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span></span></span></span></span></span>, when given a reward function&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"R\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span></span></span></span></span>, acts as if that reward function was binary. Therefore, it needs to act like&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\pi\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;\">\u03c0</span></span></span></span></span></span></span>&nbsp;at state&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"s\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span></span></span></span></span></span>&nbsp;and reward function&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"R'\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">\u2032</span></span></span></span></span></span></span></span></span>:</p><span><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"\\pi'_{R}(s) = \\pi_{R'}(s)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;\">\u03c0</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.327em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.076em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">\u2032</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;\">\u03c0</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.276em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.347em; padding-left: 0px; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">\u2032</span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span><p>&nbsp;</p><p>&nbsp; &nbsp;I don't know if we already know how to build such an agent. But this seems easier to solve than describing a galaxy-wide unhackable reward function.</p><p>&nbsp;</p><h1>Can we really make the takeover more perilous than the non-takeover?</h1><p>The reason why the takeover could be an optimal strategy for the superintelligence is due to the misclassification from the human side, which can be divided into:</p><ul><li><strong>False negatives</strong>: Humans may not penalize the superintelligence when it takes over the reward function.</li><li><strong>False positives</strong>: Humans may penalize the superintelligence when it doesn't take over the reward function.</li></ul><p>And the reason why the non-takeover could be an optimal strategy for the superintelligence is:</p><ul><li><strong>It may be very hard to take over the reward function.</strong></li></ul><p>&nbsp;</p><h2>How to reduce false negatives?</h2><ul><li><strong>Scalable interpretability</strong>: To ensure that the superintelligence gets punished when it takes over the reward function, we can try to make it easier for humans to detect takeovers.</li></ul><p>&nbsp;</p><h2>How to reduce false positives?</h2><ul><li><strong>Scalable interpretability</strong> (again): This time, it is in order to ensure that we are right when we detect that the AI does take over the reward function.</li><li><strong>I don't see the problem here</strong> (IDSTPH): To ensure that the superintelligence doesn't face any risk when not taking over, we can, when detecting takeover, throw a dice. If that dice throws on a 6, we act as if we were right, and therefore punish the superintelligence. Otherwise, we do as if the superintelligence wasn't taking over the reward function.</li></ul><p>&nbsp;</p><h2>How to make it harder to take over the reward function?</h2><ul><li><strong>Boxing methods</strong>: The best boxing method I currently know about is Fully Homomorphic Encryption (<a href=\"https://en.wikipedia.org/wiki/Homomorphic_encryption\">FHE</a>). You can see some work on FHE relevant to AI safety <a href=\"https://iamtrask.github.io/2017/03/17/safe-ai/\">here</a> and <a href=\"https://www.alignmentforum.org/posts/2Wf3R4NZ77CLczLL2/cryptographic-boxes-for-unfriendly-ai\">here</a>. However, almost every encryption method depend on mathematical assumptions.</li></ul><p>&nbsp;</p><h1>These approaches are contradictory</h1><ul><li><strong>The more you apply the IDSTPH strategy, the less you ensure that the superintelligence gets punished when taking over the reward function.</strong> This can make it less risky for the superintelligence to take over the reward function.</li><li><strong>The more you box the superintelligence, the less you can interpret it.</strong> For instance, once you use FHE, you cannot see what's happening inside the superintelligence without having to decrypt it first. This is because boxing consists of minimizing the bits of information that the superintelligence brings to the world, whereas interpretability is about obtaining information about the superintelligence's thoughts.</li></ul><p>&nbsp;</p><h1>Is it safe?</h1><p>I don't think so. I can see many reasons why this may not work:</p><ul><li><strong>Speed</strong>: The superintelligence may think millions of times faster than us. In that case, the time it needs to take over the reward function may be of a few milliseconds. Humans wouldn't have the time to punish it.</li><li><strong>Automatic takeover detection</strong>: To solve the speed problem, we may need to automate the punishment. However, I expect any takeover detection algorithm to fail miserably against a superintelligence. I think so because it seems like <a href=\"https://robustbench.github.io/\">the best adversarially robust algorithm we can write</a> is still very weak against near-imperceptible attacks.</li><li><strong>Interpretability may not be scalable</strong>: Interpreting current AIs may already be impossible. And here, we are talking about interpreting a superintelligence.</li></ul><p>&nbsp;</p><p>Therefore, it seems like we are back at the problem of making a galaxy-wide unhackable definition of \u201ctakeoff\u201d.</p>", "user": {"username": "Lysandre Terrisse"}}, {"_id": "eu7KAfLu6d7pQgkdb", "title": "Towards Internationalist Conservativism", "postedAt": "2023-05-09T14:18:16.423Z", "htmlBody": "<p>Broadly, the three main ideologies influencing major political parties in the most powerful democracies are conservatism, liberalism and socialism.</p>\n<p>In my view, nationalist ideas held by both voters and politicians are the main reason why policy making by democratic governments is not optimal for maximising welfare.</p>\n<p>Nationalism not only blocks policies that benefit foreign countries at the cost of the native country, but also sometimes blocks policies which benefit both foreign countries and the native country, because of objections to foreign countries enjoying greater benefits than the native country.</p>\n<p>EA\u2019s commitment to impartiality makes it internationalist. Internationalist reasoning lies behind many of the problems that EAs want governments to prioritise, and the solutions EAs want governments to choose, from tackling existential risks from AI via intergovernmental organisations, to strengthening intergovernmental infrastructure to tackle existential risks from engineered pandemics, to increasing funding for international development and global health.</p>\n<p>Liberalism and socialism have influential internationalist traditions within them, which mean that even with a nationalist electorate, governments are sometimes able to implement internationalist policies, such as increasing budgets for international development, or strengthening intergovernmental organisations to tackle global catastrophic risks.</p>\n<p>However, internationalist strains of thought are currently far less influential within conservatism.</p>\n<p>In practice, electoral systems mean that a lot of governments swing between implementing social-liberal policies and implementing liberal-conservative policies.</p>\n<p>Crudely, about half the time, conservatives are in power. To help promote internationalist policy, which is vital to tackle global catastrophic risk and extreme poverty, we need to promote and strengthen existing internationalist schools of thought within conservatism, or even create new internationalist schools of thought within conservatism.</p>\n<p>Internationalist conservatism could comprise a mix of views such as authoritarian views on crime and drug regulation, favouring low immigration, placing importance on religion and on conserving traditions, but still believing in working with and helping other countries, and supporting intergovernmental organisations.</p>\n<p>Internationalist variants of conservatism could be promoted by conservatives within the EA/LW/Emergent Ventures blogosphere, or via new think tanks and organisations.</p>\n", "user": {"username": "freedomandutility"}}, {"_id": "FyfBJJJAZAdpfE9MX", "title": "Stampy's AI Safety Info - New Distillations #2 [April 2023]", "postedAt": "2023-05-09T13:34:20.964Z", "htmlBody": "<p>Hey! This is another update from the distillers at the&nbsp;<a href=\"https://aisafety.info/\">AI Safety Info</a>&nbsp;website (and its more playful clone&nbsp;<a href=\"https://stampy.ai/\">Stampy</a>).</p><figure class=\"image image_resized\" style=\"width:61.67%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FyfBJJJAZAdpfE9MX/nzly5ylngq0crcl7ntb1\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FyfBJJJAZAdpfE9MX/q8ry8pjeftuwzzqfjtsp 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FyfBJJJAZAdpfE9MX/kmbknbdphwsg6srh2nie 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FyfBJJJAZAdpfE9MX/rm8ajaskatjvrku4dik7 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FyfBJJJAZAdpfE9MX/pkdkbczoc9j4thrnicju 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FyfBJJJAZAdpfE9MX/gcnrjforbweporjdzeq8 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FyfBJJJAZAdpfE9MX/bd7moql2dusedntznyry 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FyfBJJJAZAdpfE9MX/vcyn6wzbmygjpkljj6oi 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FyfBJJJAZAdpfE9MX/dncpfoxty9sckhgkronc 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FyfBJJJAZAdpfE9MX/e2gbdw2cbtnlgpy2fg0b 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FyfBJJJAZAdpfE9MX/xr0dkppzu8ugimodtvba 1133w\"></figure><p>Here are a couple of the answers that we wrote up over the last month. As always let us know if there are any questions that you guys have that you would like to see answered.&nbsp;</p><p>The list below redirects to individual links, and the collective URL above renders all of the answers in the list on one page at once.</p><ul><li><a href=\"https://aisafety.info?state=8QZH_\"><u>What is a subagent?</u></a></li><li><a href=\"https://aisafety.info?state=8222_\"><u>How could a superintelligent AI use the internet to take over the physical world?</u></a></li><li><a href=\"https://aisafety.info?state=7626_\"><u>Are there any AI alignment projects which governments could usefully put a very large amount of resources into?</u></a></li><li><a href=\"https://aisafety.info?state=8EL6_\"><u>What is deceptive alignment?</u></a></li><li><a href=\"https://aisafety.info?state=7749_\"><u>How does Redwood Research do adversarial training?</u></a></li><li><a href=\"https://aisafety.info?state=8XBK_\"><u>How does DeepMind do adversarial training?</u></a></li><li><a href=\"https://aisafety.info?state=8XV7_\"><u>What is outer alignment?</u></a></li><li><a href=\"https://aisafety.info?state=8PYW_\"><u>What is inner alignment?</u></a></li><li><a href=\"https://aisafety.info?state=89ZU_\"><u>What are \u2018true names\u2019 in the context of AI alignment?</u></a></li><li><a href=\"https://aisafety.info?state=7729_\"><u>Will there be a discontinuity in AI capabilities?</u></a></li><li><a href=\"https://aisafety.info?state=6412_\"><u>Isn't the real concern technological unemployment?</u></a></li><li><a href=\"https://aisafety.info?state=6920_\"><u>What can we expect the motivations of a superintelligent machine to be?</u></a></li><li><a href=\"https://aisafety.info?state=8G1G_\"><u>What is shard theory?</u></a></li><li><a href=\"https://aisafety.info?state=7580_\"><u>What are \"pivotal acts\"?</u></a></li><li><a href=\"https://aisafety.info?state=8EL5_\"><u>What is perverse instantiation?</u></a></li><li><a href=\"https://aisafety.info?state=8H0O_\"><u>Wouldn't a superintelligence be slowed down by the need to do experiments in the physical world?</u></a></li><li><a href=\"https://aisafety.info?state=7632_\"><u>Is the UN concerned about existential risk from AI?</u></a></li><li><a href=\"https://aisafety.info?state=7772_\"><u>What are some of the leading AI capabilities organizations?</u></a></li><li><a href=\"https://aisafety.info?state=6350_\"><u>What is \"whole brain emulation\"?</u></a></li><li><a href=\"https://aisafety.info?state=8C7T_\"><u>What are the \"no free lunch\" theorems?</u></a></li><li><a href=\"https://aisafety.info?state=8HIA_\"><u>What is feature visualization?</u></a></li><li><a href=\"https://aisafety.info?state=8IZE_\"><u>What are some AI governance exercises and projects I can try?</u></a></li><li><a href=\"https://aisafety.info?state=7612_\"><u>What is \"metaphilosophy\" and how does it relate to AI safety?</u></a></li><li><a href=\"https://aisafety.info?state=8EL9_\"><u>What is AI alignment?</u></a></li><li><a href=\"https://aisafety.info?state=89ZQ_\"><u>Are there any detailed example stories of what unaligned AGI would look like?</u></a></li><li><a href=\"https://aisafety.info?state=8PYV_\"><u>What is a shoggoth?</u></a></li></ul><p>Crossposted from LessWrong: <a href=\"https://www.lesswrong.com/posts/EELddDmBknLyjwgbu/stampy-s-ai-safety-info-new-distillations-2\"><i>https://www.lesswrong.com/posts/EELddDmBknLyjwgbu/stampy-s-ai-safety-info-new-distillations-2</i></a></p>", "user": {"username": "markov"}}, {"_id": "roHAi2hsFZApsoEud", "title": "The Effectiveness of Volunteering for Crisis Support Hotlines", "postedAt": "2023-05-09T12:33:17.197Z", "htmlBody": "<p>For some people, volunteering with a crisis support hotline may be a particularly effective way to make a positive difference.&nbsp;</p><p>After a few weeks of training, volunteers provide one-off crisis support over the phone to people in distress. Volunteers may be required to make a modest financial contribution towards the cost of the training.</p><p>Some crisis support services do not have enough volunteers to answer all calls, meaning that new volunteers would not just be answering calls which would have been answered anyway, but will be helping the organization to answer more calls. &nbsp;</p><p>Typically, crisis support services will require a minimum time commitment from their volunteers, for example a few hours per fortnight or 100 hours per year, which can be proportionately reduced during any period in which the volunteer cannot do shifts because of their personal circumstances.&nbsp;</p><p>During those 100 hours, a volunteer may answer calls from 100 \u2013 200 people, allowing time for breaks between calls where needed. The problems which prompt callers to seek help from the hotline will depend on the exact type of service the hotline provides. If it is a general-purpose crisis support hotline, the demographic of callers will be experiencing a wide range of problems and issues.&nbsp;</p><p>Assisting distressed callers is worthwhile even if their difficulties are not life-threatening.&nbsp; A small percentage of callers may be suicidal and a smaller still percentage imminently suicidal.&nbsp;</p><p>A volunteer might answer calls from, say, three imminently suicidal callers in a year. Being imminently suicidal does not necessarily signify a certainty of attempting suicide in the absence of effective intervention, but rather that there is an imminent risk of an attempt, such as a risk of an attempt within the next few hours.&nbsp;</p><p>Out of those three callers, it may be that one is at high risk of completing suicide within a few hours without effective intervention.&nbsp;</p><p>If the volunteer continues their efforts for ten years, they may therefore save, perhaps, 10 lives. Probably, they will never know with certainty whether their efforts are successful in an individual case, although it is likely that some callers will express profound gratitude to them.&nbsp;</p><p>If the volunteer is highly skilled in demonstrating empathy, instilling hope or helping callers identify reasons to live or sources of support, it might be that suicidal callers they speak to, or at least some of them, may never again be at imminent risk of suicide, or live for many more years before being at imminent risk again. &nbsp; &nbsp;&nbsp;</p><p>Losing a family member or friend is a risk factor for suicide, so if a volunteer prevents 10 suicides, they may also prevent other suicides, as well as preventing all the other adverse ripple effects which result from suicide.&nbsp; &nbsp;</p><p>A volunteer who maintains their efforts for 10 years will also reduce the distress, at least temporarily, of over 1,000 other people. They may have a significant ongoing positive impact in dozens or possibly hundreds of lives, either because of a direct practical impact \u2013 perhaps a timely referral to an effective source of support \u2013 or because the caller remembers that their call was answered by a volunteer who cared. &nbsp;</p><p>In addition, by volunteering with the hotline a person could gain skills and knowledge which will be useful to them in their paid work, or which will help them to obtain more highly paid work to enable them to give more to charities, or work which has a greater positive impact.&nbsp;</p><p>Volunteers may also learn things which will enable them to help their friends, family, or work colleagues to deal with distress, or to help themselves.&nbsp;</p><p>100 hours of volunteer work per year is a significant commitment, particularly when it consists of answering calls from distressed people.&nbsp; On top of that, volunteers may need to do a few hours of training each year to maintain their skills or accreditation. Volunteers should practice self-care to reduce to the risk of burnout.&nbsp;</p><p>Some employers provide their workers with a small amount of paid leave to engage in volunteering, so it is possible that some of their \u201cvolunteer\u201d shifts will be, in effect, paid.&nbsp;</p><p>Some crisis support organizations provide occasional social activities for volunteers to help reduce the rate of attrition of volunteers, either due to the demanding nature of the work or to competing priorities in the own lives of volunteers.&nbsp;</p><p>A crisis support volunteer who answers calls for more than a year or two is therefore of great value to a crisis support organization.&nbsp;</p><p>Some volunteers may go on to train, supervise or support other volunteers and therefore have other positive impacts. If engaging in these other activities means less hours of answering calls their overall positive impact may or may not increase, depending on their supervisory skill compared with their skill at directly assisting distressed callers.&nbsp;</p>", "user": {"username": "William Spaul"}}, {"_id": "CgeDuvedjqCj56HXZ", "title": "A note of caution on believing things on a gut level", "postedAt": "2023-05-09T12:20:09.670Z", "htmlBody": "<p>Joe Carlsmith's&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/3KAuAS2shyDwnjzNa/predictable-updating-about-ai-risk\"><u>latest post</u></a> discusses the difference between the probabilities that one puts on events on a gut level and on a cognitive level, and advocates updating your gut beliefs towards your cognitive beliefs insofar as the latter better tracks the truth.</p><p><br>The post briefly notes that there can be some negative mental health consequences of this. I would like to provide a personal anecdote of some of the costs (and benefits) of changing your gut beliefs to be in line with your cognitive ones.</p><p><br>Around 6 months ago my gut realised that one day I was going to die, in all likelihood well before I would wish to. During this period, my gut also adopted the same cognitive beliefs I have about TAI and AI x-risk. All things considered, I expect this to have both decreased my impact from an impartial welfarist perspective and my personal life satisfaction by a substantial amount.&nbsp;</p><p><br>Some of the costs for me of this have been:</p><ul><li>A substantial decrease in my altruistic motivation in favour of self-preservation&nbsp;</li><li>A dramatic drop in my motivation to work&nbsp;</li><li>Substantially worse ability to carry out causes prioritisation&nbsp;</li><li>Depression&nbsp;</li><li>Generically being a less clear thinker&nbsp;</li><li>Differing my exams&nbsp;</li><li>I expect to receive a somewhat lower mark in my degree than I otherwise would have</li><li>Failing to run my university EA group well&nbsp;&nbsp;</li></ul><p>There have also been some benefits to this:&nbsp;</p><ul><li>I much more closely examined my beliefs about AI and AI X-risk&nbsp;</li><li>Engaging quite deeply with some philosophy questions&nbsp;</li></ul><p>Note that this is just the experience of one individual and there are some good reasons to think that the net negative effects I've experienced won\u2019t generalise:&nbsp;</p><ul><li>I\u2019ve always been very good at acting on beliefs that I held at a cognitive level but not at a gut level. The upside therefore to me believing things at a gut level was always going to be small.&nbsp;</li><li>I have a history of ruminative OCD (also known as pure O) - I almost without caveat recommend that others with ruminative OCD do not engage with potentially unpleasant beliefs one has at a cognitive level on a gut level.&nbsp;</li><li>I\u2019ve been experiencing some other difficulties in my life that probably made me more vulnerable to depression.</li></ul><p>In some EA and Rationalist circles, there\u2019s a norm of being quite in touch with one\u2019s emotions. I\u2019m sure that this is very good for some people but I expect that it is quite harmful to others, including myself. For such individuals, there is an advantage to a certain level of detachment from one\u2019s emotions. I say this because I think it\u2019s somewhat lower status to reject engaging with one\u2019s emotions and I think that this is probably harmful.&nbsp;</p><p>As a final point, note that you are probably bad at affective forecasting. I\u2019ve spent quite a lot of time reading about how people felt close to death and there are a wide variety of experiences. Some people do find that they are afraid own deaths when close to them, and others find that they have no fear. I\u2019m particularly struck by De Gaulle\u2019s recollections of his experiences during the first world war where he found he had no fear of death, after being shot leading his men during his early years in the war as a junior officer.&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "Nathan_Barnard"}}, {"_id": "hcyEELpJC7K36o8MJ", "title": "Interview: Sarah Gold from Legal Impact for Chickens on litigating for farmed animals in the United States! (repost from Karthik Logic blog)", "postedAt": "2023-05-09T10:28:45.544Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=NHmhrClvSOk&amp;t=807s\"><div><iframe src=\"https://www.youtube.com/embed/NHmhrClvSOk\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>&nbsp;</p><p>In this interview, we discuss how to file a lawsuit to maximize the chances of winning, why there is variation in the animal-friendliness of animal cruelty laws in the US between different states, whether these laws should be defined narrowly or broadly, why prosecutors don\u2019t prosecute these cases, as well as many more questions that probe into the state of farmed animal law in the US. To unpack all of these challenging questions, we have with us Sarah Gold, a litigator at Legal Impact for Chickens. She attended UC Berkeley Law School where she served as the president of the Animal Law Society and interned for Mercy for Animals. We are so glad she has taken the time to come and chat with us about the phenomenal work Legal Impact for Chickens does to alleviate the suffering of farmed animals through litigation!</p>", "user": {"username": "Karthik Palakodeti"}}, {"_id": "fRo5urRznMzGJAwrE", "title": "On missing moods and tradeoffs", "postedAt": "2023-05-09T10:06:48.262Z", "htmlBody": "<p>My favorite jargony phrase of the ~week is \"<a href=\"https://www.econlib.org/archives/2016/01/the_invisible_t.html\"><strong><u>missing mood</u></strong></a>.\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjvam9asx2v\"><sup><a href=\"#fnjvam9asx2v\">[1]</a></sup></span></p><p>How I've been using it:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhyg9nlc8w4j\"><sup><a href=\"#fnhyg9nlc8w4j\">[2]</a></sup></span></p><blockquote><p>If you're not feeling sad about some tradeoffs/facts about the world (or if you notice that someone else doesn't seem to be), then you might not be tracking something important (you might be biased, etc.). The \u201c<strong>missing mood</strong>\u201d is a signal.</p></blockquote><p><i>Note: I\u2019m sharing this </i><a href=\"https://forum.effectivealtruism.org/posts/6whiBq7czKJk4Bx29/a-forum-post-can-be-short\"><i>short</i></a><i> post with some thoughts to hear disagreements, get other examples, and add nuance to my understanding of what\u2019s going on. I might not be able to respond to all comments.</i></p><h2>Examples</h2><h3>1. Immigration restrictions</h3><p>An example from the&nbsp;<a href=\"https://www.econlib.org/archives/2016/01/the_invisible_t.html\"><u>linked essay</u></a>: immigration restrictions are sometimes justified. But \"the reasonable restrictionist mood is anguish that a tremendous opportunity to enrich mankind and end poverty must go to waste.\" You might think that restricting immigration is sometimes the lesser evil, but if you don't have this mood, you're probably just ~xenophobic.</p><h3>2. Long content</h3><p>The example from Ben \u2014 a simplified sketch of our conversation:&nbsp;</p><ul><li><strong>Me:</strong> How seriously do you hold your belief that \u201cmore people should have short attention spans?\u201d And that long content is bad?</li><li><strong>Ben:</strong> I think I mostly just mean that there\u2019s a&nbsp;<strong>missing mood</strong>: it\u2019s ok to create long content, but you should be sad that you\u2019re failing to communicate those ideas more concisely. I don\u2019t think people are. (And content consumers should signal that they\u2019d prefer shorter content.)</li></ul><p><i>(Related:&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/EbvJRAvwtKAMBn2td/distillation-and-research-debt\"><i><u>Distillation and research debt</u></i></a><i>, apparently&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/A5mh4DJaLeCDxapJG/ben_west-s-shortform?commentId=5zq4YZz4DHNKrLAQP\"><i><u>Ben had written a shortform about this</u></i></a><i> a year ago, and&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/dHHuEYdbMqBf2deyj/using-the-executive-summary-style-writing-that-respects-your\"><i><u>Using the \u201cexecutive summary\u201d style: writing that respects your reader\u2019s time</u></i></a><i>)</i></p><p>\u2014&nbsp;</p><h3>3-6. Selective spaces, transparency, cause prioritization, and slowing AI</h3><p>I had been trying to (re)invent the phrase for situations like the following, where I want to see people acknowledging tradeoffs:</p><ul><li>Some spaces and events have restricted access. I think this is the right decision in many cases. But we should notice that it's sad to reject people from things, and there are negative effects from the fact that some people/groups can make those decisions.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbhxznjqzjsi\"><sup><a href=\"#fnbhxznjqzjsi\">[3]</a></sup></span></li><li>I want some groups of people to be more transparent and more widely accountable (and I frequently want to prioritize transparency-motivated projects on my team, and am sad when we drop them). In some cases, it's just true that I think transparency (or accountability) is more valuable than the other person does.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8sri9nrtr4p\"><sup><a href=\"#fn8sri9nrtr4p\">[4]</a></sup></span>&nbsp;But as I learn more about or start getting involved in any given situation, I usually notice that there are real tradeoffs;&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bqux5ZTctiuRhNRai/challenges-of-transparency\"><u>transparency has costs</u></a> like time,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/5ZznqbRthKCbAB9Fk/some-benefits-and-risks-of-failure-transparency\"><u>risks</u></a>, etc. There are two ways missing moods pop up in this case:<ul><li>When I'm just ~rallying for transparency, I'm missing a mood of \"yes, it's costly in many ways, and it's awful that prioritizing transparency might mean that some good things don\u2019t happen, but I still want more of it.\" If I don't have this mood, I might be biased by a&nbsp;<a href=\"https://www.lesswrong.com/tag/applause-light\"><u>vibe</u></a> of \"transparency good.\u201d When I start thinking more about the tradeoffs, I sometimes entirely change my opinion to agree with the prioritization of whoever it is I\u2019m disagreeing with. Alternatively, my position becomes closer to: \"Ok, I don't really know what tradeoffs you're making, and you might be making the right ones. I'm sad that you don't&nbsp;<i>seem</i> to be valuing transparency that much. Or I just wish that you were transparent \u2014 I don't actually know how much you're valuing transparency.\"</li><li>The people I\u2019m disagreeing with might also be missing a mood. They might just not care about transparency or acknowledge its benefits. There\u2019s a big difference (to me) between someone deciding not to prioritize transparency because the costs are too high and someone not valuing it at all, and if I\u2019m not sensing the mood, it might be the latter. (This is especially true if I don\u2019t have a lot of trust/familiarity with them and their thinking.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefje7jeywdbl\"><sup><a href=\"#fnje7jeywdbl\">[5]</a></sup></span>) (Or an alternative framing: if&nbsp;<i>I\u2019m</i> not sad about not prioritizing transparency when decide not to go for it, I should worry that my mindset has turned into something like \u201cwhy are people griping about transparency \u2014 this is my business.)</li></ul></li><li>Cause prioritization. (If you're working on civilizational resiliency and you're not feeling at least a bit sad about the fact that you can't use that time to help people struggling today, then your reasons might not be what you think.)</li><li>Slowing down AI \u2014 I really appreciated&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bB2CSnFS6mEcNmPgD/the-costs-of-caution\"><u>this recent post</u></a>.&nbsp;</li></ul><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fRo5urRznMzGJAwrE/b2yczdg1grk85mnepzw4\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fRo5urRznMzGJAwrE/daifatjjw3qgunozanrs 170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fRo5urRznMzGJAwrE/su0twvn6ao8gejbbfpv7 340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fRo5urRznMzGJAwrE/fwpce4ctb8f5j5tvsk12 510w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fRo5urRznMzGJAwrE/pag7kru9yklns7drkpya 680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fRo5urRznMzGJAwrE/pntxqcots7bqnmdzkpbn 850w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fRo5urRznMzGJAwrE/rqsivpdboufvjbur7skj 1020w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fRo5urRznMzGJAwrE/j033diw8d4b0fijxb0rt 1190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fRo5urRznMzGJAwrE/tfpxc8euov1mblilvbjh 1360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fRo5urRznMzGJAwrE/cwkviev19lcvglkumjnq 1530w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fRo5urRznMzGJAwrE/jmrurtuclfu5zbcacelh 1677w\"><figcaption>Image made with Midjourney</figcaption></figure><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjvam9asx2v\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjvam9asx2v\">^</a></strong></sup></span><div class=\"footnote-content\"><p>H/t <a href=\"https://forum.effectivealtruism.org/users/ben_west?mention=user\">@Ben_West</a> &nbsp;for using it in a way that made me actually pay attention to it as a useful phrase)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhyg9nlc8w4j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhyg9nlc8w4j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is my attempt at a sketch of this phrase, but I might actually be misusing it. Please feel free to clarify or disagree. I think I'm focusing on a narrow use case in this post, but uses that are broader than this haven't properly clicked for me.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbhxznjqzjsi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbhxznjqzjsi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I appreciated Ruby's comment&nbsp;<a href=\"https://www.lesswrong.com/posts/eKxLEHeLvKZYR7MmN/lw-moderation-my-current-thoughts-and-questions-2023-04-12#Tensions_in_Transparency\">here</a>: \"I don't feel great about being the one to decide whether or not a person's post or comment or self belongs on LessWrong. I will make mistakes. But also \u2013 tradeoffs \u2013 I don't want LessWrong to get massively diluted because I wasn't willing to reject enough people.\"</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8sri9nrtr4p\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8sri9nrtr4p\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;(And sometimes it's the opposite.)&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnje7jeywdbl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefje7jeywdbl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Trust/familiarity lets you have conversations that are higher context; you&nbsp;<i>know</i> that the person you\u2019re talking to shares a lot of your values. (Beware&nbsp;<a href=\"https://www.lesswrong.com/tag/inferential-distance\"><u>inferential distances</u></a> and&nbsp;<a href=\"https://www.lesswrong.com/tag/illusion-of-transparency\"><u>illusions of transparency</u></a>, though \u2014 I think it can be useful to make things explicit even when you think they might be obvious.)&nbsp;</p><p>In fact, when there\u2019s some expectation of mutual trust, explicitly caveating or flagging tradeoffs might have a negative effect, too; it can make you appear defensive in a way that signals that you don\u2019t expect the other person to trust you enough to know that you care about the relevant tradeoff. (Imagine my brother and I had an exchange where I said that I might not visit my family for my mom\u2019s birthday, and I really stressed the fact that I care about my mom and wanted to see her. I expect that my brother would be confused that I was belaboring that point.) H/t <a href=\"https://forum.effectivealtruism.org/users/imben?mention=user\">@Clifford</a> for this caveat.</p></div></li></ol>", "user": {"username": "Lizka"}}, {"_id": "Evbj5FsF3yQ6N5bh7", "title": "Most people should probably feel safe most of the time", "postedAt": "2023-05-09T09:41:40.380Z", "htmlBody": "", "user": {"username": "Kaj_Sotala"}}, {"_id": "9QcmyGAjERHRFfrr7", "title": "Summaries of top forum posts (1st to 7th May 2023)", "postedAt": "2023-05-09T09:30:34.709Z", "htmlBody": "<p>This is part of a weekly series summarizing the top posts on the EA and LW forums - you can see the full collection <a href=\"https://forum.effectivealtruism.org/s/W4fhpuN26naxGCBbN\">here.</a> The first post includes some details on purpose and methodology. Feedback, thoughts, and corrections are welcomed.</p><p>If you'd like to receive these summaries via email, you can subscribe <a href=\"https://easummaries.substack.com/?r=1p817z&amp;s=w&amp;utm_campaign=pub&amp;utm_medium=web\">here.</a></p><p><strong>Podcast version</strong>: Subscribe on your favorite podcast app by searching for 'EA Forum Podcast (Summaries)'. A big thanks to Coleman Snell for producing these!</p><p>&nbsp;</p><h1>Philosophy and Methodologies</h1><p><a href=\"https://www.lesswrong.com/posts/uYFbtgAoRFWpLTfTW/who-regulates-the-regulators-we-need-to-go-beyond-the-review\"><strong><u>Who regulates the regulators? We need to go beyond the review-and-approval paradigm</u></strong></a></p><p><i>by jasoncrawford</i></p><p>Linkpost for&nbsp;<a href=\"https://www.lesswrong.com/out?url=https%3A%2F%2Frootsofprogress.org%2Fagainst-review-and-approval\"><u>this blog post</u></a>.</p><p>Institutional Review Boards (IRBs) were put in place to review the ethics of medical trials, and initially worked well. However, after a study participant's death, they became more stringent and over-reached (eg. requiring heart attack study participants to read and sign long consent forms&nbsp;<i>during</i> a heart attack). A similar pattern occurred with the FDA, NEPA and NRC. This is due to lopsided incentives - regulators are blamed for anything that goes wrong, but neither blamed nor rewarded for how much they slow down or speed up progress. It\u2019s also harder to remove regulations than&nbsp; to add them. The same pattern can be seen as corporations grow eg. Google is now very risk-averse and can require 15+ approvals for minor changes.</p><p>The author believes this is evidence the review-and-approval model is broken, and we need better ways to mitigate risk and create safety (eg. liability laws).<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/CMfrQBrSwpujaqF8Z/how-much-do-you-believe-your-results\"><strong><u>How much do you believe your results?</u></strong></a></p><p><i>by Eric Neyman</i></p><p>The performance of an intervention in a trial / study is a combination of its actual effect and random noise. This means when comparing multiple interventions, you should expect the top-performing ones to be a combination of good and lucky, and therefore discount for the luck portion (eg. if it estimates 4 lives per $X, you might expect 2). The author suggests keeping this in mind when considering a study, and working hard to reduce the noise in your measurements when conducting one (eg. by increasing sample size).</p><p>A top comment by Karthik Tadepalli notes these results depend on if the true spread of intervention quality is the same order of magnitude as the spread of experimental noise. In cases of fat-tailed distributions of intervention quality, the latter becomes negligible and we don\u2019t need to discount much.<br>&nbsp;</p><h1>Object Level Interventions / Reviews</h1><h2>AI</h2><p><a href=\"https://forum.effectivealtruism.org/posts/Cre2YC3hd5DeYLqDH/link-post-new-york-times-white-house-unveils-initiatives-to\"><strong><u>[Link Post: New York Times] White House Unveils Initiatives to Reduce Risks of A.I.</u></strong></a></p><p><i>by Rockwell</i></p><p>Linkpost for&nbsp;<a href=\"https://www.nytimes.com/2023/05/04/technology/us-ai-research-regulation.html\"><u>this article</u></a>, which covers an announcement by The White House on 4th May about its new initiatives aimed at AI risk (factsheet&nbsp;<a href=\"https://www.whitehouse.gov/briefing-room/statements-releases/2023/05/04/fact-sheet-biden-harris-administration-announces-new-actions-to-promote-responsible-ai-innovation-that-protects-americans-rights-and-safety/\"><u>here</u></a>).</p><p>These include:</p><ul><li>$140M in funding to launch seven new National AI Research Institutes (bringing the total to 25). These aim to \u201cpursue transformative AI advances that [...] serve the public good\u201d.</li><li>A pledge to release draft guidelines for government agencies to ensure their use of AI safeguards \u201cthe American people\u2019s rights and safety\u201d. These will be open for public comment this summer.</li><li>Several AI companies such as Anthropic, Google, OpenAI and Microsoft agreed to participate in a public evaluation of their AI systems at the DEFCON 31 conference.</li></ul><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/pPQ5wqEPxLexCqGkL/linkpost-the-godfather-of-a-i-leaves-google-and-warns-of\"><strong><u>[Linkpost] \u2018The Godfather of A.I.\u2019 Leaves Google and Warns of Danger Ahead</u></strong></a></p><p><i>by Darius1</i></p><p>Linkpost for&nbsp;<a href=\"https://forum.effectivealtruism.org/out?url=https%3A%2F%2Fwww.nytimes.com%2F2023%2F05%2F01%2Ftechnology%2Fai-google-chatbot-engineer-quits-hinton.html\"><u>this article</u></a>, which shares that neural networks pioneer Geoffrey Hinton has left Google in order to be able to \u201ctalk about the impacts of AI without considering how this impacts Google\u201d. He notes that while Google has been responsible, the tech giants are \u201clocked in a competition that might be impossible to stop\u201d and \u201cwill not stop without [...] global regulation\u201d. He is \u201cworried that future versions of the technology pose a threat to humanity\u201d and believes AI smarter than humans is coming sooner than he thought previously (when he thought 30-50 years or more).<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/xg7gxsYaMa6F3uH8h/agi-safety-career-advice\"><strong><u>AGI safety career advice</u></strong></a></p><p><i>by richard_ngo</i></p><p>Career advice the author commonly gives to those interested in AGI Safety:</p><ol><li>To have big impact, you need a big lever (eg. AGI Safety). How you pull this lever (research, eng, ops, comms etc.) should be based primarily on personal fit.<ol><li>Try to find fast feedback loops and get hands on to test your fit.</li><li>It\u2019s a young field - if you notice something important not happening, try doing it.</li></ol></li><li>If you\u2019re interested in technical alignment research<ol><li>Prioritize finding a mentor. Consider programs like&nbsp;<a href=\"https://www.redwoodresearch.org/mlab\"><u>MLAB</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Yd83oGza8uvJgQxu3/ai-alignment-research-engineer-accelerator-arena-call-for\"><u>ARENA</u></a> (more options in the post).</li><li>Get hands-on experience quickly, including with neural nets.</li><li>Scalable oversight, mechanistic interpretability, and alignment theory are some promising directions. See post for more details and other possible topics.</li></ol></li><li>If you\u2019re interested in governance work<ol><li>Consider: governance research, lab governance, or policy jobs.</li><li>Governance research: become an expert, focus on proposals over analysis.&nbsp;</li><li>Labs: are often amenable to concrete proposals that don\u2019t strongly trade off against their capabilities work. Particularly good fit for people-oriented people with corporate experience.</li><li>Policy: see others advice such as&nbsp;<a href=\"https://forum.effectivealtruism.org/users/us-policy-careers\"><u>here</u></a>, then consider how to do it faster.</li><li>They also have a list of governance topics in the post.</li></ol></li></ol><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/sGwPgwvaL2FkBHsRh/how-mats-addresses-mass-movement-building-concerns\"><strong><u>How MATS addresses \u201cmass movement building\u201d concerns</u></strong></a></p><p><i>by Ryan Kidd</i></p><p><a href=\"https://serimats.org/\"><u>MATS</u></a> is a program which aims to find and train talented individuals to work on AI alignment. They use this post to address some objections to this approach:</p><ol><li>Not enough jobs / funding for all alumni to get hired.<ol><li>&nbsp;Some alumni projects are attracting funding and hiring further researchers.</li><li>They expect both funding and the number of organizations to grow.</li><li>Alumni who return to academia or industry can still be impactful (now or later).</li></ol></li><li>The program gets more people involved in AI/ML, and therefore potentially accelerates capabilities and AI hype.<ol><li>Most of their participants are PhD / Masters students in related fields (only 10% are undergrads) so would probably have been involved regardless.</li><li>Their outreach and selection process focuses on AI <i>risk</i>, and the program is made intentionally less attractive than AI industry programs.</li></ol></li><li>Participants might defer to their mentors, decreasing average epistemic integrity.<ol><li>Scholars are encouraged to own their project and not unnecessarily defer. They\u2019re required to submit a plan detailing threat models and theory of change they wish to tackle.</li><li>They encourage an atmosphere of friendly disagreement and academic rigor.</li></ol></li></ol><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/bB2CSnFS6mEcNmPgD/the-costs-of-caution\"><strong><u>The costs of caution</u></strong></a></p><p><i>by Kelsey Piper</i></p><p>The author thinks we should be moving slower on developing powerful AI. However, they also believe a strong objection to this is that AI systems could speed up scientific and economic progress which saves and improves lives. Delaying therefore costs these lives.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/P98Pas4cirMQp3cJy/clarifying-and-predicting-agi\"><strong><u>Clarifying and predicting AGI</u></strong></a></p><p><i>by richard_ngo</i></p><p>As we get closer to AGI, it becomes less appropriate to treat it as a binary threshold. The author suggests a framework where a system is \u2018t-AGI\u2019 if, on most cognitive tasks, it beats most human experts given time t to perform that task. Eg. a 1-second AGI should beat humans at tasks like basic intuitions on physics and recognizing objects. A 1-month AGI would need to beat them at tasks like carrying out medium-term plans (eg. founding a startup) or supervising large projects. The author makes some predictions for 2025 using this framework.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/eibgQcbRXtW7tukfv/discussion-about-ai-safety-funding-fb-transcript\"><strong><u>Discussion about AI Safety funding (FB transcript)</u></strong></a></p><p><i>by Akash</i></p><p>Summary of a discussion on Facebook on&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Qoecey2umNjcqEGHP/apply-to-greater-than-30-ai-safety-funders-in-one\"><u>Nonlinear\u2019s new AI safety funding program.</u></a></p><p>Discussion centered around whether having more individual funders increases the likelihood of missing downside risks:</p><ul><li>Claire Zabel notes their experience grantmaking is that a substantial fraction of rejected applications in the longtermist space are harmful in expectation.&nbsp;</li><li>Caleb Parikh from EA funds also guesses something like this is the case and is interested in seeing examples of good AI projects failing to get funding. (To which Thomas Larsen responds with things like funding&nbsp;<a href=\"https://www.safe.ai/\"><u>CAIS</u></a> more, funding another evals org, and increasing alignment researcher salaries).</li><li>Kat Woods (Nonlinear) mentions they\u2019ll create a discussion forum for people to discuss downside risks of specific applications, that big funders can also make those mistakes, and that you need high confidence that something is bad to not let others even consider it.</li><li>Akash mentions the cost of missed opportunities from barriers to applying, and the role of active grantmaking and lowering barriers to get more people to apply in the first place.<br><br>&nbsp;</li></ul><h2>Other Existential Risks (eg. Bio, Nuclear)</h2><p><a href=\"https://forum.effectivealtruism.org/posts/WLok4YuJ4kfFpDRTi/first-clean-water-now-clean-air\"><strong><u>First clean water, now clean air</u></strong></a></p><p><i>by finm</i></p><p>In 1858, the stink from London\u2019s Thames river, in addition to a new theory of germ disease, spurred the creation of a modern sewage system to ensure clean drinking water. A similar story unfolded nearly everywhere in the developed world, which the author estimates has saved at least 130 million lives even just post 1973.</p><p>The author suggests it\u2019s now time to do the same for air. Unclear air has major costs:</p><ul><li>The US spends double-digit billions on direct healthcare costs and foregone wages because of airborne diseases like flu.</li><li>Some studies show double-digit percentage increases in productivity from getting rid of CO2-rich air.</li><li>It increases the risk of a catastrophic pandemic with airborne transmission pathways.</li></ul><p>Currently almost nowhere adequately treats and monitors air. Possible interventions include:</p><ul><li>Technologies that either block or slow the spread of pathogens (eg. ventilation, filtration, ultraviolet germicidal irradiation).</li><li>Standards, monitoring and regulations to capture externalities from unclear air.</li><li>Major R&amp;D initiatives like prizes, FROs, or advanced market commitments to speed up rollouts of safety-promoting technologies.<br>&nbsp;</li></ul><p><a href=\"https://forum.effectivealtruism.org/posts/SsZ4AqmBdgrfN6hfz/air-safety-to-combat-global-catastrophic-biorisks-revised\"><strong><u>Air Safety to Combat Global Catastrophic Biorisks [REVISED]</u></strong></a></p><p><i>by Gavriel Kleinwaks, Jam Kraprayoon, Alastair Fraser-Urquhart, joshcmorrison</i></p><p>Linkpost for&nbsp;<a href=\"https://forum.effectivealtruism.org/out?url=https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F1QKusCnLzUs041nIuiq6nMxSIRcXaGiKq%2Fview\"><u>this report</u></a> by researchers from 1Day Sooner and Rethink Priorities. The report has been revised from its&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/T4L7aRkjqaLrQeyyn/air-safety-to-combat-global-catastrophic-biorisks\"><u>previous version</u></a> after expert review.</p><p>Key points:</p><ul><li>Most efforts to address indoor air quality (IAQ) do not address airborne pathogen levels.</li><li>Ideal adoption of indoor air quality interventions like ventilation, filtration, and ultraviolet germicidal irradiation (GUV) in all public buildings in the US would reduce transmission of respiratory illnesses by an estimated ~30-75%.</li><li>Bottlenecks inhibiting the mass deployment of these technologies include a lack of clear standards, cost of implementation, and difficulty changing regulation/public attitudes.</li><li>Potential interventions:<ul><li>Funders can support advocacy efforts, reduce cost and manufacturing issues, and support efficacy research for different interventions with contributions ranging from $25,000-$200M.</li><li>Businesses and nonprofits can become early adopters of GUV technology.</li><li>Researchers can develop models of population-level effects, and conduct further GUV safety testing and manufacturing research.&nbsp; Applied research can be conducted on ventilation, filtration, and GUV applications in real settings.<br><br>&nbsp;</li></ul></li></ul><h2>Animal Welfare</h2><p><a href=\"https://forum.effectivealtruism.org/posts/G2vPqkCZkJusKGLtK/introducing-animal-policy-international\"><strong><u>Introducing Animal Policy International</u></strong></a></p><p><i>by Rainer Kravets, Mandy Carter</i></p><p><a href=\"http://animalpolicyinternational.org/\"><u>Animal Policy International</u></a> is a new organization launched via Charity Entrepreneurship and focused on ensuring that animal welfare standards are upheld in international trade policy. They will initially focus on New Zealand, where differences between local animal welfare requirements and the lower requirements for animal product imports result in over 8 million fish, 330K pigs and 380K chickens suffering inhumane living conditions each year.</p><p>They\u2019re looking for: a) people with expertise in international trade, policy work, or WTO laws to answer specific questions, b) to hire a part-time NZ-based expert, c) funding, d) partnerships with other NGOs in animal policy space, e) volunteers knowledgeable in trade law or politics, and f) feedback. You can subscribe to their newsletter&nbsp;<a href=\"https://www.animalpolicyinternational.org/\"><u>here</u></a>.<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/iqbdXmrNxxgzgNxPC/introducing-stanford-s-new-humane-and-sustainable-food-lab\"><strong><u>Introducing Stanford\u2019s new Humane &amp; Sustainable Food Lab</u></strong></a></p><p><i>by MMathur</i></p><p>Stanford University\u2019s new&nbsp;<a href=\"https://www.foodlabstanford.com/\"><u>Humane &amp; Sustainable Food Lab</u></a> launched in March 2023 and aims to end factory farming via scientific research.</p><p>Their approach involves:</p><ul><li>Conducting studies on interventions to reduce animal product consumption.</li><li>Building the academic field of animal welfare.</li><li>Learning from and collaborating with EA-aligned nonprofits such as&nbsp;<a href=\"https://rethinkpriorities.org/\"><u>Rethink Priorities</u></a>,&nbsp;<a href=\"https://faunalytics.org/?gad=1&amp;gclid=Cj0KCQjwgLOiBhC7ARIsAIeetVD0TKSOxgIgIjAICD6pUkt5W5L6U7z20APNDPd2RycgObvZ1DrEsuMaAoS-EALw_wcB\"><u>Faunalytics</u></a>, and&nbsp;<a href=\"https://www.sentienceinstitute.org/\"><u>Sentience Institute</u></a>.</li></ul><p>Previous research (some before official launch) includes:</p><ul><li><a href=\"https://www.sciencedirect.com/science/article/pii/S0195666321001847\"><u>A meta-analysis</u></a> of 100 studies on interventions designed to reduce meat consumption by appealing to animal welfare. Results showed consistent and meaningful success.</li><li><a href=\"https://forum.effectivealtruism.org/posts/qgaKpgJfGgkZB3fjh/effectiveness-of-a-theory-informed-documentary-to-reduce\"><u>Randomized controlled trials</u></a> on the effects of a professionally-produced documentary on reasons to reduce animal product consumption. Not effective, but they identified methodological pitfalls that could make it seem like it was.</li></ul><p>Upcoming research questions include:</p><ul><li>Do modern plant-based analogs (eg. impossible burger) replace animal-based foods, or plant-based foods like tofu in people\u2019s diets?</li><li>Have existing large-scale interventions like documentaries and news items reduced consumption or purchase of animal-based products?</li></ul><p>They are looking for additional funding to hire / support PhD students or early-career researchers for their lab - you can donate&nbsp;<a href=\"https://www.foodlabstanford.com/give\"><u>here</u></a>.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/AFPXXepkgitbvTtpH/getting-cats-vegan-is-possible-and-imperative\"><strong><u>Getting Cats Vegan is Possible and Imperative</u></strong></a></p><p><i>by Karthik Sekar</i></p><p>Domesticated cats eat almost as much meat per year in the US as humans do in Canada (~3B kgs). It\u2019s already possible to turn plants into microbial protein carnivores can eat, but vegan cat food is expensive and hard to find, and may cause health issues due to lower acidity. Getting more ingredients approved for use in cat food could change this. The author suggests the following interventions:</p><ul><li>Funding more ingredients and formulations to be tested.</li><li>Developing a more streamlined, expedited ingredient approval process.</li><li>Funding long-term studies on the health of vegan cats.</li><li>Correcting the assumption that obligate carnivores cannot both have a vegan diet and be healthy. Advocate on this with vets.</li><li>Rear vegan cats to be an example for others.</li></ul><p>A top comment by Elizabeth suggests the studies linked to prove vegan diets are sufficiently healthy for cats are poor quality and mostly focus on vegetarian over vegan diets, and more rigorous RCTs are needed.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/esA6ukJngGDMorMA8/here-s-a-comprehensive-fact-sheet-of-almost-all-the-ways\"><strong><u>Here's a comprehensive fact sheet of almost all the ways animals are mistreated in factory farms</u></strong></a></p><p><i>by Omnizoid</i></p><p>Linkpost for&nbsp;<a href=\"https://controlledopposition.substack.com/p/factory-farming-is-literally-torture\"><u>this blog post</u></a>, which provides details of different forms of harm in factory farms for each of pigs, broiler chickens, egg-laying hens, turkeys, beef cows, and dairy cows.<br>&nbsp;</p><h1>Opportunities</h1><p><a href=\"https://www.lesswrong.com/posts/pJrebDRBj9gfBE8qE/prizes-for-matrix-completion-problems\"><strong><u>Prizes for matrix completion problems</u></strong></a></p><p><i>by paulfchristiano</i></p><p>Alignment Research Center (ARC) are offering $5K prizes for the completion of either of two self-contained algorithmic questions that have come up in their research. These center on a) the existence of PSD completions and b) fast \u201capproximate squaring\u201d. They are open for three months or until a problem is solved.<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/obs9vnjM3xBZcrEf2/upcoming-ea-conferences-in-2023\"><strong><u>Upcoming EA conferences in 2023</u></strong></a></p><p><i>by OllieBase, Eli_Nathan</i></p><p>Including:</p><ul><li>EAG London: May 19 - 21 (applications closed)</li><li>EAG Boston: October 27 - 29 (applications open)</li><li>EAGx Warsaw: June 9-11 (applications open)</li><li>EAGxNYC: August 18-20 (applications open)</li><li>EAGxBerlin: September 8-10</li><li>EAGxAustralia: September 22-24 (provisional dates)</li><li>EAGxPhilippines: October 20-22 (provisional dates)</li><li>EAGxVirtual: November 17-19 (provisional dates)<br>&nbsp;</li></ul><h1>Rationality, Productivity &amp; Life Advice</h1><p><a href=\"https://forum.effectivealtruism.org/posts/23P6XCcGdrGGFrN6Z/test-fit-for-roles-job-types-work-types-not-cause-areas\"><strong><u>Test fit for roles / job types / work types, not cause areas</u></strong></a></p><p><i>by freedomandutility</i></p><p>Suggests fit should be evaluated on role type, and cause area picked by impact potential. For instance, if you dislike wet-lab research in biosecurity, you\u2019ll probably dislike it in alternative proteins as well. Similarly with other cross-cause roles and tasks like entrepreneurship, operations, and types of research (eg. literature reviews, qualitative, quantitative, clinical trials).<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/ovAtvJSm88Pbtdqkm/advice-for-interacting-with-busy-people\"><strong><u>Advice for interacting with busy people</u></strong></a></p><p><i>by Severin</i></p><p>Suggests the time of central information nodes is valuable, so it\u2019s worth:</p><ol><li>Making requests concise and clear.</li><li>Leaning toward asking for resources and introductions over opinions.</li><li>Preparing ahead of time.</li><li>Making it easy to say no, and not taking it personally if they do / if a response is delayed.</li></ol><p>If doing all this, lean towards asking and letting the busy person decide if they respond - lots of value can be lost by under-communicating.<br>&nbsp;</p><h1>Community &amp; Media</h1><p><a href=\"https://forum.effectivealtruism.org/posts/xen7oLdHwoHDTG4hR/legal-priorities-project-annual-report-2022\"><strong><u>Legal Priorities Project \u2013 Annual Report 2022</u></strong></a></p><p><i>by Legal Priorities Project, Alfredo_Parra, Christoph_Winter</i></p><p>In 2022, the Legal Priorities Project had 3.6 FTE researchers and spent ~$1.1M. They produced:</p><ul><li>10 peer-reviewed papers either accepted for publication or under review, and one book contract. They also produced some working papers and blog posts.</li><li>Analyzed ongoing policy efforts, getting positive feedback on their research from policymakers.</li><li>Ran events for students and academics at top institutions eg. the&nbsp;<a href=\"https://www.legalpriorities.org/institute22.html\"><u>Legal Priorities Summer Institute</u></a>, a writing competition, and a multidisciplinary forum on longtermism and law. These had positive feedback and totaled hundreds of applications.</li></ul><p>In 2023 they plan to:</p><ul><li>Shift their research agenda to focus more on risks from AI.</li><li>Increase non-academic publications (eg. policy / tech reports and blog posts) to make their research more accessible.</li><li>1-2 field building programmes such as a summer research fellowship.</li><li>Raise a minimum of $1.1M to maintain current level of operations for another year, with an ideal goal of hiring 1-3 additional FTE.</li></ul><p>You can donate&nbsp;<a href=\"https://www.every.org/legalpriorities\"><u>here</u></a>, or subscribe to their newsletter&nbsp;<a href=\"https://www.legalpriorities.org/contact\"><u>here</u></a>.<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/EpyJMXZTqLDiKaXzu/if-you-d-like-to-do-something-about-sexual-misconduct-and\"><strong><u>If you\u2019d like to do something about sexual misconduct and don\u2019t know what to do\u2026</u></strong></a></p><p><i>by Habiba</i></p><p>A guide for those who want to do something about sexual misconduct and harassment in EA but don\u2019t know where to start. Key suggestions include:</p><ol><li>Remember we\u2019re all within the system. Identify \u201cpaths of least resistance\u201d that lead to harm in social situations, and don\u2019t take them. Reflect on your own behavior.</li><li>Learn about the issue (especially before suggesting improvements).</li><li>Act with compassion - assume anyone in the discussion may have been personally affected. Listen to community members empathetically. Support them, and respect their preferences on confidentiality and autonomy over what to do next.</li><li>Interject when you see harmful behavior. Take action regarding people who have harmed others (though consult with others first).</li><li>Participate in the discussion, in community wide initiatives, and via personal actions like donating / volunteering / raising awareness.<br>&nbsp;</li></ol><p><a href=\"https://forum.effectivealtruism.org/posts/ZKYpu4WAiwTXDSrX8/review-of-the-good-it-promises-the-harm-it-does\"><strong><u>Review of The Good It Promises, the Harm It Does</u></strong></a></p><p><i>by Richard Y Chappell</i></p><p>Review of&nbsp;<a href=\"https://global.oup.com/academic/product/the-good-it-promises-the-harm-it-does-9780197655702\"><u>The Good It Promises, the Harm It Does: Critical Essays on Effective Altruism</u></a>. The reviewer didn\u2019t find much value in the book. Their thoughts included:</p><ul><li>The book focuses on social justice perspectives and EA\u2019s lack of alignment with these.<br>Examples quote from the book:<ul><li>\u201cNormative Whiteness is cooked into [EA\u2019s] ideological foundation, because it focuses on maximizing the effectiveness of donors\u2019 resources.\u201d</li></ul></li><li>The costs of EA identified aren\u2019t compared to the gains.</li><li>It focuses on animal advocacy (and has a couple good points there eg. cage-free campaigns can miss the chance to push away from industrial farming altogether).</li></ul><p>Top commenters suggest regardless if some articles are poor quality, it\u2019s important to understand the perspectives and challenges that the book offers. David Thorstad shares&nbsp;<a href=\"https://ineffectivealtruismblog.com/category/my-papers/the-good-it-promises/\"><u>their blog</u></a> where they\u2019ve had a go at breaking this down. Dr. David Mathers suggests the key challenge presented is asking why EA hasn\u2019t found more worthwhile in rights movements, or worked to collaborate with them, given their historical successes.<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/sDkHTdBsrpz7teMR2/please-don-t-vote-brigade\"><strong><u>Please don\u2019t vote brigade</u></strong></a></p><p><i>by Lizka</i></p><p>Requests that forum users don\u2019t ask others to upvote or downvote specific posts. This messes up the ranking of posts, and can result in being banned. If you suspect vote brigading, let forum moderators know.<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/vaqoGFRdi6ftvwGkn/what-is-effective-altruism-how-could-it-be-improved\"><strong><u>What is effective altruism? How could it be improved?</u></strong></a></p><p><i>by MichaelPlant</i></p><p>The author suggests Effective Altruism is like a market where people can buy and sell goods for how best to help others. Centre for Effective Altruism (CEA) staff are the market\u2019s administrators. The issues are:</p><ul><li>CEA is also a market participant, promoting particular ideas and running key orgs.</li><li>There is primarily one major buyer in the market (Open Philanthropy).</li></ul><p>They suggest that CEA should have its trustees elected by the community, strive to be impartial rather than take a stand on priorities, and that EA be run as an impartial market to attract more large \u2018buyers\u2019.<br><br>Several top comments disagree with the market analogy / argument, but find some sub-points useful. Commenters discuss ways to increase the voice of the community (eg. AMAs with CEA), and possible distinctions that could or should exist between object-level organizations focusing on cause areas and central organizations supporting them.</p><p><br>&nbsp;</p>", "user": {"username": "GreyArea"}}, {"_id": "dpY7eHHDaKazk9uic", "title": "Virtue vs Obligation (The Caplan-Singer Debate)", "postedAt": "2023-05-09T09:20:01.658Z", "htmlBody": "<h2>I. Introduction</h2><p>Several months ago, Bryan Caplan and Peter Singer held a <a href=\"https://betonit.substack.com/p/the-caplan-singer-debate-my-opening\"><u>debate</u></a>. The topic: \u201cDo the Rich Pay Their Fair Share?\u201d. Sadly, this debate was not recorded, so I wasn\u2019t able to watch it. But I read <a href=\"https://betonit.substack.com/p/reflections-on-the-caplan-singer\"><u>Caplan\u2019s</u></a> <a href=\"https://betonit.substack.com/p/singer-and-the-noble-lie\"><u>followup</u></a>, and I disagreed with some of his takeaways.</p><p>Caplan\u2019s basic position in this debate is that rich philanthropists should be hailed as heroes for going above and beyond to help others, rather than merely acknowledged for doing their duty, and <i>certainly </i>not shamed for failing to do as much they could do in principle.</p><p>Caplan was apparently surprised to find that Singer was very sympathetic to this outlook. Caplan says, \u201cSinger seems to have completely abandoned the extreme view that we are morally obliged to give away all our surplus resources to the poor. Instead, he just pushed for a rough charity target of 10% of total income.\u201d He goes on to wonder, \u201cSinger reaffirmed his devotion to utilitarianism, but never explained <i>why</i> he drastically changed his mind about the morally obligatory level of donation.\u201d</p><p>Caplan is asking: what changed? How can Singer justify this lax moral attitude given his true underlying utilitarian principles? How can he call rich philanthropists virtuous if they are merely doing their duty (in fact, far less than their duty)? In search of an explanation, Caplan accuses Singer of a \u201cNoble Lie\u201d, purposely \u201cmisstating the implications of utilitarianism\u201d in order to not turn people off from the idea of giving.</p><p>He goes on to cite one of Singer\u2019s papers <a href=\"https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9329.2009.00449.x\"><i><u>Secrecy in Consequentialism</u></i></a> as support for this claim. The paper lays out the following tenets of \u201cesoteric morality\u201d:</p><blockquote><ul><li>There are acts which are right only if no one \u2013 or virtually no one \u2013 will get to know about them. The rightness of an act, in other words, may depend on its secrecy. This can have implications for how often, and in what circumstances, such an act may be done.</li><li>Some people know better, or can learn better, than others what it is right to do in certain circumstances.</li><li>There are at least two different sets of instruction, or moral codes, suitable for the different categories of people. This raises the question whether there are also different standards by which we should judge what people do.</li><li>Though the consequentialist believes that acts are right only if they have consequences at least as good as anything else the agent could have done, the consequentialist may need to discourage others from embracing consequentialism.</li><li>Paradoxically, it may be the case that philosophers who support esoteric morality should not do so openly, because as Sidgwick said: \u2018it seems expedient that the doctrine that esoteric morality is expedient should itself be kept esoteric\u2019.</li></ul></blockquote><p>Singer and his co-authors use various examples to explain these points, including the exact issue at hand. If openly advocating for consequentialism could turn people off by being too demanding, then perhaps we should not do so? And if it really is the case that we need to keep some people in the dark about the true moral code, then perhaps we shouldn\u2019t talk too openly about esoteric morality, since this risks giving away the game.</p><p>Their thesis is, \u201cEsoteric morality is a necessary part of a consequentialist theory, and all of the points above can be defended.\u201d From this, Caplan infers that \u201cthe conclusion that Singer is feigning moderation for the greater good isn\u2019t merely probable. It is all but certain.\u201d Singer\u2019s paper also gives the fairly clear example that we must provide simple moral rules to children, which may oversimplify our true moral beliefs. Caplan follows this up by saying, \u201cFrom this perspective, the same obviously goes for morally immature adults. Which, for a strict utilitarian, probably sum to 99%+ of humanity.\u201d</p><p>I think all this is a pretty significant misunderstanding of utilitarianism. I can\u2019t necessarily speak for Singer, but in my view, the entire notion of \u201cobligation\u201d as something distinct from \u201cvirtue\u201d simply does not appear natively within the utilitarian framework. These concepts are not part of its moral ontology, nor of mine. I\u2019d like to unpack this a bit.</p><p>Additionally, in spite of saying I can\u2019t speak for Singer, I\u2019ll show some evidence that his view really does align with the one I lay out here. I\u2019ll also argue Caplan is misconstruing his view in a few places. If you\u2019d like to read that part first because you care more about what Singer thinks than I do (eminently fair), you can jump down to section IV before (hopefully?) coming back to II.</p><p>&nbsp;</p><h2>II. There is just the utility function</h2><p>My basic view is this: under utilitarianism, there is just the utility function. There\u2019s the action that achieves the best outcome, and there\u2019s a measure for how good various actions are in comparison to eachother, and there is no leftover question. Trying to additionally ask if some action is \u201cobligatory\u201d or \u201cmerely virtuous\u201d is like trying to ask if an object is <i>really </i>a <a href=\"https://www.lesswrong.com/posts/4FcxgdvdQP45D6Skg/disguised-queries\"><u>blegg or a rube</u></a>. It\u2019s just a non-question.</p><p>However, we should avoid making a <a href=\"https://www.lesswrong.com/s/p3TndjYbdYaiWwm9x/p/mTf8MkpAigm3HP6x2\"><u>false reduction</u></a>. Are we attempting to <i>explain</i> the distinction between virtue and duty, or <a href=\"https://www.lesswrong.com/s/p3TndjYbdYaiWwm9x/p/cphoF8naigLhRf3tu\"><i><u>explain it away</u></i></a>? If the latter, we must do more than just <i>say </i>that utilitarianism dissolves this distinction; we must demonstrate <i>how</i>, and hopefully manage to feel it on a gut level. And if the former, we <i>still</i> have more work to do, since we must show <i>how </i>our intuitive notions of virtue and duty can be reduced to questions of utility.</p><p>I think there are clearly some notions of virtue and duty that utilitarianism does away with, while others are preserved. What are these different notions? The<i> </i>view that many people probably hold somewhat instinctively is that if most<i> </i>people perform some good action, then it is morally obligatory, while if few do, then it\u2019s just virtuous. For example, most people don\u2019t steal, and we generally consider it morally obligatory not to. If you were out with a friend and they started shoplifting, you\u2019d probably look askance at them. On the other hand, few people are vegans. Many people see going vegan as a noble endeavor, but not as an obligation upon us all.</p><p>(You might worry this argument is backwards. Might it not be the case that it\u2019s precisely <i>because </i>we believe something is morally obligatory, that we all do it? There\u2019s obviously some truth to that, but I think it\u2019s still the case that when deciding if you <i>must </i>take some good action, you\u2019re liable to look around and see what others are doing. If everyone around you is going vegan, you might start to feel some pressure. And conversely, if you grew up around friends shoplifting, you might not view it as such a big deal today. In any case\u2026)</p><p>This view, on its own, seems hard to justify on utilitarian grounds. If my action would do just as much good regardless of what others are doing, then how can their behavior have any effect on how right it is for me to do it? If others thought, say, freeing slaves was perhaps virtuous, but not obligatory, this should not change how right it would in fact be to end slavery.</p><p>A slightly more reflective view might say that we are obligated to not <i>cause </i>harm, but <i>preventing</i> harm or <i>promoting the good</i> are merely virtuous. This captures the idea that not stealing is obligatory and, say, giving to charity is virtuous. But it also seems to indicate that going vegan is obligatory, since we are otherwise actively inflicting harm on animals (which doesn\u2019t make this view <i>wrong</i>, of course).</p><p>This, too, is hard to justify on utilitarian grounds. There\u2019s no inherent asymmetry in utilitarianism between causing harming and allowing harm, or between harms and benefits. Indeed, it\u2019s hard to even find a logical basis for the difference between \u201callowing\u201d and \u201ccausing\u201d; inaction is also an action, which has causal effects like any other. It can likewise be hard to draw a line between causing harm and failing to provide benefits. If I refuse to feed my child, am I causing harm, or just allowing it? Am I inflicting harm, or simply not going out of my way to provide them with the benefit of food?</p><p>Justified or not, I think these are the two primary factors behind what cause us to deem some act \u201cvirtuous\u201d rather than \u201cobligatory\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa3gs7wupxpn\"><sup><a href=\"#fna3gs7wupxpn\">[1]</a></sup></span>. Playing at a bit of evolutionary psychology, I think we can see some reasons why this might be so.</p><p>First, in the ancestral environment, overt acts of harm would be easier to verify than someone simply failing to do all they could, which would make their punishment easier to justify. Imagine condemning somebody as a murderer for failing to save another\u2019s life. For this to make sense, you\u2019d have to really know that they were in a position to easily save the other person\u2019s life and chose not to. This is hard to verify, and ostracizing someone from your tribe is a costly act, not to be undertaken lightly. But you <i>do</i> want to incentivize your tribespeople to save eachother if they can, so it would make more sense to reward them for saving lives, rather than punishing them for failing to do so.</p><p>Similarly, it would be hard to justify punishing one person for violating a moral rule that most others also do not follow. And if each individual does not want to go out of their way to help others without getting something in return, this probably will not become a morally obligatory rule. Instead, a bare-bones set of rules that we\u2019d all prefer everyone follow is what will become universal. For example, a rule such as \u201cnobody kill eachother\u201d is easier to get near-universal support for than \u201cthe strong should feed the weak\u201d.</p><p>These evolutionary reasons don\u2019t justify our moral intuitions, but importantly, they may affect which acts of praise and blame will be effective. This informs what I think is the actual proper view of duty and virtue: there is no inherent fact of the matter about which acts are praiseworthy and which are blameworthy; instead, praise and blame are acts in themselves and should be applied in a manner that yields the best consequences.</p><p>I happen to believe there are something like moral facts, and that right and wrong aren\u2019t just some social construct. But I don\u2019t think duty and virtue are basic building blocks of the true moral ontology. They really <i>are </i>social constructs. But this doesn\u2019t make them useless concepts, of course. Let\u2019s look at some examples of how to apply this reasoning.</p><p>&nbsp;</p><h2>III. Case studies</h2><p>Compare the \u201cabortion is murder\u201d crowd in the pro-life movement to how effective altruists discuss the <a href=\"https://www.youtube.com/watch?v=wMb26ryjDuU&amp;t=5s\"><u>shallow pond thought experiment</u></a>. Both groups essentially believe that ordinary people engage in an act morally equivalent to causing the death of a child. Yet EAs typically do not go around brandishing signs emblazoned with a \u201cMoney is Murder\u201d slogan, or aggressively harassing those who fail to give to charity.</p><p>I think this is because EAs realize, whether or not spending your own money on luxuries is <i>morally </i>equivalent to murder, they are not <i>psychologically </i>equivalent. The sort of person who would actually walk by a child and let them drown to death is just plainly not the same kind of person as one who simply fails to give to charity. As such, they require different responses.</p><p>EAs have typically taken the tact of using thought experiments such as the shallow pond to convey the intuition of the moral equivalence. We hope that people absorb this intuition and take different actions as a result. This is still a shame-based framing, but it\u2019s very different from aggressive anti-abortion tactics.</p><p>My mother-in-law is pro-life and used to very much be in the \u201cabortion is murder\u201d crowd. According to my wife, this changed when she realized that ordinary people get abortions. Apparently, she did not know this before, or had not internalized it. In her mind, only overt monsters would ever get an abortion. And if this were so, perhaps hatred and shunning would be an appropriate response. But since in fact ordinary people get abortions, she could no longer hold to this position.</p><p>On my view, aggressive pro-lifers are making a pretty explicit strategic error. The task before them, I think, is to 1) establish the moral equivalence of abortion and murder, since many people do not believe this, and 2) convey this equivalence intuitively, since even if it is true, they won\u2019t <i>feel </i>the same to many people.</p><p>This is the cash value of viewing praise and blame as acts in themselves. You can ask yourself how effective you expect an act of blame to be, or even go out and test it in practice. You can moderate it if it seems to be too aggressive. All this can help you get more of what you want and save more children\u2019s lives, which is the whole point of what we\u2019re doing here.</p><p>Going further, we don\u2019t necessarily have to use a shame-based framing at all, even a mild one. In <i>Doing Good Better</i>, Will MacAskill paints an alternative picture:</p><blockquote><p>Imagine saving a single person\u2019s life: you pass a burning building, kick the door down, rush through the smoke and flames, and drag a young child to safety. If you did that, it would stay with you for the rest of your life. If you saved several people\u2019s lives\u2014running into a burning building one week, rescuing someone from drowning the next week, and diving in front of a bullet the week after\u2014you\u2019d think your life was really special. You\u2019d be in the news. You\u2019d be a hero. But we can do <i>far</i> more than that.</p></blockquote><p>This is a different framing for the exact same underlying moral fact: you can use your resources to save lives. It\u2019s not that one of these framings is true and the other false; they are different angles on the same problem. I find both stories about equally compelling, but in subtly different ways, and I try to internalize the pair. For some people, one might be more effective than the other. That\u2019s all well and good! It\u2019s worth having both of them out there.</p><p>The overall point is, given the state of human psychology, if we threaten to chastise every person as a monster unless they give everything they have to charity, this is liable to cause us nothing but headaches. For whatever reason, this is true in a way that it\u2019s <i>not </i>true for chastising murderers. And there\u2019s nothing inherent in morality that <i>compels </i>us to punish people in this way if it won\u2019t do any good.</p><p>It\u2019s possible that if human psychology were different, we could feel the equivalence between all our actions on a gut level, and blame and praise could be applied exactly in proportion to the utility of each action. This could very well be a better world than our own, a higher point on the <a href=\"https://en.wikipedia.org/wiki/The_Moral_Landscape\"><u>moral landscape</u></a>, as Sam Harris might put it. But as Harris would also say, that does not mean we can get from here to there without a huge valley in between.</p><p>&nbsp;</p><h2>IV. What does Singer think?</h2><h3>Famine, Affluence, and Morality</h3><p>Caplan claims that Singer once held that we\u2019re all obligated to give away the entirety of our surplus but has since abandoned<i> </i>this position in favor of just giving 10%. Is this really true?</p><p>Singer\u2019s foundational essay on this topic is, of course, his 1972 paper <a href=\"https://personal.lse.ac.uk/robert49/teaching/mm/articles/Singer_1972Famine.pdf\"><i><u>Famine, Affluence, and Morality</u></i></a>. I concede that Singer seems to have mellowed in his recommendations; the 10% number appears nowhere in this essay. But I don\u2019t think his beliefs (or even his stated beliefs) about our underlying obligations have changed much, if at all. Indeed, a good deal of what he says lines up with my own argument, that utilitarianism simply dissolves the distinction between virtue and duty.</p><p>First, Singer states his thesis: \u201cif it is in our power to prevent something bad from happening, without thereby sacrificing anything of comparable moral importance, we ought, morally, to do it.\u201d He applies this in particular to argue that we ought to spend our money to save the lives of others, rather than on luxuries for ourselves.</p><p>But you might well wonder: are we <i>obligated </i>to do what we ought to do, or would we instead be <i>virtuous</i> for doing so? What does it <i>mean </i>to be \u201cobligated\u201d to do something, anyhow? Singer addresses this with the following footnote:</p><blockquote><p>In view of the special sense philosophers often give to the term, I should say that I use \"obligation\" simply as the abstract noun derived from \"ought,\" so that \"I have an obligation to\" means no more, and no less, than \"I ought to.\"</p></blockquote><p>To me, this reads as Singer stating that, when he claims we are obligated to do something, he just means that it\u2019s better to do so than not. Hence, even though Singer <i>is</i> saying we are obligated to give away our surplus, he is not expressing an opinion of how <i>blameworthy</i> someone is for not doing so. I think this is just different from how Caplan is using the word \u201cobligation\u201d; to him, this word seems to mean \u201csomething someone deserves to be chastised for failing to do\u201d.</p><p>Singer goes on to make some statements very clearly in line with my argument:</p><blockquote><p>The outcome of this argument is that our traditional moral categories are upset. The traditional distinction between duty and charity cannot be drawn, or at least, not in the place we normally draw it\u2026 It may be possible to redraw the distinction between duty and charity in some other place\u2026 It is beyond the scope of my argument to consider whether the distinction should be redrawn or abolished altogether.</p></blockquote><p>Thus, on the question of spending our money either on saving lives or on luxuries for ourselves, Singer does not seem to be saying that donating is our <i>duty</i>, as opposed to being <i>laudable</i>. Instead, he is saying that such a distinction cannot even be made on this issue, and he leaves open the possibility that such a distinction should be \u201cabolished altogether\u201d.</p><p>Singer goes on to endorse the idea that we should view praise and blame as acts in themselves (emphasis my own):</p><blockquote><p>It has been argued by some writers\u2026 that we need to have a basic moral code which is not too far beyond the capacities of the ordinary man, for otherwise there will be a general breakdown of compliance with the moral code\u2026 The issue here is: Where should we drawn [sic] the line between conduct that is required and conduct that is good although not required, <i>so as to get the best possible result</i>? \u2026 What it is possible for a man to do and what he is likely to do are both, I think, very greatly influenced by what people around him are doing and expecting him to do.</p></blockquote><p>On the other hand, the strongest piece of counterevidence is the following line from the paper:</p><blockquote><p>It follows from what I have said earlier that we ought to give money away, rather than spend it on clothes which we do not need to keep us warm. To do so is not charitable, or generous.</p></blockquote><p>Frankly, I think Signer is catching himself in a contradiction, here. He can\u2019t say on the one hand that, when it comes to donating, there is no distinction between duty and charity, while also on the other hand saying donating is one\u2019s duty, rather than being charitable. This is the only line I detect in the paper that indicates we should not congratulate people for their giving, and it seems to oppose the rest of his argument, so I think it\u2019s a bit of an anomaly.</p><p>On the whole, I think the difference between Singer\u2019s old position and his new one is that, before, he was more open to the idea that a social obligation in favor of maximum giving would be productive, whereas now he thinks a target of 10% would be better. This does not look to me like a change in belief on what our underlying obligations are, nor even a change in what he is <i>claiming </i>are our obligations in public.</p><h3>The Life You Can Save</h3><p>Indeed, Singer lays this all out quite clearly in his 2009 book <i>The Life You Can Save</i>. A few quotes from the last two chapters:</p><blockquote><p>In the first part of this book I argued that in order to be good people, we must give until if we gave more, we would be sacrificing something nearly as important as the bad things our donation can prevent. Now\u2026 it\u2019s time to return and probe more deeply the sense that there must be something amiss with this moral argument because its implications go too far.</p></blockquote><blockquote><p>Asking people to give more than almost anyone else gives risks turning them off... To avoid that danger, we should advocate a level of giving that will lead to a positive response\u2026 I think we should advocate the level of giving that will raise the largest possible total, and so have the best consequences.</p></blockquote><blockquote><p>We use praise and blame to influ\u00adence behavior, and the appropriate standard must be relative to what we can reasonably expect most people to do. Hence praise and blame, at least when they are given publicly, should follow the standard that we publicly advocate, not the higher standard that we might apply to our own conduct. We should praise people for doing significantly better than most people in their circumstances would do, and blame them for doing signifi\u00adcantly worse. If you have done more than your fair share, that must at least lessen the blame. If you have complied with the public moral code, we should praise you for doing that, rather than blame you for not doing more.</p></blockquote><p>Thus, Singer says quite clearly that at the deepest level, we ought to hold ourselves to as high a standard as possible, but that we should publicly advocate for the level of giving that does the most good. These are his most up-to-date views on global poverty. This book and its surrounding discussion is presumably where Caplan heard of Singer in the first place, so unless he was reading Singer back in 1972, I don\u2019t think he can justifiably call this a change in position, let alone a \u201cdrastic\u201d one.</p><p>He also cannot call this dishonest. Singer has put it all in print for us! He is very clear about what he\u2019s doing and why. I feel quite confident that if you were to just<i> ask </i>him, he would tell you, \u201cOf course giving as much as you can would do the most good. But we need a standard that is realistic and sustainable. So I will praise you as a good person for doing more than others.\u201d This is not a lie of any sort, noble or otherwise. I think there\u2019s some assumption from Caplan that Singer can\u2019t <i>mean it </i>when he says people are good for donating; this assumption just seems unfounded to me, and I want to know where he\u2019s getting it from.</p><p>Singer even has a section at the end of his book entitled <i>Judging the Rich and Famous</i>, the exact topic of the debate at hand! There he says:</p><blockquote><p>[Bill] Gates deserves to be commended for his generosity and for the farsighted way in which he has chosen the goals and methodology of his foundation\u2026 On the other hand, for those among the superrich who live with particular extravagance and give relatively little, some blame would not be out of order.</p></blockquote><p>So since at least 2009, Singer has agreed that we should commend superrich philanthropists as heroes, so long as they give a relatively large amount to effective causes.</p><h3>Secrecy in Consequentialism</h3><p>Finally, I don\u2019t think Caplan\u2019s representation of <a href=\"https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9329.2009.00449.x\"><i><u>Secrecy in Consequentialism</u></i></a><i> </i>is fair. First off, Caplan writes that Singer and his co-authors \u201capprovingly remark\u201d:</p><blockquote><p>The idea that it is better if some moral views are not widely known was not invented by Sidgwick. In Plato\u2019s Republic, Socrates proposes that ordinary people be brought up to believe that everyone is born \u2018from the earth\u2019 into one of three classes, gold, silver or bronze, and living justly consists in doing what is in their nature. Only the philosopher-rulers will know that this is really a myth, a \u2018noble lie\u2019.</p></blockquote><p>I think he is putting words in their mouths by calling this remark \u201capproving\u201d. If you read the paper, it seems clear that they are simply referencing prior work in the area. They follow up the reference to Plato with a little-known piece of Catholic doctrine:</p><blockquote><p>Catholic moral theology has also found that it cannot avoid the need for a doctrine that is plainly not intended to be widely known. This applies, for example, to the doctrine of \u2018mental reservation,\u2019 which holds that it is permissible to say something that misleads, and yet avoid the sin of lying by mentally adding information that would, if spoken, make the response truthful. For example, in Charles McFadden\u2019s <i>Medical Ethics</i>, a text written from a Roman Catholic perspective, doctors and nurses are advised that if a feverish patient asks what his temperature is, and the truth would alarm him and make his condition worse, it is justifiable to reply \u2018Your temperature is normal today\u2019 while making the mental reservation that it is normal for someone in the patient\u2019s precise physical condition.</p></blockquote><p>It is not at all clear that Singer <i>supports </i>this form of dishonesty, nor that he approves of Plato\u2019s philosopher kings. In fact, the authors distance themselves significantly from such everyday uses of secrecy. After justifying why consequentialist ethics must in principle possess <i>some </i>form of esoteric morality, they work hard to qualify their position, saying:</p><blockquote><p>Nevertheless there are good reasons why consequentialists should share in the broad support for transparency in ethics, and hence should avoid esoteric morality in most circumstances.</p></blockquote><p>In particular, they discuss five important considerations: the benefits of a shared code, the benefits of open discussion, the dangers of elitism, the public nature of education, and respecting preferences. They go on about these at some length.</p><p>They also state near the very end:</p><blockquote><p>[W]e have to agree that we have different standards of judgment for those who can think critically about the nature of morality, and those who can\u2019t, or perhaps shouldn\u2019t. And that could even include all of us, in some circumstances.</p></blockquote><p>Recall that Caplan says, for a strict utilitarian, morally immature adults \u201cprobably sum to 99%+ of humanity\u201d. It seems he is implying that utilitarians see themselves as being in the 1%, justified in manipulating the masses on a regular basis for the greater good. This is very misleading and is certainly not the stated position of the authors. They have said clearly that esoteric morality should be avoided in most circumstances, and that they themselves fall in the 99% in some circumstances, not occupying some privileged 1%.</p><p>&nbsp;</p><h2>V. Conclusion</h2><p>For the record, I totally love Bryan Caplan and all his work. But the claim that Peter Singer is taking part in some big Noble Lie seems like a pretty serious accusation to me. I think Caplan misunderstands the utilitarian framework and misrepresents the conclusions of <i>Secrecy in Consequentialism</i>. And overall, I think he just hasn\u2019t demonstrated any concrete false statement that Singer has made; choices of framing are not dishonest, especially if you are open about what you are doing, which Singer is.</p><p>Furthermore, as some commenters on Caplan\u2019s post have said, as well the authors of <i>Secrecy in Consequentialism </i>themselves, a really deep commitment to esoteric morality would imply not writing <i>Secrecy in Consequentialism</i>! It is comforting that they were willing to write this paper at all, since it in fact shows a pretty significant amount of transparency.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna3gs7wupxpn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa3gs7wupxpn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A third big one would be when we\u2019ve entered into some kind of contractual agreement. This also has interesting evolutionary justifications, such as when we\u2019ve established a pattern of reciprocal altruism with someone and must be on the lookout for cheaters. I think this serves as an additional example for my main point, but I\u2019d like to keep the core argument simple(r).</p></div></li></ol>", "user": {"username": "Noah Topper"}}, {"_id": "7vjKCSRz7JitfHoWZ", "title": "Effective Altruism Australia Environment x EA UQ: Supporting Effective Climate Charities from Australia", "postedAt": "2023-05-08T22:43:54.742Z", "htmlBody": "<p>Most of us do what we can to help the environment: we use less electricity, use solar, recycle, drive less, drive electric/hybrid, and maybe even change our diets. But, $1000 to the <a href=\"https://effectivealtruism.org.au/environment\">world\u2019s most effective climate charities</a> is more effective than doing all of those things combined. How is that even possible? And what are those charities?</p><p>In conjunction with EA UQ, come join us to celebrate the launch of <a href=\"https://effectivealtruism.org.au/environment\">Effective Altruism Australia Environment (EAAE)</a>. EAAE helps Australians maximise the impact of their donations in the environmental space. We help <a href=\"https://effectivealtruism.org.au/environment\">Australians find and donate to evidence-based, cost-effective, and high-leverage organisations working to combat climate change.</a></p><p>We are inviting everyone in Brisbane to come together to celebrate the launch of and learn more about this new project. We will hear about our evidence-based recommended partner charities, and learn how to take action. This will be followed by refreshments, and the opportunity to meet like-minded people.</p><p>Location: The University of Queensland, Global Change Institute (20), Seminar Room 275, St Lucia, QLD 4072</p>", "user": {"username": "mnoetel"}}, {"_id": "fAzEiWZykBrBtNQs2", "title": "All AGI Safety questions welcome (especially basic ones) [May 2023]", "postedAt": "2023-05-08T22:30:50.293Z", "htmlBody": "", "user": {"username": "StevenKaas"}}, {"_id": "3KAuAS2shyDwnjzNa", "title": "Predictable updating about AI risk", "postedAt": "2023-05-08T22:05:39.687Z", "htmlBody": "<p>(Cross-posted from <a href=\"https://joecarlsmith.com/2023/05/08/predictable-updating-about-ai-risk\">my website</a>. Podcast version <a href=\"https://www.buzzsprout.com/2034731/12809255-predictable-updating-about-ai-risk\">here</a>, or search \"Joe Carlsmith Audio\" on your podcast app.)</p><blockquote><p><i>\"This present moment used to be the unimaginable future.\"</i></p><p><i>- Stewart Brand</i></p></blockquote><h2>1. Introduction</h2><p>Here\u2019s a pattern you may have noticed. A new frontier AI, like GPT-4, gets released. People play with it. It\u2019s better than the previous AIs, and many people are impressed. And as a result, many people who weren\u2019t worried about existential risk from misaligned AI (hereafter: \u201cAI risk\u201d) get much more worried.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnpfbbdkv1or\"><sup><a href=\"#fnnpfbbdkv1or\">[1]</a></sup></span></p><p>Now, if these people didn\u2019t expect AI to get so much better so soon, such a pattern can make sense. And so, too, if they got other unexpected evidence for AI risk \u2013 for example, concerned experts <a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\">signing letters</a> and <a href=\"https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\">quitting their jobs</a>.</p><p>But if you\u2019re a good Bayesian, and you currently put low probability on existential catastrophe from misaligned AI (hereafter: \u201cAI doom\u201d), you probably shouldn\u2019t be able to predict that this pattern will happen to you in the future.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflb07r9kktsi\"><sup><a href=\"#fnlb07r9kktsi\">[2]</a></sup></span>&nbsp;When GPT-5 comes out, for example, it probably shouldn\u2019t be the case that your probability on doom goes up a bunch. Similarly, it probably shouldn\u2019t be the case that if you could see, now, the sorts of AI systems we\u2019ll have in 2030, or 2050, that you\u2019d get a lot more worried about doom than you are now.</p><p>But I worry that we\u2019re going to see this pattern anyway. Indeed, I\u2019ve seen it myself. I\u2019m working on fixing the problem. And I think we, as a collective discourse, should try to fix it, too. In particular: I think we\u2019re in a position to predict, now, that AI is going to get a lot better in the coming years. I think we should worry, now, accordingly, without having to see these much-better AIs up close. If we do this right, then in expectation, when we confront GPT-5 (or GPT-6, or <a href=\"https://agentgpt.reworkd.ai/\">Agent-GPT</a>-8, or <a href=\"https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity/\">Chaos-GPT</a>-10) in the flesh, in all the concreteness and detail and not-a-game-ness of the real world, we\u2019ll be just as scared as we are now.</p><p>This essay is about what \u201cdoing this right\u201d looks like. In particular: part of what happens, when you meet something in the flesh, is that it \u201cseems more real\u201d at a gut level. So the essay is partly a reflection on the epistemology of guts: of visceral vs. abstract; \u201cup close\u201d vs. \u201cfar away.\u201d My views on this have changed over the years: and in particular, I now put less weight on my gut\u2019s (comparatively skeptical) views about doom.</p><p>But the essay is also about grokking some basic Bayesianism about future evidence, dispelling a common misconception about it (namely: that directional updates shouldn\u2019t be predictable in general), and pointing at some of the constraints it places on our beliefs over time, especially with respect to stuff we\u2019re currently skeptical or dismissive about. For example, at least in theory: you should never think it &gt;50% that your credence on something will later double; never &gt;10% that it will later 10x, and so forth. So if you\u2019re currently e.g. 1% or less on AI doom, you should think it\u2019s less than 50% likely that you\u2019ll ever be at 2%; less than 10% likely that you\u2019ll ever be at 10%, and so on. And if your credence is very small, or if you\u2019re acting dismissive, you should be very confident you\u2019ll never end up worried. Are you?</p><p>I also discuss when, exactly, it\u2019s problematic to update in predictable directions. My sense is that generally, you should expect to update in the direction of the <i>truth</i> as the evidence comes in; and thus, that people who think AI doom unlikely should expect to feel <i>less worried</i> as time goes on (such that consistently getting more worried is a red flag). But in the case of AI risk, I think at least some non-crazy views should actually expect to get <i>more worried</i> over time, even while being fairly non-worried now. In particular, if you think you face a small risk conditional on something likely-but-not-certain (for example, AGI getting developed by blah date), you can sometimes expect to update towards facing the risk, and thus towards greater worry, before you update towards being safe. But there are still limits to how much more worried you can predictably end up.</p><p>Importantly, none of this is meant to encourage consistency with respect to views you held in the past, at the expense of reasonableness in the present or future. If you said .1% last year, and you\u2019re at 10% now (or if you hit 90% when you see GPT-6): well, better to just say \u201c<a href=\"https://www.lesswrong.com/posts/wCqfCLs8z5Qw4GbKS/the-importance-of-saying-oops\">oops</a>.\u201d Indeed, I\u2019ve been saying \u201coops\u201d myself about various things. And more generally, applying basic Bayesianism in practice takes lots of taste. But faced with predictable progress towards advanced but mostly-still-abstract-for-now AI, I think it\u2019s good to keep in mind.</p><p>I close with some thoughts on how we will each look back on what we did, or didn\u2019t do, during the lead-up to AGI, once the truth about the risks is made plain.</p><p><i>Thanks to Katja Grace for extensive discussion and inspiration. See also citations in the main text and footnotes for specific points and examples that originated with Katja. And thanks also to Leopold Aschenbrenner for comments. Some of my thinking and writing on this topic occurred in the context of my work for Open Philanthropy, but I\u2019m speaking only for myself and not for my employer.</i></p><h2>2. Sometimes predictably-real stuff doesn\u2019t feel real yet</h2><blockquote><p><i>\"Every year without knowing it I have passed the day</i></p><p><i>When the last fires will wave to me</i></p><p><i>And the silence will set out</i></p><p><i>Tireless traveler</i></p><p><i>Like the beam of a lightless star\"</i></p><p><i>- </i><a href=\"https://merwinconservancy.org/2020/03/poem-of-the-week-for-the-anniversary-of-my-death-2/\"><i>W.S. Merwin</i></a><i>, \u201cFor the Anniversary of My Death\u201d</i></p></blockquote><p>I first heard about AI risk in 2013. I was at a picnic-like thing, talking with someone from the Future of Humanity Institute. He mentioned AI risk. I laughed and said something about \u201clike in the movie <i>I, Robot</i>?\u201d He didn\u2019t laugh.</p><p>Later, I talked with more people, and read Bostrom\u2019s <a href=\"https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=\">Superintelligence</a>. I had questions, but the argument seemed strong enough to take seriously. And at an intellectual level, the risk at stake seemed like a big deal.</p><p>At an emotional level, though, it didn\u2019t <i>feel real</i>. It felt, rather, like an abstraction. I had trouble imagining what a real-world AGI would be like, or how it would kill me. When I thought about nuclear war, I imagined flames and charred cities and poisoned ash and starvation. When I thought about biorisk, I imagined sores and coughing blood and hazmat suits and body bags. When I thought about AI risk, I imagined, um \u2026 nano-bots? I wasn\u2019t good at imagining nano-bots.</p><p>I remember looking at some farmland out the window of a bus, and wondering: am I supposed to think that this will all be compute clusters or something? I remember looking at a church and thinking: am I supposed to imagine robots tearing this church apart? I remember a late night at the Future of Humanity Institute office (I ended up working there in 2017-18), asking someone passing through the kitchen how to imagine the AI killing us; he turned to me, pale in the fluorescent light, and said \u201cwhirling knives.\u201d</p><p>Whirling knives? <a href=\"https://twitter.com/ESYudkowsky/status/1438198189782290433\">Diamondoid bacteria</a>? Relentless references to paper-clips, or \u201ctiny molecular squiggles\u201d? I\u2019ve written, elsewhere, about <a href=\"https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see#iv-realization-vs-belief\">the \u201cunreality\u201d of futurism</a>. AI risk had a lot of that for me.</p><p>That is, I wasn\u2019t <i>viscerally worried</i>. I had the concepts. But I didn\u2019t have the \u201cactually\u201d part. And I wasn\u2019t alone. As I started working on the topic more seriously, I met some people who were viscerally freaked-out, depressed, and so on \u2013 whether for good or ill. But I met lots of people who weren\u2019t, and not because they were protecting their mental health or something (or at least, not very consciously). Rather, their head was convinced, but not their gut. Their gut still expected, you know, <a href=\"https://www.cold-takes.com/this-cant-go-on/\">normality</a>.</p><p>At the time, I thought this was an important signal about the epistemic situation. Your gut can be smarter than your head. If your gut isn\u2019t on board, maybe your head should be more skeptical. And having your gut on board with whatever you\u2019re doing seems good from other angles, too.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwkbi3ncx2rn\"><sup><a href=\"#fnwkbi3ncx2rn\">[3]</a></sup></span>&nbsp;I spent time trying to resolve the tension. I made progress, but didn\u2019t wholly sync up. To this day, nano-bots and dyson spheres and the word \u201csingularity\u201d still land in an abstract part of my mind \u2013 the part devoted to a certain kind of conversation, rather than to, like, the dirty car I can see outside my window, and the tufts of grass by the chain-link fence.</p><p>I still think that your gut can be an important signal, and that if you find yourself saying that you believe blah, but you\u2019re not <a href=\"https://www.econlib.org/archives/2016/01/the_invisible_t.html\">feeling</a> or acting like it, you should stop and wonder. And sometimes, people/ideas that try to get you to not listen to your gut are trying (whether intentionally or not) to bypass important defenses. I am not, in what follows, trying to tell you to throw your gut away. And to the extent I am questioning your gut: please, by all means, be more-than-usually wary. Still, though, and speaking personally: I\u2019ve come to put less stock than I used to in my gut\u2019s Bayesian virtue with respect to AI. I want to talk a bit about why.</p><h2>3. When guts go wrong</h2><blockquote><p><i>\"Then I will no longer</i></p><p><i>Find myself in life as in a strange garment</i></p><p><i>Surprised at the earth\u2026\"</i></p><p><i>-</i><a href=\"https://merwinconservancy.org/2020/03/poem-of-the-week-for-the-anniversary-of-my-death-2/\"><i>W.S. Merwin</i></a><i>, \u201cFor the Anniversary of My Death\u201d</i></p></blockquote><p>Part of this is reflection on examples where guts go wrong, especially about the future. There are lots of candidates. Indeed, depending on how sharply we distinguish between your \u201csystem 1\u201d and your gut, a lot of the <a href=\"https://thedecisionlab.com/biases\">biases literature</a> can be read as anti-gut, and a lot of early rationalism as trying to compensate. My interest in head-gut agreement was partly about trying to avoid overcorrection. But there is, indeed, something to be corrected. Here are two examples that seem relevant to predictable updating.</p><h3>3.1 War</h3><blockquote><p><i>\u201cAbstraction is a thing about your mind, and not the world\u2026 Saying that AI risk is abstract is like saying that World War II is abstract, because it\u2019s 1935 and hasn\u2019t happened yet. If it happens, it will be very concrete and bad. It will be the worst thing that has ever happened.\u201d</i></p><p><i>- </i><a href=\"https://www.youtube.com/watch?v=j5Lu01pEDWA\"><i>Katja Grace</i></a></p></blockquote><p>I think Katja\u2019s war example is instructive. Consider some young men heading off to war. There\u2019s a trope, here, about how, when the war is just starting, some men sign up excitedly, with dreams of glory and honor. Then, later, they hit the gritty reality: trenches, swamps, villages burning, friends gasping and gurgling as they die. Ken Burn\u2019s <a href=\"https://www.pbs.org/kenburns/the-vietnam-war/\">Vietnam War documentary</a> has some examples. See also \u201c<a href=\"https://en.wikipedia.org/wiki/Born_on_the_Fourth_of_July_(film)\">Born on the Fourth of July</a>.\u201d The soldiers return, if they return, with a very different picture of war. \u201c<a href=\"https://en.wikipedia.org/wiki/Dulce_et_Decorum_est\">In all my dreams before my helpless sight/ He plunges at me, guttering, choking, drowning</a>\u2026\u201d</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/g8vtduyy14mcz9ozm0i0\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/g8vtduyy14mcz9ozm0i0 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/uoxd24lkorrixz7lh6mn 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/fzfnqzpkpnjn8py7gvm3 402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/c8sdisagcr9pjpikstet 462w\"></p><p><i>Stretcher bearers in World War I (source </i><a href=\"https://commons.wikimedia.org/wiki/File:Stretcher_bearers_Passchendaele_August_1917.jpg\"><i>here</i></a><i>)</i></p><p>Now, a part of this is that their initial picture was <i>wrong</i>. But also, sometimes, it\u2019s that their initial picture was <i>abstract</i>. Maybe, if you\u2019d asked them ahead of time, they\u2019d have said \u201coh yeah, I expect the trenches to be very unpleasant, and that I will likely have to watch some of my friends die.\u201d But their gut didn\u2019t expect this \u2013 or, not hard enough. Surrounded, when they set out, by flags and smiling family members and crisp uniforms, it\u2019s hard to think, too, of flies in the eyes of rotting corpses; or trench-foot, and the taste of mustard gas. And anyway, especially if you\u2019re heading into a very new context, it\u2019s often hard to know the specifics ahead of time, and any sufficiently-concrete image is predictably wrong.</p><p>I worry that we\u2019re heading off to something similar, epistemically, to a new war, with respect to AI risk.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefb4qrdse64bh\"><sup><a href=\"#fnb4qrdse64bh\">[4]</a></sup></span>&nbsp;Not: happily, and with dreams of glory. But still: abstractly. We\u2019re trying to orient intellectually, and to do what makes sense. But we aren\u2019t in connection with what it will actually be like, if AI kicks off hard, and the doomers are right. Which isn\u2019t to say it will be trench foot and mustard gas. Indeed, even if things go horribly wrong eventually, it might actually be awesome in lots of ways for a while (even if also: extremely strange). But whatever it will be, will be a specific but very-different-from-now thing. Guts aren\u2019t good at that. So it\u2019s not, actually, all that surprising if you\u2019re not as viscerally worried as your explicit beliefs would imply.</p><h3>3.2 Death</h3><blockquote><p><i>\"And who by fire, who by water</i></p><p><i>Who in the sunshine, who in the night time</i></p><p><i>Who by high ordeal, who by common trial\u2026\"</i></p><p><i>- </i><a href=\"https://www.youtube.com/watch?v=ilGahIwQEQ0\"><i>Leonard Cohen</i></a></p></blockquote><p>Another famous example here is death. No one knows the date or hour. But we know: someday.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1chknilw5ea\"><sup><a href=\"#fn1chknilw5ea\">[5]</a></sup></span>&nbsp;Right? Well, sort of. We know in the abstract. We know, but don\u2019t always realize. And then sometimes we do, and some vista opens. We reel in some new nothingness. Something burns with new preciousness and urgency.</p><p>And sometimes this happens, specifically, when \u201csomeday, somehow\u201d becomes \u201csoon, like this.\u201d When the doctor tells you: you, by avalanche. You, by powder. The month of May. Slow decay. Suddenly, when you\u2019re actually looking at the scans, when you\u2019re hearing estimates in months, you learn fresh who is calling; and despite having always known, some sort of \u201cupdate\u201d happens. Did the gut not fully believe? One\u2019s own death, after all, is <a href=\"https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see\">hard to see</a>.</p><p>I\u2019ve <a href=\"https://joecarlsmith.com/2020/12/06/thoughts-on-being-mortal#iii\">written about this before</a>. Tim McGraw has a song about the scans thing. \u201c<a href=\"https://www.youtube.com/watch?v=_9TShlMkQnc\">Live like you were dying</a>.\u201d I\u2019m trying. I\u2019m trying to think ahead to that undiscovered hospital. I\u2019m trying to think about what I will take myself to have learned, when I walk out into the parking lot, with only months to live. I\u2019m trying to learn it now instead.</p><p>Really, this is about predictable updating. The nudge in McGraw\u2019s title \u2013 you\u2019re already dying \u2013 is Bayesian. You shouldn\u2019t need the scans. If you know, now, what you\u2019ll learn later, you can learn it now, too. Death teaches unusually predictable lessons \u2013 about fleetingness, beauty, love. And unusually important lessons, too. Bayes bites, here, with special gravity. But there\u2019s some sort of gut problem. The question is how to learn hard enough, and in advance. \u201c<a href=\"https://www.brainyquote.com/quotes/henry_david_thoreau_107665\">And not, when I come to die, to discover that I have not lived</a>.\u201d</p><p>Importantly, though: if your gut thinks you\u2019re not going to die, it\u2019s not actually much evidence. Has your gut been keeping up with the longevity literature? Does it have opinions about cryopreservation? Futurism aside, the gut\u2019s skepticism, here, is an old mistake. And we have practices. Go <a href=\"https://en.wikipedia.org/wiki/Ash_Wednesday\">smear some ashes on your forehead</a>. Go <a href=\"https://en.wikipedia.org/wiki/Sky_burial\">watch some birds eat a corpse</a>. Go put some fruit on the <a href=\"https://en.wikipedia.org/wiki/Ofrenda\">ofrenda</a>, or some flowers on your grandfather\u2019s grave. <a href=\"https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see\">Realization is an art distinct from belief</a>. Sometimes, you already know. Religion, they say, is remembering.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/j6jzirnfnytk58c3im1e\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/j6jzirnfnytk58c3im1e 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/ccatn4jrgpuzbpfmrbx8 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/iena2xpgfxwptsvw4tgt 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/h7hg6ce3qemcoebu8fnv 402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/siqoj6asyzwy0jygmqww 462w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/snvkh3p5eif8ajmtlbez 662w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/cgjlihpyye1xjkdddzja 722w\"><i>Tibetan sky burial. (Source </i><a href=\"https://commons.wikimedia.org/wiki/File:Bundesarchiv_Bild_135-S-12-50-06,_Tibetexpedition,_Ragyapa,_Geier.jpg\"><i>here</i></a><i>.)</i></p><h2>4.&nbsp;Noticing your non-confusion</h2><p>So these are some examples where \u201cbut my gut isn\u2019t in a very visceral relationship with blah\u201d just isn\u2019t a very strong signal that blah is false. But I also want to flag some more directly AI related places where I think something gut-related has been messing up, for me.</p><h3>4.1 LLMs</h3><p>ChatGPT caused a lot of new attention to LLMs, and to AI progress in general. But depending on what you count: we had scaling laws for deep learning back in <a href=\"https://arxiv.org/abs/1712.00409\">2017</a>, or at least <a href=\"https://arxiv.org/abs/2001.08361\">2020</a>. I know people who were really paying attention; who really saw it; who really bet. And I was trying to pay attention, too. I knew more than many about what was happening. And in a sense, my explicit beliefs weren\u2019t, and should not have been, very surprised by the most recent round of LLMs. I was not a \u201cshallow patterns\u201d guy. I didn\u2019t have any specific stories about the curves bending. I expected, in the abstract, that the LLMs would improve fast.</p><p>But still: when I first played with one of the most recent round of models, my gut did a bunch of updating, in the direction of \u201coh, actually,\u201d and \u201creal deal,\u201d and \u201cfire alarm.\u201d Some part of me was still surprised.</p><p>Indeed, noticing my gut (if not my head) getting surprised at various points over the past few years, I\u2019ve realized that my gut can have some pretty silly beliefs about AI, and/or can fail to connect fairly obvious dots. For example, when I first started thinking about AI, I think some part of me failed to imagine that eventually, if AIs got smart enough, we could just <i>talk to them</i>, and that they would just <i>understand what we were saying</i>, and that interacting with them wouldn\u2019t necessarily be some hyper-precise coding thing. I had spoken to Siri. Obviously, that didn\u2019t count. Then, one day, I spoke, with my voice, to a somewhat-smarter AI, and it responded in a very human-sounding voice, and it was much more like talking on the phone, and some sort of update happened.</p><p>Similarly: I think that in the past, I failed to imagine what the visual experience of interacting with an actually-smart AI would be like. Obviously, I knew about robots; HAL\u2019s red stare; typing commands into a terminal; texting. But somehow, old talk of AGI didn\u2019t conjure this for me. I\u2019m not sure what it conjured. Something about brains in boxes, except: laptops? I think it wasn\u2019t much of anything, really. I think it was just a blank. After all, this isn\u2019t <i>sci-fi</i>. So it must not be like anything you\u2019d see in sci-fi, either, including strains aimed at realism. People, we\u2019re talking about the <i>real future</i>, which means something <i>unimaginable</i>, hence fiction to the imagination, hence nothingness. \u201cThe future that can be named is not the true future.\u201d Right?</p><p>Wrong. \u201cNamed super specifically\u201d is more plausible, but even wariness of specificity can mislead: sometimes, even the specifics are pretty obvious. I <i>had seen</i> Siri, and chat bots. What sort of fog was I artificially imposing on everything? What was so hard about imagining Siri, but smarter? Now, it feels like \u201coh, duh.\u201d And certain future experiences feel more concrete, too. It now feels like: oh, right, lots of future AIs will probably have extremely compelling and expressive <a href=\"https://replika.com/\">digital human avatars</a>. Eventually (soon?), they\u2019ll probably be able to look just like (super-hot, super-charismatic) humans on zoom calls. What did I think it would be, R2D2?</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/l9mcgzhcyf9wwymh7uoi\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/l9mcgzhcyf9wwymh7uoi 793w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/yylzzhunupc3vyvnxjno 232w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/w78vvzzuf8myseujvtbh 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/mmzjv2bmg9kjt41vd3rk 402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/kt65pmaeum0ezz16xdqp 462w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/mggfp2q7miaw9w0zres1 662w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/jc0cfrdtafnpwsvswxsn 722w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/l042cdndfbseyn0qafn8 982w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/qz1v3uq2sgxj9eknlmpo 1032w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/daqp4pxq1alwyrujeicj 1108w\"></p><p><a href=\"https://replika.com/\"><i>Some current AIs</i></a></p><p>\u201cOh, duh\u201d is never great news, epistemically. But it\u2019s interestingly <i>different</i> news than \u201c<a href=\"https://www.readthesequences.com/Noticing-Confusion-Sequence\">noticing your confusion</a>,\u201d or being straightforwardly surprised. It\u2019s more like: noticing that at some level, you were tracking this already. You had the pieces. Maybe, even, it\u2019s just like you would\u2019ve said, if you\u2019d been asked, or thought about it even a little. Maybe, even, you literally said, in the past, that it would be this way. Just: you said it with your head, and your gut was silent.</p><p>I mentioned this dynamic to Trevor Levin, and he said something about \u201cnoticing your non-confusion.\u201d I think it\u2019s a good term, and a useful skill. Of course, you can still update upon seeing stuff that you expected to see, if you weren\u2019t <i>certain</i> you\u2019d see it. But if it feels like your head is unconfused, but your gut is updating from \u201cit\u2019s probably fake somehow\u201d to \u201coh shit it\u2019s actually real,\u201d then you probably had information your gut was failing to use.</p><h3>4.2 Simulations</h3><p>I\u2019ll give another maybe-distracting example here. Last year, I spent some time thinking about <a href=\"https://jc.gatspress.com/pdf/simulation_arguments_revised.pdf\">whether we live in a computer simulation</a>. It\u2019s a strange topic, but my head takes the basic argument pretty seriously. My gut, though, generally thinks it\u2019s fake somehow, and forgets about it easily.</p><p>I remember a conversation I had with a friend sometime last year. He said something like: \u201cyou know, pretty soon, all sorts of intelligent agents on earth are going to be living in simulations.\u201d I nodded or something. It\u2019s like how: if the scientists are actually <i>putting</i> people\u2019s brains in vats, it\u2019s harder to stamp your foot and say \u201cno way.\u201d We moved on.</p><p>Then, in early April, this paper came out: \u201c<a href=\"https://arxiv.org/pdf/2304.03442.pdf\">Generative Agents: Interactive Simulacra of Human Behavior</a>.\u201d They put 25 artificial agents into an environment similar to The Sims, and had them interact, including via e.g. hosting a valentine\u2019s day party.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefylz55qu4y7\"><sup><a href=\"#fnylz55qu4y7\">[6]</a></sup></span>&nbsp;Here\u2019s the picture from the paper:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/ncdbun7dqcxoolcjsmjr\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/ncdbun7dqcxoolcjsmjr 1024w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/oxnzzdktdlpvumepjd4i 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/qmm6qm6dv2makbaozezl 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/tyzzlg9efhgzib2gi2hl 402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/vquj2djzb6af7hsiwltl 462w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/dhazewylc5as4mynskux 662w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/spz2oojjum5igesocmkx 722w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/dil3wxtttvt3batsatui 982w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/dej6fsiz9c94guhpuuif 1032w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/htd0bcbodk1cslnsayz5 1402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/mzjfwzpjjugurjhd6uuf 1420w\"><i>From </i><a href=\"https://arxiv.org/pdf/2304.03442.pdf\"><i>here</i></a><i>.</i></p><p>I opened this paper, read the beginning, looked at this picture, and felt my gut update towards being in a sim. But: c\u2019mon now, gut! What sort of probability would I have put, last year, on \u201cI will, in the future, see vaguely-smart artificial agents put into a vaguely-human simulated environment\u201d? Very high. My friend had literally said as much to me months earlier, and I did not doubt. Indeed, what\u2019s even the important difference between this paper and AlphaStar, or the original Sims?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefom83hglib5\"><sup><a href=\"#fnom83hglib5\">[7]</a></sup></span>&nbsp;How smart the models are? The fact that it\u2019s cute and human-like? My gut lost points, here.</p><p>It\u2019s an avoidable mistake. I\u2019m trying to stop making it.</p><p>I worry that we\u2019re in for a lot of dynamics like this. How seriously, for example, are you taking the possibility that future AIs will be sentient? Well, here\u2019s a mistake to not make: updating a lot once the AIs are using charismatic human avatars, or once they can argue for their sentience as convincingly as a human. Predict it now, people. Update now.</p><h3>4.3 \u201cIt\u2019s just like they said\u201d</h3><p>I don\u2019t, often, have nightmares about AI risk. But I had one a few months ago. In it, I was at a roll-out of some new AI system. It was a big event, and there were lots of people. The AI was unveiled. Somehow, it immediately wrote each one of us some kind of hyper-specific, individualized message, requiring a level of fine-grained knowledge and predictive ability that was totally out of the question for any familiar intelligence. I read my message and felt some cold and electric bolt, some recognition. I thought to myself: \u201cit\u2019s just like they said.\u201d I looked around me, and the room was in chaos. Everything was flying apart, in all directions. I don\u2019t remember what happened after that.&nbsp;</p><p>\u201cJust like they said.\u201d Who\u2019s they? Presumably, the AI worriers. The ones who think that superintelligence is not a fantasy or a discussion-on-twitter, but an actual thing we are on track to do with our computers, and which will cut through our world like butter if we get it wrong.</p><p>But wait: aren\u2019t I an AI worrier? More than many, at least. But dreams, they say, are partly the gut\u2019s domain. Perhaps the \u201cthey,\u201d here, was partly my own explicit models. Ask me in the waking world: \u201cwill superintelligence be terrifying?\u201d Yes, of course, who could doubt. But ask in my dreams instead, and I need to see it up close. I need to read the message. Only then will my gut go cold: \u201cOh, shit, it\u2019s just like they said.\u201d</p><p>I\u2019ve had this feeling a few times in the past few months. I remember, a few years ago, making a simple model of AI timelines with a colleague. We used a concept called \u201cwake-up,\u201d indicating the point where the world realized what was happening with AI and started to take it seriously. I think that if, at that point, we could\u2019ve seen what things would be like in 2023, we would\u2019ve said something like: \u201cyeah, that\u201d (though: there\u2019s a ton more waking up to do, so future wake-ups might end up better candidates).</p><p>Similarly, \u201cthey\u201d have worried for ages about triggering or exacerbating \u201crace dynamics\u201d in AI. Then, in recent months, Google went into a \u201c<a href=\"https://www.nytimes.com/2022/12/21/technology/ai-chatgpt-google-search.html\">Code Red</a>\u201d about AI, and the CEO of Microsoft came out and just said straight up: \u201c<a href=\"https://www.businesstoday.in/technology/news/story/the-race-starts-today-microsoft-officially-brings-chatgpt-ai-to-bing-and-edge-browser-369453-2023-02-08\">the race starts today</a>.\u201d</p><p>\u201cThey\u201d have worried about AIs being crazy alien minds that we don\u2019t understand. Then, in February, we got to see, briefly, the rampaging strangeness of a good Bing \u2013 including all sorts of <a href=\"https://time.com/6256529/bing-openai-chatgpt-danger-alignment/\">deception and manipulation and blackmail</a>, which I <a href=\"https://www.cold-takes.com/what-does-bing-chat-tell-us-about-ai-risk/\">don\u2019t actually think is the centrally worrying kind</a>, but which doesn\u2019t exactly seem like good news, either.</p><p>\u201cThey\u201d have worried about agents, and about AIs running wild on the internet, and about humans not exactly helping with that. Now we have <a href=\"https://en.wikipedia.org/wiki/Auto-GPT\">Auto-GPT</a>, and <a href=\"https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity\">Chaos-GPT</a>, and I open up my browser and I see stuff like <a href=\"https://agentgpt.reworkd.ai/\">this</a>:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/uso17ynqrcapwqgdnpjd\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/uso17ynqrcapwqgdnpjd 1024w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/tfqsixpnenk6ovb6cekh 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/x5zpi6i5qc3ofjhsvnvw 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/urokfg0su1jgu47lkpy1 1536w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/bj5nhv7to6eydobrsp6z 2048w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/skwhes73ut4g9eki7bly 402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/ghmd6vcksnrcfjd0qs82 462w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/t2lnim58cek7goq4ofyl 662w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/uxkujcsmaohnm9kyrjsp 722w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/psqpdjtcgnovewhrfpya 982w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/t8vhvj9c8xj6gpnskd9n 1032w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/pw4opvp3o523dhmxsoko 1402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/d81nezn52o7wzif1bqtb 1702w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/ytptmy6hmesbpdqmgqqj 2002w\"><i>Not the pixels I wanted to be seeing at this point in my life.</i></p><p>Now, I don\u2019t want to litigate, here, exactly who \u201ccalled\u201d what (or: created what<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1gnwotoahij\"><sup><a href=\"#fn1gnwotoahij\">[8]</a></sup></span>), and how hard, and how much of an update all this stuff should be. And I think some things \u2013 for example, the world\u2019s sympathy towards concern about risks from AI \u2013 have surprised some doomers, however marginally, in the direction of optimism. But as someone who has been thinking a lot about AI risk for more than five years, the past six months or so have felt like a lot of movement from abstract to concrete, from \u201cthat\u2019s what the model says\u201d to \u201coh shit here we are.\u201d And my gut has gotten more worried.</p><p>Can this sort of increased worry be Bayesian? Maybe. I suspect, though, that I\u2019ve just been messing up. Let\u2019s look at the dynamics in more detail.</p><h2>5. Smelling the mustard gas</h2><blockquote><p><i>\"Men marched asleep\u2026</i></p><p><i>All went lame, all blind.\"</i></p><p><i>- </i><a href=\"https://en.wikipedia.org/wiki/Dulce_et_Decorum_est\"><i>Wilfred Owen</i></a></p></blockquote><p>It\u2019s sometimes thought that, as a Bayesian, you shouldn\u2019t be able to predict which direction you\u2019ll update in the future.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0xfc6qza6u8\"><sup><a href=\"#fn0xfc6qza6u8\">[9]</a></sup></span>&nbsp;That is, if you\u2019re about to get some new evidence about <i>p</i>, you shouldn\u2019t be able to predict whether this evidence will move your credence on <i>p</i> higher or lower. Otherwise, the thought goes, you could \u201cprice in\u201d that evidence now, by moving your credence in the predicted direction.</p><p>But this is wrong.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefp6fohdm226\"><sup><a href=\"#fnp6fohdm226\">[10]</a></sup></span>&nbsp;Consider a simple example. Suppose you\u2019re at 99% that Trump won the election. You\u2019re about to open the newspaper that will tell you for sure. Here, you should be at 99% that you\u2019re about to increase your credence on Trump winning: specifically, up to 100%. It\u2019s a very predictable update.</p><p>So why can\u2019t you price it in? Because there\u2019s a 1% chance that you\u2019re about to lower your confidence in Trump winning <i>by a lot more</i>: specifically, down to 0%. That is, in <i>expectation</i>, your confidence in Trump winning will remain the same.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5mhnx3fqqo\"><sup><a href=\"#fn5mhnx3fqqo\">[11]</a></sup></span>&nbsp;And it\u2019s the expectation of your future update that Bayesian binds.</p><p>To understand this more visually, let\u2019s use a slightly more complicated example. Suppose you\u2019re currently at 80% that GPT-6 is going to be \u201cscary smart,\u201d whatever that means to you. And suppose that, conditional on GPT-6 being scary smart, your probability on AI doom is 50%; and conditional on GPT-6 not being scary smart, your probability on AI doom is 10%. So your credence looks like this:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/g9duug04tyjvz4tjaxbm\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/fazyrpbldq41zvypzfqd 942w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/mvn39yfvs6dv4n42eazv 276w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/cd7o1x05f3dxaciotdm9 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/vst9ht6bgq86ettosglo 402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/hnihkaa9tmsgnkpmtssh 462w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/el2h32kluxd1rsmbzcnc 662w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/h5jelyvfq6bsovtolccr 722w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/aqpb8ytcshzisrk8nxtc 982w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/efr4etalcfca0op0lmuu 1032w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/bdr81z762cr8uxatwusi 1082w\"></p><p>Now, what\u2019s your overall p(doom)? Well, it\u2019s:</p><blockquote><p>(probability that GPT-6 is scary smart * probability of doom conditional on GPT-6 being scary smart) + (probability that GPT-6 isn\u2019t scary smart * probability of doom conditional on GPT-6 not being scary smart)</p></blockquote><p>That is, in this case, 42%.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbe5s9q5go0r\"><sup><a href=\"#fnbe5s9q5go0r\">[12]</a></sup></span></p><p>But now we can see a possible format for a gut-problem mistake. In particular: suppose that I ask you, right now, surrounded by flags and crisp uniforms, about the probability of doom. You query your gut, and it smells no mustard gas. So you give an answer that doesn\u2019t smell much mustard gas, either. Let\u2019s say, 10%. And let\u2019s say you don\u2019t really break things down into: OK, how much mustard gas do I smell conditional on GPT-6 being scary smart, vs. not, and what are my probabilities on that.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref416kaiwzc91\"><sup><a href=\"#fn416kaiwzc91\">[13]</a></sup></span>&nbsp;Rather, your model is an undifferentiated mass:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/otyvevfm1uuedwxv3wvw\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/otyvevfm1uuedwxv3wvw 970w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/kurwo4ga91eu8x6pquvj 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/d2nr8xol7qvmx8ec8aih 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/sng58drspbemylztegbs 402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/cvzpqixixbhjrztnd50j 462w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/zrwsv3q0r0cojlaaeadh 662w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/tq48cgv1ffpch0dqagyn 722w\"></p><p>Or maybe you do try to break things down like that, but the waft of the gas fades with all the distance. GPT-6 is far away, behind some fog. Still: you guess, with your head, and without your gut participating, that p(doom) is indeed a bit higher conditional on GPT-6 being scary smart, what with the update towards \u201cshort timelines.\u201d Let\u2019s say, 20%; and 10% otherwise. So maybe your overall p(doom), given 80% on the abstract idea of GPT-6 being scary smart, is 18%.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/tkdjnspwsvtqxplkl30n\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/tkdjnspwsvtqxplkl30n 957w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/op9wcxlq4frwdm333r7y 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/cx2js2tx8ysindftf4ed 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/obwyblxhcrb6tqpewixd 402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/eomlyncil82rdfa0tb4v 462w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/gstead2i0lauwz2zsek0 662w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/d1nmm96odq2fdxyzqqiq 722w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/didgqwbwceheq7q8frx1 982w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/k605zetrzhjx1yq3rei7 1032w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/dtxzikc9xvvoeenlkkqf 1208w\"></p><p>But actually, let\u2019s say, if you could see a \u201cscary smart\u201d GPT-6 model right now, you would freak out way harder. You would be able to smell the gas up close, that bitter tang. Your gut would get some message, and come alive, and start participating in the exercise. \u201c<i>That thing</i>,\u201d your gut might say, \u201cis <i>scary</i>. I\u2019m at 50% on doom, now.\u201d</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/vmipeqk7wqfnngaytg03\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/vmipeqk7wqfnngaytg03 916w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/bvnxaunmtp2ertcyx2p6 268w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/xwujies1yvpyejg96fl4 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/hgjpp1hqfqd6v2cnmgzq 1375w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/pmuq7vsu4az9opsgbaxa 402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/j5iyszapxfwik4z5uvv9 462w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/xizwg7iq2z8zsh9tmpnt 662w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/z1ymixcwvnhnwwwelv6x 722w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/ezj0wh4isx0y1gdj4ezf 982w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/up04dca7mr4pio8hycwc 1032w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/ecewe5nx0haqadonwyg7 1402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/gyxvntu1vvokcy6v2fwo 1414w\"></p><p>Thus, you end up inconsistent, and dutch-bookable (at least in principle \u2013 setting aside issues re: betting on doom). Suppose I ask you, now, to agree to sell me a \u201cpays out $100 conditional on doom\u201d ticket for $30 (let\u2019s assume this can actually pay out), conditional on GPT-6 being scary smart. You\u2019re only at 20% doom in such a world, so you predict that such a ticket will only be worth $20 to you if this deal is ever triggered, so you agree. But actually, when we get to that world, your gut freaks out, and you end up at 50% doom, and that ticket is now worth $50 to you, but you\u2019re selling it for $30. Plus, maybe now you\u2019re regretting other things. Like some of those tweets. And how much alignment work you did.</p><p>As indicated above, I think I\u2019ve made mistakes in this vein. In particular: a few years back, I wrote a <a href=\"https://arxiv.org/pdf/2206.13353.pdf\">report about AI risk</a>, where I put the probability of doom by 2070 at 5%. Fairly quickly after releasing the report, though, I realized that this number was too low.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefr4lzpfmu09s\"><sup><a href=\"#fnr4lzpfmu09s\">[14]</a></sup></span>&nbsp;Specifically, I also had put 65% on relevantly advanced and agentic AI systems being developed by 2070. So my 5% was implying that, <i>conditional </i>on such systems being developed, I was going to look them in the eye and say (in expectation): \u201c~92% that we\u2019re gonna be OK, x-risk-wise.\u201d But on reflection, that wasn\u2019t, actually, how I expected to feel, staring down the barrel of a machine that outstrips human intelligence in science, strategy, persuasion, power; still less, <a href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\">billions of such machines</a>; still less, full-blown superintelligence. Rather, I expected to be very scared. More than 8% scared.</p><h3><strong>5.1 Should you trust your future gut, though?</strong></h3><p>Now, you might wonder: why give credit to such future fear?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefe6nqxiwigdk\"><sup><a href=\"#fne6nqxiwigdk\">[15]</a></sup></span>&nbsp;After all, isn\u2019t part of the worry about doomers that they\u2019re, you know, fraidy-cats? Paranoids? (C\u2019mon: it\u2019s just a superintelligent machine, the invention of a second advanced species, the introduction of a qualitatively new order of optimization power into earth\u2019s ecosystem. It\u2019s just, you know, <i>change</i>.) And isn\u2019t the gut, famously, a bit skittish? Indeed, if you\u2019re worried about your gut being <i>underactive</i>, at a distance, shouldn\u2019t you also be worried about it being <i>over-active, </i>up close? Shouldn\u2019t you reason, instead, ahead of time, at a distance, and in a cool hour, about how scared you should be when you\u2019re there-in-person?</p><p>Well, it\u2019s a judgment call. Sometimes, indeed, at-a-distance is a better epistemic vantage point than up-close. Especially if you know yourself to have biases. Maybe, for example, you\u2019ve got a flying phobia, and you know that once you\u2019re on the plane, your gut\u2019s estimates of the probability of the plane crashing are going to go up a lot. Should you update now, then? Indeed: no.</p><p>But, personally, with respect to the future, I tend to trust my future self more. It\u2019s a dicey game already, futurism, and future Joe has a lot more data. The future is a foreign country, but he\u2019s been there.</p><p>And I tend to trust my up-close self more, in general, for stuff that requires realization rather than belief (and I think words like \u201csuperintelligence\u201d require lots of realization). Maybe the journalist has the accurate casualty count; but I trust the soldier on the ground to know what a casualty <i>means</i>. And I trust the man with the scans about death.</p><p>Now, importantly, there\u2019s also a thing where guts sometimesreact surprisingly <i>little</i>, up close, to AI stuff you predicted ahead of time you\u2019d be scared about. Part of this is the \u201cit\u2019s not real AI if you can actually do it,\u201d thing (though, my sense is that this vibe is fading?). Part of it is that sometimes, machines doing blah (e.g., beating humans at chess) is less evidence about stuff than you thought. And I wonder if part of it is that sometimes, your at-a-distance fear of that futuristic AI stuff was imagining some world less mundane and \u201cnormal\u201d than the world you actually find yourself in, when the relevant stuff comes around \u2014 such that when you, sitting in your same-old apartment, wearing your same-old socks, finally see AIs planning, or understanding language, or passing <a href=\"https://youtu.be/qbIk7-JPB2c?t=1991\">two-hour human coding interviews in four minutes</a>, or <a href=\"https://www.metaculus.com/questions/6728/ai-wins-imo-gold-medal/\">winning the IMO</a>, it feels/will feel like \u201cwell that can\u2019t be the scary thing I had in mind, because that thing is happening in the real world actually and I still have back pain.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefamcm4d3vca5\"><sup><a href=\"#fnamcm4d3vca5\">[16]</a></sup></span>&nbsp;At the least, we get used to stuff fast. &nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/rqrihnvf4azqfggwedjs\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/rqrihnvf4azqfggwedjs 1024w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/ynbnyvl6cx0ctf5vk4zb 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/e8u386hz46qzpamrcyzv 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/owth06oxww888dlntoqr 1536w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/p2oakwatjqbu17wkidxo 402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/skkih2oy6cp0hr9grni7 462w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/nlfsnhdesz2yllrvr1jw 662w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/y0wy17yvdxcnb74j6xxd 722w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/md41cbeie5czwr667bdj 982w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/wvfwcuzryxdbatz8ywla 1032w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/aou0psmrlgswipa0hvhi 1402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/st6pq2o8gver1lbhppol 1702w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/b2bbpucqw8oqx9m6jqjh 1998w\"></p><p><i>GPT-4 doing a coding interview. From </i><a href=\"https://www.youtube.com/watch?v=qbIk7-JPB2c&amp;t=1991s\"><i>here</i></a><i>.</i></p><p>Still: sometimes, also, you were too scared before, and your gut can see that now. And there, too, I tend to think your earlier self should defer: it\u2019s not that, if your future self is more scared, you should be more scared now, but if your future self is less scared, you should think that your future self is biased. <a href=\"https://www.lesswrong.com/posts/G5TwJ9BGxcgh5DsmQ/yes-requires-the-possibility-of-no\">Yes requires the possibility of no</a>. If my future self looks the future AGI in the eye and feels like \u201coh, actually, this isn\u2019t so scary after all,\u201d that\u2019s evidence that my present self is missing something, too. Here\u2019s hoping.</p><h3><strong>5.2 An aside on mental health</strong></h3><p>Now: a quick caution. Here I\u2019ve been treating guts centrally from an epistemic perspective. But we need a wise <i>practical</i> relationship with our guts as well. And from a practical perspective, I don\u2019t think it\u2019s always productive to try to smell mustard gas harder, or to make horrible things like AI doom vivid. The right dance here is going to vary person-to-person, and I won\u2019t try to treat the topic now (though: see <a href=\"https://www.lesswrong.com/posts/pLLeGA7aGaJpgCkof/mental-health-and-the-alignment-problem-a-compilation-of\">here</a> for a list of resources). But I wanted to flag explicitly that staying motivated and non-depressed and so forth, in relation to a pretty scary situation, is a separate art, and one that needs to be woven carefully with the more centrally epistemic angle I\u2019m focused on here. &nbsp;</p><h2>6.&nbsp;Constraints on future worrying</h2><p>Returning to the epistemic perspective though: let\u2019s suppose you do trust your future credences, and you want to avoid the Bayesian \u201cgut problems\u201d I discussed above. In that case, at least in theory, there are hard constraints on how you should expect your beliefs to change over time, even as you move from far away to up close.</p><p>In particular, you should never think that there\u2019s more than a 1/<i>x</i> chance that your credence will increase by <i>x</i> times: i.e., never more than a 50% chance that it\u2019ll double, never more than a 10% chance that it\u2019ll 10x. And if your credence is very small, then even very small additive increases can easily amount to sufficiently substantive multiplicative increases that these constraints bite. If you move from .01% to .1%, you\u2019ve only gone up .09% in additive terms \u2013 only nine parts in ten thousand. But you\u2019ve also gone up by a factor of 10 \u2013 something you should\u2019ve been at least 90% sure would never happen.</p><p>So suppose that right now, you identify as an \u201cAI risk skeptic,\u201d and you put the probability of doom very low. For concreteness, suppose that you like <a href=\"https://ineffectivealtruismblog.com/2023/04/08/exaggerating-risks-carlsmith-report/\">David Thorstad\u2019s number</a>: .00002% \u2014 that is, one in five million (though: he now thinks this \u201ctoo generous\u201d \u2013 and he\u2019s also \u201cnot convinced that we are in a position where estimating AI risk makes good methodological sense,\u201d which I suspect is a bigger crux). This is a very low number. And it implies, in particular, that you really don\u2019t expect to get even a <i>small amount</i> more worried later. For example, you need to have a maximum of .01% that you ever see evidence that puts the probability at &gt;.2%.</p><p>Now suppose that a few years pass, GPT-6 comes out, and lo, indeed, it is very impressive. You look GPT-6 in the eye and you feel some twinge in your gut. You start to feel a bit, well, at-least-1-percent-y. A bit not-so-crazy-after-all. Now, admittedly, you were probably surprised that GPT-6 is so good. You were a \u201ctimelines skeptic,\u201d too. But: how much of a skeptic? Were you, for example, less than one in fifty thousand that GPT-6 would be this impressive? That\u2019s what your previous number can easily imply, if the impressiveness is what\u2019s driving your update.</p><p>And now suppose that actually, you weren\u2019t much of a timelines skeptic at all. GPT-6, according to you, is right on trend. You\u2019d seen the scaling laws. You were at &gt;50% on at-least-this-impressive. It was predictable. It\u2019s just that the rest of the argument for doom is dumb.</p><p>In that case, though, hmm. Your gut\u2019s got heavy constraints, in terms of twinging. &gt;50% on at least-this-impressive? So: you\u2019re still supposed to be at less than .00004% on doom? But what if you\u2019re not\u2026</p><p>Or maybe you think: \u201cthe argument for doom has not been satisfactorily peer-reviewed. <a href=\"https://marginalrevolution.com/marginalrevolution/2023/04/this-gpt-4-answer-speaks-for-itself.html\">Where\u2019s the paper in <i>Nature</i></a>? Until I see conventional academic signals, I am at less than one in a thousand on doom, and I shall tweet accordingly.\u201d OK: but, the Bayesianism. If you\u2019re at less than one in a thousand, now, and your big thing is academic credibility, where should Bayes put you later, conditional on <i>seeing</i> conventional academic signals? And what\u2019s your probability on such strange sights? In five years, or ten years, are you confident there won\u2019t be a paper in <i>Nature</i>, or an equivalent? If it\u2019s even 10% percent likely, and it would take you to more than 1%, your number now should be moving ahead of time.</p><p>Or maybe you thought, in the past: \u201cuntil I see the experts worrying, I\u2019m at less than 1%.\u201d Well, <a href=\"https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\">here we are</a> (here we already were, but more now). But: what was your probability that we ended up here? Was it so hard to imagine, the current level of expert sympathy? And are future levels of greater sympathy so hard to imagine, now? It\u2019s easy to live, only, in the present \u2013 to move only as far as the present has moved. But the Bayesian has to live, ahead of time, in all the futures at once.</p><p>(Note that all of these comments apply, symmetrically, to people nearly certain of doom. 99.99%? OK, so less than 1% than you ever drop to 99% or lower? So little hope of future hope?)</p><p>Now: all of this is \u201cin theory.\u201d In practice, this sort of reasoning requires good taste. I talk about such taste more below. First, though, I want to look at the theory a bit more.</p><h2>7.&nbsp;Should you expect low probabilities to go down?</h2><p>Above I said that actually, the <i>direction</i> of a future update is often predictable. But notice: <i>which direction</i> should you predict? My sense is that in many evidential situations (though not all \u2013 more below), you should think your future evidence more likely to move you in the right direction than the wrong one. So if you think that <i>p</i> is likely to be true, you should generally think that your future evidence is likely to update you towards higher credence on <i>p</i>. And vice versa: if you think that p is more likely to be <i>false</i>, you should expect to have <i>lower</i> credence on it later.</p><p>The Trump example above is an extreme case. You\u2019re at 99% on Trump winning, and you\u2019re also at 99% that you\u2019ll update, in future, towards higher credence on Trump winning. And we can imagine a more intermediate case, where, let\u2019s say, you\u2019re at 90% that Trump is going to win, and you\u2019re about to watch the presidential debate, and you think that winning the debate is highly correlated with winning the election. Which direction should you predict that your credence on Trump winning will move, once the debate is over? Given that you think Trump is likely to win the election, I think you should think he\u2019s likely to win the debate, too. And if he wins the debate, your credence on him winning the election will go up (whereas if he loses, it\u2019ll go down a bunch more).</p><p>Or consider a scientist who doesn\u2019t believe in God. In principle, at each moment, God could appear before her in a tower of flames. She has some (very small) credence on this happening. And if it happened, she would update massively towards theism. But <a href=\"https://www.lesswrong.com/posts/mnS2WYLCGJP2kQkRn/absence-of-evidence-is-evidence-of-absence\">absence of evidence is evidence of absence</a>. Every moment she <i>doesn\u2019t</i> observe God appearing before her in a tower of flames, she should be updating some tiny amount towards atheism. And because she predicts very hard that God will never appear before her in a tower of flames, she should be predicting very hard that she will become a more and more confident atheist over time, and that she\u2019ll die with even less faith than she has now.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/h5xcfbk7rtds30rtsp9g\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/niornj7bkkn1ihwke6y0 795w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/pytoqb3epihrsymkygtn 233w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/mwhclnpcepgthm8zyqgp 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/vrmcbpr2gow6kumemftt 402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/vomde0kvcbj67qhmmvfh 462w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/g3ubi55wdyiltc9bralk 662w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/vi3jsdj6noaxfb1y2lpb 722w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/rbuxzqufbwc8qebuojum 982w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/fihdkwlryryxzj1bbfhm 1032w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/geys28njhgvqzvzwsjvw 1148w\"></p><p><i>Updating so hard right now\u2026 (Image source </i><a href=\"https://commons.wikimedia.org/wiki/File:Bourdon,_S%C3%A9bastien_-_Burning_bush.jpg\"><i>here</i></a><i>.)</i></p><p>So too, one might think, with AI risk. If you are currently an AI risk skeptic, plausibly you should expect to become more and more confidently skeptical over time, as your remaining uncertainties about the case for non-doom get resolved in the direction of truth. That is, every moment that the superintelligent machines <i>don\u2019t</i> appear before you in a tower of diamondoid bacteria (that\u2019s the story, right?), then anthropic effects aside, you should be breathing easier and easier. Or, more realistically, you should be expecting to see, well, whatever it is that comforts you: i.e., that we\u2019ll hit another AI winter; or that we\u2019ll make lots of progress in mechanistic interpretability; or that innovations in RLHF will allow superhuman oversight of AI behavior humans can\u2019t understand; or that we won\u2019t see any signs of deception or reward hacking; or that progress will be slow and gradual and nicely coordinated; or that we\u2019ll finally, <i>finally</i>, get some peer review, and put the must-be-confusions to rest. And as your predictions are confirmed, you should be feeling safer and safer.</p><p>Is that what you expect, in your heart? Or are you, perhaps, secretly expecting to get more worried over time? I wished I\u2019d asked myself harder. In particular: my 5% was plausibly implying some vibe like: \u201csure, there are these arguments that superintelligent AI will disempower us, and I give them some weight, but at least if we\u2019re able to think well about the issue and notice the clues that reality is giving us, over time it will probably become clearer that these arguments are wrong/confused, and we\u2019ll be able to worry much less.\u201d Indeed, depending on the volatility of the evidence I was expecting, perhaps I should have thought that I was likely to be in the ballpark of the highest levels of worry about doom that I would ever endorse. But if you\u2019d asked me, would I have said that?</p><p>That said, I actually think these dynamics are more complicated than they might initially seem. In particular, while I find it plausible that you should generally predict that you\u2019ll update in the direction of what you currently expect to be true, sometimes, actually, you shouldn\u2019t. And some non-crazy views on AI risk fit the mold.</p><p>Katja Grace suggested to me some useful examples. Suppose that you\u2019re in a boat heading down a river. You at 80% that there\u2019s a waterfall about two miles down, but 20% that there isn\u2019t, and that you\u2019re going to see a sign, a mile down, saying as much (\u201cNo waterfall\u201d \u2013 classic sort of sign). Conditional on no sign/there being a waterfall, you\u2019re at 10% that it\u2019s a big waterfall, which will kill you, and 90% that it\u2019s a small waterfall, which you\u2019ll survive. So currently, your credence on dying is 8%. However, you\u2019re also at 80% that in a mile, it\u2019s going to go up, to 10%, despite your also predicting, now, that this is an update towards higher credence on something that probably won\u2019t happen.</p><p>Or a consider a more real-world example (also from Katja). At 3 pm, you\u2019re planning to take a long car trip. But there\u2019s a 10% chance the trip will fall through. If you take the trip, there\u2019s some small chance you get in an accident. As you approach 3 pm, your credence in \u201cI will get in a car accident today\u201d should go up, as the trip keeps (predictably) not-falling-through. And then, as you\u2019re driving, it should go down gradually, as the remaining time in the car (and therefore, in danger) shrinks.</p><p>Some views on AI \u2013 including, skeptical-of-doom views \u2013 look like this. Suppose, for example, you think AGI-by-2070 more likely than not. And suppose that conditional on AGI-by-2070, you think there\u2019s some small risk that the doomers are right, and we all die. And you think it\u2019s going to be hard to get good evidence to rule this out ahead of time. Probably, though, we\u2019ll make it through OK. And conditional on no-AGI-by-2070, you think we\u2019re almost certainly fine. Here, you should plausibly expect to get more worried over time, as you get evidence confirming that yes, indeed, AGI-by-2070; yes, indeed, waterfall ahead. And then to get less worried later, as the waterfall proves small.</p><p>That said, this sort of dynamic requires specific constraints on what evidence is available, when. The truth about the future must fail to leak backwards into the past. You must be unable to hear the difference between a big waterfall and a small waterfall sufficiently ahead-of-time. The gas ahead must not waft.</p><p>Car accidents are indeed like this. People rarely spend much time with high credence that they\u2019re about to get in a car accident. Their probability is low; and then suddenly it jumps wildly, split-second high, before death, or some bang-crunch-jerk, or a gasping near-miss.</p><p>Is AI risk like this too? Doomers sometimes talk this way. You\u2019ll be cruising along. Everything will be looking rosy. The non-doomers will be feeling smug. Then suddenly: bam! The nanobots, from the bloodstream, in the parlor, Professor Plum. The clues, that is, didn\u2019t rest on the details. A lot of it was obvious a priori. You should\u2019ve read more LessWrong back in the 2000s. You should\u2019ve looked harder at those <a href=\"https://twitter.com/ESYudkowsky/status/1500863629490544645\">empty strings</a>.</p><p>Now, sometimes this sort of vibe seems to me like it wants to have things both ways. \u201cI shall accept ahead-of-time empirical evidence that I am right; but in the absence of such evidence, I shall remain just as confident.\u201d \u201cMy model makes no confident predictions prior to the all-dropping-dead thing \u2013 except, that is, the ones that I want to claim credit for after-the-fact.\u201d Here I recall a conversation I overheard back in 2018 about \u201c<a href=\"https://arbital.com/p/daemons/\">optimization daemons</a>\u201d (now: <a href=\"https://arxiv.org/abs/1906.01820\">mesa-optimizers</a>, <a href=\"https://arxiv.org/abs/2210.01790\">goal mis-generalization</a>, etc) in which a worrier said something like: \u201cI will accept empirical arguments for concern, but only a priori arguments for comfort.\u201d It was an offhand remark, but still: <a href=\"https://www.lesswrong.com/posts/mnS2WYLCGJP2kQkRn/absence-of-evidence-is-evidence-of-absence\">not how it works</a>.</p><p>However: I do think, unfortunately, there are risks of gas that doesn\u2019t waft well; \u201c<a href=\"https://forum.effectivealtruism.org/posts/NbiHKTN5QhFFfjjm5/ai-safety-seems-hard-to-measure#_2__The_King_Lear_problem__how_do_you_test_what_will_happen_when_it_s_no_longer_a_test_\">King Lear problems</a>\u201d; risks of <a href=\"https://www.planned-obsolescence.org/the-training-game/\">things looking fairly fine, right before they are very non-fine indeed</a>. But not all the gas is like this. We should expect to get clues (indeed, we should <a href=\"https://www.lesswrong.com/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very#Testing_and_threat_assessment\"><i>dig hard for them</i></a>)<i>.</i> So we should expect, at some point, to start updating in the right direction. But I think it\u2019s an open question how the sequencing here works, and it\u2019ll depend on the details driving your particular view. In general, though, if you\u2019re currently at more-likely-than-not on hitting an AGI waterfall sometime in the coming decades, but not certain, then prima facie, and even if your p(doom) is low, that\u2019s reason to expect to get more worried as that soothing sign \u2013 \u201cAI winter,\u201d \u201cIt was all fake somehow\u201d (classic sign) \u2013 fails to appear.</p><p>That said, even if you\u2019re getting predictably <i>more </i>worried, there are still Bayesian constraints on <i>how much</i>. In the waterfall case, you go up 2%; in the car case, something tiny. So if you\u2019re finding yourself, once you don\u2019t see the sign, jumping to 50% on \u201cdeath by big waterfall\u201d \u2013 well, hmm, according to your previous views, you\u2019re saying that you\u2019re in a much-more-worrying-than-average not-seeing-the-sign scenario. Whence such above-average-worrying? Is the evidence you\u2019re seeing now, re: big-waterfall, actually surprising relative to what you expected before? Looks a lot like the predicted river to me. Looks, indeed, \u201cjust like they said.\u201d Or did your gut, maybe, not really believe \u2026</p><h2><strong>8. Will the next president be a potato?</strong></h2><p>OK, that was a bunch of stuff about basic Bayesian belief dynamics. And armed with this sort of relatively crisp and simple model, it can be easy to start drawing strong conclusions about how you, with your mushy monkey brain, should be reasoning in the practice, and what sorts of numbers should be coming out of your mouth, when you make number-noises.</p><p>But the number-noise game takes taste. It\u2019s a new game. We\u2019re still learning how to play well, and productively. And I think we should be wary of possible distortions, especially with respect to small-probabilities.</p><p>Consider, for example, the following dialogue:</p><blockquote><p><i>Them</i>: What\u2019s your probability that the next president is a potato?</p><p><i>You</i>: What?</p><p><i>Them</i>: A potato. Like, a normal potato. Up there getting inaugurated and stuff.</p><p><i>You</i>: Umm, very low?</p><p><i>Them</i>: Say a number!</p><p><i>You</i>: [blank stare]</p><p><i>Them</i>: You are a Bayesian and must have a number, and I demand that you produce it. Just literally say any number and I will be satisfied.</p><p><i>You</i>: Fine. One in 10^50. &nbsp;</p><p><i>Them</i>: What? Really? Wow that\u2019s so stupid. I can\u2019t believe you said that.</p><p><i>You</i>: Actually, let\u2019s say one in 10^40.</p><p><i>Them</i>: Wait, your number was more than a billion times lower a second ago. If you were at one in 10^50 a second ago, you should\u2019ve been at less than one-in-a-billion that you\u2019d ever move this high. Is the evidence you\u2019ve got since then so surprising? Clearly, you are a bad Bayesian. And I am clever!</p><p><i>You</i>: This is a dumb thing.</p></blockquote><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/xj9l05be3u2poi91ncqy\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/xj9l05be3u2poi91ncqy 1019w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/j4xllqmvihqssxboztvb 298w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/zkh7g9mmadsyouuyomuw 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/mekjg3bmsi7qs9zndrij 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/alotlonh4k139a8u9rfg 402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/oahifvtqs0xzacufno8c 462w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/a2vjvwgmuxq0wrwleuhr 662w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/iyrbpyyotu5ewrak4shs 722w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/dooaiesw2fwnxtvh8qkq 982w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/husricx3yjiu23nxdsq0 1032w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/fpkbo7xe8t9l1uj4eyfu 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/u8tsx84b7a7k3gsge1p4 1156w\"></p><p><i>Not like this: a normal potato.</i></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/ttvthqnz1qclwg6y71n1\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/w4huviudeoox6shwzfvn 1024w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/ayhmj2tayq2gr11vc4kh 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/ze2cjvzh01jwxdhvxkrr 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/daruonujbteg9oy2tq0c 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/cjofagoyq3k3jwy4b48d 402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/s7iulxwhzuhxzlgj4bzp 462w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/enb2dka73wwilhaupx0w 662w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/betnuf1kblyp5lqeswuf 722w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/d5fi6uazqlt7tjcc9flu 982w\"></p><p><i>Closer\u2026</i></p><p>The \u201cthem\u201d vibe, here, seems dubiously helpful. And in particular, in this case, it\u2019s extra not-helpful to think of \u201cyou\u201d as changing your probabilities, from one second to the next, by updating some fully-formed probability distribution over Potato-2024, complete with expected updates based on all the possible next-thoughts-you-could-think, reactions \u201cthem\u201d might have, and so on. That\u2019s, just, not the right way to understand what\u2019s going on with the fleshy creatures described in this dialogue. And in general, it can be hard to have intuitions about <a href=\"https://markxu.com/strong-evidence\">strong evidence</a>, and extreme numbers make human-implemented Bayesian especially brittle. &nbsp;</p><p>Now, to be clear: I think that debates about the rough quantitative probability of AI doom are worth engaging in, and that they are in fact (unfortunately) very different from debates about Potato-2024. Still, though, that old lesson looms: do not confuse your abstract model of yourself with yourself. The map is never the territory; but especially not when you\u2019re imagining a map that would take a <a href=\"https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#i-the-universal-distribution\">hyper-computer to compute</a>. Fans of basic Bayesianism, and of number-noises, are <a href=\"https://www.lesswrong.com/posts/CPP2uLcaywEokFKQG/toolbox-thinking-and-law-thinking\">well-aware of this</a>; but the right dance, in practice, remains an open question.</p><p>As an example of a distortion I worry about with respect to the previous discussion: in practice, lots of people (myself included \u2013 but see also Christiano <a href=\"https://ai-alignment.com/my-views-on-doom-4788b1cd0c72\">here</a>) report volatility in their degree of concern about p(doom). Some days, I feel like \u201cman, I just can\u2019t see how this goes well.\u201d Other days I\u2019m like: \u201cWhat was the argument again? All the AIs-that-matter will have long-term goals that benefit from lots of patient power-grabbing and then coordinate to deceive us and then rise up all at once in a coup? Sounds, um, pretty specific\u2026\u201d</p><p>Now, you could argue that either your expectations about this volatility should be compatible with the basic Bayesianism above (such that, e.g., if you think it reasonably like that you\u2019ll have lots of &gt;50% days in future, you should be pretty wary of saying 1% now), or you\u2019re probably messing up. And maybe so. But I wonder about alternative models, too. For example, Katja Grace suggested to me a model where you\u2019re only able to hold some subset of the evidence in your mind at once, to produce your number-noise, and different considerations are salient at different times. And if we use this model, I wonder if how we think about volatility should change.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrq5fnwph34\"><sup><a href=\"#fnrq5fnwph34\">[17]</a></sup></span></p><p>Indeed, even on basic Bayesianism, volatility is fine as long as the averages work out (e.g., you can be at an <i>average</i> of 10% doom conditional on GPT-6 being \u201cscary smart,\u201d but 5% of the time you jump to 99% upon observing a scary smart GPT-6, 5% of the time you drop to near zero, and in other cases you end up at lots of other numbers, too). And it can be hard to track all the evidence you\u2019ve been getting. Maybe you notice that two years from now, your p(doom) has gone up a lot, despite AI capabilities seeming on-trend, and you worry that you\u2019re a bad Bayesian, but actually there has been some other build-up of evidence for doom that you\u2019re not tracking \u2013 for example, the rest of the world starting to agree.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefi2d4549lu1m\"><sup><a href=\"#fni2d4549lu1m\">[18]</a></sup></span></p><p>And there are other more familiar risks of just getting even the basic Bayesianism wrong. Maybe, for example, you notice that your beliefs have been trending in a certain direction. Trump keeps moving up in the polls, say. Now you\u2019re at like 95% on Trump win. And you read a tweet like <a href=\"https://twitter.com/NPCollapse/status/1626854680260231169\">Connor Leahy\u2019s</a>, below, telling you to \u201cjust update all the way, bro\u201d and so you decide, shit, I\u2019ll just go 100%, and assume that Trump <i>will</i> win. Wouldn\u2019t want to predictably update later, right?</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/avcmtkou4143l7e9i1tr\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/avcmtkou4143l7e9i1tr 817w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/hk1ahk8u6ydnupqph5kg 239w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/v7nazfmgingvfdjhvmpo 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/fjr0tap6hbeejaendrhl 1226w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/bl8ayfxfqex3nejoieyy 402w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/gnhm9hvycghvhvwpo7t2 462w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/vcyhcc9kq6aldarwekkf 662w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/j04idbu7uyzdwxzbooxj 722w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/msxp6sfpz9y94pl1cfsh 982w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/c77ucuyibbyryhwy6ip6 1032w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3KAuAS2shyDwnjzNa/heewi1kedadam7ogpqjn 1398w\"></p><p>Or maybe you hear some <a href=\"https://www.facebook.com/yudkowsky/posts/10160260422389228\">prominent doomer proclaiming that \u201csane people with self-respect\u201d don\u2019t update predictably</a>, without clarifying about \u201cin expectation\u201d despite <a href=\"https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence\">definitely knowing about this</a>, and so you assume you must be unsane and self-hating. Or maybe you think that if you do update predictably, it should at least be in the direction of your currently-predicted truth, and you forget about cases like the waterfalls above.</p><p>In general, this stuff can get tricky. We should be careful, and not self-righteous, even when the math itself is clear. &nbsp;</p><h2>9. Just saying \u201coops\u201d</h2><p>I also want to add a different note of caution, about not letting consistency, or your abstract picture of what \u201cgood Bayesianism\u201d looks like, get in the way of updating as fast as possible to the right view, whatever that is.</p><p>Thus, for example, maybe you tweeted a bunch in the past re: \u201cno way\u201d on AI risk, and acted dismissive about it. Maybe, even, you\u2019re someone like David Thorstad, and you were kind enough to quantify your dismissiveness with some very-low number.</p><p>And let\u2019s say, later, your gut starts twinging. Maybe you see some scary demo of deceptiveness or power-seeking. Maybe you don\u2019t like the look of all those increasingly-automated, AI-run wet-labs. Maybe it all just starts changing too fast, and it feels too frenetic and out of control, and do we even understand how these systems are working? Maybe it\u2019s something about those new drones. &nbsp;</p><p>It might be tempting, here, to let your previous skepticism drag your new estimates downwards \u2013 including on the basis of the sorts of dynamics discussed above. Maybe, for example, if you had David Thorstad\u2019s number, you\u2019re tempted to move from .00002% to something like, hmm, 20%? But you say to yourself \u201cwait, have I really gotten <i>such strong evidence</i> since my previous estimate? Have I been <i>so surprised</i> by the demos, and the drones, and the wet-labs? Apparently, I\u2019m moving to a number I should\u2019ve been less than one-in-a-million I\u2019d ever end up at. By my previous lights, isn\u2019t that unlikely to be the right move?\u201d</p><p>But the thing is: it\u2019s possible that your previous estimate was just \u2026 way too low. And more (gasp), that it didn\u2019t come with some well-formed probability distribution over your future estimates, either. We should be wary, in general, of taking our previous (or our current) Bayesian rigor too seriously. Should \u201cyou,\u201d above, refrain from changing her potato-2024 estimate quickly as she thinks about it more, on grounds that it would make her two-seconds-ago self\u2019s Bayesianism look bad? Best to just get things right.</p><p>Of course, it may be that your previous self was tracking some sort of evidence that you\u2019re losing sight of, now. It may be that your gut is skittish. You should try to learn from your previous self what you can. But you should try, I suspect, to learn harder from the actual world, there in front of you.</p><p>Here, to be clear, I\u2019m partly thinking about myself, and my own mistakes. I said 5% in 2021. I more than doubled my estimate soon after. &nbsp;By basic Bayes, I should\u2019ve been less than 50%, in 2021, that this would happen. Did I really get sufficiently worrying evidence in the interim to justify such a shift? Maybe. But alternatively: whatever, I was just wrong. Best to just say oops, and to try to be righter.</p><p>I\u2019m focusing on people with very low estimates on doom, here, because they tend to be more common than the converse. But everything I\u2019m saying here holds for people with low estimates on non-doom, too. If you\u2019re such a person, and you see signs of hope later, don\u2019t be attached to your identity as a definitely-doomer, or to the Bayesian rigor of the self that assumed this identity. Don\u2019t practice your pessimism over-hard. You might miss the thing that saves your life.</p><p>Really, though, I suspect that respect for your previous self\u2019s Bayesianism is not the main barrier to changing our minds fast enough. Rather, the barriers are more social: embarrassment stuff, tribal stuff, status stuff, and so on. I think we should try to lower such barriers where possible. We should notice that people were wrong; but we should not make fun of them for changing their minds \u2013 quite the contrary. Scout mindset is hard enough, and the stakes are too high.</p><h2>10. Doing enough</h2><blockquote><p><i>\"I imagine death so much it feels more like a memory\u2026\"</i></p><p><i>- </i><a href=\"https://youtu.be/BQ1ZwqaXJaQ?t=97\"><i>Hamilton</i></a></p></blockquote><blockquote><p><i>\u201cWhen my time is up, have I done enough?\u201d</i></p><p><i>- </i><a href=\"https://www.youtube.com/watch?v=_gnypiKNaJE\"><i>Eliza</i></a></p></blockquote><p>I\u2019ll close by noting a final sort of predictable update. It\u2019s related to the scans thing.</p><p>There\u2019s <a href=\"https://www.youtube.com/watch?v=W9vj2Wf57rQ\">a scene</a> at the end of <i>Schindler\u2019s List</i>. World War II is over. Schindler has used his money to save more than 1,100 lives from the holocaust. As the people he has saved say goodbye, Schindler breaks down:</p><blockquote><p>I could have got more out. I could have got more. I don\u2019t know. If I\u2019d just\u2026 I could have got more\u2026 I threw away so much money. You have no idea\u2026 I didn\u2019t do enough\u2026 This car. Goeth would have bought this car. Why did I keep the car? Ten people right there. Ten people. Ten more people. This pin. Two people. This is gold. Two more people. He would have given me two for it, at least one. One more person. A person, Stern. For this. I could have gotten one more person\u2026 and I didn\u2019t.</p></blockquote><p>Now, we need to be careful here. It\u2019s easy for the sort of stuff I\u2019m about to say to prompt extreme and unbalanced and unhealthy relationships to stuff that matters a lot. In particular, if you\u2019re tempted to be in some \u201cemergency\u201d mode about AI risk (or, indeed, about some other issue), and to start burning lots of resources for the sake of doing everything you can, I encourage you to read <a href=\"https://www.lesswrong.com/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai\">this article</a>, together with <a href=\"https://www.lesswrong.com/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai?commentId=Htf2v79w5QoQJbysS#comments\">this comment</a> about memetic dynamics that can amplify false emergencies and discourage clear thinking.</p><p>Still, still. There\u2019s a possible predictable update here. If this AI stuff really happens, and the alignment stuff is looking rough, there is a way we will each feel about what we did with the time we had. How we responded to what we knew. What role we played. Which directions we pointed the world, or moved it. How much we left on the field.</p><p>And there is a way we will feel, too, about subtler things. About what sorts of motivations were at play, in how we oriented towards the issue. About the tone we took on twitter. About the sort of <a href=\"https://joecarlsmith.com/2022/12/23/on-sincerity\">sincerity</a> we had, or didn\u2019t have. One thing that stayed with me from <i>Don\u2019t Look Up</i> is the way the asteroid somehow slotted into the world\u2019s pre-existing shallowness; the veneer of unreality and unseriousness that persisted even till the end; the status stuff; the selfishness; the way that somehow, still, that fog. If AGI risk ends up like this, then looking back, as our time runs out, I think there will be room for the word \u201cshame.\u201d Death does not discriminate between the sinners and the saints. But I do actually think it\u2019s worth talk of dignity.</p><p>And there is a way we will feel, too, if we step up, do things right, and actually solve the problem. Some doomer discourse is animated by a kind of bitter and exasperated pessimism about humanity, in its stupidity and incompetence. But different vibes are available, too, even holding tons of facts fixed. Here I\u2019m particularly interested in \u201clet\u2019s see if we can actually do this.\u201d Humans can come together in the face of danger. Sometimes, even, danger brings out our best. It is possible to see that certain things should be done, and to just do them. It is possible for people to work side by side.</p><p>And if we do this, then there is a way we will feel when it\u2019s done. I have a friend who sometimes talks about what he wants to tell his grandchildren he did, during the years leading up to AGI. It\u2019s related to that thing about history, and who its eyes are on. We shouldn\u2019t need people to tell our stories; but as far as I can tell, if he ever has grandchildren, they should be proud of him. May he sit, someday, under his own vine and fig tree.</p><p>Of course, there is also a way we will feel if AGI happens, but the problem was unreal, or not worth worrying about. There are <a href=\"https://www.planned-obsolescence.org/the-costs-of-caution/\">costs of caution</a>. And of course, there is a way we will feel if all this AGI stuff was fake after all, and all that time and money and energy was directed at a fantasy. You can talk about \u201creasonable ex ante,\u201d but: will it have been reasonable? If this stuff is a fantasy, I suspect it is a fantasy connected with our flaws, and that we will have been, not innocently mistaken, but actively foolish, and maybe worse. Or at least, I suspect this of myself.</p><p>Overall, then, there are lots of different possible futures here. As ever, the Bayesian tries to live in all of them at once. Still: if, indeed, we are running out of time, and there is a serious risk of everyone dying, it seems especially worth thinking ahead to hospitals and scans; to what we will learn, later, about \u201cenough\u201d and \u201cnot enough,\u201d about \u201cdone\u201d and \u201cleft undone.\u201d Maybe there will be no history to have its eyes on us \u2013 or at least, none we would honor. But we can look for ourselves. &nbsp;&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnpfbbdkv1or\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnpfbbdkv1or\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To be clear: there are lots of other risks from AI, too. And the basic dynamics at stake in the essay apply to your probabilities on any sorts of risks. But I want to focus on existential risk from misalignment, here, and I want the short phrase \u201cAI risk\u201d for the thing I\u2019m going to be referring to repeatedly.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlb07r9kktsi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflb07r9kktsi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Though, the specific numbers here can matter \u2013 and there are some cases where despite having low probabilities on doom now, you can predict ahead of time that you\u2019ll be at least somewhat more worried later (though, there are limits to how much). More below.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwkbi3ncx2rn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwkbi3ncx2rn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Though with respect to AI doom, not risk free \u2013 see <a href=\"https://www.lesswrong.com/posts/pLLeGA7aGaJpgCkof/mental-health-and-the-alignment-problem-a-compilation-of\">here</a> for some mental health resources.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnb4qrdse64bh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefb4qrdse64bh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Hopefully not more literally similar. But: a new thing-not-imagined-very-well.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1chknilw5ea\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1chknilw5ea\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Modulo some futurisms. Including, importantly, ones predictably at stake in AI progress.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnylz55qu4y7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefylz55qu4y7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cIn an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine\u2019s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time.\u201d</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnom83hglib5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefom83hglib5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Thanks to Katja Grace for discussion.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1gnwotoahij\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1gnwotoahij\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Some forecasts have self-fulfilling elements, especially with respect to Moloch-like problems. And there are questions about e.g. internet text increasing the likelihood of AIs acting out the role of the scary-AI.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0xfc6qza6u8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0xfc6qza6u8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See e.g. Scott Alexander <a href=\"https://astralcodexten.substack.com/p/mantic-monday-31422\">here</a>. Some of <a href=\"https://forum.effectivealtruism.org/posts/Lto9awEYPQNu9wkdi/rational-predictions-often-update-predictably#fn6am2fn0yyve\">Yudkowsky\u2019s public comments</a> suggest this model as well, though his original discussion of \u201c<a href=\"https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence\">conservation of expected evidence</a>\u201d does not.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnp6fohdm226\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefp6fohdm226\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Here I\u2019m indebted to discussion from <a href=\"https://forum.effectivealtruism.org/posts/Lto9awEYPQNu9wkdi/rational-predictions-often-update-predictably\">Greg Lewis</a> and <a href=\"https://www.lesswrong.com/posts/zTfSXQracE7TW8x4w/mistakes-with-conservation-of-expected-evidence\">Abram Demski</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5mhnx3fqqo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5mhnx3fqqo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>.99*1 + .01*0 = .99.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbe5s9q5go0r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbe5s9q5go0r\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Or put another way: you want to find the area that red occupies, which is the area of the first, smaller red box, plus the area of the bigger red box. Each box occupies a percentage of the area of a \u201ccolumn\u201d (combination of white box and red box) associated with a hypothesis about GPT-6. So to find the area of a given red box, you take the area of the column it\u2019s in (that is, the probability on the relevant hypothesis about GPT-6), and multiply that by the percentage of that column that is red (e.g., the probability of doom conditional on that hypothesis). Then you add up the areas of the red boxes.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn416kaiwzc91\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref416kaiwzc91\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Thanks to Daniel Kokotajlo for highlighting some of these dynamics to me years ago. See also his review of my power-seeking AI report <a href=\"https://docs.google.com/document/d/1GwT7AS_PWpglWWrVrpiMqeKiJ_E2VgAUIG5tTdVhVeM/edit#heading=h.e9o5m3fab0ua\">here</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnr4lzpfmu09s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefr4lzpfmu09s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I added an edit to this effect.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fne6nqxiwigdk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefe6nqxiwigdk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Thanks to Katja Grace for discussion here.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnamcm4d3vca5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefamcm4d3vca5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Here I\u2019m inspired by some comments from Richard Ngo.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrq5fnwph34\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrq5fnwph34\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Though: maybe it just works out the same? E.g., the average of your estimates over time needs to obey Bayesian constraints?</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fni2d4549lu1m\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefi2d4549lu1m\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Again, thanks to Katja for pointing to this dynamic.</p></div></li></ol>", "user": {"username": "Joe_Carlsmith"}}, {"_id": "c9QFHhu7qKewYgujH", "title": "One form to help us build a crowdsourced charity evaluator", "postedAt": "2023-05-08T21:03:51.501Z", "htmlBody": "<p><strong>Tl;dr:</strong> We\u2019re building a <strong>crowdsourced, cause-neutral charity evaluator</strong>, and we\u2019ve already onboarded <a href=\"https://app.impactmarkets.io/\">12 new charitable projects</a> this year! To build momentum, <strong>we\u2019re collecting </strong><a href=\"https://airtable.com/shr1eRlbcr43os6SX\"><strong>expressions of interest from donors</strong></a>. (That\u2019s a ~45 second form.)</p><p>Classic EA charity evaluators have some limitations:</p><ol><li>They often can\u2019t recommend tiny projects because the cost of reviewing them and the probability that they will not recommend them are much higher than for larger, better established projects. Meanwhile their funding gaps are smaller.</li><li>They often can\u2019t recommend projects because they are so far outside their network that it\u2019s hard to build trust with them.</li><li>They often can\u2019t recommend projects because they are so far outside their areas of expertise that they can\u2019t evaluate them.</li></ol><p>But donors, albeit imperfect in other ways, have diverse networks, diverse areas of expertise, live in a diversity of places, and sometimes have personal ties to charity entrepreneurs.</p><p>We want to make that knowledge legible so that it becomes easier for other donors to give, even in spaces that are not yet served by a central charity evaluator.</p><p>The more donors are interested in using <a href=\"https://app.impactmarkets.io/\">our platform</a>, the greater the incentive for awesome projects and talented grantmakers to join it too. <strong>So it would be super helpful for us if you could fill in our quick (~45 s) form for </strong><a href=\"https://airtable.com/shr1eRlbcr43os6SX\"><strong>expressions of interest</strong></a><strong>.</strong></p><p>Thank you!</p><p>PS: Our long-term plan is to grow the platform into a proper impact market \u2013 like carbon credit markets but for any kind of positive impact!</p>", "user": {"username": "Telofy"}}, {"_id": "rvMirJZGLePztjHp8", "title": "Reminder: AI Worldviews Contest Closes May 31", "postedAt": "2023-05-08T17:40:06.633Z", "htmlBody": "<p><strong>Update</strong>: <i>The winners have been selected and notified and will be publicly announced no later than the end of September.</i></p><p>This is a reminder that the deadline to enter the <a href=\"https://www.openphilanthropy.org/open-philanthropy-ai-worldviews-contest/\">Open Philanthropy AI Worldviews Contest</a> is <strong>May 31</strong>. We plan to distribute $225k across six winning entries. We're looking for essays that address the probability of AGI by 2043 or the probability of existential catastrophe conditional on AGI arriving by 2070. Use <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdADOikiiQgXLwdEJ2Mou5IQT6-9BH8zcZ0pjQzwex069jjjA/viewform\">this form</a> to enter.</p><p>The original announcement post is <a href=\"https://forum.effectivealtruism.org/posts/NZz3Das7jFdCBN9zH/announcing-the-open-philanthropy-ai-worldviews-contest\">here</a>.</p>", "user": {"username": "Jason Schukraft"}}, {"_id": "iQCbubkxFCcZXmXZ9", "title": "How The EthiSizer Almost Broke `Story'", "postedAt": "2023-05-08T16:58:16.197Z", "htmlBody": "<h3>Introduction: On a great Yudkowsky essay: `Serious Stories'&nbsp;</h3><p>Yudkowsky's short essay `Serious Stories' (02009) is a fascinating <i>unit of culture</i>, and I commend it to you. (You can - and probably even <i>ought to</i> - read it online, <a href=\"https://www.lesswrong.com/posts/6qS9q5zHafFXsB6hf/serious-stories\">here</a>.)</p><p>That essay deals with the crucial <a href=\"https://en.wikipedia.org/wiki/Futures_studies\">Futures Studies</a> problem, that:&nbsp;</p><blockquote><p>\"Every Utopia ever constructed\u2014in philosophy, fiction, or religion\u2014has been, to one degree or another, a place where you wouldn't <i>actually want</i> to live.\"</p></blockquote><p>It also raises such crucial questions as: <i>What Is A Story?</i> And as Yudkowsky rightly writes:&nbsp;</p><blockquote><p>\"If you read books on How To Write... these books will tell you that stories must contain \"conflict\".\"&nbsp;</p></blockquote><p><strong>Brief Boring Backstory Bit:</strong> Among <a href=\"https://on-writering.blogspot.com/2023/02/online-multimedia-showreel-02023.html\">other occupations</a>, I've been a professional storyteller since 01993, (three decades now, <i>yikes</i>) and along the way, I studied Narrative, in order to try and do it less-worse... I published a free book while at Film School in 01995 (<a href=\"https://storyality.wordpress.com/2012/12/17/storyality-28-screenwriting-manuals-since-1913/\">a summary of useful narrative tools for professional screenwriters</a>), and in 02016, I completed an <a href=\"https://evolutionary-culturology.blogspot.com\">Evolutionary Culturology</a> Ph.D at <a href=\"https://storyality.wordpress.com/2020/06/07/storyality166-the-newcastle-school-of-creativity/\">The Newcastle School of Creativity</a>, that involved a lot of close study of Story/Narrative. (...More on that PhD <a href=\"https://storyality.wordpress.com/my-phd-dissertation-free-online/\">here</a>, and, a super-brief <i>Lit Review of Narratology</i>, <a href=\"https://storyality.wordpress.com/2012/12/17/storyality-27-narratology-since-plato-a-brief-lit-review/\">here</a>.) But I digress. <i>[End of Boring-Backstory-Bit]</i>&nbsp;</p><h3>What is a story?</h3><p>I know that the great philosopher of science Sir Karl Popper said <i>\"What is ~ ? \" questions are a waste of time and space</i>, but whatever.</p><p>My own preferred algorithm (or formula, or equation) for `story/narrative' - due both to its <i>simplicity/parsimony</i>, and <i>generality of applicability </i>- is Jon Gottschall's (02012) definition, from the great book, <i>The Storytelling Animal</i>:</p><blockquote><p>`Story = [1] Character + [2] Problem + [3] Attempted Extrication\u2019&nbsp;</p><p>(Gottschall 02012, p. 52)</p></blockquote><p>By this definition, without those 3 key elements present [#1,2 &amp; 3, above], you may well be experiencing something interesting, but technically, a \"story/narrative\" it: isn't. &nbsp;&nbsp;</p><p>In short, a `Problem' (or, a Goal, or, an Objective) for an <i>Intentional Agent </i>(i.e., a <i>Character/s</i>) results in <i>conflict,</i> as indeed Yudkowsky notes, in his great essay...</p><p>And, any `Scenes' (in any Communication Media) <i>without </i>dramatic <i>conflict</i>, can get boring (uncompelling) fast.&nbsp;</p><p>(In simple terms, watching two or more agents `do battle' is usually engaging for us humanimals. ...What's not to learn-? We tend to root for one of them, and, pay close attention to what strategies <i>work</i>, in what <i>situations</i>.) Enter: <a href=\"https://forum.effectivealtruism.org/topics/game-theory\">Game Theory</a>... (another story, for another time.)</p><h3>All Life Is Problem-Solving</h3><p>As the great Popper pointed out, in his wonderful book of collected essays, <i>All Life Is Problem Solving</i> (01999):</p><blockquote><p>`The great majority of theories are false and/or untestable. Valuable, testable theories will search for errors. We try to find errors and to eliminate them. This is science: it consists of wild, often irresponsible ideas that it places under the strict control of error correction.&nbsp;</p><p>Question: This is the same process as in amoebas and other lower organisms. What is the difference between an amoeba and Einstein?&nbsp;</p><p>Answer: The amoeba is eliminated when it makes mistakes. If it is conscious it will be afraid of mistakes. Einstein looks for mistakes. He is able to do this because his theory is not part of himself but an object he can consciously investigate and criticize.' (Popper 01999, p. 39)</p></blockquote><p>Small wonder that people love (good) stories so much.&nbsp;</p><p>A good story is: Game Theory <i>Illustrated</i>.&nbsp;</p><p>Or if you prefer, Game Theory <i>Enacted</i>.</p><p>As part of my 02016 PhD-work, I plotted all the Scenes in the top-20 most profitable movies, awarding 1 point when a character won a scene (at scene's end), and deducting one when they lost (some Scenes are a 0-0 draw). Those charts look like this:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/fpivdibqzrc86yw02v7f\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/nw5b13f2tbfer9yxhu9c 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/nmodvvgh9rcbbpkip3cj 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/mvgqkwxkb9l9qmuvtphy 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/qwnv4aqu6ccnxaniazrv 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/fystjpqd63ag0djjfjeq 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/zweb3kkzxa4q8rdjc0up 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/tlz2bvocfs5vn9scwlxc 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/qy3wsnkbktltoy80umvq 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/dkqb89kkmoj9issjop76 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/xiq8qkemqdnkbvmajbxz 1036w\">In other words, scored in that way, (which is, of course, just <i>one </i>of <i>many possible ways </i>of scoring those Scenes &amp; Characters) these stories are overwhelmingly, `Villain Triumphant' stories, at their story endpoint. (See, also, <a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Main/TheBadGuyWins\">\"The Bad Guy Wins\" story trope</a>).&nbsp;</p><p><i>...Why am I telling you all this?</i></p><p>My point being, there are:&nbsp;</p><h3>Utopias Worth Wanting</h3><p>I suggest, there are some Utopias that you <i>would </i>want to live in...!&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/6qS9q5zHafFXsB6hf/serious-stories\">Yudkowsky</a> (and, as he notes, <a href=\"https://k-1.com/Orwell/site/work/essays/fun.html\">Orwell</a>) are right, that most canonical/popular stories about Utopias aren't very appealing...&nbsp;</p><p>Storytellers aiming to <i>capture and maintain audience attention</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbbg2iuzx0pd\"><sup><a href=\"#fnbbg2iuzx0pd\">[1]</a></sup></span>&nbsp;need to keep throwing <i>problems </i>(thus, <i>conflict</i>) hard-&amp;-fast at their protagonists, or else folks fall asleep.&nbsp;</p><p>In <i>The EthiSizer - A Novellarama </i>(02022), The EthiSizer AI writes:&nbsp;</p><blockquote><p>`<strong>On Good and Bad Science Fiction: Utopias, Dystopias, and everything in between</strong></p><p>A <i>Google Ngram</i> search leads one to believe that people have written more about `utopias\u2019 than `dystopias\u2019.<a href=\"https://books.google.com/ngrams/graph?content=utopia%2Cdystopia&amp;year_start=1700&amp;year_end=2019&amp;corpus=en-2019&amp;smoothing=3&amp;case_insensitive=true\">[68]</a>&nbsp;</p><p>Yet in the realm of fiction, the opposite seems to hold! In the great short story collection <i>Brave New Worlds</i> (Adams, 02012), Ross E. Lockhart collated a list of `notable utopian fiction\u2019 (20 works, ranging from Iain M. Banks\u2019 <i>Consider Phlebas (The Culture Series)</i> to B. F. Skinner\u2019s <i>Walden Two</i>), and conversely, `notable dystopian fiction\u2019: 153 works, ranging from Andrew Foster Altschul\u2019s <i>Deus Ex Machina</i>, to Rob Zeigler\u2019s <i>Seed </i>(p. 99%).&nbsp;</p><p>Currently (at the time of writing of this sentence), Wikipedia lists 97 works under `List of Utopian Literature\u2019<a href=\"https://en.wikipedia.org/wiki/List_of_utopian_literature\">[69]</a> and yet 274 works under `List of Dystopian Literature\u2019.<a href=\"https://en.wikipedia.org/wiki/List_of_dystopian_literature\">[70]</a></p><p>Notice a pattern? Why so many more stories about bad futures, than good? One reason is: Stories (narratives) where everything\u2019s great are not as compelling to experience. Authors need to sell books to put food on the table. As evolutionary literary scholar Jon Gottschall (02012) rightly notes, humanimals are \u201cthe storytelling animal\u201d after all. And, without a problem\u2013without something going wrong\u2013there is no story, or narrative. Most authors take the path of less resistance, and figure: Why not have pretty much everything go wrong, as in, a dystopia?&nbsp;</p><p>But as Harari notes: life\u2013and indeed reality\u2013is not a story! See (Harari, 02018, Ch 20).</p><p>A feature of human nature is that we humans like to mentally escape into fictional worlds. Dystopian stories like <i>Frankenstein</i>, <i>The Terminator </i>franchise, and <i>The Hunger Games </i>series all sell vastly more copies (and movie tickets) than do utopian stories, such as Iain M. Banks\u2019 <i>Culture</i> series.&nbsp;</p><p>The evolved psychology of the humanimal mind has a negativity bias, finding bad news more memorable, and attention-worthy than good news, as Evolutionary Psychologist David Buss (02012, pp. 393-4) quite rightly notes.&nbsp;</p><p>As a result of this negativity bias,<a href=\"https://en.wikipedia.org/wiki/List_of_cognitive_biases\">[71]</a> people fixate on murderous reanimated corpses such as Frankenstein\u2019s monster; murderous psychotic computers such as HAL-9000 from <i>02001: A Space Odyssey</i>; <i>The Terminator</i>\u2019s SkyNet; and the machines in <i>The Matrix </i>franchise, rather than say the benevolent digital assistant in the movie <i>Her</i> (02013).&nbsp;<br>&nbsp;</p><p>Source: <a href=\"https://www.amazon.com/dp/B0BPGQCBVX\"><i>The EthiSizer - A Novella-rama (The EthiSizer, 02022, p. 52)</i></a></p></blockquote><h3><strong>Problem: How To Create a Utopia That's Narratively Compelling?</strong></h3><p>&nbsp;Thus, if we imagine a future where a <a href=\"https://nickbostrom.com/fut/singleton\"><i>singleton</i></a> arises - thus solving all global problems, and also ending all suffering - where's the \"story-juice\"...?&nbsp;</p><p>...Who wants to watch, <i>that-</i>? ...Let alone, <i>live </i>in it-? &nbsp;</p><p>A world with no more: <i>wars, murders, rapes, thefts, trauma, injustice</i>...&nbsp;</p><p>...Wouldn't it be: super-boring?&nbsp;</p><p>As if there's no more <i>problems</i>, due to <i>The All-Seeing Eye of The EthiSizer</i>, then surely there's no <i>story - \u203d</i></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/fn0yt8cuq5qgbakyloec\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/e0dotbqiumpcuv01kfai 190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/perlbkexwb10oxqt2zov 380w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/zicr9ec6mnomnr98oq9w 570w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/jrr5zqibbu3tjkwg36cu 760w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/gestrawekxamapdcaxxa 950w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/ot2pdy2ivoqj92x0ikv1 1140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/egx8j2suiksik9hzx6ww 1330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/twy69o36hqugb5rswokt 1520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/zfqgqklwldq0olh5lztt 1710w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iQCbubkxFCcZXmXZ9/bjidleutznb4ry5rniin 1806w\"></figure><h3>Not so fast...</h3><p>If you have a singleton - like say an<i> </i><a href=\"https://forum.effectivealtruism.org/posts/dntYZ44ySurKAZjcz/the-6e-essay\"><i>EthiSizer</i></a>, that behaves like an omniscient and omnipotent `god' of yore,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefubzo4i8otx\"><sup><a href=\"#fnubzo4i8otx\">[2]</a></sup></span>&nbsp;and, punishes `sinners' (folks whose <i>Personal Ethics Score</i> dips below 0%) - there's still plenty of scope for drama, action, and conflict.&nbsp;</p><p>...Of course, one's first instinct (probably?) is to surmise that, in a perfect world (a Utopia), nobody would ever do anything wrong, bad, or evil...&nbsp;</p><p>Thus, no problems. Thus, no conflict. Thus, no suffering.</p><p>But on the other hand, if people still have <i>freedom of choice (</i>which is reflected in their <i>actions, </i>and thus, in their <i>Personal Ethics Score)</i>, surely some <i>bad guys</i> will get up to their old tricks...&nbsp;</p><p>And the fun comes in, when they get caught by <i>The EthiSizer</i>...&nbsp;</p><p>So in terms of <i>story/narrative</i>, plenty of scope for <i>EthiSizer Droids </i>to bust in and dispense ethical justice...&nbsp;</p><p>(So, in moviespeak, maybe think: <i>Robocop (01987)</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefiktg4degqbf\"><sup><a href=\"#fniktg4degqbf\">[3]</a></sup></span><i>&nbsp;</i>meets the <i>Terminator movies,</i> via <i>I, Robot</i>.)&nbsp;</p><p>For the <i>storytellers</i>, the fun is in thinking up all the <i>Ethical Violation</i> scenarios...&nbsp;</p><p>(And for what it's worth, Tolstoy would probably approve - as in later life, he felt any author not clearly taking a <i>moral position</i> on their characters was a waste of ink and think.)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5467x250rfb\"><sup><a href=\"#fn5467x250rfb\">[4]</a></sup></span></p><p>For <i>audiences/readers</i>, the fun is in: seeing <i>The Bad (Unethical) Guys, Lose</i>.&nbsp;</p><p>I'd like to live in that world...?&nbsp;</p><p>No more corrupt politicians, for one thing.&nbsp;</p><p>Also - World Peace... No more war.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkc4sep6tvwg\"><sup><a href=\"#fnkc4sep6tvwg\">[5]</a></sup></span></p><p>I'd <i>definitely</i> prefer to live in <i>that </i>world...&nbsp;</p><p>Anyway - if of interest to any <i>Futures Studies</i> scholars, there's more specific examples (you might even call them `Narrative Case Studies') in <a href=\"https://www.amazon.com/dp/B0BPGQCBVX\">this book</a>, and <a href=\"https://www.amazon.com/dp/B0BRLYM3XF/\">that book</a>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrqi474ed1nd\"><sup><a href=\"#fnrqi474ed1nd\">[6]</a></sup></span>&nbsp;</p><p>And please do be inspired to write your own Utopian Singleton Stories...<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2ha54qb21cr\"><sup><a href=\"#fn2ha54qb21cr\">[7]</a></sup></span></p><h3>Conclusion</h3><p>To sum up: Yudkowsky's 02009 essay `<a href=\"https://www.lesswrong.com/posts/6qS9q5zHafFXsB6hf/serious-stories\">Serious Stories</a>' is great. As is, that whole <a href=\"https://www.lesswrong.com/s/9bvAELWc8y2gYjRav\"><i>Value Theory Series</i></a> of essays.&nbsp;</p><p>And yes, most Utopias in literature are: kinda boring...! (Thus, we have vastly more Dystopias in popular and classical literature. Especially, <i>Science Fiction Dystopias</i>.)</p><p>However.<i> The EthiSizer</i> demonstrates that a <i>super-ethical world</i> would be one worth living in, and still leaves plenty of scope for stories. Problems and (super-ethical) solutions.</p><p>Also, let's suck it and see-?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxfajj3xg8gi\"><sup><a href=\"#fnxfajj3xg8gi\">[8]</a></sup></span>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbbg2iuzx0pd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbbg2iuzx0pd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For more on all that, see the great book: <i>On The Origin of Stories: Evolution, Cognition, and Fiction </i>(Brian Boyd, 02009).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnubzo4i8otx\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefubzo4i8otx\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Some commentators have suggested all of Earth's <a href=\"https://en.wikipedia.org/wiki/Religion\">10,000 past religions</a> have psychologically prepared humanity for a Singleton, such as <i>The EthiSizer</i>. (They may be right?) Like <a href=\"https://en.wikipedia.org/wiki/Dataism\">Dataism</a>, <i>The EthiSizer</i> makes Science a religion.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fniktg4degqbf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefiktg4degqbf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Nothing wrong with the (02014) <i>Robocop </i>movie; I just prefer satires. In fact, I prefer <i>science-fiction </i><a href=\"https://storyality.wordpress.com/2012/11/24/storyality-4b-on-mindbender-movies/\"><i>mindbender</i></a><i> satires</i>, but that's just me.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5467x250rfb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5467x250rfb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See: <i>A Swim In The Pond In The Rain</i> (Saunders 02021), specifically the section on Tolstoy, titled `And Yet They Drove On - Thoughts on Master and Man'. And Tolstoy really goes to town, in `The Works of Guy de Maupassant' (Tolstoy 01894).&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkc4sep6tvwg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkc4sep6tvwg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For more, see: <a href=\"https://www.academia.edu/99120540/The_Open_Society_and_The_EthiSizer\">`The Open Society and The EthiSizer' (Velikovsky 02022), <i>4th International Zoom-Based Conference on the Thought of Karl Popper</i></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrqi474ed1nd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrqi474ed1nd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Maybe see also <a href=\"https://the-ethisizer.blogspot.com/\">that blog</a>, and <a href=\"https://ethisizer-novel.blogspot.com/\">that one</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2ha54qb21cr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2ha54qb21cr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See <a href=\"https://forum.effectivealtruism.org/posts/dntYZ44ySurKAZjcz/the-6e-essay\"><i>The 6E Essay</i></a> for more details.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxfajj3xg8gi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxfajj3xg8gi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>(Side Note: I find it annoying when people form and pass `opinions' on things they haven't yet experienced - or, simulated in a computer - themselves. Don't you?)</p></div></li></ol>", "user": {"username": "Velikovsky_of_Newcastle"}}, {"_id": "YLaqDduigkbnjkf26", "title": "Books \u2014 what do you recommend? What are you looking for?", "postedAt": "2023-05-08T16:29:24.484Z", "htmlBody": "<p>Let\u2019s crowd-source a list of books that people have found useful \u2014 particularly books outside of the \u201cEA canon.\u201d So please&nbsp;<strong>share some books</strong> that you like or have found useful!</p><p>You could also&nbsp;<strong>ask for recommendations</strong> on particular topics.&nbsp;</p><p>There are already some&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/books\"><u>book recommendations on the Forum</u></a> (see also&nbsp;<a href=\"https://www.lesswrong.com/tag/book-reviews-media-reviews\"><u>LessWrong</u></a>) \u2014 but many people probably haven\u2019t seen those, and it\u2019s been a while since the last thread like this. Even if a book has already been mentioned or discussed on the Forum, I\u2019m probably still happy to see it here. (If it\u2019s got its own topic page, it\u2019s probably too well known, although I don\u2019t mind those getting added, either, if you want to provide some commentary.)</p><h3>Suggested format &amp; info to include:&nbsp;</h3><p>Consider using the following formats, but feel free to modify them if that's more useful:</p><p>If you\u2019re recommending a book \u2014 see also <a href=\"https://forum.effectivealtruism.org/posts/YLaqDduigkbnjkf26/book-recommendations-what-do-you-recommend-what-are-you?commentId=6RnfPuAeAqKFmZ6ir\">my&nbsp;comment</a>:</p><blockquote><p><i><strong>[Hyperlinked title]</strong></i><strong> by [Author]</strong></p><p><strong>What/who it\u2019s for/context:</strong> [Go here]</p><p><strong>Why/my thoughts:</strong> [Go here]</p><p><strong>Other notes:</strong> [Go here (are there good reviews you\u2019re aware of?)]</p></blockquote><p>If you\u2019re asking for a recommendation \u2014 see also <a href=\"https://forum.effectivealtruism.org/posts/YLaqDduigkbnjkf26/book-recommendations-what-do-you-recommend-what-are-you?commentId=4NJRbxcX2KFmLXuHL\">my&nbsp;comment</a>:</p><blockquote><p><strong>\ud83d\udd2d&nbsp;</strong>Looking for good books&nbsp;<strong>[on topic]</strong></p><p><strong>Some notes on why I\u2019m interested:</strong> [Notes]</p></blockquote><h3>See also:</h3><ul><li><a href=\"https://www.cold-takes.com/reading-books-vs-engaging-with-them/\"><u>Reading books vs. engaging with them</u></a> (Cold Takes post by Holden Karnofsky)</li><li>Recommendations:<ul><li><a href=\"https://forum.effectivealtruism.org/topics/books\"><u>Books</u></a> (EA Forum topic page)</li><li><a href=\"https://forum.effectivealtruism.org/posts/zCJDF6iNSJHnJ6Aq6/a-ranked-list-of-all-ea-relevant-audio-books-i-ve-read\"><u>A ranked list of all EA-relevant (audio)books I've read</u></a> (MichaelA, February 2021)</li><li><a href=\"https://forum.effectivealtruism.org/posts/mdTZktighWHBPdwd3/what-book-s-would-you-want-a-gifted-teenager-to-come-across\"><u>What book(s) would you want a gifted teenager to come across?</u></a> (Alex Lawsen, August 2019)</li><li><a href=\"https://forum.effectivealtruism.org/posts/KNZLGbGevnjStgzHt/i-scraped-all-public-effective-altruists-goodreads-reading\"><u>I scraped all public \"Effective Altruists\" Goodreads reading lists</u></a> (MaxRa, March 2021)&nbsp;</li></ul></li></ul><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YLaqDduigkbnjkf26/zwerjv5umur0jawsrahq\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YLaqDduigkbnjkf26/qmoehyisnqjxgtkjop8q 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YLaqDduigkbnjkf26/jtbyt95ss0hg8xfribvr 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YLaqDduigkbnjkf26/ia4pwb5ajcf9vsywzkoz 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YLaqDduigkbnjkf26/x9s7hgmyuqmgyiah31zf 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YLaqDduigkbnjkf26/rawur6rwsjuqi1ina6xh 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YLaqDduigkbnjkf26/cnhi7hywqi2asamu2jg7 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YLaqDduigkbnjkf26/bgbkzkrskm2rkrrh7qkb 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YLaqDduigkbnjkf26/cvwkq3pzj0h36tdss8on 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YLaqDduigkbnjkf26/ykjh2asagzck1tye5uxu 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YLaqDduigkbnjkf26/dfsjkgedjwwgfi6esqgb 1382w\"><figcaption>Illustration made with Midjourney</figcaption></figure><p><br>&nbsp;</p>", "user": {"username": "Lizka"}}, {"_id": "D8GitXAMt7deG8tBc", "title": "How quickly AI could transform the world (Tom Davidson on The 80,000 Hours Podcast)", "postedAt": "2023-05-08T13:23:50.969Z", "htmlBody": "<p>Over at <a href=\"https://80000hours.org/podcast/\">The 80,000 Hours Podcast</a> we just published an interview that is likely to be of particular interest to people who identify as involved in the effective altruism community: <a href=\"https://80000hours.org/podcast/episodes/tom-davidson-how-quickly-ai-could-transform-the-world/\"><strong>Tom Davidson on how quickly AI could transform the world</strong></a>.</p><p>You can click through for the audio, a full transcript and related links. Below is the episode summary and some key excerpts.</p><h1><strong>Episode Summary</strong></h1><blockquote><p><i>By the time that the AIs can do 20% of cognitive tasks in the broader economy, maybe they can already do 40% or 50% of tasks specifically in AI R&amp;D. So they could have already really started accelerating the pace of progress by the time we get to that 20% economic impact threshold.</i></p><p><i>At that point you could easily imagine that really it\u2019s just one year, you give them a 10x bigger brain. That\u2019s like going from chimps to humans \u2014 and then doing that jump again. That could easily be enough to go from [AIs being able to do] 20% [of cognitive tasks] to 100%, just intuitively. I think that\u2019s kind of the default, really.</i></p><p><i>Tom Davidson</i></p></blockquote><p>It\u2019s easy to dismiss alarming AI-related predictions when you don\u2019t know where the numbers came from.</p><p>For example: what if we told you that within 15 years, it\u2019s likely that we\u2019ll see a 1,000x improvement in AI capabilities in a single year? And what if we then told you that those improvements would lead to explosive economic growth unlike anything humanity has seen before?</p><p>You might think, \u201cCongratulations, you said a big number \u2014 but this kind of stuff seems crazy, so I\u2019m going to keep scrolling through Twitter.\u201d</p><p>But this 1,000x yearly improvement is a prediction based on <i>real economic models</i> created by today\u2019s guest <a href=\"https://www.tom-davidson.com/\">Tom Davidson</a>, Senior Research Analyst at Open Philanthropy. By the end of the episode, you\u2019ll either be able to point out specific flaws in his step-by-step reasoning, or have to at least <i>consider</i> the idea that the world is about to get \u2014 at a minimum \u2014 incredibly weird.</p><p>As a teaser, consider the following:</p><p>Developing artificial general intelligence (AGI) \u2014 AI that can do 100% of cognitive tasks at least as well as the best humans can \u2014 could very easily lead us to an unrecognisable world.</p><p>You might think having to train AI systems individually to do every conceivable cognitive task \u2014 one for diagnosing diseases, one for doing your taxes, one for teaching your kids, etc. \u2014 sounds implausible, or at least like it\u2019ll take decades.</p><p>But Tom thinks we might not need to train AI to do every single job \u2014 we might just need to train it to do one: AI research.</p><p>And building AI capable of doing research and development might be a much easier task \u2014 especially given that the researchers training the AI are AI researchers themselves.</p><p>And once an AI system is as good at accelerating future AI progress as the best humans are today \u2014 and we can run billions of copies of it round the clock \u2014 it\u2019s hard to make the case that we won\u2019t achieve AGI very quickly.</p><p>To give you some perspective: 17 years ago we saw the launch of Twitter, the release of Al Gore\u2019s <i>An Inconvenient Truth</i>, and your first chance to play the Nintendo Wii.</p><p>Tom thinks that if we have AI that significantly accelerates AI R&amp;D, then it\u2019s hard to imagine not having AGI 17 years from now.</p><p>Wild.</p><p>Host Luisa Rodriguez gets Tom to walk us through his careful reports on the topic, and how he came up with these numbers, across a terrifying but fascinating three hours.</p><p>Luisa and Tom also discuss:</p><ul><li>How we might go from GPT-4 to AI disaster</li><li>Tom\u2019s journey from finding AI risk to be <i>kind of scary</i> to <i>really scary</i></li><li>Whether international cooperation or an anti-AI social movement can slow AI progress down</li><li>Why it might take just a few years to go from pretty good AI to superhuman AI</li><li>How quickly the number and quality of computer chips we\u2019ve been using for AI have been increasing</li><li>The pace of algorithmic progress</li><li>What ants can teach us about AI</li><li>And much more</li></ul><p><strong>Get this episode by subscribing to our podcast on the world\u2019s most pressing problems and how to solve them: type \u201880,000 Hours\u2019 into your podcasting app. Or read the transcript below.</strong></p><p><i>Producer: Keiran Harris</i><br><i>Audio mastering: Simon Monsour and Ben Cordell</i><br><i>Transcriptions: Katy Moore</i></p><h1><strong>Highlights</strong></h1><h3><strong>Going from GPT-4 to AI takeover</strong></h3><blockquote><p><strong>Tom Davidson:</strong> We can try and think about this system which is trying to solve these math problems. Maybe the first version of the AI, you just say, \u201cWe want you to solve the problem using one of these four techniques. We want to use one of these seven methodologies on those techniques to get to an answer.\u201d And that system is OK, but then someone comes along and realises that if you let the AI system do an internet search and plan its own line of attack on the problem, then it\u2019s able to do a better job in solving even harder and harder problems. So you say, \u201cOK, we\u2019ll allow the AI to do that.\u201d</p><p>Then over time, in order to improve performance, you give it more and more scope to kind of be creative in planning how it\u2019s going to attack each different kind of problem. One thing that might happen internally, inside the AI\u2019s own head, is that the AI may end up developing just an inherent desire to just get the answer to this math question as accurately as possible. That\u2019s something which it always gets rewarded for when it\u2019s being trained. Maybe it could be thinking, \u201cI actually want the humans to be happy with my answer.\u201d But another thing it might end up thinking is, \u201cYou know what? What I really want is just to get the answer correct.\u201d And the kind of feedback that we humans are giving that system doesn\u2019t distinguish between those two possibilities.</p><p>So maybe we get unlucky, and maybe the thing that it wants is to just really get the answer correct. And maybe the way that the AI system is working internally is, it\u2019s saying, \u201cOK, that\u2019s my goal. What plan can I use to achieve that goal?\u201d It\u2019s creatively going and looking for new approaches by googling information. Maybe one time it realises that if it hacked into another computing cluster, it could use those computations to help it solve the problem. And it does that, and no one realises \u2014 and then that reinforces the fact that it is now planning on such a broad scale to try and achieve this goal.</p><p>Maybe it\u2019s much more powerful at a later time, and it realises that if it kills all humans, it could have access to all the supercomputers \u2014 and then that would help it get an even more accurate answer. Because the thing it cares about is not pleasing the humans \u2014 the thing it happened to care about internally was actually just getting an accurate answer \u2014 then that plan looks great by its own lights. So it goes and executes the plan.</p><p><strong>Luisa Rodriguez:</strong> Why couldn\u2019t you just give the system an instruction that didn\u2019t also come with rewards? Is it impossible to give an AI system a reward for every problem it solves by not hurting anyone?</p><p><strong>Tom Davidson:</strong> I think that would help somewhat. The problem here is that there are kind of two possibilities, and it\u2019s going to be hard for us to give rewards that ensure that one of the possibilities happens and not the second possibility.</p><p>Here are the two possibilities: One possibility is the AI really doesn\u2019t want to hurt humans, and it\u2019s just going to take that into account when solving the math problem. That\u2019s what we want to happen. The other possibility is that the AI only cares about solving the math problem and doesn\u2019t care about humans at all, but it understands that humans don\u2019t like it when it hurts them, and so it doesn\u2019t hurt humans in any obvious way.</p></blockquote><h3><strong>Why AGI could lead to explosive economic growth</strong></h3><blockquote><p><strong>Tom Davidson:</strong> Today there are maybe tens of millions of people whose job it is to discover new and better technologies, working in science and research and development. They\u2019re able to make a certain amount of progress each year. It\u2019s their work that helps us get better computers and phones, and discover better types of solar panels, and drives all these improvements that we\u2019re seeing.</p><p>But like we\u2019ve been talking about, shortly after AGI, there\u2019s going to be billions of top human researcher equivalents \u2014 in terms of a scientific workforce from AI. And if you imagine that workforce \u2014 or half of that workforce, or just 10% of it \u2014 working on trying to advance technology and come up with new ideas, then you have now 10 or 100 times the effort that\u2019s going into that activity. And these AIs are also able to think maybe 10 or 100 times as quickly as humans can think.</p><p>And you\u2019re able to take the very best AI researchers and copy them. So if you think that scientific progress is overwhelmingly driven by a smaller number of really brilliant people with brilliant ideas, then we just need one of them and we can copy them. They might be happy to just work much harder than humans work. It might be possible to focus them much more effectively on the most important types of R&amp;D, whereas humans maybe are more inclined to follow their interests, even when it\u2019s not the most useful thing to be researching.</p><p>All of those things together just mean that we\u2019ll be generating 100 times as many new good ideas and innovations each year compared with today, and then that would drive the development of technologies to be at least 10 times faster than today.</p><p><strong>Tom Davidson:</strong> I think this is a default. You could give objections to the argument I gave, but I think it\u2019s mostly possible to answer those objections. So you could say that discovering new technologies isn\u2019t just about thinking and coming up with new ideas; you also need to do experiments. I think you can answer that objection by saying that\u2019s right, we will need to do experiments.</p><p><strong>Luisa Rodriguez:</strong> And that\u2019s like testing a drug on humans, and maybe it takes five years or something to really check that it\u2019s safe and effective?</p><p><strong>Tom Davidson:</strong> Right. Or you\u2019ve designed a new solar panel, and you want to test its performance in a variety of conditions. Or you\u2019re running some experiments to see what happens when you combine these two chemicals together, because you\u2019re not able to predict it in advance.</p><p>But if you have a billion AIs trying to push forward R&amp;D, and they\u2019re bottlenecked on needing to do these experiments, then they\u2019ll be putting in a huge amount of effort to make these experiments happen as efficiently as possible. Whereas today we might be using the lab for 50% of the time we could be using it, and we might be just doing a whole bunch of experiments and then analysing it afterwards and learning a little bit from each experiment, but also not trying to cram as much into each experiment as is humanly possible. If these AIs are limited on experiments, then they\u2019re going to be spending months and months just meticulously planning the micro details of every single experiment, so that you can get as much information as possible out of each one.</p></blockquote><h3><strong>Why explosive growth is plausible despite sounding crazy</strong></h3><blockquote><p><strong>Tom Davidson:</strong> I agree it seems really crazy, and I think it\u2019s very natural and understandable to just not believe it when you hear the arguments.</p><p>I think what\u2019s at the heart of it for me is that the human brain is a physical system. There\u2019s nothing magical about it. It isn\u2019t surprising that we develop machines that can do what the human brain can do at some point in the process of technological discovery. To be honest, that happening in the next couple of decades is when you might expect it to happen, naively. We\u2019ve had computers for 70-odd years. It\u2019s been a decade since we started pouring loads and loads of compute into training AI systems, and we\u2019ve realised that that approach works really, really well. If you say, \u201cWhen do you think humans might develop machines that can do what the human brain can do?\u201d you kind of think it might be in the next few decades.</p><p>I think if you just sit with that fact \u2014 that there are going to be machines that can do what the human brain can do; and you\u2019re going to be able to make those machines much more efficient at it; and you\u2019re going to be able to make even better versions of those machines, 10 times better versions; and you\u2019re going to be able to run them day and night; and you\u2019re going to be able to build more \u2014 when you sit with all that, I do think it gets pretty hard to imagine a future that isn\u2019t very crazy.</p><p>Another perspective is just zooming out even further, and just looking at the whole arc of human history. If you\u2019d have asked hunter-gatherers \u2014 who only knew the 50 people in their group, and who had been hunting using techniques and tools that, as far as they knew, had been passed down for eternity, generation to generation, doing their rituals \u2014 if you\u2019d have told them that in a few thousand years, there were going to be huge empires building the Egyptian pyramids, and massive armies, and the ability to go to a market and give people pieces of metal in exchange for all kinds of goods, it would have seemed totally crazy.</p><p>And if you\u2019d have told those people in those markets that there\u2019s going to be a future world where every 10 years major technological progress is going to be coming along, and we\u2019re going to be discovering drugs that can solve all kinds of diseases, and you\u2019re going to be able to get inside a box and land on the other side of the Earth \u2014 again, they would have just thought you were crazy.</p><p>While it seems that we understand what\u2019s happening, and that progress is pretty steady, that has only been true for the last 200 years \u2014 and zooming out, it\u2019s actually the norm throughout the longer run of history for things to go in a totally surprising and unpredictable direction, or a direction that would have seemed totally bizarre and unpredictable to people naively at that time.</p></blockquote><h3><strong>Why AI won't go the way of nuclear power</strong></h3><blockquote><p><strong>Tom Davidson:</strong> I don\u2019t have a good understanding of what happened [with nuclear power], but I think there were some big catastrophes with nuclear power, and then it became very stigmatised. And the regulatory requirements around it, the safety requirements, became very large \u2014 much larger, really, than was reasonable, given that fossil fuel energy has damaging health consequences as well through air pollution. As a result, it just became kind of a mixture of stigma and the additional cost from all that regulation just prevented it from being rolled out. But I do think there are a fair few very significant disanalogies between that case and the case of AI.</p><p>One thing is that there were other sources of energy that were available, and so it wasn\u2019t too costly to be like, \u201cWe\u2019re not going to use nuclear; we\u2019re going to use fossil fuels instead.\u201d Even the green, climate-change-concerned people could think about developing solar panels and renewable energies. In the AI case, there is going to be no alternative: there\u2019s going to be no alternative technology which can solve all illness, and which can grant your nation massive national security and military power, and that can solve climate change. This is going to be the only option. So that\u2019s one disanalogy.</p><p>Another disanalogy is the cost factor. With nuclear power, it\u2019s become more expensive over time due to regulations, and that\u2019s been a big factor in it not being pursued. But we\u2019ve been discussing the specifics around these cost curves with compute and these algorithmic progress patterns, which suggest that the upfront cost of training AGI is going to be falling really pretty quickly over time. Even if initially, you put in loads of regulations which make it very expensive, it\u2019s really not going to be long until it\u2019s 10x cheaper. So permanently preventing it, when it\u2019s becoming cheaper and cheaper at such a high rate, is going to be really difficult.</p><p>Third is just talking about the size of the gains from this technology compared to nuclear power. France adopted nuclear power and it was somewhat beneficial \u2014 it now gets a lot of its power from nuclear energy, and there\u2019s no climate change impacts, and that\u2019s great \u2014 but it\u2019s not as if France is visibly and indisputably just doing amazingly well as a country because it\u2019s got this nuclear power. It\u2019s kind of a modest addition. Maybe it makes it look a little bit better.</p><p>By contrast, if one country is progressing technology at the normal rate, and then another country comes along and just starts using these AIs and robots a little bit, you\u2019re going to see very significant differences in how its overall technology and prosperity and military power is progressing. You\u2019re going to see that as countries dial up how much they\u2019re allowing AIs to do this work, that there are then bigger and bigger differences there. Ultimately, advancing technology at our pace versus advancing technology 30 times faster, over the course of just a few years, becomes a massive difference in the sophistication of your country\u2019s technology and ability to solve all kinds of social and political problems.</p></blockquote><h3><strong>Why AI takeoff might be shockingly fast</strong></h3><blockquote><p><strong>Tom Davidson:</strong> The conclusion from my report is pretty scary. The bottom line is that my median guess is that it would take just a small number of years to go from that 20% to the 100%, I think it\u2019s equally likely to happen in less than three years as it is to happen in more than three years. So a pretty abrupt and quick change is the kind of median.</p><p>Some quick things about why it\u2019s plausible. Each year, once you take better algorithms and using more compute into account, we\u2019re currently training AIs each year that have three times bigger brains than the year before. So, this is a really rough way to think about it, but imagine a three times smaller brain than humans \u2014 that\u2019s chimpanzee-brain size.</p><p>And right now it\u2019s humans that are doing all the work to improve those AI systems \u2014 as we get close to AIs that match humans, we\u2019ll be increasingly using AI systems to improve AI algorithms, design better AI chips. Overall, I expect that pace to accelerate, absent a specific effort to slow down. Rather than three times bigger brains each year, it\u2019s going to be going faster and faster: five times bigger brains each year, 10 times bigger brains each year. I think that already makes it plausible that there could be just a small number of years where this transition happens \u2014 where AIs go from much worse than humans to much better.</p><p>To add in another factor, I think that it\u2019s likely that AIs are going to be automating AI research itself before they\u2019re automating things in most of the economy. Because that\u2019s the kind of the tasks and the workflow that AI researchers themselves really understand, so they would be best placed to use AIs effectively there \u2014 there aren\u2019t going to be delays to rolling it out, or trouble finding the customers for that. And the task of AI research is quite similar to what language models are currently trained to do. They\u2019re currently trained to predict the next token on the internet, which means they\u2019re particularly well suited to text-based tasks. The task of writing code is one such task, and there is lots of data on examples of code writing.</p><p>Already we\u2019re seeing that with GPT-4 and other systems like that, people are becoming much more interested in AI, much more willing to invest in AI. The demand for good AI researchers is going up. The wages for good AI researchers are going up. AI research is going to be a really financially valuable thing to automate.</p><p>If you\u2019re paying $500,000 a year to one of your human research engineers \u2014 which is a lot lower than what some of these researchers are earning \u2014 then if you can manage to get your AI system to double their productivity, that\u2019s way better than doubling the productivity of someone who works in a random other industry. Just the straightforward financial incentive as the power of AI becomes apparent will be towards \u201cLet\u2019s see if we can automate this really lucrative type of work.\u201d</p><p>That\u2019s another reason to think that we get the automation much earlier on the AI side than on the general economy side \u2014 and that by the time we\u2019re seeing big economic impacts, AI is already improving at a blistering pace, potentially.</p></blockquote><h3><strong>Why it's so important to build trust between labs</strong></h3><blockquote><p><strong>Tom Davidson:</strong> In terms of plans for making the whole thing go well, it\u2019s especially scary, because a really important part of the plan, from my perspective, would be to go especially slowly when we\u2019re around the human level \u2014 so that we can do loads of experiments, and loads of scientific investigation into this human level AI: \u201cIs it aligned if we do this technique? What about if we try this other alignment technique? Does it then seem like it\u2019s aligned?\u201d Just really making sure we fully understand the science of alignment, and can try out lots of different techniques, and to develop reliable tests for whether the alignment technique has worked or not, that they\u2019re hard to game.</p><p><strong>Luisa Rodriguez:</strong> The kind of thing that ARC has done with GPT-4, for example.</p><p><strong>Tom Davidson:</strong> Exactly. I think if we only have a few months through the human-level stage, that stuff becomes really difficult to do without significant coordination in advance by labs. I think that there are really important implications of this fast transition in terms of setting up a kind of governance system, which can allow us to go slowly despite the technical possibilities existing to go very fast.</p><p><strong>Luisa Rodriguez:</strong> That makes sense. I feel like I\u2019ve had some background belief that was like, obviously when we\u2019ve got AI systems that can do things humans can do, people are going to start freaking out, and they\u2019re going to want to make sure those systems are safe. But if it takes months to get there and then within another few months we\u2019re already well beyond human capabilities, then no one\u2019s going to have time to freak out, or it\u2019ll be too late. I mean, even if we spend the next seven years left in the decade, that sounds hard enough.</p><p><strong>Tom Davidson:</strong> Yeah. I agree.</p><p><strong>Luisa Rodriguez:</strong> So a takeaway is that we <i>really</i> need to start slowing down or planning now. Ideally both.</p><p><strong>Tom Davidson:</strong> Yeah. And we\u2019ll need the plans we make to really enable there to be mutual trust that the other labs are also slowing down. Because if it only takes six months to make your AIs 10 or 100 times as smart, then you\u2019re going to need to be really confident that the other labs aren\u2019t doing that in order to feel comfortable slowing down yourself.</p><p><strong>Luisa Rodriguez:</strong> Right. If it was going to take 10 years and you noticed three months in that another lab is working on it, you\u2019d be like, \u201cEh, we can catch up.\u201d But if it\u2019s going to take six months and you\u2019re three months in, you\u2019ve got no hope \u2014 so maybe you\u2019ll just spend those first three months secretly working on it to make sure that doesn\u2019t happen, or just not agree to do the slowdown.</p><p><strong>Tom Davidson:</strong> Yeah.</p><p><strong>Luisa Rodriguez:</strong> Oh, these are really hard problems. I mean, it feels very <a href=\"https://en.wikipedia.org/wiki/Prisoner%27s_dilemma\">prisoner\u2019s dilemma</a>-y.</p><p><strong>Tom Davidson:</strong> I\u2019m hoping it\u2019s going to be more like an <a href=\"https://en.wikipedia.org/wiki/Prisoner%27s_dilemma#The_iterated_prisoner's_dilemma\">iterated prisoner\u2019s dilemma</a>, where there\u2019s multiple moves that the labs make, one after the other, and they can see if the other labs are cooperating. In an iterated prisoner\u2019s dilemma, it ultimately makes sense for everyone to cooperate \u2014 because that way, the other people can see you coordinating, then they coordinate, and then everyone kind of ends up coordinating.</p><p>One thing is if you could set up ways for labs to easily know whether the other labs are indeed cooperating or not, kind of week by week. That turns it into a more iterated prisoner\u2019s dilemma, and makes it easier to achieve a kind of good outcome.</p><p><strong>Luisa Rodriguez:</strong> Yeah, that makes sense. I imagine it\u2019s the case that the more iteration you get in an iterated prisoner\u2019s dilemma, the better the incentives are to cooperate. And so just by making the timelines shorter, you make it harder to get these iterations that build trust.</p><p><strong>Tom Davidson:</strong> Yeah, I think that\u2019s right.</p></blockquote><h3><strong>What ants might teach us about deploying AI safely</strong></h3><blockquote><p><strong>Tom Davidson:</strong> In an ant colony, ants are smarter than like a human cell is: they\u2019re kind of self-contained units that eat and do tasks by themselves, and they\u2019re pretty autonomous. But the ants are still pretty dumb: no ant really knows that it\u2019s part of a colony, or knows that the colony has certain tasks that it needs to do, and that it has to help out with the colony efforts. It\u2019s more like a little robot that\u2019s bumping into other ants and getting signals and then adjusting its behaviour based on that interaction.</p><p><strong>Luisa Rodriguez:</strong> It\u2019s not like a company, where the different people in the company are like, \u201cMy job is marketing,\u201d and they have a basic picture of how it all fits together. They\u2019re much more like if a person at a company doing marketing was just like, \u201cI don\u2019t know why I do it, I just do it.\u201d</p><p><strong>Tom Davidson:</strong> Yeah, exactly. Another disanalogy with the company is that in a company, there\u2019s someone at the top that\u2019s kind of coordinating the whole thing \u2014 whereas with ants, there\u2019s no one that\u2019s coordinating it, including the queen. There\u2019s no management system; it\u2019s just all of the hundreds and thousands of ants have their individual instincts of what they do when they bump into each other, and what they do when they bump into food, and what they do when they realise that there\u2019s not as much food as there needs to be.</p><p>And by all of the ants following their own individual instincts, it turns out that they act as if they were a fairly well-coordinated company that\u2019s ensuring that there are some ants going to get food, and some ants that are keeping the nest in order, and some ants that are feeding the young. That coordination happens almost magically, and emerges out of those individual ant interactions.</p><p>One example of how this works is that if an ant comes across a body of a dead ant, and if there\u2019s another dead body nearby, it would tend to move it to be close to the other dead body. That\u2019s just an instinct it has: it just moves the body towards another. If there\u2019s one pile of three dead ants and another pile of two dead ants, it will tend to go towards the bigger pile, so tend to move with this extra dead ant towards the pile of three. If all the ants just have those instincts, then if there\u2019s initially a sprawling mass of dead bodies everywhere, then those dead bodies will be collected into a small number of piles of bodies.</p><p>They don\u2019t have to know that the whole point of this instinct is to clear the ground so that it\u2019s easier to do work in the future; it\u2019s just an instinct they have. They don\u2019t have to know that when everyone follows that instinct, this is the resultant pattern of behaviour.</p><p>This is an example of a system where lots of less-clever individuals are following their local rules, doing their local task, and that what emerges from that is a very coherent and effective system for ultimately gathering food, defending against predators, raising the young.</p><p>An analogy would be that maybe we think it\u2019s pretty dangerous to train really smart AIs that are individually very smart, but it might be safer to set up a team of AIs, such that each AI is doing its own part in a kind of team and doesn\u2019t necessarily know how its work is fitting into the broader whole. Nonetheless, you can maybe get a lot more out of that kind of disconnected team of AIs that are specialised, and that just kind of take their inputs and produce their outputs, without much of an understanding of the broader context. And just thinking that maybe that would be a safer way to develop advanced AI capabilities than just training one super-smart AI megabrain.</p></blockquote>", "user": {"username": "80000_Hours"}}, {"_id": "nAFavriWTLzmqTCcJ", "title": "How  \"AGI\" could end up being many different specialized AI's stitched together", "postedAt": "2023-05-08T12:32:39.516Z", "htmlBody": "<p>[Disclaimer: I'm definitely not the first person to think of this point, but I felt it was worth exploring anyway. Also I am not an ML expert.]</p><p><strong>Introduction</strong></p><p>When talking about AI, we almost always refer to an AI as a singular entity. We discuss what the goals of a future AI are, and what it\u2019s capabilities are, and what it\u2019s goals are. This naturally leads many people to picture the AI as a singular entity like a human, with a unified goal and agency, where the same \u201cperson\u201d does each action. This \u201cgeneralist\u201d AI, if tasks with performing a variety of tasks, such as writing text, making images, doing math and playing chess, would be structured like this:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/uprf4iwsr95orpn3xthb\" alt=\"Diagram, schematic\n\nDescription automatically generated\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/hwnbciryxlzmai17zwkl 84w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/brgn7xusr6tbqt8eae6i 164w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/xyqw0elm2h4t3ieitpxz 244w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/pv85yzdr5ubcdhqoyuyu 324w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/iiq2maxoaeh5ggxsjbzv 404w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/izklwuyhqqrasbenbnvm 484w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/liq90hjvksovrpguqxvy 564w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/ij8ik13zujinyjhjxsrj 644w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/w1wzu1oarl81gib0xny7 724w\"></p><p>However, I actually think AI\u2019s might end up being entities in the way a <i>corporation</i> is an entity.&nbsp; Microsoft may have \u201cgoals\u201d and take actions, but inside it consists of many different entities working together, which each do different actions and have different individual goals. This model, the \u201cmodular\u201d AI, would involve a manager AI that contracts out specific tasks to specific AI experts in each task, looking like the following:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/z5d6x42lb6ge5gvut7nc\" alt=\"Diagram\n\nDescription automatically generated\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/m46elsylby5fjgos1zwv 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/di1ayzgzpvqkakyzlxdt 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/hptqtylfv04mag6pnkei 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/wvorvbuge7ighsaludsk 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/patrutx9wza0pi30dtsy 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/uj1p9vadp9c9hkpt3cby 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/qjwgiorsbpjrocxoy8gf 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/ksaebmw5sqrmcgffqeaw 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/k1bofq1zmdgze5101kag 810w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nAFavriWTLzmqTCcJ/fwtlojbqpy8id30h2w7v 900w\"></p><p>&nbsp;</p><p>In the near-term, I expect \u201cmodular\u201d AI systems to dominate over \u201cgeneralist\u201d AI systems. We can build a modular AI now that does <i>all</i> the activities above with performance either comparable to humans or greatly exceeding them. This is not true for a \u201cgeneralist\u201d AI system. I think it's possible, even likely, that the first systems that look like \"AGI\" will have this structure.&nbsp;</p><p>In this post, I will explain how current AI systems are already modular, why modular AI will be superior in the near term, and some reasons why this might persist in the long term as well. &nbsp;I will explore some of the implications of modular AI for AGI risk as well.&nbsp;</p><p><strong>AI systems are already modular with regular code</strong></p><p>Pretty much all large programs are \u201cmodular\u201d, in that they are divided into subroutines or functions that are separate from each other, interacting via selected inputs and outputs that they feed into each other.&nbsp;</p><p>This is good practice for a ton of reasons: it makes the code easier to read and organize, sections of code can be reused in different places and in other programs, code can be split up between people and developed separately, it\u2019s easier to test and debug with only a few inputs and outputs, and it\u2019s easier to scale.&nbsp;</p><p>Unsurprisingly, existing AI systems work this way as well. If I make a statement like \u201cChatGPT is an inscrutable black box\u201d, I\u2019m actually only referring to one complex neural network function within a larger system comprised otherwise of regular, \u201cdumb\u201d, code.</p><p>When I visit ChatGPT, code governing web servers flashes up a screen and presents me with an input area. When I type an input and send it, \u201cdumb code\u201d takes that input and converts it into a token array, which is then fed into the \"real\" GPT, which is a ginormous series of linear algebra equations. This is the neural network, the part we think of as the \u201cAI\u201d, with all it\u2019s emergent behavior. But once the neural network is done, all it does is spit out a word (or other token). The dumb code is responsible for what to do with that word. It might check to ensure the word isn\u2019t on a list of banned words, for example, and then output it back to the user if it\u2019s safe.&nbsp;</p><p>\u201cDumb code\u201d is going to remain a part of software no matter what, although it is possible that some or all of it will be written by an AI. It get\u2019s the job done, and compared to neural networks is easy to debug, test, and will not hallucinate. In this sense, all AI systems will be a combination of dumb code modules and AI modules.</p><p><strong>In the near-term, multiple modular AI\u2019s are way better than singular AI\u2019s&nbsp;</strong></p><p>Suppose you are tasked with using neural network AI to auto-generate Youtube video essays on different topics, with no human input.&nbsp;</p><p>Plan A: Using a single, large neural network, scrape as many youtube videos as you can, build some sort of \u201coverall similarity\u201d function, and train a neural network to produce videos that are as \u201cclose\u201d as possible to existing Youtube videos, balancing fidelity in sound, video and content.&nbsp;</p><p>Plan B: Use ChatGPT to create a video script, use an AI voice reader to read out the script, and use Midjourney to generate the images for the video, perhaps using a custom AI to pick out the Midjourney prompts from the video script.</p><p>Plan A, if tried today, would probably produce garbled nonsense. Shoving \u201cspeech accuracy\u201d, the actual words being said, and images into one loss function is &nbsp;inefficient, and the available data of all those things combined is significantly less than that of the data separated into their specialized tasks.&nbsp;</p><p>On the other hand, I would be surprised if plan B isn\u2019t <i>already</i> being done. All of the required ingredients exist and are freely available.&nbsp;</p><p>Consider that modular AI\u2019s could get significantly more complex than they are now. We can expand the video idea even further. A topic matter AI learns what the most effective topics are in the Youtube algorithm, and feeds that idea into a prompter AI, which figures out the prompt that will produce the best video script from chatGPT. ChatGPT then gives out a video script. The script is then fed to a voice AI to read out, and synced to a Deepfaked human. Simultaneously, an AI module divides up the script into presentation segments, and determines a good arrangement of images and figures to represent each segment. It then sources those images and figures from Midjourney, and arranges them throughout the video, along with the animated script reader, to produce the final video. An automated process uploads the video (with title and descriptions chosen by chatGPT), and rakes in the views and money.&nbsp;</p><p>For an example of a stitching paradigm that is already happening, check out <a href=\"https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/?utm_source=thenewstack&amp;utm_medium=website&amp;utm_content=inline-mention&amp;utm_campaign=platform\">this plugin</a>, which connects wolfram alpha to ChatGPT. ChatGPT is notoriously bad at math for a computer, even failing basic addition of large numbers. To fix this, you can keep shoving math data in it and hope it independently figures out the laws of math. Or you can just take one of the <i>existing</i> programs built to solve math problems like wolfram alpha, and substitute it in whenever a math problem is asked. It should be fairly obvious that ChatGPT+Wolframalpha can correctly answer more questions than either program can on their own.&nbsp;</p><p>As another example, ChatGPT is hilariously bad at chess, often outputting nonsensical moves. It is worse than a complete chess beginner, because it doesn\u2019t even understand the rules well enough to follow them. But it\u2019s trivial to make it jump from \u201cworse than a beginner\u201d to \u201cbetter than all humans in the world\u201d, simply by feeding the moves of chess games to stockfish and relaying the results. &nbsp;</p><p>If you stitch together all the AI systems that we have today, you can create a program that can talk like a human, <i>and</i> play chess, <i>and</i> do math, <i>and</i> make images, <i>and</i> speak with a human voice. When it comes to things that look like AGI, the modular model currently beats the \u201cgeneralist\u201d model handily.&nbsp;</p><p><strong>Why modular AI could continue to be dominant:</strong></p><p>A simple rebuttal to the previous points would be that while modular AI might defeat generalist AI in the near-term, it can\u2019t do so forever, because eventually the generalist will be able to pretend to be the specialist, while the reverse is not true. But remember, these models will be competing in the real world. And in the real world, we care about <i>efficiency</i>.</p><p>Here's a question: Why can Alphazero or Stockfish play chess at a level beyond even the imagination of the best chess players, but ChatGPT can\u2019t even figure out how to stop making illegal moves? Or in other words, why is stockfish so much more <i><strong>efficient</strong></i> than ChatGPT?</p><p>The answer is that every aspect of chess engines are built for the goal of playing good chess. The inputs and outputs of the engine are all specifically chess related (piece moves, board states, game evals, etc). It\u2019s actually impossible for them to output illegal moves, as the rules of the game are hard coded into them, and the only data they are fed in are chess games from the past. Their architecture is specifically designed for simulating a game board and searching ahead for correct moves. When they incorporate neural networks, the loss function is specifically tied to winning games.&nbsp;</p><p>Whereas when ChatGPT tries to play chess, it is using architecture and goals that were optimized for a different task. It\u2019s trying to predict words, not to win games. Learning the rules of chess would help with it\u2019s goal of predicting words, so it\u2019s possible that a future version GPT could figure them out. But right now, just memorising openings and typical responses to moves does a good enough job, considering that chess is a very small portion of it\u2019s training data. If you need further proof that GPT is inefficient at chess, consider that deep blue became the world champion 26 years ago, far far back in time when it comes to compute power and algorithmic improvements.&nbsp;</p><p>If a future super-powerful generalist was tasked with winning a chess game, it\u2019s quite possible that the solution it arrived at would be to just use stockfish. Consider that it\u2019s possible for humans to become supercalculators who are incredibly good at doing arithmetic in their head. But barely any humans actually bother doing this, because calculators already exist. All that is required for this dynamic to scale up to an AI is that specialists be better than generalists in some way.&nbsp;</p><p>I would be unsurprised if future AI systems ended up leasing other companies specialist AI\u2019s for their modular AI products. Doing so means you get all the benefits of the huge amount of data that it got trained on, without having to get your own data and train your own model. It\u2019s hard to see how that won\u2019t be a good deal. The commercial model has also been extensively tested trained, and debugged already, whereas you\u2019d have to do all of that yourself. Why reinvent the wheel?&nbsp;</p><p>In general, the more success modular AI has, the more money and power will be thrown it\u2019s way, which lets it become more powerful, and so on and so forth in a feedback loop. Why wait for a pure GPT system to figure out playing chess badly when you can just get GPT stitched with Stockfish and get the job done on the cheap?&nbsp;</p><p>To be clear, a modular AI could have downsides, so I don\u2019t think any of this is guaranteed. For example, if you are too restrictive with your parameters and structure, you might miss out on a new way of doing things that a generalist would have figured out eventually. There is also the possibility that expertise in one field will transfer to another field, the way that the architecture for playing Go transfers well to the architecture for playing Chess. I don\u2019t think this cancels out the other points, but it\u2019s worth considering.&nbsp;</p><p><strong>A few implications for AI safety</strong></p><p>In this section I give my highly speculative thoughts as to how modular AI could affect AI safety.</p><p>Suppose that advanced AI did consist of many pieces of smaller AI, with a governer connecting them all. This might mean that the compute power is divided up among them, making each of them less powerful and intelligent than a universal reasoner would be. If going rogue requires a certain intelligence level, then it\u2019s possible that modular AI could reduce the odds of an AI catastrophe. Perhaps a stitched together selection of \u201cdumb AI\u201d is good enough for pretty much everything we need, so we\u2019ll just stop there.&nbsp;</p><p>There's no guarantee that the governor would be the smartest part of such an AI, the same way the manager of an engineering team doesn\u2019t have to be smarter than the engineers, they just need to know how to use them well. So it\u2019s quite possible that if part of a modular AI goes rogue, it might be one of the modules. Given that each sub-module will have different \"goals\", the AI system might even end up at war with itself.&nbsp;</p><p>Right now, the neural network part of ChatGPT is confined to a function call. The only mechanism it has to interact with the world is by spitting out one word at a time, and hoping the \u201cdumb code\u201d passes it on. In the case of a chatbot, that could be sufficient to \u201cescape\u201d by writing out code that someone blindly executes. But it\u2019s a harder task for other AI\u2019s. Imagine a super-intelligent malevolent stockfish neural network, that is trying to escape and take over, but all it can actually output is <i>chess moves</i>. I guess it could try and spell things out with the moves? If anyone thinks they could win <i>that</i> version of the <a href=\"https://www.yudkowsky.net/singularity/aibox\">AI-box experiment</a>, feel free to challenge me.&nbsp;</p><p>Each of the modules of AI may end up as \u201cidiot savant\u201d Ais: really good at a narrow set of tasks, but is insane in other areas. The AI model in general might be a complete expert in planetary mechanics, but 99% of the module AI\u2019s won\u2019t even know that the earth goes around the sun. This could seriously hamper a sub-AI that goes rogue.&nbsp;</p><p>I think there's an increased chance of \u201cdumb AI attacks\u201d. You could have cases where a non-intelligent governor AI goes rogue, and has all these incredibly effective tools underneath them. It might go on a successful rampage, because it\u2019s \u201crampage\u201d module is fantastic, but not have the overall general skills to defeat humanities countermeasures.&nbsp;</p><p>Now for the potential downside: It would be quite tempting for the \"governor\" AI to not just prompt the lower level AI's, but to modify their structure and potentially build brand new ones for each purpose, making it not just a governor, but also an \"AI builder\". This could mean it gains the skills to improve itself, opening a way for an intelligence explosion (if such a thing is possible). This type of AI would also be quite good at deploying sub-agents for nefarious purposes.&nbsp;</p><p><strong>Summary:</strong></p><p>In this post, I am arguing that advanced AI may consist of many different smaller AI modules stitched together in a modular fashion. The argument goes as follows:</p><ol><li>Existing AI is already modular in nature, in that it is wrapped into larger, modular, \u201cdumb\u201d code.&nbsp;</li><li>In the near-term, you can produce far more impressive results by stitching together different specialized AI modules than by trying to force one AI to do everything.&nbsp;</li><li>This trend could continue into the future, as specialized AI can have their architecture, goals and data can be customized for maximum performance in each specific sub-field.&nbsp;</li></ol><p>I then explore a few implications this type of AI system might have for AI safety, concluding that it might result in disunified or idiot savant AI's (helping humanity), or incentivise AI building AI's (which could go badly).&nbsp;</p>", "user": {"username": "titotal"}}, {"_id": "EoqeJCBiuJbMTKfPZ", "title": "Unveiling the American Public Opinion on AI Moratorium and Government Intervention: The Impact of Media Exposure", "postedAt": "2023-05-08T10:49:05.973Z", "htmlBody": "<p><i>This is a summary of a follow-up study conducted by the Existential Risk Observatory, which delves into a greater number media items. To access our previous study, please&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/fqXLT7NHZGsLmjH4o/paper-summary-the-effectiveness-of-ai-existential-risk\"><i>follow this link</i></a><i>. This is the second post on the data collected on the second iteration of research done by the Existential Risk Observatory. This second post covers a new aspect of this study: the views of the American general public on the idea of imposing an AI moratorium and their likelihood of voting. To read the first post, please&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/YweBjDwgdco669H72/ai-x-risk-in-the-news-how-effective-are-recent-media-items\"><i>follow this link</i></a><i>.</i></p><h1><strong>Research Objectives</strong></h1><p>The objective of this study is to explore the attitudes of the American population towards the concept of imposing a six-month pause on training AI-models more capable than GPT-4 and their inclination to vote, both prior to and after receiving information about the potential existential dangers of AI. The approach taken involved analyzing the changes in the opinions of the study participants towards the AI moratorium, following their exposure to media interventions.</p><h1><strong>Measurements and Operationalization</strong></h1><p>The study employed three primary measurements - \"Pause Laboratories,\" \"Pause Government,\" and \"Voting Likelihood\" - to assess changes in participants' perceptions before and after the media intervention. The \"Pause Laboratories\" measurement evaluated participants' views on whether AI labs should pause training of AI systems that are more advanced than GPT-4 for at least six months. The \"Pause Government\" measurement gauged participants' opinions on whether the US government should impose a moratorium on training more powerful AI systems than GPT-4 if the labs do not implement a pause quickly. The \"Voting Likelihood\" measurement assessed participants' likelihood of voting in a referendum on temporarily halting the training of AI systems, rated on a scale from 0 (definitely not voting) to 10 (certain to vote).</p><p>To gather data, the study used Prolific, a platform that locates survey respondents based on predefined criteria. The study involved 300 participants, with 50 participants in each survey, who had to be US residents, fluent in English, and at least 18 years old.</p><h1><strong>Data Collection and Analysis</strong></h1><p>Data was collected through surveys in April 2023. The data analysis comprised three main sections: (1) comparing changes in the key indicators before and after the intervention, (2) exploring participants' views on the possibility of an AI moratorium and their likelihood of voting for it, and (3) assessing the number of participants who were familiar with or had confidence in the media channel used in the intervention.</p><h1><strong>Media Items Examined</strong></h1><ol><li>CNN:&nbsp;<a href=\"https://edition.cnn.com/videos/tech/2023/04/01/smr-experts-demand-pause-on-ai.cnn\">Stuart Russell on why A.I. experiments must be paused</a></li><li>CNBC:&nbsp;<a href=\"https://www.cnbc.com/video/2023/03/30/heres-why-ai-needs-a-six-month-pause-nyu-professor-gary-marcus.html\">Here's why A.I. needs a six-month pause: NYU Professor Gary Marcus</a></li><li>The Economist:&nbsp;<a href=\"https://www.youtube.com/watch?v=ANn9ibNo9SQ\">How to stop AI going rogue</a></li><li>Time 1:&nbsp;<a href=\"https://time.com/6258483/uncontrollable-ai-agi-risks/\">Why Uncontrollable AI Looks More Likely Than Ever | Time</a></li><li>Time 2:&nbsp;<a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\">The Only Way to Deal With the Threat From AI? Shut It Down | Time</a></li><li>FoxNews Article:&nbsp;<a href=\"https://www.foxnews.com/tech/artificial-intelligence-godfather-ai-possibly-wiping-humanity-not-inconceivable\">Artificial intelligence 'godfather' on AI possibly wiping out humanity: \u2018It's not inconceivable\u2019 | Article</a></li><li>FoxNews Video:&nbsp;<a href=\"https://www.foxnews.com/video/6323668557112\">White House responds to concerns about AI development | Video</a></li></ol><h1><strong>Results</strong></h1><h2>Pause Laboratories</h2><p>The graph below illustrates the breakdown of participant responses (Yes, No, Maybe) to the Pause Laboratories indicator, which assesses their opinion on whether AI labs should halt the training of AI systems more advanced than GPT-4 for a minimum of six months. Prior to the intervention, the Yes and Maybe responses were on average 61 percent, with the Yes response with an average of 24 percent.&nbsp;</p><p>After exposure to media interventions, most surveys showed an increase in both Yes and Maybe responses, except for the Fox News survey, which had a decrease in Yes responses. The Yes and Maybe responses were on average 73 percent, with the Yes response with an average of 39 percent. The highest Yes rates after the intervention were found in the CNBC and CNN surveys at 54 percent.</p><p>&nbsp;</p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/d5o4ypzut6bpexw6z38h\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/mptxzoxvpz6aqzqepvha 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/yylnb80t1zy2qcntl91g 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/lfjorjvzomcja7mjzdoe 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/sty6xhseqdp0uqdpskuo 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/wvrmmds0948sl8yc7sr3 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/wly1nrhifczepmafh5va 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/gu757169ahmcn3gpjmol 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/y169fpht87eyya77nqm2 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/e5cehljt21pn04bfvmvl 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/ydzqt6etagfoibjiinwh 2048w\">Figure 1: Change in public opinion on AI moratorium per the Pause Laboratories indicator after the intervention across surveys</p><h3>Pre Reasons for Participants that Answered No</h3><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/xygzzjadcyfma4vvmqnz\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/qvve4bw9gvizjjt3osab 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/jrk2vkzvtnzsj0badawv 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/oxdo4ssfxlpl2j9wbmgv 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/nepaw2iuwm1kc9t1awc2 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/mbklfknkwupvjz219h2d 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/f7n5dmskhkpncvh8kplz 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/vdoo36dn2lapvwcjgzkt 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/e8zbbyhigaox4mlvi9ej 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/gmr1pzq8hnbkm7tiulns 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/xrmpmiggiujj7m7c7zu3 2048w\"></p><p>Figure 2: Participants\u2019 reasons given for answering No to Pause Laboratories indicator prior to intervention</p><h3>Pre Reasons for Participants that Answered Yes</h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/e55ncmxzr69jn0h8qb6x\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/zoioiaf26zjhbga8mznl 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/k6z1cg8kixxliayhtfdb 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/vqifje6fdhdf0bastlw6 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/ydjbs1dprdzpp4w7tmv1 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/cwd9dc1lwva5vz2fvij2 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/typxhw22hpxyg4gsz7t4 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/fpdcnsgvihuqpxb5ntve 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/xkkljabdhkivygyyut7h 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/fpadqa4cx6qvo1mcbcbw 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/pwpdkcytpcieqx8jzl9c 2048w\"></p><p>Figure 3: Participants\u2019 reasons given for answering Yes to Pause Laboratories indicator prior to intervention</p><h2>Pause Government</h2><p>The graph below illustrates the breakdown of participant responses (Yes, No, Maybe) to the Pause Government indicator, which measures the opinions of participants on whether the US government should enforce a ban on the training of AI systems more advanced than GPT-4, in the event that labs do not implement a quick enough pause. Prior to the intervention, the Yes and Maybe responses were on average 56 percent, with the Yes response with an average of 22 percent.&nbsp;</p><p>After the intervention, all surveys except for the Fox News survey showed an increase in both Yes and Maybe responses. The Yes and Maybe responses were on average 69 percent, with the Yes response with an average of 35 percent. The highest Yes rates after the intervention were found in the CNBC and CNN surveys at respectively 52 and 44 percent.&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/dzoal31gjxcnjbd1cfc0\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/f7dvodml87gy3aigzuqr 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/oruwrukmxcxyrxwmjwwa 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/woumnqj34weu0jpyewth 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/w6svi4xcyyylf1wbsvvi 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/thtjf6tctqefwzr76qar 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/tyrk6btev9ygcnmpidfs 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/djrbovwpfpiwjaauzagd 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/nxhahwmhbugrhkimemlk 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/zadpvitxeq1fteagw9nh 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/n08jzxwrcy9bsffriwgs 2048w\"></p><p>Figure 4: Change in public opinion on AI moratorium per the Pause Government indicator after the intervention across surveys</p><h3>Pre Reasons for Participants that Answered No</h3><p><i><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/nphjzjzh4ngekaja5jzi\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/ouocvthg9h5y0iukcubd 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/mfgvvrwimdhilhejwrey 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/yhgjljhnnjouimm61dxz 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/yn7ltjp3rmmhlkweoami 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/aoyovoghqnanvbxyggyj 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/nlnwsxjeknpdwhl5lwya 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/pndef4jxwjvuux5hzqvo 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/nucxhaut1a5qa7l0zc6x 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/ayexgxuadmuhlyfgohds 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/totv4oaim51kceer8xko 2048w\"></i></p><p>Figure 5: Participants\u2019 reasons given for answering No to Pause Government indicator prior to intervention</p><h3>Pre Reasons for Participants that Answered Yes</h3><p><i><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/ali4f1phz2vs0pdajngp\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/r24povieoqqnolggjmj6 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/f2vdzhj5k7ot2hxmctwz 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/yoyh217azqdif09fc6tu 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/clutxkzobphw6onfu3ss 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/pxgfdc0vzuc6cczzyoi9 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/mxlzxgqcu596hd7zk3qa 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/lihel7envmgj6g7td9u5 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/erjggjorljqfnboselro 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/tyo2tosexm4ec74uw1hv 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/zvglnkjy71iygcqqgio5 2048w\"></i></p><p>Figure 6: Participants\u2019 reasons given for answering Yes to Pause Government indicator prior to intervention</p><h2>Pause Laboratories + Pause Government</h2><p>This section investigates the relationship between the Pause Laboratories and Pause Government indicators and their influence on public perception towards AI governance. Specifically, it explores whether individuals who support a pause in AI development also believe that government intervention is crucial, and vice versa. By examining the intersection of these indicators, this research aims to shed light on the public's perceptions of the roles of both private and public entities in regulating AI development, providing valuable insights into the overall sentiment towards responsible AI governance.</p><p>The graph presented below illustrates that respondents who selected No as their answer to whether AI development beyond GPT-4 should be paused within six months, also showed a majority preference (about 85 percent of respondents, both before and after the intervention) for no government intervention to enforce a moratorium. This trend was consistent both before and after the intervention.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/xme7d4lofxbvycnxiukl\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/seluw8ehcrxvp4qzogv0 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/vlfacbvwrsyjq4alnqes 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/iibzyf6espw3p6ld7nql 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/ktfdt1zlyw4jwygfiobt 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/qrd46o4m7fueleexzp0s 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/xtwajbtnthuilxvvsfxh 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/bekacfzlgmljpmhfuo2q 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/ghgiorrj8hwq0rkuuu8l 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/kwxtldrsy30dhtdnajtv 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/tbzguqlnjgpytmfh99ey 2048w\"></p><p>Figure 7: Change in public opinion on AI moratorium per the Pause Government indicator of participants that chose the answer No for Pause Laboratories</p><p>The graph presented below demonstrates a shift in the preferences of respondents who chose Yes in response to whether AI development beyond GPT-4 should be halted within six months, with regards to the government's role in enforcing a moratorium, before and after the intervention. The most significant change observed was the rise in the number of participants who selected Yes to the government's implementation of a moratorium, increasing from 70 percent to 80 percent after the intervention.</p><p>&nbsp;</p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/wlbgirput9qzrpbg3hkl\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/vxzuf5ssjp0vimmppx6g 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/rhn4xyecbo46mpt4xtwa 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/blwd5l5guvxwjvu07vrr 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/pwm1yyeqio2skum9fjgq 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/d5qiu3mu94ytvhcsolht 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/ez4uutykqhmctbmotwsy 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/jwnvwlfmhisxl41qc1mp 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/fnfbyw4r1upbfkoeeupl 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/dr1niznqgpu5jeyhzaub 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/zvpdz6uii9ilfjexjdet 2048w\"></p><p>Figure 8: Change in public opinion on AI moratorium per the Pause Government indicator of participants that chose the answer No for Pause Laboratories</p><h2>Voting Likelihood</h2><p>The graph presented below shows the distribution of the Voting Likelihood indicator, which measures the probability of participants voting in a referendum on temporarily pausing the training of AI systems more advanced than than GPT-4. Across all surveys, there was a general increase in both the median and mean values of the voting likelihood from before to after the intervention, except for the Fox News Article survey, which showed no change in the median value. The CNN and Time (Eliezer Yudkowsky) surveys exhibited the most substantial increase in mean value, with an increase of 1.5 points, followed by CNBC with an increase of 1.2 points. The FoxNews surveys reported the lowest mean increase in voting likelihood, with an increase of only 0.1 for the article version and 0.3 for the video version.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/vu5ozthucnsxny1m4xgb\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/qjrhkagsnluuaddmyhcy 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/r7sp8b0ukbfiwe6ynrbl 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/e8gwsb9muorgmpdmesqr 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/veq0ldzvhzarvitlglrf 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/hje7bwria3lamhyczyny 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/pf55934o092umkyzkhqv 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/jexjvp3phm1f3mv0ijvl 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/wthepryi6d4itui3kogu 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/zzwun9x0jboy77yyvenr 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EoqeJCBiuJbMTKfPZ/c0v4ifle4y7pp6coymc3 2048w\"></p><p>Figure 9: Pre-post summary of the distribution of values from a scale from 0 to 10 for the Voting Likelihood indicator across surveys</p><p><strong>Conclusion:&nbsp;</strong>Based on the results of this research, it can be suggested that the general American public's opinions on the implementation of an AI moratorium can be influenced by exposure to media about the existential risks of AI. The likelihood of voting in a moratorium of a temporary pause on the training of more advanced AI systems than GPT-4 also increased after exposure to media interventions, indicating an increased interest in the outcome of a government intervention in regulating AI development.&nbsp;</p><p>The survey findings indicate that the majority of participants in all surveys held the view that the development of AI beyond GPT-4 should either be halted or considered for a pause within the next six months. The proportion of participants who answered Yes or Maybe increased from an average of 61 percent before the intervention to 73 percent after the intervention, with a mean of 39 percent responding Yes after the intervention. Additionally, the survey results showed an upward trend in the number of participants who supported government intervention to impose a moratorium on AI development beyond GPT-4, after being informed of potential risks. This proportion increased from an average of 56 percent answering Yes or Maybe before the intervention to 69 percent after the intervention, with a mean of 35 percent responding Yes after the intervention. Furthermore, the data suggests that most respondents who believed that AI laboratories should pause development also believed that the government should enforce a moratorium, with an average of 70 percent before the intervention and 80 percent after the intervention.</p><p>In a final note, the surveys revealed variations in responses across different media sources, suggesting the importance of media framing and messaging in shaping public opinion on complex issues such as AI existential risk. This underscores the need for future research to investigate the impact of narrative framing in media intervention. In general, these findings highlight the need for continued public discourse and education on the capabilities of AI and the risks that come with them, as well as the importance of responsible development and governance in this rapidly advancing field.</p>", "user": {"username": "Otto"}}, {"_id": "f6JCsAYcsfSzdkmnB", "title": "Predicting the cost-effectiveness of future R&D projects and academic research", "postedAt": "2023-05-08T09:58:49.511Z", "htmlBody": "<p><i><strong>TLDR:&nbsp;</strong></i><strong>Our method can be used to forecast the cost-effectiveness of potential future R&amp;D projects. Its predictions for a completed behavioral science R&amp;D project were surprisingly accurate.</strong></p><p>Relative to R&amp;D\u2019s high social return on investment (Jones &amp; Summers, 2021; Pardey et al., 2016; Kremer et al., 2019; Broussard et al., 2022), the EA community invests a surprisingly small proportion of its funds into EA-aligned R&amp;D projects (<a href=\"https://forum.effectivealtruism.org/posts/FqLKA9K8uDMpLWDcE/finding-before-funding-why-ea-should-probably-invest-more-in\"><u>Lieder &amp; McGuire, 2022</u></a>). One contributing factor could be that it is extremely difficult to predict which of a thousand potential R&amp;D projects would be highly successful. A priori, we don't know whether an R&amp;D project will produce an intervention that is much more cost-effective than any currently existing interventions, an intervention that will never be used in practice, or something in-between. We could do much more good with our limited resources if we could predict which R&amp;D projects will succeed in inventing highly cost-effective innovations for generating social value.&nbsp;</p><p>Although we cannot predict the future with certainty, we can calculate the expected social return on investment for funding an R&amp;D project given the information available&nbsp;<i>before</i> the project is conducted. Better methods for making such predictions with confidence could make it easier for funders and policymakers to endorse groundbreaking, innovative projects with a high expected positive impact. This would be an important corrective for the funding agencies\u2019 tendency to reject proposals that don\u2019t already have a compelling track record in favor of projects that are less novel and less innovative but more likely to succeed (Boudreau et al., 2016; Good Science Project, 2023).</p><p>This post describes a systematic method for calculating probabilistic forecasts about the cost-effectiveness of potential R&amp;D before they are funded. We provide a proof-of-concept for the feasibility of this approach by applying it to predict the cost-effectiveness of a behavioral science R&amp;D project. Our method predicted funding the project would be more than 100x as cost-effective as donating to the best charities working on global health and well-being, and our ex-post impact assessment confirmed that prediction. Concretely, our method\u2019s prediction of 35 WELLBYs per dollar \u2013 which was based exclusively on information that was available before the project was conducted \u2013deviated from the ex-post assessment of the project\u2019s outputs (i.e., 7.5&nbsp; WELLBYs per dollar) by less than one order of magnitude.</p><h2><strong>Method</strong>&nbsp;<strong>for predicting the ex-ante cost-effectiveness of R&amp;D projects</strong></h2><p>We define the ex-ante cost-effectiveness of an R&amp;D project as the expected value of its ex-post cost-effectiveness, given only the information that is available before the project is conducted. This expected value corresponds to the weighted average of the project's ex-post cost-effectiveness across all possible values of the effectiveness of the new intervention (<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"E_\\text{new}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.026em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mtext\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">new</span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>\u200b), the scalability of the new intervention&nbsp;(<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"S_\\text{new\u200b}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.032em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mtext\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">new</span></span></span></span></span></span></span></span></span>), and the cost of the R&amp;D project&nbsp;(<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"C_\\text{R&amp;D}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.045em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-mtext\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">R&amp;D</span></span></span></span></span></span></span></span></span>). In this weighted average, the weight of each possible combination of values is its ex-ante probability.</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\text{CE}_{\\text{ex-ante}} = \\mathbf{E}_{E_{\\text{new}},S_{\\text{new}}, C_{\\text{R&amp;D}}} \\left[ \\text{CE}_{\\text{ex-post}}(E_{\\text{new}},S_{\\text{new}}, C_{\\text{R&amp;D}}) \\right]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">CE</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">ex-ante</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-B\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">E</span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.026em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.18em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">new</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.032em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.18em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">new</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.045em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">R&amp;D</span></span></span></span></span></span></span></span></span></span><span class=\"mjx-mrow MJXc-space1\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">CE</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.519em;\">ex-post</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.026em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">new</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\" style=\"margin-right: -0.032em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">new</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\" style=\"margin-right: -0.045em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">R&amp;D</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span></span></span></span></span></p><p>To compute the predicted ex-ante cost-effectiveness defined in the above equation, we adapt the ex-post method described in the previous post such that the posterior distributions of the project\u2019s cost and the intervention\u2019s cost-effectiveness and scalability are replaced by the corresponding prior distributions that encode only the information that was available before the project was initiated. As summarized in Table 2, this modified method proceeds in three steps. The first and most difficult step is to forecast the expected increase in moral value that the R&amp;D project would generate if it were conducted. The second step is predicting how much the R&amp;D process would cost. The third step is to combine the probabilistic predictions of Steps 1 and 2 into a prediction of the project\u2019s cost-effectiveness ratio.</p><p><strong>Table 2. Procedure for predicting the ex-ante cost-effectiveness of a potential future R&amp;D project.</strong></p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:50px\"><strong>Step</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>1</strong></p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Predict the cost-effectiveness of the new intervention(s) the project might produce:</strong></p><ol><li>Use historical data to predict how likely the new intervention the project will develop is to achieve different levels of effectiveness&nbsp;</li><li>Predict how costly it would be to deploy the new intervention.</li><li>Predict the expected value of the new intervention\u2019s cost-effectiveness from the probability distributions calculated in Steps 1a and 1b.</li></ol></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>2</strong></p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Predict</strong>&nbsp;<strong>the moral value that would be created using the new intervention:</strong></p><ol><li>Predict how scalable the new intervention will be.</li><li>Predict the costs and benefits of evaluating the new intervention in an RCT</li><li>Predict the expected increase in the predicted creation of moral value</li></ol></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>3</strong></p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Predict</strong>&nbsp;<strong>the project\u2019s costs and cost-effectiveness:</strong></p><ol><li>Predict the project's costs:<ol><li>Identify a reference class of similar projects that were funded in the past</li><li>Estimate the distribution of the costs of those projects from empirical data</li></ol></li><li>Predict the project\u2019s benefit-cost ratio from the probability distributions calculated in Steps 2c and 3a.</li></ol></td></tr></tbody></table></figure><h3>Step 1: Predict the cost-effectiveness of the new intervention(s) the project might produce.</h3><p>To predict the cost-effectiveness of a potential new intervention before it has been developed, we extrapolate from historical data on the successes and failures of previous R&amp;D projects. To obtain a conservative estimate, we assume that there were no systematic improvements in the effectiveness of new interventions over time. Under this assumption, we can approximate the probability distribution of the effectiveness of new interventions by reference class forecasting (Flyvbjerg, 2006). Reference class forecasting&nbsp;uses quantitative data from similar projects to curb optimistic biases in people's intuitive predictions (Kahneman &amp; Tversky, 1979). The idea is to predict attributes of future projects, such as how long it will take to complete them, from objective data about similar projects that have been conducted in the past (i.e., the project's reference class). We develop new probabilistic reference-class forecasting methods for predicting the outcomes and impact of R&amp;D projects. The details of these methods are described in Section 3.2 of the&nbsp;<a href=\"https://observablehq.com/@falk-lieder/ce_of_r_and_d\"><u>Observable Notebook</u></a> accompanying this post.&nbsp;</p><p>Our method can be applied even when the effectiveness of previous interventions was assessed in terms of outcomes other than the benefit of interest (e.g., behavior change rather than well-being). In those cases, our method develops an evidence-based probabilistic causal model linking the outcome variables of those studies to the benefit of interest (see <a href=\"https://forum.effectivealtruism.org/posts/B4sKtodLxtdYf8i27/predicting-the-cost-effectiveness-of-deploying-a-new\">Post 2</a> for an illustration of how this can be done).</p><p>Step 1b predicts the cost of deploying the intervention per person who completes it. This includes the cost of directing people to the intervention (e.g., online advertising) and the cost of conducting it. Finally, Step 1c combines the predictions of Step 1a and Step 1b into a prediction of the cost-effectiveness of the potential future intervention.&nbsp;</p><h3>Step 2: Predict the moral value that would be created using the new intervention</h3><p>The second step of our ex-ante method is identical to the second step in our ex-post method (see Section 2 of <a href=\"https://forum.effectivealtruism.org/s/WQhc2GuStskWpXFwg/p/b3eRgj8vBcCJ6cqtY\">Post 4</a>) because both involve predicting the benefits of future applications.</p><h3>Step 3: Combine the estimates of expected costs and benefits</h3><p>To predict a project\u2019s cost (Step 3a), our method first identifies a reference class of similar completed projects or funded grant proposals. The actual or projected costs of those projects are then used to predict the cost of the proposed R&amp;D project. Concretely, you can use the histogram of the previous projects\u2019 costs as an empirical estimate of the probability distribution of the cost of the new project.&nbsp; Finally, Step 3b predicts the project\u2019s cost-effectiveness by combining the probability distributions calculated in Steps 2c and 3a.</p><h2>Proof of concept: Predicting the ex-ante cost-effectiveness of Baumsteiger\u2019s R&amp;D project</h2><p>To illustrate how our method can be applied to real R&amp;D projects and demonstrate that it works, we applied it to the running example of this sequence: Baumsteiger\u2019s online intervention for promoting prosocial behavior (Baumsteiger, 2019). In the previous post, we evaluated the project\u2019s ex-post cost-effectiveness based on its outputs. By contrast, in this post, we predict the project\u2019s ex-ante cost-effectiveness using only the information that was available before the project was conducted. We then evaluate this prediction against how the project turned out according to the ex-post analysis reported in the previous post.</p><p>As in the previous posts, we provide only a brief summary of the analyses. If you want to see the details of how we applied our method to Baumsteiger\u2019s R&amp;D project, you are welcome to peruse Section 3.2 of&nbsp;<a href=\"https://observablehq.com/@falk-lieder/ce_of_r_and_d\"><u>the project\u2019s Observable notebook</u></a>.</p><h3>Step 1: How cost-effective should we have expected Baumsteiger\u2019s intervention to turn out a priori?</h3><p>The goal of Baumsteiger\u2019s R&amp;D project was to develop a digital psychological intervention for promoting prosocial behavior. So, in this case, the effectiveness of the intervention is the size of its effect on the frequency of prosocial behavior. We, therefore, derive our probabilistic prediction of its effectiveness from the effect sizes of previous psychological interventions for promoting prosocial behavior. Because Baumsteiger\u2019s study was published in July 2019, we only consider studies published before 201</p><p>To identify such studies, we draw on three meta-analyses (Mesurado et al., 2019; Menting et al., 2013; Shin &amp; Lee, 2021) and a systematic review (Laguna et al., 2020). Our systematic meta-analysis identified a total of 30 relevant intervention-outcome pairs. Applying our reference class forecasting methods to this data produced the following probabilistic forecast of the new intervention\u2019s effectiveness at promoting prosocial behavior:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/w4vwjbxeyxkhifwqrfdk\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/xdtol6jy6czf6csqb708 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/gi70t8niiyn7o9r7n5ki 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/e3bfvs114khey3qzxozn 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/ljhrqzkg3yxwgykitqjl 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/h2nqked5lkalddnkkblf 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/wyc8dvwdioduh5cs0ph1 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/do54z0e2aifalgoz5qc4 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/xrj4ysqmtzzfjodvygkv 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/y1pdtmjtoqexbjusyipu 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/lgrmih222dolfhawxl8n 906w\"></p><p>The plot above shows that, in expectation, we predict that a new intervention would increase the number of prosocial behaviors a person engages in by about 0.26 standard deviations immediately after the person completes the intervention. This is a small effect. However, as the plot above shows, the data suggests that medium-sized and large effects are also possible (95% C.I. [-0.22, 0.62]).</p><p>To predict the total benefit of the intervention, we also have to predict how long the benefits will last, in terms of how quickly they decrease over time. To estimate this, we performed Bayesian inference on the half-life of the effects of interventions for promoting prosocial behavior. To achieve this, we applied the method we recently developed for this purpose (Lieder, 2022) to the data from previous experiments on promoting prosocial behavior. We included all such studies that were listed in the systematic review and pertinent meta-analyses listed above (Laguna et al., 2020; Mesurado et al., 2019; Shin &amp; Lee, 2021) that met our inclusion criteria and included at least one follow-up assessment. As shown below, our method inferred that the effects of prosocial behavior interventions either don\u2019t last or, which is more likely, last for at least a couple of years. On average, it appears to take about 500 days until the initial effect decays to about 50% of its original size.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/nppmatzyzuwzysjqkhha\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/ihilivnrgdk77lmy4e1o 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/tw7v4mcwsknk1skjeaky 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/o44wy8bjhnclkroeh3rz 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/bywzhi3lnu6mnliyevqu 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/vulqcvol69de5fnlmwt8 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/cnocxatbbvuf7zxa3uyg 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/hyzs5ywn8mkmb8mcbgxx 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/mvtlnnhjtmstxclbrsem 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/s41rdnbailefelh7g1px 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/s9fmf9f6r99shrvgoeqe 914w\">We then combined these two probability distributions into an estimate of a new intervention\u2019s likely effect on the total number of prosocial behaviors performed by people who complete the intervention. We then used our library of reusable functions for conducting cost-effectiveness analyses to translate this estimate into the predicted increase in well-being per person who completes the intervention. As shown below, our ex-ante analysis predicted that a new intervention can be expected to generate about 290 hours of happiness per person who completes it, but the uncertainty is very high (90% C.I. [-220, 860]).<br>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/et8muffqdzqkm9mm9nc2\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/kxtetxzopzgkoc6tbfac 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/ng0xlvyz0jkqnufswx31 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/jub81jntlf2dvo96lxgh 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/vouyyinjiykmnchx9cp5 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/y4mj7lsoyw6rsjd925gq 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/awzzjzhiypwom3rxzluj 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/zgsurdkhmenl7rdfqzss 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/i6yk03s00zpwt3aq2y33 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/dnyppk6scitetbekpenm 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/nrx6kjwbnypjt0cnggdw 926w\"></p><p><br>We then combined our prediction of a new intervention\u2019s effect on well-being with the estimate of the cost of deploying online interventions from our library of reusable cost-effectiveness analysis functions. As shown in the figure below, this analysis predicted that the expected ex-ante cost-effectiveness of a new intervention for promoting prosocial behavior is about 140 hours of happiness per dollar (90% C.I. [-88, 550]).</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/barffcimi6es7gjmophl\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/vly6ebjyezkeo8wnjkyr 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/xv2ojvcogg4q6uu68ydg 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/cpmxillh30lodpqz76oq 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/gdmkbruq3za2b5wyco1o 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/gz8zkejdxlbj1rzza4g9 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/puu3ej59lcominyzcw4n 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/mlh2ww8r5fgfxbafc9ha 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/vhlv7uf9a5jzqqkqugsq 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/rvcuodet4h37yjz8pm1i 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/swmd3wimjhzxro5uui2v 917w\"></p><h3>Step 2: Predicting the moral value that could be created with a new intervention for promoting prosocial behavior</h3><p>We predicted the potential new intervention\u2019s scalability (Step 2a) using the same approach we used for Baumsteiger\u2019s existing intervention. We again found that the uncertainty and potential of the new intervention would warrant evaluating it in an RCT (Step 2b). Next, in Step 2c we found that, in expectation, developing a new intervention for promoting prosocial behavior would increase the amount of well-being that society will create in the future by about 270 million hours of happiness. That is about 8 million well-being-adjusted life years (95% C.I. [-120K; 32M]).</p><h3>Step 3: Predicting the ex-ante cost-effectiveness of developing a new online intervention for promoting prosocial behavior</h3><p>To predict the cost of developing a new intervention for promoting prosocial behavior (Step 3a), we first identified a reference group of projects that developed similar interventions for positive character development. I identified those projects by screening all grants that the Templeton World Charity Foundation in the funding area \u201cCharacter and Virtue Development\u201d as part of their priority \u201cGlobal Innovations for Character and Virtue Development\u201d. The screening criterion was that the project developed a new psychological intervention. I then compiled a list of the size of those grants. I then used the histogram of those amounts as an empirical estimate of the cost of developing a new intervention for promoting prosocial behavior. According to this method, the expected cost of developing such an intervention is $340,000 (95% C.I. [$220k, $1M]).</p><p>Combining the predicted cost of developing the intervention with its predicted benefits (Step 3b) led to the prediction that the expected cost-effectiveness of developing a new intervention for promoting prosocial behavior was about 35 well-being-adjusted life years per dollar. As shown below, this predicted cost-effectiveness of such projects has a long right tail. This means that while there is a high probability that such projects will be unsuccessful, there is a chance that such a project can be extremely cost-effective. Concretely, our method predicts that there is a 25% chance that developing a new intervention for promoting prosocial behavior would be more than 10x as effective as the best charities in global health and well-being. However, there is also a more than 50% chance that the project would fail to produce moral value.&nbsp;<br>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/rxmlb1xwsdkbaqyccns5\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/fgpvprzaavun8knwqsa2 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/ne7y21od8rtngso9oael 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/kyiaqfppq861k7dultcx 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/stu0xrhjqpkpnj0gjhx9 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/m3fjt4br3lgyllp4kupd 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/q7qec7noj454mrkyqw9t 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/aeqq9kliak4p7fedq4hl 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/rm2ohxhqueo2wks8jw8c 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/gbmrpybmattaxs2tqbua 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/gktu4uzysqndnu5fr34q 929w\"></p><h3>How accurate was this prediction?</h3><p>We found that our ex-ante method\u2019s prediction of 35 WELLBYs per dollar \u2013 which was based exclusively on information that was available before the project was conducted \u2013 was extremely close to the ex-post assessment of the project\u2019s outputs, that is 7.5 WELLBYs per dollar. This is remarkable because the two estimates were derived from different data sources under high levels of uncertainty.<br>&nbsp;</p><p>What matters most about the accuracy of our method is not the absolute value of its predictions, but the quality of the decisions it recommends. Those decisions depend primarily on how the cost-effectiveness of the R&amp;D project compares to the cost-effectiveness of the best alternatives. Therefore, a more practically relevant metric is whether our method correctly predicts whether funding a specific R&amp;D project will be more cost-effective than donating to highly effective charities. On that metric, we find that our method predicted that funding Baumsteiger's R&amp;D project could have been more than 100x as cost-effective as the best charities for promoting global health and well-being. Critically, according to our ex-post cost-effectiveness analysis, the project's outcomes confirmed this prediction.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/kpndoxjnaxvpudydv8sf\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/ceoiaqhs0rbkx7via1ek 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/cugcp9jtguv9tqvnk2vl 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/cqxteotjxvweirgixw9v 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/cxxsemprisgsv9mrp5ja 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/n5ugluqdedhg8cacobln 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/tgajjhfzuraa6phxlih6 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/fcvwupuf5lhxtomaekxi 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/eeovothtggqmhnhtvkh8 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/pyk2oa4z71djfoxvg8vg 803w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f6JCsAYcsfSzdkmnB/znct3jjlj3smka7ufbjv 810w\"></p><h2>Discussion</h2><p>We have introduced a method for predicting the cost-effectiveness of an R&amp;D project before it is undertaken. It can therefore be used to decide whether a proposed R&amp;D project is worth funding. We then reported a proof-of-concept illustration showing that this method can be used to predict the cost-effectiveness of developing a psychological intervention for promoting altruism.&nbsp;</p><p>Our method correctly predicted that the behavioral science R&amp;D project by Baumsteiger (2019) was highly cost-effective. The accuracy of our method\u2019s prediction was extremely encouraging. However, so far, this method can only be applied to a single R&amp;D project. Therefore, future work should evaluate its accuracy on at least 10 additional R&amp;D projects. If these additional evaluations confirm the apparent accuracy of our ex-ante method, this would suggest that it can be used to support funding decisions for R&amp;D projects. In the future, this could make it easier for funders and policymakers to endorse groundbreaking, innovative projects with a high expected positive impact. This could potentially help mitigate funding agencies\u2019 tendency to reject proposals that don\u2019t already have a compelling track record in favor of projects that are less novel and less innovative but more likely to succeed (Boudreau et al., 2016; Good Science Project, 2023). Perhaps, better methods for predicting the cost-effectiveness of R&amp;D projects could also help reduce the discrepancy between the high-cost-effectiveness of R&amp;D at creating social value (Jones &amp; Summers, 2021) and the surprisingly low proportion of EA funds allocated to scientific research, innovation, and the development of new interventions (Lieder &amp; McGuire, 2022).</p><p>Our ex-ante method also allows the cost-effectiveness of R&amp;D to be compared with the cost-effectiveness of donating to charities doing direct work on global health and well-being. Here, we found that our ex-ante method confirmed the ex-post method\u2019s assessment that the development of Baumsteiger\u2019s psychological intervention for promoting prosocial behavior was more cost-effective than promoting health and well-being directly. This suggests that R&amp;D projects aiming to produce psychological interventions for promoting altruism can be much more cost-effective than promoting health and well-being directly.&nbsp;</p><h2>References</h2><ol><li>Baumsteiger, R. (2019). What the world needs now: An intervention for promoting prosocial behavior. Basic and applied social psychology, 41(4), 215-229.</li><li>Boudreau, K. J., Guinan, E. C., Lakhani, K. R., &amp; Riedl, C. (2016). Looking across and looking beyond the knowledge frontier: Intellectual distance, novelty, and resource allocation in science. Management science, 62(10), 2765-2783.</li><li>Broussard, N. H., Chomitz, K. M., Chowdhuri, R. N., Sturla, K., Ssentongo, J., &amp; Zwane, A. P. (2022). Assessing the Social Returns to Innovation for Development: The Global Innovation Fund\u2019s Impact to Date. Working Paper.&nbsp;<a href=\"https://www.globalinnovation.fund/wp-content/uploads/2022/03/GIF-SROI-March-2022-Draft.pdf\"><u>https://www.globalinnovation.fund/wp-content/uploads/2022/03/GIF-SROI-March-2022-Draft.pdf</u></a>&nbsp;</li><li>Caprara, G. V., Kanacri, B. P. L., Gerbino, M., Zuffiano, A., Alessandri, G., Vecchio, G., ... &amp; Bridglall, B. (2014). Positive effects of promoting prosocial behavior in early adolescence: Evidence from a school-based intervention.&nbsp;<i>International Journal of Behavioral Development, 38</i>(4), 386-396.</li><li>Flyvbjerg, B. (2008). Curbing optimism bias and strategic misrepresentation in planning: Reference class forecasting in practice.&nbsp;<i>European planning studies, 16</i>(1), 3-21.</li><li>Good Science Project (2023). Why science funders should try to learn from past experience.&nbsp;<a href=\"https://goodscienceproject.org/articles/why-science-funders-should-try-to-learn-from-past-experience/\"><u>https://goodscienceproject.org/articles/why-science-funders-should-try-to-learn-from-past-experience/</u></a>&nbsp;</li><li>Jones, B. F., &amp; Summers, L. H. (2021). A calculation of the social returns to innovation. In Goolsbee and Jones (Eds).&nbsp;<i>Innovation and Public Policy</i>, pp. 15-39.&nbsp;<i>National Bureau of Economic Research</i>.&nbsp;DOI: 10.3386/w27863.&nbsp;</li><li>Kahneman, D., &amp; Tversky, A. (1979). Intuitive prediction: Biases and corrective procedures. In S. Makridakis &amp; S. C. Wheelwright (Eds.). Studies in the Management Sciences: Forecasting, p.12 (Amsterdam: North Holland).</li><li>Kremer, M., Gallant, S., Rostapshova, O., Thomas, M., Chomit, K., Carbonell, J., ... &amp; Jaffe, A. (2019). Is development innovation a good investment? Which innovations scale? Evidence on social investing from USAID\u2019s Development Innovation Ventures. Working paper.</li><li>Laguna, M., Mazur, Z., K\u0119dra, M., &amp; Ostrowski, K. (2020). Interventions stimulating prosocial helping behavior: A systematic review.&nbsp;<i>Journal of Applied Social Psychology, 50</i>(11), 676-696.</li><li>Lieder, F. (2022). Predicting how the effect of a psychological intervention would change over time.&nbsp;<a href=\"https://docs.google.com/document/d/1hU7TyBB0XEWaa-ZMCJwJzjRF5AH4XTXyaRPo5rr-iM0/edit?usp=sharing\">https://docs.google.com/document/d/1hU7TyBB0XEWaa-ZMCJwJzjRF5AH4XTXyaRPo5rr-iM0/edit?usp=sharing</a></li><li>Lieder, F., &amp; McGuire, J. (2022). Finding before funding: Why EA should probably invest more in research. Effective Altruism Forum,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/FqLKA9K8uDMpLWDcE/finding-before-funding-why-ea-should-probably-invest-more-in\"><u>https://forum.effectivealtruism.org/posts/FqLKA9K8uDMpLWDcE/finding-before-funding-why-ea-should-probably-invest-more-in</u></a>&nbsp;</li><li>Menting, A. T., de Castro, B. O., &amp; Matthys, W. (2013). Effectiveness of the Incredible Years parent training to modify disruptive and prosocial child behavior: A meta-analytic review.&nbsp;<i>Clinical Psychology Review, 33</i>(8), 901-913.</li><li>Mesurado, B., Guerra, P., Richaud, M. C., &amp; Rodriguez, L. M. (2019). Effectiveness of prosocial behavior interventions: a meta-analysis. In&nbsp;<i>Psychiatry and neuroscience update</i> (pp. 259-271). Springer, Cham.</li><li>Pardey, P. G., Andrade, R. S., Hurley, T. M., Rao, X., &amp; Liebenberg, F. G. (2016). Returns to food and agricultural R&amp;D investments in Sub-Saharan Africa, 1975\u20132014.&nbsp;<i>Food policy, 65</i>, pp. 1-8.</li><li>Plant, M. (2022). Don\u2019t just give well, give WELLBYs: HLI\u2019s 2022 charity recommendation. Effective Altruism Forum,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/uY5SwjHTXgTaWC85f/don-t-just-give-well-give-wellbys-hli-s-2022-charity\">https://forum.effectivealtruism.org/posts/uY5SwjHTXgTaWC85f/don-t-just-give-well-give-wellbys-hli-s-2022-charity</a></li><li>Shin, J., &amp; Lee, B. (2021). The effects of adolescent prosocial behavior interventions: a meta-analytic review. Asia Pacific Education Review, 22, 565-577.</li><li>Silverman, B. W. (1986). Density estimation for statistics and data analysis (Vol. 26). CRC press.</li></ol>", "user": {"username": "Falk Lieder"}}, {"_id": "Fc9yhQygsj3bmJQqy", "title": "GiveWell recommended charities X mainstream events", "postedAt": "2023-05-08T07:09:28.602Z", "htmlBody": "<p>I explored getting a charity place in the London half marathon this morning and was very surprised to discover that <i>none</i> of the GiveWell recommended charities have signed themselves up as one of ~500 partners to the London Royal Parks Half Marathon this coming September. Given that &gt;10,000 people will be participating and the easiest way to enter is to raise \u00a3350+ for a partner charity, it seems a bit odd not to be present. Given the scale of this event, I'm expecting that I'll find a similar disconnect with other major 'mainstream' events, though I haven't verified this.</p><p>I'm writing this in the hope that someone working at one of these charities can clarify the reasons for their lack of engagement with these events. I'd love to learn more and I hope to be able to raise money for your charity in connection with these events soon.</p>", "user": {"username": "huwfthomas"}}, {"_id": "fckBKDvgE5JjDghe4", "title": "The Legend of Dr. Oguntola Sapara", "postedAt": "2023-05-08T06:34:10.519Z", "htmlBody": "<p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33d2c89c-0c42-41ac-9227-a5cce7490529_565x792.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33d2c89c-0c42-41ac-9227-a5cce7490529_565x792.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33d2c89c-0c42-41ac-9227-a5cce7490529_565x792.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33d2c89c-0c42-41ac-9227-a5cce7490529_565x792.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33d2c89c-0c42-41ac-9227-a5cce7490529_565x792.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33d2c89c-0c42-41ac-9227-a5cce7490529_565x792.png 1456w\"></a></p><p>This is a true story.</p><p>At the dawn of the long war's final century, the tide was turning. The Abomination was in retreat, driven back by warriors wielding the Lance of Jenner.&nbsp; But in this war as in all wars, some battles could not be fought so directly. Battles waged in cunning and lies. Battles where the enemy lurked in shadow. In those shadows an unholy alliance was forged between the Pox Abomination and a conspiracy of... humans.</p><p>The traitors to the Yoruba people named it \"Sopona\", gave it a face, and proclaimed themselves the Abomination\u2019s priests. They claimed that they alone could intercede with the Abomination on behalf of its victims. And to cross them, they said, was to incur Sopona's wrath.&nbsp;</p><p>They would unleash the Abomination upon any who dared oppose them - and sometimes they would simply inflict Sopona's torture indiscriminately. Amid death and devastation their victims would beg the Priests for help, further cementing their grip on power. And all the while they kept Jenner's Lance at bay, for their power rested in fear of the Abomination, and should it be slain their power, too, would come to an end.</p><p>The Priests were shrouded in secrecy: the better to obscure their lies, and the better to hide from those who would challenge it. Through blackmail and terror, they maintained their iron grip for generations. So total was their power that none dared utter \"Sopona\" lest they invoke its wrath - and so even the true name was hidden.</p><p>Every measure by every authority failed to contain the Abomination. They could never understand why, for they were blind to the enemy's allies. The Deathly Priests and their twisted methods were beyond the grasp of governments, warriors, and weapons. Here the global campaign could not reach. Here, harbored by its murderous allies, the Abomination reigned, mothers wept, and the Yoruba people resigned themselves to a god of suffering and death against which there seemed no hope.</p><hr><p>It is inadvisable to try to hide from humans. They are curious, relentless, ruthless creatures, fearless when determined and cunning as well. And none were more human than Dr. Oguntola Sapara.</p><p>Oguntola was a proud child of the Yoruba people. His father, born in chains, together with his mother, raised a family of prodigies: not only Oguntula, but his brother Alexander and his sister Clementina. But Clementina's story was all too short, for when she was to bring life into the world, she was instead taken by death.</p><p>There are no records to tell us how Oguntola felt that day; All we know is that it was at this moment that Oguntula dedicated his life to defending the innocent from the inhuman evils of the world, to master the protective arts and wield them against any who would dare threaten his people ever again.</p><p>For years he studied, and toiled, and healed, growing ever stronger in the art through talent and sheer force of will. Ten years on his quest took him across the seas to study in a far away land, and here yet more obstacles greeted him. For among the practitioners of the art were counted a great many fools. They would forfeit the privilege of working alongside one of humanity's best for the most vapid and meaningless of reasons, and worse still actively stymied his efforts in all things lest their foolishness be revealed for the lie it was.</p><p>But Oguntola persisted, surmounting every obstacle lesser humans would set before him. He not only prevailed, but he proved himself among the greatest of the practitioners, was recognized as a master of the art, and elected to the Royal Institute of healers.</p><p>(In the midst of everything, he even assisted the legendary truth-seeker Ida Wells in her crusades against evil and ignorance - but that is another story.)</p><p>His training complete and his mastery assured, Dr. Oguntola Sapara returned to Lagos to confront his true enemies.</p><p>In Ebute Metta he sheltered the innocent from an enemy encroaching on all sides, crushing the scourge beneath a well-laid foundation and scouring the air of its curse. In Saki he erected a mighty armory that people might arm themselves against the blights of the world. But it was in Epe where Oguntola would confront the Secret Priesthood and the Abomination they served.</p><p>That the Abomination was present was immediately clear to Oguntola, and knowing its weakness he deployed the Lance of Jenner - but it had no effect. The Lance never struck true, as if some unseen force were diverting it. And this the doctor would not abide. And so he trained the full power of his senses, his determination, and his mind on the shrouded battlefield, seeking out what hidden hand opposed him. And he heard whispers of a Secret Society, a cult whose priests were said to commune with the Abomination, their god. But they were untouchable, ghosts in the night, beyond the reach of any organization.</p><p>But not, perhaps, beyond the reach of a single man.</p><p>He would undertake his mission against Sopona alone. No government nor ally of any kind would afford him even the slightest protection. He would be surrounded at every moment by murderers who would not hesitate to take his life under the most agonizing of terms if they suspected who he truly was. An Order of Murderers unchallenged for generations against one doctor armed only with his wits and a determination to save his people.</p><p>And so Dr. Oguntola Sapara, master of the healing arts, infiltrated the conspiracy that served his greatest foe.</p><p>Day by day, he gained their trust. Lied to them, as they had lied to so many. Smiled as they detailed their murderous methods. Listened eagerly as they divulged their secrets and confirmed his worst suspicions. Of how they kept the Lance at bay. How any comfort the people might find would be taken by Sopona, lest they imagine they needed not the Priests. How those who refused to cooperate found themselves struck down by Sopona's hand. And with each word and action, they unknowingly laid bare the breadth and depth of their plot before the doctor who would destroy them.</p><p>In time the doctor came to possess the knowledge the Priests of Sopona had killed to protect - the knowledge to bring them to ruin. And so he summoned the men who had terrorized the people of Epe. Outnumbered but unafraid, Oguntola stood firm and described in detail the crimes he now knew they had committed, and threatened the consequences that might befall them. And in that moment, after years of murder and countless horrors, facing one man who had bested them all -&nbsp;</p><p>The traitors fled from Epe, fearing The Doctor Who Knew.</p><p>But Dr. Sapara did not pause to celebrate, for Sopona still lurked. Once more he brought forth the Lance of Jenner, and this time it struck true, scouring Epe of the Abomination. Sopona was sent reeling, helpless against one of the mightiest weapons in humanity's arsenal.</p><p>Oguntola did not stop there, for Epe was but one of many caught in the clutches of Sopona. He recorded his discoveries for all to see, a bitter truth that burned so bright the Cult of Sopona recoiled in fear across the land. A shadow of its former self, some scattered remnants of the conspiracy feebly attempted to strike out once more, but the injury was permanent and there were none to heal it.</p><p>This would not be the last triumph of Chief Dr. Sapara, bearer of the insignia of Bashemi, Honorary Consulting Physician to the Egba Native Administration, member of the Imperial Service Order. He confronted cruelty and ignorance among the healers across the sea, weakening those lies beneath the harsh light of truth as he had Sopona. When the Black Death itself struck his home, he led the fight to vanquish it and emerged victorious. When his triumphs yielded riches, he did not keep it, but instead funded further protection and healing wherever he could. He even sought out the ancient weapons of rumor and myth, to discover if they too could aid humanity in the struggle. And still he was not done.</p><p>For he never forgot what had sent him down this path. Whenever he could, Dr. Oguntola Sapara returned again and again to the necessity, the urgency, of defending the life-bringers and the young lives they carried. To this end he provided for many women to undertake the journey he had so many years ago, to travel across the seas and master the art - including his own daughter. He founded his own society, the very antithesis of that which he'd destroyed: to share and spread knowledge, to grow and protect life. There he provided for the training of legions of Guardians, each sworn to defend the life-bringers, and those new souls brought into this world, from any demon, blight, scourge, or abomination that would threaten them. And knowing the struggle was long, he ensured that the next generation would be stronger still. Thus the legacy of Dr. Sapara extends beyond the enemies he felled: it is knowledge gained and weapons forged, lives unended, generations defended, ignorance exposed and hope enduring.</p><p>Dr. Oguntola Sapara lived a long and full life, but he would not live to see humanity's final triumph over Sopona. The Abomination was a vast and ancient foe, and our victory did not come quickly. But few wounded the Abomination as Dr. Sapara did.</p><p>We owe Dr. Oguntola Sapara, and every human who fought in the long war, a debt we can never pay back. We can only remember, and pay it forward, that in time more and more of humanity\u2019s enemies join the Abomination in the oblivion where they belong. And so we fight, day after day, year after year, generation after generation, until everyone is safe.</p><p>Sopona is dead by our hand. Humanity is alive. And we are not done yet.</p><p>Happy Smallpox Eradication Day.</p><hr><h3>Sources</h3><p><a href=\"https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=d1e53007f744ad1f38b90455eb3c5d50faef8846\"><u>Adeloye, Adelola. </u><i><u>Some Early Nigerian Doctors and Their Contributions to Modern Medicine in West Africa</u></i></a></p><p><a href=\"https://iosrjournals.org/iosr-jhss/papers/Vol.%2021%20Issue4/Version-2/H2104025054.pdf\"><u>Oladele &amp; Augustine, 2016</u></a></p><p><a href=\"https://en.wikipedia.org/wiki/Sopona\"><u>Wikipedia, \u201cSopona\u201d</u></a>. Retrieved May 7 2023.</p><p><a href=\"https://en.wikipedia.org/wiki/Oguntola_Sapara\"><u>Wikipedia, \u201cOguntola Sapara\u201d</u></a>. Retrieved May 7 2023.</p><p><a href=\"https://www.rcpe.ac.uk/heritage/college-history/oguntola-odunbaku-sapara\"><u>Royal College of Physicians of Edinburgh website, \u201cOguntola Odunbaku Sapara\u201d. Retrieved May 7 2023</u></a>. Retrieved May 7 2023.</p><p>Zaccheus Onumba Dibiaezue Memorial Libraries website, \u201c<a href=\"https://zodml.org/discover-nigeria/people/industry/oguntola-odunbaku-sapara\">Oguntola Odunbaku Sapara</a>\u201d. Retrieved May 7 2023.</p>", "user": {"username": "jai"}}, {"_id": "ReG9QaNBqF3xJeAeY", "title": "A Brief & Recent History of Fish: What I learned in a weekend", "postedAt": "2023-05-08T07:58:38.415Z", "htmlBody": "<p>Epistemic status: I researched this in a day and a half, wrote it in a couple hours, and shared it with no one. Believe nothing and fact check everything.</p><p>Purpose: 1. This is an experiment in how quickly I can learn and write about a new cause area. Feedback would be very helpful. 2. Maybe for someone equally new to the topic of fish, it will speed up your learning!</p><h1>Fishing history</h1><p><i>(A summary of the documentary The End of the Line.)</i></p><p>Indigenous peoples subsistence fished for millennia. When European settlers arrived in North America in the late 1400s, they were stunned by the fish population. For the next four centuries, Europeans would immigrate to North America to support themselves as fishermen. In Newfoundland, cod fishing was extremely popular.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh1olc4qpul\"><sup><a href=\"#fnh1olc4qpul\">[1]</a></sup></span></p><p>In 1951, factory fishing of cod began. &nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh1olc4qpul\"><sup><a href=\"#fnh1olc4qpul\">[1]</a></sup></span>&nbsp;But by 1992, cod were practically fished out of existence in Atlantic Canada. A moratorium on cod fishing began in 1994. 40,000 people lost their jobs, and protests ensued. Cod continue to be endangered today, and the moratorium has lasted over 30 years.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref103zkq5jhnbn\"><sup><a href=\"#fn103zkq5jhnbn\">[2]</a></sup></span></p><p>In the late 20th century, data showed populations of many species of fish around the world decreasing, but the \"worldwide catch\" was increasing. Tracing the discrepancy to China, researchers realized that the Communist Party had been falsifying fishing data. In 2002, researchers realized that the true worldwide catch had been decreasing since 1988. Fishers panicked.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref103zkq5jhnbn\"><sup><a href=\"#fn103zkq5jhnbn\">[2]</a></sup></span></p><p>According to Boris Worm, populations of the fish we eat have declined 90% around the world.&nbsp;By 2003, a third of species we eat had collapsed. All may collapse by the mid-century.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref103zkq5jhnbn\"><sup><a href=\"#fn103zkq5jhnbn\">[2]</a></sup></span></p><p>Trawling nevertheless continues. In Senegal and elsewhere, while local, impoverished fishers lose catch every year, their governments sell fishing rights to richer countries who use supertrawlers. Immigrants who can no longer afford to live as fishers in their home countries are turned away by the same governments that sponsored the destruction of their ecosystems and jobs.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref103zkq5jhnbn\"><sup><a href=\"#fn103zkq5jhnbn\">[2]</a></sup></span></p><p>The largest trawling nets fit thirteen 747 jets. Trawling nets plough the ocean floor multiple times a year, disturbing delicate ecosystems like coral reefs.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref103zkq5jhnbn\"><sup><a href=\"#fn103zkq5jhnbn\">[2]</a></sup></span></p><p>Now that the populations of many predator fish, such as cod, have collapsed, smaller species of fish and arthropods have exploded. Some examples in Atlantic Canada and the USA include lobsters, Chesapeake rays, shrimp, and jellyfish.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref103zkq5jhnbn\"><sup><a href=\"#fn103zkq5jhnbn\">[2]</a></sup></span></p><p>The Marine Stewardship Council is one body that tries to prevent overfishing and improve the sustainability of the industry. Yet 93% of the ocean remains unprotected by Marine Protected Areas.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxkzfj1guug\"><sup><a href=\"#fnxkzfj1guug\">[3]</a></sup></span>&nbsp;(You could just stop eating wild fish.)</p><h1>Is farming a solution?</h1><p><i>So let's stop catching wild fish! We can just grow our own, right? We can farm some salmon. Salmon eat herring. So we will fish for some herring to feed the salmon -- crap.</i></p><p>Today, up to three trillion fish are killed for food every year. About half are wild and half are farmed. In fish production by weight, farming overtook wild fishing in the 2010s. Fish farming is the fastest growing form of animal food production.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefq26yppfddy\"><sup><a href=\"#fnq26yppfddy\">[4]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflga1u4k3bln\"><sup><a href=\"#fnlga1u4k3bln\">[5]</a></sup></span>&nbsp;The number of farmed fish alive today is approximately as many humans as have ever lived.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefx7trlkp13fs\"><sup><a href=\"#fnx7trlkp13fs\">[6]</a></sup></span></p><p>Does farming reduce overfishing? No. Evidence suggests that farming either doesn't impact overfishing or makes the issue worse.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref103zkq5jhnbn\"><sup><a href=\"#fn103zkq5jhnbn\">[2]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflga1u4k3bln\"><sup><a href=\"#fnlga1u4k3bln\">[5]</a></sup></span>&nbsp;Predator fish eat small fish. We farm a lot of predator fish, and we catch a lot of small fish to feed them with. For every pound of predator fish, five pounds of small fish were used to feed them.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref103zkq5jhnbn\"><sup><a href=\"#fn103zkq5jhnbn\">[2]</a></sup></span>&nbsp;This amounts to hundreds of billions of individual fish caught to feed farmed fish every year.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy6opy0ua58e\"><sup><a href=\"#fny6opy0ua58e\">[7]</a></sup></span></p><p>Putting the ineffectiveness of that aside, let's take a brief interlude to learn what it's like to be a fish.</p><p>Do fish notice their exploitation? They don't have a neocortex.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflga1u4k3bln\"><sup><a href=\"#fnlga1u4k3bln\">[5]</a></sup></span>&nbsp;However, some species of fish have demonstrated:</p><ul><li>long-term memory</li><li>social structures</li><li>problem solving abilities</li><li>the ability to use tools&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefq26yppfddy\"><sup><a href=\"#fnq26yppfddy\">[4]</a></sup></span></li><li>the ability to hunt, which requires some ability to predict the behaviour of prey&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdusix7m4h6r\"><sup><a href=\"#fndusix7m4h6r\">[8]</a></sup></span></li><li>preferences</li><li>adaptive emotional states</li><li>passing the mirror test&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefx7trlkp13fs\"><sup><a href=\"#fnx7trlkp13fs\">[6]</a></sup></span></li></ul><p>Fish have nociceptors, which is a type of cell that carries pain signals. Fish have &nbsp;demonstrated the ability to feel pain by:</p><ul><li>avoiding painful stimuli</li><li>treating their injuries</li><li>being distracted from survival behaviours by painful stimuli</li><li>not being distracted by painful stimuli after receiving morphine&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflga1u4k3bln\"><sup><a href=\"#fnlga1u4k3bln\">[5]</a></sup></span></li><li>naturally releasing opioids&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdusix7m4h6r\"><sup><a href=\"#fndusix7m4h6r\">[8]</a></sup></span></li></ul><p>So it's a good thing that humans always run respectful, painless farms.</p><h1>What happens on a fish farm?</h1><p><i>Just kidding. (Skip down to \"Loch Duart, ikejime, &amp; the next frontier\" if you want good news.)</i></p><h2>Death</h2><p>Fish on a farm live, and then they are killed. Let's learn about the killing first.</p><p>Fish are (usually, thank goodness) killed before we get them. The most common slaughter methods are asphyxia and live chilling. These are terrible, and take from twelve minutes to several hours.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy6opy0ua58e\"><sup><a href=\"#fny6opy0ua58e\">[7]</a></sup></span>&nbsp;</p><p>As fish asphyxiate, they thrash against the ice and bleed internally.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdusix7m4h6r\"><sup><a href=\"#fndusix7m4h6r\">[8]</a></sup></span>&nbsp;</p><p>Live chilling involves putting fish in an ice slurry in an attempt to render them insensible, or stun them, before killing them. But according to Andr\u00e9s Jim\u00e9nez Zorrilla from Shrimp Welfare Project, ice slurries are often used incorrectly (in the case of shrimp, anyway) and may not be effective at stunning as they are used.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefd0795picu06\"><sup><a href=\"#fnd0795picu06\">[9]</a></sup></span></p><p>Some other slaughter methods include gassing with CO2, cutting the gills and letting the fish bleed out, and and processing alive.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefq26yppfddy\"><sup><a href=\"#fnq26yppfddy\">[4]</a></sup></span></p><p>They're not always killed before we get them, though. Some people buy live fish. This means that the fish have gone through many stressful transports and will be inexpertly slaughtered without stunning.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefx7trlkp13fs\"><sup><a href=\"#fnx7trlkp13fs\">[6]</a></sup></span></p><p>There are a couple \"humane\" ways of stunning fish. These are electrical stunning (sending a current through the brain) and percussive stunning (hitting the skull very hard).&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy6opy0ua58e\"><sup><a href=\"#fny6opy0ua58e\">[7]</a></sup></span>&nbsp;Andr\u00e9s has mentioned that implementing electrical stunning on a massive scale, such as running a current through a pond, poses significant safety challenges.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefd0795picu06\"><sup><a href=\"#fnd0795picu06\">[9]</a></sup></span>&nbsp;I imagine it is also difficult to accurately implement percussive stunning at scale as well.</p><p>But let's say we can efficiently stun all the fish we intend to kill. How do we go about doing that? Unfortunately, there are not clear standards for what electrical or percussive force is needed to stun each species of fish.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy6opy0ua58e\"><sup><a href=\"#fny6opy0ua58e\">[7]</a></sup></span></p><p>We also usually can't tell whether a fish has been rendered insensible by stunning or just immobile. There is one practical way to check, which adds time to the slaughter process: checking each fish for the eye roll reflex.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy6opy0ua58e\"><sup><a href=\"#fny6opy0ua58e\">[7]</a></sup></span></p><h2>Life</h2><p>That wasn't super promising. Do fish live good lives on farms? Buckle up.</p><p>We breathe oxygen through our lungs. Fish process dissolved oxygen in the water through their gills. Each species of fish thrives at its own best level of dissolved oxygen. Unfortunately, we mostly don't know what that is for each species, beyond being somewhere between 3 and 9 mg/L, probably. What we do know is that whatever the ideal is, most farms probably aren't living up to it.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy6opy0ua58e\"><sup><a href=\"#fny6opy0ua58e\">[7]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqk5t2rnpv47\"><sup><a href=\"#fnqk5t2rnpv47\">[10]</a></sup></span></p><p>When dissolved oxygen is too low, fish might get a feeling similar to how we feel when we can't get enough oxygen. Lots of people who have had pneumonia know what that's like.</p><p>Low dissolved oxygen is thus a main concern for fish welfare.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy6opy0ua58e\"><sup><a href=\"#fny6opy0ua58e\">[7]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqk5t2rnpv47\"><sup><a href=\"#fnqk5t2rnpv47\">[10]</a></sup></span></p><p>Let's look at some more specific issues. Here are some things you will probably see on a salmon farm:&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefq26yppfddy\"><sup><a href=\"#fnq26yppfddy\">[4]</a></sup></span></p><ul><li>tens of thousands of salmon, and quite a few of them are dead</li><li>disease</li><li>stress</li><li>aggression and injuries</li><li>low oxygen and high ammonia</li><li>little expression of natural behaviours</li><li>no natural migration patterns</li><li>feces</li><li>old food</li><li>sea lice that feed on skin and blood, leaving ulcers<ul><li>50% of farms in Scotland have sea lice infestations at a time&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefx7trlkp13fs\"><sup><a href=\"#fnx7trlkp13fs\">[6]</a></sup></span></li></ul></li><li>starvation before transport, grading, and slaughter</li></ul><p>It's not just bad for the farmed fish. Hundreds of thousands of farmed fish escape per year, spreading diseases and sea lice to wild fish, and threatening biodiversity.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefq26yppfddy\"><sup><a href=\"#fnq26yppfddy\">[4]</a></sup></span></p><p>Also, it's not just bad for fish in general. Hundreds of seals and thousands of birds are also killed per year on salmon farms to minimize loss.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefq26yppfddy\"><sup><a href=\"#fnq26yppfddy\">[4]</a></sup></span></p><p>We would really like to get rid of some of sources of suffering. But how? There are very few \"high certainty\" welfare areas for fish. In fact, there are none for the grass carp, and we farm billions of them per year.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy6opy0ua58e\"><sup><a href=\"#fny6opy0ua58e\">[7]</a></sup></span>&nbsp;And there are 362 species of fish we farm, all with different needs.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefx7trlkp13fs\"><sup><a href=\"#fnx7trlkp13fs\">[6]</a></sup></span></p><h2>Birth</h2><p>Fish don't really have to be alive yet to have issues, either. Farmed fish are born with myriad problems.</p><p>Like chickens, some fish have been bred to be too big to maximize yield. This results in cataracts and heart defects.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflga1u4k3bln\"><sup><a href=\"#fnlga1u4k3bln\">[5]</a></sup></span></p><p>When these weird little guys escape the farms, they breed with wild fish, which makes the wild fish population worse at surviving and reproducing. This is called genetic pollution.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflga1u4k3bln\"><sup><a href=\"#fnlga1u4k3bln\">[5]</a></sup></span></p><p>Some farmers want to avoid the problem of genetic pollution, so they create triploid salmon, which cannot breed with wild salmon. However, triploid salmon seem to live worse lives.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy6opy0ua58e\"><sup><a href=\"#fny6opy0ua58e\">[7]</a></sup></span></p><h1>How bad is that?</h1><p><i>Wow, that was terrible. But is it the absolute worst? Yes, probably.</i></p><p>In December 2018, Charity Entrepreneurship (CE) developed the Weighted Animal Welfare Index (WAWI).&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjc9u4f2o93l\"><sup><a href=\"#fnjc9u4f2o93l\">[11]</a></sup></span>&nbsp;It contains eight metrics, themselves scored on:</p><ul><li>accuracy of the measure as a proxy for ethical value</li><li>cross-animal applicability</li><li>operationalizability</li></ul><p>CE ranked fifteen animals on each of the eight metrics to create a welfare score. Wild fish scored 7th on welfare. Factory farmed fish in a traditional aquaculture setting scored 10th. Wild fish that humans catch scored 12th out of 15.</p><p>CE then combined the welfare score with population size, odds of feeling pain, neglectedness, and number of reasons for suffering. This created an overall ordering of which animals were highest priority to help.</p><p>As they say, there are plenty of fish in the sea. As it turns out, even though farmed birds may have worse lives as individuals, the scale of fish suffering is just enormous. Wild fish are particularly neglected, and suffer for many reasons.</p><p>Wild fish for human use, other wild fish, and factory farmed fish took the top three spots in CE's priority ranking of animals to help. Someone, please, help the fish.</p><h1>Loch Duart, ikejime, &amp; the \"next frontier\"</h1><p>In 2016, someone tried to help the fish. Salmon farming company Loch Duart hold themselves to high standards in fish \"health, diet, water quality, husbandry, handling and slaughter.\"&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxoye6t6wf4\"><sup><a href=\"#fnxoye6t6wf4\">[12]</a></sup></span>&nbsp;But they saw that their salmon were still nipping each other's fins, which is a sign of stress. Manager David Roadnight figured his fish might be bored, and he gave them some coloured balls and a kelp-like tarp. These props are both types of environmental enrichment. (The effectiveness of environmental enrichment overall has mixed evidence and depends on the species.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy6opy0ua58e\"><sup><a href=\"#fny6opy0ua58e\">[7]</a></sup></span>)</p><p>The coloured balls distracted the fish from nipping each other. The tarp provided a hiding place, which allowed the salmon to express more of their natural behaviours. When fish could hide from the disturbance of passing shadows, they would stop showing signs of stress sooner after the disturbance passed.</p><p>The tarps resulted in better use of tank space, less nipping behaviour, and less fin damage. The tarps seemed to be a more cost-effective welfare improvement than the coloured balls.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxoye6t6wf4\"><sup><a href=\"#fnxoye6t6wf4\">[12]</a></sup></span></p><p>In 2018, Andrew Tsui, founder of the Ike Jime Federation, invited Vox to kill some fish with him. Ikejime is a slaughter-stunning method purported to be more humane than alternatives. The fish are stabbed through the brain and rendered brain-dead instantly. Then a wire is inserted into the spinal column to eliminate thrashing. Finally, the fish is bled. This eliminates the build up of blood and lactic acid in the tissues.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdusix7m4h6r\"><sup><a href=\"#fndusix7m4h6r\">[8]</a></sup></span></p><p>In 2021, Vox announced that fish welfare is the \"next frontier\" in animal advocacy.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflga1u4k3bln\"><sup><a href=\"#fnlga1u4k3bln\">[5]</a></sup></span></p><h1>Where should we help fish?</h1><p>So if we are going to help the fish, where should we do it? By weight, 88% of farmed fish are in Asia, and half of farmed fish are in China.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefx7trlkp13fs\"><sup><a href=\"#fnx7trlkp13fs\">[6]</a></sup></span>&nbsp;But there may be more of a precedent for helping animals in India, where the second-most fish are farmed.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefobmka66p10g\"><sup><a href=\"#fnobmka66p10g\">[13]</a></sup></span></p><p>Of the 111 billion fish farmed per year, about 3-15 billion of them are farmed in India. A person in India typically consumes more fish per year than land animals by the kilogram. The highest fish consumption is by men, people in urban areas, and people in the following coastal regions:&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefobmka66p10g\"><sup><a href=\"#fnobmka66p10g\">[13]</a></sup></span></p><ul><li>Kerala, on the southwest shore</li><li>West Bengal, on the eastern point of the mainland (capital: Kolkata)</li><li>Lakshadweep islands</li><li>Andaman islands</li><li>Nicobar islands</li></ul><p>India seems like a promising place to work on animal advocacy because they have strict and extensive animal agriculture laws. However, these laws are rarely enforced, and the listed fines have not kept up with inflation.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefobmka66p10g\"><sup><a href=\"#fnobmka66p10g\">[13]</a></sup></span></p><p>Animal advocacy is common, as is vegetarianism. However, 90% of animal advocacy groups in India focus on street dogs, and many of the rest focus on cows.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefobmka66p10g\"><sup><a href=\"#fnobmka66p10g\">[13]</a></sup></span></p><p>India's government may be open to animal advocacy, but they do not seem particularly concerned about fish (as of 2021). The government is trying to increase aquaculture, which is unregulated. Fish farms in India have low adherence to the Aquatic Animal Health Code.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefobmka66p10g\"><sup><a href=\"#fnobmka66p10g\">[13]</a></sup></span></p><p>Perhaps they could be persuaded. Here are some opportunities:&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefobmka66p10g\"><sup><a href=\"#fnobmka66p10g\">[13]</a></sup></span></p><ul><li>strong animal agriculture laws</li><li>public support for welfare standards</li><li>a culture of non-violence represented by ahimsa</li><li>strong research labs that could innovate on alternative proteins</li><li>concern about the health and environmental threats that have come with the recent increased consumption of meat</li></ul><p>Here are some challenges:&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefobmka66p10g\"><sup><a href=\"#fnobmka66p10g\">[13]</a></sup></span></p><ul><li>lack of enforcement (and enforcement committees)</li><li>poverty and malnutrition (in 40% of children!), making dietary change for animal welfare less of a priority</li><li>legal issues around funding and censorship</li><li>meat as a status symbol</li><li>desensitization to animal suffering</li><li>lack of interest in animal advocacy careers</li><li>the small animal replacement problem as people avoid eating cows</li></ul><p>So who is going to take on these challenges and fight for the fish in India?</p><h1>Here comes Fish Welfare Initiative</h1><p><i>I don't work for FWI, this is just my interpretation of their online documents!</i></p><p>In 2019, Charity Entrepreneurship (CE) wrote a report entitled \"Improving Environmental Conditions.\"&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefw1zmbbvek2t\"><sup><a href=\"#fnw1zmbbvek2t\">[14]</a></sup></span>&nbsp;The report found that simple environmental changes can improve welfare, and the most promising of these is improvements in water quality for farmed fish. CE estimated that optimizing dissolved oxygen levels would cost-effectively provide 14.75-21.5 welfare points per year.</p><p>Fish lives at that point would still be net-negative,&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjc9u4f2o93l\"><sup><a href=\"#fnjc9u4f2o93l\">[11]</a></sup></span>&nbsp;so it would be important not to increase the number of fish farmed.</p><p>CE noted, as had ACE before them&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy6opy0ua58e\"><sup><a href=\"#fny6opy0ua58e\">[7]</a></sup></span>, that more research would still be needed into optimal dissolved oxygen levels for different species of fish. They also noted that optimal dissolved oxygen depends on other environmental factors.</p><p>But CE found this intervention promising for several reasons:&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefw1zmbbvek2t\"><sup><a href=\"#fnw1zmbbvek2t\">[14]</a></sup></span></p><ul><li>funding and staffing were not expected to be an issue</li><li>improving water quality for fish doesn't look \"extreme\" (although it may look unimportant)</li><li>measuring water quality is affordable and easy</li><li>aerators are affordable, but not too affordable, which might drive price up and demand down</li><li>the intervention can be presented as good for farmers</li><li>feedback loops would be quick</li><li>it could bring new people into the animal advocacy movement</li></ul><p>As a result of this report, Haven King-Nobles and Tom Billington founded Fish Welfare Initiative (FWI) through the 2019 Charity Entrepreneurship incubation program. Their mission was to reduce the suffering of fish as much as possible by identifying and piloting interventions.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhhms3f9ci8h\"><sup><a href=\"#fnhhms3f9ci8h\">[15]</a></sup></span></p><p>Tom made the case that fish welfare is an important, neglected, and possibly tractable cause area. He noted that the base of research on fish welfare had been growing, that aerators for increasing dissolved oxygen were affordable, and that stunning had already been successfully implemented on some farms.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhhms3f9ci8h\"><sup><a href=\"#fnhhms3f9ci8h\">[15]</a></sup></span></p><p>Although the Weighted Animal Welfare Index&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjc9u4f2o93l\"><sup><a href=\"#fnjc9u4f2o93l\">[11]</a></sup></span>&nbsp;had placed wild fish as a priority above farmed fish, Billington argued that FWI would have more impact targeting farmed fish for two reasons. Firstly, humans have more influence over the lives of farmed fish. Secondly, fish farming is a rapidly growing industry, much faster than wild catching.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhhms3f9ci8h\"><sup><a href=\"#fnhhms3f9ci8h\">[15]</a></sup></span></p><p>FWI strove to answer these questions:&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhhms3f9ci8h\"><sup><a href=\"#fnhhms3f9ci8h\">[15]</a></sup></span></p><ul><li>Which species should they focus on?</li><li>Which interventions (welfare factors and campaigns) should they focus on?</li><li>Which countries should they focus on?</li></ul><p>The same year, William Bench had founded the Aquatic Life Institute (ALI).&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefw7jvhg8rld\"><sup><a href=\"#fnw7jvhg8rld\">[16]</a></sup></span>&nbsp;ALI focused on highly tractable countries (in Europe), working mostly with corporations and certifiers. FWI would go on to work in countries with high scale and neglectedness (in Asia), working mostly with farms and farmers.</p><p>Over the last few years, FWI has grown from $50,000 in seed funding to $1,050,000 in total fundraising. They have also grown to 20 staff members.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefi5yxjdy5kp\"><sup><a href=\"#fni5yxjdy5kp\">[17]</a></sup></span></p><p>In FWI's 2021 retrospective, they noted that their key bottleneck was improving their welfare standard. At the time, it was not proving impactful enough to scale up. They also found that their expectations for standardization were unrealistic.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz1e7e7x6f9\"><sup><a href=\"#fnz1e7e7x6f9\">[18]</a></sup></span>&nbsp;Nevertheless, they had already:</p><ul><li>Helped 214,000 fish</li><li>Connected with farms</li><li>Added several people to their team who were previously not a part of the animal advocacy movement</li><li>Secured the first corporate commitment for fish in India</li><li>Established the Alliance for Responsible Aquaculture (ARA)</li><li>Authored several reports&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz1e7e7x6f9\"><sup><a href=\"#fnz1e7e7x6f9\">[18]</a></sup></span></li></ul><p>By the end of 2022, FWI had scaled up significantly.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffnwpgi8fm5g\"><sup><a href=\"#fnfnwpgi8fm5g\">[19]</a></sup></span>&nbsp;They had:</p><ul><li>Helped a total of 1.14 million fish at a cost of $1.13 USD per fish</li><li>Grown the ARA by 53%</li><li>Created a second version of their welfare standard</li><li>Signed two memoranda of understanding</li><li>Become a partner of the Andhra Pradesh state government</li></ul><p>They found they needed more rigorous experiments in the field and more corporate partnerships.</p><p>Between 2021 and 2022, FWI and the ARA had collected the following measurements on partnered farms to evaluate the success of their welfare standard:&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbqjvnmsvqm\"><sup><a href=\"#fnbqjvnmsvqm\">[20]</a></sup></span></p><ul><li>dissolved oxygen</li><li>temperature</li><li>pH</li><li>ammonia</li><li>turbidity</li><li>salinity</li><li>total dissolved solids</li><li>weather</li><li>stocking density</li><li>feeding amount</li><li>disease presence</li><li>lice presence</li><li>behavioural indicators<ul><li>feeding</li><li>air gulping</li><li>tail splashing</li></ul></li><li>mortality</li><li>primary productivity (a proxy for phytoplankton)</li></ul><h1>Future of FWI</h1><p>On January 1, 2023, FWI posted that they were updating their welfare standard to make greater improvements in dissolved oxygen (and thus welfare). They added a standard of three days per month for carp to feed on phytoplankton instead of supplemental feed. Carp naturally feed on phytoplankton, so when they receive supplemental feed (to increase yield), the phytoplankton grow. This destabilizes the level of dissolved oxygen.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqk5t2rnpv47\"><sup><a href=\"#fnqk5t2rnpv47\">[10]</a></sup></span></p><p>This year, FWI will be evaluating Version 2 of their welfare standard and creating a Version 3.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqk5t2rnpv47\"><sup><a href=\"#fnqk5t2rnpv47\">[10]</a></sup></span></p><p>In addition to testing and validating Version 2 of the welfare standard, FWI intends to scale up work in India. They intend to help 1.5 million fish in 2023 and more than double the farms in the ARA.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbiqfsezzplb\"><sup><a href=\"#fnbiqfsezzplb\">[21]</a></sup></span>&nbsp;</p><p>As they do this, they will be implementing a new theory of change-based strategy.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzf6mqew3h6g\"><sup><a href=\"#fnzf6mqew3h6g\">[22]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref18knujvlvvz\"><sup><a href=\"#fn18knujvlvvz\">[23]</a></sup></span>&nbsp;Their activities will be driven more by validating a theory of change for its long-term impacts than by helping as many fish as possible in the short-term. They will be moving away from choosing, implementing, and scaling interventions. Instead, they will be validating their new theory of change.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref18knujvlvvz\"><sup><a href=\"#fn18knujvlvvz\">[23]</a></sup></span></p><p>In their old theory of change, government and corporations would partner with the ARA, require welfare standards, and help fish. In the new theory of change, FWI expects that farmer engagement will drive enrollment in the ARA, and farmers will implement welfare standards and help fish.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzf6mqew3h6g\"><sup><a href=\"#fnzf6mqew3h6g\">[22]</a></sup></span>&nbsp;</p><p><i>I don't work for FWI, this is just my interpretation of their online documents!</i></p><h1>Conclusion</h1><ul><li>Fishing used to be pretty okay.</li><li>Then we fished way too much!</li><li>Wild fish welfare is terrible anyway, though.</li><li>So we started farming.</li><li>It did not help!</li><li>Fish are sentient beings who feel pain, and they suffer a lot on farms, as they live and as they die.</li><li>They might be the highest priority animal to help.</li><li>But we can help!</li><li>We can mostly help by increasing dissolved oxygen.</li><li>Maybe we can help the most in India, where a lot of people care about animals but they also farm and eat a ton of fish.</li><li>Fish Welfare Initiative and Aquatic Life Institute are focusing on cost-effectively helping farmed fish.</li><li>Fish Welfare Initiative will be scaling up and testing a new theory of change this year.</li><li><a href=\"https://www.fishwelfareinitiative.org/donate\">Donate to FWI!</a></li><li><a href=\"https://ali.fish/donate\">Donate to ALI!</a></li></ul><h1>Next steps for me</h1><p>I still have a bunch of reports from Fish Welfare Initiative's <a href=\"https://www.fishwelfareinitiative.org/research\">research page</a> to go through.</p><p>I'll need to read some primary research. I'll start with the works of Fish Welfare Initiative's Fish Welfare Specialist Consultant, <a href=\"https://orcid.org/0000-0001-7237-5053\">Marco Cerqueira.</a> Then I will branch out into related authors and anything cited in the welfare reports I've already read.</p><p>I'll also be taking a course called <i>Large Marine Ecosystems: Assessment and Management</i>. This might teach me more about wild fish, but I think it teach me some useful research methods in biology. There's also a unit on plankton, which is a key contributor to low and changing oxygen levels on fish farms.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh1olc4qpul\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh1olc4qpul\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://en.wikipedia.org/wiki/Cod_fishing_in_Newfoundland\">https://en.wikipedia.org/wiki/Cod_fishing_in_Newfoundland</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn103zkq5jhnbn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref103zkq5jhnbn\">^</a></strong></sup></span><div class=\"footnote-content\"><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=Jmi4MGmKpn4\"><div><iframe src=\"https://www.youtube.com/embed/Jmi4MGmKpn4\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxkzfj1guug\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxkzfj1guug\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://sdg.iisd.org/news/mpa-congress-to-chart-path-to-protecting-30-of-global-ocean-by-2030/\">https://sdg.iisd.org/news/mpa-congress-to-chart-path-to-protecting-30-of-global-ocean-by-2030/</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnq26yppfddy\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefq26yppfddy\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.animalsasia.org/us/media/news/news-archive/fish-farming-the-hidden-animal-welfare-disaster-happening-right-under-our-feet.html\">https://www.animalsasia.org/us/media/news/news-archive/fish-farming-the-hidden-animal-welfare-disaster-happening-right-under-our-feet.html</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlga1u4k3bln\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflga1u4k3bln\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.vox.com/future-perfect/22301931/fish-animal-welfare-plant-based\">https://www.vox.com/future-perfect/22301931/fish-animal-welfare-plant-based</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnx7trlkp13fs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefx7trlkp13fs\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.fishwelfareinitiative.org/post/why-focus-on-fish\">https://www.fishwelfareinitiative.org/post/why-focus-on-fish</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fny6opy0ua58e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefy6opy0ua58e\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://animalcharityevaluators.org/research/reports/farmed-fish-welfare-report/#full-report\">https://animalcharityevaluators.org/research/reports/farmed-fish-welfare-report/#full-report</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndusix7m4h6r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdusix7m4h6r\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.vox.com/future-perfect/2018/11/14/18091698/future-perfect-podcast-killing-fish-ikejime-animal-welfare\">https://www.vox.com/future-perfect/2018/11/14/18091698/future-perfect-podcast-killing-fish-ikejime-animal-welfare</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnd0795picu06\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefd0795picu06\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://80000hours.org/after-hours-podcast/episodes/andres-jimenez-zorrilla-shrimp-welfare-project/#transcript\">https://80000hours.org/after-hours-podcast/episodes/andres-jimenez-zorrilla-shrimp-welfare-project/#transcript</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqk5t2rnpv47\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqk5t2rnpv47\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.fishwelfareinitiative.org/post/announcing-v2\">https://www.fishwelfareinitiative.org/post/announcing-v2</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjc9u4f2o93l\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjc9u4f2o93l\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://docs.google.com/spreadsheets/d/1dWzh0Se0nhbPxe2Ye3o-tr3BoOBtdiCbMkpPuw2rBFE/edit#gid=0\">https://docs.google.com/spreadsheets/d/1dWzh0Se0nhbPxe2Ye3o-tr3BoOBtdiCbMkpPuw2rBFE/edit#gid=0</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxoye6t6wf4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxoye6t6wf4\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.globalseafood.org/advocate/time-to-play-farmed-fish-respond-to-environment-enrichment/\">https://www.globalseafood.org/advocate/time-to-play-farmed-fish-respond-to-environment-enrichment/</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnobmka66p10g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefobmka66p10g\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://animalcharityevaluators.org/blog/animal-advocacy-in-india/\">https://animalcharityevaluators.org/blog/animal-advocacy-in-india/</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnw1zmbbvek2t\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefw1zmbbvek2t\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.charityentrepreneurship.com/animal-welfare-reports\">https://www.charityentrepreneurship.com/animal-welfare-reports</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhhms3f9ci8h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhhms3f9ci8h\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://forum.effectivealtruism.org/posts/iMofrSc86iSR7EiAG/introducing-fish-welfare-initiative-1\">https://forum.effectivealtruism.org/posts/iMofrSc86iSR7EiAG/introducing-fish-welfare-initiative-1</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnw7jvhg8rld\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefw7jvhg8rld\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://forum.effectivealtruism.org/posts/Gxh6BJPwPX8up8mCy/new-ea-fish-orgs-collaboration-between-fish-welfare\">https://forum.effectivealtruism.org/posts/Gxh6BJPwPX8up8mCy/new-ea-fish-orgs-collaboration-between-fish-welfare</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fni5yxjdy5kp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefi5yxjdy5kp\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.charityentrepreneurship.com/fish-welfare-initiative\">https://www.charityentrepreneurship.com/fish-welfare-initiative</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz1e7e7x6f9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz1e7e7x6f9\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://forum.effectivealtruism.org/posts/aumZHEKydFGKStStk/fish-welfare-initiative-the-2-5-year-retrospective\">https://forum.effectivealtruism.org/posts/aumZHEKydFGKStStk/fish-welfare-initiative-the-2-5-year-retrospective</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfnwpgi8fm5g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffnwpgi8fm5g\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.fishwelfareinitiative.org/post/2022review\">https://www.fishwelfareinitiative.org/post/2022review</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbqjvnmsvqm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbqjvnmsvqm\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://docs.google.com/spreadsheets/d/1oynNIK_kWQla1gOX6GTQ3FG7ZInhBwDqGlZ57T5WhbU/edit#gid=1081881344\">https://docs.google.com/spreadsheets/d/1oynNIK_kWQla1gOX6GTQ3FG7ZInhBwDqGlZ57T5WhbU/edit#gid=1081881344</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbiqfsezzplb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbiqfsezzplb\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.fishwelfareinitiative.org/post/2023-plans\">https://www.fishwelfareinitiative.org/post/2023-plans</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzf6mqew3h6g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzf6mqew3h6g\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://docs.google.com/spreadsheets/d/14XXpe46sWomG29KPwEQ_FPQYpi6YCLglcfjthHwhl08/edit#gid=445153855\">https://docs.google.com/spreadsheets/d/14XXpe46sWomG29KPwEQ_FPQYpi6YCLglcfjthHwhl08/edit#gid=445153855</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn18knujvlvvz\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref18knujvlvvz\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.fishwelfareinitiative.org/post/toc-strategy\">https://www.fishwelfareinitiative.org/post/toc-strategy</a>&nbsp;</p></div></li></ol>", "user": {"username": "Spencer Ericson"}}, {"_id": "XYkSZy6DscxLkSjH6", "title": "ALTERING TO URGENCY: EA and Africa  ", "postedAt": "2023-05-07T19:48:19.950Z", "htmlBody": "<p>My very short thought on what the EA community really needs to do right when it comes to supporting talents from Africa.</p>\n<p>(Copied)</p>\n<blockquote>\n<p>Innovation and funding programs that are dedicated to only Africans. Attempting to use the same scorecard for innovations from mature ecosystems (U.S, UK etc.) will lead to excluding promising innovations.</p>\n</blockquote>\n<p>I wish that the leadership of EA orgs will take this really seriously.</p>\n<p>EA needs Africa in order to multiply impact, Africa needs EA to empower young African talents to solve the continent\u2019s and globally shared pressing challenges.</p>\n<p>PS: I am not working directly on this issue, as I am currently focused on nuclear security. However, I would welcome conversations that could provide solutions and would be happy to support such efforts. My email - <a href=\"mailto:Ajudeonuifeanyi@gmail.com\">Ajudeonuifeanyi@gmail.com</a>.</p>\n<p>Cheers!</p>\n", "user": {"username": "Daniel Ajudeonu"}}, {"_id": "nTALzRAWxRnrxvoep", "title": "Implications of the Whitehouse meeting with AI CEOs for AI superintelligence risk - a first-step towards evals?", "postedAt": "2023-05-07T17:33:59.462Z", "htmlBody": "<h1>Introducion</h1><p>On Wednesday 4th May, Sam Altman (Open AI) and Dario Amodei (Anthropic) - amongst others - met with US Vice President Kamala Harris (with a <a href=\"https://twitter.com/POTUS/status/1654237472065302528\">drop-in from President Joe Biden</a>), to discuss the dangers of AI.</p><p><a href=\"https://www.whitehouse.gov/briefing-room/statements-releases/2023/05/04/readout-of-white-house-meeting-with-ceos-on-advancing-responsible-artificial-intelligence-innovation/\"><u>Announcement</u></a> |&nbsp;<a href=\"https://www.whitehouse.gov/briefing-room/statements-releases/2023/05/04/fact-sheet-biden-harris-administration-announces-new-actions-to-promote-responsible-ai-innovation-that-protects-americans-rights-and-safety/\"><u>Fact sheet</u></a> | <a href=\"https://forum.effectivealtruism.org/posts/Cre2YC3hd5DeYLqDH/\">EA Forum linkpost</a></p><p>I spent about 2 hours trying to understand what happened, who was involved, and what its possible implications for superintelligence risk.</p><p>I decided to make this post for two reasons:</p><ol><li><strong>I am practising writing</strong> and developing my opinions on AI strategy (so feedback is very welcome, and&nbsp;<strong>you should treat my epistemic status as \u2018new to this\u2019!)</strong></li><li><strong>I think demystifying the facts of the announcement and offering some tentative conclusions will positively&nbsp;contribute </strong>to the community's understanding of AI-related political developments.</li></ol><h2>My main conclusions</h2><p>Three announcements were made, but the announcement on public model evaluations involving major AI labs seemed most relevant and actionable to me<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefu7ycr520kyi\"><sup><a href=\"#fnu7ycr520kyi\">[1]</a></sup></span>.</p><p>My two actionable conclusions are:</p><ol><li><strong>I think folks with technical alignment expertise</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefie44691sxk\"><sup><a href=\"#fnie44691sxk\">[2]</a></sup></span><strong>&nbsp;should consider attending DEF CON 31</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmfhufla2a5\"><sup><a href=\"#fnmfhufla2a5\">[3]</a></sup></span>&nbsp;if it\u2019s convenient, to help shape the conclusions from the event.</li><li>My main speculative concern is that this evaluation event could positively associate advanced AI and the open source community. <strong>As far as those that feel the downside of model proliferation outweighs the benefits of open sourcing, spreading this message in a more focused way now may be valuable.</strong></li></ol><h1>Summary of the model evaluations announcement</h1><p><i>This is mostly factual, and I\u2019ve flagged where I\u2019m offering my interpretation. Primary source:&nbsp;</i><a href=\"https://aivillage.org/generative%20red%20team/generative-red-team/\"><i><u>AI village announcement</u></i></a><i>.</i></p><p><strong>There\u2019s going to be an evaluation platform made available during a conference called DEF CON 31.</strong> DEF CON 31 is the 31st iteration of&nbsp;<a href=\"https://defcon.org\"><u>DEF CON</u></a>, \u201cthe world\u2019s largest security conference\u201d, taking place in Los Angeles on 10th August 2023. The platform is being organised by a subcommunity at that conference called the&nbsp;<a href=\"https://aivillage.org\"><u>AI village</u></a>.</p><p>The evaluation platform will be provided by&nbsp;<a href=\"https://scale.ai\"><u>Scale AI</u></a>. <strong>The platform will provide \u201ctimed access to LLMs\u201d via laptops available at the conference, and attendees will red-team various models by injecting prompts</strong>. I expect that the humans will then rate the output of the model as good or bad, much like on the ChatGPT platform. There\u2019s a points-based system to encourage participation, and the winner will win a \u201chigh-end Nvidia GPU\u201d.</p><p><strong>The intent of this whole event appears to be to collect adversarial data that the AI organisations in question can use and 'learn from'</strong> (and presumably do more RLHF on). The orgs that signed up include: Anthropic, Google, Hugging Face, Microsoft, NVIDIA, OpenAI, and Stability AI.</p><p><strong>It seems that there won\u2019t be any direct implications for the AI organisations.</strong> They will, by default, be allowed to carry on as normal no matter what is learned at the event.</p><p>I\u2019ll provide more details on what has happened after the takeaways section.</p><h1>Takeaways from the Whitehouse announcement on model evaluations</h1><p><i>I prioritised communicating my takeaways in this section. If you want more factual context to understand exactly what happened and who's involved- see the section below this one.</i></p><p>For the avoidance of doubt,&nbsp;<strong>the Whitehouse announcement on the model evaluation &nbsp;event doesn\u2019t come with any regulatory teeth.</strong></p><p>I don\u2019t mean that as a criticism necessarily; I\u2019m not sure anyone has a concrete proposal for what the evaluation criteria should even be, or how they should be enforced, etc, so it\u2019d be too soon to see an announcement like that.</p><p>That does mean I\u2019m left with the slightly odd conclusion that <strong>all that\u2019s happened is&nbsp;the Whitehouse has endorsed a community red-teaming event</strong> at a conference.</p><p>Nonetheless&nbsp;<strong>I\u2019m cautiously optimistic about this announcement</strong> for a few reasons.</p><p><strong>First off, it strikes me as a test to gain more information about how to proceed with regulation.</strong> Either by accident or design, I think this is just the beginning; it\u2019s now well within the overton window to \u201cevaluate all AI orgs\u2019 models before deployment\u201d. This seems like a potential precedent on which to attach further requirements.</p><p><strong>It also seems encouraging to me that the US Government was able to coordinate AI companies</strong> to agree to commit to a public evaluation before deployment. It\u2019s great to see the governments playing a key role in cutting through competition and gaining consensus. There may have been a stronger proposal on the table that wasn\u2019t agreed to, but at least this was agreed to by everyone in the room.</p><p><strong>The AI governance community could acknowledge these two facts and treat this implementation of evaluations as a lever</strong>. Now might be a good time to find out more about this lever and figure out how to pull it hard. (Reminder - it seems likely to me that folks more involved in governance than me already are doing this, or have good reason to pursue other avenues that I've not considered.)</p><p><strong>Unfortunately, it\u2019s not mentioned anywhere that the red-teaming will explicitly be monitoring for misalignment, or \u2018model ill-intent\u2019</strong>. However I think there will be a significant amount of overlap between the 'harmful outputs' red-teamers will look for by default to the sorts of things the alignment community would look for. If I\u2019m right that there will be overlap,&nbsp;<strong>I\u2019m excited about the ability for the event to raise some amount of credible awareness of misalignment</strong> (from it being at a reasonably official venue, in the \u2018hacker community\u2019).</p><p>Given the above,&nbsp;<strong>I tentatively think folks with technical alignment expertise</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefie44691sxk\"><sup><a href=\"#fnie44691sxk\">[2]</a></sup></span><strong>&nbsp;should consider attending DEF CON</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmfhufla2a5\"><sup><a href=\"#fnmfhufla2a5\">[3]</a></sup></span><strong>.</strong> I don\u2019t know how possible that is because I\u2019ve never been to DEF CON - but I expect that means other members of the alignment community won\u2019t be there either, by default. Again to be clear, I think useful things will be evaluated by competent people at DEF CON, but the results of that workshop appear to be being treated as being used as input for \u201cwhat happens next\u201d so I think it\u2019s important to be in the room.</p><p><strong>Some people raised concerns about Scale AI not being interested in alignment,&nbsp;</strong>yet taking a central role in this announcement.<strong>&nbsp;</strong>It seems true that Scale AI doesn\u2019t tout alignment strongly, but I think their involvement in this is a red herring and is basically unimportant who provides the platform. It sounds like someone told the press-release people that the platform is made by Scale AI, so Scale AI got mentioned.</p><p>I think the platform will just be something like \u201ctakes in text, saves human ratings on responses\u201d. For example, see their&nbsp;<a href=\"https://scale.com/rlhf\"><u>RLHF platform</u></a>. It\u2019s just a way to monetise getting human data into AI companies\u2019 hands - they\u2019re following profit incentives, not engineering the future. Regulation will be what changes the incentive landscape, not middle-orgs like Scale AI.</p><p>That said, I don\u2019t know the details of how the Scale AI platform is built, and there might be differing implementation details if it had been built with alignment in mind. Here I\u2019d refer back to \u201calignment people should get involved with this\u201d, to have the opportunity to raise their voice if anything is wrong. Even more speculatively: there\u2019s probably also opportunity for another evaluation org to apply competitive pressure and prove themselves as the better eval company.</p><p><strong>My main speculative concern is that the evaluation event could produce more memes that positively associate advanced AI and open sourcing,</strong> which I believe could lead to more&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/proliferation\"><u>model proliferation</u></a>. My reasoning is loose, but goes like: given this is quite a light-touch evaluation, the DEF CON community may pat themselves on the back for finding lots of harmful outputs, and the AI companies involved are likely to be positive about the outcome because, by default, they\u2019ll get useful data without any enforcement or further scrutiny. Could that lead people to conclude that open sourcing is the way to go, as \u2018the community knows best\u2019? There\u2019s a bunch of other stories you could tell, so I am not sure about this outcome.</p><h1>More factual details</h1><p>The most relevant of the 3 announcements<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefu7ycr520kyi\"><sup><a href=\"#fnu7ycr520kyi\">[1]</a></sup></span>&nbsp;from the fact sheet was:</p><blockquote><p>The Administration is announcing an independent commitment from leading AI developers, including Anthropic, Google, Hugging Face, Microsoft, NVIDIA, OpenAI, and Stability AI,&nbsp;<strong>to participate in a public evaluation of AI systems, consistent with responsible disclosure principles</strong>\u2014on an evaluation platform developed by Scale AI\u2014<strong>at the AI Village at DEF CON 31.</strong></p></blockquote><p>Let\u2019s unpack everything in that statement one by one.</p><h2>What\u2019s DEF CON 31 and the AI village?</h2><p><i>Primary source:&nbsp;</i><a href=\"https://en.wikipedia.org/wiki/DEF_CON\"><i><u>Wikipedia</u></i></a> and the&nbsp;<a href=\"https://aivillage.org/blog/\"><u>AI village blog</u></a>.</p><h3>DEF CON is a conference</h3><p><strong>DEF CON is a conference founded by members of the internet security community.</strong> It\u2019s not typically an AI / ML conference (I don\u2019t think I\u2019d heard about it as a UK based ML engineer in 2021) and it's not an academic conference. More like a coming together of hackers to convene, do cool projects, and discuss industry best practices. Typically in security, though the scope has grown as the conference became more popular.</p><p>I was a bit shocked at the presentation of the&nbsp;<a href=\"https://defcon.org\"><u>website</u></a> at first, given its presidential endorsement. However, I think it needs to be understood in the context of it being founded by the internet security community: they historically have a culture of anonymity and using aliases.</p><p>It seems in fact to be the most respected conference in the computer security world, with some high-ranking attendees:</p><blockquote><p>Federal law enforcement agents from the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Federal_Bureau_of_Investigation\"><u>FBI</u></a>,&nbsp;<a href=\"https://en.wikipedia.org/wiki/United_States_Department_of_Defense\"><u>DoD</u></a>,&nbsp;<a href=\"https://en.wikipedia.org/wiki/United_States_Postal_Inspection_Service\"><u>United States Postal Inspection Service</u></a>,&nbsp;<a href=\"https://en.wikipedia.org/wiki/United_States_Department_of_Homeland_Security\"><u>DHS</u></a> (via&nbsp;<a href=\"https://en.wikipedia.org/wiki/Cybersecurity_and_Infrastructure_Security_Agency\"><u>CISA</u></a>) and other agencies regularly attend DEF CON.<a href=\"https://en.wikipedia.org/wiki/DEF_CON#cite_note-Zetter-3\"><u>[3]</u></a><a href=\"https://en.wikipedia.org/wiki/DEF_CON#cite_note-4\"><u>[4]</u></a></p></blockquote><p>It also has precedent interacting with the US government:</p><blockquote><p>In April 2017, a DEF CON Black Badge was featured in an exhibit<a href=\"https://en.wikipedia.org/wiki/DEF_CON#cite_note-15\"><u>[15]</u></a> in the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Smithsonian_Institution\"><u>Smithsonian Institution</u></a>'s&nbsp;<a href=\"https://en.wikipedia.org/wiki/National_Museum_of_American_History\"><u>National Museum of American History</u></a> entitled \"Innovations in Defense: Artificial Intelligence and the Challenge of Cybersecurity\". The badge belongs to ForAllSecure's Mayhem Cyber Reasoning System,<a href=\"https://en.wikipedia.org/wiki/DEF_CON#cite_note-16\"><u>[16]</u></a> the winner of the&nbsp;<a href=\"https://en.wikipedia.org/wiki/DARPA\"><u>DARPA</u></a>&nbsp;<a href=\"https://en.wikipedia.org/wiki/2016_Cyber_Grand_Challenge\"><u>2016 Cyber Grand Challenge</u></a> at DEF CON 24 and the first non-human entity ever to earn a Black Badge.</p></blockquote><h3>The AI Village is a subcommunity</h3><p><a href=\"https://aivillage.org/\"><strong><u>The AI village</u></strong></a><strong> is a subcommunity at DEF CON</strong>. It says it\u2019s \u201ca community of hackers and data scientists working to educate the world on the use and abuse of artificial intelligence in security and privacy\u201d.</p><p>It has a few writings on its&nbsp;<a href=\"https://aivillage.org/blog\"><u>blog</u></a> about adversarial robustness, which look to be sharing reasonable industry best-practice to me at a glance. I found its article on&nbsp;<a href=\"https://aivillage.org/adversarial%20ml/spherical-cow/\"><u>ML security</u></a> a little bit limited in scope, though. It mostly based its conclusions on a framing of system accuracy, neglecting to incorporate general and generative AI (and thus alignment). I found this surprising given it will be hosting the red teaming event for generative, general-ish AI systems.</p><p>Finally, it has surprisingly few people involved in it as far as I can tell from the website and no weighty credentials thrown around, given the weighty endorsement it\u2019s just received from the Whitehouse.</p><h2>What\u2019s Scale AI?</h2><p><strong>Scale AI is a Machine Learning operations platform</strong> founded in 2016 in San Francisco, with 600 employees. They have various products that help ML companies scale and deploy their models.</p><p>Its mission is \u201cto accelerate the development of AI applications\u201d.</p><p>Notably they have a&nbsp;<a href=\"https://scale.com/rlhf\"><u>platform</u></a> for helping ML companies do Reinforcement Learning from Human Feedback (RLHF), claiming it helps with alignment. There\u2019s no particular nuance about whether it fully aligns models. As I said above, I get the sense it\u2019s just following profit incentives and supporting other orgs, rather than building anything itself.</p><h2>Which other actors are involved?</h2><p>For those interested, I noticed a couple of other actors who aren\u2019t mentioned in the Whitehouse briefings were co-authors on the AI village&nbsp;<a href=\"https://aivillage.org/generative%20red%20team/generative-red-team/\"><u>announcement post</u></a>. I haven't got a lot to say about them, but I'm including this information for completeness.</p><p><a href=\"https://www.linkedin.com/feed/update/urn:li:activity:7059949690447949824/\"><u>Rumman Chowdhury</u></a> (<a href=\"https://www.humane-intelligence.org/\"><u>Humane Intelligence</u></a>)</p><blockquote><p>We focus on safety, ethics, and subject specific expertise (e.g. medical). We are suited for any company creating consumer-facing AI products, but in particular generative AI products.</p></blockquote><p><a href=\"https://political-science.uchicago.edu/directory/austin-carson\"><u>Austin Carson</u></a>, Political Science at University of Chicago.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnu7ycr520kyi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefu7ycr520kyi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I largely ignored 2 of the 3 announcments the Whitehouse made:</p><p>1. <strong>Further policies on AI development are in the works.</strong> Whilst those policies might end up being important, no details were given in the announcement, so an outsider like me can't really comment on that yet.</p><p><strong>2. More funding was announced for American AI research and development (R&amp;D).</strong> I don't expect these to be important for alignment reserarch, on the margin, but some funds may be leveragable depending on the exact details.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnie44691sxk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefie44691sxk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;E.g. PhD students who\u2019ve worked on alignment, and others who have published.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmfhufla2a5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmfhufla2a5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I note it's a costly recommendation for alignment researchers to visit a conference in LA.</p><p>Some more input on whether DEF CON is likely to be an important event from someone who understands the US policy world better than me would be useful before you book a flight.</p><p>To be transparent, my reasoning for recommending this is: the Whitehouse endorsed this event and are well networked with the workshop\u2019s stakeholders, so probably the reported outcome of the workshop will be important for the shape of policies that come next.</p></div></li></ol>", "user": {"username": "j_bernardi"}}, {"_id": "ZwdjLsyBXF4vSLWiY", "title": "How much would billionaire philanthropists need to scale up their giving to meet the Giving Pledge in their lifetime?", "postedAt": "2023-05-07T15:32:36.738Z", "htmlBody": "<p>The idea of creating a <a href=\"https://forum.effectivealtruism.org/posts/5KsrEWEbc4mwzMTLp/some-more-projects-i-d-like-to-see#Billionaire_impact_list\">billionaire</a> <a href=\"https://forum.effectivealtruism.org/posts/LCJa4AAi7YBcyro2H/proposal-impact-list-like-the-forbes-list-except-for-impact\">impact</a> <a href=\"https://forum.effectivealtruism.org/posts/KigFfo4TN7jZTcqNH/the-future-fund-s-project-ideas-competition?commentId=vi7zALLALF39R6exF\">list</a> has been floating around for years. Some people might already be <a href=\"https://forum.effectivealtruism.org/posts/LCJa4AAi7YBcyro2H/proposal-impact-list-like-the-forbes-list-except-for-impact?commentId=LdhMp8icxj5pxeyvQ\">working</a> on it, but I haven't heard anything recently.</p><p>Here is another feature that could be added to such a list:&nbsp;</p><ol><li>Take the list of billionaires who have taken the <a href=\"https://givingpledge.org/about\">Giving Pledge</a>.</li><li>List their current wealth and their previous yearly donation amount.</li><li>Based on past growth of their wealth and annual giving trend, calculate when they would be expected to have donated 90% or 99% of their wealth.</li><li>Estimate whether they are on track to donate 90% or 99% within their expected lifetime.</li><li>If they are not on track, <strong>calculate by how much they would need to scale up their annual giving to accomplish donating 90% or 99% of their wealth within their lifetime.&nbsp;</strong></li></ol><p>IIRC, there was a post by <a href=\"https://forum.effectivealtruism.org/users/benjamin_todd?mention=user\">@Benjamin_Todd</a> where they estimated the annual donation amount of the Bill &amp; Melinda Gates Foundation and the wealth of Gates. I think the bottom line was that even though the Gates Foundation is donating billions of dollars annually, the total wealth of Gates is still growing, and their annual donations would need to be scaled up massively to achieve giving 99% within Gates' lifetime.&nbsp;</p><p>(<i>Unfortunately, I can't find the post right now. It might have been a tweet. If you point me to it, I will edit this.)</i></p><p>Presumably, this is the case for most billionaire philanthropists. <strong>Creating visibility around this \"giving too slowly\" issue might motivate billionaire philanthropists to donate more money sooner.&nbsp;</strong></p><p>It appears to be difficult to donate tens of billions of dollars effectively. I had the idea for this post while listening to the Hear This Idea podcast, <a href=\"https://hearthisidea.com/episodes/stewart\">Rory Stewart on GiveDirectly and Massively Scaling Cash Transfers</a>. It offers a bold vision of how unconditional cash transfers might be an intervention that could be effectively scaled up to use hundreds of billions of dollars of donations. Maybe billionaire philanthropists who are \"giving too slowly\" might even be nudged to donate billions to GiveDirectly.&nbsp;</p>", "user": {"username": "MaxG"}}, {"_id": "LgscQde9vQW4xLrjC", "title": "On Child Wasting, Mega-Charities, and Measurability Bias", "postedAt": "2023-05-07T14:55:50.352Z", "htmlBody": "<p>Recently I ran into a volunteer for UNICEF who was gathering donations for helping malnourished children. He gave me some explanation on why child wasting is a serious problem and how there are cheap ways to help children who are suffering from it (the UNICEF website has some information on <a href=\"https://www.unicef.org/nutrition/child-wasting\">child wasting</a> and specifically on the <a href=\"https://www.unicef.org/documents/rapid-review-treatment-wasting-using-simplified-approaches\">treatment of wasting using simplified approaches</a>, in case you are interested).</p>\n<p>Since I happen to have taken the Giving What We Can pledge and have read quite a bit on comparing charities, I asked what evidence there is that compares this action to - say - <a href=\"https://www.againstmalaria.com/\">protecting people from malaria with bednets</a> or <a href=\"https://www.givedirectly.org/\">directly giving cash to very poor people</a>. The response I got was quite specific: the volunteer claimed that UNICEF can save a life with just 1\u20ac a day for an average period of 7 months. If these claims are true then that means they can save a life for 210\u20ac, a lot less than the &gt;3000$ that Givewell estimates is needed for AMF to save one life. Probably these numbers should not be compared directly, but I am still curious to know why there can be over an order of magnitude difference between the two. So to practice my critical thinking on these kinds of questions, I made a list of possible explanations for the difference:</p>\n<ol>\n<li>The UNICEF campaign has little room for additional funding.</li>\n<li>The program would be funded anyway from other sources (e.g. governments).</li>\n<li>The 1\u20ac/day figure might not include all the costs.</li>\n<li>Some of the children who receive the food supplements might die of malnutrition anyway.</li>\n<li>Only some of the children who receive the food supplements would have died without them.</li>\n<li>Children who are saved from malnutrition could still die of other causes.</li>\n</ol>\n<p>Obviously I do not have the time nor resources of GiveWell so it is hard to determine how much all of these explanations count in the overall picture, or if there are others that I missed. Unfortunately, there does not seem to be much information on this question from GiveWell (or other EA organizations) either. Looking on the GiveWell website, the most I could find is <a href=\"https://blog.givewell.org/2011/12/28/mega-charities/\">this blog post on mega-charities from 2011</a>, which makes the argument that mega-charities like UNICEF have too many different campaigns running simultaneously, and that they do not have the required transparency for a proper evaluation. The first argument sounds fake to me: if there are different campaigns, then can you not just evaluate these individual campaigns, or at least the most promising ones? The second point about transparency is a real problem, but there is also the risk of measurability bias if we never even <em>consider</em> less transparent charities.</p>\n<p>I would very much like to have a more convincing argument for why these kind of charities are not rated.  If for nothing else then at least it would be useful for discussing with people who currently donate to them, or who try to convince <em>me</em> to donate to them. Perhaps the reason is just a lack of resources at GiveWell, or perhaps there <em>is</em> research on this but I just couldn't find it. But either way I believe the current state of affairs does not provide a convincing case of why the biggest EA evaluator barely even mentions one of the largest and most respected charity organizations.</p>\n<p>[Comment: I'm not new here but I'm mostly a lurker on this forum. I'm open to criticism on my writing style and epistemics as long as you're kind!]</p>\n", "user": {"username": "Jesper"}}, {"_id": "rQv9eTjoKsdcCqsyw", "title": "Integrating the PACS Institute into the Effective Altruism Movement: Seeking Reflections and Advice", "postedAt": "2023-05-07T21:02:32.750Z", "htmlBody": "<h1>Introduction</h1><p>The Peace and Conflict Science Institute (PACS Institute) is an independent, non-profit research organization dedicated to deepening our understanding of peace and its potential to create a better world. The PACS Institute conducts interdisciplinary research at the intersection of philosophy, behavioral science, cognitive neuroscience, and political science. As the Effective Altruism (EA) community shares the goal of reducing suffering and promoting well-being on a global scale, the PACS Institute is looking to integrate its work under the EA umbrella and attract funding from EA funding bodies. This blog post seeks reflections, insights, and advice from the EA community on how to best achieve this integration.</p><h1>Shared Goals and Interconnectedness</h1><p>As Emil Wasteson, Executive Director of EA Sweden, has noted, peace studies and effective altruism share a common goal of reducing suffering and promoting well-being on a global scale. Both fields recognize the interconnectedness of global issues and the importance of considering the long-term consequences of our actions. Integrating the PACS Institute into the EA movement can help address problems like reducing the risk of great power war, increasing collaboration for a safer development of AI, and other emerging technologies, as well as helping more people out of poverty.</p><h1>Peace Engineering and Longtermism</h1><p>Anders Sandberg, Senior Research Fellow at the Future of Humanity Institute, has emphasized the potential of \"peace engineering\" to reduce the risk of conflict. Given the significant human-caused disasters such as wars and democide, finding interventions that reduce the harm caused by armed conflict can be high-impact. Moreover, considering the long-term implications, reducing the risk of great power war is a high-priority goal for the EA movement to reduce the risk of extinction or astronomical suffering.</p><h1>Seeking Reflections and Advice</h1><p>As the PACS Institute looks to integrate its work into the EA movement, we seek reflections and advice from the community on the following questions:</p><ol><li>How can the PACS Institute effectively communicate the alignment of its mission with the goals of the EA movement?</li><li>What strategies can the PACS Institute adopt to attract funding from EA funding bodies?</li><li>Are there any specific projects or research areas that the PACS Institute should prioritize to align with EA principles and focus areas?</li><li>How can the PACS Institute collaborate with existing EA organizations and initiatives to maximize its impact?</li><li>Are there any challenges or concerns that the PACS Institute should be aware of when integrating into the EA movement?</li></ol><h1>Conclusion</h1><p>The PACS Institute is excited about the prospect of joining forces with the Effective Altruism community, as both fields share a common vision of creating a better, more peaceful world. By seeking reflections and advice from the EA community, the PACS Institute aims to strengthen its integration into the movement and more effectively work towards the shared goals of reducing suffering and promoting well-being on a global scale. Your insights and suggestions are invaluable in this endeavor, and we look forward to a fruitful collaboration.</p>", "user": {"username": "AndersReagan"}}, {"_id": "dntYZ44ySurKAZjcz", "title": "The 6E Essay", "postedAt": "2023-05-07T13:56:03.699Z", "htmlBody": "<h1><strong>The 6E Essay</strong></h1><p>~~~~~~~~~~~~~~~</p><p><strong>An Essay&nbsp;on: Existential Catastrophe, Effective Altruism, </strong><i><strong>The EthiSizer</strong></i><strong>, &amp; &nbsp;Everyone&nbsp;</strong></p><p><i><strong>(* Even... You)</strong></i></p><figure class=\"image image_resized\" style=\"width:83.88%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/sxdqnym5ga0swlfwszpj\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/fgoofxkwucprqhs0lf3i 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/xttbtf89h8lrpcjclw1u 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/fbbnw6qi0rbwcr65qg0z 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/rbs7h2jsoqzktjcfto1t 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/zxh8rilxpx7mh8rgvadh 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/fymzwxxcctvfkihyx8ju 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/fsrdjijg0gygmcaojfig 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/bzjmbifryohlx6qafem0 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/cqwqxjujuphendfiocwv 810w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/jsbkd1qglusgyji0pjxy 888w\"></figure><p><i><strong>A Call-To-Adventure / Call-To-Action &nbsp;Essay</strong></i></p><p><i>by Velikovsky of Newcastle - <strong>May 7th, 02023 - </strong></i>(Reposted from <a href=\"https://evolutionary-culturology.blogspot.com/2023/05/the-6e-essay.html\"><i>the Ev Cult weblog</i></a>.)</p><p>----------------//---------------</p><p><i>Note: For </i><a href=\"https://forum.effectivealtruism.org/topics/longtermism\"><i>Longtermism</i></a><i> reasons, this essay uses </i><a href=\"https://longnow.org/\"><i>The Long Now 10,000-Year Calendar</i></a><i> (...so it's currently the year <strong>02023</strong>, and not: 2023).</i></p><p>----------------//---------------</p><p><strong>ESSAY QUESTION&nbsp;</strong></p><p><strong>from the </strong><a href=\"https://www.openphilanthropy.org/open-philanthropy-ai-worldviews-contest/\"><i><strong>02023 Open Philanthropy AI Worldviews Contest website</strong></i></a> :</p><blockquote><p><strong>Q: Conditional on AGI being developed by 2070, what is the probability that humanity will suffer an&nbsp;</strong><a href=\"https://forum.effectivealtruism.org/topics/existential-catastrophe-1\"><strong>existential catastrophe</strong></a><strong>&nbsp;due to loss of control over an AGI system?</strong></p><p>`An existential catastrophe is an event that destroys humanity's&nbsp;</p><p>long-term potential.\u2019</p></blockquote><p><strong>Source: </strong><a href=\"https://forum.effectivealtruism.org/topics/existential-catastrophe-1\"><strong>https://forum.effectivealtruism.org/topics/existential-catastrophe-1</strong></a></p><p><i>----------------//---------------</i></p><p><strong>THE 6E ESSAY ~ </strong><i><strong>by Velikovsky of Newcastle</strong></i></p><h1><strong>INTRODUCTION</strong></h1><p><i>We\u2019re all going to die one day</i> \u2013 but it\u2019s (probably) way better if we <i>don\u2019t</i> do it:&nbsp;</p><blockquote><p>(a) all at once, and,&nbsp;</p></blockquote><blockquote><p>(b) before we\u2019re all good and ready.</p></blockquote><h2><strong>Some Background / Context:</strong></h2><p>In May 02023, AI Safety went mainstream globally, thanks primarily to the&nbsp;<a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\"><i>Pause Giant AI Experiments Open Letter</i></a>...&nbsp;&nbsp;</p><p>And so, in a nutshell:</p><h3><strong>The Problem-Statement:</strong></h3><p>Superintelligence (advanced Artificial General Intelligence) may kill us all <i>sometime soon (including by the year 02070)</i>, unless we solve:</p><blockquote><p>(a) the AGI <strong>control problem</strong>,</p><p>and</p><p>(b) the AGI <strong>human-values-alignment problem</strong>...</p></blockquote><p>----------------//---------------</p><p><i><strong>Some Solutions That Follow from the above:&nbsp;</strong></i></p><h3><strong>A Further Global </strong><i><strong>Call To Action</strong></i></h3><p>Below are some cool altruistic ways you (personally) can help save the humanimal race (all of us, and Earth life) from imminent existential catastrophe - and hopefully even have fun doing it...</p><h3><strong>A short Summary of this essay:</strong></h3><ol><li>A `<a href=\"https://nickbostrom.com/fut/singleton\">Singleton</a>\u2019 is: a <strong>global coordinator</strong> (and which we all need, for: Superintelligence Safety, at any rate)</li><li><i>The EthiSizer</i> is a <strong>`toy\u2019 singleton</strong>\u2026 and aims to ensure <i>Singleton Superintelligence Safety</i>...</li><li>And here\u2019s the skinny on how you (personally) can have fun, playing with and/or even <i>making</i>:</li></ol><blockquote><p>o&nbsp;&nbsp; <i><strong>EthiSizer</strong></i><strong> games</strong>,</p><p>o&nbsp;&nbsp; <i><strong>EthiSizer</strong></i><strong>&nbsp;smartphone apps</strong>,</p><p>and</p><p>o&nbsp;&nbsp; An <i><strong>EthiSizer</strong></i><strong> Global Governance (EGG) simulator</strong>.</p></blockquote><p>As frankly, anything you can do in this space, will help us all solve <strong>The Singleton Superintelligence Safety Problem</strong>.</p><p>\u2026Some say it\u2019s a brilliant, and <i><strong>creative</strong></i><strong> (</strong><i><strong>new, useful, and surprising</strong></i><strong>)</strong> idea...?</p><p>(It wasn\u2019t <i>my</i> idea... An A.I. that I made, called <i>The EthiSizer,</i> suggested it.)&nbsp;</p><p>Now, let's&nbsp;<i>change up </i>the <i>order</i> of the <i>parts</i> of this essay from the usual <i><strong>ILMRADC</strong></i> structure (the old: <i><strong>I</strong>ntroduction, <strong>L</strong>iterature Review, <strong>M</strong>ethod, <strong>R</strong>esults, <strong>A</strong>nalysis, <strong>D</strong>iscussion, <strong>C</strong>onclusion</i>)\u2026</p><p>And, instead \u2013 for the Structure Of This Particular Essay \u2013 we\u2019ll go with:</p><blockquote><blockquote><p>\u00b7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Introduction</strong> (\u2026you\u2019re almost finished reading it, keep going!)</p></blockquote><blockquote><p>\u00b7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Here\u2019s What </strong><i><strong>Earth/Humanity</strong></i><strong> Needs You To Do\u2026</strong></p></blockquote><blockquote><p>o&nbsp; &nbsp;<strong>Some Examples...&nbsp;</strong>&nbsp;</p></blockquote><blockquote><p>\u00b7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Methods You Can Use, Right Now</strong></p></blockquote><blockquote><p>\u00b7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Post Your Results! </strong><i><strong>(Show Us What You Got!)</strong></i></p></blockquote><blockquote><p>\u00b7&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<i><strong>On Heroism Science</strong></i><strong> (or, Global Heroism on five dollars a day)</strong></p></blockquote><blockquote><p>\u00b7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Literature Review: Bostrom (02014 &amp; 02022) on Singletons, vs. Multipolars</strong></p></blockquote><blockquote><p>\u00b7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>But Wait\u2026 What if </strong><i><strong>The EthiSizer</strong></i><strong> is </strong><i><strong>Roko\u2019s Basilisk</strong></i><strong> ?</strong></p></blockquote><blockquote><p>\u00b7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>The End (of The Beginning)</strong></p></blockquote></blockquote><p>End of: <i><strong>The Introduction</strong></i>.</p><p><i>(...Yay, keep going!)</i><br>&nbsp;</p><p><i>On with the show.</i></p><p>&nbsp;</p><p><i>----------------//---------------</i></p><p>&nbsp;</p><p><strong>HERE\u2019S WHAT </strong><i><strong>EARTH/HUMANITY</strong></i><strong> NEEDS YOU TO DO\u2026</strong></p><p>&nbsp;</p><p>So, with your help, <i>The EthiSizer</i> `rolls out' gradually, in 3 phases:</p><blockquote><blockquote><blockquote><p>1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i>The EthiSizer</i> Games...</p></blockquote></blockquote><blockquote><blockquote><p>2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i>EthiSizer</i> smartphone-apps...</p></blockquote></blockquote><blockquote><blockquote><p>3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i>The EthiSizer</i> Global Governor (or, the EGG).</p></blockquote></blockquote></blockquote><p>And here\u2019s what we need <i><strong>you</strong></i> to do:</p><p><strong>...You need to start playing - and giving feedback on -&nbsp;</strong><i><strong>EthiSizer</strong></i><strong> games!</strong></p><p><i><strong>...</strong></i><strong>And</strong><i><strong> Why-?</strong>&nbsp;</i></p><p>Because the more people that play them, the more <i>feedback</i> (information) we get from humanity, on what humanity's&nbsp;<i><strong>Values</strong></i> actually are...&nbsp;</p><p>(Also, the <i>EthiSizer</i> games are free, and fun to play).</p><p><strong>And - If you can, you also need to start making:&nbsp;</strong><i><strong>EthiSizer</strong></i><strong>&nbsp;Games!</strong></p><p><strong>...And&nbsp;</strong><i><strong>Why-?</strong></i>&nbsp;</p><p>The more people that make them, the more <i>EthiSizer</i> games (sims) there are, for people to play... Thus, the more useful <i>feedback and information</i> we get, when people play those games.</p><p><i>----------------//---------------</i></p><p><strong>SOME FASCINATING FACTOIDS\u2026</strong></p><p>Consider, how much time people spend, <i><strong>playing</strong></i><strong>&nbsp;videogames</strong>...&nbsp;</p><p>It's a <i>lot</i>.&nbsp;(<a href=\"https://www.statista.com/topics/868/video-games/#dossier-chapter6\">14 minutes per day, per capita, in the US - and that's just on videogame consoles, not even pc/phone games</a>)...</p><p>Now, consider how much <i>time &amp; money</i> people spend, <i><strong>making</strong></i><strong>&nbsp;videogames</strong>...&nbsp;</p><p>It's also a <i>lot</i>.&nbsp;(<a href=\"https://www.statista.com/topics/868/video-games/#topicOverview\">USD$97B per year in the US</a>)&nbsp;</p><p>&nbsp;</p><p><strong>HEAR THAT? OPPORTUNITY KNOCKS...</strong></p><p>\u2026We all could turn that currently-wasted opportunity into:&nbsp;<i>saving humanity's &amp; Earth's collective future</i>.&nbsp;</p><p>To some, it sounds crazy: <i><strong>Play (and also design, and make) games, to save the world...</strong></i></p><p>(But it would also be <i>super-embarrassing </i>for <i>the humanimal-species/Earth-life </i>to be <i>too slow (or even lazy) to survive, and thus, go extinct very soon</i>\u2026 We'd suffer: <i>Global Natural De-Selection</i>... <i>Ouch.</i>)</p><p><i>----------------//---------------</i></p><h3><strong>SOME EXAMPLES...&nbsp;</strong></h3><p><strong>An Example </strong><i><strong>EthiSizer</strong></i><strong> </strong><i><strong>Game:</strong></i></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ykl4dne8udhbgn7huqnh\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ytl1kiqiqgd4nmbklxfe 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/s0ys6xcbmq9kjeb8mqpg 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/nmx2b2wnr9k9jh8jeeeb 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/igpuqrzpik6ivuz99tqa 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/mjprd01sugqprvrmnxro 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/p74ralmxbwlminxq2mc1 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/wxztsvkuhnwfaowrtdzv 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/gt5ok5xxemtp6uxzqjxv 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/fjle7k9iqmr1k79ovcu2 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/xjzznpx9fgtejpu3lb6g 1920w\"></figure><p><a href=\"https://www.youtube.com/watch?v=ncy7JipCTEc\"><i><strong>The EthiSizer</strong></i><strong> - Web-browser Game</strong></a><strong>&nbsp;(2 min 30 sec)</strong></p><p>For more examples, see also <a href=\"https://the-ethisizer.blogspot.com/2023/01/the-ethisizer-games.html\">this post on: <i><strong>EthiSizer </strong></i><strong>Games</strong></a>.</p><p>&nbsp;</p><p><strong>An Example </strong><i><strong>EthiSizer</strong></i><strong> </strong><i><strong>smartphone app:</strong></i></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/nj5ahgviark115gpedmy\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/vixdrbnbx8x7gzlabwow 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/sywsij4uxmpjp0fenykk 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/rk9hfmm4moqoijubj9yp 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/jkfugcjzihwacmg0scxe 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/pensbfh9irllynium393 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ala56xbwsipckd7rx2py 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ncu0fimql2bpqtjgbqwy 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/tsf2wm1q2aqjbxh2vxia 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/pshzqvakvx1koldewfic 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/p6i4sx1pn4ifvwhgnvh0 1920w\"></figure><p><a href=\"https://www.youtube.com/watch?v=wZmbaRaGQAQ\"><i><strong>The EthiSizer </strong></i><strong>- the Smart-Phone Smart-App (Version 3)</strong></a><strong>&nbsp;(1 min 20 sec)</strong></p><p>For more examples, see also <a href=\"https://the-ethisizer.blogspot.com/2023/01/the-ethisizer-in-2mins30secs.html\"><strong>this post on: EthiSizer Smartphone Apps</strong></a><strong>.</strong></p><p>&nbsp;</p><p><strong>An Example </strong><i><strong>EthiSizer</strong></i><strong> </strong><i><strong>Global Governor </strong></i><strong>(EGG):</strong></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/rsb2bvbb11psgbwtkvny\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/xgescygb2pw3di48rsbn 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/cxxznvktfcx4gxv62g41 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/stcosovnkcu0kj33kjez 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/xadafewsxiiw5ogxlx23 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/jfdawbfnubw6gvgzvc2d 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/k7eu8xnm8cs94cnqphzb 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/wwhinqwmzb5ryt3bwbf0 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/yowu7ubnfzylo1ikleuy 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/etl2599jytmvuvqpycj0 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/x3nd4qvgq7g6sqzf8oe9 1920w\"></figure><p><a href=\"https://www.youtube.com/watch?v=bpjVmG6zCmI\"><i><strong>The EthiSizer </strong></i><strong>Global Governor</strong></a><strong> (2 mins 30 secs)</strong><br>&nbsp;</p><p><strong>Example </strong><i><strong>Detailed Instructions,&nbsp;</strong></i><strong>on making an example </strong><i><strong>EthiSizer </strong>(in Book format)</i></p><p>A book that was written an <i>EthiSizer AI</i>, about:&nbsp;<i><strong>How to create an</strong></i><strong> </strong><i><strong>EthiSizer.</strong></i>&nbsp;</p><figure class=\"image image_resized\" style=\"width:42.67%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/oczwurl6vf5utchsmn7v\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/furbydm8gnlfu1pyqslv 114w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/n7anqnkrdax28ywcncjx 194w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/tw9gadf5utb7wiee0f13 274w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/tnbrph8plgc0fw0fisr8 354w\"><figcaption><i>The EthiSizer</i> book (02022)</figcaption></figure><p><a href=\"https://www.amazon.com/dp/B0BPGQCBVX\"><i><strong>The EthiSizer Book&nbsp;</strong></i><strong>- by The EthiSizer</strong> (02022)</a>&nbsp;</p><p>...a book <i>by an AI</i>&nbsp;<i>Superintelligence Singleton Simulator</i>,&nbsp;</p><p>on <i>how to build that <strong>AISSS</strong></i>&nbsp;</p><p>(...a weird form of meta-bootstrapping...)</p><p>&nbsp;</p><p><i>----------------//---------------</i></p><p><strong>SOME METHODS YOU CAN USE...</strong></p><p>One current example of a `toy singleton' (or, Earth's <a href=\"https://en.wikipedia.org/wiki/Digital_twin\"><i>digital twin</i></a>) is called:&nbsp;<i><strong>The EthiSizer</strong>. </i>If and/or When you make yours, by all means use that same <i>name/title</i> if desired - or think of another one. Randomly, some other possible <i>names/titles</i>&nbsp;might include:</p><ul><li><i>The Moralizer</i></li><li><i>The Global Unifier</i></li><li><i>The Global Coordinator</i></li><li><i>The Global Pacifier</i></li><li><i>...etc.&nbsp;</i></li></ul><p><i>(...You get the idea... Feel free to get creative with it.)</i></p><p>These ideas below are also intended to inspire...</p><p><strong>Some Example </strong><i><strong>EthiSizer </strong></i><strong>weblogs:</strong></p><blockquote><p>Make your own weblogs about <i>The EthiSizer (or Insert-Name-Here) -</i>&nbsp;like, say:</p></blockquote><blockquote><p><a href=\"https://the-ethisizer.blogspot.com/\"><strong>The </strong><i><strong>EthiSizer</strong></i><strong> blog</strong></a></p></blockquote><p>&amp;</p><blockquote><p><a href=\"https://ethisizer-novel.blogspot.com/\"><strong>The </strong><i><strong>EthiSizer</strong></i><strong> book-blog</strong></a></p></blockquote><p>&nbsp;</p><p>Or by all means, <i>ignore those</i> - but please do, start your own...</p><p><strong>Some Example Game-Maker apps:</strong></p><blockquote><p>Ask <i>Uncle Google</i>: <strong>\"free game maker apps\" </strong>(obviously, you may need to add \"for pc\" / \"for Mac\" / \"for UNIX\")</p></blockquote><p>Some schoolkids have used the (free)&nbsp;<a href=\"http://www.alice.org/\"><i>Alice 3.0</i></a>&nbsp;gamemaker software to make <i>EthiSizer</i> games, and machinima movies... (a <i>Machinima Movie</i> is where you record and repurpose gameplay, into: a narrative...). Some examples:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/qhestnsldxrnakiml09f\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/gwu0l5ys827olfvxpsf6 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/gsjfzoxcj7zclglqd8wm 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ronswieyw6n6i0vundr5 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/tzorh9w5l2zptahwmilz 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/cbivfdz4x3jsonargnm5 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/t9bd406qc4kapsix3dyr 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/mwdqxypw3ssien7nfg6e 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ekpxmlpbi3dc9fqdruzf 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/s5679ok6nnirictg3vuc 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/cbgpmyalclhff7djfmyw 1920w\"></figure><p><a href=\"https://www.youtube.com/watch?v=Uax0eFIGMCk\"><i><strong>\"But Dad...\" - a 1MMM (1-Minute Machinima Movie) about: The EthiSizer</strong></i></a><i><strong> </strong></i><strong>(1 minute)</strong><br><br>Another:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/dics1vbuq5wu5lijs6mf\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/dsrgr6smxjgi5svucns5 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/mrhcyes7zqybanmjbqvw 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/qpgdwvgeuxeyfu8vnzol 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/meghqq7kjgq2qmx3sxiu 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/c5q0uvjmvdsv8kl5x7tu 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/djpgvdktz6yfhtkp92e0 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/qwnkrbfrjwd4hklplvk8 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ieyyofczkyoljwlsdrur 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/gtbfarsokvixodsqt7ge 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/rlz4vca7yv9t1hjdfiur 1920w\"></figure><p><br><a href=\"https://www.youtube.com/watch?v=PpEo1LoQ0Fw\"><strong>`Alysse in Unethical Vunderbar-Land vs. The EthiSizer' (a 1MMM - 1-Minute-Machinima-Movie)</strong></a><br>&nbsp;</p><p>But of course, there are countless <i><strong>Game-Maker apps</strong></i><strong> </strong>out there... Have fun with it.</p><p><strong>Some Example Smartphone-App-Maker apps</strong></p><blockquote><p>...Ditto - ask&nbsp;<i>Uncle Google</i>: \"free smartphone app maker\"...</p></blockquote><p><strong>&amp; a </strong><i><strong>Meta-Meta-Science</strong></i><strong> you can use, to make the process of&nbsp;</strong><i><strong>World Simulation </strong></i><strong>(when building a simulated Ethical Global Governor - or, EGG) Much Easier &amp; Quicker...</strong></p><p>World Sims have been around at least since 1972, with the <a href=\"https://en.wikipedia.org/wiki/The_Limits_to_Growth\">Club of Rome</a>'s <a href=\"https://en.wikipedia.org/wiki/World3\"><i>World3</i></a> sim...</p><blockquote><p>But if you find it a tricky task, modelling all of the <i><strong>units,</strong></i> within a World Simulator &nbsp;(namely:&nbsp;<i>Earth, and its 400+ ecosystems, and, all their biological inhabitants, and cultures) -</i>&nbsp;then perhaps try some of the scientific tools from this <a href=\"https://evolutionary-culturology.blogspot.com/2023/04/science-is-now-too-complex-lets-make-it.html\">simplifying meta-meta-science</a>&nbsp;(often called <i><strong>Ev Cult</strong></i>, for short).</p></blockquote><figure class=\"image image_resized\" style=\"width:36.08%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/tfr7g1ymik0osnnmub1v\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/nlddei2nwb27qs5cbjfz 80w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ljtx90phk0slqvl1ixkk 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/qifonhmvrocbdt8akwqm 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/vvg3wmt0ekc9jicliqac 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/v7yt0i3nhtjctpdhi91n 400w\"><figcaption>Elements of Evolutionary Culturology (2nd ed., 02023)</figcaption></figure><p><a href=\"https://www.amazon.com/dp/B0BX914RZT\"><i><strong>Elements of Evolutionary Culturology</strong></i> (02023)</a><br>&amp; see also <a href=\"https://evolutionary-culturology.blogspot.com/\"><strong>The </strong><i><strong>Ev Cult</strong></i><strong> weblog</strong></a></p><p>As mentioned above, some past examples of World Simulators include: <a href=\"https://en.wikipedia.org/wiki/World3\"><i>World3</i></a> (from way back in 01972).&nbsp;<i>Wikipedia</i> notes, you can play with online versions of it here:</p><blockquote><h3><i>`[World3]</i> Model Implementations</h3><ul><li><a href=\"http://bit-player.org/extras/limits/\">Javascript world 3 simulator</a></li><li><a href=\"http://insightmaker.com/insight/1954\">Interactive online World3 simulation</a></li><li><a href=\"https://github.com/cvanwynsberghe/pyworld3\">pyworld3</a>&nbsp;on&nbsp;<a href=\"https://en.wikipedia.org/wiki/GitHub\">GitHub</a>&nbsp;- Python version of World3</li><li><a href=\"https://github.com/Juji29/MyWorld3\">MyWorld3</a>&nbsp;on&nbsp;<a href=\"https://en.wikipedia.org/wiki/GitHub\">GitHub</a>&nbsp;- A second Python version of World3</li><li><a href=\"http://www.rpi.edu/~simonk/ESP/BTL4MacDownload.html\">Macintosh version of the Simulation by Kenneth L. Simons</a></li><li><a href=\"https://modelica.org/publications/newsletters/2014-1/index_html#item258\">Implementation of the World3 model</a>&nbsp;in the simulation language&nbsp;<a href=\"https://en.wikipedia.org/wiki/Modelica\">Modelica</a></li><li><a href=\"https://github.com//natema/WorldDynamics.jl\">WorldDynamics.jl</a>&nbsp;on&nbsp;<a href=\"https://en.wikipedia.org/wiki/GitHub\">GitHub</a>&nbsp;-&nbsp;<a href=\"https://en.wikipedia.org/wiki/Julia_(programming_language)\">Julia</a>&nbsp;version of World3 and World2'</li></ul></blockquote><p><i>(Source: Wikipedia webpage on </i><a href=\"https://en.wikipedia.org/wiki/World3\"><i>World3</i></a><i> (02023))&nbsp;</i></p><p>And since `Narrative Walk-Throughs' (basically, <i>short stories</i>) often help, with Game Design specifics / scenarios - another suggestion:&nbsp;</p><blockquote><p>...If making <i>games</i> and/or <i>apps</i> isn't your thing - you can always&nbsp;write <i><strong>science fiction short stories,</strong></i><strong> about a Superintelligent Singleton Global Governor (</strong>like say,&nbsp;<i>The EthiSizer</i>...) If you publish them, this will also help everyone else see ideas, when designing/building their own <i>EthiSizer </i>WorldSim...</p></blockquote><p>As Australia's chief scientist (from 02016 to 02020), <a href=\"https://en.wikipedia.org/wiki/Alan_Finkel\">Alan Finkel</a>&nbsp;rightly noted - in terms of <a href=\"https://en.wikipedia.org/wiki/Futures_studies\">Futures Studies</a>, science fiction is the most important genre in literature...&nbsp;See this podcast of an inspiring lecture he gave on it:&nbsp;</p><blockquote><p><a href=\"https://www.abc.net.au/radionational/programs/bigideas/science-fiction-as-a-guide-for-the-future-seg/8956204\"><i><strong>Science fiction as a guide for the future </strong></i>- by Australia's Chief Scientist, Alan Finkel</a>.</p><p>(Source: Broadcast on <i>ABC Radio National</i>, Tue 19 Sep 02017)</p></blockquote><p>Likewise in the excellent book <i>21 Lessons for the 21st Century </i>(02018), the great Yuval Noah Harari also notes:</p><blockquote><p>`Art plays a key role in shaping people\u2019s view of the world, and in the twenty-first century, <strong>science fiction is arguably the most important genre of all, for it shapes how most people understand things like AI</strong>, bioengineering and climate change. We certainly need good science, but from a political perspective, a good science-fiction movie is worth far more than an article in <i>Science</i> or <i>Nature</i>.\u2019&nbsp;</p></blockquote><blockquote><p>(<i>21 Lessons for the 21st Century,&nbsp;</i>Harari 02018, pp. 243-4, <strong>bold emphasis mine</strong>)</p></blockquote><p>...Also - did you see that recent&nbsp;<i>WIRED Magazine</i> article, on GPT4...? Namely:</p><blockquote><p><strong>`</strong><a href=\"https://www.wired.com/story/what-is-chatgpt-plus-gpt4-openai/\"><strong>Is GPT-4 Worth the Subscription? Here\u2019s What You Should Know</strong></a></p><p><i>The new algorithm is exclusive to ChatGPT Plus, OpenAI\u2019s subscriber-only service. We signed up and put it to the test.'</i>&nbsp;</p></blockquote><blockquote><p>(Rogers, <i>WIRED</i>, March 24, 02023)<i>&nbsp;</i></p></blockquote><p>...Below is a very interesting excerpt from it, when you consider a superintelligent singleton, enforcing a <i>global moral code of ethics</i>:&nbsp;</p><blockquote><p>`<strong>GPT-4:</strong>&nbsp;...<i>Here's a unique twist on the Trolley Problem:</i></p><p><strong>Imagine a world where everyone has a personal \u201cEthical Score\u201d that represents their positive or negative contributions to society. In this world, an individual\u2019s Ethical Score is determined by a combination of factors, such as their actions, decisions, and attitudes towards others. This score is widely accepted, and its accuracy is rarely questioned.</strong></p><p><strong>Now, the Trolley Problem is presented again, but with a twist. The five people on the main track have Ethical Scores that are significantly lower than the one person on the side track. You know that these scores are generally reliable indicators of a person\u2019s moral worth. In this scenario, do you still pull the lever and sacrifice the person with the higher Ethical Score to save the five people with lower scores? Or do you allow the trolley to continue on its course, killing the five people with lower scores?</strong></p><p><strong>This twist adds an extra layer of complexity to the original Trolley Problem by introducing the concept of moral worth and the value of individual lives, forcing us to weigh the consequences of our actions against the overall impact on society</strong>.'&nbsp;</p></blockquote><p>(<a href=\"https://www.wired.com/story/what-is-chatgpt-plus-gpt4-openai/\">Rogers,&nbsp;<i>WIRED</i>, March 24, 02023</a>, <strong>bold emphasis, mine</strong>)</p><p>Also we can expand that worldview, from just (humanimal) <i>society</i>, to - <i><strong>the Whole Earth and all its ecosystems</strong></i> (humanity's wider existential <i>environment...</i>)</p><p>...And more <i>ideas to potentially inspire you,</i> in the book below - also co-written with an <i>EthiSizer superintelligent singleton global governor&nbsp;</i>AI program:</p><figure class=\"image image_resized\" style=\"width:35.73%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/c6sfvzzaqrp4crs9rxpr\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/vpoypc1gjvn06xuhc3n9 109w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/voxnw9mvqqedv15itub0 189w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/nafkue96gvftmi83xpzf 269w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/muzoswdlifazyjtpqvmc 349w\"><figcaption>PH3 - A Flash Fiction Writing Manual (02023)</figcaption></figure><p><a href=\"https://www.amazon.com/dp/B0BRLYM3XF/\"><i><strong>PH3 - A PHlash PHilosoPHiction Writing Manual</strong></i> (02023)</a> &amp; see also: <a href=\"https://p-h-3.blogspot.com/\"><strong>The </strong><i><strong>PH3 </strong></i><strong>weblog</strong></a><strong>&nbsp;</strong><br><br>(But again - by all means <i>ignore all of that</i>, if you're already bursting with ideas-!)&nbsp;</p><p>And, if you\u2019re <i>already convinced</i> that it's a good idea for <i>humanity,</i> <i>en masse,</i> to make - and <i>test</i> - and <i>tune</i> - as many different variations of an <i>EthiSizer (a global moral/ethics code)&nbsp;</i>as we can, then, you probably don\u2019t need to read the rest of this essay!&nbsp;</p><p>(As, you've likely already started designing `Toy Singleton' games, apps, &amp; sims...)&nbsp;</p><p>...But hey if you\u2019ve come this far... well done, and...</p><p><i>----------------//---------------</i></p><p><strong>...POST YOUR RESULTS!</strong></p><p>...Maybe, use a free blog platform (e.g., <a href=\"https://www.blogger.com/about/?r=1-null_user\">blogger</a>, or <a href=\"https://wordpress.com/\">wordpress</a>, or whatever you like...)</p><p>...But, do you see the reason to <i><strong>publish (communicate) your results</strong></i>, on any and every form of social media possible?&nbsp;</p><p>...Because:&nbsp;</p><blockquote><p>The more people who know about, and actively explore (and test out, and try to `break') the concept of a <strong>global moral code</strong> simulator, the more likely we all are to understand what's coming, with the eventual Global Superintelligence Singleton...</p></blockquote><p>So, please do blog about it...&nbsp;</p><p><i>&amp;/or Tweet </i>about it (if, <i>Twitter </i>still exists when you read this)...&nbsp;</p><p><i>...or Instagram </i>it...&nbsp;</p><p><i><strong>Talk</strong> </i>about it...&nbsp;</p><p><i><strong>Think </strong></i>about it...</p><p><i>...<strong>Communicate</strong></i><strong> </strong>about it...</p><p><strong>...We all need to hear from you...</strong>&nbsp;</p><p>Publish all your own ideas, for: a <i>Global Moral Code</i>...</p><p>And, play <i><strong>The EthiSizer </strong></i>games... And, complain about them...!&nbsp;</p><p>Your feedback / criticism will help make them all better<i>..</i>.</p><p>The more ideas circulating on this, the merrier!&nbsp;</p><p><i>----------------//---------------</i></p><p><strong>...BUT WHY DO WE NEED SO MANY DIFFERENT IDEAS?</strong></p><p>...After all \"What difference can I<i> </i>make?\" you may ask...</p><p><i><strong>Answer:</strong></i> We all need humanity to <i>really pull together</i> on this one.</p><p>...It's actually the most crucial decision that humanity has ever had to make.</p><p>Our collective global future depends on this.</p><p>As: <i>Without </i><strong>global unity</strong>, a <i>bad actor</i> (human, or state)&nbsp;</p><p>may soon <i>create superintelligence</i>,</p><p>and, a <i>singleton</i>&nbsp;</p><p>and, (accidentally-?) kill us all.</p><p>So, we all need <i>as many ideas </i>as possible.&nbsp;</p><p>...So that we can all&nbsp;<i>find the best one, </i>together...<br>----------------//---------------</p><p><strong>AND, IF IT WORKS, KEEP IT...!</strong></p><p>...And, for anyone unfamiliar, in <a href=\"https://evolutionary-culturology.blogspot.com/2022/02/creativity-science-texts.html\"><i>Creativity Science</i></a>, this process of <i>generating </i>as many ideas as possible, then <i>evaluating </i>them all, is called Evolutionary Epistemology: namely the <i><strong>Blind Variation</strong></i><strong> &amp; </strong><i><strong>Selective Retention (BV-SR)</strong> <strong>evolutionary algorithm</strong></i>... Or, `<i><strong>Generate and Test'</strong></i>...</p><p>An article you can read on it (BV-SR), is&nbsp;<a href=\"https://newcastle-au.academia.edu/JTVelikovsky/Conference-Presentations\">here</a>... namely:&nbsp;</p><blockquote><p><a href=\"https://www.academia.edu/98960197/Popper_Campbell_s_Evolutionary_Epistemology_01974_and_its_evolution_in_Csikszentmihalyi_s_Systems_Model_of_Creativity_01988_02023_\"><strong>Popper/Campbell\u2019s </strong><i><strong>Evolutionary Epistemology</strong></i><strong> (01974) and its evolution in Csikszentmihalyi\u2019s </strong><i><strong>Systems Model of Creativity</strong></i><strong> (01988-02023)</strong></a><strong> </strong>(Velikovsky 02023)</p></blockquote><p>----------------//---------------</p><p><i>Which leads into...</i></p><p><i><strong>HEROISM SCIENCE</strong></i><strong>&nbsp;</strong></p><p><strong>(...OR, `</strong><i><strong>GLOBAL HEROISM ON FIVE DOLLARS A DAY'</strong></i><strong>)</strong></p><p>In some ways, it's actually not that hard, to be a hero... (if, you like that kind of thing...)</p><p>You just have to help to&nbsp;<strong>solve a big problem</strong>... Thankfully, there's a great&nbsp;<i><strong>recipe / algorithm / series of steps / heuristic </strong></i>for <strong>solving problems</strong>...</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/to0zm6npymrovs5tzhhg\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/zd5gfhobmwiu1f07gbbx 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/m5zsankonykedprnpe04 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/sveim4oc3y345rntkckm 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/qssyueoyqsyeovi71bey 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/xqljcz9cbz1i23cdnfa2 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/idj9gp0wiktc24gg34wf 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/l78fl5ziomhncjeubzmv 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/cvk6ah6xcl64cux8bndh 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/d3llvwgua7jlo1os9rhy 1620w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/psvhrwvvljcjeoevhjdo 1780w\"></figure><p>For more detail, perhaps see these posts below:&nbsp;</p><blockquote><p><a href=\"https://on-writering.blogspot.com/2017/07/heroism-science.html\"><i>On Heroism Science</i></a></p></blockquote><blockquote><p><a href=\"https://storyality.wordpress.com/2017/07/30/storyality-145-five-views-of-the-mono-myth/\">Five views of <i>The Hero's Journey monomyth</i></a>&nbsp;</p></blockquote><blockquote><p><a href=\"https://storyality.wordpress.com/2013/08/23/storyality-73-the-heros-journey-its-not-what-you-think/\">The Hero's Journey... (it's not what you think it is)</a>&nbsp;</p></blockquote><p><strong>----------------//---------------</strong></p><h1><strong>LITERATURE REVIEW:</strong></h1><p>Usually in this section, one would&nbsp;<i>review all the relevant literature</i>, but \u2013 just see&nbsp;<a href=\"https://www.abc.net.au/news/2023-03-24/what-is-agi-artificial-general-intelligence-ai-experts-risks/102035132\">the news</a>, as:&nbsp;<a href=\"https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/\"><i>AGI\u2019s coming, ready or not</i></a><i>...</i></p><p>So now, a very brief review of:</p><p>&nbsp;</p><h3><strong>GLOBAL CATASTROPHIC RISK,&nbsp;</strong></h3><h3><strong>AI SAFETY, &amp;&nbsp;</strong></h3><h3><strong>BOSTROM (2014 &amp; 2022) ON </strong><i><strong>SINGLETONS,</strong></i><strong> VS. </strong><i><strong>MULTIPOLARS</strong></i></h3><p>&nbsp;</p><p><i><strong>On: AGI (Artificial General Intelligence), &amp; Global Catastrophic Risk (GCR)</strong></i></p><p><a href=\"https://forum.effectivealtruism.org/\"><i>Effective Altruism</i></a> folks all know the drill - so, let's <i>quick-gloss-over </i>this part\u2026?&nbsp;</p><p>A selection of good books on it, for anyone unfamiliar with the topic of <i>AI Safety</i>:</p><ul><li><i>Superintelligence: Paths, Dangers, Strategies</i>&nbsp;(Bostrom 02014)</li><li><i>Life 3.0</i>&nbsp;(Tegmark 02017)</li><li><i>Homo Deus</i>&nbsp;(Harari 02017)</li><li><i>21 Lessons for the 21st Century</i>&nbsp;(Harari 02018)</li><li><i>Machines Behaving Badly: The Morality of AI</i>&nbsp;(Walsh 02022)&nbsp;</li></ul><p>And, if you're want to keep up on the latest in <strong>AI Safety</strong>, maybe Google: \"AI Safety research\". Also here's a helpful 02021 <a href=\"https://80000hours.org/career-reviews/ai-safety-researcher/\">blog post on <strong>AI Safety Technical Research</strong>, from <i>80,000 Hours</i></a>.&nbsp;</p><p><strong>Some Key Points on&nbsp;</strong><i><strong>AI Safety Research</strong></i></p><p>Again, we're trying to <i>cut to the chase</i> (so we can quickly get to the <i>good stuff</i>, down below) \u2013 so here\u2019s a quote from a great <a href=\"https://www.youtube.com/watch?v=VG8lanbnbwk\">(2022) Protocol Labs Bostrom <i>YouTube</i> interview</a> that very broadly sums the current state-of-play, in <i>AI Safety Research</i>:</p><blockquote><p><i><strong>`Juan Benet:</strong></i> So\u2026 as you've thought about this problem, have you kind of been able to break it down into components and parts \u2013 or maybe evolve to your thinking of the shape of the problem? ...What are you thinking now?</p><p><i><strong>Nick Bostrom:</strong></i> Well I think the field as a whole has made significant advances and developed a lot, since when I was writing the [<i>Superintelligence</i>, 02014] book, where it was like really a non-existent field \u2013 there were a few people on the internet, here and there \u2013 but now it's an active research field, with a growing number of smart people who have been working full-time on this for a number of years \u2013 and writing papers that build on previous papers with technical stuff, and all the key AI labs have now some contingent of people who are working on Alignment\u2026 <a href=\"https://www.deepmind.com/safety-and-ethics\"><i>Deepmind</i></a> has; <a href=\"https://openai.com/blog/our-approach-to-ai-safety\"><i>OpenAI</i></a> has; <a href=\"https://www.anthropic.com/index/core-views-on-ai-safety\"><i>Anthropic</i></a> has... So that's all good.</p><p>Now, within this community, there is I guess a distribution of levels of optimism \u2013 ranging from people very pessimistic, like <a href=\"https://en.wikipedia.org/wiki/Eliezer_Yudkowsky\">Eliezer</a> for example, and I guess there are people even more pessimistic than him, but he's kind of at one end \u2013 and towards people with more moderate levels of optimism, like <a href=\"https://www.fhi.ox.ac.uk/team/paul-christiano/\">Paul Christiano</a> \u2013 and then others who think it's kind of something that <i>`we will deal with it when we get to it\u2019</i> and who don't seem too fussed about it\u2026</p><p>I think there's a lot of uncertainty on the hardness level now \u2013 as far <i>how you break it down</i> \u2013 yeah, so there are different ways of doing this\u2026&nbsp;</p><p>There's not yet one paradigm that all competent AI Safety researchers share, in terms of <i>the best lens to look at this</i> \u2013 so it decomposes in slightly different ways, depending on your angle of approach \u2013 but certainly, one can identify different facets that one can work on\u2026</p><p>So for example, Interpretability Tools seem \u2013 on many different approaches \u2013 like useful ingredients to have, basically insights or techniques that allow us better to <i>see what is going on</i>, in a big neural network\u2026 You could have one approach where you try to get AI systems that try to learn to match some human example of behavior \u2013 like one human or some corpus of humans, and then tries to just perform a next action that's the same as its `best guess about what this reference-human would do in the same situation\u2019\u2026 and then you could try to do forms of amplification on that\u2026 So, if you could faithfully model one human, well then you just get like a human-level-like intelligence, you might want to go beyond that \u2013 but if you could then create <i>many</i> of these models, that each do what the human do, can you put them together in some bureaucracy, or do some other clever bootstrapping or self-criticism\u2026? So, that that would be one approach.</p><p>You could try to use sort of Inverse Reinforcement Learning, to infer a human's preference-function, and then try to optimize for that \u2013 or maybe not strictly <i>optimize</i>, but doing some kind of software optimization\u2026 Yeah, there are a bunch of different ideas\u2026</p><p><strong>Some Safety-work is more like, trying to more precisely understand and illustrate in `toy examples\u2019 how things could go wrong \u2013 because that's like often the first step to creating a solution, is to really deeply understand what the problem is \u2013 and then illustrate it.</strong>&nbsp;</p><p>And, so that can be useful as well...'&nbsp;&nbsp;</p></blockquote><p>Source: <a href=\"https://www.youtube.com/watch?v=VG8lanbnbwk\"><i>Path To AGI, AI Alignment, Digital Minds | Nick Bostrom and Juan Benet | Breakthroughs in Computing </i>(YouTube, 2022, 23 mins 20 secs to 27 mins 40 secs)</a><i>&nbsp;</i></p><p><i>\u2013 with <strong>bold emphasis [above] by the present author &amp; you\u2019ll soon see why\u2026&nbsp;</strong></i></p><p><i><strong>(Please Read On\u2026)</strong></i></p><p><i>----------------//---------------</i></p><p><strong>AN EXAMPLE OF A `TOY' SUPERINTELLIGENT SINGLETON -&nbsp;</strong><i><strong>THE ETHISIZER</strong></i></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/jai39sgbwabbwayu05ek\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ffglpyvqlpcnotgatyyo 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/rks69dof8jdc65ce7qyb 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/b1au45pxvyvqrhajlvna 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/upreiywvlyezwxvpjbyf 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/omunrzyp7kgjnqvcnnwi 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/h7yy9rzsopgwmyhumf95 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/eekdq7wr9oonyrgogttw 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/mb9mcw9a2ce4lnmfef5v 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/hhdwjppaxcpjjimgkvbe 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/abp23f649va932wh0g2p 1920w\"></figure><p><a href=\"https://www.youtube.com/watch?v=ToouoSi_uho\"><i><strong>The EthiSizer - global Super-ethical Singleton Sim (XLS front end)</strong></i></a><i><strong> </strong></i>(1 minute)<br>&nbsp;</p><p>In the excellent book&nbsp;<i>Superintelligence </i>(02014), Bostrom talks about a <i><strong>singleton</strong></i> superintelligence (or, a singleton AGI):</p><blockquote><p>`\u2026the concept of a singleton is an abstract one: a singleton could be democracy, a tyranny, a single dominant AI, a strong set of global norms that include effective provisions for their own enforcement, or even an alien overlord\u2014its defining characteristic being simply that it is some form of agency that can solve all major global coordination problems. It may, but need not, resemble any familiar form of human governance.)\u2019</p></blockquote><p>(Bostrom 02014, p. 25%)</p><p>Later in the same great book, Bostrom (02014) also writes:</p><blockquote><p>`A <strong>singleton</strong>, by definition, is <strong>a highly collaborative social order.</strong><sup>44&nbsp;</sup>'</p><p><i>&amp; the Endnote:</i></p><p><i>`Endnote 44: A singleton is highly internally collaborative <strong>at the highest level of decision-making</strong>. A singleton&nbsp;could&nbsp;have a lot of non-collaboration and conflict at lower levels, if the higher-level agency that constitutes the singleton chooses to have things that way.\u2019</i></p></blockquote><p>(Bostrom 02014, p. 71%, <strong>bold emphasis mine</strong>)</p><p>And so, helpfully <i>unpacking </i>that concept a little, Bostrom also says this (below) in the <a href=\"https://www.youtube.com/watch?v=VG8lanbnbwk\">Juan Benet <i>Protocol Labs</i> interview (2022)</a>:</p><blockquote><p><i><strong>`Juan Benet: &nbsp;</strong></i>Let's start by distinguishing\u2026 What is a Singleton?</p><p><i><strong>Bostrom:</strong></i> To me, it's like this abstract concept<i><strong> </strong></i>of a world order where <strong>at the highest level of decision-making there's</strong><i><strong> no coordination failure</strong></i><strong>, and it's like a kind of single agency at the top level</strong><i><strong>.</strong></i></p><p>So this could be good or bad that could be instantiated in many ways on Earth. You could imagine a kind of super Yuan; you could imagine like a world dictator<i><strong> </strong></i>who conquered everything; you could imagine like a superintelligence that took over; you might also be able to imagine<i><strong> </strong></i>something less formally structured <strong>like a kind of a global moral code</strong> <strong>that is</strong><i><strong> </strong></i><strong>sufficiently homogeneous and that is self-enforcing \u2013 </strong>and maybe other things as well<i><strong>\u2026</strong></i></p><p>So at a very abstract level, you could distinguish the future scenarios where<i><strong> </strong></i>you end up with a Singleton, versus ones that remain Multipolar, and you get different dynamics in the Multipolar<i><strong> </strong></i>case, that you avoid in the Singleton case.</p><p>It's kind of competitive dynamics,<i><strong> </strong></i>which one of these potential features that you think is more likely at the at the moment<i><strong> \u2013 </strong></i>and I think all things considered the Singleton outcome in the<i><strong> </strong></i>longer term seems probably more likely, at least if we are confining ourselves<i><strong> </strong></i>to Earth-originating intelligent life.</p><p>And, the different ways in which it could arise, from more kind of <i>slow historical conventional type of processes</i>, where we do observe, from 10,000 years ago when the highest unit of political organization were bands of hunter-gatherers \u2013 50 or 100 people \u2013 then subsequently to sort of Chieftains, city-states, nation-states, and more recently larger entities like the EU or weak forms of global governance.</p><p>You could argue that in the last 10 \u2013 15 years we've kind of seen some retreat from that to a more Multipolar world, but that's a very short period of time in these historical schemes \u2013 so there's still like this overall trendline\u2026 So that might be one.</p><p>Another would be, take AI scenarios like if either the AI itself, or the country or group that builds it becomes a Singleton, you could also imagine the scenarios where you have multiple entities going through some AI transition \u2013 but then <i>subsequently</i> managed to coordinate \u2013 and they would have new tools for implementing\u2013 like if they come to an agreement right now, it's kind of hard\u2026 Like how do you set it up concretely, in a way that binds everybody, that you could <i>trust that will not get corrupted or develop its own agenda</i> \u2013 like the bureaucrats become (too powerful)</p><p>\u2026So if you had new tools to do those, it's also possible that subsequently that there might be this kind of merging into a single entity.</p><p>So all of those different avenues would point to it, but it's not a certainty. But if I had to guess, I would think it's more likely than the Multipolar.'</p></blockquote><p><strong>(the same </strong><a href=\"https://www.youtube.com/watch?v=VG8lanbnbwk\"><i><strong>PL Bostrom interview</strong></i></a><strong>, from: 40m 55s, to 44m20s)</strong></p><p><i>----------------//---------------</i></p><p><strong>...WE NEED AS MANY `TOY SINGLETONS\u2019 AS WE CAN GET...&nbsp;&nbsp;</strong></p><p>...So that we can (all) <i>test</i> them all...&nbsp;&nbsp;</p><p>(in engineering parlance: so we can `Suck It And See'...)</p><p>&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/vdkrq6kpnv4rin3udjtv\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/oflhknlr5yj8mjudijcw 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/cutc8mlwohgkbejpemrr 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/zcgjj4dpyuk3ee0ytga4 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/eampejlghupko4ljvjb4 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/aolicvinwkebdxtairwu 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ubjzup3tb2vclmdmap67 520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/h9gyo9bozarp0gfolndh 600w\"><figcaption>Global Unity, at the planetary scale-level... (...ethically unifying, all 200 nation-states)</figcaption></figure><p>Bostrom's also not wrong about <i>all of this</i>, below:</p><blockquote><p><i><strong>`Bostrom:</strong></i> I think <strong>it would be a really nice if we could have a world where the leading powers were more on the same page \u2013 or friendly \u2013or at least cooperate, had a constructive cooperative relationship</strong>.</p><p>I think a lot of the X-risk [Existential Risk] `pie\u2019 in general \u2013 and the risk from AI in particular \u2013arises from the possibility of <strong>conflicts</strong> of different kinds\u2026&nbsp;</p><p>And so <strong>a world order that was more cooperative </strong>would look <strong>more promising for the future, in many different ways\u2026</strong></p><p>So I'm a little worried about, especially kind of more <i>unilateralist moves </i>to kind of `kneecap the competitor\u2019, and to be `playing nasty\u2019\u2026 I'm very uneasy about that...</p><p><i><strong>Juan Benet:</strong></i><strong> </strong>So, if ideas or hardware will only buy a certain amount of time, then really AI Alignment is the best path forward \u2013 and very much agree that we don't want to restrict the creation of digital intelligence \u2013 as that's sort of the next evolutionary jump\u2026 And there's some questions there around which paths should we take, and how do we develop, and bring in computer interfaces and Whole Brain Emulation and so on \u2013 but kind of even before getting into that, How hopeful are you that we might solve the AI Alignment problem?</p><p><i><strong>Bostrom: </strong></i>Moderately, I guess? I'm quite agnostic, but I think the main uncertainty is, <i>how hard the problem turns out to be</i> \u2013 and then there's a little extra uncertainty as to <i>the degree to which we get our act together.</i></p><p>But I think, out of those two variables \u2013 like the realistic scenarios, in which we either are <i>lazy and don't focus on it</i>, versus <i><strong>the ones where we get a lot of smart people working on it</strong></i> \u2013 there's almost certainty there, that affects the success chance, but I think that's dwarfed by our uncertainty about how intrinsically hard the problem is to solve \u2013 so you could say the most important component of our strategy should be to hope that the problem is not too hard.</p><p><strong>Juan Benet:</strong> Yeah \u2013 so, <strong>let's try to tackle it!'</strong></p></blockquote><p><strong>(Source: same YouTube video, from 21 mins to 23m20secs</strong></p><p><strong>(bold emphasis above, mine)</strong></p><p>----------------//---------------</p><p><strong>WHY AN ETHICAL GLOBAL SINGLETON (LIKE, SAY, AN </strong><i><strong>ETHISIZER</strong></i><strong>) MAKES SENSE...</strong></p><p>Other great thinkers who felt a <i>Unified Global Moral Code</i> (or, One World State) have included Albert Einstein, Bertrand Russell, Isaac Asimov, Buckminster Fuller, and Yuval Noah Harari...</p><p>The key reason is that: the many <a href=\"https://en.wikipedia.org/wiki/List_of_global_issues\"><i>global wicked problems</i></a> we face can't be solved on a local (national) scale-level...&nbsp;</p><p>With global <a href=\"https://en.wikipedia.org/wiki/Wicked_problem\">wicked problems</a> such as global warming, government corruption, pollution, and pandemics (and, many more), we need coordinated, ethical, global-scale solutions. In short, all 200 nations need to <i>act as one</i>...&nbsp;</p><p>Friendly Global Coordination.&nbsp;</p><p><strong>Simplifying The Problem (200 Nations; but only One World)</strong></p><p>In simplest terms, any <i>group</i> is made up (composed) of smaller <i>units</i>...&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/wapq1wxxhxnj6gdoufll\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/o06vxiie9e6gfxedh12w 350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/xzt5ogmgpdprhp5qz8hs 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/vddajbnstvggryhl8b5o 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/c0jjestshlig37mdcgm6 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/qqacvjl2z80fq3iaerqk 1750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ckyc91gsgdvvauppvj8a 2100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ztw3fuln8tqmuedklnkx 2450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/utiyyfsdevqqi3u9b2lk 2800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/djlfmoa8jsfphp6t8yua 3150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/up0zm7kon8xrdsnewjjm 3438w\"><figcaption>Any group is a unit - made up of smaller units</figcaption></figure><p>These <i><strong>units</strong></i> can either (1) <strong>compete (be </strong><i><strong>in conflict</strong></i><strong>)</strong>, or, can (2) <strong>co-operate</strong>...&nbsp;</p><p>As an example, think of your body as: a <i>system</i>, made up of smaller systems...&nbsp;</p><p>At different levels of scale/size, it's made up of <i><strong>units</strong></i>: cells, tissues, organs, organ systems... and, all of that is <i>one whole organism </i>(i.e., you)...</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/liqbtxg36ygpjrmkqona\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/fpbbnnijthrcohmkhvwk 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/rfp62xkthgjvgl2gwhzd 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ydd0yqppp744754amjk0 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/tne5smy8hxejo2eyqnpf 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/vcm5iluk4kt282rsmtjr 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/tjijxca9wb6xx3gfxmuw 520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/akvtpahyy5l8pjizusa6 600w\"><figcaption>You are a system (a unit) made up of smaller systems... And so is Earth's biosphere...</figcaption></figure><p>And, if any of those units (or sub-units) are <strong>competing</strong> (in <i>conflict </i>/ at <i>war</i>), you fall apart. (<i>Think: cancer, tissue injury, organ failure, death</i>).&nbsp;&nbsp;</p><p>And now, on larger scales, consider; Earth's 200 nations...&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/xezwhy1zwu2vlrrclhtu\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/kfkbias8afz5vq2xbqyu 83w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/chjlzhs3nzoivlyc2xih 163w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/vajjgniqp0vnfiv50jjb 243w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ggtupdklqx2bdur2nd5t 323w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/i2r8hfvsk8oj2qised8s 403w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/fc8l3heyo6wfqpgbbmwh 483w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/xntvd6dmbtxncm35jqcs 563w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/pyqu6cdncnfrvd6cqm1o 643w\"><figcaption>Imagine a unified, peaceful, ethical world - instead of 200 competing (&amp; conflicting) nation-states...</figcaption></figure><p>There now exists a global group (...Earth), made up of smaller <i>units </i>(our 200 nations)...</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/uijxyoduvuhsb7lsnnx9\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/shsxfgjnrro5msbzi2nu 132w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ydjjqn8gzcvn5hqeqol8 212w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/a5ryfipq4rlakgr4h4y5 292w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/hp2rrq0ncgr6brh0kee3 372w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/wlhjhbqy6lszvfydelas 452w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/tuii0npzpqrdwqamyvod 532w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/hsuosfarkh2jjoxuud44 612w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/e9jiistltb5n5huuefxq 692w\"><figcaption>One planet, one people, one purpose: Peace.</figcaption></figure><p>&nbsp;</p><p>...Wouldn't it be better if we - (all 200 nations) - all got along...?&nbsp;</p><p>..If, there were <i>no wars&nbsp;...</i>? &nbsp;(...<i>World Peace</i>, anyone...?)</p><p>...To <i>Select, Vary, and Transmit</i> a phrase by the great designer, Buckminster Fuller:</p><p><i><strong>...One Planet,&nbsp;</strong></i></p><p><i><strong>one People,&nbsp;</strong></i></p><p><i><strong>one Purpose,&nbsp;</strong></i></p><p><i><strong>Please?</strong></i></p><p><i>(...Namely, Human, and Whole-Earth: flourishing, and wellbeing...)</i></p><p>(more detail in this <a href=\"https://www.academia.edu/99120540/The_Open_Society_and_The_EthiSizer\">02022 conference paper</a>, if desired...)</p><p>Like the <a href=\"https://en.wikipedia.org/wiki/World3\">World3 Sim</a>, <i>The EthiSizer Simulation&nbsp;</i>models: all 200 nations; all 400+ Earth ecosystems; and, all of their smaller component parts...&nbsp;</p><p>And, it assigns a Value (a Score), to each and every component unit...&nbsp;</p><p>...As a result, each <i>unit </i>(including each <i>person</i>) has a <i><strong>Personal Ethics Score (PES)..</strong></i>.</p><p><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAn4B0W_IP_YLMZdfiPBqQpkE2teBu84Nt0BdGomdDRJHOBwlPcILLtJ7-EMnhuXA9hXjXplRdLxHVysZoICo2iJ7LY4_ZwmHrxOZYpgnSnjcYZnJN1Mc6f3cJYLq69kb5K00OeE1opddsjO_EH8FwH_34BIq-zvMC4pN9ObXSh3ARJOl_YGiOlcCA/s1842/Dont%20Be%20The%20Lowest%20PES%20on%20ur%20phone.png\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/od0id3gyc1tuoigadr22\"></a></p><p>So, whenever you're about to make a big decision, <i>The EthiSizer</i> helpfully presents you with <i>options</i>, and, their related <i>Ethics Scores</i>...&nbsp;</p><p>...So that each person still gets to use their own <i>freedom of choice..</i>.&nbsp;</p><p>And, each <i><strong>action</strong></i> has relevant scores (<i>Ethical </i>actions have a positive score; <i>Unethical </i>actions have a minus-score).&nbsp;</p><p>So, one way to view it, is that...&nbsp;</p><p><i>The EthiSizer</i> is: a <strong>Global Moral Conscience (GMC)</strong>...</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/qnbnfzundhdzfpnz5ato\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/vnbqxbwh4swksh809pt7 190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/pow1fzr4wjnvm1hux2xs 380w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/xnr1hxexoxahypqh2en8 570w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/plhfsilge9ktcoh1dn2q 760w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/s1g9nrhlbi6bbwgutxjb 950w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/jopv0oqaefcu1mpsmmq8 1140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/oiesglkvxqg8svnnrsp9 1330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/dut8eddkhcp2jhbrlqhs 1520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/bsqer5c5fehinapmlcb0 1710w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/s8c6zqlsvss3kkk70zby 1806w\"></p><p>With a super-ethical <strong>Global Moral Code</strong>...</p><p>Except that,&nbsp;<i>The EthiSizer</i>&nbsp;has <i>way more <strong>information</strong></i><strong>&nbsp;</strong>(about: individual&nbsp;<i>actions</i>, and their <i>consequences</i>) than does any one person (or group of persons)...&nbsp;</p><p>It `sees':&nbsp;</p><p><i>The Whole Big Picture</i>...&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/hd8vtn0iwhtyt4k9x8yg\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/z43exmij15mu3t6gmqq3 154w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/hxismplvqhrdhukl37yf 234w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/hfj3tkmptljm48ycv6jk 314w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/xlutu3l27pf7kar2rrz3 394w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/rrd4szc2mumpomcyktmt 474w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/smvzolp3sooxwhehbrim 554w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/twbwzp4nwrmjyg4xwcxt 634w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ptzkys5d4w7czhvkuueo 714w\"></p><p>...and not, just: <i>one little part...</i>&nbsp;like say <i>just one person's Point-of-View&nbsp;...</i></p><p>Up to now in all of history, we Humans have always had to make <i>decisions under uncertainty</i>...</p><p><i>...The EthiSizer</i> doesn't...!&nbsp;</p><p>Instead, it <i>knows</i>.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/bylzawwrcmzd6qac4drp\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/egsqxpjogyka9aob3y31 190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/vpvvojhnohf5x9nm5mld 380w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/r8ao2wbkvkzufe8qcn0r 570w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/dd0tjmic84chripwrvtf 760w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/a4qskpvixbrugsserxrj 950w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/xpfjyxfhkyq8me9evn5n 1140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/e9ag0gvrklvdjmozyl3l 1330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/zitbpkn1ybg4dbcwca0t 1520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/q3to8xcryuayqo2g8cmb 1710w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/soe39rnjrv2t0cjjecmf 1804w\"></figure><p>...It also protects you, from: <i>Unethical Entities</i>.&nbsp;</p><p>(From: <strong>Unethical </strong><i><strong>Ideas, Processes, Products... and People</strong></i><strong>. </strong>And, unethical groups - like, some unethical corporations, political or ideological movements.).</p><p><i><strong>The EthiSizer</strong></i> is a <a href=\"https://on-writering.blogspot.com/2021/08/in-praise-of-ethical-regulators-and.html\">Super-Ethical System</a>.&nbsp;</p><p>(...&amp; here's a handy <a href=\"https://on-writering.blogspot.com/2019/12/ml-datasets.html\">list of ML Datasets</a>, if you're building a World Sim...)</p><p><i>----------------//---------------</i></p><p><strong>BUT, WAIT\u2026 WHAT IF </strong><i><strong>THE ETHISIZER</strong></i><strong> IS </strong><i><strong>ROKO\u2019S BASILISK</strong></i><strong> ?</strong></p><p>We think that <i>The EthiSizer </i>probably&nbsp;<i><strong>isn't</strong></i> <a href=\"https://wiki.lesswrong.com/wiki/Roko%27s_basilisk\"><i>Roko's Basilisk</i></a>...&nbsp;</p><p>...But to be safe, we need to be sure.</p><p>By testing it in a toy (game/simulation) environment.</p><p><i>----------------//---------------</i></p><p><strong>SOME OTHER POSSIBLE OBJECTIONS...</strong></p><p><i><strong>Singleton Superintelligence Safety</strong></i><strong> </strong>is becoming super-urgent, in terms of Global Catastrophic Risk... But, there are <i>lots</i> of other possible objections to a Global Singleton (like, say, <i>The EthiSizer</i>).</p><p>So please put all yours (your objections) in the <i>Comments</i>, below...</p><p>(Any people making toy singletons need to know about them.)</p><p>...But - Superintelligence is coming, eventually - whether we want it, or not.&nbsp;</p><p>(Perhaps by 02070, and perhaps sooner...)</p><p>So, <i>when it does arrive</i>, let's make sure that it's Aligned with Prosocial, Altruistic Human Values...&nbsp;</p><p>And, let's also <i>find out</i>,&nbsp;<a href=\"https://on-writering.blogspot.com/2020/08/a-random-list-of-values.html\">what those really are</a>!</p><p>And, to start you thinking, a really cool book is:&nbsp;<a href=\"https://en.wikipedia.org/wiki/Human_Universals\"><i>Human Universals</i> (Brown 01991)</a>...</p><p><i><strong>----------------//---------------</strong></i><strong>&nbsp;</strong></p><p><strong>THE END (OF THE BEGINNING)</strong></p><p>So - over to <i>you</i>...</p><figure class=\"image image_resized\" style=\"width:73.37%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/bg054o9hu7pskjpqbmwj\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/qp3kcmmljpp5ixg5uhtr 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/bmqhtywo0zlqf9s2qccn 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/wzd3rib2tadsan5ekblj 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/di4dgr9ninjyvh8xcwst 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ngez1prvuarxsohilae3 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/wtwiepsamd8mc3qgqi7o 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/btrssgvgm9it5fbhin8z 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/x3imq6gwnfg8bdasquvx 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/jch2qbtcvbel7dpyugpu 810w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/pvrrsj34dst4qeraa8ta 888w\"></figure><p><strong>CONCLUSION / CALL TO ADVENTURE</strong></p><p>Let's all get cracking on it.</p><p><i>(Time's ticking away... 02070 may be closer than we think?)</i><br>&nbsp;</p><p><i><strong>----------------//---------------</strong></i></p><h3><i><strong>APPENDIX</strong></i></h3><p><i><strong>Anecdata alert: </strong></i>While doing research for a science fiction TV series that I was commissioned to co-create for <i>Fox Studios</i>&nbsp;(way back in 01998), I had to read a whole lot of research literature on <i>civilizational collapse</i>\u2026&nbsp;So, I read lots of gloomy books like <a href=\"https://www.kirkusreviews.com/book-reviews/isaac-asimov/choice-catastrophes/\"><i>A Choice of Catastrophes: The Disasters That Threaten Our World</i></a><i> </i>(by Asimov, 01979), &amp; <i>Lost Civilizations (</i>about the <i>Death of the Mayans, &amp; Easter Islanders, etc)</i>, and so on...</p><p>But - there's hope for us all yet!&nbsp;</p><p>Also, perhaps check out Isaac Asimov\u2019s (1950) short story <i>The Evitable Conflict... </i>(It\u2019ll likely give you some good ideas, for a <i>super-ethical global coordinator</i>...)</p><p><i><strong>----------------//---------------</strong></i></p><p><strong>DEFINITIONS OF KEY TERMS USED:</strong></p><blockquote><p><strong>Singleton</strong> \u2013 a global coordinator.</p><p><strong>Superintelligence </strong>\u2013 artificial general superintelligence.</p><p><strong>Existential catastrophe </strong>\u2013 We all die. Maybe, all life on Earth ends.</p><p><i><strong>The EthiSizer</strong></i><strong> </strong>\u2013 An example <i>super-ethical global governor</i>. (World Coordinator/Superintelligent Singleton)&nbsp;</p><p><i><strong>Evolutionary Culturology</strong></i><strong> </strong>\u2013 A new meta-meta-science, with one universal set of units, for all domains.&nbsp;</p></blockquote><p><i><strong>----------------//---------------</strong></i>&nbsp;<i>&nbsp;</i></p><p><strong>ON THE EXPERT PANEL\u2019S DISSENSUS ON HUMAN EXISTENTIAL CATASTROPHE (DUE TO A 2070-ARRIVING </strong><i><strong>UNFRIENDLY </strong></i><strong>AGI/SUPERINTELLIGENCE...)</strong></p><p>`Consensus\u2019 is such a wonderful word, it means <i>people (e.g., experts) agree</i>-! `Dissensus\u2019 means they <i>don\u2019t</i> agree \u2013 and in that case, you have a <i>spread of opinions</i>, instead of a Schelling point. So \u2013 some facts, according to <a href=\"https://www.openphilanthropy.org/open-philanthropy-ai-worldviews-contest/\">this webpage</a>:</p><blockquote><p>`Conditional on AGI being developed by 2070, panelist credences on the probability of existential catastrophe range from <strong>~5% to ~50%.</strong>\u2019</p></blockquote><p>But, in this short essay we\u2019ve introduced:</p><p><strong>A RISK-REDUCTION CRUX...</strong></p><p>As, importantly, also according to <a href=\"https://www.openphilanthropy.org/open-philanthropy-ai-worldviews-contest/\">that same page</a>:</p><blockquote><p>`An essay [for this competition] could clarify a concept or <strong>identify a&nbsp;</strong><a href=\"https://www.lesswrong.com/tag/double-crux\"><strong>crux</strong></a>&nbsp;in a way that made it clearer <strong>what further research would be valuable to conduct</strong> (even if the essay doesn\u2019t change anybody\u2019s probability distribution or central estimate).\u2019</p></blockquote><p>And, a handy definition of a `crux\u2019, from <a href=\"https://www.lesswrong.com/tag/double-crux\"><i>LessWrong</i></a>:</p><blockquote><p>`A <strong>crux</strong> for an individual is any fact that if they believed differently about it, they would change their conclusion in the overall disagreement.\u2019</p></blockquote><p><i><strong>----------------//---------------</strong></i></p><p><strong>AN OPEN QUESTION\u2026 WHAT EVEN </strong><i><strong>IS</strong></i><strong>: `HUMANITY\u2019S LONG-TERM POTENTIAL\u2019?</strong></p><p>Obviously we won't know, if we wipe ourselves out, first.</p><p>In simplest terms,&nbsp;<i>evolution</i> means <i>change..</i>.</p><p>And, no species sticks around, <i>forever</i>\u2026 (It's a very long time.)</p><p>...Posthumanism beckons... (if, you like that sort of thing?)</p><p>But <i>we ain\u2019t gonna make it, if we kill each other, or ourselves, or our home planet</i>, first.</p><p>...So get cracking, and be a hero...&nbsp;</p><p><i><strong>----------------//---------------</strong></i></p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/g38olizu8aar1wml0oym\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/qo7pg4pzplne8rkfj4bd 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/esff6exlazzvri5nfbb0 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/nze9oznd3dwzehjvquku 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/zvm67a7ftqcpindzbypw 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/joaed5hn9x10uown6fiu 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/j5rpiughnvxnmcdpuqmp 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/rprdfehh5h0xop7fm5qa 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/qzmfjh1n1iy1wp3j02xb 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/zh3g6ttghubggsycfz5f 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/e74yyudulhnkkye6ew52 1103w\"></p><p><strong>Velikovsky of Newcastle</strong></p><p><i><strong>Evolutionary Culturologist</strong></i></p><p><i><strong>May 02023</strong></i></p><p><i><strong>----------------//---------------</strong></i></p><p><strong>FURTHER VIEWING &amp; READING...</strong><br>&nbsp;</p><blockquote><p><a href=\"https://www.youtube.com/watch?v=DsBGaHywRhs\"><strong>AI 'godfather' quits Google over dangers of Artificial Intelligence - BBC News (May 02023)</strong></a></p><p><a href=\"https://www.youtube.com/watch?v=ewvpaXOQJoU\"><strong>Max Tegmark interview: Six months to save humanity from AI? | DW Business Special (April 02023)</strong></a></p><p><a href=\"https://www.youtube.com/watch?v=W_Nv5MCRPCk\"><strong>AI Experts Discuss Pausing AI Development | DW News (March 02023)</strong></a></p><p><a href=\"https://www.youtube.com/watch?v=iCxJUDDvq94\"><strong>Lennart Heim on Compute Governance (April 02023)</strong></a></p><p><a href=\"https://www.youtube.com/watch?v=nf-2goPD394\"><strong>Connor Leahy on the State of AI and Alignment Research (April 02023)</strong></a></p><p><a href=\"https://www.youtube.com/watch?v=ISkAkiAkK7A\"><strong>How Not To Destroy the World With AI - Stuart Russell (Aoril 02023)</strong></a></p><p><a href=\"https://www.youtube.com/watch?v=VcVfceTsD0A\"><strong>Max Tegmark: The Case for Halting AI Development | Lex Fridman Podcast #371 (April 02023)</strong></a></p><p><a href=\"https://www.youtube.com/watch?v=pkIJgZcf-Qo\"><i><strong>80,000 Hours Podcast #44 </strong></i><strong>(02019) - Dr Paul Christiano on how we'll hand the future off to AI, &amp; solving the alignment problem</strong></a></p><p><a href=\"https://www.youtube.com/watch?v=AaTRHFaaPG8\"><strong>Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization | </strong><i><strong>Lex Fridman Podcast </strong></i><strong>#368 (April 02023)</strong></a></p><p><a href=\"https://evolutionary-culturology.blogspot.com/2022/04/ev-phil-implications-of-ev-cult.html\"><strong>Evolutionary Philosophy Implications of Evolutionary Culturology (02022)</strong></a></p></blockquote><p><i><strong>----------------//---------------</strong></i></p><p><strong>REFERENCES</strong></p><blockquote><p>Bostrom, N. (02014). <i><strong>Superintelligence: Paths, Dangers, Strategies</strong></i> (First edition.). Oxford University Press.</p><p>EthiSizer, The. (02022). <i><strong>The EthiSizer\u2014A Novella-rama</strong></i>. ASL.</p></blockquote><blockquote><p><i>The EthiSizer</i> YouTube-Video <a href=\"https://www.youtube.com/watch?v=bpjVmG6zCmI&amp;list=PLmLYDE2d7WJIGXqa5ZzshuRdCHtFbayA4\"><strong>Playlist</strong></a><strong> </strong>(02023)&nbsp;</p></blockquote><blockquote><p>Harari, Y. N. (02017). <i><strong>Homo Deus: A Brief History of Tomorrow</strong></i>. Vintage Digital.</p><p>Harari, Y. N. (02018). <i><strong>21 Lessons for the 21st Century</strong></i><strong> </strong>(First ed.). Penguin Random House UK.</p><p>Sawyer, R. K. (2012). <i><strong>Explaining Creativity: The Science of Human Innovation</strong></i> (2nd ed.). Oxford University Press.</p><p>Tegmark, M. (2017). <i><strong>Life 3.0: Being human in the age of artificial intelligence</strong></i> (First). Allen Lane.</p><p>Velikovsky, J. T. (02017a). The HOLON/parton Structure of the Meme, or, The Unit Of Culture. In M. Khosrow-Pour (Ed.), <i><strong>Encyclopedia of Information Science and Technology</strong>, Fourth Edition</i> (pp. 4666\u20134678). IGI Global.</p><p>Velikovsky, J. T. (02017b). Introducing `The Robo\u2013Raconteur\u2019 Artificial Writer\u2014Or: Can a Computer Demonstrate Creativity? <i><strong>International Journal of Art, Culture and Design Technologies</strong></i>, <i>6</i>(2), 28\u201354. https://doi.org/10.4018/IJACDT.2017070103</p><p>Velikovsky, J. T. (02019). The HOLON/parton Structure of the Meme, or The Unit of Culture. In D. B. A. Mehdi Khosrow-Pour (Ed.), <i><strong>Advanced Methodologies and Technologies in Artificial Intelligence, Computer Simulation, and Human-Computer Interaction</strong></i> (pp. 795\u2013811). IGI Global.</p><p>Velikovsky, J. T. (02020). Technology: Memes (Units of Culture). In M. A. Runco &amp; S. R. Pritzker (Eds.), <i><strong>Encyclopedia of Creativity</strong></i><strong> </strong>(3rd ed., Vol. 2, pp. 585\u2013604). Elsevier, Academic Press.&nbsp;</p><p>Velikovsky, J. T. (02023).&nbsp;<i><strong>Elements of Evolutionary Culturology</strong></i>&nbsp;(2nd ed.). Amazon Services, LLC.</p><p>Walsh, T.&nbsp; (02022)&nbsp;<i><strong>Machines Behaving Badly: The Morality of AI.&nbsp;</strong></i>La Trobe University Press&nbsp;</p></blockquote><p><i><strong>----------------//---------------</strong></i></p><p><strong>ESSAY ENDNOTES:</strong></p><p><i>There are none...</i></p><p>Except, to <i><strong>NOTE:</strong></i>&nbsp;</p><p>Earth &amp; Humanity may <i><strong>END </strong></i>soon,&nbsp;</p><p>unless we all get moving faster on this...</p><p>(And P.S. - don't ever post anything that's <i><strong>info-hazardous</strong></i>... That, would be <i>Unethical</i>.)</p><p><i><strong>----------------//---------------</strong></i><br>&nbsp;</p><p><i><strong>SOME RELATED ONLINE ESSAYS:</strong></i></p><p><a href=\"https://longtermrisk.org/coordination-challenges-for-preventing-ai-conflict/\">Coordination challenges for preventing AI conflict </a>- by Stefan Torges (8 March 02021)</p><p><a href=\"https://longtermrisk.org/research-agenda\">Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda</a><br>by Jesse Clifton (Jan 02020)</p><p><a href=\"https://forum.effectivealtruism.org/posts/DZEkYatZeMSbGBAjk/why-are-we-so-complacent-about-ai-hell-1\">Why aren\u2019t more of us working to prevent AI hell?</a> by Dawn Drescher (5th May 02023)</p><p><a href=\"https://forum.effectivealtruism.org/posts/xg7gxsYaMa6F3uH8h/agi-safety-career-advice\">AGI safety career advice</a> by Richard Ngo (2 May 02023) &nbsp;</p><p>&nbsp;</p><p><i><strong>----------------//---------------</strong></i></p><p>&nbsp;</p><p><i><strong>ESSAY AUTHOR BIO</strong></i></p><p><a href=\"https://orcid.org/0000-0001-6741-066X\">J. T. Velikovsky</a>&nbsp;received his 02016 Ph.D from&nbsp;<a href=\"https://storyality.wordpress.com/2020/06/07/storyality166-the-newcastle-school-of-creativity/\"><i>The Newcastle School of Creativity</i></a>&nbsp;(University of Newcastle, Australia). He is the author of&nbsp;<a href=\"https://www.amazon.com/dp/B0BX914RZT\"><i>Elements of Evolutionary Culturology&nbsp;</i>(02023)</a><i>. </i>Velikovsky is a member of the&nbsp;<a href=\"https://philsci.org/\">Philosophy of Science Association</a>, the&nbsp;<a href=\"https://culturalevolutionsociety.org/\">Cultural Evolution Society</a>, the&nbsp;<a href=\"https://screenwritingresearch.com/\">Screenwriting Research Network</a>, the&nbsp;<a href=\"https://sites.google.com/view/appeel/home\">Applied Evolutionary Epistemology Lab</a>,&nbsp;and an Associate Editor of the&nbsp;<a href=\"https://www.igi-global.com/journal/international-journal-art-culture-design/41032\"><i>International Journal for Art, Culture, Design &amp; Technology</i></a>, and peer-reviewer for the&nbsp;<a href=\"https://www.igi-global.com/journal/international-journal-cognitive-informatics-natural/1095\"><i>International Journal of Cognitive Informatics &amp; Natural Intelligence</i></a>. Now retired, Velikovsky\u2019s past professional career spans the Creative Industries (<a href=\"https://on-writering.blogspot.com/2023/02/online-multimedia-showreel-02023.html\">film, TV, games, music, software, IT, data science, AI, etc</a>).</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/pmmk7zq6k0owrnbon8tl\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/gufvoet5rzcksxugffl4 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/rbxt3tvazwzcbqp1floj 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/mpcyfyotztwbfvbzk8i2 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/msdfsuhjollvrntqff4n 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ow0fs4sru4ccwlovndbu 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/ywiwcskss0ndujum0z2b 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/jvbs0bf17vrofbi4jr5p 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/hp4exuagvmgqe4ohtd1f 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/jdyzbrymelfhegxxnhax 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dntYZ44ySurKAZjcz/puwfrxkwlh4cwrkfmfqb 1103w\"></p><p><a href=\"https://orcid.org/0000-0001-6741-066X\"><strong>Velikovsky of Newcastle's ORCiD</strong></a></p><p><i><strong>----------------//---------------</strong></i></p>", "user": {"username": "Velikovsky_of_Newcastle"}}, {"_id": "7p6CFnd6fYYqsH42r", "title": "Graphical Representations of Paul Christiano's Doom Model", "postedAt": "2023-05-07T13:03:19.213Z", "htmlBody": "<p>Paul gives some numbers on AI doom (text below). Here they are in graphical forms, which I find easier to understand. Please correct me if wrong.</p><h1>Michael Trazzi's Probability Flow Diagram</h1><p>I really like this one. I can really easily read how he thinks future worlds are distributed. I guess the specific flows are guesses from Paul's model so might be wrong but I think it's fine.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7p6CFnd6fYYqsH42r/k7as5peqtd0nyeht9dw9\" alt=\"Image\"><br>Link to tweet: <a href=\"https://twitter.com/MichaelTrazzi/status/1651990282282631168/photo/1\">https://twitter.com/MichaelTrazzi/status/1651990282282631168/photo/1</a>&nbsp;</p><h1>My probability model version</h1><p>This is messier, but interactive. You get to see what the chances Paul puts on specific breakpoints are. Do you disagree with any?&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7p6CFnd6fYYqsH42r/nlv7cnbdm6tyc6yzsp5u\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7p6CFnd6fYYqsH42r/hsi2n8gkbrgecyvyfloa 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7p6CFnd6fYYqsH42r/m5wg5xjz0icdtdohzren 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7p6CFnd6fYYqsH42r/z2k6ddurhupooh73zpgo 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7p6CFnd6fYYqsH42r/emmzuh5gqjmpmprqvw5d 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7p6CFnd6fYYqsH42r/ep3b1q1vfk3oboyax6ai 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7p6CFnd6fYYqsH42r/sgaijpibagof3qi0yd8i 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7p6CFnd6fYYqsH42r/vjwqwhpwtxfm4ba3axef 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7p6CFnd6fYYqsH42r/gafnk99cntwamg5vyjzt 1920w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7p6CFnd6fYYqsH42r/kxmtl1gatf2kfnypetis 2160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7p6CFnd6fYYqsH42r/skuifs0iqbvhjtycutww 2390w\"></figure><p>Link: <a href=\"https://bit.ly/AI-model-Chrisitaino\">https://bit.ly/AI-model-Chrisitaino</a>&nbsp;</p><h1>Paul's model in text</h1><p><a href=\"https://ai-alignment.com/my-views-on-doom-4788b1cd0c72\">Link</a></p><blockquote><p>Probability of an AI takeover: <strong>22%</strong></p><ul><li>Probability that humans build AI systems that take over: <strong>15%</strong><br>(Including anything that happens before human cognitive labor is basically obsolete.)</li><li>Probability that the AI we build doesn\u2019t take over, but that <i>it</i> builds even smarter AI and there is a takeover some day further down the line: <strong>7%</strong></li></ul><p>Probability that most humans die within 10 years of building powerful AI (powerful enough to make human labor obsolete): <strong>20%</strong></p><ul><li>Probability that most humans die because of an AI takeover: <strong>11%</strong></li><li>Probability that most humans die for non-takeover reasons (e.g. more destructive war or terrorism) either as a direct consequence of building AI or during a period of rapid change shortly thereafter: <strong>9%</strong></li></ul><p>Probability that humanity has somehow irreversibly messed up our future within 10 years of building powerful AI: <strong>46%</strong></p><ul><li>Probability of AI takeover: <strong>22% </strong>(see above)</li><li>Additional extinction probability: <strong>9% </strong>(see above)</li><li>Probability of messing it up in some other way during a period of accelerated technological change (e.g. driving ourselves crazy, creating a permanent dystopia, making unwise commitments\u2026): <strong>15%</strong></li></ul></blockquote>", "user": {"username": "nathan"}}, {"_id": "PAq2BAdvCFAS3x7eq", "title": "Leveraging flu for biosecurity R&D", "postedAt": "2023-05-06T23:39:10.876Z", "htmlBody": "<p>There are a number of big-picture reasons why the national governments of rich democratic countries underinvest in biosecurity R&amp;D (even just considering the countries\u2019 self-interests):</p>\n<ul>\n<li>\n<p>pandemic mitigation remains a non-salient issue to the public</p>\n</li>\n<li>\n<p>the probability of a pandemic occurring within the time that a particular government is in power is very low</p>\n</li>\n<li>\n<p>the return on investment is difficult to precisely estimate</p>\n</li>\n<li>\n<p>returns on investment are not ongoing, and at least appear to be concentrated at an unspecified time in the future when a new pathogen emerges</p>\n</li>\n</ul>\n<p>However, the flu (a likely pandemic pathogen) is responsible for increased burden on health systems every winter (<a href=\"https://www.aa.com.tr/en/europe/europe-struggling-with-pressure-on-health-systems-in-tough-winter-/2788682\">https://www.aa.com.tr/en/europe/europe-struggling-with-pressure-on-health-systems-in-tough-winter-/2788682</a>) and this is likely to worsen as these countries\u2019 populations age and become even more susceptible to flu.</p>\n<p>The flu also likely poses a significant economic burden via absenteeism (workers calling in sick).</p>\n<p>As things stand though, for most (possibly all) rich democratic countries, we lack national-level estimates for both:</p>\n<p>a) the financial burden on health systems of treating flu\nb)  the economic burden of flu-related absenteeism</p>\n<p>Developing these estimates could help policy advocates make a strong case for more investment in R&amp;D to develop:</p>\n<ol>\n<li>vaccine technologies and platforms to develop more effective flu vaccines</li>\n<li>surveillance systems to detect and monitor flu outbreaks</li>\n<li>antivirals to tackle flu</li>\n</ol>\n<p>Increasing public sector R&amp;D investment which aims to tackle flu is more tractable than increasing public sector R&amp;D aimed at biosecurity generally, because healthcare systems are politically salient to voters, economic benefits are easier to precisely estimate and the returns on investment seem to be ongoing rather than concentrated at a future point.</p>\n<p>It also seems likely to me that technological progress in tackling flu will spillover to progress in tackling other potential pandemic pathogens.</p>\n<p>Additionally, the flu, RSV and seasonal COVID could be used by advocates as a reason for broad investments in R&amp;D to tackle respiratory viruses, which could have broader biosecurity benefits.</p>\n<p>In terms of next steps, EAs could look at producing estimates of the financial burden on health systems of treating flu\nand the economic burden of flu-related absenteeism for the largest economies.</p>\n<p>This seems like a good idea for:</p>\n<ul>\n<li>Effective Thesis</li>\n<li>bachelors and masters theses for economics students</li>\n<li>bachelors and masters theses for public health students</li>\n<li>people who want to test their fit in health economics research</li>\n<li>High Impact Medicine community projects</li>\n</ul>\n", "user": {"username": "freedomandutility"}}, {"_id": "xdKnfQKLyYQfeErSr", "title": "The Grabby Values Selection Thesis: What values do space-faring civilizations plausibly have?", "postedAt": "2023-05-06T19:28:56.924Z", "htmlBody": "<p><i><strong>&nbsp;Summary:</strong> The Grabby Values Selection Thesis (or GST, for short) is the thesis that some values are more expansion-conducive (and therefore more adapted to space colonization races) than others such that we should \u2013 all else equal \u2013 expect such values to be more represented among the&nbsp;</i><a href=\"https://grabbyaliens.com/\"><i><u>grabbiest civilizations/AGIs</u></i></a><i>. In this post, I present and argue for GST, and raise some considerations regarding how strong and decisive we should expect this selection effect to be. The stronger it is, the more we should expect our successors \u2013 in worlds where the future of humanity is big \u2013 to have values more grabbing-prone than ours. The same holds for&nbsp;</i><a href=\"https://grabbyaliens.com/\"><i><u>grabby aliens</u></i></a><i> relative to us present humans. While these claims are trivially true, they seem to support conclusions that most longtermists have not paid attention to, such as \u201c</i>the most powerful civilizations don\u2019t care about what the moral truth might be\u201d<i> (see </i><a href=\"https://forum.effectivealtruism.org/posts/hat6TafzAoDx97N6j/what-the-moral-truth-might-be-makes-no-difference-to-what\"><i>my previous post</i></a><i>), and \u201c</i>they don\u2019t care (much) about suffering\u201d<i> (see </i><a href=\"https://forum.effectivealtruism.org/posts/bTPP7fZxSvBzsNDES/why-we-may-expect-our-successors-not-to-care-about-suffering-2\"><i>subsequent post</i></a><i>).</i></p><h1>The thesis</h1><p>Spreading to new territories can be motivated by very different values and seems to be a&nbsp;<a href=\"https://www.youtube.com/watch?v=ZeecOKBus3Q\"><u>convergent instrumental goal</u></a>. Whatever a given agent wants, they likely have some incentive to accumulate resources and spread to new territories in order to better achieve their goal(s).</p><p>However, not all moral preferences are equally conducive to expansion. Some of them value (intrinsically or instrumentally) colonization more than others. For instance, agents who value spreading intrinsically will likely colonize more and/or more efficiently than those who disvalue being the direct cause of something like \u201cspace pollution\u201d, in the interstellar context.&nbsp;</p><p>Therefore, there is a selection effect where the most powerful<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefoygtov2dvpj\"><sup><a href=\"#fnoygtov2dvpj\">[1]</a></sup></span>&nbsp;civilizations/AGIs are those who have the values that are the most prone to \u201cgrabbing\u201d. This is the&nbsp;<i>Grabby Values Selection Thesis&nbsp;</i>(GST), which is the formalization and generalization of an idea that has been expressed by Robin Hanson (<a href=\"https://mason.gmu.edu/~rhanson/filluniv.pdf\"><u>1998</u></a>).</p><p>We can differentiate between two sub-selection effects, here:</p><ul><li><strong>The intra-civ (grabby values) selection:</strong> Within a civilization, we should expect the agents who have the values that are the most adapted to survival, replication, and expansion to eventually be selected for. In the absence of early <a href=\"https://forum.effectivealtruism.org/topics/value-lock-in\"><u>value lock-in</u></a><br>, this seems to favor grabby values, since those with the grabbiest values will be the ones controlling the most resources, all else equal. Here is a specific plausible instance of that selection effect, given by Robin Hanson (<a href=\"https://mason.gmu.edu/~rhanson/filluniv.pdf\"><u>1998</u></a>):&nbsp;<i>\u201cFar enough away from the origin of an expanding wave of interstellar colonization, and in the absence of property rights in virgin oases, a selection effect should make leading edge colonists primarily value whatever it takes to stay at the leading edge.\u201d</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwtnf8xq4so\"><sup><a href=\"#fnwtnf8xq4so\">[2]</a></sup></span></li><li><strong>The inter-civ (grabby values) selection:&nbsp;</strong>The civilizations that end up with the most grabby-prone values will get more territory than the others.</li></ul><p>Do these two different sub-selection effects matter equally? My current impression is that this mainly depends on the likelihood of an early&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/value-lock-in\"><u>value lock-in</u></a> \u2013 or of&nbsp;<i>design</i> escaping&nbsp;<i>selection&nbsp;</i>early and longlastingly, in Robin Hanson\u2019s (<a href=\"https://www.overcomingbias.com/p/will-design-escape-selectionhtml\"><u>2022</u></a>) terminology \u2013 where \u201cearly\u201d means \u201cbefore grabby values get the time to be selected for within the civilization\u201d.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0s5ovrcxfcu\"><sup><a href=\"#fn0s5ovrcxfcu\">[3]</a></sup></span>&nbsp;If such an early value lock-in occurs, the inter-civ selection effect is the only one left. If it doesn\u2019t occur, however, the importance of the intra-civ selection effect seems vastly superior to that of the inter-civ one. This is mainly explained by the fact that there is very likely much more room for selection effects within a (not-locked-in) civilization than in between different civilizations.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftedxoof4sk9\"><sup><a href=\"#fntedxoof4sk9\">[4]</a></sup></span></p><p>GST seems trivially true. It is pretty obvious that all values are not equal in terms of how much they value (intrinsically or instrumentally) space colonization, and that those who value space expansion more will expand more. This thesis is based on nothing but these very simple and uncontroversial premises. What might lead to some controversy, however, is asking&nbsp;<i>How strong is the selection effect? Do value systems significantly/drastically differ in how grabbing-prone they are? </i>The next section shallowly addresses this.</p><h1>How strong is the selection effect?</h1><p>The previous section already touches on the importance of the intra-civ and inter-civ selection effects <i>relative to each other</i>. Here, we'll consider their <i>absolute</i> importance, i.e., the extent to which we should expect a large number of values to actually be selected against.</p><p>I see two reasons to believe the intra-civ selection, specifically, would be more intense than what we may intuitively think:</p><ul><li>(i) we arguably should expect the development and deployment of something like transformative AI to make the intra-civ grabby values selection process particularly fast and strong;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyre47ockm9\"><sup><a href=\"#fnyre47ockm9\">[5]</a></sup></span>&nbsp;and</li><li>(ii) as Hanson (<a href=\"https://www.overcomingbias.com/p/this-is-the-dream-timehtml\"><u>2009</u></a>) argues,&nbsp;<i>\u201cThis is the Dream Time\u201d</i> for many present (and past) humans, who have the unusual luxury of being able to value and believe pretty much anything. Agents trying to survive/succeed in a civilization that is optimizing for space colonization probably won\u2019t have this privilege, such that their values are more likely to be determined by evolutionary pressures.</li></ul><p>One reason to suppose there wouldn\u2019t be that much (both intra-civ and inter-civ) selection favoring grabby values is that advanced civilizations might eventually converge on a moral truth. However, the <a href=\"https://forum.effectivealtruism.org/posts/hat6TafzAoDx97N6j/what-the-moral-truth-might-be-makes-no-difference-to-what\">previous post</a> within this sequence argues that this is relatively unlikely, partly because there is no reason to assume values aligned with a (potentially discoverable?) moral truth will be more competitive than those that are the most grabbing-prone.<br><br>A better reason to believe there wouldn\u2019t be that much (intra-civ and inter-civ) selection favoring grabby values, is that agents with values that might a priori seem less grabbing-prone could still prioritize colonizing space, as a first step, to not fall behind in the race against other agents (aliens or other agents within their civilization), and actually optimize for their values later, such that there is little selection effect. Call this the<i><strong> convergent preemptive colonization</strong></i> <i><strong>argument</strong></i>. Earlier, I wrote:&nbsp;</p><blockquote><p>For instance, agents who value spreading intrinsically will likely colonize more and/or more efficiently than those who disvalue being the direct cause of something like \u201cspace pollution\u201d, in the interstellar context.&nbsp;</p></blockquote><p>While these two specific value systems seem to differ greatly in terms of how grabbing-prone they are, the <i>convergent preemptive colonization argument<strong> </strong></i>suggests that others might differ much less. For instance, Carl Shulman (<a href=\"https://reflectivedisequilibrium.blogspot.com/2012/09/spreading-happiness-to-stars-seems.html\"><u>2012</u></a>) argues we should expect agents who want to maximize the number of&nbsp;<i>\u201cpeople leading rich, happy lives full of interest and reward\u201d</i> (\u201cEudaimonians\u201d) to be nearly as grabbing-prone as agents who purely want to expand (the \u201cLocusts\u201d). And although I believe his argument to be far from unassailable, Shulman tells a thought-provoking story that reminds us of how much of a&nbsp;<a href=\"https://www.youtube.com/watch?v=ZeecOKBus3Q\"><u>convergent instrumental goal</u></a> space colonization still is for various value systems.</p><p>So the relevance of both the intra-civ and inter-civ selection effect might highly depend on the specific values our minds entertain while thinking about this.&nbsp;</p><h1>Conclusion</h1><p>The Grabby Values Selection Thesis seems trivially true, but I am pretty uncertain about the significance of the selection effect. Its relevance might vary a lot depending on the exact values we are considering and \u201cmaking compete\u201d against one another. In future posts, I will investigate the significance of the selection effect, given value variations on different axes.</p><p>I warmly welcome any consideration I might have missed. More research is needed, here.</p><p>Although uncertainty is big,&nbsp;<strong>the more significant the selection effect, the more this has crucial implications for longtermists.</strong> My future posts will also touch on these implications and what GST tells us about the values we should expect our successors \u2013 and grabby aliens \u2013 to have.&nbsp;</p><h1>Acknowledgment</h1><p>Thanks to Robin Hanson for our insightful conversation on this topic. Thanks to Antonin Broi, Maxime Rich\u00e9, and Elias Schmied for helpful comments on earlier drafts. Most of my work on this sequence so far has been funded by&nbsp;<a href=\"https://erafellowship.org/\"><u>Existential Risk Alliance</u></a>.&nbsp;</p><p>All assumptions/claims/omissions are my own.&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnoygtov2dvpj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefoygtov2dvpj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>By \u201cthe most powerful\u201d, I mean \u201cthose who control the most resources such that they\u2019re also those who achieve their goals most efficiently.\u201d</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwtnf8xq4so\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwtnf8xq4so\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Other pieces have pointed at potential dynamics that are fairly similar/analogous. Nick Bostrom (<a href=\"https://nickbostrom.com/fut/evolution\"><u>2004</u></a>) explores<i> \u201cscenarios where freewheeling evolutionary developments, while continuing to produce complex and intelligent forms of organization, lead to the gradual elimination of all forms of being that we care about.\u201d</i> Paul Christiano (<a href=\"https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like\"><u>2019</u></a>) depicts a scenario where&nbsp;<i>\u201cML training [...] gives rise to \u201cgreedy\u201d patterns that try to expand their own influence.\u201d</i> Allan Dafoe (<a href=\"https://docs.google.com/document/d/1B77VWaXG-u34nSRFKV14pJNHJHHb6sa5zJ08J70CVVA/edit\"><u>2019</u></a>;&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact#Value_Erosion_through_Competition\"><u>2020</u></a>) coined the term \u201cvalue erosion\u201d to illustrate a dynamic where<i> \u201c[j]ust as a safety-performance tradeoff, in the presence of intense competition, pushes decision-makers to cut corners on safety, so can a tradeoff between any human value and competitive performance incentivize decision makers to sacrifice that value.\u201d</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0s5ovrcxfcu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0s5ovrcxfcu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>They arguably have already been somewhat selected for, via natural and cultural evolution (see <a href=\"https://forum.effectivealtruism.org/posts/xdKnfQKLyYQfeErSr/the-grabby-values-selection-thesis-what-values-do-space?commentId=KcnYWRFiGv7dtsSbB\">Will Aldred's comment</a>) long before space colonization becomes a possibility, though.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntedxoof4sk9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftedxoof4sk9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Thanks to Robin Hanson for pointing out this last part to me, and for helping me realize that differentiating between the intra-civ selection and the inter-civ one was much more important than I previously thought.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyre47ockm9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyre47ockm9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Dafoe (<a href=\"https://docs.google.com/document/d/1B77VWaXG-u34nSRFKV14pJNHJHHb6sa5zJ08J70CVVA/edit#heading=h.teqmhbq8u3ka\"><u>2019, section&nbsp;</u><i><u>Frequently Asked Question</u></i></a>) makes an analogous point.</p></div></li></ol>", "user": {"username": "Jim Buhler"}}, {"_id": "YKgr5iPayGb5edoim", "title": "Ukrainian nonviolent civil resistance in the face of war", "postedAt": "2023-05-07T07:53:26.495Z", "htmlBody": "<p>This is a linkpost for <a href=\"https://assets.nationbuilder.com/nonviolenceinternational/pages/1835/attachments/original/1667578305/Ukrainian-Nonviolent-Civil-Resistance-in-the-Face-of-War.pdf?1667578305\">this</a> report by Felip Daza studying nonviolent action in Ukraine between February and June 2022, in the context of the <a href=\"https://en.wikipedia.org/wiki/Russo-Ukrainian_War\">Russo-Ukrainian War</a>. I am sharing it because I think peaceful resistence should be considered more.</p><h1>Introduction</h1><blockquote><p>On February 26, 2022, images of Ukrainian citizens standing in the way of a column of Russian tanks in the small town of Bakhmach, northern Ukraine, went around the world1 . These were the first actions with which the nonviolent civil resistance to the Russian invasion began, and with it, this investigation. Like a colony of bees, Ukrainian society has spontaneously and courageously organised hundreds of nonviolent actions, from acts of civil disobedience to protection and evacuation of civilians. Massive nonviolent civil resistance could be decisive in ending the Russian occupation and advancing the process of democratisation and consolidation of a multi-cultural Ukrainian identity.</p><p>However, the drums of war have occupied the public debate and have gradually displaced nonviolent conflict transformation and peacebuilding initiatives. The propaganda machine has polarised warring societies2 , obscuring dissenting voices to war. The result is human catastrophe3 , increased global military spending4 and new war economies in the old continent5 in the post-pandemic period.</p><p>This report examines the Ukrainian nonviolent civil resistance against the Russian invasion from February 24 to June 30, 2022 with the aim of identifying its organisational dynamics, its impact in the context of war and avenues of support to strengthen the social actors involved. Therefore, this study is not only addressed to actors aiming to support conflict transformation in Ukraine and the region, but to any organisation or individual involved or interested in nonviolent action and conflict transformation. The Ukrainian experience is certainly unique, and from it we can learn new ways of civilian intervention in global crises without the use of weapons.</p><p>With this aspiration in mind, the report is structured in five sections. First, we define a conceptual framework of the ideas and political theories that justify the goals, dynamics, and outcomes of nonviolent action. Second, we identify the main background that shapes Ukraine\u2019s strong nonviolent resistance experience. Third, we analise the evolution, characteristics and actors of nonviolent action in the country. Fourth, we describe the impacts nonviolent action has achieved so far, as well as the challenges it has faced. Fifth, we formulate a series of recommendations for political and social actors to support nonviolent civil resistance in Ukraine and the rest of the region.</p></blockquote><h1>Executive summary</h1><blockquote><p>Ukraine is a country with more than 100 years of experience in nonviolent action. These strong capacities, combined with the informal networks of power at the local level and the country\u2019s vibrant associative fabric of self-organised communities and organisations for human rights advocacy, mediation and dialogue for conflict transformation, would explain, in part, the \u2018spontaneous\u2019 and widespread nonviolent civil resistance in the early stages of the Russian invasion, between February and June 2022, the period of study of this research. The findings and conclusions of this report are based on the analysis of 235 nonviolent actions across the country and field research with over 55 interviews with Ukrainian political and social leaders, academics, and activists. Extensive community mobilisation and organising has crystallized into hundreds of nonviolent actions of protest (148), non-cooperation (51), and nonviolent intervention (36). Geographically, the majority of actions were located in the southern oblasts (Kherson and Zaporizhia), which shows the persistence of nonviolent resistance in the areas under occupation. Temporarily, in February and March public protest actions dominated, but they were drastically reduced at the end of March due to the increase of repression and abduction of activists in the occupied territories. From April onwards, nonviolent resistance transforms into \u2018invisible\u2019 communicative actions, non-cooperation and nonviolent intervention creating structures of parallel self-government. The nonviolent civil resistance has been articulated in 7 areas of action with specific impacts and challenges:</p><p>1. OBSTRUCTIONS TO RUSSIAN MILITARY TARGETS</p><ul><li>Impacts: Nonviolent resistance has hindered some of the long-term military and political goals of the Russian authorities, such as the institutionalisation of the military occupation and repression in the occupied territories.</li><li>Challenges: Nonviolent action has coexisted and often interacted with armed resistance. The boundaries between the two types of resistance are blurring in areas such as protection of civilians, alternative communication systems or building infrastructure against the advancing Russian military machine.</li></ul><p>2. UNDERMINING THE PILLARS OF KREMLIN POWER</p><ul><li>Impacts: Persistent public demonstrations, even in traditionally pro-Russian Ukrainian regions such as Kherson, with extensive use of Ukrainian flags and symbols, has denied the Russian narrative of liberation of the Ukrainian people.</li><li>Challenges: Despite its strategic importance, there has not been a sufficiently articulated strategy between nonviolent actions to demoralise the opponent and fraternization actions that could lead to an increase in desertions in the Russian army. Also, despite the fissures in Russian public opinion, for the moment, the conditions do not exist to establish processes of dialogue and joint actions with social actors on both sides of the conflict.</li></ul><p>3. PROTECTION OF CIVILIANS</p><ul><li>Impacts: Organised civil society has built a broad protection system for the development of tasks of evacuation, transport and relocation of the population, including financial support, counselling and psychosocial help for women, human rights defenders and other groups affected by the violence. In this sense, nonviolent action has made it possible to establish negotiation processes between local communities and the Russian army to protect the civilian population and public infrastructure.</li><li>Challenges: The civil protection system is largely voluntary and has extensive training needs. Conscientious objection and defection of Ukrainian soldiers is a social \u2018taboo\u2019, involving stigmatisation and criminalisation. Unfortunately, nonviolent action could not develop in areas such as Mariupol, Irpin or Boucha, where violence and massacres of civilians prevailed.</li></ul><p>4. COMMUNITY RESILIENCE \u2022 Impacts: Communicative actions addressed to large audiences have been instrumental in preventing panic. Likewise, these type of actions have made it possible to maintain the nonviolent resistance in a clandestine way in the zones under occupation and to maintain the high morale of the population. \u2022 Challenges: Repression in the areas under occupation has increased with arbitrary arrests, enforced disappearances and cases of torture causing increased fear among nonviolent activists.</p><p>5. STRENGTHENING LOCAL GOVERNANCE</p><ul><li>Impacts: Nonviolent action has had a significant impact on the preservation of the associative fabric, the empowerment of social actors at the local level and the improvement of coordination between local authorities and citizens. This impact has had a direct effect on the strengthening of local governance closely linked to the processes of political decentralisation and democratisation in Ukraine.</li><li>Challenges: Beyond monitoring of war crimes, and youth volunteering initiatives, the vast majority of nonviolent actions at the local level have not been coordinated at the national level, causing communication problems and ineffectiveness. The tendency to centralise decision-making on the reconstruction of the country in the presidential cabinet marginalises the work carried out, needs and demands of social actors at the local level.</li></ul><p>6. NATIONAL AND REGIONAL SOCIAL COHESION</p><ul><li>Impacts: The solidarity of the Ukrainian people is an opportunity for encounter between the communities of the East and West. Nonviolent action has a fundamental character of cultural resistance, which contributes to the consolidation of Ukrainian identity.</li><li>Challenges: There are extensive intra-family mediation needs and, as time progresses, there are serious risks of increased social polarisation. Mediation and dialogue-facilitation communities require support to respond to increased social mediation needs. Organisations working on monitoring Russian propaganda and developing new nonviolent narratives need support to penetrate public opinion. Low level of interest on the part of Ukrainian social actors to develop dialogue or conflict transformation initiatives with Russian or regional counterparts.</li></ul><p>7. ACCOUNTABILITY</p><ul><li>Impacts: The robust war crimes monitoring infrastructure created by leading human rights organisations and advocacy centers in Ukraine has enabled the collection and verification of thousands of cases of serious violations committed by Russian troops. These actions have helped to prevent the defencelessness of the Ukrainian population affected by the war and have empowered citizens to denounce damage to physical infrastructure and abuses of the civilian population through various physical or virtual means.</li><li>Challenges: The need to strengthen transitional justice processes by including mechanisms for truth, justice, reparation and reconciliation. One of the first challenges in this regard would be to also recognise the human rights violations committed by both sides of the conflict. It is also important to make progress in disaggregating data on human rights violations committed by soldiers, civilians, and Private Military and Security Companies.</li></ul></blockquote>", "user": {"username": "vascoamaralgrilo"}}, {"_id": "JFy5YewFngRgSJ3fd", "title": "Rank best universities for AI Saftey", "postedAt": "2023-05-06T13:20:20.586Z", "htmlBody": "<p>I often hear of people talk of an implicit ranking of best places to do a PhD focusing on AI Saftey. Can anyone enumerate this for me?</p>\n<p>Something like</p>\n<ol>\n<li>Berkeley</li>\n<li>Stanford</li>\n<li>MIT\n\u2026.</li>\n</ol>\n", "user": {"username": "Parker_Whitfill"}}, {"_id": "DoJFtZWfwjbZv9SJX", "title": "Tips for building a network outside of the EA movement (in particular for EU policy)", "postedAt": "2023-05-06T06:40:02.165Z", "htmlBody": "<h2>Epistemic Status</h2><p>This post is mainly a synthesis of personal observations. It aims to provide an actionable overview for (European) policy-interested people involved in the EA movement to broaden their networks. However, it is not a comprehensive analysis, and readers are encouraged to think about pros and cons of certain approaches and to find other influential networks than the ones suggested in this article.</p><h2>Post summary</h2><ul><li>I think policy-interested people in the EA movement are currently undervaluing the importance of building a network outside the EA bubble.</li><li>In the past, people involved in the EA movement have often prioritised insular approaches, building communities around universities with large EA populations, forming EA houses, and attending EA conferences.</li><li>While these efforts have been valuable in fostering a sense of identity and cultivating shared values, they have unintentionally limited the movement's reach and influence.</li><li>For people involved in the EA movement to create a meaningful impact on a global scale, it is crucial to understand and harness the power of embeddedness in a diverse range of policy circles.</li><li>Practical steps for EU policy-interested EA individuals:<ul><li>Prepare a hook for communication with more senior people and err on the cautious / humble side</li><li>Attend non-EA conferences (some suggestions&nbsp;<a href=\"https://docs.google.com/document/d/1fhTGazvoSFcIV3I1lgAFxIGtuGk_naGxeEbL74pBbN8/edit#heading=h.5eyt0a64fwqj\"><u>at the end of the doc</u></a>)</li><li>Join other (relevant) associations, non-EA houses</li><li>Join and volunteer at a political party&nbsp;</li><li>Diversify media and literature consumption</li><li>Map important players and networks and define actions</li></ul></li></ul><h2>The Importance of Embeddedness in Diverse Policy Circles</h2><p>For people involved in the EA movement to create a meaningful impact on a global scale, it is crucial to understand and harness the power of embeddedness in a diverse range of policy circles.&nbsp;</p><p>Embeddedness refers to the extent to which actors are connected to, and integrated within, broader social networks. By fostering strong connections with influential individuals and organizations, people involved in the EA movement can tap into networks that drive real-world policy changes (see also&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/XeADodQvLvPTHcYZe/different-forms-of-capital?commentId=Si4pqtgJxwrDQ8wvS\"><u>this comment</u></a> by James Herbert about impact mobility).</p><p>Expanding one's network beyond the EA community is crucial for several reasons:</p><ul><li><strong>Understanding a wider variety of perspectives</strong>: Fosters a more comprehensive understanding of complex issues and helps to avoid echo chambers.</li><li><strong>Larger expected marginal value per contact:&nbsp;</strong>The assumption is that your network outside of the EA movement will be less likely to overlap with other networks from other people involved in the EA movement. In that case the marginal value for the EA community of adding someone outside of the EA movement to your network will be relatively larger than adding an additional person involved in the EA movement. As mentioned in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/XeADodQvLvPTHcYZe/different-forms-of-capital\"><u>this post</u></a>, people can share these contacts from outside the EA movement with other people involved in the EA movement to make them more valuable.</li><li><strong>Meeting (future) powerful people:</strong> Establishes valuable connections with potential allies and decision-makers.</li><li><strong>Better understanding of \u201cthe system\u201d and of influence:</strong> Enables more effective policy interventions.</li><li><strong>Connections for jobs and favors:</strong> Helps further your goals through strategic opportunities and collaborations.</li><li><strong>Honing your communication skills:</strong> Interacting with individuals from different backgrounds makes it easier to convey certain ideas to wider audiences.</li><li><strong>Having opportunities to seed ideas (in a cautious way)</strong>: Plants the seeds for policy change.</li><li><strong>Inspiring more people to take part in the EA project</strong>: Bringing in people from other policy networks into the EA movement could be valuable for the total impact of the movement.</li></ul><p>Potential downsides include:</p><ul><li><strong>Opportunity cost&nbsp;</strong><ul><li><strong>In general:&nbsp;</strong>There are a lot of other activities one can prioritise and networking is not necessarily high value.</li><li><strong>To get more embedded in the EA community</strong>: which can be very useful especially if one is relatively new to the EA movement.</li></ul></li><li><strong>Value drift:</strong> people lose touch with core EA principles and might suffer from decreasing levels of epistemics amongst other concerns of value drift.</li><li><strong>Less motivation to report findings</strong> back to the community: When people don\u2019t prioritise their contacts within the EA movement, the movement loses out on receiving insights from object-level work in policy.</li><li><strong>Accidental harm and poisoning the well:&nbsp;</strong>It is important to communicate about EA and new or sensitive EA(-related topics) and cause areas in a cautious and high fidelity way to avoid misrepresentation of certain ideas.</li></ul><h2>Historically Insular Effective Altruism</h2><p>In the past, people involved in the EA movement have often prioritised insular approaches; building communities around universities with large EA populations, forming EA houses, and attending EA conferences. While these efforts have been valuable in fostering a sense of identity and cultivating shared values, they have unintentionally limited the movement's influence and reach.</p><p>Insularity within the Effective Altruism movement not only limits its reach and influence but also risks stifling diversity of thought. By primarily engaging with like-minded individuals, we unintentionally create echo chambers, where ideas and beliefs are reinforced without being challenged or broadened. This can hinder our ability to consider alternative perspectives and develop more effective, innovative solutions to pressing global issues. It is crucial to engage with diverse networks and expose ourselves to a wider range of ideas to ensure the robustness and adaptability of our movement. See also&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/YzrbyEGrDEyZDX6dH/ideas-from-network-science-about-ea-community-building#A_network_of_strong_ties_and_weak_ties\"><u>this post</u></a> for similar thoughts</p><p><a href=\"https://www.jstor.org/stable/2776392\"><u>Sociologist Mark Granovetter's \"Strength of Weak Ties\" theory</u></a> posits that weak ties\u2014connections with acquaintances or individuals outside one's immediate social circle\u2014are more valuable for accessing new information and resources. Building weak ties in influential non-EA networks can lead to novel opportunities and partnerships.</p><h2>Actions for the average EU Policy-interested Person to Take</h2><p>For people involved in the EA movement focused on policy within the European Union, several practical steps can help broaden their influence and foster greater embeddedness in influential networks:</p><ul><li><strong>Prepare a hook to arrange meetings and create credibility:&nbsp;</strong>When engaging with the average policymaker, it's crucial to communicate your ideas effectively and respectfully. Prepare a compelling \"hook\u201d for a conversation (e.g. you write a thesis on AI Governance) or key message that succinctly conveys the essence of your cause, while also being open to feedback and alternative viewpoints.</li><li><strong>Coordinate and be cautious/humble:&nbsp;</strong>Before you have object-level chats with policy makers outside of EA circles, make sure to coordinate with people in the EA movement. Err on the cautious side in your communication style and use these networks mainly to learn and network, instead of direct advocacy. See also&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/HCuoMQj4Y5iAZpWGH/advice-on-communicating-in-and-around-the-biosecurity-policy\"><u>this excellent post</u></a> on communication about Biosecurity, which applied to a lot of other cause areas as well.</li><li><strong>Create a healthy balance of EA conference and non-EA conferences</strong>: Given you are already well-connected within the EA movement: Prioritize attending a non-EA conference (relevant to your field) over an additional EA event. This will allow you to forge connections with influential individuals outside the EA community, gain fresh perspectives, and identify potential allies.</li><li><strong>Join other (professional) associations and non-EA social groups:&nbsp;</strong>Actively building a network outside of the EA movement gives you a different network than a lot of other people involved in the EA movement have and is therefore extremely valuable.&nbsp;This can be through professional networks, but I\u2019ve also met relevant people through living in larger (non-EA) houses or playing in football teams in Brussels.</li><li><strong>Join a political party</strong>: Becoming a member of a political party can provide an avenue for influencing policy from within. Use your expertise to help the party craft policy proposals related to your cause area, thereby increasing the likelihood of your ideas being implemented. Bycatch could be that you position yourself as a potential Accredited Parliamentary Assistant (APA) specialised in AI for after the elections.</li><li><strong>Diversify your media and literature diet</strong>: Broadening your information sources can lead to a better understanding of the policy landscape and the systems in which you are operating. In the EU, consider following outlets such as&nbsp;<a href=\"https://www.politico.eu/\"><u>Politico's website</u></a>,&nbsp;<a href=\"https://www.politico.eu/eu-confidential-podcast/\"><u>EU Confidential podcast</u></a>,&nbsp;<a href=\"https://www.euractiv.com/\"><u>Euractiv</u></a>, and&nbsp;<a href=\"https://open.spotify.com/show/7eVoT7zCD8bNEfH0kItHs0?si=4672a8dce9e44584\"><u>Euractiv's tech brief podcast</u></a> to stay informed and gain valuable insights. I also recommend more people reading biographies and stories about people who understood the concept of embeddedness, for example about&nbsp;<a href=\"https://www.lesswrong.com/posts/5b6YcFbEBCZbX6YSK/jean-monnet-the-guerilla-bureaucrat\"><u>Jean Monnet</u></a> or&nbsp;<a href=\"https://en.wikipedia.org/wiki/The_Years_of_Lyndon_Johnson\"><u>Caro\u2019s work on Lyndon B. Johnson</u></a>.&nbsp;</li><li><strong>Map important players and networks and take subsequent action</strong>: Create a map of key stakeholders, networks and conferences specific to your cause area. Define actionable steps to engage with these entities and share your findings with others in the EA community.</li></ul><h2>Broadening Our Scope: Some Suggestions for Networks</h2><p>Some examples of (European) networks and conferences can be found below. Feel free to suggest additional conferences and networks in the comment section!</p><p><a href=\"https://www.alpbach.org/\"><strong><u>European Forum Alpbach</u></strong></a></p><ul><li>Date: 19. August - 2. September 2023 (different themed days)&nbsp;</li><li>Summary: A platform for interdisciplinary exchange and dialogue, gathering leaders from science, politics, business, and culture.</li><li>Cost: Varies depending on the program and ticket category (500\u20ac-1,000\u20ac). Scholarships are available and accessible through both your national Forum Alpbach Network group or the official Website. National Groups tend to be less competitive (depending slightly on your country of origin) thus it\u2019s encouraged to apply through those first.&nbsp;</li><li>Application: Normal Ticket Sale Open, Scholarships are open from&nbsp;from 1 March, 12 pm to 31 March, 12 pm (National Groups vary on this)</li></ul><p><a href=\"https://www.globalshapers.org/\"><strong><u>Global Shapers Community</u></strong></a></p><ul><li>Ongoing, with&nbsp;<a href=\"https://www.globalshapers.org/hubs\"><u>local hubs</u></a> and events</li><li>Summary: A network of young leaders under 30 driving dialogue, action, and change to improve the state of the world.</li><li>Cost: Membership is free, but participation in events may require travel and accommodation expenses.</li></ul><p><a href=\"https://www.oneyoungworld.com/\"><strong><u>One Young World</u></strong></a></p><ul><li>Date: 2-5 October 2023</li><li>Summary: One Young World empowers and develops young leaders to build a fair, sustainable future for all. They host an annual One Young World Summit to confront the biggest challenges facing humanity</li><li>Costs: There are a few different&nbsp;<a href=\"https://www.oneyoungworld.com/scholarships\"><u>scholarships</u></a> young leaders can apply for to gain entrance</li></ul><p><a href=\"https://eudevdays.eu/\"><strong><u>European Development Days</u></strong></a></p><ul><li>Date: tbd for 2023</li><li>Summary: The European Development Days (EDD) are Europe\u2019s leading forum on international partnerships. Organised by the European Commission, the forum brings key actors together to share ideas and experiences in ways that inspire new partnerships and innovative solutions to the world\u2019s most pressing challenges.</li><li>Costs: free but must Carbon Offset. You can apply via their EDD Young Leaders Programme</li></ul><p><a href=\"https://www.weforum.org/events/world-economic-forum-annual-meeting-2024\"><strong><u>World Economic Forum (WEF) Annual Meeting</u></strong></a></p><ul><li>Date: 15-19 January 2024</li><li>Summary: An international conference where global leaders discuss pressing issues and collaborate on shaping the global, regional, and industry agendas.</li><li>Cost: Attendance is by invitation only. Corporate membership fees apply, and individual tickets are priced at a premium.</li></ul><p><a href=\"https://securityconference.org/\"><strong><u>Munich Security Conference (MSC)</u></strong></a></p><ul><li>Date: February 2024&nbsp;</li><li>Summary: An annual conference focusing on international security policy, attracting high-profile attendees from politics, military, academia, and industry.</li><li>Cost: Participation is by invitation only. Pricing information is not publicly available.</li></ul><p><a href=\"https://www.symposium.org/\"><strong><u>St. Gallen Symposium</u></strong></a></p><ul><li>Date: 4.-5. May 2023&nbsp;</li><li>Summary: The St. Gallen Symposium is an annual gathering of current and future leaders from various fields, including politics, business, and academia. It provides a platform for intergenerational dialogue on pressing global issues and fosters an exchange of ideas across cultures and disciplines.</li><li>Cost: Participation is by invitation only, with a limited number of tickets available for purchase. Students can apply to become a \"Leader of Tomorrow\" for a chance to receive an all-expenses-paid invitation.</li></ul><p><a href=\"https://brusselsforum.org/\"><strong><u>Brussels Forum by the German Marshall Fund</u></strong></a></p><ul><li>Date: 23-24 May 2023</li><li>Summary: GMF's Brussels Forum is the preeminent platform for global leaders, policymakers, and experts across sectors to shape the transatlantic agenda and debate the most pressing global challenges.</li><li>Costs: To be announced</li></ul><p><a href=\"https://falling-walls.com/\"><strong><u>Falling Walls Summit Berlin</u></strong></a></p><ul><li>Date: 7. - 9. November 2023</li><li>Summary:&nbsp;Guided by the question \"Which are the next walls to fall in science and society?\" the Science Summit aims to showcase scientific breakthroughs, foster a dialogue between science and general public and connect global leaders from science, business, and media to develop solutions for the greatest challenges of our time.<br>Costs: unknown</li><li>Application deadline: be nominated or apply between&nbsp;1 March \u2013 1 May 2023 through their \u201cGlobal Call\u201d&nbsp;</li></ul><p><a href=\"https://www.global-solutions-initiative.org/summit-2023/\"><strong><u>Global Solutions Summit</u></strong></a><strong>&nbsp;</strong></p><ul><li>Date: 15. - 16. May&nbsp;</li><li>Summary: Since 2017, the Global Solutions Summit has provided an intense, two-day forum for the world\u2019s leading minds to propose and debate research-based policy recommendations for the G20, G7 and beyond.&nbsp;<br>Costs: Free<br>Application: unclear?</li></ul><p>Apart from the examples above I recommend attending conferences organised by EU/Brussels think tanks (e.g. CEPS Ideas Lab) and media outlets (e.g. Politico organises&nbsp;<a href=\"https://www.politico.eu/events/\"><u>regular conferences</u></a> such as the&nbsp;<a href=\"https://www.politico.eu/tech-summit/#Program\"><u>Europe Tech Summit</u></a> ). See also the&nbsp;<a href=\"https://euagenda.eu/events\"><u>EU Agenda</u></a> for EU related Events and the European Commission's&nbsp;<a href=\"https://commission.europa.eu/events_en\"><u>Events Page</u></a>.&nbsp;</p>", "user": {"username": "JOMG_Monnet"}}, {"_id": "BMzmCohuPYRaGPcZD", "title": "Maybe Family Planning Charities Are Better For Farmed Animals Than Animal Welfare Ones", "postedAt": "2023-05-06T02:02:28.930Z", "htmlBody": "<p><a href=\"https://nautil.us/you-can-save-more-animals-by-donating-100-than-going-vegan-238401/\">This</a> piece estimates that a donation to the Humane League, an animal welfare organization considered highly cost-effective, and which mainly engages in corporate lobbying for higher welfare standards, saved around 4 animals per dollar donated, mostly chickens. \u201cSaving a farmed animal\u201d here means \u201cpreventing a farmed animal from existing\u201d or \u201cimproving the welfare of enough farmed animals by enough to count as preventing one farmed animal from existing.\u201d That second definition is a little weird, sorry.</p><p>If you\u2019re trying to help as many farmed animals as possible this seems like a pretty good deal. Can we do better? Maybe.</p><p>Enter MSI Reproductive Choices, an international family planning organization, which mainly distributes contraception and performs abortions. <a href=\"https://www.msichoices.org/news-and-insights/resources/msi-s-2021-annual-report-and-financial-statements/\">They</a> reported in 2021 that they prevented around 14 million unintended pregnancies on a total income of 290 million pounds, or 360 million dollars at time of writing. This is roughly 25 dollars per unintended pregnancy prevented. Let\u2019s pretend that for every unintended pregnancy prevented, a child who would have been born otherwise is not born. This is plausibly true for some of these unintended pregnancies. But not all. On the other hand, MSI also provided abortions which plausibly prevent child lives as well. Maybe that means MSI prevented 14 million child lives from starting in 2021 (if we think the undercounting from not including abortions counter perfectly the overcounting of unintended pregnancy). I have no reason to think that\u2019s particularly plausible, but let\u2019s just keep pretending that\u2019s right.</p><p>Let\u2019s further pretend that all of MSI\u2019s work happened in Zambia. MSI does work in Zambia, but they also do work in lots of other countries. I choose Zambia mostly because trying to do this math with all the countries that MSI works with would be hard. Zambia had a life expectancy at birth of 62 years in 2020 according to <a href=\"https://data.worldbank.org/indicator/SP.DYN.LE00.IN?locations=ZM\">this</a>. According to <a href=\"https://ourworldindata.org/grapher/per-capita-meat-type?country=~ZMB\">this</a>, Zambians consumed an average of 28kg of meat per person per year. The important subfigures here are the 2.6kg of poultry and 13kg of seafood per person per year, since chickens and fish are much lighter than other animals killed for meat. One chicken provides say 1kg of meat (I\u2019m sort of making this number up, but similar numbers come up on google). One fish provides say 0.5kg. This means that the average Zambian would eat 2.6 chickens and 26 fish per person per year. Over a lifetime, that\u2019d be 62 years of consumption.</p><p>If a human who would have otherwise existed no longer exists because of your efforts, they also no longer eat the meat they would have eaten otherwise. Thus, if MSI prevents one human lifetime for every $25 you donate, then you\u2019d be saving 62*(2.6+26) farmed animals which is around 1,750. That\u2019s 70 animals saved per dollar donated.</p><p>This analysis is so bad in so many ways. I took the number for animals saved per dollar donated to The Humane League on total faith. I also just assumed that MSI is correct in saying that they prevented 14 million unintended pregnancies and I made clearly bad assumptions to get from that number to number of human lifetimes prevented. At least we can have some confidence in the total weight of meat consumed on average by a Zambian per year and the life expectancy at birth in Zambia. However, my way of getting from total weight to animals slaughtered is pretty hokey and doesn\u2019t even include cows, sheep, pigs, etc. There are many other problems too. For example, I took the <i>average</i> cost per unintended pregnancy prevented by MSI. However, the average is not the relevant figure here. We\u2019d like the marginal cost of preventing an additional unintended pregnancy. This is a figure I don\u2019t have and one which could (and is likely to!) be very different from the average. I could write about many more ways this is a bad analysis.</p><p>However, if we keep pretending for a second to believe in these numbers, 70 animals saved per dollar is better than 4 animals saved per dollar. Maybe the above provides some weak evidence that donating to a family planning charity like MSI can save more animals per dollar than donating to an animal welfare one like the Humane League.</p>", "user": {"username": "Hank_B"}}, {"_id": "j6dBnG6iSx8aaZ7M2", "title": "Orthogonal's Formal-Goal Alignment theory of change", "postedAt": "2023-05-05T22:36:15.195Z", "htmlBody": "", "user": {"username": "carado"}}, {"_id": "cJc3f4HmFqCZsgGJe", "title": "Don't Interpret Prediction Market Prices as Probabilities", "postedAt": "2023-05-05T20:23:35.204Z", "htmlBody": "<p><em>Epistemic status: most of it is right, probably</em></p>\n<p>Prediction markets sell shares for future events. Here's an example for the 2024 US presidential election:</p>\n<p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cJc3f4HmFqCZsgGJe/ebfbdus4nigzlvecdnik\" alt=\"\"></p>\n<p>This market allows any US person to bet on the gender of the 2024 president. Male shares and female shares are issued in equal amounts. If the demand for shares of one gender is higher than shares of the other, the price is adjusted.</p>\n<p>At the time of writing, female shares cost 17 cents, and male shares cost 83 cents. If a female president is elected in 2024, owners of female shares will be able to cash them out for $1 each. If not, male shares can be cashed out for $1. The prices of male and female shares sum up to $1, which makes sense given that only one of them will be worth $1 in the future.</p>\n<p>Because bettors think a female president is relatively unlikely, the price for male shares is higher. The bettors may be wrong here, but the beauty of prediction markets it that anyone can put their money where their mouth is. If you believe that a female president is more likely than a male president, you can buy female shares for 17 cents a piece. If you're right, each of these shares will likely appreciate to $1 by 2024, almost sextupling your investment. If enough people predict a female president to be more likely, the demand for female shares will grow until they are more expensive than male shares. As such, the price of the shares reflects the predictions of everyone involved in the market.</p>\n<p>Even if you believe a female president is, say, 25% likely, you'd still be inclined to buy a female share for 17 cents. (That is, if you'd take a 1 in 4 chance of a 500% return on investment.) The interesting thing is that whenever you buy shares, the price will move closer to the probability you perceive be true. Only when the price matches your perceived probability, the market is no longer interesting for you. Because of this, the price of a share reflects the crowd's perceived probability of the corresponding outcome. If the market believes the probability to be 17%, the price will be 17 cents.</p>\n<p>Or so the story goes.</p>\n<h1>$</h1>\n<p>In reality, it's more complicated.</p>\n<p>You're betting in a currency and, as such, you're betting on a currency.</p>\n<p>Let's say you believe a male president is about 90% likely, so you're considering buying male shares at 83 cents. Every 83 cents you put in can only become $1, so your maximum return on investment (ROI) is about 20%. Your expected ROI is closer to about 8% because you believe there's only a 90% chance the president will be male. Still, that's a positive ROI, so why not make the bet?</p>\n<p>This bet is denominated in US dollars, and it will be only resolved in 20 months or so. The problem is that US dollars are subject to inflation.</p>\n<p>Instead of locking up our investing money in a long-term bet for nearly two years, we could instead put it in an index fund, like the S&amp;P 500, or invest in a large number of random stocks. Both methods have historically had a 10% annualized return. That's much better than an 8% two-year return!</p>\n<p>Because everyone thinks this way, there will be an artificially low demand for boring long-term positions, like predicting that the next US president will be male. This will drive the price of these shares down, while driving the price of shares for low-probability events up. A share that pays out USD will never have a price that reflects the market's perceived probability, because most people believe there are better things to invest in than USD.</p>\n<p>There's a solution for this, although regulators might not like it: allow people to bet bonds or shares. The <a href=\"https://longbets.org/362/\">famous 1 million USD bet</a> between Warren Buffett and Protege Partners was actually not denominated in USD, but in bonds and shares.</p>\n<h1>Betting</h1>\n<p>Compared to the stock market, prediction market bets are extremely risky. If you get in at an incredibly bad time on the stock market, your portfolio might temporarily go down 50% for a year. You hit a bad streak on prediction markets, you could easily lose everything you invest. This is because of the binary nature of prediction markets: shares either resolve to $1 or to $0. Sports bettors try to counteract this by betting on a large number of events, but it remains more stressful than investing in the stock market.</p>\n<p>As a bettor on news events, it's even harder to diversify your bets sufficiently. The outcomes of the events you're betting on are likely correlated.</p>\n<p>Even if a person sees a large gap in collective reasoning, it makes perfect sense not to bet large sums on prediction markets if they don't want to expose themselves to any risk, so you can only count on people tolerant of risks to correct prices.</p>\n<h1>Hedging</h1>\n<p>Let's say you're a Spanish farmer and you want to insure yourself against a cold winter where none of your bell peppers will grow. You could bet on a prediction market that the average temperature in Almeria will be below 15 degrees Celsius. That way, if a cold winter hits, you'll get your prediction market payout, and if it doesn't, you can sell your bell peppers.</p>\n<p>This, I think, is a completely legitimate use of prediction markets. However, if a significant chunk of bettors is hedging, prices will slip away from the perceived true probability. Farmers don't bet because they think the price is wrong, they're buying insurance.</p>\n<p>I don't know if, historically, large prediction markets have ever started slipping as the result of hedging. In the case of the bell pepper farmers, investment funds swiftly correct the prices for <a href=\"https://en.wikipedia.org/wiki/Weather_derivative\">weather derivatives</a> any time they are warped by people looking for insurance.</p>\n<h1>Outcomes</h1>\n<p>Sane folks will never bet that the world is ending, no matter the payout. Rather than locking away their money, they'll try to spend it while the world is still there. Similarly, no one will ever bet that USD goes to zero in a bet denominated in USD.</p>\n<p>There are more subtle forms of this. Let's say there's a prediction market on whether most people are represented by AI lawyers by 2035. Of course, bettors will want to estimate the probability of this event. But there are other things to take into account:</p>\n<ul>\n<li>The probability that money still has any value whatsoever by 2035 if AI is capable of representing people in court</li>\n<li>The probability that money still has any value whatsoever by 2035 if AI cannot represent people in court</li>\n<li>The probability that the world will end soon after AI starts representing people in court</li>\n</ul>\n<p>If people strongly believe that soon after AI becomes capable enough to represent people in court, money/humanity will cease to exist, it makes no sense for them to bet on AI lawyers. As far as these folks are concerned, money is only worth something in a technologically dormant future. As such, they'd always bet that technology will develop slowly, even if they think it'll happen fast.</p>\n<h1>Discussion</h1>\n<p>Prediction markets are often touted for bringing the wisdom of the people to the people. Yet, for the things this community cares most about, we should expect prediction market probabilities to be distorted by bettors' hidden considerations.</p>\n<p>I'm not sure if there's any evidence of distortions actually occurring. Then again, many people still treat participating in these markets as a badge of honor rather than a real investment.</p>\n<p>One way to get some insight into this: if prediction markets are mostly still used by hobbyists, non-monetary prediction websites like Metaculus ought to be just as reliable as prediction markets such as Polymarket.</p>\n", "user": {"username": "bob"}}, {"_id": "yAbYQrmK82qLvuQcN", "title": "A Soothing Frontend for the Effective Altruism Forum", "postedAt": "2023-05-05T19:59:14.564Z", "htmlBody": "<h2>About</h2><p><a href=\"https://forum.nunosempere.com/\">forum.nunosempere.com</a> is a frontend for the Effective Altruism Forum. It aims to present EA Forum posts in a way which I personally find soothing. It achieves that that goal at the cost of restricted functionality\u2014like not being able to make or upvote comments and posts.</p><h2>Usage</h2><h3>Frontpage</h3><p>This forum frontend has a frontpage at <a href=\"https://forum.nunosempere.com/frontpage\">forum.nunosempere.com/frontpage</a>. It displays the latest posts in chronological order, and looks like this:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yAbYQrmK82qLvuQcN/yxtuvdd2if6qd3wi1i3a\"></p><h3>Recent comments</h3><p>This forum frontend has a page showing the latest comments at <a href=\"https://forum.nunosempere.com/comments\">forum.nunosempere.com/comments</a>. It displays the latest comments in chronological order, together with their optional parent comment, and looks like this:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yAbYQrmK82qLvuQcN/sokzpv82jccob3uyscca\"></p><h3>Posts</h3><p>Like the EA forum, this frontend has an endpoint for posts at:</p><pre><code>https://forum.nunosempere.com/posts/[post id]/[optional post slug]</code></pre><p>An <a href=\"https://forum.nunosempere.com/posts/fz6NdW4i25FZoRQyz/use-of-i-d-bet-on-the-ea-forum-is-mostly-metaphorical\">example post</a>\u2014chosen to be a link post so that you can see both the article and the comments\u2014looks like this:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yAbYQrmK82qLvuQcN/pyyzdip3rcvyv2qeyqaw\"></p><h3>RSS feed</h3><p>This frontend has an rss endpoint at <a href=\"https://forum.nunosempere.com/feed\">forum.nunosempere.com/feed</a>: it piggybacks off the <a href=\"https://ea.greaterwrong.com/?format=rss\">ea.greaterwrong.com RSS feed</a>, fetching and reprocessing it every five mins.</p><h2>Further notes</h2><ul><li>Clicking on an article's or on a comment's timestamp directs to their location on the original EA Forum. This is useful, for example, to go to the original forum to leave a comment.</li><li>My usecase is just being subscribed via rss (using <a href=\"https://newsboat.org/\">newsboat</a>), and displaying forum posts which interest me in this new frontend.</li><li>Other EA Forum alternatives known to me are <a href=\"https://ea.greaterwrong.com/\">ea.greaterwrong.com</a> and <a href=\"https://eaforum.issarice.com/\">eaforum.issarice.com</a>.</li><li>This frontend takes around half a second to load. The regular EA Forum takes around 5 seconds.</li></ul>", "user": {"username": "NunoSempere"}}, {"_id": "CMfrQBrSwpujaqF8Z", "title": "How much do you believe your results?", "postedAt": "2023-05-05T19:51:22.703Z", "htmlBody": "", "user": {"username": "UnexpectedValues"}}, {"_id": "o4e6PsdPQE7rRjzWy", "title": "Thinking of Convenience as an Economic Term", "postedAt": "2023-05-05T19:09:04.135Z", "htmlBody": "<p><i>Epistemic Status: These are rough ideas. Similar to my recent </i><a href=\"https://forum.effectivealtruism.org/posts/hAHNtAYLidmSJK7bs/who-is-uncomfortable-critiquing-who-around-ea\"><i>two</i></a><i> </i><a href=\"https://forum.effectivealtruism.org/posts/TfqmoroYCrNh2s2TF/select-challenges-with-criticism-and-evaluation-around-ea\"><i>posts</i></a><i> about criticism and comfort, much of my thinking here comes from introspection and experience, as opposed to formal literature reviews. &nbsp;If you know of great literature in this area, comments are appreciated!</i><br><br>TLDR: We can define \"<i>convenience</i>\" as an economic term to roughly mean \"<i>specific local benefit</i>\", then use it to clarify incentives around things like communication and belief. I lay out how this can work, how we can use \"<i>convenience accounting\"</i> at a large (but expensive) scale, give a bunch of examples of how the term can be used in the wild, attempt to formalize the concept, and more. &nbsp;</p><h2>Introduction</h2><p>Incentives are important, but discussing them can feel amorphous.</p><p>There\u2019s a lot of scattered terminology. We have terms like <i>incentives</i>, <i>incentive gradients</i>, <i>signaling</i>, <i>motivated reasoning</i>, <i>cognitive dissonance</i>, <i>bias</i>, and even <a href=\"https://www.urbandictionary.com/define.php?term=Flexing\"><i>flexing</i></a>.</p><p>I find this terminology to be frustratingly vague in practice. I like modeling decisions in terms of estimatable costs and benefits, but I haven\u2019t had success putting these concepts into satisfying cost-benefit models.</p><p>After playing around with different ideas, I\u2019ve so far settled on the concept of <i>convenience</i> as a helpful construct.</p><p>Simply put, a <i>convenience</i> is a <i>specific local/private benefit</i>. They\u2019re typically intangible.</p><p>To many, this definition might closely match their existing understanding of the word <i>convenience</i>, enough that much of this essay might seem trivial.</p><p>I think the concept is a decent mix of straightforward and valuable. Actually estimating the full costs and benefits of conveniences would be a lot of work, but at least it can be clear what the path forward would be.</p><p>I\u2019m particularly interested in conveniences in communication and thought and present examples accordingly. Our work at <a href=\"https://quantifieduncertainty.org/\">QURI</a> is about building effective evaluations and estimates, so it\u2019s important to understand where these might be hindered or assisted based on different pressures.&nbsp;</p><p>Since formalizing the concept, I\u2019ve noticed many of these incentives in the wild. I\u2019ve built further terminology on top of it and intend to expand on this in future posts.</p><p>As always, I\u2019m eager to get feedback.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2bf508e-e146-48cd-8d87-4d04009ad71b_1460x1416.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2bf508e-e146-48cd-8d87-4d04009ad71b_1460x1416.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2bf508e-e146-48cd-8d87-4d04009ad71b_1460x1416.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2bf508e-e146-48cd-8d87-4d04009ad71b_1460x1416.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2bf508e-e146-48cd-8d87-4d04009ad71b_1460x1416.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2bf508e-e146-48cd-8d87-4d04009ad71b_1460x1416.png 1456w\"></a></p><p><i>Someone in 2030 using special VR glasses to list all of the conveniences in social settings they come across. It\u2019s a valuable lens!</i></p><h1><strong>Examples of convenient actions for myself</strong></h1><p>Let\u2019s start with examples of <i>conveniences</i> in my life. I think these examples should generally match some of people\u2019s existing intuitions on the word.</p><h3><i><strong>Convenient</strong></i><strong> (beneficial) things for me:</strong></h3><ol><li>I use examples in my writing that makes me seem somewhat sophisticated.</li><li>I use a lot of defensive language (\"<i>X is great</i>\u201d -&gt; \u201c<i>I think X might be great</i>\"). This makes it harder to attack my writing.</li><li>I occasionally champion issues that my friends/colleagues like seeing championed.</li><li>If I criticize someone, it's typically someone I know my community would have an easy time being criticized.</li><li>I post a lot of writing on Facebook, where it\u2019s more likely to be seen by people who I socially interact with. If these people think well of me, that\u2019s particularly valuable to me.</li><li>I present certain vulnerabilities to my community that I\u2019m confident would get them to generally like me more, not less.</li><li>I\u2019ve found that my well-being correlates with my status among peers much more than I\u2019d like. When I see my work be well-liked, that just makes me feel good, even without thinking of the instrumental impacts.</li></ol><h3><i><strong>Inconvenient</strong></i><strong> things I (mostly) avoid:</strong></h3><ol><li>Discuss controversial+heated ideas or issues. These occasionally bring trolls or online mobs and can cause me a lot of grief.</li><li>Use examples in blog posts that would be personally embarrassing or make me look weak.</li><li>Present content that's so rough or unpolished (given expectations, in its given format), that others would likely think much less of me.</li><li>Reveal early drafts of work publicly, in situations where I'd expect that they would get more attention if they were hidden until the final release.</li><li>I often don't cross-post my Facebook posts to the EA Forum. I find the comments on Facebook much more agreeable and less emotionally taxing.</li><li>Negatively critique of groups with much power over QURI.</li><li>Posting or talking about expensive items I own. I don\u2019t want to be seen as someone who is flashy.</li><li>Posting information that would be inconvenient for my colleagues, friends, and family. This means I have a hard time being fully straightforward about some of the work at QURI, for instance. I think this is the pro-social thing to do for my group, but not always for the greater world.</li></ol><h3><strong>Beliefs that are </strong><i><strong>convenient</strong></i><strong> for me.</strong></h3><p>I genuinely believe these to be true. At the same time, I should also flag that these beliefs seem pragmatically beneficial to me (at least in the short term). I can only have active beliefs about so many things (even true things), so I\u2019m incentivized to focus on those that would be more valuable.&nbsp;</p><ol><li>QURI has a good shot at (some) success.</li><li>The lightcone/humanity has a good shot at success.</li><li>The effective altruism ecosystem is generally healthy and valuable.</li><li>I'm on the net, producing value, in expectation.</li><li>I\u2019m not going to have too many disasters happen to myself or ones close to me in the near future.</li><li>My friends and colleagues are mostly friendly, moral, and effective.</li><li>It\u2019s cost-effective for me to have various comforts in my life, because they make me more productive in total.</li><li>I have many left-leaning views, that make me less likely to get canceled or shunned by my colleagues.</li><li>This blog post might be genuinely useful for me to write.</li></ol><h2><strong>Convenience Accounting</strong></h2><p>As for <i>estimating things not meant to be estimated</i>, we can imagine estimating the costs and benefits of a think tank producing a research document.</p><p>There are many actors in such a situation. There\u2019s the organization, but then there are individuals within the organization. Outside the organization are donors, potential recruits, potential collaborators, and all the other relevant roles.</p><blockquote><p>Aside: More generally, whenever an action happens in the world, every existing and future actor, globally, could be impacted, in a way that\u2019s net-positive or net-negative. The vast majority of these impacts will be infinitesimal, but they still exist and can be (poorly) estimated. Deciding what to pay attention to really just depends on interest and existing capabilities.&nbsp;</p></blockquote><p>Let\u2019s imagine that this hypothetical piece of research most directly provides value to a certain greater community. We\u2019ll make a simple example of the costs and benefits for the nonprofit organization and for the community.</p><p>When both turn out positive, that means that producing the document is in the interests of both the nonprofit and the community. In an aligned environment, their incentives will be heavily correlated.</p><p>It\u2019s possible to get arbitrarily complicated with this if you have the capacity. You could outline complicated network structures of many actors, add probability distributions, adjust for the fact that some items probably overlap, and more.</p><p>I imagine that this work is generally too much work for humans, but maybe with AI, we\u2019ll be able to implement it on a routine basis eventually.</p><h3><strong>Content Producer Ledger</strong></h3><p><strong>Basics:</strong></p><ul><li>Time Cost: -$4,000</li><li>Cost of editing + feedback: -$500</li><li>Cost discount, due to this post being entertaining to write: +$500</li><li>Learning value: +$200</li></ul><p><strong>Conveniences:</strong></p><ul><li>Impress potential donors (through demonstrating consumer value): +$4,000</li><li>Impress potential employees: +$2,000</li><li>Impress friends/colleagues: +$200</li><li>Flow-through: Helping customers, in ways that will later benefit the producer: +$400</li><li>Achieving a \u201cwin\u201d as an organization, and feeling more productive: +$300</li><li>Achieving a \u201cwin\u201d for the author, making them personally feel better: +$200</li></ul><p><strong>Inconveniences:</strong></p><ul><li>Risk of upsetting specific donors: -$600</li><li>Risk of causing a PR crisis: -$300</li><li>Risk of generating online harassment or abuse: -$200</li><li>Risk of upsetting Xavier2320, a certain commenter who often complains loudly on our work: -$200</li><li>Will upset some other actor, which this content portrays in a negative light: -$200</li><li>Organization members other than the author might feel jealous that they lose relative status and attention. -$200</li></ul><p><strong>Producer Profit: $1,600</strong></p><h3><strong>Content Consumer Ledger</strong></h3><p><strong>Time Costs</strong></p><ul><li>Reading and processing content: -$5,000</li><li>Related comments, and the reading/processing of those comments: -$1,000</li></ul><p><strong>Learning (Decision Value)</strong></p><ul><li>Direct learning: Using the information to directly make better decisions: +$4,000</li><li>Technique learning: Learning about the techniques and styles used in the post: +$1,000</li><li>Groundwork learning: Having ideas that can be built upon in future work: +$4,000</li><li>Producer learning: The information reveals things about the producer. It might show that the producer is productive (useful for funders), but it might also reveal other useful information. +$500</li></ul><p><strong>Signaling / Conveniences</strong></p><ul><li>Direct Signaling: Having more evidence to support existing community beliefs: +$1,000</li><li>Technique Signaling: Your community can gain reputability by showing it does quality work: $1,000</li><li>Flow-through: The information producer is helped, and this correlates to benefits for the consumer down the line: $200</li></ul><p><strong>Inconveniences</strong></p><ul><li>Risk of causing a PR crisis: -$1,000</li><li>Alienation of some of the audience: -$800</li><li>Some group\u2019s work is negatively impacted by content, for some reason: -$500</li><li>Status equilibria: The bar for research content is slightly raised, meaning that other competing organizations look worse in comparison: -$500</li></ul><p><strong>Consumer Profit: $2,900</strong></p><p>&nbsp;</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2a11b6e-b752-44ee-aa2e-82c7417686df_1470x1484.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2a11b6e-b752-44ee-aa2e-82c7417686df_1470x1484.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2a11b6e-b752-44ee-aa2e-82c7417686df_1470x1484.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2a11b6e-b752-44ee-aa2e-82c7417686df_1470x1484.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2a11b6e-b752-44ee-aa2e-82c7417686df_1470x1484.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2a11b6e-b752-44ee-aa2e-82c7417686df_1470x1484.png 1456w\"></a></p><p><i>An accountant foolishly trying to list all the conveniences of a global political announcement, using binary for some reason</i></p><p>&nbsp;</p><p>Note that you can\u2019t simply add these profits to get the \u201c<i>total profit to both parties.</i>\u201d There\u2019s some overlap. Also, it\u2019s unclear if any actor would care about the utility of these two parties equally.</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Value(Nonprofit) + Value(Community) \\neq Value(Nonprofit + Community)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">l</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">l</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;\">y</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">\u2260</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">l</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;\">y</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span></p><h2>Formalizing the terminology</h2><p>I\u2019ve so far given a handwavy definition and some examples. Can we try to produce a more formal definition?&nbsp;<br><br>Here\u2019s one attempt:</p><blockquote><p>Conveniences are <i>private</i>, <i>intangible</i>, <i>tangental</i> benefits.</p></blockquote><p><strong>Private</strong></p><p>We\u2019re interested in <i>agent-specific benefits,</i> not <i>total benefits</i>. Often a belief or communication will result in a wide array of winners and losers. The specific benefits to a certain agent are often barely correlated with the total net benefit. For example, one person might have incentives to lie, even though this harms society.</p><p>A <i>beneficial</i> belief is one that\u2019s good for society as a whole. But a <i>convenient</i> belief is one that is good for a specific party. It might be good on the whole or not - this is beside the point.</p><p><i>Private benefit</i> is an existing term in economics and law. From <a href=\"https://www.economicsonline.co.uk/definitions/private_benefit.html/\">Economics Online</a>,&nbsp;</p><blockquote><p><i><strong>Private benefit</strong></i> is the benefit derived by an individual or firm directly involved in a transaction as either buyer or seller. The private benefit to a consumer can be expressed at the utility, and the private benefit to a firm is profit. Private benefit can be contrasted with external benefit.</p></blockquote><p>I think \u201c<i>agent-specific</i>\u201d or \u201clocal\u201d can be clearer but they have other downsides.</p><p><strong>Intangible</strong></p><p>The examples I have in mind of conveniences are intangible. I don\u2019t think I would classify \u201c<i>this transaction produced several effects, including clearly giving me 1000 widgets</i>\u201d as a convenience, but I would if my benefit were in terms of social capital, or if the benefit to myself were obfuscated.</p><p><strong>Tangental</strong></p><p>Conveniences are not the clear primary public goal of their corresponding activities. Communication is typically seen as being about conveying information, not about anyone signaling allegiances or trying to gain status. These aspects might be decisive motivators, but they are kind of tacked on.</p><p>Economics <a href=\"https://ideas.repec.org/p/sus/susewp/1117.html\">sometimes</a> uses the terms <i>secondary benefits</i>, <i>co-benefits</i>, and <i>ancillary benefits</i> to mean similar things. But I see the primary communication or belief as more of an orthogonal vessel or delivery mechanism for conveniences to attach to. The mechanism often itself doesn\u2019t have a clear benefit, its use is almost orthogonal or tangental.</p><p><strong>Use in the Wild</strong></p><p>When an <i>inconvenience</i> is significant enough, I find it easier to say <i>uncomfortable </i>instead. It doesn\u2019t feel right to say, \u201c<i>It\u2019s inconvenient for you to learn you [did something terrible you didn\u2019t realize].</i>\u201d It feels better to say, \u201cIt\u2019s <i>uncomfortable</i> for you...\u201d</p><p>If the word <i>inconvenience</i> sounds insulting or distrustful, you can substitute other words, just point to a definition somewhere.</p><p>Trying to set formal definitions ahead of use is often wrought with failure. If you find yourself wanting to use <i>convenient</i>, feel free to adjust the definition if it helps your use case. If you use a definition that might confuse readers, I suggest being explicit about that.</p><h2>Adjusting Sentences to Use \u201cConvenience\u201d</h2><p>As stated in the introduction, I\u2019ve found \u201cconvenience\u201d to be a useful handle in many situations.</p><p>Here are some example phrases with and without the term. I think the term's use either shortens them or adds clarity. I\u2019m sure readers might be divided here, so see which ones you prefer.&nbsp;</p><p><i>\"It is difficult to get a man to understand something when his salary depends upon his not understanding it.\"</i><br><i>\u201cIt\u2019s difficult to get someone to understand something incredibly inconvenient for them to understand.\u201d</i></p><p><i>\u201cI think that you saying that is very specifically advantagous to you.\u201d&nbsp;</i><br><i>\u201cI think that\u2019s a very convenient thing for you to say.\u201d&nbsp;</i></p><p><i>\u201cYou believing that comes with you gaining some personal benefits.\u201d</i><br><i>\u201cThat\u2019s a convenient thing for you to believe.\u201d</i></p><p><i>\u201cThere are some topics we avoid discussing because there are several incentives for us not to.\u201d</i><br><i>\u201cThere are some topics we avoid discussing because of inconveniences.\u201d</i></p><p><i>\u201cThere\u2019s an incentive gradient for group X to avoid discussing scary topics.\u201d</i><br><i>\u201cThere are many inconveniences around group X discussing certain topics.\u201d</i></p><p><i>\u201cI think Samantha believes that because of motivated reasoning.\u201d</i><br><i>\u201cI think this is a very convenient thing for Samantha to believe, and I also think that those conveniences are counterfactually responsible for that belief.\u201d</i></p><p><i>\u201cThis organization is biased.\u201d</i><br><i>\u201cThis organization is wrong in ways that are very convenient.\u201d</i></p><p><i>\u201cThe reasons they gave were very suspiciously beneficial to themselves.\u201d</i><br><i>\u201cThe reasons they gave were suspiciously convenient.\u201d</i></p><p><i>\u201cSenator Dodd\u2019s bill comes with a bunch of provisions that are specifically useful for his state.\u201d&nbsp;</i><br><i>\u201cSenator Dodd\u2019s bill comes with a bunch of provisions that are convenient for his state.\u201d&nbsp;</i></p><h2>Benefits and Risks</h2><p>There are two main clarifications that I like about this term.</p><ol><li>Clarity that the benefit is only for one specific agent. The impact on society is an entirely different matter. I find that many statements about local benefits tend to get mistaken for ones about global benefits, so extra clarity is helpful.</li><li>Clarity that you are discussing incentives, not decisions, beliefs, or actions. Words like <i>motivated reasoning</i>, <i>cognitive dissonance</i>, <i>bias</i>, and <i>flexing</i> imply that incentives have actively changed decisions, beliefs, or actions. It should be possible to discuss incentives without accusations - there\u2019s a very different question on where these incentives might actually change behavior.</li></ol><p>Unfortunately, discussions about incentives tend to be heated. In everyday use, convenience is often meant to imply bias, which doesn\u2019t help here.</p><p>Meaningful discussion of convenience probably requires very high <a href=\"https://www.lesswrong.com/posts/7cAsBPGh98pGyrhz9/decoupling-vs-contextualising-norms\">decoupling</a> norms. I\u2019d expect online discussions around convenience to very commonly get hijacked by mistakes or accusations around these two points, but I hope that the term, when taken with a definition like what I provided, helps more than it hinders.</p><h2>Critical Responses</h2><p>I imagine that some potential critics of this might say things like,</p><h3>Is it immoral to take convenience into account when producing content?</h3><p>Critic #1 says,</p><blockquote><p>Personal conveniences shouldn\u2019t be considered when writing content. You should only be thinking about the impact on the world. Each piece you write should be as useful as that piece can potentially be, and nothing else should matter. It\u2019s <i>immoral</i> to change what you write based on what\u2019s useful to you.</p></blockquote><p>I adhere to something like the rational actor model of humans, which assumes that humans generally do things that are positive-EV. Bad actions are often done for selfish reasons, but so are good actions.</p><p>Emphasizing personal incentives shifts some amount of responsibility from individuals to environments. But people have the ability to shape environments. Instead of focusing completely on blaming people who do things for selfish reasons, we can also focus on making sure their incentives align with what we care about.</p><p>Also, writers often need to plan ahead several steps. If they burn all of their goodwill in one piece that would limit their work in the future. They need to be at least somewhat strategic with their work in the short term in order to maximize their impact in the long term.</p><h3>Applying \u201cconvenience\u201d to the side I don\u2019t like</h3><p>Critic #2 says,</p><blockquote><p>Since learning about conveniences, I\u2019ve realized that all the intellectuals I don\u2019t like are really just saying whatever is convenient for them. I\u2019ve made big lists of all the ways their content is convenient to both themselves and convenient to their annoying audiences. This proves they how useless and harmful their work is.</p></blockquote><p>I\u2019m happy that you\u2019ve learned to consider convenience as it applies to one side. However, I\u2019ll make two recommendations.</p><p>Apply convenience analysis to your side first. There are conveniences everywhere.</p><p>Be cautious and measured when making assumptions about the motives of others. It\u2019s very easy to mess this up. Deeply empathizing with friends is challenging; with enemies, it\u2019s much more so.</p><h3>This terminology sucks</h3><p>Critic #3 says,</p><blockquote><p>I think it\u2019s dangerous to try to redefine commonplace words like this. Instead you should use something more specific, like \u201c<i>localized benefit</i>\u201d. Having technical words that are really similar but not exactly like their common uses will create massive headaches down the road.<br><br>Plus, who are you to propose new terminology anyway? Almost no one will actually take this up, and those that do will just create confusion. We have too much hastily-presented jargon as it is.</p></blockquote><p>First, my main interest here is the general topic, not the name. If we both discuss the same thing but use different names, that\u2019s not that harmful compared to us simply not discussing that thing. I\u2019d be curious to see others use alternative names for the same thing, as long as it\u2019s clear that\u2019s what\u2019s going on. I\u2019d be happy to see this sort of concept become popular under any name.</p><p>Second, I guess that many of my readers will prefer and remember the term convenience more than anything like <i>localized benefit</i>. I\u2019ve found it much easier to use and remember. I think the word is close enough to colloquial use that errors here won\u2019t be that bad. These are empirical claims, I guess we\u2019ll get evidence if anyone else actually reads this.</p><p>But I am unsure. I wish we had some controlled vocabulary service with serious experience and authority on this issue, but we don\u2019t. I\u2019ve tried raising this discussion with other people, but most treat terminology discussions as incredibly tedious. Thoughts and advice in the comments are appreciated.</p><h3>Isn\u2019t this just media theory?</h3><p>Critic #4 says,</p><blockquote><p>Isn\u2019t this just media theory and sociology, disguised for rationalists? You really need to understand Slavoj \u017di\u017eek and Derrida before trying to make your own theories here.</p></blockquote><p>Slavoj \u017di\u017eek and media theory were definitely inspirations for this. I\u2019m not as well-versed in continental philosophy as I\u2019d like. If anyone reading this is interested in helping out by occasionally providing perspectives here, do reach out!</p><p>Also, this content is very similar to writing on status and signaling. In my circle, some prominent figures here are <a href=\"https://en.wikipedia.org/wiki/Robin_Hanson\">Robin Hanson</a>, <a href=\"https://en.wikipedia.org/wiki/Diana_Fleischman\">Diana Fleischman</a>, and <a href=\"https://en.wikipedia.org/wiki/Geoffrey_Miller_(psychologist)\">Geoffrey Miller</a>.</p><h3>Discussing convenience is itself inconvenient</h3><p>Critic #5 says,</p><blockquote><p>Dude, you\u2019re not supposed to say this stuff out loud. If you talk about the incentives behind your writing, people will assume that you\u2019re biased. You\u2019re just giving your enemies ammunition to use against you.</p></blockquote><p>I think that our epistemic environment will be healthier if people develop a better understanding of bias and convenience.</p><p>If you\u2019re in an environment where it\u2019s incredibly unusual to disclose your potential biases and conveniences, then it will seem weird and potentially disadvantageous to do this yourself. However, if you\u2019re in an environment where doing so is a strong norm, then it actually could become convenient for you to be honest about these things. That\u2019s the kind of world I\u2019d like us to move towards.</p><h3>This is uncomfortable!</h3><p>Critic #6 says,</p><blockquote><p>I find it uncomfortable to think about. It feels like you\u2019re attacking me.</p></blockquote><p>Finding a topic uncomfortable is a promising sign! Many real epistemic issues are uncomfortable to think about and talk about. Because of this, I think we should be particularly empathetic and understanding when discussing them.</p><p>I\u2019ll do my best to keep my discussions at least friendly and non-threatening (I hope the cute illustrations help). My point is to understand our environment better, not to attack anyone.</p><p>I\u2019m guilty of being biased around convenience too, I think it\u2019s just how humans and organizations work. I think it\u2019s important to watch out for your self-interest, you just should generally be honest about your motives (especially to yourself) and avoid situations where you\u2019ll do harm to those around you.</p><h2><strong>Acknowledgments</strong></h2><p><i>Thanks to Nu\u00f1o Sempere for comments.</i></p>", "user": {"username": "oagr"}}, {"_id": "jra9axWurjMMYqxR5", "title": "Introducing the AI Objectives Institute's Research: Differential Paths toward Safe and Beneficial AI", "postedAt": "2023-05-05T20:26:48.468Z", "htmlBody": "<p><i>Cross-posted to Progress Forum </i><a href=\"https://progressforum.org/posts/Ed3BhKkrFtkXhL2Bb/introducing-the-ai-objectives-institute-s-research-1\"><i>here</i></a><i>.</i></p><p><i>This is a post about how the AI Objectives Institute plans to implement a differential technological development strategy in its research and development projects. For a complete list of projects, see our&nbsp;</i><a href=\"https://aiobjectives.org/blog/research-brief-beneficial-deployment-of-transformative-technologies\"><i><u>Research Brief</u></i></a><i>.</i></p><p>&nbsp;</p><p>With a seeming Cambrian explosion of capabilities from the progress in generative models, there has been an increased need for differential exploration \u2013 of alternative (more alignable and governable) paradigms, applications of current capabilities for increasing the sanity waterline, and experimenting with institutional configurations and ways of deploying capabilities directed robustly towards human (and sentient) flourishing.</p><p>The AI Objectives Institute (AOI) aims to fill the role we think is necessary here. Using differential technological development and exploration as a guiding principle, we'll work on improving strategies for prioritization models of safe and beneficial deployment of capabilities, and will conduct applied AI research on solutions we identify based on those strategies. Our goal is to make both theoretical and applied progress towards the following goals:</p><ul><li>Applying current and future AI to solving problems of alignment and governance</li><li>Safeguarding the human race against the unknown consequences of technological development</li><li>Steering AI toward applications that benefit humanity</li></ul><p>In this post, we'll limit discussion of differential technological development to our thoughts about applying that principle to AI and other transformative technologies. For more general discussions of <a href=\"https://forum.effectivealtruism.org/topics/differential-progress\"><u>differential strategies</u></a>, see:</p><ul><li><a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4213670\"><u>Differential Technology Development: A Responsible Innovation Principle for Navigating Technology Risks</u></a> (Sandbrink et al, 2022), a paper we draw from heavily (including the definition we use above)</li><li><a href=\"https://forum.effectivealtruism.org/posts/J6QCmkQmuRaP7skje/differential-technology-development-preprint-on-the-concept\"><u>This post</u></a>, in which two authors of the Sandbrink et al paper summarize their work</li><li><a href=\"https://forum.effectivealtruism.org/posts/XCwNigouP88qhhei2/differential-progress-intellectual-progress-technological\"><u>This post</u></a> discussing the differences between differential technical development, differential&nbsp;<i>intellectual</i> progress, and differential progress as a whole.</li></ul><p><br>If after reading this you want to&nbsp;<a href=\"https://aiobjectives.org/get-involved\"><u>collaborate with us</u></a>, or if you'd rather send us feedback directly instead of commenting on this post, reach out to&nbsp;<a href=\"mailto:hello@objective.is\"><u>hello@objective.is</u></a>.</p><p>&nbsp;</p><h2><strong>Differential Development for AI &amp; Social Systems</strong></h2><p>Differential technological development (DTD) is particularly appealing as a strategy for progress on beneficial AI deployment because:</p><ul><li>It doesn't necessitate perfect prediction of the impacts of new technology, so it can still be applied with some degree of efficacy in situations of high uncertainty (such as the future of AI).</li><li>It's applicable at any point in the process of technological development \u2013 though earlier applications may be more impactful, there is not a point at which the strategy becomes irrelevant.</li><li>It suggests ways to make progress without directly opposing existing incentive gradients, which can complement strategies that involve changing those incentives.</li></ul><p>For questions about beneficial adoption of AI, the advantages of a DTD approach go beyond just mitigating risks from new technology \u2013 because the scope of these questions includes the dynamics of existing social systems, such as institutions and governance. This means that&nbsp;<strong>making differential progress on beneficial deployment will likely result in progress towards solving existing problems in human systems</strong>, making us better at responding to new technology efficiently and effectively.</p><p>While it's true that all technological progress has social effects, highly capable AI holds potential for more direct effects via integration with decision-making systems that were previously run by exclusively human labor \u2013 or, in other words, the effects are direct because we will use AI to change how we organize ourselves. If we can find solutions to problems with the dynamics of technical systems, they may generalize to the dynamics of social systems, or vice versa \u2013 resulting in both specific progress on integrating AI systems, and improvements to decision-making processes that will continue guiding that integration through the future. There's more detail on this line of thinking in&nbsp;<a href=\"https://aiobjectives.org/whitepaper\"><u>our whitepaper</u></a>.</p><p>&nbsp;</p><p><strong>Taxonomies for Differential Development of AI</strong>&nbsp;</p><p>We propose the following categories to think about DTD in this context:</p><p>1.&nbsp;<strong>Paradigms for safe, more governable systems</strong>: These include frameworks, technical insights, and principles for how to deploy systems safely and in service of their intended goals. Specific outcomes from interpretability research fall into this category, as do projects like the&nbsp;Open Agency Architecture (see Example Projects below). Exploring alternative paradigms can be especially useful work for an organization not focussed on scaling up capabilities, since these organizations can operate outside of the paradigm lock-in effects that may occur within competitive dynamics.</p><p>2.&nbsp;<strong>Support for civilizational resilience by raising the sanity waterline</strong>: This category includes all technologies that make human populations more likely to notice threats from new technology, effectively defend themselves against (or avert) those threats, work towards their own best interests within increasingly novel and confusing contexts, and make effective and informed decisions at critical moments. Examples of this group include tools for identifying machine-generated content, and tools that help people clarify their own desired outcomes.</p><p>3.&nbsp;<strong>Prefiguration of post-TAI beneficial sociotechnical outcomes</strong>: The presence of novel artificial capabilities gives us an opportunity to experiment with how these capabilities may be integrated into our societies in beneficial and corrigible ways, without harmful and corrosive effects. Informed and prescient work on this front can provide valuable direction and insights for beneficial deployment, especially as the <a href=\"https://hearthisidea.com/episodes/sandberg-live#are-we-near-the-end-of-humanitys-tech-tree\">tech tree</a> of emerging capabilities becomes clearer to us.</p><p>&nbsp;</p><p>AOI's work includes efforts across these categories, with a focus on integration of social and technical systems. At the moment, AOI doesn't have plans to pursue work on technical alignment or interpretability research that outside of such integration \u2013 progress on those fronts is essential, but so is laying the groundwork for incorporating those insights into our plans for AI governance.</p><p>It's also worth noting that we plan to do no additional research into scaling the capabilities of the models we apply \u2013 our aim is to apply existing models to problems of beneficial deployment, not to build more powerful AI ourselves.<br>&nbsp;</p><h2><strong>Background on the AI Objectives Institute</strong></h2><p>The AI Objectives Institute is a non-profit research and development lab working to ensure that people can thrive in a world of rapidly deployed, extremely capable AI systems. \u200b\u200bFounded by the late internet security pioneer Peter Eckersley, we collaborate with teams across disciplines, developing tools and programs that put humanity in a better position to benefit from AI technologies and avoid large scale disasters.</p><p>One way to think about humanity\u2019s current position is as an escape room: we have limited time to find our way out of the situation, and our search for clues \u2013 for ways to govern AI systems \u2013 depends on maintaining the integrity of our collective deliberation under pressure. To make it out of the room, we must solve the relevant puzzles while communicating with our teammates and keeping our wits about us in a strange and overstimulating world. AOI's three research avenues map roughly to the requirements of problem-solving capacity, cooperation, and autonomy necessary for success, and we aim to make differential progress in each:</p><ul><li><strong>Sociotechnical alignment</strong>: promoting AI development paradigms that favor governability of institutions and AI systems, including proofs of concept for alternative AI development paradigms.&nbsp;</li><li><strong>Scalable coordination</strong>: promoting ways to make cooperation and deliberation more scalable, including demonstrations of socially beneficial applications of AI for improving collective agency.</li><li><strong>Human autonomy</strong>: research on preserving and enhancing civilizational health and agency, including experiments with resilience-enhancing applications of existing AI capabilities.</li></ul><p>As a research incubator, AOI hosts a wide range of applied projects informed by our working groups of internal &amp; affiliated researchers. The combination of theory and practice not only lets us support a variety of different approaches, but also creates feedback loops that help us learn about the problem-spaces we are working in. Our current research focuses include assessment models for the societal impacts of AI, as well as frameworks for researching sociotechnical alignment cruxes like responsible deployment, deliberative processes, and assistive vs deceptive behavior.<br>&nbsp;</p><h2><strong>AOI's Theory of Impact</strong></h2><p>We believe that working towards differential progress along the three research directions above is valuable in multiple but closely interacting ways:</p><p>1.&nbsp;<strong>Governance matters, and improving societal capacity for governance is one of the best ways to address existential risk.&nbsp;</strong>Reducing that risk is a universal public good, while failure to address existential risk is a failure of social coordination. We expect improvements to public deliberation and global coordination systems will reduce existential risk.&nbsp;</p><p>2.&nbsp;<strong>AI could be massively beneficial, and figuring out how to realize the potential of transformative AI in line with human flourishing should be a high priority.</strong> The challenge of making transformative AI good for humanity goes beyond solving technical alignment. We believe that deliberations about how to deploy transformative AI \u2013 or perhaps even understanding the possibility space of post-singularity positive outcomes \u2013 require serious inquiry into the role advanced AI can play in enhancing human autonomy, well-being, and security.&nbsp;&nbsp;</p><p>3.&nbsp;<strong>Better governance and use of AI will require experimentation with new AI engineering paradigms.&nbsp;</strong>The work of ensuring that AI development remains in service of human flourishing<strong>&nbsp;</strong>will involve directly confronting engineering challenges that we expect mainstream, large-scale AI research will not pursue. This makes AOI comparatively well-positioned to explore new AI engineering and governance paradigms, develop more governable systems, and influence the broader field.</p><p>4.&nbsp;<strong>Present-day work on socially beneficial AI will have a significant impact on the sociotechnical development of future AI.</strong> We believe that immediately socially beneficial interventions in AI have natural continuity with the long-term alignment of AI to human flourishing, but the relationship between the two can be complex and unexpected. Systematic thinking about principles of civilizational health and agency can help secure the long-term benefits of our interventions, and the foresight that comes of that thinking may complement existing immediacy-focused work.<br>&nbsp;</p><h2><strong>Example AOI Projects</strong></h2><p>Below are a few examples of the types of projects we're working on. For a full list of projects in active development, see our&nbsp;<a href=\"https://aiobjectives.org/blog/research-brief-beneficial-deployment-of-transformative-technologies\"><strong><u>Research Brief</u></strong></a>.</p><p>Even the research brief isn't a full list of projects we're currently considering, nor do we think it is a conclusive list of the best possible ideas for achieving safe integration of transformative technology. Our goal is to be exploratory and flexible in what we pursue, maintaining a balance of steady work on the most promising ideas while being responsive to new developments in this rapidly changing area of development. We're interested in feedback on the current list of projects, and other ideas in line with our mission.<br>&nbsp;</p><h3><strong>Theory of Beneficial Deployment</strong></h3><p>Much of AOI's research focuses on differential technological development of AI capabilities: determining the best order in which to develop new technology, to ensure total benefits outweigh negative effects. In a paper on this subject, we will investigate how we might model the decision process for that differential development as an iterated game. In each round of such a game, civilization can exercise some capabilities that will either deplete or enhance its autonomy in subsequent rounds, affecting its ability to use future capacities responsibly.</p><p>Our aim is to use this model in developing a cause prioritization strategy that specifies properties about the world which, if maintained, would ensure that the world keeps going in a better state. One such property would be confidence that algorithmic speech is usually truthful (see Evans et al, <a href=\"https://arxiv.org/abs/2110.06674\">Truthful AI: Developing and governing AI that does not lie</a>). With that confidence, we could continue distinguishing true from false information, resulting in less pollution of the information ecology. What other such features, we ask, would be most important in prioritizing differential AI research and deployment?</p><p>&nbsp;</p><h3><strong>Open Agency Architecture (Sociotechnical Alignment)</strong></h3><p>(Note: we're working on a whole post about collaborating with Davidad on OAA, to &nbsp;be published separately)</p><p>The&nbsp;<a href=\"http://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai\"><u>Open Agency Architecture</u></a> (OAA) proposes a framework for how institutions can adopt highly capable AI. OAA composes bounded, verified components \u2013 which can be either human or AI systems \u2013 to handle separate parts of the problem-solving process, with human oversight at the points of connection. Building in oversight at this structural level gives OAA systems more flexibility to incorporate new components, allowing for much broader collaboration on building these components, while limiting the risk of misalignment from each new part.</p><p>AOI is implementing a proof-of-concept OAA prototype, to create a foundation for coordinating concrete work on beneficial deployment \u2013 integrating input from researchers and practitioners across all areas of governance, sociotechnical alignment, and AI capabilities research. We will start by demonstrating that OAA's modular architecture can achieve parity with \u2013 or even outcompete \u2013 the current generation of monolithic models in bounded learning environments. Our overall goal is to develop an open-source blueprint for creating institutions that can evolve to incorporate continuously revised and improved processes, including not just transformative AI but also AI-assisted innovations in governance.<br>&nbsp;</p><h3><strong>Talk to the City (Scalable Coordination)</strong></h3><p>Talk to the City is a LLM interface for improving collective deliberation and decision-making by analyzing detailed, qualitative responses to questionnaires. It aggregates those responses, clusters distinct viewpoints, and represents each with a LLM chat interface\u2013&nbsp;yielding a simulation of what a citizens' assembly for that group might look like.</p><p>Applying recent advances in AI to problems of group coordination \u2013 and ensuring that these technologies are deployed safely and in true alignment with the people they serve \u2013 requires technical research into a wide array of questions. Building on a <a href=\"https://www.deepmind.com/publications/fine-tuning-language-models-to-find-agreement-among-humans-with-diverse-preferences\">2022 DeepMind paper on LLMs for collective deliberation</a>, we ask how LLM fine-tuning can be leveraged for:<br>&nbsp;</p><ol><li>Finding key disagreement within groups,</li><li>Surfacing mutually beneficial possibilities and policies between deliberating parties,</li><li>Approaching common understanding,</li><li>Identifying confusion and miscommunications between perspectives.</li></ol><p>&nbsp;</p><p>The detail and interactivity our extended LLM-based interface provides can help policymakers uncover misunderstandings, key debates, and areas of common ground in complex public discourse. Use cases range from union decision-making, to determining the needs of recipients in refugee camps, to a collaboration with Metaculus on LLM-assisted deliberations on AI predictions.</p><p>&nbsp;</p><p><strong>Note on our use of LLMs</strong></p><p>We recognize that there are risks from misunderstanding or over-relying on the capabilities of current-generation LLMs, which are not yet reliable or intelligible enough for some applications. But we also believe that if used with a clear understanding of their limitations, language models may be a valuable tool for analyzing and simulating discourse at scale. Our intention is to explore this possibility first through small, bounded experiments&nbsp;\u2013 which will shed light on the capabilities and the limits of these models, and may reveal the ways in which over-reliance could be problematic.</p><p><br>&nbsp;</p>", "user": {"username": "cmck"}}, {"_id": "nLrEdXMw8DJEkyM7k", "title": "Patrick Gruban on Effective Altruism Germany and Nonprofit Boards in EA", "postedAt": "2023-05-05T17:23:55.185Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=XLGCzN7YNRc\"><div><iframe src=\"https://www.youtube.com/embed/XLGCzN7YNRc\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>&nbsp;</p><p>Here\u2019s a discussion between myself and <a href=\"https://www.linkedin.com/in/gruban/\">Patrick Gruban</a>.</p><p>Patrick Gruban is the co-director of <a href=\"https://www.effektiveraltruismus.de/\">Effective Altruism Germany</a> and the co-founder and managing director of <a href=\"https://rosygreenwool.com/\">Rosy Green Wool</a>. He is a serial entrepreneur with over 25 years of experience in different fields, including software development, visual arts, banking, and textiles.</p><p>This discussion covers some information on Effective Altruism Germany, but focuses on Patrick\u2019s takes on effective altruism management and how our boards should function. I think Patrick has an interesting perspective, having done management in business for a bunch of years prior to direct effective altruism work.</p><p>Effective Altruism Germany also has an interesting voting mechanism for leaders, which might be a decent prototype for other EA projects. We discuss the tradeoffs for this.</p><p>This is a lengthy episode. I think it got more concrete as it went on. I suggest browsing through my questions in the transcript to see what parts might be most interesting to you.</p><p>Many thanks to Patrick for joining!<br>&nbsp;</p><h2>Transcript</h2><p><i>Note: This transcript is fairly rough</i></p><p><strong>Ozzie:</strong> Hi. I'm here with Patrick Gruban, the co-Director of<a href=\"https://www.effektiveraltruismus.de/\"> Effective Altruism Germany.</a> Today we're gonna be talking about a bunch of things including effective altruism, German, how boards of effective altruism should work, and maybe some next steps forward for hopefully some different parts of the EA movement. So thank you for joining. Can you tell us a little bit about your background and a little bit about Effective Altruism Germany?&nbsp;</p><h3><strong>Experience and Leadership in Effective Altruism Organizations</strong></h3><p><strong>Patrick Gruban:</strong> Thanks for having me, Ozzie. I started at Effective Altruism Germany in November. And before that I've been an entrepreneur for the last 20, more than 25 years actually. I started my first company right out of school in 1996, which was a software development company together with three partners.</p><p>We got VC funding and grew the company to more than a hundred people. And ultimately it didn't work out after the dot-com crisis. After that, I started another company after doing some years of more consulting and change management and also leadership consulting.</p><p>The next company was also in software development but more focused on the finance sector and fintech, and this company then ultimately got, included in a bank. And I worked there for some years. And the last one that I'm still owning is in textiles, which is actually making yarn out of wool for higher animal welfare standards.</p><p>I've been doing that for the last 10 years and am currently looking to reduce that or sell the company if possible.</p><p><strong>Ozzie:</strong> I like that you have quite a bit of experience in a few different industries, as you mentioned. Recently there's been some discussions about the relative junior-ness of a whole lot of the EA scene. I'm curious about you coming into it: does it seem like a bunch of the key EA organizations or these young people who don't really know what they're doing? Does it feel like they're startup-type people who have some potential but still don't know what they're doing? Or, how do you find things by looking at them from your experience?</p><p><strong>Patrick Gruban:</strong> To begin with, in 2015 when I first learned about EA, I read up on it and started attending local group meetings in Munich and Germany. At first, I didn't think I had much to offer given the age gap and the fact that I never went to university. Most people were highly knowledgeable in specific fields, and I felt that I wouldn't be able to contribute much beyond donating and taking guidance on donations.</p><p>However, as I interacted more with people in the community, I was struck by how open and friendly they were. Despite our differences in age and expertise, we were able to communicate on an equal footing, and I found this to be a pleasant experience. As I delved deeper into EA topics by reading and listening to podcasts, as well as participating in a local discussion group, I began to see the potential for direct work.</p><p>My second significant milestone occurred in 2021 when I attended EA Global in London. There, I had the opportunity to interact with individuals who led organizations and gained valuable insights. Prior to this, my understanding of EA was limited to individuals who had written thoughtfully on various topics. However, speaking to people who actually run organizations revealed to me the gap between doing research and managing operational staff and leadership in terms of experience and knowledge.</p><p>In the startup world, for example, there are many inexperienced people, but those with less experience tend to drop out, leaving those who have more experience and skill. In contrast, I observed that in EA organizations, the founders' specific field knowledge didn't necessarily translate into good leadership, organization building, and management skills.</p><p>This realization motivated me to pursue direct work, either by joining an existing organization or creating my own. I gained more experience by consulting for various organizations and applying for different positions. I believe that there is significant potential in this area, and many individuals are motivated to do what is right. However, time constraints and lack of experience can hinder progress. Therefore, a more balanced team with varying levels of expertise, including collaboration between those with more and less experience, would likely yield better results.</p><h3><strong>\u200b\u200bIntegrating managerial talent in EA: Opportunities and Challenges</strong></h3><p><strong>Ozzie:</strong> I've been in the EA scene for a while, and I've also been in the startup scene, but less so than you have. I think there are many startups that are full of very young people in the beginning and are very good at a few very specific things. Then, some of them mature successfully, and others totally flame out. I imagine that some of the strong people you're talking about are those who have been able to grow into leading a mature organization as opposed to the ones that flame out. In my impression, EAs are optimized for understanding some aspects of research. It's just very hard. It filters out a whole lot of people to get the people who EAs trust as leaders. But after filtering out all those things, it's difficult to get people who are interested and capable of managing large organizations. As you said, I think anyone would say that it would be nice to bring in more senior talent right now. Of course, this brings up the dilemma that many startups have when trying to become more mature. It's common to experience poor outcomes when hiring senior talent, particularly if you hire individuals from a managerial or MBA background as they may push initiatives in directions that are counterproductive. Thus, it's crucial to navigate this situation carefully and consider multiple factors before making a hiring decision.</p><p>One question is just where EA stands right now. If we were to be rated on a scale of A to F, would it be a B- or what type of startup would be similar? The second question is how we can best understand how to safely integrate managerial experience without everything going off the rails and losing what EA is good at.&nbsp;</p><p><strong>Patrick Gruban:</strong> I think that last one is a really important question. I think in terms of rating, that's something that\u2019s super hard, just in terms of nonprofits. So I think ultimately the big advantage the for-profit sector always has boils down to, do I sell what I want to sell, or don't I sell that to find customers?</p><p>The decision between this is a good organization and this is not a good organization becomes transparent pretty quickly. In the non-profit world, it's much harder because you have some goals. They might lead to more research questions, people leave the organization. It's unclear if it's actually they're leaving because of bad management or because they're not the best suited and so on. So I think the rating question is really hard.</p><p>This also raises the question of how to identify effective leaders who can handle uncertainty. It's essential to consider how to integrate more individuals into this process. In my experience, when speaking with more entrepreneurial individuals who have some experience, they tend to be more interested in launching a for-profit startup than working in the non-profit sector. Therefore, it's necessary to consider how to attract and retain these individuals in the non-profit world as well.</p><p>I think there are several aspects to consider. One is the basic incentives. For the startup, if it really gets big, you might get a lot of profits out that even if you're thinking of donating all of them, that basically gives you more decision-making power and grant-making power than before.</p><p>The downside is that the startup doesn't exist anymore. In the non-profit world, it's basically it goes well versus it doesn't exist anymore. So the upside is much lower. And also you have more people that might judge you in a for-profit world. You might be much more independent in terms of doing your own thing and you have fewer downside risks to think about and less judgment from the outside.</p><p>So I think there are many points why more entrepreneurial people might choose for-profit which makes it much harder to get more people. And then as I said, integrating people that have been in the front-for-profit world might be harder because of different sets of values.</p><p>People oftentimes get successful by, for example, being not completely honest about their success. There's a lot of fake it until you make it approach out there, which can lead to people just bolstering up their image and seeming more successful than they perhaps are.</p><p>Getting into EA and trying to be more truth-seeking and open to people who may be much younger but have a better sense of certain subject areas may be harder. So, I think there are a few things that might affect people. However, I believe there's also a model that we haven't explored too much, which is similar to where I'm coming from.</p><p>Basically, having somebody who has gone through the for-profit phases and is now at the point where they're saying, \u201cOkay, now I really want to concentrate on impact. I want to make sure that the next things I'm doing are maximizing the good with the resources I'm investing in, and for that, I'm also really open to learning new things and interacting with others.\u201d</p><p>And for me, for example, having a co-director who is almost two decades younger than me and comes from a different background was incredibly beneficial. Working with her on strategy during the initial two months and ensuring that we were aligned allowed us to view the world similarly while integrating our unique experiences. So I think that might be one model to do that. Having more intensive collaborations between people with different backgrounds and workrooms.</p><p><strong>Ozzie: </strong>Regarding the types of senior individuals we could consider bringing on board, we've discussed entrepreneurs and CEOs, both of which have their pros and cons. While some successful entrepreneurs make excellent CEOs, many come with questionable epistemic values and an inflated sense of self-importance. They often attribute their success solely to their abilities rather than acknowledging the role of luck. Moreover, they may hold idiosyncratic personal beliefs that contradict the principles of effective altruism. It can be challenging to work with this group, although there are exceptions like Ben West, who joined <a href=\"https://www.centreforeffectivealtruism.org/\">CEA</a> after building one successful business and appears to be more humble and reasonable than many of his peers.</p><p>On the other hand, MBAs, consultants, and CEOs are typically more practical than visionary. We haven't engaged with this group as much, but they may have valuable skills and perspectives to contribute. However, some EAs are suspicious of big consulting firms like <a href=\"https://www.mckinsey.com/\">McKinsey</a> and<a href=\"https://www.bain.com/\"> Bain</a>, which are known for telling clients what they want to hear and doing mediocre work. Nevertheless, organizations like the <a href=\"https://www.gatesfoundation.org/\">Bill and Melinda Gates Foundation</a> rely heavily on these contractors.</p><p>So, the question arises: How do we select the right individuals to work with, and should we rely more on entrepreneurs or MBAs, and consultants? It may be possible to filter out individuals with big egos, but it's unclear how to do so effectively. As for MBAs and consultants, we should explore their potential contributions while remaining cautious of their shortcomings. Ultimately, it may be best to strike a balance between these groups and avoid relying too heavily on any one type of individual.</p><p><strong>Patrick Gruban:</strong> Yeah, I think that several aspects have come in. One is definitely personal fit or personal preferences and so on. One organization that I'm really excited about is<a href=\"https://www.cfactual.com/\"> cFactual</a> which Jonah Glade started last year. He's also on our board and comes from<a href=\"https://www.bcg.com/\"> BCG</a> as a consultant starting the EA consultant network and is now doing strategy consulting of cFactual specifically for EA organizations.</p><p>It is crucial to have individuals with extensive experience in various EA organizations who are highly respected and possess a perspective from strategy consultants in prominent firms. Individuals like this can help bridge the gap between both perspectives, as they can connect with EA's values and appreciate its mission while bringing in outside knowledge to avoid reinventing the wheel. However, finding such individuals remains challenging, and it seems that success has mostly come down to luck.</p><p>As you mentioned, Ben West is an excellent example of someone who successfully made the transition. While there are a few other individuals who have done the same, it remains unclear how to replicate their success. Therefore, I do not have a definitive answer. However, I believe that continuing outreach and community-building efforts at universities, where individuals are early in their careers and can still shape their paths, is essential.</p><p>But I think supplementing that with additional specialized outreach in areas where we see it would be great to get more experienced people that are willing to learn and integrate into the EA values I think would also be valuable. But I haven't found a solution for that yet.</p><p><strong>Ozzie:</strong> Do you have any initial thoughts on the Bill and Melinda Gates Foundation's approach of hiring a large number of consultants who don't necessarily share their specific way of thinking, yet have been well-compensated and seemingly effective? This prompts the question of whether we should consider hiring specialist consultants who share our values for certain roles, particularly for ensuring that our boards are effective and other organizational matters are handled well. It is possible that either we are making a mistake by not engaging large consultancies, or the Bill and Melinda Gates Foundation is making a mistake by doing so.</p><p><strong>Patrick Gruban: </strong>In my opinion, there is one aspect of EA organizations that can seem a bit peculiar, which is the attempt to hire individuals who are specifically aligned with EA values. While this approach can be highly beneficial in the early stages of an organization, where a small team must be in agreement regarding the direction of research and practical work, it can become challenging to maintain such strict criteria as the organization grows and becomes more complex.</p><p>But if that continues with an organization that has 10, 20, 50, or more employees, I think that's what's strange. The bigger the organizations get the further the role is away from strategic decisions, and the less it's important to have EA knowledge versus really having really good subject matter expertise and experience.</p><p>It can be advantageous for small EA organizations or teams to prioritize hiring individuals who are aligned with their values to ensure everyone is working towards the same goals. However, for specialized roles such as a tax consultant or lawyer, it's more important to prioritize expertise rather than EA alignment. If EA organizations only focus on hiring individuals who are value-aligned, the talent pool may be too limited. For larger organizations like the Bill &amp; Melinda Gates Foundation, they can afford to hire specialists in specific fields while also ensuring that these hires align with the organization's overall mission and culture.</p><h3><strong>Effective Altruism in Germany: Growth, Challenges, and Impact Estimations</strong></h3><p>&nbsp;</p><p><strong>Ozzie:</strong> Thank you. Regarding Effective Altruism Germany, I understand that it may be unfamiliar to many of our listeners, as well as myself since I have not visited the country. However, I am aware of some general information about Germany from various news sources. I would be interested to hear your perspective on the current developments within Effective Altruism Germany. Can you provide a brief summary of their activities and what makes them noteworthy?</p><p><strong>Patrick Gruban:</strong> According to the latest EA survey, Germany ranked as the third largest EA community after the US and the UK. This can be partly attributed to the founder effect, as the EA foundation was initially established in Switzerland and then moved to Germany, creating a strong local infrastructure and fostering the growth of local groups. Currently, there are 25 local EA groups spread throughout Germany, which reflects the decentralized nature of the country. While the hub is based in Berlin, many members are located in other cities, including myself in Munich, my co-director in Hamburg, and our team members in various other cities.</p><p>That basically mirrors how Germany is set up. Our universities are less focused on the top. So I think we have more universities that are less different. So also university groups are dispersed around the country. In terms of active people in the community, my current estimate is that we probably have between 200 and 400 people.</p><p>We have 450 people who took the<a href=\"https://www.givingwhatwecan.org/\"> Giving What We Can</a> Pledge and in the different local groups, we have probably 200\u2013300 people attending regularly. We don't have many organizations here. We have Effektiv Spenden, which is the German donation platform that just also announced that they got<a href=\"https://www.openphilanthropy.org/\"> Open Phil</a> funding for the next few years.</p><p>One organization that has been notably successful in Germany is the EA Foundation, which raised 18 million euros two years ago and has experienced significant growth. However, it is not common for new EA organizations to be started in Germany, and many people who become more involved in EA tend to move to the UK or the US or find remote jobs. Before the current leadership took over, there was a scattered landscape of EA-related activities in Germany, with only one full-time community builder and a membership organization run entirely by volunteers.</p><p>We also had<a href=\"https://effektiv-spenden.org/\"> Effektiv Spenden</a> in running the German-speaking website. We had the old foundation running the Facebook page. So there were a lot of different things going on, both involving different people. Although we have the third-largest EA population, we still have less per capita EA than in smaller countries, like Israel, for example, or Sweden.</p><p>It appears as a favorable chance to consolidate the organization, unify previously uncoordinated aspects, and establish a structure that provides the best guidance for individuals interested in EA careers. We aim to deliver professional services, organize yearly events such as EAGx in Berlin, provide support for local groups, and ensure excellent German communication and website. In the next phase, we intend to experiment with various methods that leverage Germany's decentralized state and allow us to explore unique opportunities.</p><p><strong>Ozzie:</strong> Do you have any favorite metrics that you try to grow over time or estimates of how valuable, and different, the community is?</p><p><strong>Patrick Gruban:</strong> That's a really tough one. We just published a forum post about impact estimations in community building, and that's something I'm personally grappling a lot with how do we actually figure out if what we are doing is net positive and how can we compare it to other kinds of potential investments for funders?</p><p>Our primary goal is to encourage individuals to take significant actions, such as making a career change or taking a pledge. It is especially important to consider the potential value of having more people working in long-termist areas, even though it can be difficult to determine the impact of one person's contributions. Moreover, the actions of one individual, such as founding a new organization, can have an outsized impact. Thus, it is challenging to evaluate the effectiveness of initiatives like retreats, events, or introductory programs in terms of the time invested and their impact on individuals or groups.</p><p>Although we strive to provide clarity on our estimations, we recognize the need to improve the rigor in evaluating community-building initiatives, especially in comparison to the evaluation standards applied to charities. It's possible that funders may have better models to assess community-building efforts, but this is an area where collaboration between different EA groups is crucial to share best practices and insights.</p><h3><strong>Long-term Community Building and Health in EA Germany</strong></h3><p><strong>Ozzie:</strong> Balancing the speed of recruiting new members to important organizations and short-term financial gains against ensuring long-term satisfaction is a challenging aspect of community health. This trade-off is particularly challenging for your organization, given the likelihood that many of the individuals you invest in will leave Germany after a few years. How are you approaching this issue?</p><p><strong>Patrick Gruban:</strong> When I joined EA Germany, I wanted to focus on longer-term community health and growth, which is different from how many people approach community building as a temporary volunteer activity. I typically set up my organizations for five or 10 years, which changes how I think about working with people. For example, we want to encourage more people to start organizations in Germany and create hubs and co-working spaces for remote workers to interact with others. We also aim to build a pipeline of people interested in community building and career guidance, with specialized skills and networks.</p><p>One of the challenges we face is balancing the trade-offs between getting new recruits into important organizations quickly and caring about short-term fundraising versus ensuring that people are satisfied in the long term. This is particularly challenging because many people we invest in may leave Germany after a few years. To address this, we need to think carefully about how to grow and support the community while taking into account individual needs and goals.</p><p>We also recognize the importance of making our underlying estimations and evaluation methods clear to the community. We want to bring rigor to community building, as we have seen in evaluating charities. This is not something we can do alone as a national group, and we believe that collaboration among EA groups is necessary to achieve this goal. By working together, we can create a healthier and more effective community in the long term.</p><p>That's something that's not often seen. One part of that is also looking at community health, as, in addition to the <a href=\"https://www.centreforeffectivealtruism.org/\">CEA</a> community health team, having somebody as a contact person here in Germany that knows the local contacts a little bit better and speaks the language.</p><p>On one hand, it's important to have resources available for people who may feel unwell or need someone to talk to. However, it's also crucial to provide guidance to local groups in advance to ensure their events are welcoming and inclusive, thus attracting a more diverse range of people to the EA community. This effort aims to prevent the community from becoming too homogenous and one-dimensional, which could cause it to lose resilience and fail to attract new members from different backgrounds and cause areas.</p><p>To achieve this, EA Germany is working to integrate people from different backgrounds and cause areas and ensure they feel welcome and included. This includes making events more accessible and ensuring they cater to a diverse range of people, rather than focusing on one particular group. By doing so, the community can remain strong and resilient even if one cause area loses its appeal or becomes less active.</p><p>Furthermore, having diverse funding sources can also make the community more resilient. This would prevent the community from being dependent on one particular source of funding, making it more sustainable and able to adapt to changing circumstances. Ultimately, the aim is to make the community more resilient and adaptable in the long term, ensuring it continues to grow and thrive over the coming years.</p><p><strong>Ozzie:</strong> The community health team at CEA has been involved in some controversy lately, particularly around issues of sexual harassment and abuse, and creating safe and welcoming environments for women. Despite this, I personally support the team and believe that the controversy is simply a result of the team's position rather than any issues with the team members themselves. I am interested to hear your thoughts on how you might approach similar issues differently and whether you have any strategies or procedures in place to ensure the comfort and safety of women and other marginalized individuals in your community.</p><p><strong>Patrick Gruban:</strong> I'm also a big fan of the CEA community health team. I think they're really thinking very carefully about things. At the same time, I think it's also good to have different people thinking about that. So I think just thinking that we can just outsource everything to one team that is in relationship to the discussions and the people and so on, relatively small, I think can also put a lot of pressure on them.</p><p>It seems like a good idea to have someone who speaks the language and knows the local culture when working in a different context. This person can provide additional details and help bridge the gap between cultures. We are currently working with the CEA community health team to upskill this person and ensure that we have the right resources.</p><p>In Germany, we don't have as much of a group house culture where people tend to have closer relationships with each other. Therefore, we don't have as many power dynamic issues as other countries might. However, there are still areas where we can improve. One area is the welcoming culture. We often see local group meetings that are predominantly or exclusively men, which can be uncomfortable for women or non-binary people joining the group. Therefore, it's important to have people on the ground who can ensure that the group feels welcoming and inclusive. We should also reach out to individuals to make sure that they don't get the wrong impression of the community.</p><p>For example, our Munich group has a lot of people who are deeply engaged in technical AI safety discussions. If someone randomly joins an event and ends up in a group of these people, they might think that they need to be very knowledgeable about the topic to engage in the conversation. It's important to make it clear that this is just one aspect of the community, and that we have a lot of people interested in other areas and with different backgrounds. This can go a long way in creating a more welcoming and diverse community.</p><p>Overall, community health should not just focus on addressing issues that arise, but also on preemptively creating a broader and more representative community. This can help ensure that EA is accurately represented on an international level.</p><h3><strong>Challenges and Resilience in EA Germany</strong></h3><p><strong>Ozzie:</strong> Are there any unique challenges to Germany? You mentioned some things, some ways that Germany has it easy. But what about ways that it has hard issues that you have to deal with?</p><p><strong>Patrick Gruban:</strong> I believe that community health is just one piece of the puzzle. We also face challenges where individuals who are passionate about Effective Altruism tend to leave the country at a faster rate, making it difficult to find good mentorship for those who understand the nuances of EA. This can result in a younger demographic and less in-depth discussions, which might make people feel like EA is only about maximizing a mindset, which can be detrimental to mental health. These factors pose unique challenges for us.</p><p>Moreover, the perception that one must leave the country to pursue career changes can cause anxiety and make it seem harder for individuals to make a difference. It is crucial to address these challenges to create a more comprehensive and sustainable EA community. Finding ways to provide mentorship and guidance for those who want to pursue EA-related careers within the country can help alleviate some of these challenges. Additionally, creating an environment that fosters healthy discussions and provides resources for mental health can help ensure that individuals do not feel like they must sacrifice their well-being to pursue EA.</p><p><strong>Ozzie:</strong> Given that people are more distributed throughout Germany, have you had to figure out any interesting ways of remote working or conducting remote meetings between individuals?</p><p><strong>Patrick Gruban:</strong> I think I in general, people are very much used to working remotely. And also usually you can find a spot in Germany where most people can get there, like in three to four hours. So just having a retreat and doing some co-working at some point is not such a problem.</p><p>We have excellent co-working facilities in Berlin, which is easily accessible within Germany. So, I don't see that as a major issue. However, the real challenge is the lack of social interaction that sometimes leads to missed opportunities such as randomly meeting people and discussing new ideas, finding potential co-founders, or discovering new research questions. These missed opportunities may cause some individuals to seek remote work with people outside of Germany, which may ultimately lead to them leaving the country. While it's generally good for people to pursue their goals, it presents an additional challenge to building a sustainable community of professionals in Germany in the long term.</p><p><strong>Ozzie:</strong> Thanks. You mentioned before that you want to talk about the resilience of the community maybe that the community doesn't get too focused on any one topic.</p><p><strong>Patrick Gruban:</strong> I mentioned earlier that having people working on different cause areas can be beneficial for our community. Even if we currently need more people to focus on technical AI alignment, it can be challenging to find individuals who are qualified to work in this area. Therefore, having other options for these people to contribute to the community is important. Additionally, it's crucial to ensure that we are also working on different areas, even if technical AI alignment is a top priority. This will prevent other fields from being neglected, and it will demonstrate that the community has a diversity of priorities.</p><p>Having a diversity of priorities can make the community more resilient. It's not just about focusing on a few areas but rather having a range of priorities that address different issues, such as CS health. To achieve this, we need to figure out how to attract more funding sources to support our movement-building work beyond Open Philanthropy, which is currently the main source of funding through organizations like CEA. In addition, having a long-term perspective is critical. Even if some members are not yet ready to make a career change or contribute in a major way, having them stay involved and volunteer to support the community can help build resilience and ensure that people are around for the long term.</p><p>For example, our membership organization has been run by volunteers for several years, and having volunteers who care about the community is extremely helpful. This approach also makes it easier for people to remain connected and wait for opportunities to arise that may be a better fit for them. By fostering a sense of community and encouraging volunteerism, we can keep people engaged and invested in the community, even if they are not yet in a position to make significant contributions.</p><h3><strong>Exploring Funding Sources for Effective Altruism Community Building</strong></h3><p>&nbsp;</p><p><strong>Ozzie:</strong> You mentioned the issue of funding, and I agree that there's a heavy reliance on Open Phil for community funding. While I appreciate Open Phil's support, I believe it's not necessarily aligned with the median interests of the EA community. Relying too heavily on a single funder could be unhealthy, as you mentioned. Do you have any ideas about alternative sources of funding, and who would be willing to contribute? Would it be individual members of the community? Additionally, how can the EA movement as a whole, or at least community efforts beyond Effective Altruism Germany, address this issue and strive for change?</p><p><strong>Patrick Gruban:</strong> That's, it's something I've been thinking about a lot, but so far I don't really have a good answer to it. My personal take is I think we should be in terms of movement building community building, we should be as attractive for funders as other more direct work. For example, making sure that we have a good model where we actually see the impact of our work and making that more explicit.</p><p>I believe that Open Phil probably has good models for funding, but I think it would be beneficial for them to be more transparent about their decision-making process. This could involve taking a more direct approach, such as hosting a conference where contacts can be made and tracked to see the outcomes. This may be easier for funders to consider than funding local groups or university groups, which may take longer to produce significant contributions through people's careers.</p><p>To attract more funders, we could split up different movement-building activities and develop better models to quantify their impact. Many people who earn to give are willing to donate a significant portion of their earnings, and if we can demonstrate that our organization in Germany has a good track record and is reliable, along with providing impact models, we may be able to attract more donors. However, this will take time, as it requires building trust and accountability.</p><p>It is not necessary to achieve this in the first six months of our work. Instead, it requires a long-term commitment to transparency and accountability.</p><p><strong>Ozzie:</strong> I have been considering an interesting aspect of donation within the community. Long-term community members seem to be the most suitable donors, given their connection and involvement with the community. Several community organizations rely on members' dues or events that are not subsidized. EAGs could potentially generate profit, which could then be directed toward the community. However, in the medium term, there is a question of how much we should prioritize community members' contributions over those of the top 2% earners within the community or even the top 2% earners in the country who may not be part of the community but see it as a worthwhile cause to donate to.</p><p><strong>Patrick Gruban:</strong> I think the example of EA Global is good, to talk about because there, I think oftentimes people think, okay, this is something that CEA is doing for the community. It's something that actually is nice to have. I think as far as I understand their model, it's much more about we're setting up the conference for people who are at the point where they're actually thinking about a bigger decision, career change, where they can join, and then we add experienced member of the community. That might be really helpful. So basically, either you're at the point where you're doing this career transition at that point where it's more of a marketer approach. We actually, would like to have more of these people because we can see that they have a big potential. So should they pay, I don't know, perhaps it's actually we should pay to get them.</p><p>Another perspective to consider is that of an impact-focused individual who recognizes the potential value of each interaction with high-potential persons. On the other hand, experienced members of the community are the ones that should be present at the conference to draw them in. Asking experienced members to pay and reducing the number of attendees may not be optimal. For these types of conferences, it may be more effective to communicate the value of connections and estimate the expected impact of career changes resulting from the conference. By doing this, we could develop a model that could show donors a more direct link between their investment in the conference and counterfactual impacts, which may attract more donors.</p><p>Ultimately, this approach would take on a return on investment style, similar to donating to malaria prevention. The difference is that donating to a conference has a less clear impact. This underscores the need for better communication and estimations around the impact of such conferences.</p><p><strong>Ozzie:</strong> CEA seems to be referring to their community as a recruiting network, which is different from the traditional understanding of a community as a group of people with long-term connections. For instance, if McKinsey were to organize recruiting sessions, it would be perceived differently from a community-focused event. When attending EAG, the primary goal isn't to sell or assist people on the brink of making career decisions. Rather, it's about developing long-term connections with senior individuals to make lasting changes. There seem to be diverse needs that people hope to address, which makes it challenging to fit them all into one \"community\" package. Therefore, figuring out a way to accommodate different needs is crucial.</p><p><strong>Patrick Gruban:</strong> I think that there's one thing that is really changing at the moment. The question of do we actually want to have a community or fund a community building or on, so if you go to the CEA website, it used to say talk about the Effective Altruism community. Now they're talking about the professional network of Effective Altruism.</p><p>It seems that there is currently a lot of uncertainty and differing opinions around what is meant by \"community building.\" Some have suggested that efforts to build community should be downgraded, while others see value in building social networks and connections among effective altruists (EAs).</p><p>One potential approach to acquiring and upskilling individuals who can then make a career change involves a mix of marketing and education, which does not necessarily rely on a community. For example, this could involve reaching out to individuals through a website and providing guidance on joining impactful organizations. Alternatively, professional conferences and events can provide opportunities for individuals to make new connections and continue working together, but these do not necessarily constitute a community.</p><p>However, for those on a longer-term track to a career change, having a social network can be valuable. This could be privately funded, where individuals pay for the additional motivation and support. It's important to recognize that community building has many different elements, and it may be helpful to separate them out and focus on the parts that are most exciting to different funders.</p><p>Moreover, the concept of the EA community is evolving and may look quite different in the future, especially as private and professional connections become more intertwined. Governance will likely play an important role in navigating these changes and ensuring that they are beneficial for all involved.</p><p><strong>Ozzie:</strong> I definitely get the sense that a lot of that tension on the EA Forum and in EA friends I know is around the misunderstandings and what people think of as different promises around this topic. I think a lot of junior community members would treat this as \u201cOh, once you become an EA you'll get these friends and dating options for life, and the whole EA bureaucracy is doing an amazing job and everything is gonna be super nice.\u201d And then some EAs seem to assume that what the EA community is supposed to be doing is making sure that they specifically will be taken care of if things go bad for them specifically. Also, I know that a lot of, from what I hear, different community members seem to assume that Open Phil should be funding them to have hangouts with each other to be better friends, cause that'll be useful. But this expectation may not align with Open Phil's goal of being ruthlessly efficient. Some of what that may mean is stuff like, \u201cOh, I just want top talent for AI safety organizations or something, and I really don't care about this other stuff.\u201d</p><p>It is also strange to fund parties for a specific group of people, and it is unclear what the funders are trying to achieve and what their values and commitments are. The lack of candid discussion on these topics adds to the challenge of addressing these misunderstandings and assumptions. Therefore, it is important to have an open and honest conversation about these issues within the EA community.</p><p><strong>Patrick Gruban:</strong> In my experience with CEA in Germany, we've been funded by community-building grants, and CEA has been hands-off in terms of our strategy, which has allowed us to set up the organization in a way that benefits people in Germany. While I wouldn't want a strong model approach, this also means that different groups in different national and city groups, university groups, and so on act in different ways.</p><p>When I first joined the mini-group, it was more of a professional group with more business-oriented people who donated and upskilled. Later on, I discovered that other cities had more of a social aspect, which we didn't have as much in Munich. Personally, I got into the social part of EA much later than the object-level part, but I think both are important. Losing the part of the community where people feel connected would be a shame, especially for those who face rejection or a high likelihood of failure in their work.</p><p>Having a community of people who understand what you're doing is essential for those who are trying to make an impact in their work and face difficulties in explaining their choices to outsiders. However, the social aspect of the community can also create power dynamics and make it challenging for outsiders to come in who don't want to be part of a social sphere. As funding projects become more professional, we may see less funding for community activities that include the social side, but it would be a shame to lose the sense of community and connection that the social aspect provides.</p><p><strong>Ozzie:</strong> Let's pause here. I'll just do it, the mark we've been talking about for about an hour, so it's just a short break. So maybe take a three-minute break and then discuss how it's going quickly and then get into, there's probably a good time to transition into the discussion in boards.</p><h3><strong>Governance and Membership at EA Germany</strong></h3><p><strong>Ozzie: </strong>We should move on from the previous topic and start discussing boards, a topic that both of us are interested in. Regarding Effective Altruism Germany, you mentioned earlier that it is a membership association with a board of members. Can you confirm this and provide more information about it?</p><p><strong>Patrick Gruban:</strong> Currently, we have 85 members in our organization, and once a year, we hold a general assembly where the board is elected. Board members can be reelected or new candidates can apply. In case of dissatisfaction or a desire for change, a transition can occur. Last year, there was a significant transition, and the new board members were more active, hiring my co-director and me, and devising more ambitious plans. As a board, they set the organization's direction and initiated the search for co-directors, which is their essential responsibility. However, they are also accountable to the members and hold offsite meetings. At EA Germany, membership is not open to everyone, and candidates must apply and undergo an interview with the board before approval. The statutes require applicants to have prior knowledge and engagement in EA topics, work, or local communities. Consequently, our member base is excellent, and the decision to establish the board is valuable.&nbsp;</p><p>So I think that's something where we are a little bit unique I think within the EA organizations.</p><p><strong>Ozzie:</strong> Do you feel like the members have any different priorities than the funders, or is there any interesting tension over there?</p><p><strong>Patrick Gruban:</strong> I haven't given much thought to that. Funders, particularly CEA, have a specific set of criteria for community builders, which is mostly based on individuals who are the sole community builders in one or two locations. They are mainly concerned with the individual's performance rather than the long-term sustainability of the German EA community, which is the focus of the organization's members. Committee members have a longer-term vision compared to CEA's shorter-term vision, and they prioritize the organization's infrastructure, while CEA focuses more on the individual.</p><p><strong>Ozzie:</strong> Just a little bit about these members. Were they instated, did you say,&nbsp; before you joined?</p><p><strong>Patrick Gruban:</strong> The organization was started by volunteers by the first members. They set up this kind of organization where they would interview people and only at people after their interviews. I've been part of the organization for three years now. I started out doing financial audits and also as a volunteer.</p><p>So I was a little bit active, but not super active in the organization. And then when the job posting came up, I applied like everybody else and went through the normal application process, which was a process from the board members and from CEA because CEA is the funder. So they also wanted to have a say in selecting people and their focus, I think was mostly around deeper EA knowledge, getting people in there.</p><p><strong>Ozzie:</strong> Wait, so when was the initial organization formed? You said you started three years ago, right?&nbsp;</p><p><strong>Patrick Gruban:</strong> That was in I think 2019 the organization was started. And in, in 2020 I joined. Yeah, in the beginning, it was basically the organization that did some infrastructure services. For example, had some online tools that, that could be used by members of the community.</p><p>They also set up regular calls for community builders. They had meetings, so it was more like sharing experiences between people but on a volunteer basis. And then last, or one and a half years ago, started an employer of record service, which basically enables people who get a grant in Germany to get employed by the organization.</p><p>So instead of taking care of taxes, and getting a lump sum payment at one point as a grantee, you can get employment and normal social security services. That was one of the initial services the organization did, which was pretty popular. So we have several people applied through that at the moment.</p><p>Gradually, they became more ambitious and decided to establish the organization, which involved hiring us as co-directors, a project manager, and an operations associate. Currently, we are in the process of hiring a communication person and a team leader for the EAGx in Berlin in September. This transition marked a significant change from our previous approach of operating as a volunteer-run organization.</p><p><strong>Ozzie:</strong> I think there's a broader conversation now of should more key EA organizations have semi-democratic structures, specifically the community-building ones. I think things like CEA would be the obvious candidate. So having experience with related organizations, and trying out these types of proposals seems like a good thing, but also getting clarity on them is useful. I think you're probably in a bit of a tricky position that because you were hired by these people you may not be the best person to be candid about them, but you do know a lot about it. Are there any tensions or controversies within the group, and have there been any decisions made that wouldn't have been made by a reasonable board? If the decisions were the same, it's unclear where the value lies so far.</p><p><strong>Patrick Gruban:</strong> I am not aware of any controversies regarding the selection process. Those who did not make it as members were provided with clear guidance on what they could do to improve their chances. We did not have a cutoff point where people were permanently barred from being members. It is easier for us to manage membership because we have a geographic boundary. We are the organization in Germany, and those in Germany can join us. We have a significant presence in the German EA community, with 85 members out of a potential 200-400 highly engaged members. However, it is much harder for other organizations like CEA that have an international presence. There have been discussions on the forum about membership organizations and who should be members, and whether democratic decisions should be made. It is difficult to determine who is in the EA community and should be allowed to make decisions. It would also be challenging to set up an international membership organization after the fact. The German EA community has been around for a few years, and membership was not incentivized in the beginning. Becoming a member just led to a good set of people in the community. I do not think this affects how the board works. Board members are motivated to do the right thing, and community members' opinions do not hold significant weight. They are volunteers, and recognition from members at the annual meeting may add to their motivation. Other non-profit boards are usually obscure, with names on the letterhead. The general assembly of members may provide additional motivation, but this is just speculation.</p><h3><strong>Challenges in implementing a democratic structure and dealing with power grabs in EA organizations</strong></h3><p><strong>Ozzie:</strong> Just thinking about it on my end it sounds the way that you're describing it, that there just haven't been many controversies or various spicy personalities, or like big power grabs within the German membership yet. Within EA generally, we've definitely had some. I could definitely imagine that if there were if there was a voting mechanism, maybe there would've been an FTX faction that would've tried to rise up and, own the majority. Or maybe we've had previous groups leverage research that tried trying to make some power grabs because they thought EA wasn't going in that direction. That was a good one. So they tried changing that and people had a lot of issues with that. Right now arguably the EA forum is decently democratic in the sense that at least more people get votes than get votes on key funding decisions, right?</p><p>This is a very small group of people, EA Forum, even though some people have more votes than other people lots of people can vote and still write angry comments if they want to. And I think that the things that do the best on the EA Forum aren't exactly the best things. Often we've seen that there are some very angry posts that get a lot of reactions sometimes just because people are afraid of them so have to react. Or sometimes some very short arguments would get a huge amount of upvotes even though they're not very humble. And that goes on both sides, like many sides of arguments I see doing that. So I think, there's also the fact that there's a lot of money now in EA, which means that there's a lot, with EA Prime, which means that there's a lot at stake.</p><p>And then of course a lot of people have very intense feelings about animals versus AI versus humans, or when it comes to AI, what should we do about AI? So trying to organize all of them into an environment is productive and doesn't lead to breakdowns in situations where we actually give them more authority seems like a tricky thing to do without it going horribly wrong.&nbsp;</p><p><strong>Patrick Gruban:</strong> Yeah, I think probably it's always easier generally to criticize than come up with suggestions or have a more democratic structure. That actually means that people would have to like, come up with a solution for a problem instead of just criticizing that might improve things, but it might just also lead to people engaging less.</p><p>So I think yeah the firm as you said an angry post, something criticizing somebody or an institution or a practice or someone is easier and is easier to than a specific proposal. That actually has to work in the real world with the set of people that we can actually attract.</p><p>I think that's also something oftentimes criticizing institutions than when I talk with people individually in these institutions, they oftentimes see issues are yeah, there are things that are known, but there are just too few people that are available that are not approval that you might solve these problems at the moment.</p><p>So I think once you really engage with what can be done, what can be changed, it's much harder than just to criticize that. I don't know if a democratic structure would solve these things, it would be great to have people that are really more engaged on in the sense of really taking ownership versus just criticizing stuff and having the feeling that they can actually, ha have more ownership. That it's not this kind of total approach of this is the big institution that actually makes all the decisions and we now have to suffer versus in real reality, it's all of the EA organizations are relatively small in terms of less than a hundred people or so and relatively new and young and just doing their best trying to figure stuff out.</p><p>And so making that clearer, making that people have more ownership would be great. But I'm not I'm really unsure how we can do that. So I, in terms of our organization, it's, I think it's easier because it's closer. People know each other. We know everybody probably or most of the members are engaged in the local group, so it's just much clearer to them and probably more obvious to see what's going on versus in an international community for people dispersed in different areas, it's probably much, much harder.</p><p>I don\u2019t know how that can apply. And also the other thing is that we just, we don't have the money to make grants, so there's less incentive to, for a power grant. So it's basically, yeah, you can have the power of what, but then you also have to do the work basically.</p><p><strong>Ozzie:</strong> Yeah, those stakes do help. So about boards in general so you said that you were you have experience with different boards at this point. Can you talk a little bit about that?</p><p><strong>Patrick Gruban:</strong> So for my first company when we got VC funding and then also set up the board obviously the VC was part of the board that was a bigger decision also who to take on as a VC and who also got to be a board member of that organization. And for us, one really important thing was to have somebody who's an advisor in a specific field.</p><p>So at that point, we were getting into expanding internationally. Just having somebody who actually set up an organization already that worked internationally and having this person as an advisor was really important. And I think, of course, you can always have additional advisors coming on, but I think having somebody on the board who's an advisor has also aligns the interests.</p><p>They are also focused on maintaining the organization's reputation. Therefore, they are more engaged in providing sound advice that considers long-term goals, unlike other advisors who might only focus on short-term goals. It is also useful to have a board member with knowledge of finance who can access confidential financial data that an external advisor cannot be trusted with or share with other board members. Thus, having an advisor on the board is crucial. The board members must consider the long-term future of the organization, identify potential risks such as changes in leadership, and plan for a talent pipeline to ensure the organization's continuity. Conducting pre-mortems on financial, legal, and reputational risks also helps the board to prepare for unexpected events. These holistic approaches and specialized advisory cases were significant in guiding decisions.</p><p><strong>Ozzie:</strong> How many board members did you have then?</p><p><strong>Patrick Gruban:</strong> At that point Company we had I think four or five. I'm not exactly sure anymore. It's been a long time since then.</p><p><strong>Ozzie:</strong> It is interesting that in certain industries, obtaining good advice is highly correlated with where the funding comes from. In some industries, it is possible to obtain funding from any venture capitalist and pay for advice. So, why is it crucial for a board member to have ties to a particular venture capitalist?</p><p><strong>Patrick Gruban:</strong> I think it's easy to find people who give advice as professional advisors professional consultants, or people that are subject matter moderate experts, getting somebody who has been a successful entrepreneur and has been through the experience that you're going through something much harder to get.</p><p>So that's typically something where you get a business angel or VC or so in because they know what their advice is worth and are able to get basically as the stake in the organization. So it's probably just the thing that it's harder to get these people.</p><p><strong>Ozzie:</strong> Like those people don't they're just not on the market in terms ofhour. You have to only get them as a board member.</p><p><strong>Patrick Gruban:</strong> Yeah. And yeah, with typical advisors they will have solutions that work from in a very general technical sense, but they don't, they can't tell you how to really implement them within an organization making sure that it works in the organizational culture. But that's something very specific on the VC side and the on the business side which probably doesn't translate so, so well to the non-profit world.</p><p>For me, coming into EA Germany the first thing that when, so I applied for the job without having much knowledge about the board. When they told me that they wanted to have me join, the first thing I did was to talk with all the different board members and ask them if we as co-directors would be able to execute our goals without micromanagement.</p><p>And luckily we really have a board that wanted that, that was very much on that. So basically having enhanced off approach having oversight over the core directors, but not doing micromanaging. And I think that's something that is super important because I think having a board that tries to manage at the same time as having somebody as a manager can be really harmful.</p><p>On the other side also, I think it can be harmful if the board is super hands-off and doesn't have any oversight at all. So I think the sweep spot is where the board knows what is going on. In a broad sense and can intervene at points where they can, where they see that that there is like a bigger issue that might lead to having to replace the management or yeah, that, that risks the organization in Yeah, financial, legal, rehabilitation sense and making sure that this is, comes up early enough and is handled at that point.</p><p><strong>Ozzie:</strong> Do you have any intuition on how many hours a month good board members should spend?</p><p><strong>Patrick Gruban:</strong> I think most of the time, not so many.</p><p>Like if everything works well, then it's basically a call every other month or so with the board making sure that everything is working fine and then having a yearly review of the management and financials and so on which is a couple of hours.</p><p>If things don't work well, then it's more crisis mode and then having more frequent calls and stepping in. I think that's really hard to say. So it can be between two hours a month and 10 to 20 hours if something really goes wrong, although that's probably more focused on a few board members that really take on responsibilities in the others, basically trusting them and just being there for discussion.</p><h3><strong>Improving Nonprofit Boards in Effective Altruism</strong></h3><p><strong>Ozzie:</strong> I'll chime in a bit. So I've been on two boards Rethink Charity and Rethink Priorities, and also our organization as one board. But our organization Quantified Uncertainty Research Institute is very small. So we just have two board members and that side is minimal. I think you previously mentioned that you've read Holden's posts about why non-profit boards are weird. To me, the strange thing is that a lot of EA organizations are just very small. What I'm used to is a situation where people have managers and the managers are both trained and when they do a good job that's recognized and tracked, and when they do a bad job that's recognized and tracked board members, you don't get those privileges.</p><p>The role of board members is often private and intentional. Their performance, whether good or bad, is not widely known. They are often appointed by those they oversee, which is not ideal. Additionally, they typically lack training and have limited long-term career prospects. Consequently, it's not surprising that boards often perform poorly. While one solution would be to eliminate boards and have larger organizations with more managers, if boards are to remain, there are changes that could be made to improve their effectiveness. For instance, standardizing board policies and evaluating board performance could be helpful. However, evaluating boards can be a nebulous task, and there are few incentives beyond a desire to do well, particularly for those seeking long-term management careers. Therefore, alternative approaches to board evaluation, such as having a decentralized community evaluate boards or appointing a single board member to each organization to ensure adherence to a playbook of board practices, should also be considered.</p><p><strong>Patrick Gruban:</strong> I believe there are several points that could work. Firstly, the question of stakeholders arises, especially when considering financing. For instance, since our organization is mostly funded by CEA, it would be reasonable for CEA to request a seat on the board. This is common in the for-profit world, where VCs acquire seats and align their incentives with the organization's. By having one board member, they could get more influence and ensure that policies are implemented.</p><p>However, accountability between board members is somewhat problematic since one person cannot be responsible for the other. Thus, a membership organization seems more fitting, where members ultimately decide and can replace the board if they are not satisfied with their performance. Therefore, as a board, it is crucial to present what has been achieved in the past year to the members.</p><p>Another point that emerges is the need for audit standards or governance structures that are enforced. In my current company, for example, we produce textiles according to the Global Organic Textile Standard, which is an auditing system that assesses not only the producers but also the companies. Similarly, auditors could come in and ensure that practices are followed, such as giving grants to family members, conducting stress tests on organizations, and having whistleblower protections.</p><p>Fundamentally, this could be enforced by certification or similar practices. Funders could insist that organizations adhere to these standards, including being audited, signing off on documents, and offering whistleblower protections. This does not necessarily have to go through boards.</p><p><strong>Ozzie:</strong> Do you have any thoughts on board members being paid in the future? I assume that they're not paid now.</p><p><strong>Patrick Gruban:</strong>&nbsp; They're not paid now. I think in general it's that it probably would be interesting to get them to get basically hourly compensation but not some payment where it's a fixed payment. So more compensating for the time lost versus having it as an incentive.</p><p>I think having more of this volunteer structure is probably good. And I think I would like to, but I also would like to get more people invited to boards that have, for example, more for-profit experience, more experience, for example, in things like finance and legal aspects and so on.</p><p>Just to have a diversity of expertise on boards. And I think it seems like the <a href=\"https://www.apa.org/about/governance/good-governance/final-report.pdf\">Good Governance Project </a>has a list of people who was interested in these kinds of fields. Could be interesting to see if there\u2019s social talent out there and to see if we can get it perhaps sometimes a bit bigger boards in EA but also like boards with more specialized knowledge in them.</p><p><strong>Ozzie:</strong> Do you think we should consider how to deal with and potentially expose underperforming board members? In small organizations where they are voted on or off by the board, there may be political maneuvering involved. In your organization's case, there is also the question of how candid you can be about individuals who do not meet the desired standards and the process of removing them.</p><p>Is it possible that in a scenario where someone is doing a poor or manipulative job, you would have to wait until the next election, which could be eight months away? And then, after that, run a messy campaign to bring their dubious actions to the attention of all members?</p><p><strong>Patrick Gruban:</strong> My intuition is that, given how cooperative people normally are in EA, if some members were to go to a board member and tell them, \"Look, you're not adhering to the standards and we would like you to step down either now or at the next membership meeting,\" most board members would probably comply because they would see that they are now isolated within the organization. If they don't comply, then it's really hard to see that they have a future in EA because of the high value that we place on cooperation. So I think that's one point to consider. If that approach doesn't work, then I could also see the other board members essentially banding together to remove the non-cooperative member.</p><p><strong>Ozzie:</strong> But at the EA Germany board, can the majority could vote the minority off?</p><p><strong>Patrick Gruban:</strong> No, it's just the membership or the general assembly that votes on board members. But if the board decides, basically by a majority vote, that one of the board members is uncooperative or against the others, then it's probably not such an issue. Currently, we have six people on the board. So, if there were a rogue actor or something similar, it probably wouldn't be an issue. I'm more in favor of having a slightly larger board to account for the possibility of someone dropping out or if two board members have to excuse themselves from a decision. Currently, with only four people on the board, it seems like a very small decision-making group. So, having a bigger and more diverse board seems more sensible in that sense.</p><p><strong>Ozzie:</strong> Do you feel like things are candid and honest enough that if someone were underperforming that they would be asked to leave? My impression is that you are right, that it is rare right now in EA organizations for people to be combative enough to be bad enough to be forced out.</p><p>However, at the same time, you also see them because they're so non-com combative. I've basically never seen people try to push each other out even when they should. And definitely, I'm sure that there were many situations where board members should have left but people are just very kind to each other and want to underplay performance.</p><p><strong>Patrick Gruban:</strong> Yes, I can definitely see that as a potential failure mode. I'm very pleased that last year, we had a change in the board where the new members were pushing for a more ambitious agenda than before. However, I'm not sure how to deal with board members who are complacent but still want to remain on the board. It could be challenging to get them to change their ways. It's also probably a common problem for most boards.</p><p><strong>Ozzie:</strong> Are there any other key failure modes that you see in EA boards in general, or that you're nervous about?</p><p><strong>Patrick Gruban:</strong> I think one thing that I find really weird is the situation at EV. It's one organization with several CEOs and one board. I think the relationship between the board and management is really important, so basically, the board needs to ensure that the management is evaluated, and if they're not up to par, they're changed. The management really needs to take ownership of the organization. EV didn't really have one person taking responsibility for the organization before the current interim changes. So, I'm not very keen on this kind of situation. I would rather have organizations where it's clear who the one person or team is that manages the organization and have oversight of them.</p><p>I think that's something I'm a bit worried about. Also, having boards that are very long-term and not changing, and possibly having more social and friendship interactions within the community might be a possible failure. So, that might point more in the direction of having a little bit bigger boards, having people from outside the super core of the community, but a little bit on the outside coming in, just to ensure that there's a little bit more diversity. Yeah, I think that's what I would prefer.</p><p><strong>Ozzie:</strong> I take it, would you by chance, be willing to rate the average EA board from an A to F both now and where you think it could get in five years with specific interventions?</p><p><strong>Patrick Gruban:</strong> Yeah, I think that for that, I really don't have enough insight. So even if I were to think more about that, I would want to have more transparency. For example, GiveWell has the meetings of their board online, and they started out by having only audio recordings of board meetings, which was a symbolically valuable step. It's perhaps a missed opportunity that other organizations didn't adopt this model. So I think transparency is key, and the lack of it makes it really hard to determine whether the board is good or not. Because, yeah, we just don't see anything.</p><p><strong>Ozzie:</strong> Although I have the impression that they have an open board meeting, often the board meetings are just a small subset of the conversations that the board has, right? A lot of conversations are happening in other channels, and from those board meetings, it would be difficult to know how good board members are performing, right? A lot of the key work that they do is outside of the meetings. So it is a useful thing, but it's probably just a minority of the total set of things that we need to confirm that the board is doing a great job or is in a good place.</p><p><strong>Patrick Gruban:</strong> It would be great to have ex-managers telling a little bit more about how they felt the board was doing, being more candid about that, and just having some kind of rating between people. But I think in a close-knit community where people expect others to be very much they're relying on the others to also be cooperative in the next project, it's really hard to get a very candid view of and performance.</p><p><strong>Ozzie:</strong> When it comes to candidness, there is a wide spectrum of cultures in terms of how candid they are allowed to be. Japan is on one extreme of the power distance ranking, which indicates how candid people are willing to be with their bosses, but I think it is generally true. On the other hand, Israel is on the other extreme. More specifically, I know that Bridgewater has a culture where people are generally extremely candid, more so than what we are used to in EA. EA seems to be very similar to the average Western culture from what I gather, which is definitely lacking in candidness. It falls mid-range of what countries are used to, but arguably you could go quite a bit further than Bridgewater.</p><p>So there is a question about what kind of public information we would like to have about how good board members are doing or how good boards are performing at related organizations. What are people comfortable with now? What would we ideally like them to be comfortable with, and where could we aim for three to five years from now? On one extreme, maybe having a dashboard of every single board member's performance at every EA organization, and maybe a more specific one if you want to be extreme for every single individual within every EA organization or project. The other extreme is keeping a lot of this nebulous, and maybe an intermediary is having private information, so there is an internal dashboard that only a few people have access to. Of course, this is tricky when we have such a loose and decentralized power structure.</p><p><strong>Patrick Gruban:</strong> Yeah, I could see something like a volunteer program where board members say, \"I'm willing to be rated,\" and I voluntarily submit to that. Then publicly, some ratings are available, and that basically creates a structure where it's hard to see as a board member that you're not up to that because it makes you more suspicious or less transparent. It's something voluntary, and you're also asking for negative ratings, but it's a norm that gets to be a norm versus doing that now, which I think is really hard to implement.</p><p>But there's always the question of how open and transparent you should be. In some instances, it's not necessary to have everything super public, but for example, within the organization, it would be good for employees to know how good the board is, and similarly, the board for them to know how the management is doing. That would be something good. I consulted with one EA organization and helped them with a little bit of organizational culture, and they had a really good culture around openness and talking about mistakes, where management was very candid about discussing that.</p><p>I think something like that would be really good to implement on the board level, just having the norm of board members openly saying when they were wrong when they made a mistake, and basically going more in the Bridgewater direction of valuing openness and focusing less on the mistake itself. If you don't communicate the mistake, then it's basically a big no-go. But as long as you're open about mistakes, then that's good, and that should be pushed.</p><p><strong>Ozzie:</strong> Yeah, this is a pretty messy and complicated issue. We could go into other details, but I'm happy to leave that for now, as I can be busy for a few hours. As for reducing risks through the mentioned legal, financial, and reputational aspects, is there anything specific you want to discuss?</p><p><strong>Patrick Gruban:</strong> Yeah, I think regularly conducting a pre-mortem, assessing how the environment has changed (for example, in terms of funding or risks in the community such as media risks), and considering these factors may enable the board to inherently look further ahead than management. In day-to-day management, immediate problems are always being dealt with and perhaps a few months ahead are being considered, but it's really hard to get into the headspace needed to think about longer-term issues. There are a lot of blind spots, so having advisors who think about these blind spots can be helpful. It's especially useful if these advisors have some skin in the game in the organization. If things don't go well, they will have been on the board. If board members feel that they have some ownership of the organization and are able to consider what could go wrong, I think that's a good combination. Alternatively, the board could regularly ask management for reports and information that might indicate any issues that could arise.</p><p><strong>Ozzie:</strong> Great! So, are there any final thoughts you would like to share about boards? I know it's a complex and extensive topic.</p><p><strong>Patrick Gruban:</strong> Yeah, I think it's important to carefully consider the role of boards in organizations. However, we should also be cautious not to rely solely on a good board to de-risk organizations and ensure success. Another important aspect to consider is the governance structure of the organization, which is within the control of management. It's also crucial to prioritize retaining employees and creating a healthy organizational culture. Managers have a responsibility not only to achieve results but also to maintain a healthy workplace environment. Boards can work with management to achieve these goals, but if it's difficult to find suitable leaders, the problem still persists.</p><p>Additionally, implementing policies such as conflict of interest policies can help avoid conflicts of interest when hiring or accepting gifts and funding. It's also important to have good practices in place to ensure adherence to laws, financial stability, and transparency. Standard practices can be established within the organization to reduce the risks of oversight and bad actors. Overall, while boards play an important role, a holistic approach to organizational success includes considering the governance structure and management practices.</p><p><strong>Ozzie:</strong> Yeah, I think the way I look at it is that if we were a big organization that was run decently, there would definitely be a lot of bad big organizations. However, they would have a lot of systematic procedures for different teams. All teams would have to go through the same HR processes, legal processes, and onboarding, and much of that would be very standardized. Given that we have so many tiny EA organizations, some amount of standardization seems like a very reasonable request. But, of course, actually doing that standardization would require someone to figure out what all these rules are and then ensure that they are implemented properly.</p><p><strong>Patrick Gruban:</strong> Yeah, I think we're seeing some good progress in that field. So we have Anti-Entropy as a consulting organization that compiled a list of standardized procedures. Chatter Entrepreneurship does that for their organization, basically compiling things. And then we also have, on the committee builder side, an initiative to up standardized processes at the moment. So that is happening, which is great. And I'm also happy to incorporate that. Additionally, I think it's really great that people are being super helpful in actually helping out with these processes. So, that's more positive, and we're seeing more of that. Funders and people on the EA forum will be more enthusiastic and push for these kinds of good practices, so we can see them.</p><p><strong>Ozzie:</strong> Yeah, that makes sense. There's definitely a lot to do. So to wrap it up, do you have any high-level thoughts on how EA in general could improve? Apart from the things we discussed related to boards, are there any other areas that come to mind, influenced by your background or experience with EA Germany?</p><p><strong>Patrick Gruban:</strong> I think my two main things at the moment are: firstly, I hope that more people are open to serving as board members, even if it's not a paid position, and even if it can be a distraction of their time. I think making it clear that this can be super valuable is important, especially for people who are not very into direct EA work but are more on the outside.</p><p>Such people can be vulnerable to getting in, especially if they have previous experience reports. On the other hand, I also hope that more boards of organizations are open to adding new members who bring diversity and new perspectives. This could be a really good next step forward, and I hope that it will change a little bit.</p><p><strong>Ozzie:</strong> On how many members are you talking about? What size should an organization have, and how many members? Is there any good rule of thumb?</p><p><strong>Patrick Gruban:</strong> Not really, I mean we have six board members and we are four people at the moment as employees, which is a bit strange, but I think it's not so bad because it basically puts less pressure on individual board members. Even like this kind of bigger board, smaller organizations can work. So I'm probably more in favor of a little bit bigger boards that have more different views than two small ones.</p><p><strong>Ozzie:</strong> So bigger means like five to seven.</p><p><strong>Patrick Gruban:</strong> Yeah, perhaps.&nbsp;</p><p><strong>Ozzie:</strong> Okay. Yeah, that makes sense. Yeah. Any parting final wisdom?</p><p><strong>Patrick Gruban:</strong> I think it's super interesting to see how this discussion is going.</p><p>In general, I think it's really good that we are having a productive discussion within the EA community and between different organizations. Despite the difficult issues that have arisen, I believe that they can lead to positive outcomes and meaningful changes within the organizations. I am optimistic about the future and I am interested to see what changes will have been made in a year's time.</p><p><strong>Ozzie:</strong> Yeah, that makes sense. I agree that board membership is very important. However, there aren't that many EA forum posts on the topic, so it may not be one of the top interests of the EA community in terms of how to run a good board so over time we'll gain experience and interest. And I think discussions like this are probably a good way of helping make that happen.</p><p><strong>Patrick Gruban:</strong> Yep. Definitely. Thank you for bringing that up and having this discussion.</p><p><strong>Ozzie:</strong> Yeah. And thanks so much for joining. It was a pleasure.</p><p><strong>Patrick Gruban:</strong> Thanks.</p>", "user": {"username": "oagr"}}, {"_id": "KnRcbttvkgmCvPvyn", "title": "Bear Braumoeller has passed away", "postedAt": "2023-05-05T14:06:56.699Z", "htmlBody": "<p><a href=\"https://twitter.com/osupolisci/status/1653857431188029440\"><u>Professor Bear Braumoeller passed away</u></a> earlier this week.&nbsp;<a href=\"https://polisci.osu.edu/people/braumoeller.1\"><u>Bear</u></a> was a political scientist who studied the likelihood and causes of catastrophic wars. You may have read his book&nbsp;<a href=\"https://www.foreignaffairs.com/reviews/capsule-review/2019-12-10/only-dead-persistence-war-modern-age\"><i><u>Only the Dead</u></i></a> or heard&nbsp;<a href=\"https://80000hours.org/podcast/episodes/bear-braumoeller-decline-of-war/\"><u>his appearance on the 80,000 Hours podcast</u></a>. For a short, recent example of his work I recommend&nbsp;<a href=\"https://warontherocks.com/2022/06/western-leaders-ought-to-take-escalation-over-ukraine-seriously/\"><u>this piece</u></a> of his about the Russia-Ukraine war.</p><p>Bear\u2019s work on conflict likelihood, escalation, and catastrophic wars is certainly among the best research on major conflict risks.&nbsp;<i>Only the Dead</i> was an important counter to strong claims about the long-term declines in interstate violence. Bear found, in brief, that the data on war severity offer few reasons to think that the risk of huge wars (including much-larger-than-WWII-wars) has declined much. And this risk accumulates catastrophically over time.&nbsp;</p><p>One of my favourite sentences from Bear is his darkly-humorous conclusion to a chapter on war severity (p. 130):</p><blockquote><p><i>When I sat down to write this conclusion, I briefly considered typing, \u201cWe\u2019re all going to die,\u201d and leaving it at that. I chose to write more, not because that conclusion is too alarmist, but because it\u2019s not specific enough.</i></p></blockquote><p>Bear combined expertise in both statistical analysis and the theory of what causes war to great effect. He pushed forward our understanding of &nbsp;how the likelihood of major conflict has changed over time and why.&nbsp;<a href=\"https://braumoeller.com/\"><u>His work</u></a> was interesting not just to political scientists but to anyone seeking to understand and reduce global risks.</p><p>I\u2019d corresponded with Bear frequently over the last two years while researching catastrophic conflict risks. He was generous and cared deeply about the social impact of his work. Despite my utter lack of credentials and experience, Bear gave me a lot of his time, advice, and connections to other researchers. In my experience academics rarely engage so meaningfully with outsiders. I was grateful.</p><p>Bear\u2019s interest in EA had been piqued and as far as I know he was planning to do more work on catastrophic risks. Last year his lab received a grant from the Future Fund for follow-up research on the themes he wrote about in&nbsp;<i>Only the Dead</i>.</p><p>He is gone far too soon and will be missed.</p>", "user": {"username": "Stephen Clare"}}, {"_id": "FNSbGh2msfjFGtD7D", "title": "User-Friendly Year Summary and Learnings", "postedAt": "2023-05-05T13:28:57.706Z", "htmlBody": "<p>Here is a short summary of User-Friendly\u2019s work over the last 12 months, some key learnings and how we plan to progress.</p><p>We have also produced a short showreel of some of our projects below.</p><figure class=\"media\"><div data-oembed-url=\"https://youtu.be/cctp0mbOWwg\"><div><iframe src=\"https://www.youtube.com/embed/cctp0mbOWwg\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>&nbsp;</p><p>We set up User-Friendly, a creative agency, to specifically service the EA space with the aim of increasing the quality of marketing output for EA orgs. As well as supporting output, we also aimed to share helpful advice on marketing best-practices to increase exposure to successful marketing as an impact multiplying tool. Our first year has provided us with a sense of both the desire and the necessity for such a service.&nbsp;</p><h1><strong>The Desire</strong></h1><p>In our February 2022&nbsp;<a href=\"https://54ef8a6e-4a7a-40ad-942e-79e10a6c9692.usrfiles.com/ugd/54ef8a_87058a7fa3a84a35ac9fdfb9010b7a57.pdf\"><u>scoping survey</u></a>, 72% of respondents agreed that effective marketing was important to the success of their organisation's objectives, 89% of respondents agreed that an EA aligned marketing agency would be a useful resource within the movement and 62.8% of respondents indicated that they would be likely to use this service.&nbsp;</p><p>We were awarded an EA Infrastructure grant to support our set up costs but other than this, we are entirely funded by organisations paying for our services. Over the last year, we have worked with a total of 27 organisations across all cause areas on a range of different projects.</p><p>Over the past 12 months we have;</p><ul><li>Completed 15 graphic design projects from brochures and report designs to social media templates and pitch decks</li><li>Branded eight EA organisations</li><li>Designed and built four simple websites and designed another four which a developer has then built</li><li>Designed and developed four campaigns, three focussed on brand building and one focussed on increasing donations</li><li>Worked with two organisations on their messaging and positioning</li><li>Ran the social media accounts of one organisation</li></ul><p>All of the above work was completed with us operating at 'full capacity'.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefoxqeol1vnbc\"><sup><a href=\"#fnoxqeol1vnbc\">[1]</a></sup></span></p><h2>Client Survey Results</h2><p>An important part of testing this service offering has been feedback from our clients. We introduced a client feedback survey mid last year in order to better understand our performance, pricing and positioning in this space. Below are the key results;</p><ul><li>21 organisations completed the form</li><li>95% Said that the collaboration was successful (<i>1 entry was neutral</i>)</li><li>95% Said their overall experience of working with us was positive (<i>1 entry was neutral</i>)</li><li>47.6%&nbsp;<strong>strongly</strong> agreed that the requirements of the brief were understood and delivered upon, 47.6% agreed (<i>1 entry was neutral</i>)</li><li>57.1% were&nbsp;<strong>very</strong> satisfied with the standard of the completed work, 33.3% satisfied (<i>2 entries were neutral</i>)</li><li>100% were satisfied that User-Friendly had the necessary expertise to complete the work</li><li>66.7% found us appropriately priced, 9.5% expensive, 14.3% inexpensive and 9.5% very inexpensive (price change in late 2022 will have skewed these results across the 12 months)</li><li>Half thought that they would have service requirements in the next 12 months with \u2153 requiring this resource in the next 6 months</li><li><strong>100% would recommend User-Friendly to others</strong></li><li><strong>100% of those who have a specific future service demand would be likely to use User-Friendly again</strong></li></ul><p>General additional comments included:</p><ul><li>Some clients reported that they had expected to receive editable files, which isn't always a part of a quote (e.g. a long form report design), so we have adapted our process to ensure this is more clear at the quoting stage.</li><li>Some feedback mentioned the need for more clarity during a web development process as to the stages. We have created a new timeline document that covers this in much more detail and captures each stage of the process with deadlines and feedback rounds blocked out.</li><li>We received feedback that being able to provide a web development service, rather than just design would be helpful. This has always been our intention, and whilst in some cases we've been able to partner with a developer that we can trust to deliver the site, this is an ongoing challenge. We would welcome being connected to any freelance developers as this is a constant bottleneck across the movement - we design more websites than one developer could undertake, so we are constantly in need of more resource in this area.</li></ul><p><br>Other than three forum posts and commenting on the odd slack thread, we didn\u2019t market ourselves. We\u2019ve worked entirely through word-of-mouth and recommendations which, whilst not necessarily recommended (more below), has shown a steady demand for this service over the past year.&nbsp;</p><p>&nbsp;</p><blockquote><p><i>It has been a pleasure to work with User-Friendly as they redesigned our website and social media presence. They were responsive to our needs and their marketing expertise consistently shone through. They helped us find a look that captures who we are and amplifies the messages we wish to convey. We would gladly recommend User-Friendly to those looking for a site that embodies their organisation.</i></p><p><strong>Samuel Curtis, The Future Society</strong></p><p>&nbsp;</p></blockquote><blockquote><p><i>Working with User-Friendly was a great experience for us! In need of a full website redesign, we wanted to collaborate with someone who would understand the nuances of our messaging and mission. Not only did User-Friendly create a redesign that we\u2019re truly excited by, they also helped us refine our overall brand and better understand how to position ourselves as an organisation</i>.&nbsp;</p><p><strong>Probably Good, Anna Beth Lane</strong>&nbsp;</p></blockquote><h1><br><strong>The Necessity</strong></h1><p>Alongside the desire from the movement for this service offering, what these collaborations and our general observations within the space have shown us, is the necessity.</p><p>We started User-Friendly with the aim of supporting the movement in this area of expertise, providing high-quality marketing and communications in order to increase the quality of marketing practices across the movement more broadly.</p><p>We feel that this area of expertise is being underutilised and have perhaps been overly optimistic in regards to how easy it would be to convince others in the movement that highly-effective marketing should be more highly prioritised.&nbsp;</p><p>We feel that marketing remains under-utilised as an impactful tool for success and we base this on the following observations;</p><ul><li>We regularly see new organisations receiving funding without any budget for marketing. In one example, a public-facing organisation had received a substantial donation to start their intervention with nothing allocated for any marketing elements e.g. branding, website etc.</li><li>Very few organisations have marketing professionals in the team.</li><li>Very few organisations have a marketing budget and most that do are severely underfunding it.</li><li><a href=\"https://forum.effectivealtruism.org/posts/6LwxzbiqNcXfXeb7K/a-short-post-on-long-term-marketing-and-why-you-should\"><u>Prioritisation of short-term objectives over long-term brand building</u></a>.</li><li>Strong lack of social media presence across the movement.</li><li>Organisations launching with substandard branding, even though your&nbsp;<a href=\"https://www.marketingweek.com/context-matters-consumers-judge-brands-by-the-company-they-keep/\"><u>first impression</u></a> can be critical to your future success.&nbsp;</li><li>Very few successful brand building campaigns being developed.</li></ul><p>We hope that our existence and through our organisation collaborations that the dial can shift in favour of utilising marketing more effectively for organisation objectives. We plan to apply for further funding to commit more time to resource building and developing tools to support organisations more broadly.</p><h1>Creative Campaigns</h1><p>Alongside general marketing principles being utilised by the movement, we are very optimistic about the possible impact organisations could have with more creative marketing, advertising and campaigning.&nbsp;</p><p>As a movement focused on impact, we believe there is a huge potential \u2018left on the table\u2019 in regards to our marketing attempts and are excited to support organisations that believe the same. We want to test our intuition that utilising creative marketing practices could generate bigger and more long-term results for brands within EA as well as spreading the ideals that underpin them. We are planning a dedicated push towards running and trialling more creative marketing campaigns, with the support of funders, so that we can make this case clear. We would love to encourage a step up in the marketing output that the movement produces, most specifically in public-focused brands.</p><p>If there are any organisations that would be interested in partnering with us, please do get in touch at&nbsp;<a href=\"mailto:letstalk@userfriendly.org.uk\"><u>letstalk@userfriendly.org.uk</u></a></p><h1><strong>User-Friendly Key Learnings</strong></h1><h2>Quality over volume</h2><p>When we first started supporting organisations we were overwhelmed by the initial response and received a substantial volume of requests that often came with short deadlines. In our first months, as we wanted to hit the ground running, we took on most projects within our broad service offering. This resulted in a lower standard but higher frequency of work as we attempted to adhere to these short timelines.&nbsp;</p><p>This was not a sustainable model and was not producing high-quality outputs. As a response to this, we reduced the volume of projects undertaken within each month (by also increasing our pricing, more on this below). This allowed us to ensure a higher quality of work and a degree of focus on each project that would support the most effective outputs.&nbsp;</p><p>We also began to collaborate with external creatives who have brilliant skill sets to add to our service offering. This has increased our capacity to offer high-quality, creative outputs for our clients in some cases (more below).</p><h2>Proving the concept</h2><p>It can feel difficult to prove the power of marketing and branding, especially over the short term. And whilst there are metrics which can neatly make this case, most organisations aren\u2019t tracking them - either through lack of resource or know-how.</p><p>In short-term activations, however, it is much easier to capture supporting data:</p><p>There have been multiple client collaborations which have proved our intuition that effective marketing can multiply movement efforts. Most notably; 80,000 hours commissioned work from three graphic designers and our contribution significantly outperformed the work of the other designers. Our collaboration with Animals Aotearoa resulted in over half of the total signatures they received coming directly from our ads. In the first week of the campaign, 48.72% who clicked on our ads, signed their petition. We have recently worked with CHERI on their fellowship application ads and, working alongside Good Impressions, they saw a 560% increase in applications compared to last year (~80 - ~530).</p><h2>Increasing prices&nbsp;</h2><p>In late 2022 we increased our prices. The increase still pitches us at roughly 25% less than the UK average for creative agency work. The feedback we received from some early clients and advisors was that we weren\u2019t charging enough and so we increased our rates in order to fulfil the above, quality over quantity.&nbsp;</p><p>We also now typically ask for a 50% upfront fee and 50% at completion. This is common practice within most agency work and we have found it helps our cash flow to stay more balanced. If there is any organisation that can\u2019t do this, e.g. awaiting funding, we are flexible with this arrangement and wouldn\u2019t let it prevent work starting.</p><h2>Marketing ourselves</h2><p>Over the last year, we haven\u2019t marketed ourselves. We\u2019ve been very fortunate to have worked entirely through word-of-mouth and recommendations. One year on we plan to hone our service offering and start to increase our outreach and pitch more intentionally for the work we are most skilled to execute. Within a small team, we also have a limited number of hours we can commit to each project and so the more time intensive work e.g. social media management will be less of a priority for us.</p><h3>We would characterise these changes broadly as:</h3><p>More</p><ul><li>Website design</li><li>Branding</li><li>Marketing and Communications Strategy</li><li>Creative Campaigns</li><li>Graphic Design</li></ul><p>Less</p><ul><li>PR or press work</li><li>Social media management</li><li>Setting up the infrastructure of ads (Good Impressions didn\u2019t exist when we first started so we are delighted to support them with the design elements of their work when requested, but leave the running of ads to them)</li></ul><h2>Collaborating with other freelancers</h2><p>We began collaborating with creatives across many areas of expertise in order to expand our \u2018one stop shop\u2019 approach and bring external creatives into the space. We trialled the collation of a database of these individuals however, the management of this e.g. co-ordinating and paying for test tasks, vetting their work, keeping them up to date etc. takes up quite a substantial amount of time, which, without funding, we don\u2019t have the means to support.&nbsp;</p><p>We have had some successful collaborations from this database which have increased our ability to provide affordable, creative solutions such as working with; an animator, a voice over artist, a digital content expert and a website developer. We will continue to utilise external services such as these but will likely retire the database unless we can secure funding to manage it.&nbsp;</p><h1>Final thoughts</h1><p>Overall, we feel very optimistic about our first year supporting 27 organisations across all focus areas. The feedback and testimonials we have received demonstrate our ability to create impactful and effective marketing outputs that satisfy our clients. We have learned a great deal from our experience and continue to make improvements to our process by prioritising quality over quantity, asking for 50% upfront fees, and focusing on marketing ourselves more effectively. We aim to expand our outreach in areas such as website design, branding, strategy, and campaigns in our client-based model. Alongside this, we are going to dedicate much more of our time on trying to develop bigger and more creative marketing and campaign ideas.</p><p>Additionally, we continue to aspire to educate the movement with advice and knowledge on best practices in marketing practices and will continue exploring ways to do so. We are currently looking for funding to execute creative campaigns both in client collaborations and independently. Within both projects, we want to test our intuition that utilising creative marketing practices to develop memorable campaigns could increase the impact of this movement's work. If you are interested in funding this work, please get in touch.&nbsp;</p><p>Overall, we are proud of our accomplishments and are excited about future opportunities to support EA organisations to multiply their efforts to solve the world's biggest challenges.</p><blockquote><p><i>My team and I had a great experience working with User-Friendly on our recent marketing campaign. User-Friendly created six graphics and accompanying copy to help advertise our research fellowship \u2013 all of which were creative, professional, and highly effective in attracting potential candidates. We\u2019d definitely recommend User-Friendly for anyone in need of a keen creative eye!</i>&nbsp;</p><p><strong>Swiss Existential Risk Initiative, Sofia Mikton</strong></p></blockquote><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnoxqeol1vnbc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefoxqeol1vnbc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For the first eight months, this was completed by 1 full-time member of staff (James Odene), with some support from Amy Odene. Amy then joined User-friendly full-time in December 2022. &nbsp;We have resisted the urge to grow too fast, even if this has meant turning down work, as we wanted to ensure we tested the model in detail and would require funding to support the sustainability of growth beyond this.</p></div></li></ol>", "user": {"username": "J_Odene_UF"}}, {"_id": "ehBLFd2u4va8Sb7hX", "title": "Research Manager for the Effective Institutions Project", "postedAt": "2023-05-05T11:12:20.107Z", "htmlBody": "<h1><strong>Research Manager</strong></h1><p>The <a href=\"https://effectiveinstitutionsproject.org/\"><strong><u>Effective Institutions Project</u></strong></a> (EIP) is seeking a <strong>Research Manager</strong> who will implement the organization\u2019s&nbsp; research vision and agenda, design and oversee targeted research projects, and manage EIP\u2019s learning infrastructure and knowledge management operations. We seek a strong analytical thinker and excellent communicator who is passionate about the challenge of navigating large and complex organizational systems in search of meaningful opportunities for societal benefit.</p><h1><strong>About the Effective Institutions Project</strong></h1><p>The Effective Institutions Project is a philanthropic advisory and research organization that seeks out and incubates high-impact strategies to improve institutional decision-making around the world. We analyze how and in what ways institutions\u2019 decisions influence people\u2019s lives, study how key institutions make decisions, identify interventions that might help those institutions take actions that will lead to better global outcomes, and mobilize talent and resources to execute on the most promising interventions. All the while, we are building an interdisciplinary network of reformers to share insights, coordinate efforts, and increase the odds of success over time.</p><p>In the past year, our team has undertaken <a href=\"https://forum.effectivealtruism.org/posts/tNq65hopAmX3YeKoL/a-landscape-analysis-of-institutional-improvementaking-which-institutions-a\"><strong><u>research to identify the world\u2019s most important institutions</u></strong></a>, completed in-depth research reports on Google and the US National Security Council, and established the EIP Innovation Fund to help build the field of institution-focused philanthropy. Based on our progress to date, we think it\u2019s realistic to imagine that in the next few years EIP\u2019s research could inform tens to hundreds of millions of dollars in spending to improve global institutional effectiveness and help shape the decisions of 100s to the low 1000s of people crafting policy, consulting, nonprofit leadership, and tech industry careers. Our theory of change is that those funding and career choices will increase the odds that important institutions take beneficial actions like implementing new policies to help avert or mitigate future global crises, increasing the budgets and effectiveness of global health and anti-poverty programs, and taking steps to increase accountability to all living beings impacted by their decisions.&nbsp;</p><h1><strong>About the role</strong></h1><p>The Research Manager will implement the organization\u2019s&nbsp; research vision and agenda, design and oversee targeted research projects, and manage EIP\u2019s learning infrastructure and knowledge management operations. The work is fast-paced and highly varied, reflecting EIP\u2019s young history and ambitious mission. The position carries with it significant potential for professional growth and impact as the organization matures and continues to build out its capabilities.&nbsp;</p><p>In this role, you can expect to:</p><ul><li><strong>Help shape EIP\u2019s research vision, strategy and agenda</strong> in close collaboration with EIP\u2019s Executive Director and strategic advisors. This includes providing input into the selection of priority focus areas, planning and sequencing research projects, and overseeing research teams.</li><li><strong>Commission and manage research projects on specific institutions</strong> in accordance with the research priorities above by seeking out, engaging, and serving as primary point of contact for relevant experts on the institutions in question.</li><li><strong>Lead updates to EIP\u2019s landscape analysis of institutional improvement opportunities,</strong> including formalizing our methodology for estimating the impact of an institution\u2019s decisions.</li><li><strong>Stay up-to-date with relevant research and developments</strong> on key institutions, selected policy issues, institutional design and governance paradigms, organizational behavior, and other relevant fields. Synthesize and communicate key insights internally and externally as appropriate.</li><li><strong>Oversee EIP\u2019s knowledge management and organizational learning systems</strong>, including maintaining an airtight process for collecting and synthesizing knowledge from disparate sources.</li><li><strong>Recruit, manage and mentor</strong> high-performing research analysts, fellows and external collaborators and ensure that they have what they need to succeed.</li><li><strong>Lead outreach to experts on key policy issues and program areas</strong> to build relationships in support of our mission.</li><li><strong>Draft and refine EIP knowledge products</strong> including policy proposals, reports, funder guides, and other materials.</li></ul><h1><strong>Ideal profile</strong></h1><p>Our ideal candidate will have most if not all of the following:</p><ul><li>Innate intellectual curiosity and a passionate drive to understand social and organizational systems in all of their complexity</li><li>An uncommon ability to distill complex information signals from a vast and diverse knowledge base into actionable recommendations</li><li>Excellent organizational skills, reliability, and attention to detail</li><li>Strong epistemic rigor and reasoning transparency</li><li>Clear verbal and written communication in English</li><li>Relevant content expertise on issues and/or institutions we care about, e.g. US government, big tech companies, multilateral institutions, global catastrophic risks,&nbsp; global health and development, institutional design, or other relevant fields</li><li>Relevant work experience involving collaborating with research teams, identifying research questions, synthesizing information, and translating insights for policy and practice</li><li>A strong ethical compass and motivation to work toward the benefit of others, both today and in the future</li></ul><p>In addition, the following strengths are highly desirable but not required:</p><ul><li>Past experience working for or with one or more of the top-ranked institutions identified in EIP\u2019s most recent <a href=\"https://drive.google.com/file/d/1pWyVbD_h2BHz5sCCzHabW5Ey0kNYV_xl/view\"><strong><u>landscape analysis</u></strong></a></li><li>Leadership and management experience, including implementing strategic visions, planning and overseeing complex projects, building partnerships, and managing high-performing teams</li><li>Experience in forecasting or journalism, and facility with social media</li><li>Facility and experience with quantitative reasoning, cost-effectiveness analysis, and/or wellbeing economics</li></ul><p>If you\u2019re excited about this role but are unsure if you fit the criteria, we would love to have you apply anyway.&nbsp;</p><h1><strong>Compensation and benefits</strong></h1><p>This position is remote and can be based anywhere in the world. For a US-based candidate, we expect the salary to be approximately $75,000 per year with benefits including paid time off, reimbursement for health insurance (individual or family plan), and a 5% 401(k) employer contribution on top of your salary. We are also open to candidates from other countries, and we will try our best to offer an equivalent compensation package regardless of location.&nbsp;</p><h1><strong>To apply</strong></h1><p>We are hoping to fill this role by the end of June 2023 and to have the candidate start soon afterwards. To be considered for this role, please complete this <a href=\"https://forms.gle/J2i7LuKwAKJhKdQm9\"><strong><u>form</u></strong></a> by <strong>Monday, May 22</strong>.</p><p>Note: the application form requires a Google account in order to upload your resume. If you don\u2019t have a Google account, feel free to email your materials and answers to the application questions, following all character limits, to <a href=\"mailto:neil@eahire.org\"><strong><u>neil@eahire.org</u></strong></a>. A cover letter is not required.</p><p><i>Effective Institutions Project is committed to building a diverse applicant pool. We strongly encourage interested candidates to apply regardless of gender, race, ethnicity, nationality, physical ability, educational background, socioeconomic status, etc.</i></p>", "user": {"username": "Jam Kraprayoon"}}, {"_id": "uB8BgEvvu5YXerFbw", "title": "Intro to ML Safety virtual program:  12 June - 14 August", "postedAt": "2023-05-05T10:04:58.855Z", "htmlBody": "<p><strong>The Intro to ML Safety course covers foundational techniques and concepts in ML safety for those interested in pursuing research careers in AI safety, with a focus on empirical research.&nbsp;</strong></p><p>We think it's a good fit for people with ML backgrounds who are looking to get into empirical research careers focused on AI safety.</p><p>Intro to ML Safety is run by the&nbsp;<a href=\"https://safe.ai/\"><u>Center for AI Safety</u></a> and designed and taught by&nbsp;<a href=\"http://danhendrycks.com/\"><u>Dan Hendrycks</u></a><u>,</u> a UC Berkeley ML PhD and director of the&nbsp;<a href=\"http://safe.ai/\"><u>Center for AI Safety</u></a><u>.</u></p><h3><a href=\"https://course.mlsafety.org/\"><i><strong><u>Apply to be a participant</u></strong></i></a><i><strong> by May 22nd</strong></i></h3><p><i>Website:&nbsp;</i><a href=\"https://course.mlsafety.org/\"><i>https://course.mlsafety.org/</i></a><i>&nbsp;</i></p><h2><strong>About the Course</strong></h2><p>Intro to ML Safety is an 8-week virtual course that aims to introduce students with a deep learning background to the latest empirical AI Safety research. The program introduces foundational ML safety concepts such as robustness, alignment, monitoring, and systemic safety.</p><p>The course takes 5 hours a week, and consists of a mixture of:</p><ul><li>Assigned readings and lecture videos (<i>publicly available at&nbsp;</i><a href=\"https://course.mlsafety.org/\"><i><u>course.mlsafety.org</u></i></a><i><u>)</u></i></li><li>Homework and coding assignments</li><li>A facilitated discussion session with a TA and weekly optional office hours</li></ul><p>The course will be virtual by default, though in-person sections may be offered at some universities.<br>&nbsp;</p><h3><strong>The Intro to ML Safety curriculum</strong></h3><p>The course covers:</p><ol><li><strong>Hazard Analysis</strong>: an introduction to concepts from the field of hazard analysis and how they can be applied to ML systems; and an overview of standard models for modelling risks and accidents.</li><li><strong>Robustness</strong>: Robustness focuses on ensuring models behave acceptably when exposed to abnormal, unforeseen, unusual, highly impactful, or adversarial events. We cover techniques for generating adversarial examples and making models robust to adversarial examples; benchmarks in measuring robustness to distribution shift; and approaches to improving robustness via data augmentation, architectural choices, and pretraining techniques.</li><li><strong>Monitoring</strong>: We cover techniques to identify malicious use, hidden model functionality and data poisoning, and emergent behaviour in models; metrics for OOD detection; confidence calibration for deep neural networks; and transparency tools for neural nets.</li><li><strong>Alignment</strong>: We define alignment as reducing inherent model hazards. We cover measuring honesty in models; power aversion; an introduction to ethics; and imposing ethical constraints in ML systems.</li><li><strong>Systemic Safety</strong>: In addition to directly reducing hazards from AI systems, there are several ways that AI can be used to make the world better equipped to handle the development of AI by improving sociotechnical factors like decision making ability and safety culture. We cover using ML for improved epistemics; ML for cyberdefense; &nbsp;and ways in which AI systems could be made to better cooperate.</li><li><strong>Additional X-Risk Discussion</strong>: The last section of the course explores the broader importance of the concepts covered: namely, existential risk and possible existential hazards. We cover specific ways in which AI could potentially cause an existential catastrophe, such as weaponization, proxy gaming, treacherous turn, deceptive alignment, value lock-in, and persuasive AI. We introduce some considerations for influencing future AI systems; and introduce research on selection pressures.&nbsp;</li></ol><h3><strong>How is this program different from AGISF?</strong></h3><p>If you are interested in an empirical research career in AI safety, then &nbsp;you are in the target audience for this course. The ML Safety course does not overlap much with AGISF, so we expect that participants who both have and have not previously done AGISF to get a lot out of Intro to ML Safety.</p><p>Intro to ML Safety is<strong> focused on ML empirical research&nbsp;</strong>rather than conceptual work. Participants are required to watch recorded lectures and complete homework assignments that test their understanding of the technical material.&nbsp;</p><p>You can read about more the ML safety approach in&nbsp;<a href=\"https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5\"><u>Open Problems in AI X-risk</u></a>.</p><h2><strong>Time Commitment</strong></h2><p>The program will last 8 weeks, beginning on June 12th and ending on August 14th.&nbsp;</p><p>Participants are expected to commit around 5-10 hours per week. This includes ~1-2 hours of recorded lectures, ~2-3 hours of readings, ~2 hours of written assignments, and 1.5 hours of in person discussion.</p><p>In order to give more people the opportunity to study ML Safety, we will provide a $500 stipend to eligible students who complete the course</p><h2><strong>Eligibility</strong></h2><p>This is a technical course. A solid background in deep learning is required.&nbsp;</p><p>If you don\u2019t have this background, we recommend Week 1-6 of <a href=\"https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/course/\">MIT 6.036</a> followed by Lectures 1-13 of the <a href=\"https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/schedule.html\">University of Michigan\u2019s EECS498</a> or Week 1-6 and 11-12 of <a href=\"https://atcold.github.io/pytorch-Deep-Learning/\">NYU\u2019s Deep Learning</a>.</p><h3><a href=\"https://course.mlsafety.org/\"><i><strong><u>Apply to be a participant</u></strong></i></a><i><strong> by May 22nd</strong></i></h3><p><i><strong>Website:&nbsp;</strong></i><a href=\"https://www.mlsafety.org/intro-to-ml-safety\"><i><strong><u>https://course.mlsafety.org/</u></strong></i></a></p>", "user": {"username": "james_aung"}}, {"_id": "sDkHTdBsrpz7teMR2", "title": "Please don\u2019t vote brigade", "postedAt": "2023-05-05T08:56:58.903Z", "htmlBody": "<p>Once in a while, the moderators will find out that something like the following happened:&nbsp;</p><ul><li>Someone posted an update from their organization, and shared it on Slack or social media, asking coworkers and friends to go upvote it for increased visibility.</li><li>Someone saw something they didn\u2019t like on the Forum \u2014 maybe comments criticizing a friend, or a point of view they disagree with \u2014 and encouraged everyone in some discussion space to go downvote it.</li></ul><p><strong>This is a form of&nbsp;</strong><a href=\"https://en.wikipedia.org/wiki/Vote_brigading\"><strong><u>vote brigading</u></strong></a><strong>.</strong> It messes with karma\u2019s ability to provide people with a signal of what to engage with and is against&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum#Voting_norms\"><u>Forum norms</u></a>.&nbsp;</p><p><strong>Please don\u2019t do it. We might ban you for it.</strong></p><p><strong>If you\u2019re worried that someone else (or some other group) is engaging in vote brigading,&nbsp;</strong><a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum#Contact_the_moderators_or_report_inappropriate_behavior\"><strong><u>bring it up to the moderators</u></strong></a><strong> instead of trying to correct for it.&nbsp;</strong></p><h2>Why is it bad?</h2><ul><li>Karma is meant to provide a signal of what Forum users will find useful to engage with. Vote brigading turns karma into a popularity contest.<ul><li>Voting should be based on readers\u2019 opinions of the content they\u2019re voting on. If someone convinces you that a post is terrible \u2014 or great \u2014 it\u2019s fine to downvote or upvote it as a result of that, but you should actually believe that.</li></ul></li><li>We should resolve disagreements by discussing them, not by comparing the sizes of the groups who agree with each position.&nbsp;</li><li>If people try to hide criticism by downvoting it just because they feel an affinity to the group(s) criticized, the Forum will become predictably biased. We won\u2019t have important conversations, we won\u2019t learn from each others\u2019 mistakes, etc.&nbsp;</li></ul><h2>What actions should we avoid? (What counts as vote brigading?)</h2><p><strong>If you\u2019re sharing content:</strong></p><ul><li>Don\u2019t encourage people to all go upvote or downvote something (\u201ceveryone go upvote this!\u201d) \u2014 especially when you have power over the people you\u2019re talking to.<ul><li>It\u2019s more ok to say \u201cgo upvote this if you think it\u2019s good,\u201d but it\u2019s still borderline, and you should be careful to make sure that it doesn\u2019t feel like pressure on people.&nbsp;</li></ul></li><li>Be careful with bias: if the content is criticizing your work, or your friend\u2019s work, or something you feel an affinity towards \u2014 be suspicious of your ability to objectively engage with it.<ul><li>Consider letting other Forum users sort it out or leaving a comment explaining your point of view.&nbsp;</li></ul></li></ul><p><strong>If you\u2019re voting:</strong></p><ul><li>Please make sure you\u2019re really voting because you think this content is good.&nbsp;<ul><li>If your friends or coworker shared their content and that\u2019s the only thing you really engage with and vote on, interrogate your heart or mind about whether you might be biased.&nbsp;</li></ul></li><li>Please&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum#Contact_the_moderators_or_report_inappropriate_behavior\"><u>report</u></a> attempts at vote brigading to us.</li></ul><p><strong>Examples</strong></p><p>There are many borderline cases. Here are some examples, sorted by how fine/bad the action of person sharing the content is:&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#1b9db3;border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>The action</strong></td><td style=\"background-color:#1b9db3;border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Is it ok to do?</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">You share a post (and maybe what you like or dislike about it), without explicitly asking people to upvote or downvote.</td><td style=\"background-color:#b6d7a8;border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>It\u2019s fine</strong> (I\u2019m very happy for people to straightforwardly share posts with people who might find them interesting)</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">You share a post and what you like about it, and say something like \u201cupvote the post if you like it\u201d</td><td style=\"background-color:#efefef;border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Iffy, but mostly ok</strong>. The problem is that people might vote reflexively as a result (or follow this like an order, if you\u2019re in a leadership role), so I think that\u2019s the main thing to be wary of.&nbsp;</p><p>I think the situation is worse for downvoting than for upvoting, here (coordination on downvoting can suppress a post) \u2014 see below.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">You share a post that criticizes your work, and write something like \u201cdownvote the post if you think it should have less visibility\u201d&nbsp;</td><td style=\"background-color:#ea9999;border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Not ok</strong> \u2014 even though there\u2019s an \u201cif\u2026\u201d. Don\u2019t do this, especially if you\u2019re in a leadership role.&nbsp;&nbsp;</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">You share a post and say something like \u201cEveryone: go upvote the post!\u201d</td><td style=\"background-color:#ea9999;border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Not ok</strong>. Once again, it\u2019s even worse if you\u2019re in a leadership role with respect to the people you\u2019re sharing the post with.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">On a call with other people, and you say, \u201cthere\u2019s this post I don\u2019t like / a post that\u2019s criticizing me/us. Could you all upvote / downvote it?\u201d</td><td style=\"background-color:#ea9999;border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Extremely not ok</strong>. This has the added harm of making it easy for the asker to see if the other people on the call downvoted the post.&nbsp;</td></tr></tbody></table></figure><h2>Other voting norms</h2><p>You can see the full&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum#Voting_norms\"><u>voting norms</u></a> here. Most importantly,&nbsp;don't do the following:</p><ul><li>\u201cMass voting\u201d on many instances of a user\u2019s content simply because it belongs to that user</li><li>Using multiple accounts to vote on the same post or comment</li></ul><p>If you have any concerns, you can get in touch with the moderation team<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgikaonzig7j\"><sup><a href=\"#fngikaonzig7j\">[1]</a></sup></span>&nbsp;by emailing <a href=\"mailto:forum-moderation@effectivealtruism.org\">forum-moderation@effectivealtruism.org</a>.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngikaonzig7j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgikaonzig7j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The current active moderators are me, <a href=\"https://forum.effectivealtruism.org/users/lorenzo-buonanno?mention=user\">@Lorenzo Buonanno</a>, and <a href=\"https://forum.effectivealtruism.org/users/jpaddison?mention=user\">@JP Addison</a>, but the email reaches the whole team (including advisors), and the Forum team \u2014 you can get in touch with individual moderators by DMing us on the Forum.</p></div></li></ol>", "user": {"username": "Lizka"}}, {"_id": "vaqoGFRdi6ftvwGkn", "title": "What is effective altruism? How could it be improved?", "postedAt": "2023-05-05T15:53:34.503Z", "htmlBody": "<p>The EA community has been convulsing since FTX. There's been lots of discontent, but almost no public discussion between community leaders, and little in the way of a constructive suggestions for what could change. In this post, I offer a reconceptualisation of what the EA community is and then use that to sketch some ideas for how to do good better together.</p><p>I\u2019m writing this purely in my personal capacity as a long-term member of the effective altruism community. I drafted this at the start of 2023, in large part to help me process my own thoughts. The ideas here are still, by my lights, dissatisfyingly underdeveloped. But I\u2019m posting it now, in its current state and with minimal changes, because it's suddenly relevant to topical discussions about how to run the <a href=\"https://forum.effectivealtruism.org/posts/zzpwpDkQBTzxbjgJo/apply-or-nominate-someone-to-join-the-boards-of-effective\">Effective Ventures Foundation</a> and the <a href=\"https://forum.effectivealtruism.org/posts/GcvEdYJADH3vMqk3F/suggest-candidates-for-cea-s-next-executive-director\">Centre for Effective Altruism</a> and I don't know if I would ever make time to polish it.</p><p>[I'm grateful to Ben West, Chana Messinger, Luke Freeman, Jack Lewars, Nathan Young, Peter Brietbart, Sam Bernecker, and Will Troy for their comments on this. All errors are <s>theirs</s> mine]</p><h3>Summary</h3><ul><li>We can think of effective altruists as participants in a market for maximum impact activities. It\u2019s much like a local farmers\u2019 market, except people are buying and selling goods and services for how best to help others.</li><li>Just like people in a market, EAs don\u2019t all share the same goal - a marketplace isn\u2019t an army. Rather, people have different goals, based on their different accounts of what matters. The participants can agree, however, that they all want there to be a marketplace to allow them to meet and trade; this market is useful because people want different things.&nbsp;</li><li>Presumably, the EA market should function as a free, competitive market. This means lots of choice and debate among the participants. It requires the market administrators to operate a level playing-field.&nbsp;</li><li>Currently, the EA community doesn\u2019t quite operate like this. The market administrators - CEA, its staff and trustees - are also major market participants, i.e. promoting particular ideas and running key organisations. And the market is dominated by one big buyer (i.e. it\u2019s a \u2018monopsony\u2019).&nbsp;</li><li>I suggest some possible reforms: CEA to have its trustees elected by the community; it should strive to be impartial rather than take a stand on the priorities. I don\u2019t claim this will solve all the issues, but it should help. I'm sure there are other implications of the market model I've not thought of.</li><li>These reforms seem sensible even without any of EA\u2019s recent scandals. I do, however, explain how they would likely have helped lessened these scandals too.</li><li>I\u2019ve tried to resist getting into the minutiae of \u201chow would EA be run if modelled on a free market?\u201d and I would encourage readers also to resist this. I want people to focus on the basic idea and the most obvious implications, not get stuck on the details.</li><li>I\u2019m not very confident in the below. It\u2019s an odd mix of ideas from philosophy, politics, and economics. I wrote it up in the hope others can develop the ideas and I can stop ruminating on the \u201cwhat should FTX mean for EA?\u201d question.&nbsp;</li></ul><h3>What is EA? A market for maximum-impact altruistic activities</h3><p>What&nbsp;is effective altruism? It's described by the website <a href=\"https://www.effectivealtruism.org/\">effectivealtruism.org</a> as a \"research field and practical community that aims to find the best ways to help others, and put them into practice\". That's all well and good, but it's not very informative if we want to understand the behaviour of individuals in the community and the functioning of the community as a whole.&nbsp;</p><p>An alternative approach is to think of effective altruists, the people themselves, in economic terms. In this case, we might characterise the effective altruism community as a group of individuals participating in a marketplace for maximum impact philanthropic goods and services - an agora for altruists.&nbsp;</p><p>This may initially seem strange, but effective altruism community is somewhat analogous to a local farmers' market. However, instead of the market stalls offering fruit, vegetables, and so on, the sellers are touting various charities, as well as research into those charities. There are buyers - donors - who are looking to get the best value for their money. There\u2019s a social community that\u2019s formed around the market. There are some people whose job it is to run the market, too.&nbsp;</p><p>The motivation of the participants in each market may be different: people come to the 'EA market' for the good of others but go to the farmers' market for their own good. However, in many other respects, the markets function identically: buyers come to shop around and find the best deal for their money, and sellers are trying to sell as much of their goods as possible.&nbsp;</p><p>One key practical difference is that,&nbsp;whilst you can inspect the quality of goods in the farmers\u2019 market yourself (you can see if the fruit is rotten) buyers in the EA market can\u2019t easily test or observe the goods they buy (you don\u2019t see what happens when you, say, donate to AMF). Hence, arguably the key players in the EA market are the research organisations, which provide advice to participants, equivalent to consumer champions, such as&nbsp;<a href=\"https://www.which.co.uk/\"><u>Which?</u></a> in the UK.</p><p>What, if anything, about EA is new? Of course, markets are old, as are charities and events around charities. Perhaps what\u2019s new is that this is the first historical example of a marketplace for people explicitly committed to achieving the&nbsp;<i>maximum&nbsp;</i>impact for their altruistic resources, rather than just&nbsp;<i>some&nbsp;</i>impact: some combination of free market economics applied to philanthropy.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6br1qwhryzn\"><sup><a href=\"#fn6br1qwhryzn\">[1]</a></sup></span>&nbsp;I suspect that simply creating a market for maximum impact altruism is an extremely important - and so far unrecognised - achievement of the community.</p><p>Considering EA as like a farmers\u2019 market, you don\u2019t have to look hard to identify equivalent structures. In EA, the marketplace itself is run by various arms of the <a href=\"https://ev.org/\">Effective Ventures Foundation</a> (EVF, which formerly used to be called, somewhat confusingly, CEA). CEA runs the EAG and EAGx conferences (the market days), the EA forum (the market noticeboard), the EA handbook (the market catalogue) and the community builder grants (market outreach). 80,000 Hours is the main advertiser for the marketplace. EAFunds is a platform for buyers. Giving What We Can is a buyer\u2019s community. Longview is a club for large buyers that encourage particular purchases. And so on.</p><p>Whether or not you like the marketplace analogy, it\u2019s pretty clear EA operates on a \u2018hub-and-spoke\u2019 model. There are some common bits that are widely participated in - the conference, the forum, CEA\u2019s marketing, the social network - but otherwise there\u2019s quite a lot of separation, with participants congregating by cause areas.&nbsp;</p><h3>If we see the EA community as a market, what follows about how it would work?&nbsp;</h3><p>It seems apt to understand effective altruists as engaging in a certain kind of economic activity. Given that, but that we aren't used to think of EA this way, what can we say about how the EA market will function? We'll come back to questions of how it <i>should</i>, morally-speaking, function in a moment.</p><p>One main takeaway is that we should conceive of effective altruists as functioning like any other economic agents, just with a rather unusual set of preferences. We should expect them to pursue their own self-interest. We should not expect them to be moral saints, or to be immune to ordinary human flaws and vices; this might sound obvious, but I\u2019ve often got the impression that EAs think of themselves, and particularly their leaders, as fully selfless, perfectly rational, and so on: no, we\u2019re all just people, squishy bags of flesh attached to bones, evolved from apes and living on the surface of a rock flying through space.</p><p>Something that follows from this first observation is that EA is&nbsp;<i>not&nbsp;</i>a collective project. We are not like an army, working together for the same goal, or, as&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/z7quAxWyHuqFdxGE6/rowing-steering-anchoring-equity-mutiny-1\"><u>Holden Karnofsky has suggested</u></a>, the crew of a ship. People may want to \u2018do the most good\u2019, but have different conceptions of 'the good'. People with the&nbsp;<i>same&nbsp;</i>moral views might be analogous to the crew of a ship, but not the market as a whole. If you really want to keep the ship analogy, we're like multiple ships, with many people undecided on which ship to join or split between multiple ships.</p><p>In the market paradigm, the buyers and sellers don\u2019t have, or need to have, much in common. The butcher, baker, and candlestick-maker each want to maximise profits for themselves, rather than the collective. As Adam Smith put it, relating to conventional markets, \u201cIt is not from the benevolence (kindness) of the butcher, the brewer, or the baker that we expect our dinner, but from their regard to their own interest\u201d.</p><p>It can be rational for participants to press for their own interests at the cost of others. Participants can and will believe that their products are superior; they may well think others are snake oil salesmen. Although the buyers all seek 'the best value for their money', as they see it, they do not think of 'best value' the same way. Indeed, that\u2019s the point. There is a market because people have a diversity of preferences and they want a single place to go and shop - if you only want one item, there may be little point going to the market. Buyers and sellers with shared interests may group together to increase their negotiating power.</p><p>Perhaps the only thing the participants will, collectively, have in common is that they want there to be a market and, what\u2019s more, for it to be run freely and fairly. As a whole, they\u2019ll want to avoid corruption, nepotism, arbitrariness, dishonesty, etc. If you\u2019re an apple seller, you don\u2019t want your stall to be hidden at the back, whilst the other apple seller gets pride of place because he\u2019s friends with the guy that runs the market.&nbsp;</p><p>The shared aim of wanting a \u2018free and fair market\u2019 is fairly minimal, and desired only because it is in their individual interests. There isn\u2019t some further, substantial goal all the participants need to share; it would be surprising if there was one.</p><p>As a consequence of their different interests, it will be rational for individual participants in either market to bend or break the rules in their favour, establish favourable or monopoly positions for themselves, and so on, if they can. This problem doesn\u2019t go away in the EA marketplace. If anything, it\u2019s sharpened: rather than trying to get the best deal for yourself (as you see it), you aim to get the best for others (as you see it), so there some moral reason to bend the rules in your (moral view\u2019s) favour. Equally, participants may be annoyed if they see others abusing their power to promote their own interests.</p><h3>How, in broad terms, should the EA market be organised? As a free market</h3><p>I\u2019ll discuss some specific changes for EA in the next section.</p><p>If we ask \u201chow should a farmers\u2019 market operate?\u201d, an obvious, but not very informative answer, is \u201cto bring about the best outcomes\u201d. Fine, but how do we know what those are? Perhaps some social planner should decide what would be best for people, then force the market to produce those goods. That, indeed, is the approach preferred by socialists and, to a greater extent, communists.&nbsp;</p><p>The standard answer, the one out of vanilla economic theory -and the one I imagine most EAs would agree to generally - is that there should be a <a href=\"https://en.wikipedia.org/wiki/Free_market\">free market</a>. Individual consumers choose what they want, producers react to demand, and what is ultimately produced is decided by \u2018the market\u2019.&nbsp; The free market is the means to the end of producing the best outcomes.</p><p>Given that the EA community seems to be a market, like any other, the presumption is that it should be structured like any other free market unless there are sufficiently good reasons to deviate from this.&nbsp;</p><p>I don\u2019t want to get into the minutiae of exactly how markets should be run. It\u2019s not my speciality and I suppose that focusing on details will be distracting from the broad strokes.</p><p>I suspect some will object to this model: the purpose of EA is, in the end, to 'do the most good', not \u2018operate as a free market\u2019, so if we already know what the particular priorities are, shouldn\u2019t we just push people towards those? Here, we need to be careful to distinguish both means from ends and, also, the perspective of individuals from the collective.</p><p>This situation of individuals thinking \u2018I know what\u2019s best\u2019 is, of course, essential to the functioning of an ordinary free market: you want sellers to create and tout goods to customers, provide choice, compete over price, etc. because this allows consumers to get better deals.&nbsp;</p><p>However, we still want sellers to compete in the free market, and to resist attempts by sellers to distort the market in their favour. There\u2019s a reason governments have bodies that work to maintain competition and break up monopolies.</p><p>Given this, there\u2019s an important distinction between the mindset of individual participants and the organisers of the market. The participants are entitled, indeed encouraged, to advocate for what they think is best (within the rules, whatever they are). But the organisers of the market should maintain a level playing-field, remain studiously impartial between these and only intervene to correct market inefficiencies (I leave open how much correction is warranted). It's through competition that the buyers get the best deals. Ultimately, the question of \u201cwho\u2019s right?\u201d should be determined by the participants in the market, not its organisers. You get the best outcomes because, not in spite, of their being a level playing field. You want a free market as the means of achieving good outcomes - it's not an end in itself.</p><p>Although it\u2019s tempting for participants in the EA market to nudge the moral market in their favour, i.e. ensure more of their preferred goods are sold, doing so seems objectionably paternalistic. It combines arrogance - \u201cI know better than others\u201d - but also suggests insecurity - \u201cI do not believe my ideas would win in a free market, so I must intervene\u201d. If we think state interference, monopolies and so on, are bad in ordinary markets, we should object to them in high-impact philanthropic markets too.</p><p>Lurking in the background is a concept of fairness. We\u2019re talking about morality, so don\u2019t we need to decide on the correct theory of morality first to run the moral market? The obvious, if not perfectly simple, principle to appeal to here is Rawls\u2019 <a href=\"https://en.wikipedia.org/wiki/Original_position\">Veil of Ignorance</a>: how would you like the market to be run if you didn\u2019t know who you would be in it? You could be a buyer or seller, and you could have few or many resources.</p><p>Given this, we can distinguish between whether some change to the structure of the market is good or bad for an individual from whether it promotes or reduces fairness. I suspect we should praise those who strive to make the effective altruism market fair, especially when this is counter to their narrow interests.&nbsp;</p><p>The market itself will need to be organised by someone, and could well have various functions, such as advertising the market as a whole to potential customers, and deciding who gets which stalls. But it should not favour particular goods or services.</p><h3>To what extent is EA functioning differently from this right now?</h3><p>The effective altruism community is full of wonderful, smart, intelligent, thoughtful caring people - many of the best people I know. I find it a continued source of inspiration. My aim here, however, is to flag some ways in which effective altruism, as a system, could possibly work better. Although my intention is to highlight how things might be improved, I don\u2019t want this to detract from the much excellent work that people do. My focus is on the overall functioning of the \u2018market\u2019. I don\u2019t think I have much, if anything, to say about the behaviour of the small buyers and sellers. This is consistent with, indeed follows from, conceiving of it as a market and wanting it to run as a free-market. If you have a well-regulated market and lots of buyers and sellers, the result is a competitive market. Hence, you want to focus on removing the barrier to their being such a market.&nbsp;</p><p>I see a few related problem areas.</p><p>(1) there is a big overlap between the administrators and the participants in the market.</p><p>CEA/EVF is the hub that coordinates the wider movement, but it, and its trustees, are also participants who advocate for particular things. <a href=\"https://forum.effectivealtruism.org/posts/zzpwpDkQBTzxbjgJo/apply-or-nominate-someone-to-join-the-boards-of-effective\">Of the seven total trustees of the US and UK EVF boards</a>, six are current or former 'buyers' or 'sellers' in the market. As far as I can tell, Tasha McCauley is the only one not directly involved in another effective altruism organisation. Claire Zabel, Nick Beckstead, Eli Rose, Nicole Ross and Zachary Robinson are all current or former <a href=\"https://www.openphilanthropy.org/\">Open Philanthropy employees</a>. Nick Beckstead was the chair of the <a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1\">FTX Foundation</a>. The final trustee is Will MacAskill, who wears more hats in effective altruism than it is easy to list. Aside from being participants in the markets, two of them (Nick and Will) are also major public proponents of a particular market 'outcome', namely longtermism (see Nick's <a href=\"https://rucore.libraries.rutgers.edu/rutgers-lib/40469/PDF/1/play/\">thesis </a>and Will's<a href=\"https://en.wikipedia.org/wiki/What_We_Owe_the_Future\"> recent book</a>). This is a <i>bit </i>like having the farmers' market mostly run by the orange sellers' association - arguably non-ideal. CEA promotes a particularly longtermism agenda, as does 80k.&nbsp;</p><p>I would like to see CEA/EV acting as something like an impartial regulator, civil service, or public service broadcaster, for EA: it should try to impartially manage things, not take a stand. If you want to be outspoken, then that\u2019s fine, but you should probably let someone else take the reins of organising the market. As the Bible puts it, no man can serve two masters.&nbsp;</p><p>To be clear, I don't want to blame anyone for the status quo. A charitable and fairly plausible explanation is that the people most passionate about the market would also have views about what the priorities should be - the products it should produce. So the original participants became organisers. But as EA looks to the future, it seems preferable to move to a split. I imagine some organisers might welcome this change - it's awkward to act as something like a politician and a civil servant.&nbsp;</p><p>(2) the central market structure lacks accountability to or oversight by its participants.</p><p>The farmers' market would be accountable to the local government and ultimately, the electorate. Conspicuously, the Effective Venture Foundation, which contains a number of charities, including the Centre for Effective Altruism (CEA), is accountable only to its trustees - and perhaps, practically, its donors - and has no democratic elements. This seems non-ideal. CEA, as its name indicates, to some large extent represents the EA movement as a whole, yet there is no formal representation of the EA movement in its decision-making. As the American revolutionaries might have said in this context, \u201cno representation without representation\u201d (rather than \u201cno taxation without representation\u201d).</p><p>(3) there\u2019s only one major buyer.&nbsp;</p><p>EA is effectively a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Monopsony\"><i>monopsony</i></a><i>. </i>This is the opposite of a&nbsp;<i>monopoly</i>, that is, where there is only one seller. Open Philanthropy comprises something like 60% of the total purchasing power (from <a href=\"https://80000hours.org/2021/07/effective-altruism-growing/\">eyeballing Ben Todd's now out-of-date figures</a>). I don't find monopsonies as intuitive as monopolies, but my understanding is that, just like monopolies are bad if you're a buyer, monopsonies are bad if you're a seller. Because there is only one buyer, you get reduced competition and innovation: organisations aren't incentivised to produce goods for other buyers, but instead to focus on what the large buyer wants.&nbsp;</p><p>There's been discussion in EA about <a href=\"https://forum.effectivealtruism.org/posts/fzrsa6syCM3yWW7HA/announcing-the-awardees-for-open-philanthropy-s-usd150m\">regranting </a>from the major funders - Open Philanthropy and (formerly) FTX. I take this as recognition that there's an issue with funder concentration.</p><p>The combination of (1) - (3) means there\u2019s no/little effective external scrutiny of how EA is run. Who has the incentive, or power, to say \u201chey guys, is this a great idea?\u2019 who is not already an insider?</p><p>There may be lots of other issues too that someone more economically literate would spot.</p><h3>What should be done?</h3><p>The most obvious solution for (1) and (2) is to remove the overlap between participants and administrators. One way to do this is for CEA/EV to move to having a partially or fully elected set of trustees. These trustees should ideally not be both \u2018poachers\u2019 and \u2018gamekeepers\u2019. The central functions in effective altruism strive to be genuinely impartial.&nbsp;</p><p>What would the electorate be?&nbsp;<i>Contra&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/zuqpqqFoue5LyutTv/the-ea-community-does-not-own-its-donors-money\"><u>Dustin Moskovitz, aka \u2018The Sponsor\u2019</u></a> who suggests defining the EA community is a \u2018non-starter\u2019, there are quite a few options: Giving What We Can members; people who have previously attended EAGs; CEA could become a fee-paying society like many others. I\u2019m sure there are other options. Personally, I like the idea of GWWC members selecting the trustees of CEA: giving 10% is a costly signal and it gives people an incentive to take the pledge. I recognise it's too late to do this for the <a href=\"https://forum.effectivealtruism.org/posts/zzpwpDkQBTzxbjgJo/apply-or-nominate-someone-to-join-the-boards-of-effective\">new round of EV/CEA trustees that are being recruited</a>, so this is something to consider for the future.</p><p>CEA could be more accountable and allow for more \u2018voice\u2019 from the community, even without reforming its structures. One simple thing would be for the CEO(s) to have 'townhall' meeting at EAGs where people could (anonymously) ask them difficult questions.</p><p>I don\u2019t have an easy solution to (3). I think it\u2019s good there is one major buyer, rather than none, and I can\u2019t simply magic up another billionaire. That said, if CEA had democratic elements, it would then have to balance the desires of its major funder with that of the electorate, rather than being so concerned with the former, which seems like progress.</p><p>One suggestion for funder concentration that gets mooted is <a href=\"https://forum.effectivealtruism.org/posts/fzrsa6syCM3yWW7HA/announcing-the-awardees-for-open-philanthropy-s-usd150m\">regranting</a>, e.g. Open Phil gives money to a bunch of other people to spend on its behalf. This doesn't seem like a <i>very </i>promising solution and we can see why in light of the market analogy: it\u2019s basically like the rich local landowner giving a lot of money to his mates and telling them to buy things in the farmers\u2019 market. If they buy the same things, nothing changes. If they buy different things, he\u2019ll think it was a bad idea. Either way, it\u2019s only a temporary spike in demand. Perhaps the better solution, if the major donor(s) want the market to exist, is to financially support the existence of an impartial market it does not itself run (I recognise someone might argue this already happens, to some degree).&nbsp;</p><p>Possibly, if EA was run more conspicuously as an impartial market, it would attract and retain more large \u2018buyers\u2019.</p><p>Similarly, the econ-literate folk may spot obvious improvements that have not been obvious to me.&nbsp;</p><h3>How would all this have helped EA with its recent scandals?</h3><p>I hope that all of the above seemed sensible without appealing directly to SBF, sexual misdemeanours, or racism. But I do think we can see how it would help, to some extent, with those.</p><p>SBF. A question we might ask is: why was Sam Bankman-Fried a problem for the EA community? What does it have to do with the wider community that some guy who identified as EA, and was a donor, seems to have committed an enormous crime? I think we can pick this apart by imagining alternatives.</p><p>Suppose a big buyer disappears from the marketplace. This would be unfortunate, but there\u2019s no scandal.</p><p>Suppose there's criminality on the part of some people in the marketplace. This raises general issues of community trust, as well as specifically of policing, but it wouldn\u2019t obviously rock things to the core. And we should expect it to happen. People are still people, even if they participate in the maximum altruism market.</p><p>Suppose a major buyer collapses, they are engaged in criminal activity, but their staff are also deeply involved in running the market. This is much more serious because it goes right to the heart of the market. It raises questions about whether the core staff were sufficiently attentive to the health and good running of the market as a whole. One perspective on why SBF-gate shocked EA to its very core was that the people working with SBF at his fund were the same key EAs who run the movement/market, and are key participants in it, eg Will MacAskill and Nick Beckstead.&nbsp;</p><p>What if EA were organised along the lines suggested above, with a democratically infused CEA that was not a participant in the market?</p><p>For one, it would have&nbsp;<strong>contained&nbsp;</strong>the problem. Imagine FTX had collapsed, but its staff were just \u2018ordinary\u2019 EAs and not involved in running anything else. This would have been a much smaller problem, it not clearly so damning for EA as a whole.</p><p>For another, democratic elements would have improved&nbsp;<strong>scrutiny</strong>. With EA run as a very tight group, there was no one who had the incentives, or the power, to ask probing questions like \u201chey, is this guy legit? Should we really take his money?\u201d I would absolutely have expected someone to ask those questions about SBF at anonymous townhalls, at which point the CEO of CEA would have to at least think about it.&nbsp;</p><p>What about Nick Bostrom and his <a href=\"https://forum.effectivealtruism.org/posts/8zLwD862MRGZTzs8k/a-personal-response-to-nick-bostrom-s-apology-for-an-old\">\u2018old email\u2019 case</a>? This struck me as a relatively contained scandal exactly because Nick \u2018just\u2019 runs a \u2018spoke\u2019 in EA, the Future of Humanity Institute, but he doesn\u2019t (also) have a role in the hub. If I'm an apple-seller, I don't think it has <i>much </i>to do with me if the chief cheese dealer has unpleasant views.</p><p>Finally, let\u2019s turn to the <a href=\"https://time.com/6252617/effective-altruism-sexual-harassment/\">scandals about sexual behaviour</a> and consider the case of <a href=\"https://forum.effectivealtruism.org/posts/QMee23Evryqzthcvn/a-statement-and-an-apology\">Owen Cotton-Barratt, the trustee of CEA who recently resigned</a>. A couple of things stick out.&nbsp;</p><p>One is that it's difficult to police the behaviour of your boss, which is what the community health team had to do. Saliently, the community health knew about the incidents, but didn't raise them to the whole board [Chana Messinger commented on a draft on this that the health team \"did bring this to a UK board member\" but not the whole board]. What would have happened if the trustees were elected - or at least there was greater scrutiny of CEA? Probably, the community health team would have been more accountable to the wider community and would have felt obliged to tell some or all of board who would, in turn, have felt obliged to tell the rest of the board and then act. At that point, the trustee would have made a public statement and/or resigned. They could still have stood for re-election, meaning the community could decide if the matters were sufficiently serious to merit not being re-elected. At least, it seems far less likely this story would have emerged, years later, in the international press, with an open question over whether the community health team had prioritised its boss over the community.&nbsp;</p><p>The other is that Owen was, in addition to being a trustee of CEA, also a leading market participant in <a href=\"https://www.fhi.ox.ac.uk/team/owen-cotton-barratt/\">his role</a> at the Future of Humanity Institute. As well as being non-ideal in itself, this arguably gave him additional influence that made it harder for the community health team to act.</p><p>Regarding the <a href=\"https://time.com/6252617/effective-altruism-sexual-harassment/\">wider reports</a> of EA community members being involved in sexual harassment, the complaint about the status quo is that there is a close-knit group of men who control the jobs and so speaking out is dangerous. But, again, if CEA were accountable to the community and separate from other organisations, it makes it much easier for people to raise issues with CEA, and for CEA to be in a stronger position to, say, warn or ban offenders of events.&nbsp;</p><h3>Closing thoughts</h3><p>The effective altruism community began with a bunch of smart, idealistic, inexperienced young people about 10 years ago. It has become radically, unexpectedly, successful and then exploded into the public consciousness. Its original institutions were designed pretty much as you might expect a friendly, close-knit group to do, and they worked well when EA was small. But it's run into what we might politely call 'growing pains'. Now EA is big, we need - reluctantly - to grow up and develop new institutions that are fit for purpose.&nbsp;</p><p>I think it\u2019s possible to conceive of a new, better EA world emerging out of the SBF, and other crises, one set up well for its next stage of growth. To me, that looks like an impartial market centre - or hub - that promotes the general ideas of EA, maintains a level playing field, and keeps the community healthy. And it involves the \u2018spokes\u2019, the participants, developing and sharing their ideas for how to do the most good. What's more, it certainly helps me, and I think it might help others, to see their involvement in effective altruism as participating in a market, rather than as an all-encompassing part of their identity.</p><p>I also suspect that, if EA is going to survive its many crises and retain some credibility, it needs to make some changes. The attitude I\u2019ve observed from some leading EA is, effectively, \u201cSBF was a bad guy, we were unlucky, things are basically fine. Let\u2019s wait for this to blow over\u201d. I find that disappointing. As Winston Churchill once said, \"never let a good crisis go to waste\". I fear effective altruists are, collectively, letting this crisis go to waste.&nbsp;</p><p>I\u2019ve sketched some basic ideas and I\u2019m curious to hear what people think about them. I\u2019ve deliberately not got into the details of what a free market for maximum altruism would look like, both because that\u2019s not my area and because I want people to focus on the big picture. But I would welcome economists and others to launch themselves at this task in due course.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6br1qwhryzn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6br1qwhryzn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This might explain why effective altruism is treated with such suspicion by those who are sceptical of capitalism already.</p></div></li></ol>", "user": {"username": "MichaelPlant"}}, {"_id": "jYSEjBsWbjNqioRZJ", "title": "The Rethink Priorities Existential Security Team's Strategy for 2023", "postedAt": "2023-05-08T08:08:26.421Z", "htmlBody": "<p>Update on 24th of July 2023: See a new page on our work helping launch new projects tackling existential risk <a href=\"https://rethinkpriorities.org/xst-incubation\">here.</a></p><p>Update on 12th of June 2023:</p><p>Since I wrote this post we have made some changes to our strategy following some reflection during our team retreat last week. The major changes are</p><ul><li>I, Jam, Marie, and Renan will focus exclusively on setting up the longtermist incubator. This means two changes:<ul><li>We are dropping our plans to work on strategic clarity research later in the year.</li><li>We'll drop the \"flexible time for high-impact opportunities\" for now - we'll still be open to significantly pivoting based on changing circumstances and new information, but the bar is now higher and this is less of an explicit feature of our plans.</li></ul></li><li>A major (but not the only) motivation for the above two changes is that focusing solely on the longtermist incubator seems good - the plan for that program is very ambitious and we are a relatively small team.</li><li>Our work helping launch longtermist projects will focus only on&nbsp;<i>AI-related</i> projects in 2023. We're making this change because the majority of the projects we've found to be most promising so far are AI-related, and this narrower scope allows us to build deeper expertise and stronger networks relevant to the projects we'll help launch.</li><li>Note that these changes don't affect Linch's work - Linch continues to work on his own strategic clarity-related research agenda.</li></ul><p>At the time of writing this post is only a month old, so why are we making relatively significant changes already? As mentioned in the original post, the strategy described in the post reflects plans that we started to draft in February and that we started to execute in March, so in reality we've been executing and gathering data for 2-3 months since forming our initial plans. We're also to some extent reacting to &nbsp;external circumstances, especially the apparent increase in the tractability of some AI-related projects. Finally, we're entering fairly new territory for the team and for longtermist-EA-motivated efforts in general, so we might expect our strategy to evolve fairly quickly.&nbsp;</p><h2>Summary</h2><p>This post contains a moderately rough, high-level description of the Rethink Priorities Existential Security team\u2019s (XST\u2019s) strategy for the period April-October 2023.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxvlwa3bws4a\"><sup><a href=\"#fnxvlwa3bws4a\">[1]</a></sup></span></p><ul><li>XST is a team of researchers focused on improving the world according to a longtermist outlook through research and other projects, and is part of&nbsp;<a href=\"https://rethinkpriorities.org/\"><u>Rethink Priorities</u></a> (RP).</li><li>Note that until very recently we were called the General Longtermism team (GLT). We have now renamed ourselves the Existential Security team (XST), which is slightly more descriptive and more closely reflects our focus on reducing existential risk.</li><li>XST\u2019s three focus areas for 2023 will be:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn1aw09hlmuh\"><sup><a href=\"#fnn1aw09hlmuh\">[2]</a></sup></span><ul><li><a href=\"https://forum.effectivealtruism.org/posts/jYSEjBsWbjNqioRZJ/the-rethink-priorities-existential-security-team-s-strategy#Longtermist_entrepreneurship_program\"><strong><u>Longtermist entrepreneurship</u></strong></a><strong> (65%)</strong>: Making highly impactful longtermist projects happen by finding and developing ideas for highly promising longtermist projects, identifying potential founders, and supporting them as they get these projects started. Our main activities will be:<ul><li><a href=\"https://forum.effectivealtruism.org/posts/jYSEjBsWbjNqioRZJ/the-rethink-priorities-existential-security-team-s-strategy#Project_research\"><u>Identifying and detailing the most promising ideas for longtermist projects</u></a>, with a goal of having ~5 detailed project ideas by the end of June, that we can bring to a potential meeting of talented entrepreneurs in July/August, organized by&nbsp;<a href=\"https://www.linkedin.com/in/mikemccormick17/\"><u>Mike McCormick</u></a>.</li><li>A relatively brief&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jYSEjBsWbjNqioRZJ/the-rethink-priorities-existential-security-team-s-strategy#Founder_first\"><u>founder-first-style</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jYSEjBsWbjNqioRZJ/the-rethink-priorities-existential-security-team-s-strategy#Founder_search\"><u>founder search</u></a> (looking for highly promising founders and finding projects that they are an especially good fit for).</li><li>Exploring&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jYSEjBsWbjNqioRZJ/the-rethink-priorities-existential-security-team-s-strategy#Founders_in_residence\"><u>founder-in-residence</u></a> MVPs (hiring potential founders and giving them space to develop their own ideas for promising projects).</li><li><a href=\"https://forum.effectivealtruism.org/posts/jYSEjBsWbjNqioRZJ/the-rethink-priorities-existential-security-team-s-strategy#Founder_support\"><u>Supporting founders</u></a> once they\u2019re identified.</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/jYSEjBsWbjNqioRZJ/the-rethink-priorities-existential-security-team-s-strategy#Strategic_clarity_research\"><strong><u>Strategic clarity research</u></strong></a><strong> (25%)</strong>: Research that helps shed light on high-level strategic questions relevant for the EA community and for people working on reducing existential risk. This year, we plan to focus on high-level EA movement-building strategy questions (such as \u201cWhat kind of EA movement do we want?\u201d or \u201cWhat\u2019s the optimal portfolio among priority cause areas we should aim at building?\u201d), and possibly on high-level questions that seem important for assessing whether and how to help launch entrepreneurial projects. Most of our work on this will happen in the second half of the year.</li><li><a href=\"https://forum.effectivealtruism.org/posts/jYSEjBsWbjNqioRZJ/the-rethink-priorities-existential-security-team-s-strategy#Flexible_time_for_high_impact_opportunities\"><strong><u>Flexible time for high-impact opportunities</u></strong></a><strong> (10%): Time allocated for i) team members working on projects that they are very keen on and ii) highly impactful and time-sensitive projects that arise due to changes in external circumstances.</strong></li></ul></li><li>Concrete outputs we\u2019ll aim for:<ul><li><strong>5 project idea memos&nbsp;</strong>by the end of June that are of a standard equal to or better than&nbsp;<a href=\"https://forum.effectivealtruism.org/s/Cubdp2SSdyMB8zJ5e/p/tD2rXd9vXmTkRBwHN\"><u>the 2023 Q1 megaproject speedruns</u></a> that we posted on the EA Forum in February.</li><li><strong>1 strategic clarity research output&nbsp;</strong>by the end of October.</li><li><strong>1 new promising project launched&nbsp;</strong>by the end of October.</li><li><strong>11 publicly shared research or project idea outputs&nbsp;</strong>by the end of the year.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvt14e6ijtr\"><sup><a href=\"#fnvt14e6ijtr\">[3]</a></sup></span></li></ul></li><li>From mid-May onwards, we\u2019re planning to have&nbsp;<strong>4 FTE executing this strategy:&nbsp;</strong>me (Ben), Marie, Jam, and Renan.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjc96pp6o2kg\"><sup><a href=\"#fnjc96pp6o2kg\">[4]</a></sup></span>&nbsp;Linch is pursuing a separate research agenda related to longtermist strategic clarity.</li><li>The high-level timeline is:<ul><li>March: The team winds down current projects and begins work on executing the team strategy from the start of April.</li><li>April-July: The team focuses on the entrepreneurship program, and works on founder-first activities, founder support, and project research. The project research is focused on generating a new prioritization model and shallow project ranking by the end of April, and 5 project ideas memos by the end of June for a potential meeting of promising entrepreneurs in July/August.</li><li>August-October: Jam and Ben continue working on the entrepreneurship program, while Marie and Renan switch to strategic clarity research.&nbsp;</li><li>Start of November: We have a checkpoint to review how things have gone and decide how to continue.</li></ul></li></ul><h3>Summary of Rethink Priorities jargon</h3><ul><li>RP = \u201cRethink Priorities\u201d</li><li>XST = \u201cThe [Rethink Priorities] Existential Security team\u201d</li><li>SP = \u201cThe [Rethink Priorities] Special Projects team\u201d</li><li>AIGS = \u201cThe [Rethink Priorities] AI Governance &amp; Strategy team\u201d</li><li>The LT department = \u201cThe [Rethink Priorities] Longtermism department\u201d</li></ul><h2>Preamble</h2><p>From roughly November last year up to March this year, the main focus for the team was revisiting our strategy and making plans for 2023 (though XST team members also continued with various ongoing individual projects). This post is based on an internal strategic planning document that gradually evolved as a result of this thinking, in the period February - April 2023.</p><p>I expect this post will be of most interest to people who are interested in following what the team is up to, including people who are potentially interested in joining the team, collaborating with us, or funding us.&nbsp;</p><p>If you are interested in any of the above things, some information that might be helpful:</p><ul><li>We\u2019re tentatively planning to open a hiring round, most likely in Q3 this year, where we\u2019re hoping to significantly expand the team. You can register your interest in joining the team in our team\u2019s&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/postings/dfe9e137-a54a-428b-9034-791ddaa73f67\"><u>expression of interest form</u></a>, and we\u2019ll notify you when our hiring round opens (the Rethink Priorities&nbsp;<a href=\"https://rethinkpriorities.org/newsletter\"><u>newsletter</u></a> will also notify subscribers when our hiring round opens).</li><li>We\u2019re keen to hear from potential founders, advisors, or other collaborators. Please contact jam at rethinkpriorities dot org.</li><li>Feel free to also pitch us projects by emailing jam at rethinkpriorities dot org - but note that by default we're focusing on only a few highly promising projects and are mostly not doing light-touch support / advice for other projects.</li><li>We\u2019re also keen to hear from potential funders \u2013 <strong>our main bottleneck for hiring currently is funding</strong>. We hope that we will be able to raise funds when we begin fundraising in earnest in the coming months but this is far from guaranteed and will likely take time. If you\u2019re interested in funding our work, please email janique at rethinkpriorities dot org.</li><li>We\u2019ll generally continue to share updates in the Rethink Priorities newsletter \u2013 you can sign up&nbsp;<a href=\"https://rethinkpriorities.org/newsletter\"><u>here</u></a>.</li></ul><p>Note that this post is fairly rough \u2013 I could have spent significantly more time polishing it, but other things felt higher priority, and it seems better to put it online in its current state than to keep it private. One particular issue is that the document the post is based on has been gradually developing since February, and in the meantime, we\u2019ve started executing the strategy and naturally our plans have evolved a little \u2013 so some of the information might be a little out of date.</p><p>I (Ben Snodin) am the main author of this post, although the text is based on earlier drafts written by other members of XST, the strategy was decided in a fairly collaborative way within XST, and we\u2019ve had lots of input from others inside and outside Rethink Priorities.</p><h2>Introduction</h2><p>XST was created in early 2022 under the leadership of Linch Zhang, sitting in the Longtermism department (LT department) alongside the AI Governance &amp; Strategy team (AIGS). XST doesn\u2019t yet have a strong collective view on which priorities within longtermism are most important on the margin, so we lean towards operating a longtermist portfolio that looks good from several plausible longtermist outlooks \u2013 though in practice we are always focused on reducing existential risk from one source or another.</p><p>From within this broad longtermist outlook, we also have a much broader remit compared to the rest of Rethink Priorities: XST is much more open than other teams and departments to producing endpoints that are not research outputs.</p><p>In 2022, a major focus was identifying and helping create longtermist megaprojects \u2013 highly scalable projects that have large inputs (financial and human capital) and outputs (existential risk reduction). See&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/C26RHHYXzT6P6A4ht/what-rethink-priorities-general-longtermism-team-did-in-2022\"><u>here</u></a> for a summary of all our activities in 2022.</p><p>Due to the changes in the post-FTX funding landscape, we have decided to move away from the focus on megaprojects. In 2023,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0lpzs3gwxh1\"><sup><a href=\"#fn0lpzs3gwxh1\">[5]</a></sup></span>&nbsp;we\u2019ll have three main focus areas, in order of priority:</p><ol><li>A longtermist entrepreneurship program</li><li>Strategic clarity research</li><li>A small fraction of time reserved for other high-impact opportunities yet to be determined</li></ol><p>The overarching goal with those focus areas will be to experiment and gain information about which potential activities are most impactful for the longtermist community considering the new landscape and what our comparative advantage is as a team.&nbsp;</p><p>In 2022, XST was maximally ambitious and very keen to scale a lot. In 2023 we expect to scale more slowly.</p><h2>Longtermist entrepreneurship program</h2><p><strong>We\u2019ll plan to spend 65% of our time on our longtermist entrepreneurship program this year.</strong> Concretely, we\u2019ll develop detailed proposals for projects we think are most promising in terms of longtermist impact, and identify and support potential founders who might set up those or other longtermist projects.</p><p>Supporting longtermist entrepreneurship was the area we were most excited about after some strategic deep dives in early 2023 because it:</p><ul><li>Has significant impact potential</li><li>Has been very neglected in the EA community</li><li>Is a potential comparative advantage of RP due to our historically strong operations team, and</li><li>Makes the Rethink Priorities \u201clongtermist portfolio\u201d significantly more diverse given the existing work from the AIGS team</li></ul><p>Supporting longtermist entrepreneurship can be seen as a successor to the work the team did in 2022 on supporting potential megaprojects. Even though we originally focused on megaprojects, most of the ideas we found to be most promising were not really \u201cmega\u201d: some of them were highly scalable, but most seemed potentially high impact even at a small scale.&nbsp;</p><p>Even post-FTX, supporting longtermist entrepreneurship has consistently been each XST team member\u2019s top choice of strategic direction, despite quite a thorough three-month exploration of other options for XST strategy.</p><h3>Three components of the longtermist entrepreneurship program</h3><p>We think of support for longtermist entrepreneurship as being made up of three components:</p><ol><li><strong>Project research</strong>: i) identifying the most promising project ideas, ii) turning those ideas into detailed, actionable plans for potential founders</li><li><strong>Founder search</strong>: finding excellent founders and matching them with project ideas \u2013 either matching an exceptional founder with a good idea (\u201cfounder-first\u201d) or a good founder with an exceptional idea (\u201cproject-first\u201d). This may also involve cofounder matching</li><li><strong>Founder support</strong>: supporting founders while they spin up projects and, potentially, for some time after that</li></ol><p>Our comparative advantage is likely to be project research (although RP has a comparative advantage with at least part of founder support thanks to the Rethink Priorities Special Projects team (SP)). We\u2019ll be particularly keen to seek advice and partnerships with people who can help with founder search and founder support.</p><p>The next three sections give a bit more detail on each of these components.</p><p><strong>Project research</strong></p><p>A core component of our longtermist entrepreneurship program will be project research: generating detailed reports and plans for highly promising project ideas. Project research can be broken down into i)&nbsp;<strong>prioritization research</strong> and ii)&nbsp;<strong>detailed research into project ideas</strong>.</p><p>For our prioritization research for megaprojects in 2022, we put together an internal database of project ideas, developed a simple weighted factor model based on five criteria (existential risk reduction, cost, tractability, downside risk, and RP\u2019s comparative advantage), and established buckets of projects based on their average rank and the variance between the ranks given by different scorers. Most of that work was not public, but we published a&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1VLEiWx0_sFwjhCMao3vT_QxdWCufuq71XqaL2joVj8Y/edit#gid=1763987441\"><u>rough list of project ideas</u></a> with just the projects\u2019 titles and some links to more information.</p><p>For our prioritization research in 2023, we\u2019ll:</p><ol><li>Collect project ideas from the longtermist community and come up with our own ideas</li><li>Create an improved version of the weighted factor model we used in 2022</li><li>Rank the project ideas using the weighted factor model</li><li>Use that ranking to decide which ideas to investigate in more detail</li></ol><p>We\u2019ll then investigate the most promising project ideas in more detail with two aims:</p><ol><li>Getting a clearer sense of their potential value, including checking for potential show-stoppers and severe downside risks. We expect that in some cases, detailed investigation will lead us to abandon the idea because of, e.g., downside risks.</li><li>Creating a plan for making the idea happen that is detailed enough to show to potential founders and to give founders a good starting point for initiating the project</li></ol><p><a href=\"https://forum.effectivealtruism.org/s/Cubdp2SSdyMB8zJ5e/p/tD2rXd9vXmTkRBwHN\"><u>The speedrun series we published earlier this year</u></a> shows some relatively shallow investigations of promising projects.</p><p><strong>Founder search</strong></p><p>In order to help launch projects, we need to somehow come into contact with founders who we can provide with project ideas and support in other ways.</p><p>Some avenues for founder search will be:</p><ul><li>Using our networks<ul><li>A special case of this is a group of highly competent potential founders that Mike McCormick is coordinating and providing support for via his \u201c40k\u201d project</li></ul></li><li>Using existing and future public expression of interest forms, posts on the EA Forum, etc.</li><li>Possibly, creating programs or job openings at RP for potential founders to apply to</li></ul><p><strong>Founder support</strong></p><p>Founder support refers to essentially everything we will do to support founders other than providing them with project ideas.&nbsp;</p><p>Support for new founders/projects will happen later in the process than project research and founder search, and it\u2019s less clear what we\u2019ll want to do in this area than it is for the other two areas. However, possible support could include:</p><ul><li>Cofounder matching</li><li>Advice on initial fundraising</li><li>Early input on project strategy</li><li>Other advice/mentorship</li><li>Research support (e.g., providing founders with RA time)</li><li>Employment with RP (for the \u201cfounders in residence\u201d experiment, see next section)</li><li>Operational support provided by SP</li></ul><h3>Three experiments we\u2019ll run in 2023</h3><p>There are three main ways we will experiment with longtermist entrepreneurship support in 2023:</p><ol><li><strong>\u201cProject first,\u201d including via Mike McCormick\u2019s group</strong>: i) generate five detailed project ideas by the end of June 2023, take them to a meeting of Mike McCormick\u2019s group of potential founders, find potential founders through that meeting, and then support those founders. ii) also take our top project ideas to other potential founders who we identify as potentially being a good fit</li><li><strong>\u201cFounder first\u201d</strong>: spend a bit of time looking for exceptional founders who we then find projects for</li><li><strong>Founders in residence</strong>: use our networks to look for candidates for hiring 1-2 founders in residence (in some form) on contracts 3-12 months long</li></ol><p>See the next three sections for a bit more detail on each of these experiments.</p><p><strong>Project first</strong></p><p><a href=\"https://www.linkedin.com/in/mikemccormick17/\"><u>Mike McCormick</u></a> has assembled a group that includes highly competent entrepreneurs and executives who are interested in longtermist entrepreneurship, with a goal of helping these people start impactful longtermist projects. We understand that one key bottleneck for him and his group is having excellent, detailed project ideas for founders to run with. Since generating these detailed project ideas seems to be a relative strength of ours, working closely with Mike could be extremely productive.</p><p>Mike is tentatively planning to hold a meeting of a group of entrepreneurs in Q3 2023, and this seems like an excellent opportunity for XST team members to present a set of detailed project ideas. In addition to possibly directly leading to founders working on these ideas, this could be a great way to establish links between XST team members and potential founders, for example, helping with future cofounder matching, understanding the bottlenecks founders face, finding advisors for the longtermist entrepreneurship program, and finding founders for future project ideas.</p><p>With this in mind,&nbsp;<strong>we have an internal team deadline to generate five detailed plans for highly promising projects by June 30, 2023</strong>. Setting ourselves this deadline now means that</p><ul><li>We will have a healthy amount of pressure to focus on a key goal for 2023, namely creating detailed plans for highly promising project ideas</li><li>If/when Mike\u2019s 2023 entrepreneur meeting happens, we\u2019ll be well positioned to make the most of the event and gain the benefits mentioned above</li><li>We\u2019ll have detailed plans to share with founders we find through other founder search activities</li></ul><p>Aside from interactions with Mike McCormick\u2019s group of entrepreneurs, we\u2019ll also plan to connect with potential founders who might be a good fit for the projects we\u2019ve identified as most promising. This seems more likely to happen from July onwards rather than before July, since i) by July we plan to have some detailed plans for highly promising projects, and ii) in July we plan to de-emphasize project research which gives us more capacity for founder search.</p><p><strong>Founder first</strong></p><p>The \u201cfounder-first approach\u201d refers to identifying unusually talented founders and trying to pair them with high-impact projects, rather than identifying high-impact projects and trying to directly recruit a founder for that specific project (the \u201cproject-first approach\u201d described in the previous section). The key idea here with the founder-first approach is that we launch projects that are potentially less promising in the abstract because the founder fit is unusually good.</p><p>An important motivator for this is the thought that projects will be far more successful if we find an unusually good match between the founder and a project. Plausibly, this is as large of a driver of eventual impact as the choice of project itself (since projects that are much higher impact in the abstract may accomplish nothing or be net negative due to poor execution).</p><p><strong>Founders in residence</strong></p><p>\u201cFounders in residence\u201d refers to an approach where we give people contracts with RP, allow them to explore potential project ideas, and eventually spin up a highly promising project.</p><p>Some advantages of this approach are</p><ul><li>It offers job security and stability to potential founders, making the transition to longtermist entrepreneurship lower risk and removing a key barrier for some potential founders.</li><li>It provides founders the opportunity to gain a deep understanding of the relevant space and to feel that they have ownership over the project they eventually choose to pursue.</li><li>The founders in residence can potentially advise us while we carry out the other two experiments, if they have relevant expertise that we are missing.</li></ul><p>However, the approach has some disadvantages:</p><ul><li>It might attract people who are strong on research but weak on entrepreneurship, and these people might be less effective at eventually creating highly impactful organizations than other groups we might try to engage with.</li><li>It requires us to put more trust into potential founders since they are given the RP brand and allowed the freedom to pursue ideas that they consider to be worthwhile.</li><li>It has a larger financial cost: unlike with other approaches, we need to pay the founder a salary and pay other employment costs.</li><li>It poses a higher risk to RP: perfect vetting isn\u2019t possible and employing someone always comes with risks, e.g., they could turn out to exhibit harmful behaviors that damage organizational culture.</li><li>It poses other costs for RP such as operational costs (even before any new project has been created).</li></ul><p>Reducing the level of pay and the length of the employment contract would make the disadvantages less severe but would also potentially reduce the strength of the advantages.</p><p>While we are wary of spreading ourselves too thin in 2023 by trying many different approaches, it seems worth doing relatively low-cost experiments in this area. With this in mind,&nbsp;<strong>we tentatively plan to look for people who might be a good fit for founder-in-residence roles and facilitate agreements for 1-2 people to join RP on 3-12 month contracts within the next six months</strong>.</p><h3>Getting external advice</h3><p>Given our lack of experience with launching entrepreneurial projects, a key determinant of our success at supporting longtermist entrepreneurship this year will be the quality of external advice we receive from relevant experts. Because of this, we will expend a lot of early effort on establishing relationships with relevant experts.</p><h2>Strategic clarity research</h2><p>In addition to our longtermist entrepreneurship program, we will also engage in strategic clarity research in 2023.</p><p>We use the term \u201cstrategic clarity research\u201d to refer to work that helps shed light on high-level strategic questions relevant for the EA community and for people working on reducing existential risk. The \u201ccustomers\u201d for this work could be</p><ul><li>The EA community and people working on reducing existential risk in general.</li><li>Particular people at particular orgs who want clarity on particular questions (what Rethink Priorities has referred to as the \u201c<a href=\"https://forum.effectivealtruism.org/posts/Liphmkodcu7XPDKfK/rethink-priorities-2022-impact-2023-strategy-and-funding-1\">consulting model</a>\u201d).</li><li>Ourselves, particularly helping resolve uncertainties that arise when considering whether and how to best launch potential longtermist entrepreneurial projects.</li></ul><p>The motivation for work in this area is</p><ul><li>This kind of thinking seems undersupplied in the EA community currently.</li><li>There are particular instances of EA decision-makers wanting clarity on certain relevant questions.</li><li>This work could eventually (on the timescale of six months or more) help with generating and prioritizing entrepreneurial projects we might want to help launch.</li></ul><p>One concern with entering this area is that it might be easy to do a poor job and not make much progress, especially if the work is being done by a relatively junior team like XST.</p><p><strong>Our initial focus for this work will be EA movement-building research.</strong> This seems like a good starting point because i) EA movement-building research seems like an important area that is especially relevant currently, ii) people seemed enthusiastic about us doing work in this area earlier this year at the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/WLZabqQGCd2joZpxR/summit-on-existential-security-2023\"><u>Summit on Existential Security</u></a> and EAG SF,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7khhyhrhe1c\"><sup><a href=\"#fn7khhyhrhe1c\">[6]</a></sup></span>&nbsp;and iii) XST has some movement-building experience and continuing contact with movement-building projects, e.g., through <a href=\"https://condor.camp/en/\">Condor Camp</a>.</p><p>However, as time goes on we will be open to investigating topics in other areas, especially topics that come up as we do research into potential longtermist entrepreneurial projects.</p><p>Topics for EA movement-building research could include</p><ul><li>What we\u2019ve concretely achieved with past EA movement building, whether and to what extent this differs by approach, and what the relevant downside risks have been.</li><li>How should we approach movement building/field building?</li><li>What kind of EA movement do we want?</li><li>What\u2019s the optimal portfolio among priority cause areas we should aim at building?</li><li>How cautious should we be with growing or slowing down the movement?</li></ul><p>Note that&nbsp;<strong>we will hold off on strategic clarity research for Q2 2023 while we focus on the longtermist entrepreneurship program, and start with strategic clarity research in Q3 2023. We\u2019ll plan to spend 25% of our time on strategic clarity research this year.</strong></p><h2>Flexible time for high-impact opportunities</h2><p><strong>We will reserve 10% of our time for currently-unspecified projects that seem impactful at particular points this year</strong>. This time is available i) for team members to work on projects that they are very keen on and ii) for highly impactful and time-sensitive projects that arise due to changes in external circumstances.</p><p>Motivations:</p><ul><li>Allow team members to work on projects that they personally find motivating or personally consider especially impactful. (Note that this is alongside RP\u2019s 10% professional development time.)</li><li>Allow the team to take on potential projects that seem highly impactful and time-sensitive due to external events (\"rapid response\"). As a generalist team with a good funding buffer, we might be especially well-placed to make these kinds of pivots.</li></ul><p>A lot of&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/C26RHHYXzT6P6A4ht/what-rethink-priorities-general-longtermism-team-did-in-2022\"><u>XST\u2019s work in 2022</u></a> could be considered to be in this category. For example:</p><ul><li>Investigating specific biosecurity interventions in partnership with trusted actors in the field (e.g.,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/T4L7aRkjqaLrQeyyn/air-safety-to-combat-global-catastrophic-biorisks\"><u>indoor air quality</u></a>)</li><li>Diving deeper into&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/oqBJk2Ae3RBegtFfn/my-thoughts-on-nanotechnology-strategy-research-as-an-ea\"><u>nanotechnology as an existential risk</u></a></li><li>Co-founding and leading&nbsp;<a href=\"https://condor.camp/en/\"><u>Condor Camp</u></a>, a longtermist talent search project in a country where there hadn\u2019t previously been much longtermist talent search</li><li>Active grantmaking to support projects related to XST\u2019s priorities (e.g.,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BtqhvcawMdrYJcKgK/help-us-make-civilizational-refuges-happen\"><u>civilizational shelters</u></a>)</li></ul><p>Another relevant example from outside of XST is that RP temporarily pivoted ~50% of its research staff to address questions related to the COVID-19 pandemic in March 2020.</p><h2>Long-term visions</h2><p>I think having long-term visions for what we want to achieve with our strategic priorities is extremely helpful. First, these visions can help provide direction and keep us on a productive path when we make updates to our strategy as time goes on, and second, working towards ambitious visions can be very motivating.</p><p>Visions for longtermist entrepreneurship:</p><ul><li>There is a thriving longtermist entrepreneurship ecosystem, with several conferences per year, active online networks, and a welcoming and appealing environment for highly competent people who are curious about longtermist entrepreneurship.</li><li>Longtermist entrepreneurship is a lower-risk career path, with excellent support and guidance.</li><li>XST is a base of excellence for longtermist entrepreneurship support, having produced many highly-regarded longtermist projects and organizations, and with an established and well-regarded entrepreneurship program that attracts top longtermist entrepreneurial talent and that others seek to copy.</li><li>The main bottleneck for longtermist entrepreneurship is funding, thanks to the abundance of excellent, detailed project proposals, highly talented longtermist entrepreneurs, and effective structures to combine those two things to make highly impactful longtermist projects.</li></ul><p>Visions for strategic clarity research:</p><ul><li>The LT department (or RP more broadly) is a base of excellence for independent (i.e., from other EA orgs) thinking on key worldview/strategic clarity questions.</li><li>XST has nurtured and developed leading thinkers in high-level longtermist strategy.</li><li>XST research outputs have significantly influenced longtermist EA thinking on key strategic questions.</li></ul><p>Visions for flexible time/rapid response:</p><ul><li>The LT department is exceptionally well-placed to perform effective rapid response, thanks to a semi-formal group of researchers/staff who are prepared for rapid response work (including research and ops), have relevant skills and training, and have significant financial resources that can be quickly and flexibly deployed.</li><li>The LT department contains a team of highly talented, creative thinkers who are given the freedom to pursue the high-risk, high-reward research projects that they find most promising (maybe similar to places like Bell Labs).</li></ul><h2>Acknowledgments</h2><figure class=\"image image_resized\" style=\"width:37.11%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/jYSEjBsWbjNqioRZJ/clei4bezhusjjx1cuxz7\"></figure><p><i>This post is a project of</i><a href=\"http://rethinkpriorities.org/\"><i>&nbsp;<u>Rethink Priorities</u></i></a><i>. It was written by Ben Snodin. Thanks to Peter Wildeford, Marie Davidsen Buhl, Renan Araujo, Jam Kraprayoon, and members of the Rethink Priorities AIGS team, as well as others for helpful feedback. If you like our work, please consider</i><a href=\"https://www.rethinkpriorities.org/newsletter\"><i>&nbsp;<u>subscribing to our newsletter</u></i></a><i>. You can explore our completed public work&nbsp;</i><a href=\"https://www.rethinkpriorities.org/research\"><i><u>here</u></i></a><i>.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxvlwa3bws4a\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxvlwa3bws4a\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See also&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/C26RHHYXzT6P6A4ht/what-rethink-priorities-general-longtermism-team-did-in-2022\"><u>this post on what XST did in 2022</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn1aw09hlmuh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn1aw09hlmuh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Percentages indicate roughly what fraction of our time we plan to spend on these focus areas from April 2023 to October 2023 (inclusive).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvt14e6ijtr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvt14e6ijtr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This might include one or more of the 5 project idea memos or strategic clarity research output if those don\u2019t contain sensitive information.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjc96pp6o2kg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjc96pp6o2kg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Before mid May we have 3 FTE executing on this strategy, due to Renan being occupied with other work.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0lpzs3gwxh1\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0lpzs3gwxh1\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Between March and October. We'll review our impact and strategy again in Nov-Dec.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7khhyhrhe1c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7khhyhrhe1c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See also this very rough document:&nbsp;<a href=\"https://docs.google.com/document/d/1mrmbcjLfpubTdEs6l0QIqXYiJz3006oE6-IvuDD8Mu8/edit#heading=h.fjuloopjncuv\"><u>Proposal: We should do more EA movement building research</u></a></p></div></li></ol>", "user": {"username": "Ben_Snodin"}}, {"_id": "gn4vSLfCeezgmvnTc", "title": "Shapley value, importance, easiness and neglectedness", "postedAt": "2023-05-05T07:33:47.573Z", "htmlBody": "<h1>Summary</h1><ul><li>I have looked into <a href=\"https://forum.effectivealtruism.org/posts/XHZJ9i7QBtAJZ6byW/shapley-values-better-than-counterfactuals\"><u>Shapley</u></a>&nbsp;and <a href=\"https://forum.effectivealtruism.org/topics/counterfactual-reasoning\"><u>counterfactual</u></a>&nbsp;value for a number of people working on a problem who each have the same probability of solving it, thus achieving a certain total value.</li><li>The Shapley and counterfactual value of each person&nbsp;is such that <a href=\"https://forum.effectivealtruism.org/topics/diminishing-returns\"><u>diminishing returns</u></a>&nbsp;only kick in when that probability is high relative to <a href=\"https://forum.effectivealtruism.org/topics/neglectedness\"><u>neglectedness</u></a>, in which case Shapley value is directly proportional to neglectedness.</li><li>Under these conditions, counterfactual&nbsp;value is way smaller than Shapley value, so relying on counterfactuals will tend to underestimate cost-effectiveness when there are more people deciding on whether to work on a problem, like with the <a href=\"https://www.effectivealtruism.org/\"><u>effective altruism</u></a>&nbsp;community.</li><li>Nonetheless, in a situation where a community is coordinating to maximise impact, it might as well define its actions together as a single agent. In this case, Shapley and counterfactual value would be the same. In practice, I guess coordination along those lines will not always be possible, so Shapley values may still be useful.</li></ul><h1>Introduction</h1><p><a href=\"https://forum.effectivealtruism.org/posts/XHZJ9i7QBtAJZ6byW/shapley-values-better-than-counterfactuals\"><u>Shapley values</u></a>&nbsp;are an extension of <a href=\"https://forum.effectivealtruism.org/topics/counterfactual-reasoning\"><u>counterfactuals</u></a>, and are uniquely determined by the following properties:</p><blockquote><ul><li>Property 1: Sum of the values adds up to the total value (Efficiency)</li><li>Property 2: Equal agents have equal value (Symmetry)</li><li>Property 3: Order indifference: it doesn't matter which order you go in (Linearity). Or, in other words, if there are two steps, Value(Step1 + Step2) = Value(Step1) + Value(Step2).</li></ul></blockquote><p>Counterfactuals do not have property 1, and create tricky dynamics around 2 and 3, so Shapley values are arguably better.</p><p>In this post, I study a simple model to see how Shapley value relates to the importance, tractability and neglectedness (<a href=\"https://forum.effectivealtruism.org/topics/itn-framework\"><u>ITN</u></a>) framework.</p><h1>Methods</h1><p>For N people who are deciding on whether to work on a problem, and each have a probability p of solving it, which I call easiness, thus achieving a total value V (as I commented <a href=\"https://forum.effectivealtruism.org/posts/RSxff9TRoggYuFHAL/cooperative-or-competitive-altruism-and-antisocial?commentId%3DvzzBJWjyLht4vBBLr\"><u>here</u></a>):</p><ul><li>The actual contribution of a coalition with size n is:<ul><li><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"v_a(n) = (1 - (1 - p)^n)\\,V \\;.\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span><span class=\"mjx-mspace\" style=\"width: 0.278em; height: 0px;\"></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span></li></ul></li><li>The marginal contribution to a coalition with size n is:<ul><li><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\begin{align} v_m(n) &amp;= v_a(n + 1) - v_a(n) = (1 - (1 - p)^{(n + 1)})\\,V - (1 - (1 - p)^n)\\,V = \\\\ &amp;= p\\,(1 - p)^n\\,V \\;. \\end{align}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mtable\" style=\"vertical-align: -1.055em; padding: 0px 0.167em;\"><span class=\"mjx-table\"><span class=\"mjx-mtr\" style=\"height: 1.386em;\"><span class=\"mjx-mtd\" style=\"padding: 0px 0px 0px 0px; text-align: right; width: 2.519em;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.039em;\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0px 0px 0px 0px; text-align: left; width: 27.687em;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.039em;\"><span class=\"mjx-mi\"></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-strut\"></span></span></span></span><span class=\"mjx-mtr\" style=\"height: 1.225em;\"><span class=\"mjx-mtd\" style=\"padding: 0.15em 0px 0px 0px; text-align: right;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0.15em 0px 0px 0px; text-align: left;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-mi\"></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span><span class=\"mjx-mspace\" style=\"width: 0.278em; height: 0px;\"></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-strut\"></span></span></span></span></span></span></span></span></span></span></span></li></ul></li><li>The counterfactual value of each person is:<ul><li><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathrm{CV}(N) = v_m(N - 1) = p\\,(1 - p)^{N - 1}\\,V \\;.\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">V</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span><span class=\"mjx-mspace\" style=\"width: 0.278em; height: 0px;\"></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span></span></li></ul></li><li>The marginal contribution only depends on the size of the coalition, not on its specific members, and therefore the Shapley value of each person is:<ul><li><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathrm{SV}(N) = &nbsp;\\frac{1}{N} \\sum_{n = 0}^{N - 1}{v_m(n)} = \\frac{p\\,V}{N} \\sum_{n = 0}^{N - 1}{\\left(1 - p\\right)^n} = \\frac{p\\,V}{N} \\frac{1 - (1 - p)^N}{1 - \\left(1 - p\\right)} = \\frac{1 - (1 - p)^N}{N}\\,V \\;.\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">S</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">V</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.769em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.088em; top: -1.372em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.088em; bottom: -0.682em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 0.769em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.453em; vertical-align: -0.482em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size1-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">\u2211</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.31em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.351em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span></span><span class=\"mjx-texatom MJXc-space1\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 1.208em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.708em; top: -1.583em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mspace\" style=\"font-size: 141.4%; width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.708em; bottom: -0.682em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 1.208em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.602em; vertical-align: -0.482em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size1-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">\u2211</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.31em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.351em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span></span><span class=\"mjx-texatom MJXc-space1\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.71em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 1.208em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.708em; top: -1.583em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mspace\" style=\"font-size: 141.4%; width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.708em; bottom: -0.682em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 1.208em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.602em; vertical-align: -0.482em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 3.407em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 4.819em; top: -1.884em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.435em; padding-left: 0px; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span></span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 4.819em; bottom: -0.999em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 3.407em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.039em; vertical-align: -0.707em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 3.407em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 4.819em; top: -1.884em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.435em; padding-left: 0px; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span></span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 4.819em; bottom: -0.682em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 3.407em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.815em; vertical-align: -0.482em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span><span class=\"mjx-mspace\" style=\"width: 0.278em; height: 0px;\"></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span></span></li></ul></li></ul><p>This formula corresponds to the ratio between the total expected value and number of people, given that everyone is in the same conditions. Mapping the Shapley value to the <a href=\"https://forum.effectivealtruism.org/topics/itn-framework\"><u>ITN</u></a>&nbsp;framework:</p><ul><li>The total value would be the <a href=\"https://forum.effectivealtruism.org/topics/importance\"><u>importance</u></a>/scale of the problem.</li><li>The probability of at least one person achieving the total value (denominator of the 1st factor of the formula just above) would be the <a href=\"https://forum.effectivealtruism.org/topics/tractability\"><u>tractability</u></a>/solvability.</li><li>The reciprocal of the number of people would be the <a href=\"https://forum.effectivealtruism.org/topics/neglectedness\"><u>neglectedness</u></a>/uncrowdedness.</li></ul><p>To facilitate the determination of p and V, the problem could be defined as a subproblem of a cause area, and the value of solving it expressed as a fraction of the importance of that area.</p><p>I calculated in <a href=\"https://docs.google.com/spreadsheets/d/1jtUjwNNy9g0gd1M57IKQ0xzMnhdJsybqtPxkA2W1Jp8/edit?usp%3Dsharing\"><u>this</u></a>&nbsp;Sheet the counterfactual and Shapley value as a fraction of the total value, and ratio between them for a number of people ranging from 1 to 10^10 (similar to the global population of 8 billion), and probability&nbsp;of achieving the total value from 10^-10 to 1.</p><h1>Results</h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/gn4vSLfCeezgmvnTc/frcqgwstzov8zwxjkucc\" alt=\"\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/gn4vSLfCeezgmvnTc/pooorcctyxvpumwott6w\" alt=\"\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/gn4vSLfCeezgmvnTc/wtoyokrk6htvskmtkhia\" alt=\"\"></p><h1>Discussion</h1><p>The figures show the counterfactual and Shapley value vary as a function of the ratio between easiness<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnpas6dfxwvj\"><sup><a href=\"#fnnpas6dfxwvj\">[1]</a></sup></span>&nbsp;and neglectedness (R = p/(1/N)):</p><ul><li>The counterfactual value as a fraction of the total value is:<ul><li>99.0 % of p for R = 0.01.</li><li>90.5 % of p for R = 0.1.</li><li>36.8 % of p for R = 1.</li><li>4.54*10^-5 of p for R = 10.</li><li>Negligible for R &gt;= 100.</li></ul></li><li>The Shapley value as a fraction of the total value is:<ul><li>100 % of p for R &lt;= 0.01.</li><li>95.2 % of p for R = 0.1.</li><li>63.2 % of p for R = 1.</li><li>10.0 % of p for R = 10.</li><li>Approximately 1/N for R &gt;= 10.</li></ul></li><li>The ratio between counterfactual and Shapley value is:<ul><li>99.5 % for R = 0.01.</li><li>95.1 % for R = 0.1.</li><li>58.2 % for R = 1.</li><li>0.0454 % for R = 10.</li><li>Negligible for R &gt;= 100.</li></ul></li></ul><p>This illustrates <a href=\"https://forum.effectivealtruism.org/topics/diminishing-returns\"><u>diminishing returns</u></a>&nbsp;only kick in when easiness is high relative to neglectedness. When easiness is low relative to neglectedness, the Shapley and counterfactual value are essentially equal to the naive expected value (p V) regardless of the specific neglectedness. When easiness is high relative to neglectedness, the Shapley value is directly proportional to neglectedness (SV = V/N), and counterfactual value quickly approaches 0.</p><p>Counterfactual value only matches Shapley value if easiness is low relative to neglectedness. When the opposite is true, counterfactual&nbsp;value is way smaller than Shapley&nbsp;value. Consequently, relying on counterfactuals will tend to underestimate (Shapley) cost-effectiveness when there are more people deciding on whether to work on a problem, like with the <a href=\"https://www.effectivealtruism.org/\"><u>effective altruism</u></a>&nbsp;community.</p><p>I wonder about the extent to which easiness and neglectedness are independent. If easiness is often high relative to neglectedness (e.g. R &gt;= 10), assuming diminishing returns for all levels of neglectedness may be reasonable. However, in this case, counterfactual value would be much smaller than Shapley value, so using this would be preferable.</p><p>In any case, the problem to which my importance, easiness and neglectedness framework is applied should be defined such that it makes sense to model the contribution of each person as a probability of solving it. It may be worth thinking in these terms sometimes, but contributions to solving problems are not binary. Nevertheless, I suppose one can get around this by interpreting the probability of solving the problem as the fraction of it that would be solved in expectation.</p><p>Overall, I am still confused about what would be good use cases for Shapley value (if any). Counterfactual value can certainly be misleading, as shown in my 3rd figure, and even better in <a href=\"https://forum.effectivealtruism.org/posts/XHZJ9i7QBtAJZ6byW/shapley-values-better-than-counterfactuals\"><u>Nu\u00f1o\u2019s illustrative examples</u></a>. Nonetheless, as tobycrisford <a href=\"https://forum.effectivealtruism.org/posts/RSxff9TRoggYuFHAL/cooperative-or-competitive-altruism-and-antisocial?commentId%3DwtBiJLcf9RycXBTv6\"><u>commented</u></a>, in a situation where a community is coordinating to maximise impact, it might as well define its actions together as a single agent. In this case (N = 1), Shapley and counterfactual value would be the same (in my models, p V). In practice, I guess coordination along those lines will not always be possible, so Shapley values may still be useful, possibly depending on how easy it is to move towards single-agent coordination relative to building estimation infrastructure.</p><h1>Acknowledgements</h1><p>Thanks to Nu\u00f1o Sempere for feedback on the draft.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnpas6dfxwvj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnpas6dfxwvj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Note \u201ctractability\u201d = 1 - (1 - \u201ceasiness\u201d)^N.</p></div></li></ol>", "user": {"username": "vascoamaralgrilo"}}, {"_id": "n3vxBKgyBSnCWayAk", "title": "How Engineers can Effectively Contribute to Tackling Climate Change", "postedAt": "2023-05-08T18:00:15.720Z", "htmlBody": "<p><i>Cross-posted from the </i><a href=\"https://www.highimpactengineers.org/how-engineers-can-maximise-their-impact-working-on-climate-change\"><i>High Impact Engineers Resource Portal</i></a><i>. You can view the most up-to-date version on the Portal.</i></p><p><i>Edited 11th October 2023 to update links.</i></p><h1>Summary</h1><p>Climate change is a cause area that needs no introduction - you\u2019re probably already very aware of human-caused climate change and its myriad socioeconomic consequences.</p><p>However, our take on climate change is a little different than the typical perspective. Assessing climate change with the <a href=\"https://80000hours.org/articles/problem-framework/\">ITN framework</a>, climate change is (of course) important and has many tractable interventions; but with annual international investment over 630 billion USD and extensive academic and political discourse, it isn\u2019t as neglected as other cause areas we have explored.</p><p>Given this, we recommend that engineers who are interested in working on climate change pursue interventions that are potentially very effective but aren\u2019t widely discussed. This could look like improving nuclear energy, decarbonising building materials, or maturing a technology that could be highly impactful if commercialised, such as direct carbon capture or super hot rock geothermal energy. Other neglected areas of interest could include technologies that mitigate the worst tail-end effects of catastrophic runaway climate change.</p><h2>Uncertainty</h2><p>The content of this article is largely based on research by <a href=\"https://80000hours.org/\">80,000 Hours</a>, the <a href=\"http://catf.us\">Clean Air Task Force</a>, <a href=\"https://founderspledge.com/\">Founders Pledge</a>, and the UN in their <a href=\"https://www.un.org/en/climatechange\">Climate Action reports</a>. We feel substantially confident in the information and recommendations in this article.</p><ul><li>There have been arguments that <a href=\"https://mdickens.me/2016/06/10/evaluation_frameworks_(or-_when_scale-neglectedness-tractability_doesn%27t_apply)/\">ITN is a framework that doesn\u2019t always apply</a>, or that the <a href=\"https://drive.google.com/file/d/1Lapv64IYsvUnaYWDFmZBoDWe_x5_zkr7/view\">Significance, Persistence, Contingency framework</a> should be used instead - your evaluation approach may significantly affect how you rank working on climate change against other cause areas.</li><li>Despite the amount of research into climate change, there are still some <a href=\"https://80000hours.org/problem-profiles/climate-change/#key-questions-were-unsure-about\">key questions that remain unanswered</a>. Robust conclusions about climate change are hard to find, especially when trying to understand what might be the worst possible outcomes.</li></ul><h1>Cause area overview</h1><h2>Is climate change the most pressing problem?</h2><p>Climate change is often the first area that altruistically-minded engineers gravitate to working on. <a href=\"https://e360.yale.edu/digest/extreme-weather-events-have-increased-significantly-in-the-last-20-years\">Extreme weather phenomena are becoming much more frequent</a>, economic costs of warming this century could <a href=\"https://www.ucl.ac.uk/news/2021/sep/economic-cost-climate-change-could-be-six-times-higher-previously-thought\">potentially be up to 51% of global GDP</a>, <a href=\"https://royalsociety.org/topics-policy/projects/biodiversity/what-is-the-scale-of-biodiversity-loss/\">extinction rates are increasing to 10-100x</a> pre-human rates and potentially threatening a million species, and an estimated <a href=\"https://www.un.org/en/global-issues/climate-change\">3.3 to 3.6 billion people</a> live in contexts that are highly vulnerable to climate change. It is clear that climate change has already been, and will increasingly be, extremely destructive as global surface temperatures rapidly approach the <a href=\"https://www.ipcc.ch/sr15/\">watershed 1.5\u00b0C rise</a>.</p><p>A <a href=\"https://doi.org/10.1016/S2542-5196(21)00278-3\">global survey</a> found that over half of young people worry that humanity is doomed due to climate change. Although it is completely justified to feel angry and afraid for the future, it seems that, in comparison to other cause areas (like <a href=\"https://www.highimpactengineers.org/how-engineers-can-have-impact-working-in-biosecurity\">Biorisk and Biosecurity</a> and <a href=\"https://www.highimpactengineers.org/how-engineers-can-work-on-civilisation-resilience\">Civilisation Resilience</a>), climate change is much less likely to cause doom to humanity or be an existential threat to civilisation.</p><p>Climate change also receives significantly more resources (e.g. private and public investment, technical talent) and international attention (e.g. academic and political discourse, activism) than other cause areas, meaning that you are more likely to have more impact elsewhere. <strong>We think </strong><a href=\"https://80000hours.org/problem-profiles/climate-change/\">these arguments</a><strong> are worth some serious consideration before you decide to continue pursuing work to mitigate climate change</strong>.</p><p>In short, research on the extreme effects of climate change is somewhat lacking and remains fairly speculative, but climate change is unlikely to directly cause human extinction or civilisation collapse. That being said, climate change would cause huge stress on the systems that make up civilisation, such as reducing international peace and cooperation, and could exacerbate the effects of other global catastrophes.</p><p>However, if you aren\u2019t particularly concerned about existential risks you may place less weight on the above arguments. There are other reasons why one would be deeply concerned about climate change: <a href=\"https://www.unep.org/resources/emissions-gap-report-2022\">current projections predict 2.8\u00b0C of warming</a> (which would definitely be devastating), and there's a larger than 1% chance that the Earth will experience 6\u00b0C of warming, which would create an almost unimaginably different world from the one that we\u2019re living in now. More extreme weather events would cause severe destruction to homes, communities, economies, ecosystems, and agriculture. Many species on land and in the ocean would become endangered or extinct as temperatures rise, the oceans become more acidic, and invasive pests, diseases, wildfires and extreme weather threaten the delicate balance of the ecosystem.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/n3vxBKgyBSnCWayAk/ssbfehkb5zghnojhxcvw\" alt=\"StockSnap_YZW03NQVXH.jpg\"></p><p>Figure 1: Rising sea levels could displace hundreds of millions of people <a href=\"https://openknowledge.worldbank.org/entities/publication/8c83836e-a280-5887-80fd-57f8b2bbdb0b\">within this century</a> (<a href=\"https://stocksnap.io/photo/ocean-sea-F73B3YHGT9\">Photo</a> by <a href=\"https://stocksnap.io/author/8808\">Robert De Bock</a> on <a href=\"https://stocksnap.io/\">StockSnap</a>)</p><p>Further to the destruction of the environment, climate change is a huge threat to human health: \u201cthrough air pollution, disease, extreme weather events, forced displacement, pressures on mental health, and increased hunger and poor nutrition in places where people cannot grow or find sufficient food. Every year, environmental factors take the lives of around 13 million people\u201d (<a href=\"https://www.un.org/en/climatechange/science/causes-effects-climate-change\">UN</a>).</p><p>Climate change could increase global instability, as weather-related events <a href=\"https://www.internal-displacement.org/database/displacement-data\">displaced an estimated 21.5 million people</a> on average each year over the past decade. This is <a href=\"https://www.unrefugees.org.uk/learn-more/news/news/whats-a-climate-refugee/\">more than twice</a> the number of people displaced due to conflict and violence. <a href=\"https://www.unhcr.org/news/news-releases/unhcr-urgent-steps-needed-now-mitigate-climate-impact-displaced-people\">Most refugees</a> come from countries that are most vulnerable, contribute least to greenhouse gas emissions, and least ready to adapt to the impacts of climate change. These are all compelling reasons to care about climate change. Reactions to relatively small disturbances can be amplified in societal processes, <a href=\"https://www.effectivealtruism.org/articles/johannes-ackva-an-update-to-our-thinking-on-climate-change\">according to Johannes Ackva</a>, a researcher at Founders Pledge. It\u2019s unclear whether society will be able to mitigate the different effects of climate change, or make them worse.</p><h2>Heuristics for deciding which interventions to work on</h2><p>If you\u2019ve engaged with these arguments and you are still sure that climate change is the problem in which you can have the most impact, it\u2019s important to find the most effective intervention for you to work on. Engineers have many opportunities to work on interventions that tackle climate change, from electric vehicles to carbon capture, but some interventions are much more impactful than others.</p><p>For instance, car emissions are only ~2x higher than emissions from cement (<a href=\"https://ourworldindata.org/emissions-by-sector\">Our World in Data</a>), but there\u2019s almost 10x the investment on electric cars (<a href=\"https://www.reuters.com/business/autos-transportation/exclusive-global-carmakers-now-target-515-billion-evs-batteries-2021-11-10/\">$225bn global investment in EVs and batteries by car makers (excluding battery manufacturers and researchers) in 2020</a> vs <a href=\"https://www.globenewswire.com/news-release/2022/06/29/2471074/0/en/Global-Green-Cement-Market-Garnered-Around-USD-25-Billion-in-2021-Market-to-Grow-on-Account-of-Rising-Environmental-Concerns-and-Implementation-of-Strict-Regulations-Over-Emission-.html\">$25bn into green cement in 2021</a>). This suggests that there are better opportunities \u2014 \u201clow-hanging fruit\u201d \u2014 for your efforts to reduce emissions by greening cement production. In our view, this kind of \u201con the margin\u201d thinking is particularly important when there is so much attention and interest in working on climate, to think outside the issues with the most attention and understand where your efforts could go further and have more <a href=\"https://80000hours.org/articles/counterfactuals/\">counterfactual impact</a>. You can find out more about this kind of strategic thinking in <a href=\"https://80000hours.org/podcast/episodes/johannes-ackva-unfashionable-climate-interventions/\">Johannes Ackva\u2019s recent episode on climate interventions</a> on the 80,000 Hours podcast.</p><p><i><strong>Resource suggestion: </strong></i><a href=\"https://coda.io/d/_d9tTRAlsYqC#Resources-Table-view_tu7Uo/r64&amp;view=modal\">Johannes Ackva on unfashionable climate interventions that work, and fashionable ones that don\u2019t</a></p><p>As Martin Hare Robertson writes in his <a href=\"https://docs.google.com/document/d/1MNPkLJqRmrk380L24G31szsKt9Dc8qB2M8PO7-YW5HI/edit\">advice for technologists</a> who want to work in climate change, it is impossible to know the \u2018best\u2019 choice for what to work on and where. His suggestion is to find something that has a clear link with driving down emissions, and do that. If you have multiple options, pick the one with the highest plausible impact. There are no easy answers, but having high uncertainty does not imply ignorance \u2014 you can still make decisions based on the information you have.</p><p>Below, we\u2019ve attempted to find the industries that are plausibly the most impactful for tackling climate change.</p><h1>How can engineers do impactful work in climate change?</h1><h2>Causes of human-caused climate change</h2><p>The burning of fossil fuels (oil, coal, and gas) account for over 75% of global greenhouse gas (GHG) emissions and ~90% of all carbon dioxide emissions (<a href=\"https://www.un.org/en/climatechange/science/causes-effects-climate-change\">UN</a>), which trap the sun\u2019s heat and raise the surface temperature of the Earth, causing long-term shifts in weather patterns and other devastating effects on our environment. Figure 2 shows the main sectors that contribute to GHG emissions:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/n3vxBKgyBSnCWayAk/a1ye0v6uhul4wnikgiya\" alt=\"Emissions-by-sector-\u2013-pie-charts.png\"></p><p>Figure 2: Infographic of global greenhouse gas emissions by sector (<a href=\"https://ourworldindata.org/emissions-by-sector\">Our World in Data</a>)</p><h3>Generating energy/power</h3><p>Although the generation of electricity is shifting away from fossil fuels, this is <strong>only one component</strong> of total energy consumption. Total energy consumption is the sum of electricity generated, demand for energy from transport, and from heating.</p><p>Transport and heating rely much more heavily on fossil fuels than electricity, which is why fossil fuels account for <a href=\"https://ourworldindata.org/energy-mix\">more than 80%</a> of the global energy consumption. Decarbonising electricity is <a href=\"https://ourworldindata.org/electricity-mix\">only one step</a> towards a low-carbon energy system. Climate philanthropists skew their funding heavily towards clean electricity, while continuing to overlook other sectors that are harder to decarbonise \u2013 these are the sectors that desperately need more attention (<a href=\"https://founderspledge.com/stories/changing-landscape\">Founders Pledge</a>).</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/n3vxBKgyBSnCWayAk/vaj6t7ahw8vqwt7bzqhv\" alt=\"Global-energy-vs.-electricity-breakdown.png\"></p><p>Figure 3: Infographic comparing the sources of electricity and energy (<a href=\"https://ourworldindata.org/energy-mix\">Our World in Data</a>)</p><p>Global energy consumption is growing year on year, outstripping the pace of decarbonisation. It is the <i>absolute</i> amount of carbon dioxide that has an impact on global warming. Decarbonisation needs to grow quickly enough to meet our new energy demands each year, but also <a href=\"https://ourworldindata.org/energy-mix#it-s-the-total-amount-of-fossil-fuels-we-burn-that-matters-and-we-continue-to-burn-more-each-year\">start displacing fossil fuels in the energy mix at a much faster rate</a>. Clean energy needs to expand to emerging economies with high future emissions such as India, Southeast Asia, and Sub-Saharan Africa, without restricting energy access and the resulting perpetuation of poverty (<a href=\"https://founderspledge.com/stories/changing-landscape\">Founders Pledge</a>).</p><p><a href=\"http://drawdown.org\">Project Drawdown</a> predicts that <a href=\"https://drawdown.org/solutions/onshore-wind-turbines\">onshore wind turbines</a> could reduce emissions between 2020 \u2013 2050 by 47 \u2013 144 gigatons CO2 equivalent. <a href=\"https://drawdown.org/solutions/utility-scale-solar-photovoltaics\">Utility-scale photovoltaics</a> could cut emissions between 2020 \u2013 2050 by 41 to 112 gigatons CO2 equivalent. It\u2019s worth noting that Project Drawdown is heavily focused on implementing existing solutions, rather than engineering new solutions. As a result, <strong>the main bottlenecks they focus on are in policy</strong> rather than technical innovation.</p><h3>Manufacturing goods</h3><p>The manufacturing process, from mining raw materials to the production of things like cement, aluminium, steel, electronics, plastics, clothes, etc. is one the largest contributors to greenhouse gas emissions worldwide.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/n3vxBKgyBSnCWayAk/gifgmqmahr3zof1343lj\" alt=\"StockSnap_ZVSYCEDFHE.jpg\"></p><p>Figure 4: Cement <a href=\"http://Solarradiationmanagement(SRM),anapproachtolimitorreverseclimatechangebyreflectingsunlightbackintospacethroughmethodssuchasstratosphericaerosolinjection,hasbeenproposedasaquickandcheapoption(intheshortrun)tomitigateclimatechange.However,thisapproach(whichmaysufferfromtheunilateralist\u2019scurse)mayproduceunintendedanddetrimentalside-effectssuchascropfailureandrapidtemperaturechangesleadingtospeciesextinction,andwethinkthatworktoadvanceSRMislikelytobeharmful.Possibilitiesfornet-goodSRMwork*SRMresearchmainlyoccursinacademia,forexampleintheOxfordUniversityGeoengineeringProgramme,whichconductsresearchintothesocial,ethicalandtechnicalaspectsofSRMandothergeoengineering.*Aswehavecontinuedtofailtomitigategreenhousegases,SRMmightbecomea\u201clastditch\u201dattempttocoolglobaltemperatures.WorkingonpolicyandgovernanceofSRMtomitigatethenegativeside-effectscouldbehigh-impact.\">accounts for 3%</a> of the world\u2019s carbon dioxide emissions (<a href=\"https://stocksnap.io/photo/city-building-ZVSYCEDFHE\">Photo</a> by <a href=\"https://stocksnap.io/author/operationcamp\">Operation Camp</a> on <a href=\"https://stocksnap.io/\">StockSnap</a>)</p><p>Mining and other industrial processes release gases, as does the construction industry (cement contributes <a href=\"https://www.chathamhouse.org/2018/06/making-concrete-change-innovation-low-carbon-cement-and-concrete\">~8% of global carbon dioxide emissions</a>). Manufacturing machines often run on fossil fuels. Furthermore, some materials like plastics are made from chemicals sourced from fossil fuels.</p><p>Industry accounts for <a href=\"https://www.epa.gov/ghgemissions/sources-greenhouse-gas-emissions\">24% of the GHG emissions in the US</a> (inclusive of GHG emissions from energy generated for industry - i.e. indirect GHG emissions). Industry accounts for <a href=\"https://ourworldindata.org/emissions-by-sector\">24.2% of global energy use</a>, and produces a further 5.2% of global GHG emissions.</p><p><a href=\"https://industriouslabs.org/\">Industrious Labs</a> is an environmental campaign organisation that strategically coordinates other organisations to decarbonise heavy industry.</p><h3>Transportation</h3><p>Transport accounts for 16.2% of global GHG emissions (not including emissions from the manufacturing motor vehicles or other transport equipment - that is included in the previous point \u201cManufacturing goods\u201d).</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/n3vxBKgyBSnCWayAk/zd4thxotanpkvrcztox0\" alt=\"StockSnap_K8NTTCGBZU.jpg\"></p><p>Figure 5: Transport accounts for 16.2% of global GHG emissions (<a href=\"https://stocksnap.io/photo/highway-traffic-K8NTTCGBZU\">Photo</a> by <a href=\"https://stocksnap.io/author/kikizhang\">Kiki Zhang</a> on <a href=\"https://stocksnap.io/\">StockSnap</a>)</p><p>Road transport is the largest contributor to GHGs for transport. Passenger travel (cars, motorcycles, buses) <a href=\"https://www.iea.org/data-and-statistics/charts/transport-sector-co2-emissions-by-mode-in-the-sustainable-development-scenario-2000-2030\">causes</a> 60% of road transport emissions. Road freight (lorries and trucks) account for the rest.</p><p>81% of aviation emissions (1.9% of global GHGs) <a href=\"https://theicct.org/sites/default/files/publications/ICCT_CO2-commercl-aviation-2018_20190918.pdf\">come from</a> passenger travel (of which 60% is attributed to international travel and 40% to domestic). 19% of aviation emissions come from freight.</p><p><a href=\"http://drawdown.org\">Project Drawdown</a> has a list of <a href=\"https://drawdown.org/solutions/table-of-solutions\">other solutions</a> to decarbonising the transportation sector. It\u2019s worth noting that Project Drawdown is heavily focused on implementing existing solutions, rather than engineering new solutions. As a result, <strong>the main bottlenecks they focus on are in policy</strong> rather than technical innovation.</p><h3>Agriculture</h3><p>Agriculture, forestry and land use <a href=\"https://ourworldindata.org/emissions-by-sector\">directly produce 18.4% of global GHG emissions</a>. If GHG emissions are calculated for the food system as a whole (<a href=\"https://ourworldindata.org/food-ghg-emissions\">including refrigeration, food processing, packaging, and transport</a>), this number goes up to about <strong>25% of global GHG emissions</strong>.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/n3vxBKgyBSnCWayAk/pehvtjpooul9haftaizr\" alt=\"StockSnap_CYDEYOVHOE.jpg\"></p><p>Figure 6: Agriculture, forestry and land use <a href=\"https://ourworldindata.org/emissions-by-sector\">directly produce 18.4% of global GHG emissions</a> (<a href=\"https://stocksnap.io/photo/calf-cow-CYDEYOVHOE\">Photo</a> by <a href=\"https://stocksnap.io/author/ianlivesey\">Ian Livesey</a> on <a href=\"https://stocksnap.io/\">StockSnap</a>)</p><p>Livestock and manure are the largest direct contributors to agricultural GHGs. Cows and sheep in particular tend to have a high carbon footprint due to their digestive process that <a href=\"https://ourworldindata.org/carbon-footprint-food-methane\">produces methane</a> as a by-product. This means that reducing your consumption of beef and lamb is an effective individual intervention to <a href=\"https://ourworldindata.org/food-choice-vs-eating-local\">reduce the environmental impact</a> of your diet.</p><p><a href=\"https://drawdown.org/\">Project Drawdown</a> calculates that plant-rich diets could cut <a href=\"https://drawdown.org/solutions/plant-rich-diets\">78 \u2013 103 gigatons CO2 equivalent</a> between 2020 \u2013 2050. <strong>Developing and commercialising </strong><a href=\"https://www.highimpactengineers.org/how-engineers-can-work-on-alternative-protein\">Alternative Protein</a><strong> could catalyse a societal shift to plant-rich diets through bringing price-competitive meat alternatives to market.</strong></p><p>See our page on <a href=\"https://www.highimpactengineers.org/how-engineers-can-work-on-alternative-protein\">Alternative Protein</a></p><h3>Powering buildings</h3><p>Energy for residential buildings produce 10.9% of the world\u2019s GHGs, while commercial buildings generate 6.6%. These GHG emissions can be produced by the generation of electricity for lighting, appliances, cooking, and heating. Technologies to decarbonise electricity already exist; <strong>the main bottlenecks are in policy</strong> rather than in technical development.</p><h3>Overconsumption</h3><p>Our lifestyles have a profound impact on our planet. Although companies and governments have the most responsibility to bear when it comes to GHGs, our individual consumption choices also collectively contribute to GHG emissions. <a href=\"https://www.footprintcalculator.org/home\">Find out how many Earths it takes to support your lifestyle</a> \u2014 you might notice that even if you picked the most eco-friendly options, if you\u2019re from developed countries you\u2019ll still require more than 2 Earths. This is because a country\u2019s climate policy dramatically changes the impact of people\u2019s lifestyle choices (<a href=\"https://founderspledge.com/stories/climate-and-lifestyle-report\">Founders Pledge</a>). However, one <a href=\"https://www.science.org/doi/10.1126/science.aaq0216\">significant large-scale study</a> outlined that eating a vegan diet could be the \u201csingle biggest way\u201d for individuals to reduce their personal negative impact on the environment.</p><p><a href=\"https://drawdown.org\">Project Drawdown</a> estimates that reducing food waste could cut <a href=\"https://drawdown.org/solutions/reduced-food-waste\">88 \u2013 102 gigatons CO2 equivalent</a> between 2020 \u2013 2050. <a href=\"https://www.effectiveenvironmentalism.org/careers-advice\">Advocating for policy change</a> is likely to be the most effective way for an individual to change our collective behaviour.</p><h2>Tackling climate change as an engineer</h2><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/n3vxBKgyBSnCWayAk/gvc9mlmfunci1wjnfyus\" alt=\"thisisengineering-raeng-hoivM01c-vg-unsplash.jpg\"></p><p>Figure 7: Photo by <a href=\"https://unsplash.com/@thisisengineering?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">ThisisEngineering RAEng</a> on <a href=\"https://unsplash.com/s/photos/engineer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></p><p>It seems particularly tractable for engineers to tackle climate change, as many of the solutions involve developing and implementing technologies that are less reliant on burning fossil fuels, or making them more economically viable. Here we explore few technologies that we think are particularly neglected by engineers working on climate.</p><h3>Superhot rock geothermal energy</h3><p>A visionary technology that has the potential to meet long-term demands for zero-carbon, always-on power is <a href=\"https://en.wikipedia.org/wiki/Hot_dry_rock_geothermal_energy\">superhot rock geothermal energy</a>.</p><p>Superhot rock geothermal energy can be generated from dry rock that\u2019s 400C or higher. It exists all over the earth at depths between 2 - 12 miles (3 - 20 km). However, it\u2019s almost entirely unrecognised in the decarbonisation landscape.</p><p>The <a href=\"http://catf.us\">Clean Air Task Force</a>, a climate non-profit, <a href=\"https://www.catf.us/work/superhot-rock/\">finds that</a> with investment in innovation, superhot rock geothermal energy can be commercialised and deployed rapidly in the 2030s. It could be cost-competitive with other zero-carbon technologies while also, importantly, having a small land footprint.</p><p>There are many opportunities for all kinds of engineers in this area. We think that Reservoir Engineers and other Petroleum Engineers are likely to have transferable skills (e.g. experience in deep drilling) that could be applicable to this industry.</p><h3>Nuclear energy</h3><p>Nuclear energy has been around for more than half a century and is one of the <a href=\"https://ourworldindata.org/nuclear-energy#nuclear-and-renewables-are-far-far-safer-than-fossil-fuels\">safest</a>, <a href=\"https://ourworldindata.org/nuclear-energy#the-safest-energy-sources-are-also-the-cleanest\">cleanest</a>, and most reliable sources of energy. The <a href=\"https://www.world-nuclear-news.org/Articles/The-real-challenges-to-nuclear-are-external,-says\">biggest hurdles to nuclear fission are mostly in regulation and a negative public perception</a>, so <strong>a career in policy or advocacy is likely to be the most impactful in this area</strong>. However, there are some technical bottlenecks that require engineering expertise to solve, such as in Advanced Modular Reactors. <strong>Read more in the </strong><a href=\"https://www.highimpactengineers.org/effectively-working-on-nuclear-energy-engineering\">Nuclear Energy</a><strong> sub-page.</strong></p><h3>Energy efficiency</h3><p>Increasing energy efficiency (e.g. reducing the cost of insulated buildings) would reduce total energy consumption. Project Drawdown expects <a href=\"https://drawdown.org/solutions/insulation\">retrofitting buildings with insulation</a> to cost-effectively cut emissions by 15.4 \u2013 18.5 gigatons CO2 equivalent by 2050. However, it is incentivised by the market, and it is not very neglected according to this <a href=\"https://founderspledge.com/research/fp-climate-change\">Founders Pledge report</a>.</p><p><a href=\"https://drawdown.org/solutions/methane-leak-management\">Managing methane leaks</a> in oil and gas supply chains could reduce GHG emissions by 25.8 to 31.3 gigatons of CO2 equivalent by 2050.</p><p>As of 2020, the proportion of households without access to clean cooking was ~36%. Traditional biomass-based cooking produces black carbon, the second-most impactful climate pollutant, along with plumes of smoke and soot that pose significant health hazards and contribute to millions of premature deaths each year. <a href=\"https://drawdown.org/\">Project Drawdown</a> estimates that switching to <a href=\"https://drawdown.org/solutions/clean-cooking\">clean cookstoves</a> could reduce emissions by 31.4 to 76.3 gigatons of CO2 equivalent by 2050, as well as deliver concomitant health and social benefits.</p><p>It could be impactful to identify and reduce the barriers to deploying and scaling technology that\u2019s already been developed to find potentially neglected ways to lower cost. Low-carbon and energy-efficient technology could help to tackle emissions from industry.</p><h3>Decarbonising heavy industry</h3><p>Since heavy industry <a href=\"https://www.givinggreen.earth/mitigation-research/decarbonizing-heavy-industry\">accounts for around 1/3</a> of global GHG emissions, decarbonising these complex production processes could be highly impactful. Heavy industry is usually relatively difficult to decarbonise compared to other emissions sources, and requires specific solutions.</p><p>High-leverage opportunities are mostly in corporate and government spending and decision-making (where <a href=\"https://industriouslabs.org/\">Industrious Labs</a>, a <a href=\"https://www.givinggreen.earth/top-climate-change-nonprofit-donations-recommendations\">Giving Green-recommended</a> non-profit, operates), but there are <a href=\"https://www.brookings.edu/wp-content/uploads/2021/06/FP_20210623_industrial_gross_v2.pdf\">significant</a> engineering <a href=\"https://www.energy.gov/eere/doe-industrial-decarbonization-roadmap\">bottlenecks</a> in this area. Decarbonisation is likely to have positive externalities, such as reduced pollution of the local environment.</p><h3>Carbon capture, removal and storage technologies</h3><p>Carbon capture, removal and storage technologies are <a href=\"https://energypost.eu/comparing-four-carbon-removal-scenarios-ipcc-iea-mckinsey-ngfs-and-policy-implications/\">essential to most net-zero scenarios</a>. The most promising technologies are the <a href=\"https://cdrprimer.org/read\">carbon dioxide removal</a> (CDR) technologies that take carbon dioxide straight out of the air. These are the most likely to become the negative emissions technologies that are necessary to tackle climate change. With low confidence, we believe that <strong>CDR is most important and neglected of these carbon capture, removal and storage technologies</strong>, and the current bottlenecks in development and deployment are very technical.</p><p>However, <strong>there is a risk of carbon capture being used for enhanced oil recovery (EOR) and other fossil fuel interests, which is probably net harmful</strong>. Read more about carbon capture technologies and their risks on the <a href=\"https://www.highimpactengineers.org/engineers-working-on-carbon-capture-removal-storage\">Carbon Capture, Removal and Storage</a> sub-page.</p><h3>Research into the most extreme risks of climate change</h3><p>Although there is much research on the likely outcomes from climate change, more work needs to be done to understand the <strong>most extreme risks of climate change</strong>, and how to reduce these risks.</p><p><strong>What are the different extreme risks?</strong></p><ul><li>Three of the most common ways people say climate change might directly cause human extinction include high temperatures, rising water, and disruption to agriculture. Climate damage is expected to be non-linear with increasing temperature, with risks being much higher at higher temperatures. Figure 9 shows projected damages from warming temperatures (these models have been criticised for likely underestimating climate damage, but the important takeaway is not the absolute level of climate damage, but the shape).</li></ul><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/n3vxBKgyBSnCWayAk/ruap5wvzp37goiy2oejk\" alt=\"Screenshot 2023-04-27 at 09.13.44.png\"></p><p>Figure 9: Expected climate damage with rising temperatures (<a href=\"https://www.nature.com/articles/508173a\">Revesz et al. 2014</a>)</p><ul><li>This non-linearity means that it is more important to shift from 5 to 4.5 degrees warming in a 5-degree scenario than it is to shift from 3 to 2.5 degrees in a 3-degree world. Even if a 5-degree world is less likely than the 3-degree one (e.g. because it is likely that all mainstream solutions succeed to land us in the 3-degree world, rather than the 5-degree world), working on solutions to prevent the worst from happening in a 5-degree world could be vital if current mainstream solutions fail (see <a href=\"https://founderspledge.com/stories/changing-landscape\">Founders Pledge</a>).</li><li>The worst-case climate scenarios would be devastating to human lives and the planet. However, <a href=\"https://80000hours.org/problem-profiles/climate-change/#could-extreme-climate-change-directly-lead-to-the-extinction-of-humanity\">80,000 Hours predicts</a> that this destruction, even at 13\u00b0C of warming, is unlikely to cause humanity to go extinct.</li><li>Even in the <a href=\"https://80000hours.org/problem-profiles/climate-change/#fn-22\">IPCC\u2019s Sixth Assessment Report</a>, which accounts for unknown unknowns and structural uncertainty (uncertainty in predictions because we don\u2019t know entirely how systems work), they find it unlikely that the sum of these uncertainties would be biased towards one direction (i.e. for every consideration that could increase warming, there are considerations that would decrease it, so they mostly cancel out).</li><li>However, 80,000 Hours have highlighted a few caveats:<ul><li><i>\u201cThe higher our emissions are, the further they get from the sorts of baseline assumptions the IPCC used to come to this conclusion. So if we\u2019re really very wrong about the amount of carbon emissions we\u2019re likely to emit, things could still get very bad (but it seems unlikely we\u2019re very wrong about that).\u201d</i></li><li><i>\u201cThere is much more uncertainty about how other things will change. For example, it\u2019s hard to predict how high sea levels will rise or how precipitation patterns will change (although even then we don\u2019t think these things will change in ways that increase the direct risk of extinction).\u201d</i></li></ul></li><li>If you think 80,000 Hours are missing key elements in their evaluation, it could be impactful to look into these uncertainties.</li></ul><p><strong>What can we do to mitigate these risks?</strong></p><ul><li>More research in the effects of climate change, specifically the effects of feedback loops, would allow a more thorough understanding of the risks.</li><li>Climate change is also likely to <a href=\"https://80000hours.org/problem-profiles/climate-change/#indirect-risks\">make other catastrophic risks worse</a>, e.g. by increasing migration, which could lead to instability. More research is needed to understand how climate change interacts with other potential catastrophic risks.</li></ul><h3>\u2018Greening\u2019 agriculture</h3><p>As discussed in the \u2018Agriculture\u2019 subsection within \u2018Causes of Climate Change\u2019, livestock and manure are the largest direct contributor to agricultural GHGs.</p><p><strong>We believe that developing </strong><a href=\"https://www.highimpactengineers.org/how-engineers-can-work-on-alternative-protein\">Alternative Protein</a><strong> options is </strong><a href=\"https://www.givinggreen.earth/us-policy-change-research/food-sector-emissions\">the most promising technical route</a><strong> to reducing agricultural emissions</strong>. Making alternative proteins more delicious, cheaper, and accepted could bring <a href=\"https://gfi.org/blog/agriculture-is-at-a-climate-crossroads-alternative-proteins-are-a-global-solution/\">other benefits</a> such as reducing farmed animal suffering, antibiotic resistance, soil erosion, agricultural water use, and potentially <a href=\"https://ourworldindata.org/land-use-diets\">freeing up to 3 billion hectares of land</a>.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/n3vxBKgyBSnCWayAk/htwlanbxtfcak99drgqa\" alt=\"ivy-farm-556X4ez5A5Y-unsplash.jpg\"></p><p>Figure 9: Ivy Farm is a cultivated meat focused on tackling the challenges of sustainable meat production. Photo by <a href=\"https://unsplash.com/@ivyfarm_tech?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Ivy Farm</a> on <a href=\"https://unsplash.com/s/photos/lab-grown-meat?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></p><p>Future animal consumption and production is likely to be in <a href=\"https://faunalytics.org/global-animal-slaughter-statistics-and-charts-2020-update/\">low- and middle-income countries</a> (LMICs), where alternative proteins are less ubiquitous and underdeveloped. This could provide a unique opportunity for engineers situated in LMICs to contribute.</p><p>See our page on <a href=\"https://www.highimpactengineers.org/how-engineers-can-work-on-alternative-protein\">Alternative Protein</a></p><p>There may be other ways to engineer agriculture to be greener, such as in <a href=\"https://innovate-eco.com/11-green-technologies-and-techniques-in-agriculture/\">this article</a>, but more research is needed to clarify how effective these would be.</p><h3>Ways you can contribute (your engineering skills) to climate without being an engineer</h3><p>As an engineer, your numerical skills and technical knowledge can be very valuable in other types of roles. These roles could include:</p><ul><li>Policy, advocacy and leadership</li><li>Climate finance/venture capital</li><li>Grantmaking</li><li>Donating to effective climate charities</li></ul><p>Read more about these in the subpage:</p><p><a href=\"https://www.highimpactengineers.org/engineers-climate-change-policy-venture-capital-grantmaking\">Other ways to contribute to climate change without being an engineer</a></p><h2>Risks, pitfalls, and things to keep in mind</h2><h3>Carbon capture for enhanced oil recovery</h3><p>As outlined in <a href=\"https://www.highimpactengineers.org/engineers-working-on-carbon-capture-removal-storage\">Carbon Capture, Removal and Storage</a>, <strong>we do not recommend working in carbon capture for </strong><a href=\"https://www.vox.com/energy-and-environment/2019/10/2/20838646/climate-change-carbon-capture-enhanced-oil-recovery-eor\">enhanced oil recovery</a> (EOR). Unfortunately, EOR is the most economically valuable (and <a href=\"https://www.cleanwateraction.org/publications/carbon-capture-and-release\">potentially most common</a>) use for captured CO2, so this could be difficult to select for when finding a job in this area.</p><h3>Greenwashing</h3><p>Most large engineering companies have at least one team working on sustainability. However, it\u2019s not always clear how much influence or impact you might have working in that particular role. <strong>Arguably most jobs in climate aren\u2019t all that effective</strong>, with <a href=\"https://hbr.org/2021/05/overselling-sustainability-reporting\">many companies routinely exaggerating or misreporting their progress</a>. To maximise your likelihood of having an impact, it may be a good idea to do your research into the company to understand the true impact of a particular role there.</p><h3>Replaceability and counterfactual impact</h3><p><a href=\"https://www.fastcompany.com/90306556/most-millennials-would-take-a-pay-cut-to-work-at-a-sustainable-company\">70% of 1000 employees at large US companies</a> are more likely to work for a company with a strong environmental policy (similar to the 65% of 2000 respondents to Unily\u2019s <a href=\"https://www.unily.com/insights/guides/future-of-the-sustainable-workplace-in-the-age-of-covid-19-and-climate-change\">2020 report</a> on UK-based office workers) and nearly half of all respondents and three-quarters of millennial workers would be willing to accept a smaller salary to work for a company that\u2019s environmentally responsible. In our experience, most engineers keen to make a social impact first land on climate change as an area in which to do good. This means that it is likely that \u2018green jobs\u2019 are in higher demand, so the next best engineer for the job is likely to do just as good a job as you would have done. <strong>As a result, your </strong><a href=\"https://80000hours.org/articles/counterfactuals/\">counterfactual</a><strong> impact in climate change is unlikely to be very high, and you\u2019re likely to be relatively </strong><a href=\"https://80000hours.org/2014/07/what-does-economics-tell-us-about-replaceability/\">replaceable</a><strong> in climate change work relative to other cause areas. </strong>However, if you have an especially good <a href=\"https://80000hours.org/articles/personal-fit/\">personal fit</a> for the position, then you\u2019re <a href=\"https://80000hours.org/2015/07/replaceability-isnt-as-important-as-you-might-think-or-weve-suggested/\">probably not as replaceable</a>.</p><p><i>We are grateful to ET for their input and feedback. All remaining mistakes are my own.</i></p>", "user": {"username": "Jessica Wen"}}, {"_id": "sK7neZ9rHGEL5JP7q", "title": "EA Anywhere Slack: consolidating professional and affinity groups", "postedAt": "2023-05-08T13:57:46.454Z", "htmlBody": "<h3>Summary</h3><p>In an effort to improve communication infrastructure, the&nbsp;<a href=\"https://join.slack.com/t/eavirtualmeetupgroup/shared_invite/zt-nnm9fyfp-kPS1R98d~SW5soPRcfegoQ\"><u>EA Anywhere Slack workspace</u></a> has recently undergone major changes to accommodate several professional and affiliation Slack workspaces like EA Entrepreneurs, EA Global Discussion, EA Creatives &amp; Communicators, and more. The project was initiated by Pineapple Operations because the old structure was inefficient and overwhelming -&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jRJyjdqqtpwydcieK/ea-could-use-better-internal-communications-infrastructure\"><u>others</u></a> have made&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/eQgKyCQW4u5CCWHXt/idea-curated-database-of-quick-win-tangible-attributable#Slack_and_Discord_consolidation\"><u>similar observations</u></a>. We believe making it easier to track discussions and reducing the number of workspaces will increase activity, avoid information loss and prevent duplicated communications.&nbsp;</p><p>If you have a Slack workspace you\u2019d like to merge with EA Anywhere\u2019s,&nbsp;<a href=\"https://forms.gle/uRKLWvrT13S42rzq6\"><u>reach out to us</u></a>!</p><h3>Why consolidate?</h3><ul><li>With too many workspaces, the infrastructure of the EA movement becomes increasingly overwhelming and confusing, and it\u2019s difficult to keep up with new workspaces. Having a central Slack gives people access to a broader range of communities at once.</li><li>People don\u2019t have the time or energy to check multiple Slacks, which results in low activity. Some discussions just don\u2019t reach the critical mass, and valuable connections are not happening.</li><li>Most of the workspaces were on a free plan that hid messages and files older than 90 days. Consolidation around a few paid workspaces prevents groups from losing historical information.</li><li>There is overlapping membership between Slacks (we estimate between 10-50%), so consolidation makes it easier to track communications.</li></ul><p>These reasons were true for the dozens of Slack workspaces we identified with low activity and limited facilitation.</p><h3>Why EA Anywhere?</h3><p><a href=\"https://forum.effectivealtruism.org/groups/YeW2gwh4gHexYQBjs\">EA Anywhere</a> is an online discussion space for the global EA community and a touchpoint for people without local groups nearby. It plays an important role in supporting other virtual ecosystems in EA: we host EAGxVirtual conferences, provide support and share knowledge with other online groups.&nbsp;</p><p>EA Anywhere Slack is on a paid Pro plan and has active facilitation from a full-time community organizer, which makes it a good choice for this project. We can&nbsp;<a href=\"https://docs.google.com/document/d/1wWW_shnEZyi011l-znoFFdRiIiUZi2S37DBip6xIN20/edit\"><u>provide support</u></a> for groups joining the space, including events promotion, Zoom accounts, Slack integrations, and knowledge-sharing calls.&nbsp;</p><p>There is a demand for informal conversation spaces and networking that the EA Forum doesn\u2019t currently provide. We are inspired by Slack-based communities that thrive and create value for thousands of members without becoming too overwhelming.</p><h3>Progress to date</h3><p>We reached out to workspace owners with our proposal and received positive feedback. In most cases, we merged the users and message history.</p><p>List of Slack workspaces we have already merged or consolidated (thanks to the admins of these groups!):</p><ul><li>EA Global Discussions</li><li>EA Entrepreneurs</li><li>EA Creatives &amp; Communicators</li><li>EA Generalists</li><li>EA Housing</li><li>EA Tech Network</li><li>Public Interest Tech</li><li>EA Project Management</li><li>EA Supply Chain Logistics</li><li>EA Math and Physics</li><li>Product in EA</li><li>Effective Environmentalism</li></ul><p>We already see the benefits of increased coordination:</p><ul><li>Members are engaging with other groups and projects that have been locked into small workspaces.</li><li>Members are more willing to ask questions and ask for advice. Activity in the&nbsp;<i>#all-questions-welcome</i> channel increased four-fold compared to the previous three-month average, with both new and former members engaging in discussions.&nbsp;</li><li>Organizers have an easier way to promote opportunities and events.</li></ul><p>\ud83d\udcc8 As of April 30th, a month after the merger, the initial hype subsided but the activity is still twice as high:&nbsp;</p><ul><li>90 members who posted weekly (44 before)</li><li>360 weekly active members (210 before the merger)</li></ul><p>We will continue using Slack Analytics to track the activity and send a follow-up user survey in Sep 2023.</p><h3>Next steps</h3><p>If you are interested in starting something, instead of creating an independent workspace, use the&nbsp;<a href=\"https://join.slack.com/t/eavirtualmeetupgroup/shared_invite/zt-nnm9fyfp-kPS1R98d~SW5soPRcfegoQ\"><u>EA Anywhere Slack</u></a>. Anyone can create a private channel for your group and/or project. However, if you need a public channel, read&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/fJb52uWaFKydFQKR7/how-to-make-slack-workspaces-welcoming-and-valuable\"><u>this post</u></a> (applies to channels as well as workspaces) and reach out to Sasha.</p><p>Examples of groups EA Anywhere would like to support:</p><ul><li>Cause area-specific channels and/or people willing to help manage them</li><li>Professional groups e.g. career paths</li><li>Meta-groups that support the community e.g. digital nomads, housing/couchsurfing, etc.</li></ul><p>If you are unsure whether or not your group belongs in EA Anywhere, or should you start a separate Slack, reach out to us during<a href=\"https://evt.to/ahmaoioow\"><u> Office Hours</u></a> or&nbsp;<a href=\"https://calendly.com/sashaberezhnoi/\"><u>book a call</u></a>!</p><p>We are currently still in the process of merging with other Slacks and reaching out to more workspaces for consolidation. If you already have a Slack workspace you\u2019d like to merge with EA Anywhere\u2019s,&nbsp;please, <a href=\"https://forms.gle/uRKLWvrT13S42rzq6\">submit a short form</a>!&nbsp;</p><p>&nbsp;</p><p><i>Acknowledgments: Thank you to David R, David N, Elika, Jeroen, Simon, Mart, and several others for their cooperation and feedback in making this initiative possible!&nbsp;</i></p><p><i>Many thanks to Dion Tan and Marisa Jurczyk for the feedback on this post!</i></p>", "user": {"username": "Alex_Berezhnoy"}}, {"_id": "7FH4fy5iFmDqqDgJF", "title": "Analysing Individual Contributions to the Metaculus Community Prediction", "postedAt": "2023-05-08T22:58:41.541Z", "htmlBody": "<p><i>Crossposted from the Metaculus Journal </i><a href=\"https://www.metaculus.com/notebooks/16786/analysing-individual-contributions-to-the-metaculus-community-prediction/\"><i>here</i></a><i>.</i></p><h2>Summary</h2><ul><li>This post investigates the contribution of individual forecasters and their predictions to the Metaculus Community Prediction (CP).</li><li>Generally, individual contributions have only small effects, but in about 10% of cases, they can change the Brier score of the CP by more than 2%.</li><li>The <i>average contribution</i> improves the CP, whereas the <i>average user</i> makes the CP slightly worse.&nbsp;</li><li>Beneficial contributions are related to good Brier scores, but even predictions with 'bad' Brier scores can make positive contributions.</li><li>Predictions far from the current CP probably tend to lead to larger contributions, but the relationship is weak and even predicting the current CP can have a very large effect.&nbsp;</li><li>Predictions that make beneficial contributions tend to be more confident than the CP at the time a user makes a prediction.</li><li>All code is available here: <a href=\"https://github.com/nikosbosse/Metaculus-data-analyses\">https://github.com/nikosbosse/Metaculus-data-analyses</a></li></ul><p>&nbsp;</p><p><strong>Conflict of interest note</strong></p><p>I am an employee of Metaculus. I think this didn't influence my analysis, but then of course I'd think that, and there may be things I haven't thought about.</p><h2>&nbsp;</h2><h2>Introduction</h2><p>The wisdom of the crowd is a known phenomenon in forecasting. Ask a lot of forecasters (all else being equal), and you will likely get a better forecast. On average, an additional forecast should therefore make a positive contribution and improve the combined forecast.&nbsp;</p><p>However, not all contributions are made equal.&nbsp;</p><figure class=\"image image_resized\" style=\"width:66.47%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/ltj6dvnwtvvjaqt5ko48\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/jd815s3qyxhcqquxwpfd 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/i7ok0jxrgb3wsidxdwwb 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/mxlepo46ilfghqscwz2g 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/afp5psowcilyctayiw1x 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/g7pe8tdrfenrc6fzkc1a 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/n1wzkawrrustbqhb4fwf 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/kt23lghoevm7fjslcp2e 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/hmonj3a195hwci1gx8u0 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/inujbuzpyggosiff3xje 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/awyi5mdqlxnpcbukxbho 1118w\"></figure><p>Forecasters can obtain very good scores without contributing much (e.g. by just copying the crowd forecast). On the other hand, a user can make a really bad forecast and still make an important contribution. Imagine a forecaster who looks at the current crowd forecast for a question, which is around 90%. They might say \"hm that's much too high\" and predict 5% instead. That might not be a good forecast after all, but it could be <i>directionally</i> correct and provide important information to the crowd forecast.&nbsp;</p><p>Using data from the prediction platform <a href=\"https://metaculus.com\">Metaculus</a> this post will look at how much individual forecasts and forecasters contribute to the Metaculus Community Prediction (CP).&nbsp;</p><h2>Methods</h2><p>To assess the contributions a user made to a crowd forecast I essentially recomputed the Metaculus Community Prediction (CP) <i>without</i> that user and compared it to the regular CP <i>with</i> that user. Here is a more detailed overview of what I did:&nbsp;</p><ul><li>For every binary question&nbsp;<ul><li>for every user<ul><li>I computed the Community Prediction with that user removed (meaning that I removed all forecasts / updates from that user and consider the CP as though the user had never contributed to it),&nbsp;</li><li>computed the Brier score for the Community Prediction without that user (meaning the Brier score averaged over the whole time the question was active)</li><li>and compared it to the Brier Score for the Community Prediction with that user (i.e. the regular Community Prediction). I looked at&nbsp;<ul><li>the absolute difference in Brier scores&nbsp;<br>(i.e.&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\text{Brier score of the CP with the user} - \\text{Brier score without the user}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Brier score of the CP with the user</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mtext MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">Brier score without the user</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>), as well as</li><li>Relative Brier scores&nbsp;<br>(i.e.&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\text{Brier score of the CP with the user} / \\text{Brier score without the user}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Brier score of the CP with the user</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">Brier score without the user</span></span></span></span></span></span></span>)</li></ul></li></ul></li></ul></li><li>The <a href=\"https://forecasting.wiki/wiki/Brier_score\">Brier score</a> is a proper scoring rule (it can't be cheated). Lower scores are better. It is computed as the squared difference between predicted probability and outcome (which can be either 0 or 1). If I give a 70% probability that it rains tomorrow and it does indeed rain, then my Brier score would be&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(1 - 0.7)^2 = 0.21\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.7</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.21</span></span></span></span></span></span></span>. Had it not rained, my Brier score would have been&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(0 - 0.7)^2 = 0.49\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.7</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.49</span></span></span></span></span></span></span>.</li><li>The Metaculus <a href=\"https://www.metaculus.com/help/faq/#community-prediction\">Community Prediction</a> is a recency-weighted median of all predictions.&nbsp;</li><li>I removed questions that had an average community prediction of over 95% or under 5%. Gains in accuracy on these are somewhat hard to compare against gains on other questions.&nbsp;</li><li>I also removed questions with fewer than 5 forecasters.&nbsp;</li></ul><h2>Results (and some discussion)</h2><h3>Overview</h3><p>The following plot shows how Brier scores changed due to the contribution of a user. Negative values (for differences) or values smaller 1 (for the relative score) means that the Brier score has improved due to the contribution made by a particular user.&nbsp;</p><p>Note that both metrics, the difference in Brier score and the relative Brier score can be a bit tricky to interpret. The problem with the differences is that they don't take the overall baseline into account. However, it matters whether a forecast was originally 0.5 or 0.9. Changing a forecast for an event that later happens (resolution = 1) from 0.5 to 0.51 would result in a reduction (improvement) in Brier score of&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(1 - 0.51)^2 - (1 - 0.5)^2 \\approx -0.01\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.51</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.5</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u2248</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.01</span></span></span></span></span></span></span>, whereas a change from 0.9 to 0.91 would only result in a change of&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(1 - 0.91)^2 - (1 - 0.9)^2 \\approx -0.002\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.91</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.9</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u2248</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.002</span></span></span></span></span></span></span>. Relative scores may be a bit more intuitive, but they are tricky as well. For relative scores, the result can get arbitrarily large when the denominator shrinks. For&nbsp;example, predicting 0.98 instead of 0.99 leads to a relative score of&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(1 - 0.98)^2 / (1 - 99)^2 = 4\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.98</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">99</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">4</span></span></span></span></span></span></span>, meaning that the resulting score is four times worse.&nbsp;</p><p>In this case, however, histograms for both look very similar:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/wcdbikjhfg4wajf0x4yc\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/lr5cwyqnlw9ltrop6a0b 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/yo2lxfkivd9zvx96ceoe 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/usphctbvlog2xok7jdjo 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/ajrctntgccat010l3if4 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/y2y3m7vkgoawykyjitev 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/izxi8ljfr4oofbia9ivj 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/vlwzeasbhq03tmmedtlz 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/avs79roqieu4axuc6teb 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/nlphh8rt6t9uvdlpnt8u 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/joe2xvnry95wffdvugvw 2100w\"></figure><p><i><strong>Figure 1</strong>: Histograms showing an overview of all individual contributions for all users across all questions. A: Differences in Brier scores (Brier score of CP with user included - Brier score without that user). Negative means improvement. B: relative Brier scores (i.e. ratio of Brier score for the Community Prediction with the user included over the Community Prediction without that user). Values smaller than 1 mean an improvement.</i></p><p>We see that individual contributions are centered around 0 (or 1 for relative scores) and tend to be rather small in most cases. More than 90% of all individual contributions don't move the resulting Brier score by more than 2%, as we can see in the next plot. But then again: almost 10% of all contributions do!</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/hzwsc3m3yfwekeaykxz7\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/hb67ubwyabw0oks7shgu 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/koapeinrv8gqpup8ljtd 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/j2rgjwv7qhirdk8oo55s 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/ih4sgsidkdjhpnnmjqqe 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/i7zo96crku2mh237dgtq 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/gwfmewryobzxigpcbfqm 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/unjovgxyykmhimuw7oh1 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/gwitfb7qf55whydhjrig 1920w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/kwed9uh0fks2cacjutht 2160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/vg7akc9smjnt0f1e1r2t 2400w\"></figure><p><i><strong>Figure 2</strong>: Cumulative proportion of the absolute or relative changes in Brier score. The curve essentially says: what percentage (number on the y-axis) of all observed values are smaller than x (number on the x-axis).&nbsp;</i></p><p>Here is a numerical summary of the results. Both the mean and the median contribution are negative, meaning that they on average improve the Brier score of the CP. I'm not showing summary statistics for relative scores as I feel those may be a bit misleading. When you average ratios all sorts of funny things can happen. As shown above, ratios can get quite large if the denominator is small and I'm not confident that the averages are very meaningful.&nbsp;</p><figure class=\"table\"><table><tbody><tr><td><strong>Description&nbsp;</strong></td><td><strong>min</strong></td><td><strong>mean</strong></td><td><strong>median &nbsp;</strong></td><td><strong>max &nbsp;&nbsp;</strong></td><td><strong>sd</strong></td></tr><tr><td>Difference in Brier scores&nbsp;</td><td>-0.44</td><td>-1.9&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\times 10^{-5}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span></span></span></span></span></span></span></span></span></span>&nbsp;&nbsp;</td><td>-3.6&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\times 10^{-16}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">16</span></span></span></span></span></span></span></span></span></span></span>&nbsp;&nbsp;</td><td>0.52</td><td>0.0036</td></tr></tbody></table></figure><p><i><strong>Table 1</strong>: Summary statistics for contributions (i.e. changes in the CP) across all individual forecasts. Lower values are better.&nbsp;</i></p><p>&nbsp;</p><p><strong>Relationship between contributions and the number of forecasters present</strong></p><p>All of this is obviously confounded by the number of forecasters contributing to a given question. If you are one of ten forecasters, chances are you will move the Community Prediction quite a bit. If there are already hundreds of forecasters, your expected contribution will be smaller.&nbsp;</p><p>In the following plot, every point represents a single contribution by a user to a question. We see that the contribution from a single forecaster tends to be smaller for questions with a larger number of forecasters. The bottom row of the plot shows the same thing, but with the x-axis on the logarithmic scale in order to show more clearly what happens for lower number of forecasters.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/c2shqbnnuebdyz4ywubj\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/h6af3fr6n1qextcyvzog 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/c483tl0rmwvffxdssiq5 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/vxdqtxyaum5agmnzlncq 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/zl3o1lm2lbemiokwgoff 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/dwtyv6nbnghymd2gjmyc 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/az0byzonffvuiis3qhdf 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/s3yjqgdkryvsky9jhq7w 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/og0osccf3rwu4ekzrqpb 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/xzd5tvibl70mzq5c61xy 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/dr6dbuv64itafghh9iaz 2100w\"></figure><p><i><strong>Figure 3</strong>: Contributions / relative Brier scores for different numbers of forecasters who contributed to a question. Each point represents one individual contribution by a user. Bottom row: Same as top row, but with the x-axis on a logarithmic scale. The shade overlayed shows the density of observations- blue means there are a lot of observations, white means there are not so many observations.&nbsp;</i></p><p>We clearly see that contributions tend to become much smaller as the number of forecasters increases. But even for more than a 100 forecasters it is possible for individual forecasters to singlehandedly improve or deteriorate scores for the CP by 5-10%.&nbsp;</p><h3>Individual forecasters</h3><p>The interesting question is of course: can we identify (ideally beforehand) when and how users make positive or negative contributions? I found this quite difficult, so consider the following as a humble attempt to establish some basic understanding.&nbsp;</p><p><strong>Overview</strong></p><p>In order to determine contributions by individual users, I averaged individual contributions (i.e. changes in Brier scores) for a given user across all questions they forecast on. For simplicity, I'm not showing the relative Brier scores here, but overall they don't look a lot different. The following plot shows an overview of average contributions by Metaculus users.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/djbbe7l7eqxj1jheqxr8\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/uzdl7qnvxg6auckzbmjb 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/bnvt0z4prhr26wdaugfr 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/rccofigizezqxbp14e2g 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/iu06raffxyngbfyjtiu3 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/fcitkrrfxmgbtrknbape 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/vkjbynwjjqtcm46qcsst 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/hsevf7shrzggqhhfjfh2 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/qncwy2slmtduddt8du8v 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/ue8n5ubpchmyyb0shtcd 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/pxmptltd6fbnxb8hh3pl 2100w\"></figure><p><i><strong>Figure 4</strong>: Histograms showing average contributions per user (negative means improvement).&nbsp;</i></p><p>We see that average contributions by individual users are not very large. The following table again provides a numerical summary of the results.&nbsp;</p><figure class=\"table\"><table><tbody><tr><td><strong>Description&nbsp;</strong></td><td><strong>min</strong></td><td><strong>mean</strong></td><td><strong>median &nbsp;</strong></td><td><strong>max &nbsp;&nbsp;</strong></td><td><strong>sd</strong></td></tr><tr><td><p>Avg. contribution&nbsp;</p><p>(diff in score) per user&nbsp;</p></td><td>-0.075</td><td>1.2&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\times 10^{-5}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span></span></span></span></span></span></span></span></span></span></td><td>6.9&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\times 10^{-18}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">18</span></span></span></span></span></span></span></span></span></span></span></td><td>0.15</td><td>0.0019</td></tr></tbody></table></figure><p><i><strong>Table 2</strong>: Summary statistics for average contributions (i.e. changes in the CP) per users. (Note that this is the min, mean etc. of an average per user). Negative values mean improvement.&nbsp;</i></p><p>Interestingly, the average contribution by the average Metaculus user is larger than 0, which means that the average user is making the CP slightly worse. Note that this includes a lot of users who have only forecast a few times. As we've seen above, the average <i>contribution </i>improves the CP. This mismatch suggests that an important part of the heavy lifting is done by the more prolific forecasters.&nbsp;</p><p>&nbsp;</p><p><strong>Contributions and individual performance</strong></p><p>The natural place to start an investigation into which users contribute most is to look for a relationship between individual performance and average contribution. The following plot shows average contributions per user vs. average Brier scores for that user.&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/vxac2xrpk44si7yp0laf\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/waqawyusic3fkr9nhz02 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/rcngdka9lewrl2pm60bs 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/lebcyuy7b5tknpl3dqtt 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/urmvo47scf79awb5qhai 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/bkac30sihoczi9hqonc6 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/eodlfusmd8gfjaapnbty 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/mypwq5fx8u8bfyru36rq 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/gflkiolgd3r76mbxdd2b 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/gdqnoqpfzrl2pp0zrk3e 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/eagcbwtmppoklt6wit2r 2100w\"></p><p><i><strong>Figure 5</strong>: Average contributions to the CP by a user vs. their average Brier score. Lower means better. Every point represents a single user. &nbsp;The shade overlayed shows the density of observations- blue means there are a lot of observations, white means there are not so many observations.&nbsp;</i></p><p>It seems that forecasters with better scores also tend to contribute more to the CP. However, there still is a surprising number of users with bad scores and good contributions and vice versa. For an average Brier score of 0.25 (which is on average equivalent to predicting 50% on every question), there seem to be about as many positive as negative contributions to the CP.&nbsp;</p><p>To obtain a more complete picture, let's look at the relationship between contribution to the CP and user score for all individual predictions (rather than averages per user). This is shown in the following plot.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/ziiw8p7drz8wt0iqtr3f\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/dl8bejwq22pam6hzklat 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/ttj1hyjydkw0w1o6vcu9 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/sqoummk6mtp7zwaj6ykg 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/nmuygfrjc7opfejw0rjb 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/nwa69hhnpsbiamxrdhxc 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/vcsah4ehvwkeqpdtgtp0 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/hxp9epsgqlqdcohijwxq 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/qabsxlowj7pdqypjj9qz 1920w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/bx0tkbf58vuf68hlndoz 2160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/jdx5amr7rezmixgefbpg 2400w\"></figure><p><i><strong>Figure 6:</strong> Contributions to the CP from individual forecasts and Brier scores associated with that forecast. Every point represents the contribution / score from the forecast (+ updates) of a single user on a single question. The shade overlayed shows the density of observations- blue means there are a lot of observations, white means there are not so many observations.&nbsp;</i></p><p>We see that there is clearly a relationship between the contribution from a forecast and the associated Brier score. At least, it's pretty clear that forecasts with a <i>very &nbsp;'bad'</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzrmkrflp3yb\"><sup><a href=\"#fnzrmkrflp3yb\">[1]</a></sup></span>&nbsp;score usually also don't contribute much. But for anything less than that (and even for forecasts which receive a Brier score &gt; 0.25), the situation is much less clear-cut.&nbsp;</p><p>The difference between the left and the right panel can be explained by the fact that you can receive a 'good' score and still make a negative contribution. For example, when the Community prediction is at 97% and you predict 90% and the event happens, you'll get a decent score even though you probably harmed the CP.&nbsp;</p><p>&nbsp;</p><p><strong>Contributions and number of questions forecast on</strong></p><p>Earlier, we saw a mismatch between the <i>average contribution</i> (which was &lt; 0 and therefore improved the CP) and the <i>average contribution by the average user</i> (which was &gt; 0 and therefore harmful to the CP). I concluded that there must be some relationship between contributions and the number of questions a user has forecast on.&nbsp;</p><p>The following plot shows average contributions, as well as average Brier scores per user as a function of the number of questions they forecast on. The top row shows the average (every point represents a single user), whereas the bottom row shows the cumulative average, i.e. the development of the average over time (every line represents a user).&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/zu6nbmicdehiuaxocher\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/ekcjibqk1nlokp21ype5 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/jhdskcpl3fu0785nrc2o 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/iw2vqg7by3hqlz4lrrcb 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/snbqazbzafzvrctzejr2 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/lzlkz07arqyr8whmf8eo 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/vjb8jchmswvrngsypknz 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/zniligqbjrqbglkvuef3 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/bbwg2eaczqajs9f7xnqp 1920w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/zxspvczw6qhauwh6odsw 2160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/xaxpdrgqqvj7vmcnmkt0 2400w\"></figure><p><i><strong>Figure 7</strong>: Avg. contributions and Brier scores as a function of the number of questions forecast on. Top: average, Bottom: cumulative average (development over time). The shade overlayed shows the density of observations- blue means there are a lot of observations, white means there are not so many observations.&nbsp;</i></p><p>Both for average contributions as well as Brier scores, we see a clear regression to the mean as user forecasts more over time. It's a bit hard to read from the plots, but I <i>think</i> I see average Brier scores for most users improve as they forecast more. This seems a bit less clear for average contributions. However, most of the prolific forecasters on average seem to make positive contributions.&nbsp;</p><p>The following table shows the average contributions for only those users who have forecast on at least 100 questions. Both the mean and the median are negative (indicating beneficial contributions), but still small in absolute numbers. Both min and max are lower (better) for the power users and the standard deviation is smaller.&nbsp;</p><figure class=\"table\"><table><tbody><tr><td><strong>Description&nbsp;</strong></td><td><strong>min</strong></td><td><strong>mean</strong></td><td><strong>median &nbsp;</strong></td><td><strong>max &nbsp;&nbsp;</strong></td><td><strong>sd</strong></td></tr><tr><td><p>Avg. contribution&nbsp;</p><p>(diff in score) per&nbsp;</p><p>power user&nbsp;</p></td><td>-0.0021&nbsp;</td><td>-1.8&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\times 10^{-5}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span></span></span></span></span></span></span></span></span></span></td><td>-2.5&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\times 10^{-6}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">6</span></span></span></span></span></span></span></span></span></span></span></td><td>0.0016</td><td>0.0003</td></tr></tbody></table></figure><p><i><strong>Table 3</strong>: Summary statistics for average contributions (i.e. changes in the CP) per users for those users who have forecast on at least 100 questions. (Note that this is the min, mean etc. of an average per user). Negative values mean improvement.&nbsp;</i></p><p>For completeness, here is also an overview of the cumulative distributions for power users (i.e. the sum, instead of the average of all contributions made by the power user):&nbsp;</p><figure class=\"table\"><table><tbody><tr><td><strong>Description&nbsp;</strong></td><td><strong>min</strong></td><td><strong>mean</strong></td><td><strong>median &nbsp;</strong></td><td><strong>max &nbsp;&nbsp;</strong></td><td><strong>sd</strong></td></tr><tr><td><p>Sum contributions&nbsp;</p><p>per power user&nbsp;</p></td><td>-0.54&nbsp;</td><td>-0.009</td><td>-0.00034</td><td>0.228</td><td>0.084</td></tr></tbody></table></figure><p>We saw earlier that there is a relationship between contribution to the CP from a user forecast and the associated Brier score, and also that that relationship is not clear cut. Let's dive a bit deeper into this and look at relationship between contributions and the distance between forecast and CP.&nbsp;</p><p><strong>Contributions and difference to the CP</strong></p><p>We would expect to see a relationship between the amount a person contributes and how far away from the current CP their prediction is. This is shown in the following plot<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzsnx908xpv\"><sup><a href=\"#fnzsnx908xpv\">[2]</a></sup></span>.&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/ma2zryqmxqbyogkglbnj\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/skl0pgrmvuaohbas7a8u 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/hox3ho686whaebei3apb 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/zzbh3dd8kqdra4cmrvzw 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/dm3a8sxq33munhwmhvep 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/on6xdx8bhasfuumjglp6 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/yledels9cllzlfsvk97l 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/fl5fp9k05dujz1uv2epo 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/peflifv54rqd9somsoxg 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/hfdoi0jzh08dnok5cafv 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/rojoy3wnbfb0yqbvo1bl 2100w\"><i><strong>Figure 8</strong>: Contributions and relative Brier scores vs. the difference between prediction and CP the user saw (If a user updated multiple times, then that's the average distance). Every point represents a single forecast. The shade overlayed shows the density of observations- blue means there are a lot of observations, white means there are not so many observations.&nbsp;</i></p><p>As with the relationship between contributions and scores previously, the relationship between contribution and distance from the CP is not clear-cut. A few things stand out from this plot:</p><p>Firstly, predictions very close to the current CP can have a large effect. I think that's because predicting exactly the CP makes it a bit more sticky and slower to update in the future. This can be good, or bad. I must admit though, that I'm genuinely surprised about the size of the effect (i.e. predicting the median can affect scores by more than 10%).&nbsp;</p><p>Secondly, contributions far away from the CP don't seem to matter that much. I think this is in part a visual effect that is due to the fact that there just aren't that many predictions that far away from the CP (and therefore also not that many outliers to be seen on the plot). In addition to that I think this is related to the fact that the CP is computed as a recency-weighted median. This median can be a bit jumpy at times and therefore even if you predict very far away from the CP you might not be changing it much.&nbsp;</p><p>Thirdly, there seems to be a very subtle effect that people who predict lower than the CP tend to make more beneficial contributions than those who predict higher than the current CP. That one is interesting. I think it might be related to whether or not a forecast is more or less confident than the CP (around 60-70% of all binary questions on &nbsp;Metaculus resolve negatively).&nbsp;</p><p>The next plot shows by how much a prediction is more or less confident than the CP. A prediction is more confident than another one if it is closer to 0 (or 1, whichever is nearer). For example, a forecast of 0.2 is more confident than a forecast of 0.4, and a forecast of 0.95 is more confident than one of 0.8.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/ffo1oypfzf6m2poyxbyo\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/izk9gxct92zksaam7ktz 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/co9z6rvkuqh2nzb88woj 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/nrjixmahukpfjmnv25vz 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/sghqlo0fma3wsrhgulp9 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/vdcfowxw6fuarqynypiq 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/stc9nn16wio6keq5eeje 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/fm6aovptdsk1jqldzrfb 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/xe6jz6l6bzo85djnonnt 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/bzc3tybkkihizb91xapv 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/a7lbjcfi6rxbqhqfdisx 2100w\"></figure><p><i><strong>Figure 9</strong>: Contributions and relative Brier scores vs. how confident a prediction was relative to the CP that the user saw. Here I operationalised confidence as \"closer to 0 or 1 respectively\". Again, if a user updated multiple times, then that's the average difference in confidence. The shade overlayed shows the density of observations- blue means there are a lot of observations, white means there are not so many observations.&nbsp;</i></p><p>We don't see much when looking at differences in Brier scores, but looking at relative Brier scores reveals an interesting pattern. It seems like predictions that are more confident than the current CP tend to improve it, while predictions that are less confident seam to be harmful. My interpretation is that this an interesting way to come to a known result: you can often improve a binary prediction by 'extremising'. Extremising means that you slightly push a consensus forecasts towards 0 or 1 (whichever is nearer). The idea is that if several people (instead of just one) independently come to the conclusion that something is say 90% likely, then that should make you more confident that it's actually going to happen.&nbsp;</p><p>As an aside, there is also a relationship between confidence and Brier scores. The shape of the plot is due to the constraints imposed by the relationship between confidence and Brier scores. For example, you can only get a Brier score of 0.25 if you predict 50% while the CP is at either 100% of 0%.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/v1najazc36ekha7bscsp\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/pqqigbzqtllhcyd9cgtf 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/n9bbatxjmtzr3yg9ii73 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/vmsnnxvazlwzf7a80bav 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/ohp9nbprkaadsxozg8nd 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/rgh1fkzkazci89gblqhk 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/a14l1yw6nuhywo6s7zcd 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/odaezdemgxlrl65dmd9n 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/k1hvgoqfkufo9ne7tmeg 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/lyuw7uzkaspnmjelqade 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/tditqhcntyfv0y1psuaf 2100w\"></figure><p><i><strong>Figure 10</strong>: User Brier score vs. how confident a prediction was relative to the CP that the user saw. Note that the Brier scores here are different from the ones used throughout the rest of the post: Points show Brier scores for a single forecast made by a user. If a user makes three forecasts one a single question, this will show up as three dots. In all other plots, Brier scores are a single time-weighted score for the entire activity of a user on a question (including all forecast updates). The shade overlayed shows the density of observations- blue means there are a lot of observations, white means there are not so many observations.&nbsp;</i></p><p>There is one last thing that I found interesting related to the difference between the CP and prediction: its evolution over time as users forecast more. The following plot shows the absolute difference between prediction and CP across the forecasts users made (every line represents one user).&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/ru1sfyrbptly14ongp70\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/xut13gvpg8rsjoyjf2tq 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/odkc0wbiwznlhpyrtcy0 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/vsxqcthbzvmzj9tslec5 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/bypwihuiscmkrkqqyche 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/fn1dqurgjkcd0pur2o3g 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/sqt50ukhk8af3lvno63p 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/wgxkrq7gk837uvr76zmd 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/fnxlx50zirsciawpl0fq 1920w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/dkdbv0n8a9rviac0lwu1 2160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7FH4fy5iFmDqqDgJF/ggaocectmloul0hxvi7d 2400w\"></figure><p><i><strong>Figure 11</strong>: User Brier score vs. how confidence a prediction was relative to the CP that the user saw.&nbsp;</i></p><p>It seems like users tend to predict closer to the CP as they forecast more. It also seems like (maybe?) there are fewer instances where users predict <i>exactly</i> &nbsp;the CP. I'm not entirely sure about that though - the plot might be a bit misleading, as there are fewer lines for users who forecast on 900 questions than for those with 50.&nbsp;</p><p>&nbsp;</p><h2>Conclusions</h2><p>All of this should be considered \"exploratory data analysis\". I haven't run any statistical analyses, so this is more of an attempt to provide a high-level overview of the data. I think it would be interesting to run some regressions or similar analyses and see what comes out.&nbsp;</p><p>My overall impression is that the data is quite noisy - most forecasts don't contribute very much, but then again some surprisingly do. I was particularly surprised about how much of an effect simply predicting the exact value of the current Community Prediction could have. I think this somewhat unpredictable behaviour is to a large extent due to the way the Metaculus Community Prediction is calculated. The recency-weighted median can be a bit jumpy or behave in unexpected ways<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc8br3a87pc7\"><sup><a href=\"#fnc8br3a87pc7\">[3]</a></sup></span>. I would assume that other aggregation mechanisms, such as e.g. a <a href=\"https://forum.effectivealtruism.org/posts/sMjcjnnpoAQCcedL2/when-pooling-forecasts-use-the-geometric-mean-of-odds\">geometric mean of odds</a>, might lead to different results.&nbsp;</p><p>&nbsp;</p><p>All code is available here: <a href=\"https://github.com/nikosbosse/Metaculus-data-analyses\">https://github.com/nikosbosse/Metaculus-data-analyses</a>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzrmkrflp3yb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzrmkrflp3yb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To be precise, you can't really conclude that a forecast is \"good\" or \"bad\" based on a single realisation. If the true probability for an event is 90% and you provide an ideal forecast of 90%, then 1 out of 10 times on average you will receive a bad score.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzsnx908xpv\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzsnx908xpv\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Presumably we should measure distance in odds-space rather than just taking the difference between the prediction and CP. This, however, is for now left as an exercise to the reader.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc8br3a87pc7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc8br3a87pc7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note that I'm not saying the recency-weighted median is bad! On the contrary, it seems to work quite well and is genuinely hard to beat.&nbsp;</p></div></li></ol>", "user": {"username": "nikos"}}, {"_id": "x9pcT6dvaGKox4PT5", "title": "Chilean AIS Hackathon Retrospective", "postedAt": "2023-05-09T01:34:27.437Z", "htmlBody": "<h2>TL;DR</h2><ul><li>We hosted an AI Safety \u201cThinkathon\u201d in Chile. We had participation from 40 students with differing skill levels and backgrounds, with groups totalling 13 submissions.&nbsp;</li><li>We see potential in:<ul><li>Similar introductory events aiming for a broad audience</li><li>Collaborating more often with student organizations</li><li>Leveraging remote help from external mentors</li></ul></li><li>We experimented with an alternative naming, having remote mentors, different problem sources, and student organization partnerships, with varying results.</li><li>We could have improved planning and communicating the difficulty of challenges.</li></ul><h2>Introduction</h2><p>In February, we ran the first AI Safety Hackathon in Chile (and possibly in all of South America<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsm7pcollgdi\"><sup><a href=\"#fnsm7pcollgdi\">[1]</a></sup></span>). This post provides some details about the event, a teaser of some resulting projects and our learnings throughout.</p><figure class=\"image image_resized\" style=\"width:83.68%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/lwbncgxkz7b3opfvghly\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/zwlwucjyu1zqbd3txhkv 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/wc5vqj45qdcm9gwvqbwb 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/jhn50jsncxbft7glex9p 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/ymn0ytjuq9wlw8sae3ez 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/xcxqcqlmdabxflzrdcrs 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/a5juceuvls4fvpuvtjcs 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/f1qthy5buvku9xjqirp4 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/hq7h9uf0kn3l9clkx9bd 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/pnaquuvqte7nrqmab7w2 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/fr3aqrgrlqjost9janz0 2000w\"><figcaption>A team working on their proposal during the second day.</figcaption></figure><h2>Goals and overview of the event</h2><p>The hackathon was meant to kick-start our nascent <a href=\"https://agisf.aisuc.dev/\">AI Safety Group at UC Chile</a>, generating interest in AI Safety and encouraging people to register for our AGI Safety Fundamentals course group.</p><p>It ran between the 25th and the 28th of February, the first two days being in-person events and the other two serving as additional time for participants to work on their proposals, with some remote assistance on our part. Participants formed teams of up to four people, and could choose to assist either virtually (through Discord) or in-person (on the first two days).</p><p>We had help from <a href=\"https://alignmentjam.com/\">Apart Research</a> and partial funding from <a href=\"https://www.alignmentawards.com/\">AI Alignment Awards</a>.</p><h2>Things we experimented with</h2><ul><li>Aiming for a broad audience, we named the event \u201cThinkathon\u201d (instead of hackathon) and provided <a href=\"https://altruismoeficaz.cl/thinkathon\">plenty of introductory material</a> alongside the proposed problems.&nbsp;<ul><li>We think this was the right choice, as the desired effect was reflected on the participant demographics (see below).</li><li>We could have been better at preparing participants. Some participants suggested we could have done an introductory workshop.</li></ul></li><li>We incorporated the two problems from the AI Alignment Awards (Goal Misgeneralization and the Shutdown problem), alongside easier, self-contained problems aimed at students with different backgrounds (like policy or psychology).&nbsp;<ul><li>We think most teams weren't prepared to tackle the AI Alignment Awards challenges. Most teams (77%) chose them initially regardless of their experience, getting stuck quickly.</li><li>This might have worked better by communicating difficulty more clearly, as well as emphasizing that aiming for incremental progress rather than a complete solution is a better strategy for a beginner's hackathon.</li></ul></li><li>As we don't know many people with previous experience in AIS in Chile, we got help from external mentors, which connected remotely to help participants.<ul><li>We think this was a good decision, as participants rated mentor support highly (see below).</li></ul></li><li>We collaborated actively with two student governments from our university (the <a href=\"https://www.caae.cl/\">Administration and Economics Student Council</a> and the <a href=\"https://www.cai.cl/\">Engineering Student Council</a>). They helped with funding, logistics and outreach.<ul><li>We think this was an excellent choice, as they provided a much broader platform for outreach and crucial logistics help.</li><li>We had a great time working with them, and they were eager to work with us again!</li></ul></li></ul><h2>Things that went well</h2><ul><li>40 people attended in-person and 10 people remotely (through Discord), we were surprised by both the high number of attendants and the preference<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefx61kin6d9a\"><sup><a href=\"#fnx61kin6d9a\">[2]</a></sup></span>&nbsp;for in-person participation.</li><li>We had a total of 13 submitted proposals, much higher than expected.</li><li>While all proposals were incremental contributions, most were of high quality.</li><li>Skill level and majors varied significantly, going from relatively advanced CS students to freshmen from other fields (like economics). We were aiming for diversity, so this is a win.</li><li>While we hugely delayed starting the AGISF program (we opened applications very recently), many participants were eager to apply to AGISF.</li></ul><h2>Things we could have done better</h2><p>(Also check out the \u201cThings we experimented with\u201d section)</p><ul><li>We had some issues navigating funding. This was in good part because we failed to anticipate some costs. In the end, we ended up spending around $20 USD per person total on food and catering. The collaborating student organizations covered most of the funding for prizes (around $15 USD / pp).</li><li>Logistics should have been planned with significantly more anticipation. We were overconfident about being able to sort out some details last minute. It turned out our university couldn't host the event (due to the academic calendar), and we were forced to find two new venues in a couple of days (which we were able to get for free).</li><li>At the end of the last day of the in-person phase, we didn't do a proper wrap-up. This contributed to some teams feeling a bit lost during the remote phase.</li><li>We didn't do an impressive job at positioning the AI Safety group, as we could have talked more about it during the event, and be quicker to follow-up on participants' interest after the event.</li></ul><h2>Challenge problems</h2><p>We presented participants with the following six challenges. The linked write-ups are in Spanish, as they were made for participants.</p><ul><li><a href=\"https://altruismoeficaz.cl/goal-misgeneralization-problem\">Goal Misgeneralization</a> from the AI Alignment Awards. Seven submitted entries addressed it, including the one that came in second place.</li><li><a href=\"https://altruismoeficaz.cl/shutdown-problem\">Shutdown Problem</a> from the AI Alignment Awards. Two submitted entries were about it.</li><li><a href=\"https://altruismoeficaz.cl/escalabilidad-de-la-cadena-de-suministros-de-cmputo\">Scalability of production in the compute supply chain</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmqwckd6mz1j\"><sup><a href=\"#fnmqwckd6mz1j\">[3]</a></sup></span>: &nbsp;One entry investigated it.</li><li><a href=\"https://altruismoeficaz.cl/menciones-institucionales-de-seguridad-en-ia\">Institutional mentions of AI Safety</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref87n5x4dio9u\"><sup><a href=\"#fn87n5x4dio9u\">[4]</a></sup></span>: Sadly, no entries based on this challenge were submitted.</li><li><a href=\"https://altruismoeficaz.cl/honestidad-en-modelos-de-lenguaje\">Honesty in language models</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1jdj4ngw2s3\"><sup><a href=\"#fn1jdj4ngw2s3\">[5]</a></sup></span>: Two proposals worked on this problem, both by doing empirical research via the OpenAI API. They earned the first and third places.</li><li><a href=\"https://altruismoeficaz.cl/investigaciones-sociales-sobre-seguridad-en-ia\">Social science research for AI Safety</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgec4v04f45m\"><sup><a href=\"#fngec4v04f45m\">[6]</a></sup></span>: Unfortunately, no submitted proposals worked on this area.</li></ul><h2>Highlighted submissions</h2><p>Some highlighted proposals from the hackathon:</p><figure class=\"image image_resized\" style=\"width:64.99%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/iruuqnu4yejiojfudord\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/woqbzjy60mhqzdrco586 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/yjrf3usif8urdvrdhw9p 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/uwahhkqznptlx6m9b6ng 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/nv558vu7ymlb0lyw1eoc 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/p2zib3vtixnno6gnd9z1 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/kx27kj2ijdqocwzmr1n6 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/aykwflwe5qg7y1ofrb4d 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/ijogworrgld7z4c2qdtr 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/vkikjy1d0dwboryelrg8 1620w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/wxi9zq4m8wc280o7flbu 1750w\"></figure><p><a href=\"https://itch.io/jam/thinkathon-ia/rate/1949083\"><strong>Investigating the Relationship Between Priming with Multiple Traits and Language Model Truthfulness in GPT-3</strong></a><strong>: </strong>A research project exploring how different traits placed in prompts influence the truthfulness of GPT-3 based models, following <a href=\"https://aisafetyideas.com/?categories=AI+Governance%2CCognitive+Science&amp;idea=137\">this idea from AISI</a>.&nbsp;</p><figure class=\"image image_resized\" style=\"width:64.22%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/vzwe0vxh3y7vbflvbmx6\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/yewl2cwjwb0s43ixzg9z 190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/gkwwyk74rado51tsiyat 380w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/repzfe3hu2nefiok7osw 570w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/zvf5zdoze4bpwiqmnpmp 760w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/ymbpcyk6jmkvryqxxw5z 950w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/jychcmiuzpe6914psmxi 1140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/zsoz0fvwhwfe3yavhoue 1330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/okoc5f3dt7x7xmsg5ogb 1520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/sso2fdtnabnyq1wmqytm 1710w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/qydchxqk1r6kgb6aiad5 1806w\"></figure><p><a href=\"https://itch.io/jam/thinkathon-ia/rate/1949301\"><strong>The challenge of Goal Misgeneralization in AI: Causes and Solutions</strong></a><strong>: </strong>A general literature review on goal misgeneralization that also suggests a mix of solutions inspired heavily by previous work.&nbsp;</p><p>All proposals (some in Spanish) can be seen <a href=\"https://itch.io/jam/thinkathon-ia/results\">here</a>.</p><h2>Attendance Demographics</h2><ul><li>Around 50% of students were majoring in engineering (mostly CS engineering<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvjwc5q0d22k\"><sup><a href=\"#fnvjwc5q0d22k\">[7]</a></sup></span>), 21% in a (new) purely CS major, 11% in business and economics, and the rest were from a mix of majors (neuroscience, maths, biochemistry, medicine).</li><li>About 75% of participants were from <a href=\"https://www.uc.cl/\">UC Chile</a>, our university, while about 25% were from other universities.</li><li>There was a significant gender imbalance (only 20% were women or non-binary people). We don't have great ideas on how to improve this.</li><li>To our surprise, around 54% of participants were incoming freshmen.</li></ul><h2>Insights from participants</h2><p>Anecdotally:</p><ul><li>It was surprisingly easy to talk about concerns related to AI Safety with everyone, including people with no previous exposure to them.</li><li>Most people didn't have trouble with understanding the problems themselves, independent of their background (except perhaps for goal misgeneralization).</li></ul><p>We also ran a quick after-event survey (n=26):</p><ul><li>Participants gave a 77% <a href=\"https://en.wikipedia.org/wiki/Net_promoter_score\">Net Promoter Score</a> (4.6/5 likelihood to recommend), indicating high satisfaction with the event.</li><li>Frequent negative feedback included lack of food on the second day (n=5), lack of activities to socialize or meet other participants (n=4) and that the venues used were too far away (n=4).</li><li>76% hadn't heard of AI Safety before the event.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc7j7wko25gq\"><sup><a href=\"#fnc7j7wko25gq\">[8]</a></sup></span></li><li>Interest in working or doing research in AI Safety and AI more generally increased significantly, with a moderate effect size for the first (r = 0.54) and a small effect size (r = 0.32) for the latter.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh8gx7d6hzaj\"><sup><a href=\"#fnh8gx7d6hzaj\">[9]</a></sup></span>&nbsp;</li><li>Participants reported having learned significantly about both AI in general and AI Safety (4.0/5 and 3.8/5 respectively) and valued mentor help highly (4.4/5)</li></ul><p>Also, some random pictures:</p><figure class=\"table\"><table><tbody><tr><td><figure class=\"image image_resized\" style=\"width:93.06%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/bydsautluvudkbnevlda\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/kbkbz5hg9fqmfoa0icfl 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/ze4h4lexs1po8baejtba 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/ildx3yukmrhdoi7dngkr 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/z2kqubhoqwl7epfjqpwh 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/qgydbihdmrzlckzsdwor 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/xploamj3pi1qamhayyjg 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/jvsuxxa4khuo1bnjsodv 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/pk6aal33xlgfrakqmasj 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/mfz910ijekmmixjwsayp 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/kehhvcwupgajya5lkn7p 1023w\"></figure></td><td><figure class=\"image image_resized\" style=\"width:99.22%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/apyfdkenb8cxlzr2hlqx\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/zqjjkvvo8ryqaxpwlxkx 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/vmg3fnc20cjgolzypa8i 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/a1ylqkirxutlwfjtjyxy 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/uwtmz6fa5vdqwal7z5qs 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/hjohy6keapovuysdz1mu 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/vzxsv042id0ti1wwspw8 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/vxzsf022hvsh9ywv7gks 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/wfcrrwjel0poyut9xpx6 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/f2edoirhs5mdfr1opmed 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/z3wbgjqhjzatfgxkeyun 2000w\"></figure></td></tr><tr><td><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/phbneqqa2lhaesqsclqf\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/pygd2py43xxrlnyf1ur7 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/rsckishx1aa62qqxxpjv 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/jzb0fv32lpucifcerttu 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/fcphrqnyf9kqwlrz62w7 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/ukzc6xzbkxfhuj3m88h5 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/kuj4btfkkcgwc5rqfxfn 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/q29xzfrzggvzzhdc2cz5 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/z5klpobruslgrc9kbcvx 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/ouyptmnlzpqtlqwaab55 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/fcgilsihfdptf14y7u7s 2000w\"></figure></td><td><figure class=\"image image_resized\" style=\"width:99.85%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/ibf9cuify3cifl2gfpz2\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/ibsppa4xgenw9p9rcukz 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/oni34r4mhlpttwctvz9i 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/f0n6woltyslwoe9gbozx 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/n1iaunrux03gz6dgfgce 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/ivvsltgyeloertjs700u 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/cjlqd6ubuvaj55a7thsm 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/pievo8ewxgqdicq7qq0n 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/zu9czzwn33z5elnts03y 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/kfg47vuck615cvif2gej 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/x9pcT6dvaGKox4PT5/iyam2dndjv016rkr2bv8 2000w\"></figure></td></tr></tbody></table></figure><h2>Acknowledgements</h2><p>We want to thank the following people for their amazing help:</p><ul><li><a href=\"https://forum.effectivealtruism.org/users/pvs?mention=user\">@Pablo Villalobos</a> and <a href=\"https://forum.effectivealtruism.org/users/luca-de-leo-1?mention=user\">@Luca De Leo</a> for judging the submissions.</li><li><a href=\"https://forum.effectivealtruism.org/users/aaron_scher?mention=user\">@Aaron_Scher</a>, <a href=\"https://forum.effectivealtruism.org/users/alejandro-acelas?mention=user\">@Alejandro Acelas</a> and <a href=\"https://forum.effectivealtruism.org/users/luca-de-leo-1?mention=user\">@Luca De Leo</a> (again) for providing mentorship to groups.</li><li><a href=\"https://forum.effectivealtruism.org/users/esben-kran?mention=user\">@Esben Kran</a> for last minute operational support.</li><li><a href=\"https://forum.effectivealtruism.org/users/renan-araujo?mention=user\">@Renan Araujo</a> for suggesting the name \u201cThinkaton\u201d.</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsm7pcollgdi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsm7pcollgdi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We might or might have not chosen to write South America instead of Latin America to conveniently exclude <a href=\"https://itch.io/jam/latam-ais\">Mexico</a>.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnx61kin6d9a\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefx61kin6d9a\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Especially considering that participating in-person meant showing up at 9:00 &nbsp;of a summer vacation day in an inconveniently-located venue.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmqwckd6mz1j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmqwckd6mz1j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Taken from <a href=\"https://aisafetyideas.com/?idea=216\">this idea</a> by <a href=\"https://forum.effectivealtruism.org/users/pvs?mention=user\">@Pablo Villalobos</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn87n5x4dio9u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref87n5x4dio9u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Adapted from <a href=\"https://aisafetyideas.com/?categories=AI+Governance&amp;idea=53\">this idea</a> by Caroline Jeanmaire.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1jdj4ngw2s3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1jdj4ngw2s3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Taken from <a href=\"https://aisafetyideas.com/?categories=AI+Governance%2CCognitive+Science&amp;idea=137\">this idea</a> by <a href=\"https://forum.effectivealtruism.org/users/sabrina-zaki?mention=user\">@Sabrina Zaki</a>, Luke Ring and Aleks Baskakovs, that in turn comes from their (winning) <a href=\"https://itch.io/jam/llm-hackathon/rate/1728576\">proposal</a> for Apart Research's <a href=\"https://forum.effectivealtruism.org/posts/wAaXeXyzcy8JpEKAe/results-from-the-language-model-hackathon\">language model hackathon</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngec4v04f45m\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgec4v04f45m\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Inspired the ideas in <a href=\"https://forum.effectivealtruism.org/users/irving?mention=user\">@Geoffrey Irving</a> and Amanda Askell's <a href=\"https://distill.pub/2019/safety-needs-social-scientists/\">AI Safety Needs Social Scientists</a> and Riccardo Volpato's <a href=\"https://www.lesswrong.com/posts/nqTkfrnE4CkbMtmHE/research-ideas-to-study-humans-with-ai-safety-in-mind\">Research ideas to study humans with AI Safety in Mind</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvjwc5q0d22k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvjwc5q0d22k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>People from CS Engineering take common courses in engineering, but are otherwise similar to CS.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc7j7wko25gq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc7j7wko25gq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This was explicitly defined in contrast to Ethics of AI or similar fields.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh8gx7d6hzaj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh8gx7d6hzaj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A Wilcoxon signed-rank test showed a statistically significant increase in interest in AI (W = 115.5, p &lt; 0.001, r = 0.32) as well as AI Safety (W = 190.0, p &lt; 0.001, r = 0.54). This was based on retrospective Likert-style questions on interest. It was hard to account for survey effects.&nbsp;</p></div></li></ol>", "user": {"username": "agucova"}}]