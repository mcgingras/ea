[{"_id": "67wJauuQKuJmjcwZp", "postedAt": "2023-09-18T00:44:12.754Z", "postId": "DBDpnAhxvRWmfmtfv", "htmlBody": "<blockquote><p>I think GiveWell shouldn\u2019t be modeled as wanting to recommend organizations that save as many current lives as possible. I think a more accurate way to model them is \u201cGiveWell recommends organizations&nbsp;that are [within the Overton Window]/[have very sound data to back impact estimates] that save as many current lives as possible.\u201d</p></blockquote><p>This is correct if you look at <a href=\"https://www.givewell.org/how-we-work/criteria\">GiveWell's criteria for evaluating donation opportunities</a>. GiveWell\u2019s highly publicized claim \u201cWe search for the charities that save or improve lives the most per dollar\u201d is somewhat misleading given that they only consider organizations with RCT-style evidence backing their effectiveness.</p>", "parentCommentId": null, "user": {"username": "michaelchen"}}, {"_id": "5pfSESznENzqbLt2f", "postedAt": "2023-09-18T20:32:43.079Z", "postId": "DBDpnAhxvRWmfmtfv", "htmlBody": "<p>Thanks for the post. It was an interesting read.</p><p>According to <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">The Case For Strong Longtermism</a>, 10^36 people could ultimately inhabit the Milky Way. Under this assumption, one micro-doom is equal to 10^30 expected lives.</p><p>If a 50%-percentile AI safety researcher reduces x-risk by 31 micro-dooms, they could save about 10^31 expected lives during their career or about 10^29 expected lives per year of research. If the value of their research is spread out evenly across their entire career, then each second of AI safety research could be worth about 10^22 expected future lives which is a very high number.</p><p>These numbers sound impressive but I see several limitations of these kinds of naive calculations. I'll use the three-part framework from <a href=\"https://forum.effectivealtruism.org/topics/what-we-owe-the-future\">What We Owe the Future</a> to explain them:</p><ul><li>Significance: the value of research tends to follow a long-tailed curve where most papers get very few citations and a few get an enormous number. Therefore, most research probably has low value.</li><li>Contingency: the value of some research is decreased if it would have been created anyway at some later point in time.</li><li>Longevity: it's hard to produce research that has a lasting impact on a field or the long-term trajectory of humanity. Most research probably has a sharp drop off in impact after it is published.</li></ul><p>After taking these factors into account, I think the value of any given AI safety research is probably much lower than naive calculations suggest. Therefore, I think grant evaluators should take into account their intuitions on what kinds of research are most valuable rather than relying on expected value calculations.</p>", "parentCommentId": null, "user": {"username": "Stephen McAleese"}}, {"_id": "tCTRNsTkgH7bPFtMq", "postedAt": "2023-09-18T20:45:28.395Z", "postId": "DBDpnAhxvRWmfmtfv", "htmlBody": "<blockquote><p>I think grant evaluators should take into account their intuitions on what kinds of research are most valuable rather than relying on expected value calculations.</p></blockquote><p>&nbsp;</p><p>In case of EV calculations where the future is part of the equation, I think using microdooms as a measure of impact is pretty practical and can resolve some of the problems inherent with dealing with enormous numbers, because many people have cruxes which are downstream of microdooms. Some think there'll be 10^40 people, some think there'll be 10^20. Usually, if two people disagree on how valuable the long-term future is, they don't have a common unit of measurement for what to do today. But if they both use microdooms, they can compare things 1:1 in terms of their effect on the future, without having to flesh out all of the post-agi cruxes.</p>", "parentCommentId": "5pfSESznENzqbLt2f", "user": {"username": "Nikola"}}, {"_id": "BLhn9CKwDuoxJzenb", "postedAt": "2023-09-20T07:02:29.689Z", "postId": "DBDpnAhxvRWmfmtfv", "htmlBody": "<blockquote><p>But if they both use microdooms, they can compare things 1:1 in terms of their effect on the future, without having to flesh out all of the post-agi cruxes.</p></blockquote><p>I don't think this is the case for all key disagreements, because people can disagree a lot about the duration of the period of heightened existential risk, whereas microdooms are defined as a reduction in <i>total</i> existential risk rather than in terms of per-period risk reduction. So two people can agree that AI safety work aimed at reducing existential risk will decrease risk by a certain amount over a given period, but one may believe such work averts 100x as many microdooms as the other because they believe the period of heightened risk is 100x shorter.</p>", "parentCommentId": "tCTRNsTkgH7bPFtMq", "user": {"username": "Pablo_Stafforini"}}, {"_id": "xhTGtAyvXjrvxCKN8", "postedAt": "2023-09-21T03:29:43.915Z", "postId": "DBDpnAhxvRWmfmtfv", "htmlBody": "<blockquote><p>I think GiveWell shouldn\u2019t be modeled as wanting to recommend organizations that save as many current lives as possible. I think a more accurate way to model them is \u201cGiveWell recommends organizations&nbsp;that are [within the Overton Window]/[have very sound data to back impact estimates] that save as many current lives as possible.\u201d If GiveWell wanted to recommend organizations that save as many human lives as possible, their portfolio would probably be entirely made up of AI safety orgs.</p></blockquote><p>This paragraph, especially the first sentence, seems to be based on a misunderstanding I used to share, which Holden Karnofsky tried to correct back in 2011 (when he was still at GiveWell) with the blog post <a href=\"https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">Why we can\u2019t take expected value estimates literally (even when they\u2019re unbiased)</a> in which he argued (emphasis his):</p><blockquote><p>While some people feel that GiveWell puts too much emphasis on the measurable and quantifiable, there are others who go further than we do in quantification, and justify their giving (or other) decisions based on fully explicit expected-value formulas. The latter group tends to critique us \u2013 or at least disagree with us \u2013 based on our preference for strong evidence over high apparent \u201cexpected value,\u201d and based on the heavy role of non-formalized intuition in our decisionmaking. ...</p><p>We believe that people in this group are often making a fundamental mistake... [of] estimating the \u201cexpected value\u201d of a donation (or other action) based solely on a fully explicit, quantified formula, many of whose inputs are guesses or very rough estimates.&nbsp;</p><p>We believe that any estimate along these lines needs to be adjusted using a \u201cBayesian prior\u201d; that this adjustment can rarely be made (reasonably) using an explicit, formal calculation; and that most attempts to do the latter, even when they seem to be making very conservative downward adjustments to the expected value of an opportunity, are not making nearly large enough downward adjustments to be consistent with the proper Bayesian approach.</p><p>This view of ours illustrates why \u2013 while we seek to ground our recommendations in relevant facts, calculations and quantifications to the extent possible \u2013 every recommendation we make incorporates many different forms of evidence and involves a strong dose of intuition. And <strong>we generally prefer to give where we have </strong><i><strong>strong evidence that donations can do a lot of good</strong></i><strong> rather than where we have </strong><i><strong>weak evidence that donations can do far more good</strong></i> \u2013 a preference that I believe is inconsistent with the approach of giving based on explicit expected-value formulas (at least those that (a) have significant room for error (b) do not incorporate Bayesian adjustments, which are very rare in these analyses and very difficult to do both formally and reasonably).</p></blockquote><p>(He since developed this view further in the 2014 post <a href=\"https://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">Sequence thinking vs cluster thinking</a>.) Further down, Holden wrote&nbsp;</p><blockquote><p><strong>My prior for charity is generally skeptical</strong>, as outlined at <a href=\"https://blog.givewell.org/2009/12/05/a-conflict-of-bayesian-priors/\">this post</a>. Giving well seems <a href=\"https://blog.givewell.org/2011/06/11/why-we-should-expect-good-giving-to-be-hard/\">conceptually quite difficult to me</a>, and it\u2019s been my experience over time that the more we dig on a cost-effectiveness estimate, the more unwarranted optimism we uncover.</p></blockquote><p>This guiding philosophy hasn't changed; in GiveWell's <a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness\">How we work - criteria - cost-effectiveness</a> they write:</p><blockquote><p>Cost-effectiveness is the single most important input in our evaluation of a program's impact. However, <strong>there are many limitations to cost-effectiveness estimates, and we do not assess programs solely based on their estimated cost-effectiveness.</strong> We build <a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/cost-effectiveness-models\"><strong>cost-effectiveness models</strong></a> primarily because:</p><ul><li>They help us compare programs or individual grant opportunities to others that we've funded or considered funding; and</li><li>Working on them helps us ensure that we are thinking through as many of the relevant issues as possible.</li></ul></blockquote><p>which jives with what Holden wrote in the relative advantages &amp; disadvantages of sequence thinking vs cluster thinking article above.&nbsp;</p><p>Note that this is for global health &amp; development charities, where the feedback loops to sense-check and correct cost-effectiveness analyses that guide resource allocation &amp; decision-making are much clearer and tighter than for AI safety orgs (and other longtermist work more generally). If it's already this hard for GHD work, I get much more skeptical of CEAs in AIS with super-high EVs, just in <a href=\"https://www.lesswrong.com/posts/GrtbTAPfkJa4D6jjH/confidence-levels-inside-and-outside-an-argument\">model uncertainty terms</a>.&nbsp;</p><p>This isn't meant to devalue AIS work! I think it's critical and important, and I think some of the \"p(doom) modeling\" work is persuasive (<a href=\"https://arxiv.org/abs/2206.09360\">MTAIR</a>, <a href=\"https://forum.effectivealtruism.org/posts/Z7r83zrSXcis6ymKo/dissolving-ai-risk-parameter-uncertainty-in-ai-future\">Froolow</a>, and <a href=\"https://arxiv.org/pdf/2206.13353.pdf\">Carlsmith</a> come to mind). Just thought that \"<i>If GiveWell wanted to recommend organizations that save as many human lives as possible, their portfolio would probably be entirely made up of AI safety orgs</i>\" felt off given what they're trying to do, and how they're going about it.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Mo Nastri"}}]