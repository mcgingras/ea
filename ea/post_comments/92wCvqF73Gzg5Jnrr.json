[{"_id": "ofL4YpnGRLftyKYFi", "postedAt": "2018-08-14T01:23:03.839Z", "postId": "92wCvqF73Gzg5Jnrr", "htmlBody": "<p>Hey, a rough point on a doubt I have. Not sure if it's useful/novel.</p>\n<p>Going through the mental processes of a utilitarian (roughly defined) will correlate with others making more utilitarian decisions as well (especially when they're similar in relevant personality traits and their past exposure to philosophical ideas). </p>\n<p>For example, if you act less scope-insensitive, ommission-bias-y, or ingroup-y, others will tend to do so as well. This includes edge cases \u2013 e.g. people who otherwise would have made decisions that roughly fall in the deontologist or virtue ethics bucket. </p>\n<p>Therefore, for every moment you end up shutting off utilitarian-ish mental processes in favour of ones where you think you're doing moral trade (including hidden motivations like rationalising acting from social proof or discomfort in diverging from your peers), your multi-universal compatriots will do likewise (especially in similar contexts). </p>\n<p>(In case it looks like I'm justifying being a staunch utilitarian here, I have a more nuanced anti-realism view mixed in with lots of uncertainty on what makes sense.)</p>\n", "parentCommentId": null, "user": {"username": "remmelt"}}, {"_id": "85yKwD5yos4pTnLoD", "postedAt": "2018-08-14T13:15:21.842Z", "postId": "92wCvqF73Gzg5Jnrr", "htmlBody": "<p>A few doubts:</p>\n<ol>\n<li><p>It seems like MSR requires a multiverse large enough to have many well-correlated agents, but not large enough to run into the problems involved with infinite ethics. Most of my credence is on no multiverse or infinite multiverse, although I'm not particularly well-read on this issue.</p>\n</li>\n<li><p>My broad intuition is something like &quot;Insofar as we can know about the values of other civilisations, they're probably similar to our own. Insofar as we can't, MSR isn't relevant.&quot; There are probably exceptions, though (e.g. we could guess the direction in which an r-selected civilisation's values would vary from our own).</p>\n</li>\n<li><p>I worry that MSR is susceptible to self-mugging of some sort. I don't have a particular example, but the general idea is that you're correlated with other agents <em>even if you're being very irrational</em>. And so you might end up doing things which seem arbitrarily irrational. But this is just a half-fledged thought, not a proper objection.</p>\n</li>\n<li><p>And lastly, I would have much more confidence in FDT and superrationality in general if there were a sensible metric of similarity between agents, apart from correlation (because if you always cooperate in prisoner's dilemmas, then your choices are perfectly correlated with CooperateBot, but intuitively it'd still be more rational to defect against CooperateBot, because your decision algorithm isn't similar to CooperateBot in the same way that it's similar to your psychological twin). I guess this requires a solution to logical uncertainty, though.</p>\n</li>\n</ol>\n<p>Happy to discuss this more with you in person. Also, I suggest you cross-post to Less Wrong.</p>\n", "parentCommentId": null, "user": {"username": "richard_ngo"}}, {"_id": "aH9vMqzLsy7rsnLXA", "postedAt": "2018-08-14T19:14:39.658Z", "postId": "92wCvqF73Gzg5Jnrr", "htmlBody": "<p>I remain unsure with MSR how to calculate the measure of agents in worlds holding positions to trade with so that we can figure out how much we should acausally trade with each. Also, how to address uncertainty about if anyone will independently arrive at the same position you hold and so be able to acausally trade with you since you can't tell them about what you would actually prefer.</p>\n", "parentCommentId": null, "user": {"username": "gworley3"}}, {"_id": "SmwifaWWADqhJ9wrL", "postedAt": "2018-08-15T00:02:14.558Z", "postId": "92wCvqF73Gzg5Jnrr", "htmlBody": "<p>I still have doubts as to whether you should pay in Counterfactual Mugging since I believe that (non-quantum) probability is in the map rather than the territory. I haven't had the opportunity to write up these thoughts yet as my current posts are building up towards it, but I can link you when I do.</p>\n", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "adXEgGni6L3enY9yK", "postedAt": "2018-09-02T23:27:49.574Z", "postId": "92wCvqF73Gzg5Jnrr", "htmlBody": "<p>Re 4): Correlation or similarity between agents is not really necessary condition for cooperation in the open source PD. LaVictoire et al. (2012) and related papers showed that 'fair' agents with completely different implementations can cooperate. A fair agent, roughly speaking, has to conform to any structure that implements &quot;I'll cooperate with you if I can show that you'll cooperate with me&quot;. So maybe that's the measure you're looking for.</p>\n<p>A population of fair agents is also typically a Nash equilibrium in such games so you might expect that they sometimes do evolve.</p>\n<p>Source:\nLaVictoire, P., Fallenstein, B., Yudkowsky, E., Barasz, M., Christiano, P., &amp; Herreshoff, M. (2014, July). Program equilibrium in the prisoner\u2019s dilemma via L\u00f6b\u2019s theorem. In AAAI Multiagent Interaction without Prior Coordination workshop.</p>\n", "parentCommentId": "85yKwD5yos4pTnLoD", "user": null}, {"_id": "25vKYg86M6zjvZyXi", "postedAt": "2019-01-27T02:02:39.076Z", "postId": "92wCvqF73Gzg5Jnrr", "htmlBody": "<p>The example you&#x27;ve given me shows that agents which implement exactly the same (high-level) algorithm can cooperate with each other. The metric I&#x27;m looking for is: how can we decide how similar two agents are when their algorithms are non-identical? Presumably we want a smoothness property for that metric such that if our algorithms are very similar (e.g. only differ with respect to some radically unlikely edge case) the reduction in cooperation is negligible. But it doesn&#x27;t seem like anyone knows how to do this.</p>", "parentCommentId": "adXEgGni6L3enY9yK", "user": {"username": "richard_ngo"}}, {"_id": "iXXvEremjJtedccwh", "postedAt": "2023-07-03T21:20:39.457Z", "postId": "92wCvqF73Gzg5Jnrr", "htmlBody": "<p>One way I imagine dealing with this is that there is an oracle that tells us with certainty, for two algorithms and their decision situations, what the counterfactual possible joint outputs are. The smoothness then comes from our uncertainty about (i) the other agents' algorithms (ii) their decision situation (iii) potentially the outputs of the oracle. The correlations vary smoothly as we vary our probability distributions over these things, but for a fully specified algorithm, situation, etc., the algorithms are always either logically identical or not.<br><br>Unfortunately, I don't know what the oracle would be doing in general. I could also imagine that, when formulated this way, the conclusion is that humans never correlate with anything, for instance.</p>", "parentCommentId": "25vKYg86M6zjvZyXi", "user": {"username": "Johannes_Treutlein"}}]