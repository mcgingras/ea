[{"_id": "JAyzCQbjbaa4FXhpR", "title": "Introduction to EA | Ajeya Cotra | EAGxBerkeley 2016", "postedAt": "2017-04-30T23:00:00.000Z", "htmlBody": "", "user": {"username": "Jesse Rothman"}}, {"_id": "K2gqAkjbC4GwJt3hF", "title": "New Report on Early Field Growth", "postedAt": "2017-04-26T13:19:33.230Z", "htmlBody": "<p>As part of our research on the <a href=\"https://www.openphilanthropy.org/research/history-of-philanthropy\">history of philanthropy</a>, I recently investigated several case studies of early field growth, especially those in which philanthropists purposely tried to grow the size and impact of a (typically) young and small field of research or advocacy.</p><p>The <a href=\"http://www.openphilanthropy.org/some-case-studies-early-field-growth\">full report</a> includes brief case studies of bioethics, cryonics, molecular nanotechnology, neoliberalism, the conservative legal movement, American geriatrics, American environmentalism, and animal advocacy. My key takeaways are:</p><ul><li>Most of the \u201cobvious\u201d methods for building up a young field have been tried, and those methods often work. For example, when trying to build up a young field of academic research, it often works to fund workshops, conferences, fellowships, courses, professorships, centers, requests for proposals, etc. Or when trying to build up a new advocacy community, it often works to fund student clubs, local gatherings, popular media, etc.</li><li>Fields vary hugely along several dimensions, including (1) primary sources of funding (e.g. large philanthropists, many small donors, governments, companies), (2) whether engaged philanthropists were <a href=\"http://www.openphilanthropy.org/blog/challenges-passive-funding\">\u201cactive\u201d or \u201cpassive\u201d</a> in their funding strategy, and (3) how much the growth of the field can be attributed to endogenous factors (e.g. explicit movement-building work) vs. exogenous factors (e.g. changing geopolitical conditions).</li></ul><p>Besides these major takeaways, I also learned many more specific things about particular fields. For example:</p><ul><li>The rise of bioethics seems to be a case study in the transfer of authority over a domain (medical ethics) from one group (doctors) to another (bioethicists), in large part due to the first group\u2019s relative neglect of that domain. [<a href=\"http://www.openphilanthropy.org/some-case-studies-early-field-growth#Bioethics\">More</a>]</li><li>In the case of cryonics and molecular nanotechnology, plausibly growth-stunting adversarial dynamics arose between advocates of these young fields and the scientists in adjacent fields (cryobiology and chemistry, respectively). These adversarial dynamics seem to have arisen, in part, due to the young fields\u2019 early focus on popular outreach prior to doing much scientific or technical work, and their disparagement of those in adjacent fields. [<a href=\"http://www.openphilanthropy.org/some-case-studies-early-field-growth#FailureModes\">More</a>]</li><li>The rise of neoliberalism is a victory for an explicit strategy of decades-long investment in the academic development and intellectual spreading of a particular set of ideas, though this model may not work as well when the ideas themselves don\u2019t happen to benefit a naturally well-resourced set of funders (large corporations and their wealthy owners, as in the case of neoliberalism). [<a href=\"http://www.openphilanthropy.org/some-case-studies-early-field-growth#Neoliberalism\">More</a>]</li><li>A small group of funders of the conservative legal movement managed to critique their own (joint) strategy, change course, and succeed as a result. [<a href=\"http://www.openphilanthropy.org/some-case-studies-early-field-growth#ConservativeLegal\">More</a>]</li><li>The rise of the environmental and animal advocacy movements contrast sharply with the cases above, both because they grew mostly via a large network of small funders rather than a small network of large funders, and because many of those movements\u2019 activities do not materially benefit <i>any</i> funder or political actor (e.g. in the case of wilderness preservation or campaigns against factory farming). [<a href=\"http://www.openphilanthropy.org/some-case-studies-early-field-growth#Environmentalism\">More</a>]</li></ul><p>For more detail, see <a href=\"http://www.openphilanthropy.org/some-case-studies-early-field-growth\">the full report</a>.</p>", "user": {"username": "lukeprog"}}, {"_id": "onpvizbPgSrkigSjn", "title": "The fidelity model of spreading ideas", "postedAt": "2017-05-04T22:38:48.218Z", "htmlBody": "<h1>Overview</h1><p>In this post I develop a distinction between mechanisms for spreading EA ideas according to how likely they are to keep the nuance of the ideas intact. I then use this distinction to argue that movement builders ought to prefer mechanisms for spreading EA ideas that retain the nuance of the&nbsp;ideas.</p><p>This consideration has led CEA to be skeptical of attempts to spread EA ideas through mechanisms like the mass media and to instead prefer other mechanisms of spreading EA&nbsp;ideas.</p><h1>Definition</h1><p><strong>Fidelity</strong><br>The key term in this model is \"fidelity.\" Therefore it will be useful to define this term. By fidelity I have in mind nothing more than the classic <a href=\"http://dictionary.reference.com/browse/fidelity\">dictionary definition</a> of \"adherence to fact or detail\" or \"accuracy;&nbsp;exactness.\"</p><p>As an example, imagine I am shooting a movie on an old camera. If the image captured by the camera causes it to seem as though I am wearing a blue shirt when actually my shirt is red, then the image captured by the camera is low&nbsp;fidelity.</p><h1>The Model</h1><h2>The \u201cTelephone Game\u201d&nbsp;Effect</h2><p>Imagine the childhood party staple: the Telephone Game. In the telephone game one player whispers a message to another which is then passed through a line of people until the message is finally announced. Invariably, the announced message is very different from the original, often to comedic&nbsp;effect.</p><p>This game works because whispering is a relatively low-fidelity way of transmitting information. The participant must understand the previous message, hold it in their mind, and then successfully transmit the message to the next person. As it turns out, this is no trivial task, and so the message becomes increasingly distorted with each&nbsp;pass.</p><p>Different ways of communicating ideas demonstrate the Telephone Game Effect to different&nbsp;degrees.</p><p>For example, imagine that the first person had simply written the message down and then passed the message to the next person who also wrote it down and so on. We might expect that the final message would be much closer to the original than in the telephone game, but we might also expect some distortions due to factors like poor&nbsp;handwriting.</p><p>Alternatively, we could imagine that the message is written in an email and then transmitted via email forward from person to person. Here we might expect that the final recipient of the email will have an exact copy of the original&nbsp;message.</p><p>The basic idea is that we can put mechanisms for spreading a message on a continuum between mechanisms that retain almost nothing of the original message and those that retain almost everything of the original message. Those that retain most of the original message are very high fidelity and those that retain little of the original message are very low&nbsp;fidelity.</p><h2>Spreading EA Ideas</h2><p>In the case of spreading EA ideas, the problem of fidelity takes a slightly different form than in the Telephone Game. In general, the problem we face is not that the audience will fail to receive the intended message. Instead, the problem of fidelity is that \"effective altruism\" contains a large number of nuanced and interrelated ideas. Some methods of spreading these ideas require stripping away either the depth of the ideas or their nuance, or&nbsp;both.</p><p>When the context gets stripped away, those who receive the ideas leave with something that's similar to effective altruism, but different. Thus, when we hear the EA message repeated back to us, we get sentences like \"EA is about earning all the money you can and donating it to GiveWell charities\" or \"EAs only care about interventions that are supported by randomized controlled trials.\" To a certain extent we can influence the sentences we get back by being more clever about how we frame our ideas, but it seems unlikely that framing can do all the&nbsp;work.</p><p>A great example is the concept of Earning to&nbsp;Give.</p><p>The core idea of Earning to Give is nuanced and sophisticated. It includes complicated considerations like replaceability, comparative advantage, career capital, the fungibility of resources and so on. In fact, Will published a 15 page paper on the topic in peer-reviewed philosophy journal, Ben wrote a 100 page master's thesis on the topic and 80K has written about this topic extensively on their&nbsp;blog.</p><p>Unfortunately, while a philosophy journal or a research blog are high fidelity methods of transmission, the mass media is not. So, when Will and other early promoters of Earning to Give presented simplistic version of the idea in the mass media, it is perhaps unsurprising that headlines like the following resulted:\"<a href=\"http://nonprofitchronicles.com/2015/10/04/forget-your-dreams-earn-to-give/\">Forget your dreams. Earn to Give.</a>,\" \"<a href=\"https://www.washingtonpost.com/news/wonk/wp/2013/05/31/join-wall-street-save-the-world/\">Join Wall Street. Save the world.</a>,\" and \"<a href=\"http://www.dailymail.co.uk/news/article-2334682/Young-professionals-joining-Wall-Street-save-world.html\">The young professionals who believe their best chance at trying to save the world is by joining Wall Street and making millions.</a>\" These articles communicate the core concepts of earning to give in varying degrees of fidelity, but, unsurprisingly, none come close to explaining the idea with the fidelity that a philosophy paper or series of blog posts&nbsp;can.</p><p>In some cases this is fine. The articles linked to above likely reached many orders of magnitude more people than Will's philosophy paper. Will and other EAs engaged in those interviews with the hope that reaching more people would be worth the inevitable simplifications and distortions involved in media depictions. Yet, low fidelity mechanisms of spreading EA ideas gain this increased distribution at the expense of risking spreading ideas that are related to, but importantly different from, the ideas we want to&nbsp;spread.</p><h2>How can we judge the relative fidelity of different mechanisms for spreading&nbsp;EA?</h2><p>We can analyze the fidelity of a particular mechanism for spreading EA by looking at four&nbsp;components:</p><ul><li><strong>Breadth</strong>: How many ideas can you&nbsp;explore?</li><li><strong>Depth</strong>: How much nuance can you add to the&nbsp;ideas?</li><li><strong>Environment</strong>: Will the audience be in an environment that is conducive to updating their&nbsp;opinions?</li><li><strong>Feedback</strong>: Can you adapt your message over time to improve its&nbsp;fidelity?</li></ul><p>An example of an extremely low fidelity method of communicating EA would be during a heated political discussion on&nbsp;Twitter.</p><p>Given Twitter's character limit you can neither explore many ideas nor explore the ideas in any depth, nor get much useful feedback, and since politics is a very bad environment for updating, this would fail all four&nbsp;components.</p><p>An example of a high fidelity method of communicating EA would be a lengthy personal conversation. In this context you could cover a large number of ideas in great detail in an environment (face-to-face conversation) that is particularly well-suited to updating. A similarly high-fidelity method of spreading EA would be a multi-day, in-person conference with experts in the EA community (e.g. EA&nbsp;Global).</p><h1>Implications</h1><p>When combined with additional models and assumptions we can begin to construct a general case for being cautious about spreading EA ideas through low fidelity mechanisms. I explain how this consideration interacts with other models&nbsp;below.</p><h2>The Awareness/Inclination Model</h2><p>Owen Cotton-Barratt <a href=\"http://globalprioritiesproject.org/wp-content/uploads/2015/05/MovementGrowth.pdf\">has developed a model of movement growth</a> called the Awareness/Inclination Model according to which we can compress knowledge of EA into two dimensions: how much they know about the ideas (awareness) and how favorably they feel, or would feel, towards the ideas (inclination). One implication of this model is that increasing awareness without increasing inclination can cause increased adoption of the ideas in the short term but at the expense of decreasing the total potential number of people that might adopt the ideas in the future (given some&nbsp;assumptions).</p><p>Given the assumption that many people would respond favorably to EA ideas if they understood them, it seems plausible that low fidelity mechanisms for spreading EA ideas increase the probability that we increase awareness for the ideas without increasing inclination. Articles like \"Join Wall Street. Save the world.\" may be doing a good job of increasing awareness without doing a good job of increasing inclination (as some <a href=\"http://www.nytimes.com/2013/06/04/opinion/brooks-the-way-to-produce-a-person.html?ref=opinion\">criticisms of the piece</a> suggests) and thus may be decreasing the maximum number of people that might adopt the ideas in the&nbsp;future.</p><p>On the other hand, high fidelity mechanisms for spreading EA ideas (e.g. conferences) may do a much better job of increasing both inclination and&nbsp;awareness.</p><h2>The dilution model of movement&nbsp;collapse</h2><p>A common concern about spreading EA ideas is that the ideas will get \"diluted\" over time and will come to represent something much weaker than they do currently. For example, right now when we talk about which cause areas are high impact, we mean that the area has strong arguments or evidence to support it, has a large scope, is relatively neglected, and is potentially&nbsp;solvable.</p><p>Over time we might imagine that the idea of a high impact cause comes to mean that the area has some evidence behind it and has some plausible interventions that one could perform. Thus, in the future, adherence to EA ideas might imply relatively little difference from the status&nbsp;quo.</p><p>I'm uncertain about whether this is a serious worry. Yet, if it is, spreading messages about EA with low fidelity would significantly exacerbate the problem. As the depth and breadth of ideas gets stripped away, we should expect the ideas around EA to weaken over time which would eventually cause them to assume a form that is closer to the&nbsp;mainstream.</p><h2>Changes in CEA&nbsp;strategy</h2><p>This model (along with other models and observations) has resulted in a number of concrete changes in CEA strategy. Some of these are listed and briefly explained&nbsp;below:</p><ul><li>Decreased focus on mass media<ul><li>Historically, spreading EA via the mass media was a key focus of CEA. Over time it became clear that the mass media is not particularly well suited to spreading ideas with high fidelity. Therefore, we have pivoted away from this focus and towards higher-fidelity methods like books and&nbsp;podcasts.</li></ul></li><li>Focus on being local and in-person<ul><li>Many CEA projects focus on causing people to meet together to talk about EA with people close to them (e.g. EA Global, EAGx, and local community building). One reason for this focus is that in-person interactions have the potential to be especially high fidelity. People seem to update better when talking to each other in person, and in-person communication allows the conversation to focus on areas of misconception or&nbsp;disagreement.</li></ul></li><li>Academic credibility and the EA brand<ul><li>CEA has become more interested in establishing the academic credibility of some of our ideas. Academia provides a high fidelity method of spread ideas and can help create more high fidelity opportunities to spread ideas in the&nbsp;future.</li></ul></li></ul>", "user": {"username": "Kerry_Vaughan"}}, {"_id": "hk9Xh6xK5j3SQmkcn", "title": "Can one person make a difference?", "postedAt": "2017-04-04T00:57:48.629Z", "htmlBody": "<p>It\u2019s easy to feel like one can person can\u2019t <a href=\"https://80000hours.org/career-guide/making-a-difference/\">make a difference</a>. The world has so many big <a href=\"https://80000hours.org/problem-profiles/\">problems</a>, and they often seem impossible to solve.</p>\n<p>So when we started <a href=\"https://80000hours.org/\">80,000 Hours</a> \u2014 with the aim of helping people do good with their careers \u2014 one of the first questions we asked was \u201chow much difference can one person really make?\u201d</p>\n<p>We learned that while many common ways to do good, such as becoming a doctor, have less impact than you might first think; others have allowed certain people to achieve an extraordinary impact.</p>\n<p>In other words, one person <em>can</em> make a difference, but they might have to do something a little unconventional.</p>\n<p>In this article, we start by estimating how much good you could do by becoming a doctor. Then, we share stories about some of the highest-impact people in history, and consider what they mean for your career.</p>\n<h2><strong>How much impact do doctors have?</strong></h2>\n<p>Many people who want to help others become doctors. One of our early readers, Dr. Greg Lewis, did exactly that. \u201cI want to study medicine because of a desire I have to help others,\u201d he wrote on his university application, \u201cand so the chance of spending a career doing something worthwhile [is something] I can\u2019t resist.\u201d</p>\n<p>So, we wondered: how much difference does becoming a doctor really make? In 2012, we teamed up with Greg to find out.</p>\n<p>Since a doctor\u2019s main purpose is to improve health, we tried to figure out how much extra \u201chealth\u201d one doctor actually adds to humanity. We found that, on average in the course of their career, a doctor in the UK will enable their patients to live an extra combined 140 years of healthy life, either by extending their lifespans or by improving their overall health. There is, of course, a huge amount of uncertainty in this figure, but the real figure is unlikely to be more than ten times higher than 140.<sup class=\"footnote-ref\"><a href=\"#fn-xGZymCzTFobxHH9wz-1\" id=\"fnref-xGZymCzTFobxHH9wz-1\">[1]</a></sup></p>\n<p>Using a standard conversion rate (used by the World Bank among other institutions) of 30 extra years of healthy life to one \u201clife saved,\u201d 140 years of healthy life is equivalent to 5 lives saved. This is clearly a significant impact, however it\u2019s less of an impact than many people expect doctors to have.</p>\n<p>There are three main reasons for this:</p>\n<ol>\n<li>Researchers largely agree that medicine has only increased average life expectancy by a few years. Most gains in life expectancy over the last 100 years have instead occurred due to better nutrition, improved sanitation, increased wealth, and other factors.</li>\n<li>Doctors are only one part of the medical system, which also relies on nurses and hospital staff, as well as overhead and equipment. The impact of medical interventions is shared between all of these elements.</li>\n<li>Most importantly, there are already a lot of doctors in the developed world, so if you don\u2019t become a doctor, someone else will be available to perform the most critical procedures. <em>Additional</em> doctors therefore only enable us to carry out procedures that deliver less significant and less certain results.</li>\n</ol>\n<p>This last point is illustrated by the chart below, which compares the impact of doctors in different countries. The y-axis shows the amount of ill health in the population, measured in Disability-Adjusted Life Years (aka \u201cDALYs\u201d) per 100,000 people, where one DALY equals one year of life lost due to ill health. The x-axis shows the number of doctors per 100,000 people.</p>\n<p><img src=\"https://cdn.80000hours.org/wp-content/uploads/2017/03/80K_graph_Dalys-doctors_V1-01.jpg\" alt=\"\"></p>\n<p><em>DALYs per 100,000 people versus doctors per 100,000 people. We used WHO data from 2004. The line is the best fitting hyperbola determined by non-linear least square regression. Full explanation in <a href=\"https://drive.google.com/file/d/0B2kbwTsBB1fwZThKLVEtME5idU0/view\">this paper</a>.</em></p>\n<p>You can see that the curve goes nearly flat once you have more than 150 doctors per 100,000 people. After this point (which almost all developed countries meet), additional doctors only achieve a small impact on average.</p>\n<p>So if you become a doctor in a rich country like the US or UK, you may well do more good than you would in many other jobs, and if you would be an exceptional doctor, then you\u2019ll have a bigger impact than these averages. But it probably won\u2019t be a huge impact.</p>\n<p>These findings motivated Greg to switch from clinical medicine into public health.</p>\n<h2><strong>Who were the highest-impact people in history?</strong></h2>\n<p>Despite this uninspiring statistic about how many lives a doctor saves, some doctors have had <em>much</em> more impact than this. Let\u2019s look at some examples of the highest-impact careers in history, and see what we might learn from them. First, let\u2019s turn to medical research.</p>\n<p>In 1968, while working in a refugee camp on the border of Bangladesh and Burma, Dr. David Nalin discovered a breakthrough treatment for patients suffering from diarrhea. He realised that giving patients water mixed with the right concentration of salt and sugar would rehydrate them at the same rate at which they lost water. This prevented death from dehydration much more cheaply than did the conventional treatment of using an intravenous drip.</p>\n<p><img src=\"https://cdn.80000hours.org/wp-content/uploads/2016/06/1024px-Cholera_rehydration_nurses.jpg\" alt=\"\"></p>\n<p>Since then, this astonishingly simple treatment has been used all over the world, and the annual rate of child deaths from diarrhea has plummeted from 5 million to 1.3 million. Researchers estimate that the therapy has saved about 50 million lives, mostly children\u2019s.<sup class=\"footnote-ref\"><a href=\"#fn-xGZymCzTFobxHH9wz-2\" id=\"fnref-xGZymCzTFobxHH9wz-2\">[2]</a></sup></p>\n<p>If Dr. Nalin had not been around, someone else would, no doubt, have discovered this treatment eventually. However, even if we imagine that he sped up the roll-out of the treatment by only five months, his work alone would have saved about 500,000 lives. This is a very approximate estimate, but it makes his impact more than 100,000 times greater than that of an ordinary doctor:</p>\n<p><img src=\"https://cdn.80000hours.org/wp-content/uploads/2016/03/80K_doctorschart_V2S2.png\" alt=\"\"></p>\n<p>But even just within medical research, Dr. Nalin is far from the most extreme example of a high-impact career. For example, one estimate puts Karl Landsteiner\u2019s discovery of blood groups as saving tens of millions of lives.<sup class=\"footnote-ref\"><a href=\"#fn-xGZymCzTFobxHH9wz-3\" id=\"fnref-xGZymCzTFobxHH9wz-3\">[3]</a></sup></p>\n<p><img src=\"https://cdn.80000hours.org/wp-content/uploads/2016/03/80K_doctorschart_V2S3.png\" alt=\"\"></p>\n<p>Leaving the medical field, <a href=\"https://80000hours.org/career-guide/high-impact-jobs/\">later in the guide</a>, we\u2019ll cover the story of a hugely impactful mathematician, Alan Turing, and bureaucrat, Viktor Zhdanov.</p>\n<p>Or, let\u2019s think even more broadly. Roger Bacon and Galileo pioneered the scientific method, without which none of the discoveries we covered above would have been possible, along with other major technological breakthroughs like the Industrial Revolution. These individuals were able to do vastly more good than even outstanding medical practitioners.</p>\n<h3><strong>The unknown Soviet Lieutenant Colonel who saved your life</strong></h3>\n<p><img src=\"https://cdn.80000hours.org/wp-content/uploads/2016/04/Stanislav-Petrov.png\" alt=\"\"></p>\n<p>Or consider <a href=\"http://www.bbc.com/news/world-europe-24280831\">the story of Stanislav Petrov,</a> a Lieutenant Colonel in the Soviet army during the Cold War. In 1983, Petrov was on duty in a Soviet missile base when early warning systems apparently detected an incoming missile strike from the United States. Protocol dictated that the Soviets order a return strike.</p>\n<p>But Petrov didn\u2019t push the button. He reasoned that the number of missiles was too small to warrant a counterattack, thereby disobeying protocol.</p>\n<p>If he had ordered a strike, there\u2019s at least a reasonable chance hundreds of millions would have died. The two countries may have even ended up engaged in an all-out nuclear war, leading to billions of deaths and, potentially, the end of civilisation. If we\u2019re being conservative, we might quantify his impact by saying he saved one billion lives. But that could be an underestimate, because a nuclear war would also have devastated scientific, artistic, economic and all other forms of progress leading to a huge loss of life and well-being over the long run. Yet even with the lower estimate, Petrov\u2019s impact likely dwarfs that of Nalin and Landsteiner.</p>\n<p><img src=\"https://cdn.80000hours.org/wp-content/uploads/2016/03/80K_doctorschart_V2S4.png\" alt=\"\"></p>\n<h2><strong>What does this spread in impact mean for your career?</strong></h2>\n<p>We\u2019ve seen that some careers have had huge positive effects, and some have vastly more than others.</p>\n<p>Some component of this is due to luck \u2013 the people mentioned above were in the right place at the right time, affording them the opportunity to have an impact that they might not have otherwise received. You can\u2019t guarantee you\u2019ll make an important medical discovery.</p>\n<p>But it wasn\u2019t all luck: Landsteiner and Nalin chose to use their medical knowledge to solve some of the most harmful health problems of their day, and it was foreseeable that someone high up in the Soviet military could have a large impact by preventing conflict during the Cold War. So, what does this mean for you?</p>\n<p>People often wonder how they can \u201cmake <strong>a</strong> difference\u201d, but if some careers can result in thousands of times more impact than others, this isn\u2019t the right question. Two career options can both \u201cmake a difference\u201d, but one could be dramatically better than the other.</p>\n<p>Instead, the key question is, \u201chow can I make <strong>the most</strong> difference?\u201d In other words: what can you do to give yourself a chance of having one of the highest-impact careers? Because the highest-impact careers achieve so much, a small increase in your chances means a great deal.</p>\n<p>The examples above also show that the highest-impact paths might not be the most obvious ones. Being an officer in the Soviet military doesn\u2019t sound like the best career for a would-be altruist, but Petrov probably did more good than our most celebrated leaders, not to mention our most talented doctors. Having a big impact might require doing something a little unconventional.</p>\n<p>So how much impact can <em>you</em> have if you try, while still doing something personally rewarding? It\u2019s not easy to have a big impact, but there\u2019s a lot you can do to increase your chances. That\u2019s what we\u2019ll cover in the <a href=\"https://80000hours.org/career-guide/anyone-make-a-difference/\">next couple of articles</a>.</p>\n<p>But first, let\u2019s clarify what we mean by \u201cmaking a difference\u201d. We\u2019ve been talking about lives saved so far, but that\u2019s not the only way to do good in the world.</p>\n<h2><strong>What does it mean to \u201cmake a difference?\u201d</strong></h2>\n<p>Everyone talks about \u201cmaking a difference\u201d or \u201cchanging the world\u201d or \u201cdoing good\u201d or \u201cimpact\u201d, but few ever define what they mean.</p>\n<p>So here\u2019s our definition. Your social impact is given by:</p>\n<blockquote>\n<p>The number of people whose lives you improve, and how much you improve them, over the long-term.</p>\n</blockquote>\n<p>This means you can increase your social impact in three ways: by helping more people, or by helping the same number of people to a greater extent (pictured below), or by doing something which has benefits that last for a longer time.</p>\n<p>We think the latter is especially important, because many of our actions affect <a href=\"https://80000hours.org/articles/future-generations/\">future generations</a>.</p>\n<p>For example, if you improve the quality of government decision-making, you might not see many quantifiable short-term results, but you will have solved lots of other problems over the long term.</p>\n<p><img src=\"https://cdn.80000hours.org/wp-content/uploads/2016/04/Social-impact-how-to-change-the-world-help-people-more-or-help-more-people-1024x667.png\" alt=\"\"></p>\n<h3><strong>Optional: Why did we choose this definition?</strong></h3>\n<p>Many people disagree about what it means to make the world a better place. But most agree that it\u2019s good if people have happier, more fulfilled lives, in which they reach their potential. So our definition captures this idea.</p>\n<p>Moreover, as we\u2019ll show, some careers do far more to improve lives than others, so it captures a really important difference between options. If some paths can do good equivalent to saving hundreds of lives, while others have little impact at all, that\u2019s an important difference.</p>\n<p>But, the definition is also broad enough to cover many different ways to make the world a better place. It\u2019s even broad enough to cover environmental protection, since if we let the environment degrade, the future of civilisation might be threatened. In that way, protecting the environment improves lives.</p>\n<p>Many of our readers also expand the scope of their concern to include non-human animals, which is one reason why we did a profile on <a href=\"https://80000hours.org/problem-profiles/factory-farming/\">factory farming</a>.</p>\n<p>That said, the definition doesn\u2019t include <em>everything</em> that might matter. You might think the environment deserves protection even if it doesn\u2019t make people better off. Similarly, you might value things like justice and aesthetic beauty for their own sake.</p>\n<p>In practice, our readers value many different things. Our approach is to focus on how to improve lives, and then let people independently take account of what else they value. To make this easier, we try to highlight the main value judgments behind our work. It turns out there\u2019s a lot we can say about how to do good in general, despite all these differences.</p>\n<h3><strong>How to measure social impact?</strong></h3>\n<p>We are always uncertain about how much impact different actions will have, but that\u2019s okay, because we can use probabilities to make the comparison. For instance, a 90% chance of helping 100 people is roughly equivalent to a 100% chance of helping 90 people. Though we\u2019re uncertain, we can quantify our uncertainty and make progress.</p>\n<p>Moreover, we can still use rules of thumb to compare different courses of action. For instance, in an upcoming article we argue that, all else equal, it\u2019s higher-impact to work on neglected areas. So, even if we can\u2019t precisely <em>measure</em> social impact, we can still be <em>strategic</em> by picking neglected areas. We\u2019ll cover many more rules of thumb for increasing your impact in the upcoming articles.</p>\n<p>(Read more about <a href=\"https://80000hours.org/articles/the-meaning-of-making-a-difference/\">our definition of social impact</a> and the <a href=\"https://80000hours.org/key-ideas/#the-big-picture\">value assumptions that lie behind our work</a>.)</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-xGZymCzTFobxHH9wz-1\" class=\"footnote-item\"><p>In our <a href=\"https://80000hours.org/career-guide/top-careers/profiles/medical-careers/\">career review on medical careers</a>, which is based on the research we mention, we provide an optimistic all considered estimate of 4 DALYs averted per year (mean). Over a 35 year career, that\u2019s 140 DALYs averted. Individual doctors will do more or less depending on their ability and speciality. (Source: <a href=\"https://openknowledge.worldbank.org/bitstream/handle/10986/7039/364010PAPER0Gl101OFFICIAL0USE0ONLY1.pdf?sequence=1\">World Bank</a>, p. 402, retrieved 31 March 2016.) <a href=\"#fnref-xGZymCzTFobxHH9wz-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-xGZymCzTFobxHH9wz-2\" class=\"footnote-item\"><p>Since the adoption of this inexpensive and easily applied intervention, the worldwide mortality rate for children with acute infectious diarrhoea has plummeted from 5 million to about 1.3 million deaths per year. Over fifty million lives have been saved in the past 40 years by the implementation of ORT. (Source: Science Heroes. <a href=\"https://web.archive.org/web/20160304060242/http://scienceheroes.com/index.php?option=com_content&amp;view=article&amp;id=65&amp;Itemid=113\">Archived link</a>, retrieved 4 March 2016.)</p>\n<p>Very roughly, this means 50/40 = 1.25 million lives have been saved per year. So if Dr Nalin sped up the discovery by 5 months (just a guess), that means that (5/12)*1.25 = 0.52 million extra lives were saved by his actions. This is a highly approximate estimate and could easily be off by an order of magnitude. See more comments in the next footnote <a href=\"#fnref-xGZymCzTFobxHH9wz-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-xGZymCzTFobxHH9wz-3\" class=\"footnote-item\"><p>Every source quoted an amazing number of transfusions and potential lives saved in countries and regions worldwide. High-impact years began around 1955 and calculations are loosely based on 1 life saved per 2.7 units of blood transfused. In the USA alone an estimated 4.5 million lives are saved each year. From these data, I determined that 1.5% of the population was saved annually by blood transfusions and I applied this percentage on population data from 1950-2008 for North America, Europe, Australia, New Zealand, and parts of Asia and Africa. (Source: Science Heroes. <a href=\"https://web.archive.org/web/20160304060254/http://scienceheroes.com/index.php?option=com_content&amp;view=article&amp;id=128&amp;Itemid=137\">Archived link</a>, retrieved 4 March 2016.)</p>\n<p>This rate may inflate the effectiveness of transfusions in the early decades, but also excludes the developing world entirely.</p>\n<p>If we assume a constant number of lives saved per year, then that\u2019s about 10 million lives per year. If he sped up the discovery by two years, then that\u2019s 20 million lives saved.</p>\n<p>This is a highly approximate estimate and could easily be off by an order of magnitude in either direction, and seem more likely to be too high than too low. We\u2019re a bit sceptical of the Science Heroes figures. Moreover, our attempt at modelling the speed-up is very simple. Since most of the lives were saved in the modern era once a large number of people had medical care, it\u2019s possible that speeding up the discovery wouldn\u2019t have had much impact at all. On the other hand, the discovery of blood groups probably made other scientific advances possible, and we\u2019re ignoring their impact. Nevertheless, the basic point stands: Landsteiner\u2019s impact was likely vastly greater than that of a typical doctor. <a href=\"#fnref-xGZymCzTFobxHH9wz-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "Benjamin_Todd"}}, {"_id": "4DfTbSR5zBbHhMQ3C", "title": "Three ways anyone can make a difference, no matter their job", "postedAt": "2017-04-15T22:19:19.123Z", "htmlBody": "<p>No matter which career you choose, anyone can make a difference by donating to charity, engaging in advocacy, or volunteering \u2014 especially if you do so in the most effective ways we\u2019ll introduce here.</p>\n<p>Unfortunately, many attempts to help others in these ways are ineffective, and some even make things worse.</p>\n<p>Take sponsored skydiving, which has become a popular way to raise money for charity in the UK. Every year, thousands of people collect donations for good causes and throw themselves out of planes to draw attention to whatever charity they\u2019ve chosen to support. This sounds like a win-win: The fundraiser gets an exhilarating once-in-a-lifetime experience while raising money for a worthy cause.</p>\n<p><img src=\"https://cdn.80000hours.org/wp-content/uploads/2017/04/Skydiving_over_Cushing_wide.jpg\" alt=\"parachute\"></p>\n<p>However, according to a study of two popular parachuting centres, over a five-year period (1991 to 1995) approximately 1,500 people went skydiving for charity and collectively raised more than \u00a3120,000. That sounds pretty impressive \u2014 until you consider a few caveats.</p>\n<p>First, the cost of the diving expeditions came out of the donations. So of the \u00a3120,000 raised, only \u00a345,000 went to charity.</p>\n<p>Second, because most of the skydivers were first-time jumpers, they suffered a combined total of 163 injuries, resulting in an average hospital stay of nine days.</p>\n<p>In order to treat these injuries, the UK\u2019s National Health Service spent around \u00a3610,000. That means that for every \u00a31 raised for the (usually health-related) charities, the health service spent roughly \u00a313, so the net effect was to <em>reduce</em> resources for health services.<sup class=\"footnote-ref\"><a href=\"#fn-25ZcuHAKBzmYsg9EF-1\" id=\"fnref-25ZcuHAKBzmYsg9EF-1\">[1]</a></sup></p>\n<p>What about volunteering? One problem is that volunteers need to be managed. If untrained volunteers use the time of trained managers, it\u2019s easy for them to cost the organisation more than the value they add.</p>\n<p>In fact, the main reason many volunteering schemes persist is that if someone is a volunteer for an organisation, they are more likely to donate. When FORGE <a href=\"https://web.archive.org/web/20081025050124/http://www.socialedge.org/blogs/forging-ahead/archive/2008/10/20/how-we-got-into-this-financial-crunch\">cut their volunteering scheme</a> to be more effective, they inadvertently triggered a big drop in donations.</p>\n<p>So while <a href=\"https://80000hours.org/articles/volunteering/\">volunteering can be effective in the right circumstances</a>, it\u2019s often not.</p>\n<p>In our research, we\u2019ve found that <em>any college graduate in a rich country can do a huge amount to improve the lives of others</em> \u2014 and they can do this without changing jobs, or making big sacrifices.</p>\n<p>We\u2019ll cover three examples: donating 10% of your income to an effective charity, advocating for important causes, and helping others be more effective.</p>\n<h2><strong>1. Donating effectively</strong></h2>\n<p>How can you take whichever job you find the most personally rewarding, <em>and</em> do a huge amount of good?</p>\n<p>Give 10% of your income to the world\u2019s poorest people. It\u2019s as simple as that.</p>\n<h3><strong>How much of a difference can donations make? A lower bound</strong></h3>\n<p>Since 2008, <a href=\"https://www.givedirectly.org/\">GiveDirectly</a> has made it possible to give cash directly to the poorest people in East Africa via mobile phone.</p>\n<p>We don\u2019t think this the most effective way to donate to charity by any means \u2013 later we\u2019ll discuss higher-impact approaches \u2013 but it\u2019s simple and quantifiable, so it makes a good starting point.</p>\n<p>As <a href=\"https://80000hours.org/articles/money-and-happiness/\">we\u2019ve written about</a>, the more money you have, the less additional money will improve your life. For instance, in the US, doubling your income is only associated with about half a point gain in life satisfaction, on a scale of one to ten.</p>\n<p>These surveys have been extended across the world. There are examples in the chart below. (Though note this shows correlation only \u2013 <a href=\"https://80000hours.org/articles/money-and-happiness/\">more detail on this research</a>.)</p>\n<p><img src=\"https://cdn.80000hours.org/wp-content/uploads/2016/06/80K_lifesatisfaction_graphV3.png\" alt=\"life satisfaction graph\"></p>\n<p><em>Stevenson, Betsey, and Justin Wolfers. Subjective well-being and income: Is there any evidence of satiation? No. w18992. National Bureau of Economic Research, 2013. <a href=\"https://web.archive.org/web/20160610135416/http://www.brookings.edu/~/media/research/files/papers/2013/04/subjective-well-being-income/subjective-well-being-income.pdf\">Archived link</a></em>, retrieved June 2016.</p>\n<p>The poor served by GiveDirectly in Kenya have an average individual consumption of about $500 per year.<sup class=\"footnote-ref\"><a href=\"#fn-25ZcuHAKBzmYsg9EF-2\" id=\"fnref-25ZcuHAKBzmYsg9EF-2\">[2]</a></sup> This figure is based on how much $500 could buy in the US, meaning it already takes account of the fact that money goes further in poor countries.</p>\n<p>The average US college graduate had an annual individual working income of about $77,000 in 2017, or $54,000 post-tax.<sup class=\"footnote-ref\"><a href=\"#fn-25ZcuHAKBzmYsg9EF-3\" id=\"fnref-25ZcuHAKBzmYsg9EF-3\">[3]</a></sup> This means that, assuming the above relationship holds, a dollar will do about 108 times more good if you give it to someone in Kenya rather than spending it on yourself.<sup class=\"footnote-ref\"><a href=\"#fn-25ZcuHAKBzmYsg9EF-4\" id=\"fnref-25ZcuHAKBzmYsg9EF-4\">[4]</a></sup></p>\n<p>If someone earning that average level of income were to donate 10%, they could <em>double the annual income of 11 people</em> living in extreme poverty each year. Over the course of this person\u2019s career, they could have a major positive impact on hundreds of people.</p>\n<p>Grace, 48, is a <a href=\"https://www.givedirectly.org/blog-post.html?id=7814451799017800737\">typical recipient of donations from GiveDirectly</a>. She\u2019s a widow who lives with four children:</p>\n<blockquote>\n<p>\"I would like to use part of the money to build a new house, since my house is in a very bad condition. Secondly, I would wish to pay fees for my son to go to a technical institute\u2026</p>\n</blockquote>\n<blockquote>\n<p>My proudest achievement is that I have managed to educate my son in secondary school.</p>\n</blockquote>\n<blockquote>\n<p>My biggest hardship in life is [that I] lack a proper source of income.</p>\n</blockquote>\n<blockquote>\n<p>My current goals are to build and own a pit latrine and dig a borehole since getting water is a very big problem.\"</p>\n</blockquote>\n<p>GiveDirectly <a href=\"http://www.givewell.org/International/top-charities/give-directly#FindingsfromtheRCT\">conducted a randomised controlled trial of their programme</a>, and found that recipients experienced significant reductions in hunger, stress, and other bad outcomes for years after receiving the transfers. These results add to <a href=\"http://www.givewell.org/International/top-charities/give-directly#Generallyspeakingareunconditionalcashtransfersapromisingapproachtohelpingpeople\">substantial existing literature showing that cash transfers have significant benefits</a>.</p>\n<h3><strong>How much sacrifice will this involve?</strong></h3>\n<p>Normally when we think of doing good with our careers, we think of paths like becoming a teacher or charity worker, which often pay under half what you could earn in the private sector, and may not align with your skills or interests. Compared to switching to those careers, giving 10% of your income could easily be less of a sacrifice.</p>\n<p>Moreover, as <a href=\"https://80000hours.org/articles/money-and-happiness/\">we\u2019ve discussed</a>, once you start earning more than about $40,000 per year as an individual, extra income won\u2019t affect your happiness that much \u2014 while acts that help others, like giving to charity, probably do make you happier.</p>\n<p>To take just one example, one study found that in 122 of 136 countries, if respondents answered \u2018yes\u2019 to the question \u2018did you donate to charity last month?\u2019, their life satisfaction was higher by an amount also associated with a doubling of income.<sup class=\"footnote-ref\"><a href=\"#fn-25ZcuHAKBzmYsg9EF-5\" id=\"fnref-25ZcuHAKBzmYsg9EF-5\">[5]</a></sup> In part, this is probably because happier people give more, but we expect some of the effect runs the other way too.</p>\n<p>(Read more on <a href=\"https://web.archive.org/web/20171001114910/https://www.givingwhatwecan.org/sites/givingwhatwecan.org/files/attachments/giving-without-sacrifice.pdf\">whether giving 10% of your income is better or worse for your happiness</a> than not donating at all.)</p>\n<h3><strong>How to have a bigger impact than being a doctor</strong></h3>\n<p>The reason donations can be so effective is that it\u2019s possible that you can send your money to the best organisations in the world, working on the biggest and most neglected issues. Although many charities aren\u2019t effective, the best are.</p>\n<p>And while GiveDirectly is certainly an effective charity, there are others that some experts argue are even better. <a href=\"http://www.givewell.org/\">GiveWell</a>, a leading independent charity evaluator, estimates that the Against Malaria Foundation can prevent a death for about every $4,000 in donations it receives.<sup class=\"footnote-ref\"><a href=\"#fn-25ZcuHAKBzmYsg9EF-6\" id=\"fnref-25ZcuHAKBzmYsg9EF-6\">[6]</a></sup> In addition, this provides other benefits that come with the treatment of malaria \u2013 such as overall quality of life and increased income \u2013 and this causes further ripple effects over time.</p>\n<p>With a typical US graduate salary, donating 10% of your income to the Against Malaria Foundation could therefore save more than one life every year.</p>\n<p>These kinds of proven, cost-effective health programmes offer such a good opportunity to do good that even <a href=\"https://web.archive.org/save/http://blog.givewell.org/2015/11/06/the-lack-of-controversy-over-well-targeted-aid/\">the most prominent aid sceptics</a> have offered few arguments against them.</p>\n<p>One life saved per year would amount to 40 lives saved over a 40-year career. In a <a href=\"https://80000hours.org/career-guide/how-much-difference-can-one-person-make/\">previous article</a>, we estimated that a typical doctor in clinical medicine saves five lives over their career. So by donating 10% of your income, you could achieve eight times as much impact.</p>\n<p>We\u2019ve just used the Against Malaria Foundation and GiveDirectly to provide a concrete lower-bound on what you can achieve. We actually think there are many charities that are even more effective.</p>\n<p>Some charities work on issues that seem even higher stakes and more neglected, such as preventing a catastrophic pandemic. See our <a href=\"https://80000hours.org/problem-profiles/\">summary of which issues we think are most pressing</a> (or this <a href=\"https://80000hours.org/career-guide/world-problems/\">more popular introduction</a>).</p>\n<p>Some charities use approaches that potentially offer a greater scale of impact per dollar than direct services, such as research and advocacy. We think it\u2019s often possible to do more good by taking a <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">\u2018hits based\u2019</a> approach.</p>\n<p>Read more about how to identify <a href=\"https://80000hours.org/articles/best-charity/\">the most effective charity to donate to</a>.</p>\n<p>Whatever you think is the best way to spend the money, the impact of making donating 10% of your income more normal would be profound. If everyone in the richest 10% of the world\u2019s population donated 10% of their income, that would be $4.4 trillion per year.<sup class=\"footnote-ref\"><a href=\"#fn-25ZcuHAKBzmYsg9EF-7\" id=\"fnref-25ZcuHAKBzmYsg9EF-7\">[7]</a></sup> That would be enough to double scientific research funding, raise everyone in the world above the $1.90/day poverty line, provide universal basic education, and still have plenty left to fund a renaissance in the arts, go to Mars, and then invest a trillion dollars in mitigating climate change. None of this would be straightforward to achieve, but it at least illustrates the enormous potential of greater giving.<sup class=\"footnote-ref\"><a href=\"#fn-25ZcuHAKBzmYsg9EF-8\" id=\"fnref-25ZcuHAKBzmYsg9EF-8\">[8]</a></sup></p>\n<h3><strong>How is this possible?</strong></h3>\n<p>It\u2019s astonishing that we can do so much good while sacrificing so little. How is this possible?</p>\n<p>Consider one of the most important graphs in economics, the graph of world income:</p>\n<p><img src=\"https://cdn.80000hours.org/wp-content/uploads/2017/04/80K_articles_worldincome_V5-01.jpg\" alt=\"world income graph\"></p>\n<p><em>PovcalNet and Milanovi\u0107<sup class=\"footnote-ref\"><a href=\"#fn-25ZcuHAKBzmYsg9EF-9\" id=\"fnref-25ZcuHAKBzmYsg9EF-9\">[9]</a></sup></em></p>\n<p>The x-axis shows the percentage of people in the world who earn each level of income (as indicated by the y-axis). Income has been adjusted to indicate how much that specific dollar amount will buy in a person\u2019s home country (i.e. purchasing power parity). If the world were completely equal, the line would be horizontal.</p>\n<p>As citizens of countries like the US and the UK, we know we\u2019re rich by global standards, but we don\u2019t usually think of ourselves as the richest people in the world \u2014 we\u2019re not the bankers, CEOs or celebrities, after all. But actually, if you earn $54,000 per year after taxes and don\u2019t have kids, then globally speaking, <em>you are the 1%</em>.</p>\n<p>Find out how rich <em>you</em> are by using <a href=\"https://howrichami.givingwhatwecan.org/how-rich-am-i\">this quick calculator</a> (which uses the same data we use in the chart, adjusted for inflation up to 2013).</p>\n<p>These numbers are approximate, but it\u2019s still the case that if you\u2019re reading this, you are very likely in that big spike on the right of the graph (and perhaps even way off the chart), while almost everyone else in the world is in the flat bit at the bottom that you can hardly even see.</p>\n<p>There\u2019s no reason to be embarrassed by this fact, but it does emphasise how important it is to consider how you can use your good fortune to help others. In a more equal world, we could just focus on helping those around us, and making our own lives go well. But it turns out we have an enormous opportunity to help other people with little cost to ourselves \u2014 and it would be a terrible shame to squander it.</p>\n<h3><strong>Take action right now</strong></h3>\n<p>All of us at 80,000 Hours have been so persuaded by these arguments that we\u2019ve pledged to give at least 10% of our lifetime income to the world\u2019s most effective charities.</p>\n<p>Some of the team, <a href=\"https://www.youtube.com/watch?v=9iwJht-GI-g\">like Habiba</a>, actually went a bit further and pledged to donate all income above a certain threshold, chosen to be the amount they think they need to stay happy and productive, adjusted for inflation.</p>\n<p>We did it through an organisation called <a href=\"https://www.givingwhatwecan.org/\">Giving What We Can</a>, with whom we are partnered.</p>\n<p>Giving What We Can enables you to take a public pledge to give 10% of your income to the charities you believe are most effective.</p>\n<p>You can take the pledge in just a few minutes. It\u2019s likely to be the most significant thing you can do right now to do more good with your life.</p>\n<p>It\u2019s not legally binding, you can choose where the money goes, and if you\u2019re a student, it only commits you to give 1% until after you graduate. You\u2019ll be joining over 5,000 people who\u2019ve collectively pledged over a billion dollars.</p>\n<p>The pledge is not for everyone. We\u2019d recommend being cautious if you\u2019re planning to have an impact mainly through your work, and especially if that might involve lower-wage work (e.g. at a charity), if you have significant debt or financial problems, or if you\u2019re not sure you can stick to it.</p>\n<p>If you\u2019re interested in taking the pledge but not quite ready yet, Giving What We Can allows you to pledge to give as little as 1% of your income for any period you choose to see how it goes before making any long-term commitment with the <a href=\"https://www.givingwhatwecan.org/get-involved/try-giving/\">Try Giving pledge</a>. Another option is to <a href=\"https://www.effectivealtruism.org/get-involved/set-up-an-effective-legacy/\">donate your will</a> or <a href=\"https://80000hours.org/career-guide/how-to-be-successful/#8-use-these-tips-to-save-more-money\">see this advice on saving money</a>.</p>\n<p>If you\u2019d like to learn more about how to pick an effective charity, <a href=\"https://80000hours.org/articles/best-charity/\">read our article</a>.</p>\n<h2><strong>2. How to help through effective political advocacy</strong></h2>\n<p><img src=\"https://cdn.80000hours.org/wp-content/uploads/2016/11/freedom-to-vote.jpg\" alt=\"freedom to vote\"></p>\n<p>Just as we happen to be rich by virtue of where we were born, we also happen to have political influence for the same reason.</p>\n<p>Rich countries have a disproportionate impact on issues like global trade, migration, climate change, and technology policy, and are generally at least partly democratic. So if you\u2019d prefer to do something besides giving money, consider advocating for important issues.</p>\n<p>We were initially sceptical that one person could have real influence through political advocacy, but when we dug into the numbers, we changed our minds.</p>\n<p>Let\u2019s take perhaps the simplest example: voting in elections. Several studies have used statistical models to estimate the chances of a single vote determining the US presidential election. Because the US electoral system is determined at the state level, if you live in a state that strongly favours one candidate, your chance of deciding the outcome is effectively zero. But if you live in a state that\u2019s contested, your chances rise to between one in ten million and one in a million. That\u2019s quite a bit higher than your chances of winning the lottery.</p>\n<p>Remember, the US federal government is very, very big. Let\u2019s imagine one candidate wanted to spend 0.2% more of GDP on foreign aid. That would be about $144 billion extra foreign aid over their four-year term.<sup class=\"footnote-ref\"><a href=\"#fn-25ZcuHAKBzmYsg9EF-10\" id=\"fnref-25ZcuHAKBzmYsg9EF-10\">[10]</a></sup> One millionth of that is $144,000 dollars. So if voting takes you an hour, it could be the most important hour \u2014 the highest in expected value -\u2014you\u2019ll spend that year. (The figures are similar in other rich countries \u2013 smaller countries have less at stake, but each vote counts for more. Read more about these estimates on <a href=\"https://80000hours.org/articles/is-voting-important\">why voting is important</a>.)</p>\n<p>We\u2019ve used the example of voting since it\u2019s quantifiable, but we expect the basic idea \u2014 the very small chance of changing a very big thing \u2014 applies to other forms of (well-chosen) advocacy, such as petitioning your Congressperson, getting out the vote for the right candidate, or going to a town hall. We think this is likely to be even more true if you\u2019re careful to focus on the <a href=\"https://80000hours.org/problem-profiles/\">most important and more neglected issues</a>, though more work is needed to determine the best policies to promote.</p>\n<p>If you\u2019d like to explore advocacy more, <a href=\"https://80000hours.org/topic/strategies/advocacy-strategies/\">see all our articles and podcasts on the topic</a>.</p>\n<h2><strong>3. Make a difference by being a \u2018multiplier\u2019 \u2014 and help others be more effective</strong></h2>\n<p>Suppose you don\u2019t have any money or power, and you don\u2019t feel like you can contribute by working on an important problem. What then?</p>\n<p>One option is to try to change that. We cover how to invest in yourself \u2014 no matter what job you have \u2014 <a href=\"https://80000hours.org/career-guide/how-to-be-successful/\">in a separate article</a>.</p>\n<p>That aside, you might know someone who does have some money, power, or skills. So you can make a difference by helping them achieve more.</p>\n<p>For instance, suppose you\u2019ve come across a high-impact job, but you\u2019re not sure it\u2019s a good fit for your skills. If you can tell someone else about the job, and <em>they</em> take it, that does as much good as taking it yourself, and in fact more if they\u2019re a better fit for it than you (and saves you a lot of time!).</p>\n<p>This is an example of being a <em>multiplier</em>. By mobilising others, it\u2019s often possible to do more than you could through just your own efforts.</p>\n<p>For whichever global issues you think are most pressing and neglected, you can find ways to share them with others. This is not about preaching, or struggling to convince people. Only share with people you think will find the issue interesting; you probably know some who would.</p>\n<p>This is also not just about <a href=\"https://www.youtube.com/watch?v=gNz-P5WMQ8w\">raising awareness</a>. Try to identify concrete actions that people can take which might help (like taking a specific job), and spread knowledge of those. Many people are interested in contributing if there\u2019s an opportunity that\u2019s actually effective.</p>\n<p>Besides spreading issues, you can also spread important <em>norms</em> to guide action. There are many norms that are important to promote in society \u2014 see <em><a href=\"https://forum.effectivealtruism.org/posts/3PmgXxBGBFMbfg4wJ/everyday-longtermism\">Everyday Longtermism</a></em> by Owen Cotton-Barratt for a discussion in the context of longtermism.</p>\n<p>One nice feature of taking a multiplier strategy is that you can multiply the efforts we mentioned earlier. For instance, if you pledge to donate 10% of your income to charity, you might well encourage someone else to do it as well, doubling your impact. Leading by example can be powerful, as many of our readers have discovered. <a href=\"https://80000hours.org/podcast/episodes/cass-sunstein-how-change-happens/\">Cass Sunstein\u2019s research</a> suggests that a few early adopters can sometimes trigger a chain of adoption by many others.</p>\n<p>As another example of a multiplier, it\u2019s often possible to raise more for charity through fundraising than you might be able to donate yourself. One easy example is to <a href=\"https://www.thelifeyoucansave.org/blog/donate-your-birthday-and-raise-money-for-charity/\">\u2018donate your birthday\u2019</a>. Or, if you work at a company with a donation matching scheme, you might be able to <a href=\"https://forum.effectivealtruism.org/posts/BG68BvbaaT32u76kr/how-we-promoted-ea-at-a-large-tech-company-1\">encourage other employees to make use of it</a>.</p>\n<p>If you\u2019re interested in <a href=\"https://80000hours.org/key-ideas/#effective-altruism\">effective altruism</a>, then an especially good option in this category is to help run a <a href=\"https://www.effectivealtruism.org/get-involved/start-an-ea-local-group/\">local effective altruism group</a> and to <a href=\"https://www.effectivealtruism.org/get-involved/run-a-local-ea-event/\">host events in your area</a> or <a href=\"https://www.effectivealtruism.org/get-involved/do-workplace-activism/\">workplace groups</a>. It\u2019s often possible to get several other people interested in having a big impact, doing several times as much good as you might do by yourself.</p>\n<p>If you\u2019re still at university, you have especially good opportunities to reach people. We know several cases of people who seemed to have more impact by running a student group part-time than they did in their first years of work.</p>\n<p>Another example of a multiplier strategy is to find someone who\u2019s having a huge impact, and then to help them achieve more.</p>\n<p>Kyle became the assistant to Nick Bostrom, a researcher he thinks is doing world-changing work. If he can save Nick time compared to the next-best assistant, then he\u2019s enabling him to perform more research, and so also contributing to the world-changing work. Read more about <a href=\"https://80000hours.org/articles/high-impact-careers/#Be-research-manager-or-a-PA-for-someone-doing-really-valuable-work\">being a high-impact assistant</a>.</p>\n<p>But you can also support people you know who are doing great work without changing jobs. For example, you can help them by giving them a great recommendation for a productivity tool. Or, if you have more experience in an area, you could give advice to someone less experienced.</p>\n<p>What matters is that more good gets done, not that you do it with your own hands. We\u2019re reminded of an old (most likely fictional) story about a time when President John F. Kennedy visited NASA. Upon meeting a janitor, Kennedy asked him what he was doing. The janitor replied, \u201cWell, Mr. President, I\u2019m helping put a man on the moon\u201d.</p>\n<p>In summary, you can be a multiplier by:</p>\n<ol>\n<li>Telling people about high-impact opportunities</li>\n<li>Spreading knowledge about effective solutions to neglected global problems</li>\n<li>Leading by example and spreading good norms.</li>\n<li>Running a local effective altruism group</li>\n<li>Fundraising</li>\n<li>Finding someone who\u2019s having a big impact and helping them increase it</li>\n</ol>\n<p>You can also read more about <a href=\"https://80000hours.org/articles/volunteering/\">effective volunteering</a>.</p>\n<h2><strong>Conclusion: Anyone can make a difference</strong></h2>\n<p>So, good news: you don\u2019t need to throw yourself out of a plane to do good. In fact, there are far easier (and safer) ways to have an impact that are much more effective.</p>\n<p>Due to our fortunate positions in the world, there\u2019s a lot we can do to make a difference without making significant sacrifices, whatever jobs we end up in.</p>\n<p>Here are some key ways to make a big positive impact without changing jobs:</p>\n<ol>\n<li>Give 10% of your income to effective charities</li>\n<li>Use your political influence, such as by voting or going to a town hall</li>\n<li>Help others have an impact, through mentoring, leading by example, or introducing them to ideas or resources</li>\n</ol>\n<p>You might like to consider taking the 10% pledge right now.</p>\n<p>Or take a moment to consider how else you might be able to make a big impact with little sacrifice.</p>\n<p>What if you want to make a difference <em>directly</em> through your career? If you can achieve so much with just 10% of your income, then what you could achieve with your entire job over decades could be huge. That\u2019s what we aim to help you do with most of our work at 80,000 Hours.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-25ZcuHAKBzmYsg9EF-1\" class=\"footnote-item\"><p>Of 174 patients with injuries of varying severity, 94% were first-time charity-parachutists. The injury rate in charity-parachutists was 11%, at an average cost of \u00a33,751 per casualty. Sixty-three percent of casualties who were charity-parachutists required hospital admission, representing a serious injury rate of 7%, at an average cost of \u00a35,781 per patient. The amount raised per person for charity was \u00a330. Each pound raised for charity cost NHS \u00a313.75 in return.</p>\n<p>Source: \u201cParachuting for charity: is it worth the money? A 5-year audit of parachute injuries in Tayside and the cost to the NHS.\u201d CT, Lee, P, Williams and WA, Hadden. (1999). <a href=\"http://web.archive.org/web/20170406091512/https://www.ncbi.nlm.nih.gov/pubmed/10476298\">Archived link</a>, retrieved April 2017.</p>\n<p>We\u2019ve been told by skydivers that safety has improved significantly since the 1990s, so it might not be as bad an idea these days. Nevertheless, it\u2019s still an example of ineffective do-gooding that was pursued by over 1,000 people, and we think you can probably do much better even than modern-day parachuting. <a href=\"#fnref-25ZcuHAKBzmYsg9EF-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-25ZcuHAKBzmYsg9EF-2\" class=\"footnote-item\"><p>\u201cOverall, mean and median daily per capita consumption among eligible households are $0.65 and $0.55 at nominal rates, and 74% are below the Kenyan poverty line, indicating a very poor population.\u201d</p>\n<p>Source: GiveDirectly, Offering Memorandum (January 2012), Pgs 23-24, as quoted on GiveWell\u2019s report on GiveDirectly, <a href=\"https://web.archive.org/web/20170409004412/http://www.givewell.org/charities/give-directly\">Archived link</a>, retrieved 8 April 2017.</p>\n<p>To convert USD0.65 per day to a \u2018purchasing power parity\u2019 figure, we use the nominal KES to USD exchange rate used in the <a href=\"https://web-beta.archive.org/web/20140629072420/http://web.mit.edu/joha/www/publications/haushofer_shapiro_uct_2013.11.16.pdf\">relevant GiveDirectly study</a> (62.38:1) to convert this to 40.55 Kenyan Shillings.</p>\n<p>We then use the <a href=\"http://data.worldbank.org/indicator/PA.NUS.PPP?locations=KE\">World Bank\u2019s PPP conversion factors</a> for 2011 (the year most of the study was conducted), which indicates that 34.30 KES buys the equivalent of $1 in the US. This suggests an effective consumption level of $USD1.18 in that year.</p>\n<p>We then inflation-adjust that figure using the US Consumer Price Index from 2011 to 2017, arriving at a figure of $US1.28 per person per day in 2017.\n1.28 * 365 = $467.2</p>\n<p>This figure is, of course, imprecise. It\u2019s difficult to compare the purchasing power of people living in different countries and circumstances. We discuss some of the problems <a href=\"https://80000hours.org/2017/04/how-accurately-does-anyone-know-the-global-distribution-of-income/\">here</a>. There are reasons it might be both too low and too high. However, we\u2019d be surprised if it were off by more than a factor of five. Moreover, all the official estimates we\u2019ve seen of the income of the poorest billion people agree that they are about 10 times poorer than almost everyone living in a rich country, and about 100 times poorer than someone living on an upper middle class salary in a rich country. <a href=\"#fnref-25ZcuHAKBzmYsg9EF-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-25ZcuHAKBzmYsg9EF-3\" class=\"footnote-item\"><p>Source: Carnevale, Anthony P., Stephen J. Rose, and Ban Cheah. \u201cThe college payoff: Education, occupations, lifetime earnings.\u201d (2011).\n<a href=\"https://cew.georgetown.edu/wp-content/uploads/2014/11/collegepayoff-complete.pdf\">Link</a>.</p>\n<p>Working: \u201cThe mean (average) earnings of those with a Bachelor\u2019s degree is $500,000 higher than the median ($2.7 million).\u201d</p>\n<p>They are using a 40-year career to calculate annual working income:</p>\n<p>\u201cOverall, the median lifetime earnings for all workers are $1.7 million, which is just under $42,000 per year ($20 per hour). Over a 40-year career, those who didn\u2019t earn a high school diploma or GED are expected to bring in less than $1 million, which translates into slightly more than $24,000 a year ($11.70 per hour).\u201d</p>\n<p>So 2.7 million / 40 = $67,500</p>\n<p>The paper was released in 2011, but wages have grown since then. In January 2011, average wages per hour in the US were $22.85, and they were $26 in January 2017. That\u2019s a growth of 14%, which suggests that the average college graduate now earns $77,000. This is likely an underestimate, because college graduate earnings have been growing faster than average earnings.</p>\n<p>Source: <a href=\"https://fred.stlouisfed.org/series/CES0500000003\">FRED Economic Data</a>, \u201cAverage Hourly Earnings of All Employees: Total Private (CES0500000003)\u201d, retrieved 5 Feb 2017.</p>\n<p>This growth only matches inflation, which was also <a href=\"http://www.inflation.eu/inflation-rates/united-states/historic-inflation/cpi-inflation-united-states.aspx\">about 14% over the period</a>.</p>\n<p>If you\u2019re trying to predict what you\u2019ll earn in the long term, then you should also take into account future wage growth, but we\u2019re ignoring that.</p>\n<p>To calculate post-tax income, we plugged $77,000 into the Smart Asset online income tax calculator for someone living in California, and it came out at $54,000 post-tax. This includes federal income tax, FICA, and state tax, working out at an effective rate of 28%. California generally has higher taxes, so this is an upper bound. (As of 3 April 2017.)</p>\n<p>If you choose to have a child, then you would also need to support them for 18+ years. This would reduce your effective income by about 25% during that period. We <a href=\"https://80000hours.org/career-guide/job-satisfaction/#fn-8\">explain more here</a>. <a href=\"#fnref-25ZcuHAKBzmYsg9EF-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-25ZcuHAKBzmYsg9EF-4\" class=\"footnote-item\"><p>If the relationship between income and wellbeing is logarithmic, then doubling someone\u2019s income increases their wellbeing by a constant amount. That means if someone has an income of $54,000 and another has an income of $500, you\u2019d need to increase the first person\u2019s income by $54,000 to increase their wellbeing as much as you would if you increased the second person\u2019s income by $500. $54000/$500 = 108. We explain why we think the relationship is logarithmic (or perhaps even weaker) in our <a href=\"https://80000hours.org/articles/money-and-happiness/\">evidence review on income and happiness.</a></p>\n<p>In addition there is empirical evidence for significant benefits. GiveDirectly has had <a href=\"https://www.givedirectly.org/research-at-give-directly/\">randomised controlled-trials performed on their programmes</a> by academics, and there is a wider literature showing benefits from cash transfers \u2014 see <a href=\"https://forum.effectivealtruism.org/posts/PPNgQSvKrcKvsM2ZN/the-effect-of-cash-transfers-on-subjective-well-being-and\">here</a> and <a href=\"http://www.givewell.org/International/top-charities/give-directly#Generallyspeakingareunconditionalcashtransfersapromisingapproachtohelpingpeople\">here</a>. <a href=\"#fnref-25ZcuHAKBzmYsg9EF-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-25ZcuHAKBzmYsg9EF-5\" class=\"footnote-item\"><p>This is the study we quoted: Aknin, Lara, Christopher P. Barrington-Leigh, Elizabeth W. Dunn, John F. Helliwell, Robert Biswas-Diener, Imelda Kemeza, Paul Nyende, Claire Ashton-James, Michael I. Norton (2010). \u201cProsocial Spending and Well-Being: Cross-Cultural Evidence for a Psychological Universal.\u201d Harvard Business School Working Paper 11-038. <a href=\"http://www.hbs.edu/faculty/Publication%20Files/11-038.pdf\">Link</a>.</p>\n<p>Though there is some evidence that part of the reason for the correlation is that happier people give more. See: Boenigk, S. &amp; Mayr, M.L. J Happiness Stud (2016) 17: 1825. doi:10.1007/s10902-015-9672-2. <a href=\"https://link.springer.com/article/10.1007/s10902-015-9672-2\">Link</a>.</p>\n<p>For a more comprehensive review of the question, see <em>Giving without sacrifice</em>, by Andreas Mogensen, Giving What We Can Research, <a href=\"http://web.archive.org/web/20170406103218/https://www.givingwhatwecan.org/sites/givingwhatwecan.org/files/attachments/giving-without-sacrifice.pdf\">Archived Link</a>, retrieved 6 April 2017. <a href=\"#fnref-25ZcuHAKBzmYsg9EF-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-25ZcuHAKBzmYsg9EF-6\" class=\"footnote-item\"><p>GiveWell\u2019s estimate of the cost to save a life through the Against Malaria Foundation has varied over time between $1,000 and $10,000. See <a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/cost-effectiveness-models\">their latest cost-effectiveness models</a>.</p>\n<p>Our understanding as of December 2020 is that <a href=\"https://docs.google.com/spreadsheets/d/16XOOB1oWse1ICbF0OVXUYtwWwpvG3mxAAQ6LYAAndQU/edit#gid=1034883018\">the median estimate of the staff</a> is that \u201ccost per life saved\u201d via donating to AMF is $4,106 and via Helen Keller International is $2,795.</p>\n<p>GiveWell also reported how cost effective they think these charities are compared to GiveDirectly, considering a wider range of effects (e.g. improvements to education and income), and typically estimate that they\u2019re between 10 and 20 times more effective.</p>\n<p>10% of $77,000 is $7,700, which is enough to prevent at least one death per year.</p>\n<p>Of course, there is a lot more to say about how valuable these donations are when we try to consider <em>all</em> the possible effects. You can read more about the philosophical problem of <a href=\"https://forum.effectivealtruism.org/posts/LdZcit8zX89rofZf3/evidence-cluelessness-and-the-long-term-hilary-greaves\">cluelessness</a>. In general, we\u2019d encourage you to consider which global problems you think are most pressing, all things considered (we take a longtermist perspective), and find the best organisations working to address those issues. <a href=\"https://80000hours.org/articles/best-charity/\">Read more about how to pick a charity</a>. <a href=\"#fnref-25ZcuHAKBzmYsg9EF-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-25ZcuHAKBzmYsg9EF-7\" class=\"footnote-item\"><p>This is based on estimates suggesting that the top 10% of income earners account for a bit over 50% of world income (PPP) from <a href=\"http://documents.worldbank.org/curated/en/959251468176687085/pdf/wps6259.pdf\">Global Income Inequality by the Numbers: in History and Now</a> written by Branko Milanovi\u0107 and published by The World Bank Development Research Group. While these figures are from 2008, we expect they remain fairly accurate today.</p>\n<p>So, if the top 10% give 10%, then that\u2019s 5% of world income. World income is about $88 trillion, so that\u2019s $4.4 trillion. Source: <a href=\"http://data.worldbank.org/indicator/NY.GDP.MKTP.CD\">World Bank</a>, retrieved 3 Dec 2020. <a href=\"#fnref-25ZcuHAKBzmYsg9EF-7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-25ZcuHAKBzmYsg9EF-8\" class=\"footnote-item\"><p>Economists have estimated it would cost $159 billion to close the \u2018poverty gap\u2019, or the distance between every poor person\u2019s income and the global poverty line of $1.90 a day.</p>\n<p>\u201cThe average person in extreme poverty lives on $1.33 per day. It would therefore take just $0.57 per day to rescue them from this plight. That observation invites a thought experiment. If it were somehow possible to transfer without cost the right amount of money into the right hands, how much would it take to end extreme poverty altogether? The answer is just $159 billion a year, according to the World Bank, or less than 0.2% of global GDP.\u201d</p>\n<p>Source: <em>How the other tenth lives</em>, The Economist, 2016, <a href=\"http://web.archive.org/web/20170406110217/http://www.economist.com/news/finance-and-economics/21708245-world-should-be-both-encouraged-and-embarrassed-latest-global-poverty\">Archived Link</a>, retrieved 6 April 2017.</p>\n<p>Research and development expenditure as a % of GDP was about 2.1% in 2013.</p>\n<p>Source: World Bank, 2013 data, <a href=\"http://data.worldbank.org/indicator/GB.XPD.RSDV.GD.ZS\">Link</a>, retrieved April 2017.</p>\n<p>Assuming this proportion has stayed roughly constant, if world GDP is about $88 trillion annually (see footnote 6) it would cost about $1.7 trillion per year to double global R&amp;D.</p>\n<p>Globally an estimated 263 million children and youth are out of school. If we suppose it would cost $1,000 each per year to provide them with education, that would be $263 billion per year.</p>\n<p>Source: <em>UNESCO Institute for Statistics</em>, <a href=\"http://web.archive.org/web/20170406161236/http://www.unesco.org/new/en/education/themes/leading-the-international-agenda/education-for-all/single-view/news/263_million_children_and_youth_are_out_of_school_from_primar/\">Archived Link</a>, retrieved 6 April 2017.</p>\n<p>In 2012, contributions to the arts totalled $31 billion annually, so it would cost $31 billion to double it.</p>\n<p>Source: <a href=\"http://web.archive.org/web/20170406163541/https://www.arts.gov/sites/default/files/how-the-us-funds-the-arts.pdf\">Archived Link</a>, retrieved 6 April 2017.</p>\n<p>According to Wikipedia, going to Mars has been <a href=\"https://en.wikipedia.org/wiki/Human_mission_to_Mars#Funding\">estimated to cost $500 billion</a>. Though it suggests this is likely an underestimate, we also wouldn\u2019t have to pay for it all in one year, so will go with this figure for our purposes.</p>\n<p>Summing all the above with a trillion put toward mitigating climate change gets us to just over $3.5 trillion annually, meaning we\u2019d still have nearly a trillion to spare. So, although all these figures are very imprecise (and budgets often blow up), we don\u2019t doubt the basic point that it would be a huge amount of resources to the most urgent problems.</p>\n<p>If this many resources were actually suddenly given to charity, it would take time for the economy to adapt, corrupt leaders might try to extract it from their citizens, and there might be other unpredictable effects \u2014 it certainly wouldn\u2019t be straightforward to use them effectively. However, these figures at least show there is the potential for enormous gains from greater and more effective charitable giving.<a href=\"https://80000hours.org/career-guide/making-a-difference/#fn-ref-8\">\u21a9</a> <a href=\"#fnref-25ZcuHAKBzmYsg9EF-8\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-25ZcuHAKBzmYsg9EF-9\" class=\"footnote-item\"><p>For a detailed discussion of the origins and accuracy of this graph, see our blog post <a href=\"https://80000hours.org/2017/04/how-accurately-does-anyone-know-the-global-distribution-of-income/\">How accurately does anyone know the global distribution of income?</a>.</p>\n<p>Briefly, the data for percentiles 1 to 79 were taken from <a href=\"http://iresearch.worldbank.org/PovcalNet/home.aspx\">PovcalNet</a>: the online tool for poverty measurement developed by the Development Research Group of the World Bank. Note that this is in fact a measure of <em>consumption</em>, which closely tracks income and is the standard way of tracking the wealth of people towards the lower part of the distribution. The data for income percentiles 80 to 99 were provided by <a href=\"https://en.wikipedia.org/wiki/Branko_Milanovi%C4%87\">Branko Milanovi\u0107</a> in private correspondence. <a href=\"#fnref-25ZcuHAKBzmYsg9EF-9\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-25ZcuHAKBzmYsg9EF-10\" class=\"footnote-item\"><p>According to the <a href=\"http://data.worldbank.org/indicator/NY.GDP.MKTP.CD?locations=US\">World Bank</a> US GDP was about $18 trillion in 2015, 0.2% of that is $36 billion (data retrieved April 2017). Over a four-year term, that\u2019s $144 billion. <a href=\"#fnref-25ZcuHAKBzmYsg9EF-10\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "80000_Hours"}}, {"_id": "Zt82Dbx7fodsMPYaX", "title": "SHIC's Recommended Charities 2017", "postedAt": "2017-05-16T17:56:32.696Z", "htmlBody": "<html><body><p>Better late than never!</p>\n<p>As is the goal with everything SHIC publishes, this post is meant to be accessible to those unfamiliar with Effective Altruism. Read the full post here: <a href=\"http://www.shicschools.org/single-post/2017/05/15/SHIC-Recommended-Charities-2017\">http://www.shicschools.org/single-post/2017/05/15/SHIC-Recommended-Charities-2017</a></p>\n<p>--------</p>\n<h2>Why SHIC is updating its list</h2>\n<p>When <a href=\"/shicschools.org\">Students for High-Impact Charity (SHIC)</a> was founded in early 2016, we knew that the organization would need to go beyond teaching about which factors make a charity high-impact. We would offer a list of recommended charities so that those who are new to the concepts we espouse would be provided tangible examples of causes and charities that are among the best in the world (by metrics related to reducing suffering). The list was not meant to be comprehensive of all high-impact charities, but we felt it was adequately representative (without feeling overwhelming) of causes to be covered by SHIC&#x2019;s material.</p>\n<p>At the end of 2016, both Animal Charity Evaluators (ACE) and GiveWell, two reputable charity evaluators, updated their top recommendations for the giving season. You can find <a href=\"http://blog.givewell.org/2016/11/28/updated-top-charities-giving-season-2016/\">GiveWell&#x2019;s latest recommendations here</a> and <a href=\"https://animalcharityevaluators.org/blog/updated-recommendations-december-2016/\">ACE&#x2019;s here</a>.</p>\n<p>Since then, we&#x2019;ve decided reevaluate SHIC&#x2019;s list of recommended charities. Though SHIC will continue to maintain a relatively broad stance with regards to cause selection, we would still like to recommend charities front and center that adequately represent the values we see as critical to being high-impact, as we feel it&#x2019;s important to give students concrete examples as models for highly effective charities. &#xA0;</p>\n<p>In addition, SHIC has always maintained the policy of recommending charities making direct impact rather than &#x201C;meta-charities&#x201D; (charities whose primary purpose includes&#xA0;supporting other specific charities). This is because SHIC aims to introduce effective giving to an audience who may be new to the concept of charity evaluators. Recommending meta-charities takes students a step away from direct impact.</p>\n<p>The SHIC program will nonetheless continue promoting selected charity evaluators and meta-charities through our curriculum. We will maintain focus on the importance of forming one&#x2019;s own opinion based on critical thinking, while providing the tools and roadmaps that can aid in forming those conclusions.</p>\n<p>Moreover, supporting carefully chosen charities signals the implicit value of specific ethical approaches we endorse and would like to instill within students as a logical foundation for doing the most good in the world. Some of these concepts include:</p>\n<ul>\n<li>\n<p>Interventions with strong evidence for high cost-effectiveness are preferable when such data are is available.</p>\n</li>\n<li>\n<p>When cost-effectiveness information is unavailable, assess a cause based on its counterfactual impact, solvability, and neglectedness.</p>\n</li>\n<li>\n<p>If we value all human lives equally, we&#x2019;re generally better off supporting interventions in developing countries than developed ones.</p>\n</li>\n<li>\n<p>Non-human animal lives and lives in the future are important to consider when prioritizing causes.</p>\n</li>\n<li>\n<p>When possible, we should respect the preferences of those we&#x2019;re aiming to help. Oftentimes the global poor can best identify their own self-interests.</p>\n</li>\n<li>\n<p>Factory farming is by far the greatest source of non-human animal suffering currently caused by humans.</p>\n</li>\n<li>\n<p>It&#x2019;s important to recognize we don&#x2019;t have all the answers and to continually update our viewpoints based on new evidence.</p>\n</li>\n</ul>\n<h2>Charities remaining on the list</h2>\n<p>From our original list, the following charities will remain among SHIC&#x2019;s Recommended Charities:</p>\n<ul>\n<li>\n<p><strong>The Against Malaria Foundation</strong> - provides long-lasting insecticide-treated bednets.</p>\n</li>\n<li>\n<p><strong>Mercy for Animals</strong> - spreads awareness about the horrors of factory farming.</p>\n</li>\n<li>\n<p><strong>Cool Earth</strong> - forms partnerships with villages bordering rainforests to determine the best ways to protect them.</p>\n</li>\n<li>\n<p><strong>Schistosomiasis Control Initiative</strong> - implements deworming programs to rid children of neglected tropical diseases.</p>\n</li>\n<li>\n<p><strong>GiveDirectly</strong> - provides unconditional cash transfers to help people escape poverty.</p>\n</li>\n<li>\n<p><strong>Development Media International</strong> - &#xA0;promotes healthy living in developing countries through radio programming.</p>\n</li>\n<li>\n<p><strong>Future of Humanity Institute</strong> - researches potential causes of and solutions to global catastrophic risks.</p>\n</li>\n<li>\n<p><strong>Living Goods</strong> - employees locals in developing countries to sell affordable health-related items door-to-door.</p>\n</li>\n</ul>\n<h2>Charities added to the list</h2>\n<p>The following charities will be added to SHIC&#x2019;s list of Recommended Charities:</p>\n<ul>\n<li>\n<p><strong>Malaria Consortium</strong> - for their seasonal malaria chemoprevention program.</p>\n</li>\n<li>\n<p><strong>Project Healthy Children</strong> - food nutrient fortification.</p>\n</li>\n<li>\n<p><strong>The Humane League</strong> - interventions including corporate outreach for veganism.</p>\n</li>\n<li>\n<p><strong>The Good Food Institute</strong> - promotion and development of meat/dairy alternatives.</p>\n</li>\n<li>\n<p><strong>Machine Intelligence Research Institute</strong> - studying responsible implementation of artificial intelligence.</p>\n</li>\n</ul>\n<h2>Subgrouping recommended charities</h2>\n<p>To make our growing list of charities more digestible, we have divided charities into their respective cause areas. We recognize that some charities could fall into multiple subgroups, but don&#x2019;t see this as significant concern. This division allows visitors/participants to pick up on the general themes we endorse much more easily than if we simply listed the charities.</p>\n<h3>Disease Prevention</h3>\n<ul>\n<li>\n<p><strong>Against Malaria Foundation</strong></p>\n</li>\n<li>\n<p><strong>Schistosomiasis Control Initiative</strong></p>\n</li>\n<li>\n<p><strong>Malaria Consortium</strong></p>\n</li>\n<li>\n<p><strong>Project Healthy Children</strong></p>\n</li>\n</ul>\n<h3>Human Empowerment</h3>\n<ul>\n<li>\n<p><strong>GiveDirectly</strong></p>\n</li>\n<li>\n<p><strong>Development Media International</strong></p>\n</li>\n<li>\n<p><strong>Living Goods</strong></p>\n</li>\n</ul>\n<h3>Animal Welfare</h3>\n<ul>\n<li>\n<p><strong>Mercy for Animals</strong></p>\n</li>\n<li>\n<p><strong>The Humane League</strong></p>\n</li>\n<li>\n<p><strong>The Good Food Institute</strong></p>\n</li>\n</ul>\n<h3>Environmental and Catastrophic Risk</h3>\n<ul>\n<li>\n<p><strong>Cool Earth</strong></p>\n</li>\n<li>\n<p><strong>Future of Humanity Institute</strong></p>\n</li>\n<li><strong>Machine Intelligence Research Institute</strong></li>\n</ul>\n<p>--------</p>\n<p>For more, including our reasoning for choosing&#xA0;to include or exclude specific charities in/from our list, read the full post <a href=\"http://www.shicschools.org/single-post/2017/05/15/SHIC-Recommended-Charities-2017\">here</a>.</p></body></html>", "user": {"username": "baxterb"}}, {"_id": "T4F6ZW5Hzv4Tz9Mrm", "title": "The value of money going to different groups", "postedAt": "2017-05-16T13:11:45.984Z", "htmlBody": "<html><body><p>We all know that an extra dollar&#xA0;is worth more to you the poorer you are. That&apos;s why it can be good to donate money to an organisation like GiveDirectly even when&#xA0;a few cents in the dollar get used up in transaction costs. But how much more is it worth? Economists have a good quantitative&#xA0;model of what is going on, which can enable us to make rough comparisons about whether, say, people on $1,000 per year would get more value from an extra $100 than people on $2,000 per year would get from $200.&#xA0;This can help us work out how much additional cost we should bear to get money to the very poorest people.</p>\n<p>It can also be useful for improving our thinking about the relative values of different financial flows such as remittances and aid. It is easy to find out the sizes of these in dollars, but what about the size in terms of value to the individuals? If the individuals in one case are substantially&#xA0;richer, then this can really change things.</p>\n<p>I&apos;ve written <a href=\"https://www.centreforeffectivealtruism.org/blog/the-value-of-money-going-to-different-groups\">an article</a> explaining how all of this works up on centerforeffectivealtruism.org. Have a read and let me know what you think.</p></body></html>", "user": {"username": "Toby_Ord"}}, {"_id": "8AABHubnwpoqZXABh", "title": "Are Giving Games a better way to teach philanthropy?", "postedAt": "2017-05-13T00:36:41.371Z", "htmlBody": "<html><body><p>Wouldn&#x2019;t it be nice if our educational system taught students about good giving?&#xA0; The good news is that over $8 million has been spent teaching university students about philanthropy.&#xA0; The bad news is that the prevailing model of student philanthropy hasn&#x2019;t grown for the better part of a decade and at best reaches a few thousand people a year.</p>\n<p>EAs will probably find a some irony in my analysis of the history of the philanthropy education sector: the organizations responsible for teaching students about effective giving do so using an intervention that provides very little bang for the buck.&#xA0; But I also show how <a href=\"https://www.thelifeyoucansave.org/Giving-Games\">Giving Games</a> and other models that deploy resources where they&#x2019;ll provide the highest marginal return offer the potential to teach philanthropy at mass scale.</p>\n<p>&#xA0;Full article here, originally published in Alliance Magazine:</p>\n<p><a href=\"https://www.thelifeyoucansave.org/Blog/ID/1355/Are-Giving-Games-a-Better-Way-to-Teach-Philanthropy\">https://www.thelifeyoucansave.org/Blog/ID/1355/Are-Giving-Games-a-Better-Way-to-Teach-Philanthropy</a></p></body></html>", "user": {"username": "Jon_Behar"}}, {"_id": "ohmc3GKcHz2b38R4S", "title": "Understanding Charity Evaluation", "postedAt": "2017-05-11T14:55:05.711Z", "htmlBody": "<html><body><p>In this short post I try to set out a model of understanding charity evaluation (and cause prioritisation) research, how and when such this research is useful to people and how it can be done better.</p>\n<p>&#xA0;</p>\n<p><strong>A MODEL OF DO-GOODERS</strong></p>\n<p>There are many people who want to make the world a better place by giving money to charity and who would (or could be persuaded) to use some amount of charity evaluation research to help guide their decisions.</p>\n<p>Each such person has a set of moral beliefs about what it means to do good. I like to break these down into their core intrinsic values&#xA0;and the causes they believe in.</p>\n<ul>\n<li>Their core values are a result of moral introspection. For example wanting a happy world or a just world.&#xA0;If someone is asked&#xA0;why they hold their core values there is no underlying reason except a strong intuitive belief. These values are&#xA0;rarely changed by facts about the world.</li>\n<li>The causes they believe in stem from their beliefs about the world and their core values. For example wanting to end poverty or fight crime or fight capitalism.</li>\n</ul>\n<p>The greater extent that such individuals are willing to step back and make decisions based on their core values rather than based on a cause or charity area they stumbled upon at some point then the better decisions they will make and the more applicable any charity evaluation research will be.</p>\n<p>However, the difficulty with producing comprehensive charity evaluation or cause prioritisation advice is that all of these people have different core values, different moral intuitions. They may be subtly different. For example I may want to maximise happiness of everyone in the world (classical utilitarianism) and my friend Sally may want to maximise the fulfilment of preferences of everyone in the world (preference utilitarianism). They may be extremely different. I may not care at all about prevent suffering of animals but my friend Sammy might believe that animals are of equal moral importance to humans.</p>\n<p>&#xA0;</p>\n<p><strong>A PROCESS</strong></p>\n<p>Here is how I have seen charity evaluation research happening:</p>\n<p>1&#x2022; Have&#xA0;an audience _ Find a group of people who care about doing good and would use charity evaluation research and who&#x2019;s core values are at most subtly different. For example Giving What We Can (GWWC) started with utilitarian philosophers.</p>\n<p>2&#x2022; Find a consensus _ Make sure your audience agree on the change they want to see, and compromise where necessary. &#xA0;For example all the utilitarian philosophers founding GWWC wanted a world with more happiness and less suffering.</p>\n<p>3&#x2022; Narrow the scope _ The utilitarian founders of GWWC realised that their &#xA3; could go further in the developing world than the developed world which let them narrow the scope of the charities they were considering.</p>\n<p>4&#x2022; Choose a metric _ Ideally a metric that different charities can be ranked upon. Eg QALYS for early GWWC research. Eg. years of animal suffering spared for early Animal Charity Evaluators research. Eg. reducing suicide rates for people who care about preventing extreme suffering.</p>\n<p>5&#x2022; Rank charities _ Use your metric to put charities or intervention types in order of apparent effectiveness. Eg by QALYs: http://dcp-3.org/sites/default/files/dcp2/DCP02.pdf.</p>\n<p>6&#x2022; In depth investigation _ look in detail at the charities that come top of the list. Check they are actually any good. For example the best intervention at reducing suicide rates does not tackle depression but makes suicide harder by reducing the amount of harmful chemicals in fertiliser. This step requires understanding issues like room for more funding, fungibility and so on. See: http://www.givewell.org/charity-evaluation-questions</p>\n<p>&#xA0;</p>\n<p><strong>THE LIMITS OF THIS PROCESS</strong></p>\n<p>Firstly, even of the people who are going to use the results of such research no one is going to be perfectly happy with the results.</p>\n<p>Some of the utilitarian philosophers involved in the early days of GWWC would&#xA0;care more about the long run economic effects of the interventions (would think it is better to give to a charity with stronger evidence of leading to long run economic growth, eg SCI over AMF) some of them may worry about the knock on effects on animals (would think it is better to give to a charity that saves lives in vegetarian regions like India, eg Deworm the World over SCI)</p>\n<p>&#xA0;</p>\n<p>Secondly, for anyone who has different motivating core values the charity evaluation research is going to unpersuasive and of limited use.</p>\n<p>If I care about making sure that society is just, if I care about the long run more than the short term, if I care about helping the worse off the most, if I care about protecting freedom, then the research done by a bunch of people with utilitarian values is going to be of little use to me.</p>\n<p>&#xA0;</p>\n<p><strong>AGAINST TRYING TO CHANGE OTHERS VALUES</strong></p>\n<p>One response to this might be to assume that if someone else has different moral intuitions to you they clearly have incorrect moral intuitions. I think there is a time and a place for challenging an others moral intuitions and values. However I think in almost all cases it is a poor idea. These beliefs are deeply held hard to change and trying to change them can come across as unaccepting argumentative and unwelcoming. I am not going to defend this position in this post but for a discussion on this see: http://effective-altruism.com/ea/18u/intuition_jousting_what_it_is_and_why_it_should/</p>\n<p>&#xA0;</p>\n<p><strong>SOME CONCLUSIONS</strong></p>\n<p>&#x2022; Apply or improve this methodology. I hope having a written out idea of how charity evaluation happens is useful for analysing and improving how the EA community evaluates charities. I think that the early stages have happened each time the EA community has evaluated charities but I have not seen something like this written up. This is at a higher level than I have seen discussed previously such as by GiveWell or on this forum.</p>\n<p>&#x2022; Be aware of the limits of existing charity evaluation research and recognise the differences in values of others. If you are trying to convince someone who cares primarily about creating a just society of the value of GiveWell&apos;s research this may well be a waste of time (or at least require a much softer approach). Much of the research will not be that relevant to helping them do good.</p>\n<p>&#x2022; Spot the gaps in existing charity evaluation research. See the section below on the <em>London equality and justice cause prioritisation project</em></p>\n<p>&#x2022; I have written this about charity evaluation. I think the model, process and conclusions above rough apply to all cause prioritisation research.</p>\n<p>&#xA0;</p>\n<p><strong>THE LONDON EQUALITY AND JUSTICE CAUSE PRIORITISATION PROJECT</strong></p>\n<p>I think effective altruism is too utilitarian / too welfarist. In particular it feels to me that the effective altruism community has not done research that speaks to people who say they care and are motivated by, above all else, a desire for a just and equal society.</p>\n<p>So I wanted to spark some research to address this. I have put &#xA3;1000 of my donations at the whims of people who care about equality and justice, if they can do some research to find the best charity to give to for them. So far the plan is to roughly follow the process set out above.</p>\n<p>To follow this group please go to: https://www.facebook.com/groups/699926826860322/, where you can read the write up of the first session and see the map of our values. If you are in London Please come to our next event on the 18th May.</p>\n<p>I do not yet know how this will go:</p>\n<ul>\n<li>Perhaps everyone is secretly a utilitarian at heart and is they think about their values for long enough they will realise that.</li>\n<li>Maybe at the end of extensive research a group would just say the same charities currently recommend in the EA community are the best place to give.</li>\n<li>Maybe this task is too difficult and no progress will be made. Maybe the process set out above will not work to deliver conclusions.</li>\n</ul>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p><strong>COMMENTS</strong></p>\n<p>Comments, criticisms of the above, pointing out of spelling mistakes, and so on would be hugely appreciated.</p>\n<p>Is sharing models like this useful? I have a model of how humans think about doing good in the world and about how useful charity&#xA0;evaluation work is. I have tried to put this model into words. However I am uncertain about how useful sharing models like this is?</p>\n<p>I have already received one earnest anonymous criticism that a smaller homogenous EA community is better as it has much less risk of collapse, so trying to expand EA research to people with different values is a bad thing. Do others agree with this?</p>\n<p>&#xA0;</p>\n<p>Sam Hilton</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "weeatquince"}}, {"_id": "SMRHnGXirRNpvB8LJ", "title": "Fact checking comparison between trachoma surgeries and guide dogs ", "postedAt": "2017-05-10T22:33:21.864Z", "htmlBody": "<html><body><p><span>In a</span><a href=\"https://www.ted.com/talks/peter_singer_the_why_and_how_of_effective_altruism\"><span> 2013 TED talk</span></a><span> Peter Singer claims</span></p>\n<blockquote>\n<p><span>&#x201C;It costs about 40,000 dollars to train a guide dog and train the recipient so that the guide dog can be an effective help to a blind person. It costs somewhere between 20 and 50 dollars to cure a blind person in a developing country if they have trachoma.&#x201D;</span></p>\n</blockquote>\n<p>&#xA0;</p>\n<p><span><span>Unfortunately, this claim is not accurate. To begin with, </span><a href=\"http://www.who.int/mediacentre/factsheets/fs382/en/\"><span>blindness from trachoma is irreversible</span></a><span> so it&apos;s only possible to </span><span>prevent</span><span> blindness from trachoma, not to </span><span>cure</span><span> it. According to a</span><a href=\"http://blog.givewell.org/2009/01/30/surgeries-performed-vs-cases-of-blindness-prevented/\"> <span>GiveWell blog post</span></a><span>, it does cost ~$20-60 to perform one trachoma surgery but &#x201C;there can be a small improvement in vision following surgery&#x201D;. According to their back-of-envelope calculation with some assumptions, 1 case of full-blown blindness is averted for every 6-20 successful surgeries.</span><span> In any case, my point is that <a href=\"https://www.youtube.com/watch?v=UKX_xzUGEcI\"><span>people</span></a><a href=\"/ea/70/the_moral_imperative_towards_costeffectiveness/\">&#xA0;<span>who</span></a><a href=\"/ea/4a/to_save_the_world_dont_get_a_job_at_a_charity_go/\">&#xA0;<span>use</span></a><a href=\"http://www.thirdsector.co.uk/effective-altruism-will-donors-change-ways/fundraising/article/1384629\">&#xA0;<span>this</span></a><a href=\"http://www.independent.co.uk/voices/just-giving-isn-t-enough-for-the-new-philanthropists-a6760951.html\">&#xA0;<span>example</span></a> to advertise GiveWell don&apos;t read what GiveWell has to say about it.</span></span></p>\n<p>&#xA0;</p>\n<p><span>-------</span></p>\n<p><span><strong>EDIT</strong> (2017-05-16): Even though GiveWell haven&apos;t made such claim and may have a different opinion, one doctor (who has a much deeper understanding of these issues than me) <a href=\"/ea/19w/we_are_often_giving_wrong_facts_about_trachoma/ay8\">commented</a> that she &quot;</span><span><span>would be comfortable with saying that for about $100 we can prevent trachoma-induced blindness&quot; and that Singer&apos;s claim was not as nearly as inaccurate as I made it seem.</span></span></p>\n<p><span>-------</span></p>\n<p>&#xA0;</p>\n<p><span>As of 2017-05-10, Giving What We Can </span><span>also gives <a href=\"https://www.givingwhatwecan.org/get-involved/what-we-can-achieve/\">a similar example</a>:</span></p>\n<blockquote>\n<p><span>&#x201C;In the developing world there are more than a million people suffering from trachoma-induced blindness and poor vision which could be helped by a safe eye operation, costing only about $100 and preventing 1-30 years of blindness and another 1-30 years of low vision, according to</span><a href=\"http://www.givewell.org/international/technical/programs/SAFE\"> <span>GiveWell.org</span></a><span>&#x201D;</span></p>\n</blockquote>\n<p><span>They also do something EAs (including me) don&#x2019;t do often enough &#x2014; provide a source. The source is</span><a href=\"http://www.givewell.org/international/technical/programs/SAFE\"> <span>a GiveWell page</span></a><span> which is published in 2009 and has a disclaimer</span></p>\n<blockquote>\n<p><span>&#x201C;The content on this page has not been recently updated. This content is likely to be no longer fully accurate, both with respect to the research it presents and with respect to what it implies about our views and positions.&#x201D;</span></p>\n</blockquote>\n<p><span>The page has the following text:</span></p>\n<blockquote>\n<p><span>&#x201C;We have not done thorough cost-effectiveness analysis of this program. Because such analysis is highly time-consuming - and because the results can vary significantly depending on details of the context - we generally do not provide cost-effectiveness analysis for an intervention unless we find what we consider to be a strong associated giving opportunity.</span></p>\n<p><span>We provide some preliminary figures based on the</span><a href=\"http://www.givewell.org/international/technical/criteria/program-evaluation#DiseaseControlPrioritiesReport\"> <span>Disease Control Priorities in Developing Countries report</span></a><span>, which we previously used for cost-effectiveness estimates until we vetted its work in 2011, finding</span><a href=\"http://blog.givewell.org/2011/09/29/errors-in-dcp2-cost-effectiveness-estimate-for-deworming/\"> <span>major errors</span></a><span> that raised</span><a href=\"http://blog.givewell.org/2011/11/04/some-considerations-against-more-investment-in-cost-effectiveness-estimates/\"> <span>general concerns</span></a><span>.</span></p>\n<p><span>We have relatively little information about the likely impact of this program, so it&apos;s difficult to estimate the cost-effectiveness.&#x201D;</span></p>\n<p><span>[...]</span></p>\n<p>&#xA0;</p>\n<p><span>&#x201C;Using a simple</span><a href=\"http://www.givewell.org/research/DALY#InterpretingtheDALYmetric\"><span> conversion calculation</span></a><span>, we estimate that $100 prevents 1-30 years of blindness and an additional 1-30 years of low vision when spent on surgeries (though insignificant benefits, in these terms, when spent on antibiotics). <strong>The source of the Disease Control Priorities in Developing Countries report&apos;s estimate is unclear and these figures should be taken with extreme caution.</strong>&#x201D;</span></p>\n</blockquote>\n<p>&#xA0;</p>\n<p><span>It seems unfair to just provide the numbers and skip all these disclaimers. Despite knowing about this uncertainty, sometimes I feel temptation to also omit disclaimers and just present the numbers to be more convincing. After all, the goal is very admirable - to help more people living in extreme poverty. But I believe that in the long run EA will achieve more if we are being totally honest and upfront about uncertainties and never take any shortcuts. Otherwise we might not be trusted the next time we have something to say. Furthermore, to influence the world we need our community to have a correct model of the world.</span></p>\n<p>&#xA0;</p>\n<p><span>On the other hand, trachoma is a horrible disease. Just watch this excerpt:</span></p>\n<p><span><iframe src=\"//www.youtube.com/embed/hv5iy1G_-Fo?start=212&amp;end=325\"></iframe></span></p>\n<p><span>tl;dw: eyelids turn inwards and eyelashes scrape the eyeball, causing intense pain on every blink. That scraping eventually causes blindness. People treat themselves by pulling out their eyelashes with tweezers. One woman said she does it every 2 weeks. Horrible.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>If you worry about being convincing, you can talk about <em>that</em> and then honestly talk about uncertainty regarding numbers.</span><a href=\"http://lesswrong.com/lw/hw/scope_insensitivity/\"> <span>Most people are scope insensitive</span></a><span> anyway. Or you can talk about cataract surgery instead of trachoma because disclaimers in</span><a href=\"http://www.givewell.org/international/technical/programs/cataract-surgery#ourCEA\"> <span>this page</span></a><span> seem slightly less severe. </span><span>Or just talk about your favorite charity and then add &quot;imagine that suffering could be prevented so cheaply in our country, action would be taken urgently&quot;. But the main points of this post are </span></p>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><span>many of us were overstating the point that money goes further in poor countries</span></p>\n</li>\n<li>\n<p><span><span>many of us </span>don&#x2019;t do enough fact checking, especially before making public claims</span></p>\n</li>\n<li><span>many of us should communicate uncertainty better</span></li>\n</ul>\n<p><span>-------</span></p>\n<p><span><span><span><strong>EDIT</strong> (2017-05-15): </span></span></span></p>\n<p><span><span><span>Many people in the comments gave other reasons not to use the comparison but if you decide to use it anyway and want to quote GiveWell, you could also use figures from this <a href=\"/ea/19w/we_are_often_giving_wrong_facts_about_trachoma/axt\">Peter Singer&apos;s comment</a>. </span></span></span><span><span><span>Alternatively, you can use one of the other <a href=\"/ea/19w/we_are_often_giving_wrong_facts_about_trachoma/ay5\">comparisons proposed by Ben Todd</a>.</span></span></span></p>\n<p>&#xA0;</p></body></html>", "user": {"username": "saulius"}}, {"_id": "PCQ4i5EK7vZ8pR2tF", "title": "Why you should consider going to EA Global", "postedAt": "2017-05-09T14:31:18.798Z", "htmlBody": "<html><body><p><span>My main motivation behind writing this is to help you consider whether going to an </span><a href=\"https://www.eaglobal.org/\"><span>Effective Altruism Global (EAG) conference</span></a><span> this year is worth it. After having doubted the value, I was convinced otherwise at EAGx Oxford 2016. Therefore, I&#x2019;m sharing my personal highlights from that weekend to attempt to demonstrate that these conferences are among the most valuable events *anyone* can attend because they have great content, a unique framework and exceptional attendees.</span></p>\n<p><span>Let&#x2019;s start with a quick overview of the topics from the official sessions I attended:</span></p>\n<p>&#xA0;</p>\n<table><colgroup><col><col></colgroup>\n<tbody>\n<tr>\n<td>\n<ul>\n<li>\n<p><span>High Impact Career Planning</span></p>\n</li>\n<li>\n<p><span>Probability and Statistics</span></p>\n</li>\n<li>\n<p><span>Applied Rationality</span></p>\n</li>\n<li>\n<p><span>Research Heuristics</span></p>\n</li>\n<li>\n<p><span>Logical Fallacies</span></p>\n</li>\n<li>\n<p><span>International Development</span></p>\n</li>\n</ul>\n</td>\n<td>\n<ul>\n<li>\n<p><span>Founding Organisations</span></p>\n</li>\n<li>Big Data</li>\n<li>\n<p><span>Catastrophic and Existential Risks </span></p>\n</li>\n<li>\n<p><span>Utilitarians</span></p>\n</li>\n<li>\n<p><span>On What Matters (III)</span></p>\n</li>\n</ul>\n</td>\n</tr>\n</tbody>\n</table>\n<p>&#xA0;</p>\n<p><span>Most of the time, two other sessions ran simultaneously and a lot of high profile speakers didn&#x2019;t make choosing any easier (</span><a href=\"https://docs.google.com/spreadsheets/d/1PKvTLvtCDGEE6Wjn3l3bP1s9NWQGdQjaV1RqcRW82uY/edit#gid=0\"><span>click here</span></a><span> for the detailed schedule). So within our Genevan team, we made sure we covered all relevant sessions and exchanged notes afterwards. I personally missed the Artificial Intelligence related sessions, so none of them will be part of this post but if that&#x2019;s what you&#x2019;re interested in, take a look at </span><a href=\"http://afterallitcouldbeworse.blogspot.it/2016/11/notes-from-ea-global-oxford.html\"><span>this post that I randomly found while absolutely not procrastinating</span></a><span>.</span></p>\n<p>&#xA0;</p>\n<p><span><img src=\"https://lh5.googleusercontent.com/j94dh5oagOpWFvD_ZJQ2BChM2OiRCx_657s3c15Uboew04uJbEELNaLeQJ3PdptgjS9UTY2ARSOyodGFyhoqXNV5bF487VkE_pDEPR136awHK4LRXm1o2HEO2dDzbmlWhZKgNRv0\"></span></p>\n<p><span>Highlights</span></p>\n<p><span>Humans</span></p>\n<p><span>Online, the EA community can sometimes seem less heartwarming than it really is (like me). However, once you make it to an in-person event it is hard to miss that the movement consists of many lovely humans trying their best to make sure we figure things out in time. Humans are the top highlight of any EA event. Everyone is warm (&#xB1;37&#xB0;C, ideally), open-minded, reasonable and curious. Conversations range from casual chatting to serious truth-seeking</span><span> and everyone is super knowledgeable in the most different matters. Even better, you can ask anyone anything and they&#x2019;ll be happy to help you out.</span></p>\n<p>&#xA0;</p>\n<p><span>I used my free time to reconnect with friends, meet new people and process all the input. The general vibe is super easygoing - almost like you&#x2019;re at a music festival in Portugal, but you&#x2019;re not. You&#x2019;re in one of the world&#x2019;s academic capitals in chilly, rainy England. The speakers could often be spotted at other sessions, too, blending in with the crowd. Acting like mere muggles, you could even ask them mundane questions. Thus, if you want to see what this movement is all about, be inspired and gain motivation to do something: go meet its human subsets at an EAG conference and you will have a hard time not liking it (for those of you who will still have a hard time because you care more about humanity than about its individual subsets, I wrote the next section, I can understand you sometimes).</span></p>\n<h1><span>Top three sessions</span></h1>\n<h1><span>Workshop appetisers from the Center for Applied Rationality (CFAR)</span></h1>\n<p><span>If you&#x2019;re serious about ensuring our best possible future, CFAR is dedicated to turning you into the best goal-achiever</span><span> ever. At the conference, they served three appetisers of their immersive 4-day curriculum compressed into short, one-hour sessions. They assumed that the crowd at the conference was advanced enough to deal with the implementation independently, hence, they mainly explained their reasoning behind each technique and the technique itself.</span></p>\n<p>&#xA0;</p>\n<p><span>I had heard about CFAR and their work but I wasn&#x2019;t aware of just how useful it would be. I can honestly say that I now think through how we go about doing things from discussing to making plans to implementing new habits more convincingly. On top of that, Duncan, the coach, had a lot of great analogies and remarks to make that made the sessions very enjoyable. I hope to attend their full course soon because there&#x2019;s still far too much self-improvement to be done.</span></p>\n<p>&#xA0;</p>\n<p><span>&#x201C;That&#x2019;s what you get if you&#x2019;re running computers made of meat that wrote themselves.&#x201D;</span></p>\n<p><span>- Duncan, CFAR coach</span></p>\n<p>&#xA0;</p>\n<p><span>To give you a more concrete idea, here the three techniques we learned, with explanatory links:</span><span><br></span></p>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><span>Building Blocks of Behaviour Change: &#x2018;</span><a href=\"http://lesswrong.com/r/discussion/lw/o7c/making_intentions_concrete_triggeraction_planning/\"><span>Trigger-Action-Plans</span></a><span>&#x2019;</span><span><br></span><span>TAPs create an incremental transition by iterating a, basically zero-effort, three-step process that is designed to </span><span>&#x201C;summon your sapience&#x201D;</span><span> and let you rewire your brain.</span></p>\n</li>\n</ul>\n<p><span><span>&#xA0;</span></span></p>\n<ul>\n<li>\n<p><span>Navigating Intellectual Disagreement: &#x2018;</span><a href=\"http://lesswrong.com/lw/o6p/double_crux_a_strategy_for_resolving_disagreement/\"><span>Double crux</span></a><span>&#x2019;</span><span><br></span><span>A technique to turn disagreements into a collaborative search for truth, or at least for you, to learn as much as possible from other worldviews and gather more data.</span></p>\n</li>\n</ul>\n<p><strong>&#xA0;</strong></p>\n<ul>\n<li>\n<p><span>Overcoming Planning Biases: &#x2018;</span><a href=\"https://mindlevelup.wordpress.com/planning-101/\"><span>Murphy-Jitsu</span></a><span>&#x2019;</span><span><br></span><span>Murphy-Jitsu is designed to make us think about things we actually can anticipate but usually don&#x2019;t when making future plans. The obvious often is non-obvious to.</span></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<p><span>&quot;The universe is a dark maze and at some point, all of us run into a wall.</span></p>\n<p><span>Face first.</span></p>\n<p><span>Because we had a belief of where to go.&quot;</span><span><br></span><span>- Duncan.</span></p>\n<p>&#xA0;</p>\n<p><span><img src=\"https://lh5.googleusercontent.com/1mr4Sorb4JO8U1VHmE13eOqbIL_EmD0r_aGNkUk1eJNHqXqnxA7RSPjdWTH7yjIWU57hVS1BoXSjBPkn2f6_2kekK3My2TvA3_KS4gD8h7gofeF1FHxyGXCLPBjnu81ogZ307xBG\"></span></p>\n<p><span>Presentation: What do People think about Utilitarians?</span></p>\n<p><span>This talk, by </span><a href=\"http://crockettlab.org\"><span>Molly Crockett</span></a><span> on her research and the conclusions she had come to, was quite interesting to hear about because a large part of the EA community identifies as some kind of utilitarian. However, the word alone seems to divide crowds. Thus, I was eagerly hoping for a few insights on how one can avoid coming across as cold and heartless when presenting trade-offs and calculations that, even when based on global empathy, come off as inhuman(e).</span></p>\n<p>&#xA0;</p>\n<p><span>Crockett&#x2019;s lab found that utilitarians are generally seen as (i) less trustworthy; (ii) less empathic; and (iii) less likely to cooperate. She even claims that humans have developed a default morality - deontology. That might be to signal our value as a cooperator on the partnership market, rooted in the value of implicit social contracts that most of our societal fabric relies on. And utilitarian logic poses a direct threat to this fabric. Therefore, people who claim that it&#x2019;s obvious/easy to sacrifice something - even if it is for the greater good - quickly alienate themselves from society because the rest of the group fears being used.</span></p>\n<p>&#xA0;</p>\n<p><span>It follows that, if we really want to appeal to evidence and reason in our decision-making, we ought to appeal simultaneously to our &#x2018;why&#x2019; - our values, our altruism. Without understanding that, it is understandably off-putting to listen to statistics and cost-benefit-analyses. For EA, that means that we need to emphasise the &#x2018;A&#x2019; part of the movement more proactively, especially when talking about the &#x2018;E&#x2019;. Additionally, the movement could make more of an effort to support individual autonomy and diversity to build an unshakeable basis of trust to thrive and ensure it&#x2019;s not being misunderstood.</span></p>\n<p><span>Presentation: Heavy Tails &amp; Power Laws</span></p>\n<p><span>&#x201C;Normal is not normal!&#x201D;</span><span> proclaimed the Future of Humanity Institute (FHI)&#x2019;s Anders Sandberg. He started his fun talk by explaining why the &#x2018;normal distribution&#x2019;, or &#x2018;bell curve&#x2019; should really only be called &#x2018;Gaussian distribution&#x2019;: except for well-known things, like the intelligence of humans and rolling dice, the Gaussian distribution and Central Limit Theorem</span><span> can be very misleading. That is because we live in &#x2018;Extremistan&#x2019; and figuring things out that we don&#x2019;t already know requires a different mindset here.</span></p>\n<p><span><img src=\"https://lh4.googleusercontent.com/dolPWH9pgsPIJWpYYog3w_dVh4arJAzPhJNH8RxQ_zAnVePZyIlozRacXL_axH6ThIkKg3kw98E77uO_qKUIqNAqIPTRoLPwWKIyc_YAdmC5bBdwbdFwXaLiksx2i96BlLynYhl_\"></span></p>\n<p><span>In Extremistan, freak events (or more beautifully called &#x2018;Black Swans&#x2019;) occur. And when such events occur, they are far more intense than usual events, often triggering further extreme events. This is due to the complex (inter-)dependencies and correlations in our world. Therefore, a &#x2018;real&#x2019; normal distribution might have the centre of a bell curve, but the tails are nowhere close. There are a lot of cascade effects in Extremistan with its fractal-geometrical nature, so expecting most values to lie within two or three standard deviations of the mean is a dangerous assumption we tend to make intuitively.</span><span><br></span><span><br></span><span>Anders&#x2019; talk illustrated how dangerous oversimplifications are, and how unaware we are of so-called &#x2018;Dragon Kings&#x2019;. Or, to say it less beautifully: how unaware we are of maxima that are caused by non-linear dynamics in complex systems, creating statistical outliers that throne above anything we&#x2019;d seen before. Yet, there is hope: studying these dynamics in detail might allow us to see more Black Swans as Dragon Kings - events that we could predict with more complex models. Instead of saying </span><span>&#x201C;oh, that was unlikely&#x201D;</span><span> we ought to say </span><span>&#x201C;oh, the model was wrong&#x201D;</span><span> an awful lot more often.</span></p>\n<p>&#xA0;</p>\n<p><span>This is why the EA movement is trying to figure out how to get into those heavy tails - </span><span>&#x201C;thinking meta matters!&#x201D;</span><span> Figuring out where tails actually cut off and finding the dragon kings could help us prepare for extreme events. No matter how probable such events are, with Dragon Kings you&#x2019;re better safe than sorry. Anders is trying to do exactly that at the FHI; drawing nature&#x2019;s lottery tickets and stacking the deck wherever possible. In addition to that, he&#x2019;s a terrific speaker. Here are </span><a href=\"https://dl.dropboxusercontent.com/u/50947659/Normal%20is%20not%20normal.pdf\"><span>the slides</span></a><span> to this talk. He also gave a </span><a href=\"https://dl.dropboxusercontent.com/u/50947659/Selecting%20ways%20of%20bettering%20humans.pdf\"><span>presentation on Human Enhancement</span></a><span> that I regret not having been to because these slides alone are already incredibly interesting.</span></p>\n<p><span>General takeaways</span></p>\n<p><span>Many different people were talking about policy work as a potential top priority and made me convinced that the movement keeps updating in the right direction. The same goes for designing and giving presentations. At previous events, I was always a little baffled at how bad the slides and how unprepared some speakers were, but I saw only one presentation for which I could say that this time. Along with being more professional, the general organisation and management was done extremely well and even the vegan food choices were quite nice.</span><span><br></span><span><br></span><span>Other than that, I was astonished at how much more low-hanging fruit</span><span> there seems to be in fighting extreme poverty. Two talks on international development outlined how much better we could use (big) data if only it was all publicly available. How much that alone would contribute to ending poverty? $3 trillion/year in value, claims Alena Stern from AidData, who also emphasised that development aid wasn&#x2019;t scientific at all before the nineties. Additionally, if programs weren&#x2019;t divided along country borders but focused on only the poorest regions, we could do a lot more for those who are the worst off. Further, cheaper than creating randomised controlled trials, we could just analyse geospatial data and satellite imagery to set up quasi-experiments</span><span>, all the way back to the eighties. However, even when such data is available, data illiteracy, lack of trust and education are still significant hindrances in the relevant areas. It seems we&#x2019;re still at the beginning of the data revolution, after all.</span><span><br></span><span><br></span><span>The next paragraph is a combination of different talks with implications for the movement&#x2019;s general strategy. Taken from and inspired by:</span></p>\n<p>&#xA0;</p>\n<p><span>(i) Amanda Askell&#x2019;s &#x201C;Look, Leap or Retreat&#x201D;</span></p>\n<p><span>(ii) Owen Cotton-Barratt&#x2019;s &#x201C;</span><a href=\"https://www.youtube.com/watch?v=67oL0ANDh5Y&amp;t\"><span>Prospecting For Gold</span></a><span>&#x201D;</span></p>\n<p><span>(iii) Stefan Schubert&#x2019;s &#x201C;The Younger Sibling Fallacy&#x201D;</span></p>\n<p>&#xA0;</p>\n<p><span>(i) Often, looking and doing research is worth a lot more than we&#x2019;d expect intuitively, especially when the expected value is unclear, because then additional data points will allow us to be significantly more precise about the possible positive outcomes of an action; (ii) if we want to motivate others to join in the (re-)search, we should strategically plan on a group level to make the most out of each person&#x2019;s individual comparative advantage and (ii) we need to avoid shortening our message, as not being understood means risking costly deviations. (iii) As we have the tendency to see others as less proactive than ourselves, we often dismiss their capacities and overlook the (potential for) cascade effects our actions have; (ii) which means that we should always try to move furthest into the very end of the heavy tails.</span></p>\n<p>&#xA0;</p>\n<p><span>One, last thing that was emphasised multiple times, was the value of asking. Most of us still don&#x2019;t do it enough. If one doesn&#x2019;t understand something, if we don&#x2019;t know where to start, if we want people to support us - there&#x2019;s one simple trick: just ask. We tend to feel like there is some social cost to asking but simply asking can provide extremely valuable support and only has downsides if we do it too often. So far, most of us don&#x2019;t do it enough.</span></p>\n<p><span>Conclusion</span></p>\n<p><span>Before going, I didn&#x2019;t expect much from the conference beyond socialising and a lot of fuzzies caused by present humans. Most of the sessions didn&#x2019;t sound like they were going to provide much beyond what I had been reading about every day for the past two years. But then, the pre-conference workshops alone blew my low expectations out of the water before anything had officially started. Only then came the lovely humans and more mind-broadening sessions. And more lovely humans (whom you can ask anything without hesitation). At the very worst, some talks were just a little too superficial, but nothing was bad. My sole suggestion for possible improvements would be to introduce different &#x201C;difficulty&#x201D; tracks, to allow newbies and savants to enjoy different sessions, instead of dividing tracks along topics. That seems complicated to implement though.</span></p>\n<p><br><span>So, seriously, the &#x2018;mind-broadenings per minute&#x2019; and the &#x2018;density of lovely people per m</span><span>2&#x2019;</span><span> seem to reach the global optimum at EAG. </span><a href=\"https://www.eaglobal.org/\"><span>Go, sign up</span></a><span>, be one of these lovely people this year. There&#x2019;s also a lot of financial support available if that&#x2019;s what&#x2019;s keeping you from it.<br>___<br><br>Originally posted on the <a href=\"http://eageneva.org/blog/2017/5/9/heres-why-you-should-consider-going-to-an-ea-global-conference\">EA Geneva blog</a>.</span></p></body></html>", "user": {"username": "konrad"}}, {"_id": "CJZGFxzHfdPuu2X76", "title": "A mental health resource for EA community", "postedAt": "2017-05-06T02:07:05.630Z", "htmlBody": "<p>A lot has been written about handling depression and anxiety, and with good reason! They are very common and can be very debilitating.</p>\n<p>But this piece addresses some less common problems: mania and psychosis. These are not as commonly understood, so people are often ill-equipped to recognize or handle them when they come up.</p>\n<p>Why might the EA community need resources on this topic? We have a lot of young adults, who are particularly likely to be caught unprepared by mental health crises. And we have a lot of people traveling to areas where they have few supports and resources.</p>\n<h2>How common are these problems?</h2>\n<p>The National Institute of Mental Health estimates a 12-month prevalence for the following illnesses (the chance that an adult in the US met the criteria during the last year):</p>\n<ul>\n<li>Bipolar disorder: <a href=\"https://www.nimh.nih.gov/health/statistics/prevalence/bipolar-disorder-among-adults.shtml\">2.6</a>% (a proxy for people who experience mania)</li>\n<li>Schizophrenia: <a href=\"https://www.nimh.nih.gov/health/statistics/prevalence/schizophrenia.shtml\">1.1</a>% (a proxy for people who experience psychosis)</li>\n</ul>\n<p>In other words, if you\u2019re friends with 100 random American adults, around four of them will likely meet the criteria for one of these disorders this year. This doesn\u2019t include people who experience psychosis but don\u2019t meet all the criteria for schizophrenia (for example, because the psychosis is drug-induced).</p>\n<p>A person is most likely to have their first manic episode between age 20-25 (<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/15997605\">source</a>). Men are most likely to experience a first psychotic episode between age 18-25, and women age 25-35 (<a href=\"https://www.hindawi.com/journals/schizort/2012/916198/\">source</a>).</p>\n<h1>About mania</h1>\n<h2>What is mania?</h2>\n<p><a href=\"https://en.wikipedia.org/wiki/Mania\">Mania</a> (or, in its lesser form, hypomania) is a period of heightened emotion, activity, and energy. Some people experience both periods of mania/hypomania and periods of depression, while others experience only mania/hypomania \u2014 these are both forms of <a href=\"https://en.wikipedia.org/wiki/Bipolar_disorder\">bipolar disorder</a>.</p>\n<p>Hypomania might include some of the below signs but be shorter and less intense and not disrupt the person\u2019s life as much. Mania is a more intense version that impairs a person\u2019s normal functioning (for example, through risky behavior).</p>\n<p>A hypomanic or manic episode might look like:</p>\n<ul>\n<li>\n<p>Decreased need for sleep</p>\n</li>\n<li>\n<p>Talking more or faster than usual</p>\n</li>\n<li>\n<p>Feeling euphoric or giddy, \u201con top of the world\u201d</p>\n</li>\n<li>\n<p>More irritable or hostile than usual</p>\n</li>\n<li>\n<p>Feeling your thoughts are moving fast or won\u2019t stop</p>\n</li>\n<li>\n<p>Feeling very motivated, engaging in lots of activities at once</p>\n</li>\n<li>\n<p>Lots of energy</p>\n</li>\n<li>\n<p>More sociable than usual, talking or arguing with everyone</p>\n</li>\n<li>\n<p>Easily distracted by unimportant details</p>\n</li>\n<li>\n<p>Unusually high self-esteem</p>\n</li>\n<li>Fascination with big ideas and grand plans</li>\n<li>\n<p>Pursuing fun and risky activities more than usual: shopping, sex, gambling, drug use, driving fast, unlikely business schemes</p>\n</li>\n<li>\n<p>Feeling your brain is working on a whole new level, everything suddenly makes sense</p>\n</li>\n<li>\n<p>Might lose touch with reality (seeing, hearing, or believing things that aren\u2019t real)</p>\n</li>\n</ul>\n<p>These symptoms can last from days to months. Some people experience some of these at the same time as depression (a \u201cmixed episode.\u201d)</p>\n<p>&nbsp;</p>\n<h2>Common triggers of mania in people who are prone to it:</h2>\n<ul>\n<li>\n<p>Sleep disruption, including due to crossing time zones</p>\n</li>\n<li>\n<p>Stress</p>\n</li>\n<li>\n<p>Recently starting or raising dose of antidepressant medication</p>\n</li>\n<li>\n<p>Stimulants: caffeine, nicotine, cocaine, amphetamines, steroids, appetite suppressants, ADHD medications</p>\n</li>\n<li>\n<p>Some cold medicine and thyroid medicine</p>\n</li>\n<li>\n<p>Season/light changes \u2014 more common in summer</p>\n</li>\n<li>\n<p>Missing doses of psych meds</p>\n</li>\n</ul>\n<h2>&nbsp;</h2>\n<h2>Is hypomania always bad?</h2>\n<p>Some people feel that the euphoria and creativity that comes with hypomania works well for them. Many others find that periods of hypomania, while enjoyable, are often followed by periods of depression or full mania which cause serious problems for them.</p>\n<p>&nbsp;</p>\n<h1>About psychosis</h1>\n<h2>What is psychosis?</h2>\n<p>Psychosis is losing touch with reality.</p>\n<p>This may look like:</p>\n<ul>\n<li>\n<p>Hallucinations (hearing, seeing, smelling, or feeling things that aren\u2019t there). Sometimes people recognize that these aren\u2019t real, while other times they\u2019re very sure they\u2019re experiencing something real. This can be very distressing for them.</p>\n</li>\n<li>\n<p>Delusions (strongly held beliefs despite evidence to the contrary). Common delusions include:</p>\n</li>\n<ul>\n<li>\n<p>Belief that people are trying to follow or harm you (paranoia)</p>\n</li>\n<li>\n<p>Belief that things refer to you: thinking strangers are talking about you, that insignificant events have special importance, that mass media like TV has special messages for you</p>\n</li>\n<li>\n<p>Belief that something is wrong with your body, in the absence of evidence</p>\n</li>\n<li>\n<p>Belief that people are romantically or sexually interested in you, in the absence of evidence</p>\n</li>\n<li>\n<p>Hugely overestimating your own importance and abilities</p>\n</li>\n</ul>\n<li>\n<p>Unusual or bizarre behavior</p>\n</li>\n<li>\n<p>Changes in physical motion: repeating meaningless motions, or not moving at all</p>\n</li>\n<li>\n<p>Thoughts and speech seem disorganized, not making sense, getting distracted by thoughts in mid-sentence</p>\n</li>\n<li>\n<p>Showing and feeling no emotion, \u201cblank\u201d look</p>\n</li>\n<li>\n<p>Loss of interest in usual activities, apathy</p>\n</li>\n</ul>\n<p>Many of these symptoms may also occur for other reasons. Some may come from physical problems with the brain (for example, a stroke). This is one of the reasons it\u2019s a good idea to get medically evaluated if things seem off.</p>\n<p>&nbsp;</p>\n<h2>Early symptoms</h2>\n<p>Some people may experience these symptoms before a full psychotic episode:</p>\n<ul>\n<li>\n<p>Trouble concentrating</p>\n</li>\n<li>\n<p>Feeling your mind is playing tricks on you</p>\n</li>\n<li>\n<p>Hearing things like your name being called</p>\n</li>\n<li>\n<p>Seeing glimpses of that aren\u2019t there out of the corner of your eye, or seeing moving patterns or shadows</p>\n</li>\n</ul>\n<h3>&nbsp;</h3>\n<h2>Common triggers of psychosis</h2>\n<ul>\n<li>\n<p>Extreme sleep deprivation</p>\n</li>\n</ul>\n<ul>\n<li>\n<p>Trauma or extreme stress</p>\n</li>\n<li>\n<p>Some medications or drugs, especially marijuana or MDMA</p>\n</li>\n<li>\n<p>Withdrawal from some drugs, especially alcohol</p>\n</li>\n<li>\n<p>Physical illness or injury (head injury, infection, blood sugar imbalance, electrolyte imbalance, brain disease such as Parkinson\u2019s)</p>\n</li>\n<li>\n<p>The weeks after childbirth</p>\n</li>\n<li>\n<p>No special trigger, just underlying genetic predisposition</p>\n</li>\n</ul>\n<h3>&nbsp;</h3>\n<h2>Does someone who experiences psychosis have a particular illness?</h2>\n<p>A psychotic episode may or may not indicate an ongoing mental health problem. After a first episode, about \u2153 of people will have another episode within 3 years (<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/22393215\">source</a>). In some circumstances, like sensory deprivation or bereavement, hallucinations are very common and not predictive of future problems.</p>\n<p>Some people have only one episode and recover fully. Others have multiple episodes and benefit from ongoing treatment but retain basically normal functioning between episodes. Others get progressively worse. People with recurring episodes would probably be diagnosed with one of the <a href=\"https://my.clevelandclinic.org/health/diseases/4568-schizophrenia\">schizophrenia spectrum disorders</a>.</p>\n<h2>&nbsp;</h2>\n<h1>Family history</h1>\n<p>Bipolar disorder and schizophrenia seem to have some common genetic risk factors. People with a family history of either disorder are more likely to develop one of them.</p>\n<h2>&nbsp;</h2>\n<h1>Drug use</h1>\n<p>Drugs that may be relatively safe for some people may be much less safe for others.</p>\n<p>There\u2019s not clear evidence as to whether marijuana increases risk for psychosis, but it seems <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2424288/\">very plausible</a> that it worsens existing psychosis and makes people who already have risk factors (like a family history) more likely to develop psychosis.</p>\n<p>While drugs such as MDMA have been <a href=\"http://www.drugpolicy.org/drug-facts/mdma-ecstasy-molly/can-mdma-be-used-medicine-or-therapy\">tested</a> as therapies for conditions like PTSD, the findings of these studies may not be very generalizable because:</p>\n<ul>\n<li>\n<p>The studies screen out participants that are seen as being at high risk (for example because they already had other medical or mental health problems).</p>\n</li>\n<li>\n<p>The participants were given actual MDMA, while what\u2019s bought on the informal market is often <a href=\"http://www.playboy.com/articles/molly-party-drug-ecstasy\">diluted</a> <a href=\"http://www.therooster.com/blog/why-people-are-dying-molly-and-how-stop-it-0\">with</a> other substances, ranging from harmless (chalk) to ones that may cause unwanted effects (methamphetamines, which like other stimulants can kick off mania in some people).</p>\n</li>\n</ul>\n<p>In other words, what was safe for carefully selected study participants with carefully selected drugs may not be safe for you.</p>\n<p>The Drug Policy Alliance\u2019s <a href=\"http://www.drugpolicy.org/drug-facts/psychedelics-facts\">statement</a> on psychedelics:<br>\u201cAn individual's experience using a psychedelic drug is strongly influenced by two key factors: set and setting. The set is the internal mental environment, and the beliefs, of the person who has ingested the substance. Setting is the external environment. If someone uses a psychedelic in a threatening or chaotic set or setting, that person is more likely to have a threatening or chaotic experience. Likewise, if psychedelics are used in a safe, supportive environment, it will be easier for the person to allow his or her experience to develop in a coherent, potentially meaningful manner \u2013 though some parts may still be overwhelming or psychologically jarring.\u201d</p>\n<h2>&nbsp;</h2>\n<h1>How to help</h1>\n<p>Most people don\u2019t get help soon enough. Someone who experiences psychosis usually doesn\u2019t get treatment until more than a <a href=\"http://ajp.psychiatryonline.org/doi/full/10.1176/appi.ajp.157.11.1727\">year</a> later. Someone with bipolar typically isn\u2019t diagnosed until more than <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2796048/\">three years</a> after their first mood episode.</p>\n<p>A <a href=\"https://www.nami.org/psychosis/report\">survey</a> by the National Alliance on Mental Illness asked people who have experienced psychosis who helped them during the early stage of their illness. The most common answer was \u201cno one.\u201d (Parents, psychiatrists, and therapists were the next most common answers.)</p>\n<p>In the survey, people who had experienced psychosis listed ways others had helped them:</p>\n<ul>\n<li>\n<p>Identifying problems early</p>\n</li>\n<li>\n<p>Listening patiently and compassionately, without making judgments</p>\n</li>\n<li>\n<p>Making suggestions without being confrontational; remaining gentle and calm</p>\n</li>\n<li>\n<p>Keeping them from harming themselves</p>\n</li>\n<li>\n<p>Taking them to an emergency room or making appointment and taking them to a doctor</p>\n</li>\n<li>\n<p>Providing a safe place to rest or recover</p>\n</li>\n<li>\n<p>Flying or driving long distances to be with them</p>\n</li>\n<li>\n<p>Explaining the nature of the illness and what was happening</p>\n</li>\n<li>\n<p>Building trust by making decisions together</p>\n</li>\n<li>\n<p>Prescribing the right medication</p>\n</li>\n<li>\n<p>Prescribing cognitive behavioral therapy</p>\n</li>\n<li>\n<p>Providing child care, cooking, or taking on other daily chores</p>\n</li>\n<li>\n<p>Providing financial support</p>\n</li>\n<li>\n<p>Encouragement that \u201cnormalized the experience,\u201d such as to finish school or return to work</p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p>They also listed their most important needs during periods of crisis:</p>\n<ul>\n<li>\n<p>Getting rid of voices and paranoia</p>\n</li>\n<li>\n<p>Knowing the difference between what was real and unreal</p>\n</li>\n<li>\n<p>Hospitalization, medication and stabilization</p>\n</li>\n<li>\n<p>A safe place and protection</p>\n</li>\n<li>\n<p>Access to a good psychiatrist or counselor</p>\n</li>\n<li>\n<p>Sleep</p>\n</li>\n<li>\n<p>Validation of their experience; someone to listen who could be trusted</p>\n</li>\n<li>\n<p>Information and explanation</p>\n</li>\n<li>\n<p>Financial assistance</p>\n</li>\n</ul>\n<h2>&nbsp;</h2>\n<h2>Professional help</h2>\n<p>Seek medical care if you\u2019re concerned that you or someone else isn\u2019t doing well. This is the standard advice for a good reason, which is that things may get worse if you try to just wait it out. You may miss the opportunity for treatment that would have been helpful. The problem may be due to something you don\u2019t expect (like a neurological problem, a substance you didn\u2019t realize the person took, or an infection). Or it may get beyond what you can safely handle.</p>\n<h3>US</h3>\n<p>In an emergency, call 911 or go to an emergency room (would be called A&amp;E in UK) at a local hospital.</p>\n<p>Many areas have a psychiatric crisis team that can send trained mental health staff to where you are; call 911 or the local non-emergency police number.</p>\n<p>National suicide prevention <a href=\"https://suicidepreventionlifeline.org/chat/\">chat</a> or hotline: 1\u2011800\u2011273\u20118255</p>\n<p>Suicide crisis text line: Reach a counselor 24/7 by texting 741-741</p>\n<p>Berkeley Mobile Crisis Team: (510) 981-5900</p>\n<p>National Alliance on Mental Illness (NAMI) hotline: 800-950-6264</p>\n<p><a href=\"http://mhaac.org/support/mental-health-resources/community-mental-health-resources.html\">Alameda County mental health resources</a></p>\n<p><a href=\"http://www.mhbsf.org/resources/\">San Francisco mental health resources</a></p>\n<h3>&nbsp;</h3>\n<h3>UK</h3>\n<p>In an emergency, call 999 or go to an A&amp;E department (would be called emergency room in US) at a local hospital.</p>\n<p>Mental health helpline: 116 123</p>\n<p><a href=\"https://www.getselfhelp.co.uk//helplines.htm\">More helplines</a></p>\n<p><a href=\"http://www.nhs.uk/NHSEngland/AboutNHSservices/mental-health-services-explained/Pages/accessing%20services.aspx\">NHS mental health services</a></p>\n<p><a href=\"https://www.rethink.org/advice-and-information/carers-hub/getting-help-for-someone-in-a-mental-health-crisis/\">Crisis services</a></p>\n<h2>&nbsp;</h2>\n<h2>Other types of help</h2>\n<p>If, for whatever reason, you decide not to get medical help, here are some basic safety tips.</p>\n<ul>\n<li>\n<p>Get the person to a calm, quiet environment.</p>\n</li>\n<li>\n<p>Help them establish a regular routine of sleeping, eating, and quiet activity. During mania, trying to \u201cwork off\u201d excess energy through activity is counterproductive; getting lots of rest is better.</p>\n</li>\n<li>\n<p>Help them stay hydrated, particularly if they\u2019ve had a lot of alcohol or MDMA.</p>\n</li>\n<li>\n<p>Contact someone who knows more about what\u2019s been helpful to them in the past, like their family.</p>\n</li>\n<li>\n<p>If they\u2019re agitated or aggressive, take this seriously. Keep yourself safe and re-consider calling for medical help.</p>\n</li>\n</ul>\n<h2>&nbsp;</h2>\n<h2>Other resources</h2>\n<p><a href=\"http://www.treatmentadvocacycenter.org/someone-i-know-is-in-crisis\">Someone I know is in crisis</a> from Treatment Advocacy Center</p>\n<p><a href=\"http://www.robot-hugs.com/harm-reduction/\">Harm Reduction</a> from Robot Hugs</p>\n<p><a href=\"http://www.robot-hugs.com/harm-reduction/\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/CJZGFxzHfdPuu2X76/ro480vzbgo1kl8ad0adv\" alt=\"Screen Shot 2017-05-03 at 1.56.06 PM.png\"></a></p>\n<p>Some people find that mood/sleep tracking <a href=\"http://emoodtracker.com/\">apps</a> help them recognize when a manic episode is approaching.</p>\n<p><a href=\"https://www.psychologytoday.com/blog/promoting-hope-preventing-suicide/201501/what-s-mad-map\">Advance directives</a> for mental health, sometimes called <a href=\"https://www.nami.org/Find-Support/Family-Members-and-Caregivers/Being-Prepared-for-a-Crisis\">wellness plans</a> or mad maps. These are plans for what steps you want to take when. This includes information like:</p>\n<ul>\n<li>\n<p>What I\u2019m like when I\u2019m well</p>\n</li>\n<li>\n<p>Things that have helped in the past</p>\n</li>\n<li>\n<p>Symptoms that indicate I\u2019m no longer able to make decisions for myself</p>\n</li>\n<li>\n<p>People I do and do not want involved in my care</p>\n</li>\n<li>\n<p>Preferred treatments and treatment facilities</p>\n</li>\n<li>\n<p>Contact information for people you would want to contact in a crisis</p>\n</li>\n</ul>\n\n<p><a href=\"http://slatestarcodex.com/2014/06/16/things-that-sometimes-help-if-youre-depressed/\">Things that sometimes help if you have depression</a> from Scott Alexander (including info on why people with bipolar need different treatment from people with depression).</p>\n<p><a href=\"https://fireweedcollective.org/wp-content/uploads/2020/03/IcarusNavigatingCrisisHandoutLarge05-09.pdf\">Navigating Crisis</a> from Icarus Project</p>\n<p><a href=\"http://www.bipolarcaregivers.org/supporting-the-person/supporting-a-person-with-mania-or-hypomania\">Supporting a person with mania or hypomania</a></p>\n<p><a href=\"http://www.bipolarcaregivers.org/supporting-the-person/helping-to-reduce-bipolar-triggers\">Reducing bipolar triggers</a></p>\n<p><a href=\"https://psychcentral.com/lib/bipolar-disorder-helping-your-loved-one-manage-a-manic-episode/\">Helping a loved one manage a manic episode</a></p>\n<p><a href=\"http://www.heretohelp.bc.ca/workbook/dealing-with-psychosis-a-toolkit-for-moving-forward-with-your-life\">Dealing with Psychosis: A Toolkit for Moving Forward with Your Life</a></p>\n<p><a href=\"https://www.nami.org/\">National Alliance on Mental Illness</a></p>", "user": {"username": "Julia_Wise"}}, {"_id": "oxkNRmKkoQYvcEWvy", "title": "Where should anti-paternalists donate?", "postedAt": "2017-05-04T09:36:53.654Z", "htmlBody": "<html><body><p>GiveDirectly gives out unconditional cash transfers to some of the poorest people in the world. It&#x2019;s clearly an outstanding organisation that is exceptionally data driven and transparent. However, according to <a href=\"http://blog.givewell.org/2016/11/28/updated-top-charities-giving-season-2016/#Sec3a\">GiveWell&#x2019;s</a> cost-effectiveness estimates (which represent a weighted average of the diverse views of GiveWell staffers), it is significantly less cost-effective than other recommended charities. For example, the Against Malaria Foundation (AMF) is ~4 times as cost-effective, and Deworm the World (DtW) is ~10 times as cost-effective. This is a big difference in terms of welfare. (The welfare can derive from averting deaths, preventing illness, increasing consumption, etc).</p>\n<p>One <em>prima facie </em>reason to donate to GiveDirectly in spite of this, suggested by e.g. <a href=\"https://www.youtube.com/watch?v=TIrEbiUIVQQ\">Matt Zwolinski</a> and <a href=\"https://medium.com/@moskov/breakthrough-philanthropy-just-give-them-the-money-c597f409706f\">Dustin Moskovitz</a>, is that it is not paternalistic.<a title=\"\" href=\"file:///C:/Users/Admin/Documents/My%20docs/Documents/EA%20blogs/Paternalism%20and%20cash%203.docx#_edn1\"><span><!-- [if !supportFootnotes]--><span><span>[1]</span></span><!--[endif]--></span></a> Roughly: giving recipients cash respects their autonomy by allowing them to choose what good to buy, whereas giving recipients bednets or deworming drugs makes the choice for them in the name of enhancing their welfare. On the version of the anti-paternalism argument I&#x2019;m considering, paternalism is <em>non-instrumentally</em> bad, i.e. it is bad regardless of whether it produces bad outcomes.</p>\n<p>I&#x2019;ll attempt to rebut the argument from anti-paternalism with two main arguments.</p>\n<p>(i) Reasonable anti-paternalists should value welfare to some extent. Since bednets and deworming are so much more cost-effective than GiveDirectly, only someone who put a very high, arguably implausible, weight on anti-paternalism would support GiveDirectly.</p>\n<p>(ii) More importantly, the premise that GiveDirectly is much better from an anti-paternalistic perspective probably does not hold. My main arguments here are that: the vast majority of beneficiaries of deworming and bednets are children; deworming and bednets yield cash benefits for others that probably exceed the direct and indirect benefits of cash transfers; and the health benefits of deworming and bednets produce long-term autonomy benefits.</p>\n<p>Some of the arguments made here have been discussed before e.g. by <a href=\"https://www.givingwhatwecan.org/blog/2012-11-30/givewell%E2%80%99s-recommendation-of-givedirectly\">Will MacAskill </a>&#xA0;and <a href=\"http://blog.givewell.org/2012/05/30/giving-cash-versus-giving-bednets/\">GiveWell</a>, but I think it&#x2019;s useful to have all the arguments brought together in one place.</p>\n<p>It is important to bear in mind in what follows that according to GiveWell, their cost-effectiveness estimates are highly uncertain, not meant to be taken literally, and that the outcomes are very sensitive to different assumptions. Nonetheless, for the purposes of this post, I assume that the cost-effectiveness estimates are representative of the actual relative cost-effectiveness of these interventions, noting that some of my conclusions may not hold if this assumption is relaxed.</p>\n<p>&#xA0;</p>\n<p>[Cross-posted from my <a href=\"http://johnhalstead.org/index.php/2017/05/04/anti-paternalists-donate/\">blog</a>]</p>\n<p>&#xA0;</p>\n<p><u>1. What is paternalism and why is it bad?</u></p>\n<p>A sketch of the paternalism argument for cash transfers goes as follows:</p>\n<p><!-- [if !supportLists]--><span>&#xB7;<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span></span><!--[endif]-->Anti-malaria and deworming charities offer recipients a specific good, rather than giving them the cash and allowing them to buy whatever they want. This is justified by the fact that anti-malaria and deworming charities enhance recipients&#x2019; welfare more than cash. Thus, donating to anti-malaria or deworming charities to some extent bypasses the autonomous judgement of recipients in the name of enhancing their welfare. Thus, anti-malaria and deworming charities are more paternalistic than GiveDirectly.</p>\n<p>This kind of paternalism, the argument goes, is non-instrumentally bad: even if deworming and anti-malaria charities in fact produce more welfare, their relative paternalism counts against them. Paternalism is often justified by appeal to the value of <em>autonomy</em>. Autonomy is roughly the capacity for self-governance; it is the ability to decide for oneself and pursue one&#x2019;s own chosen projects.</p>\n<p>Even if the argument outlined in this section is sound, deworming and bednets improve the autonomy of recipients <em>relative to no aid</em> because they give them additional opportunities which they may take or decline if they (or their parents) wish. Giving people new opportunities and options is widely agreed to be autonomy-enhancing. This marks out an important difference between these and other welfare-enhancing interventions. For example, tobacco taxes reduce the (short-term) autonomy and liberty of those subject to them by using threats of force to encourage a welfare-enhancing behaviour.</p>\n<p>&#xA0;</p>\n<p><u>2. How bad is paternalism?</u></p>\n<p>Even if one accepted the argument in section 1, this would only show that donating to GiveDirectly is less paternalistic than donating to bednets or deworming. This does not necessarily entail that anti-paternalists ought to donate to GiveDirectly. Whether that&#x2019;s true depends on how we ought to trade off paternalism and welfare. With respect to AMF for example, paternalism would have to be bad enough that it is worth losing ~75% of the welfare gains from a donation; with respect to DtW, ~90%.</p>\n<p>It might be argued that anti-paternalism has &#x2018;trumping&#x2019; force such that it always triumphs over welfarist considerations. However, &#x2018;trumping&#x2019; is usually reserved for rights violations, and neither deworming nor anti-malaria charities violates rights. So, trumping is hard to justify here.</p>\n<p>Nonetheless, it&#x2019;s difficult to say what weight anti-paternalism should have and giving it very large weight would, if the argument in section 1 works, push one towards donating to GiveDirectly. However, there are a number of reasons to believe that donating to deworming and bednets is actually attractive from an anti-paternalistic point of view.</p>\n<p>&#xA0;</p>\n<p><u>3. Are anti-malaria and deworming charities paternalistic?</u></p>\n<p>(a) The main beneficiaries are children</p>\n<p>Mass deworming programmes overwhelmingly target children. According to GiveWell&#x2019;s <a href=\"https://docs.google.com/spreadsheets/d/1KiWfiAGX_QZhRbC9xkzf3I8IqsXC5kkr-nwY_feVlcM/edit#gid=472531943\">cost-effectiveness model</a>, 100% of DtW&#x2019;s recipients are children, Sightsavers ~90%, and SCI ~85%. Around a third of the <a href=\"https://docs.google.com/spreadsheets/d/1KiWfiAGX_QZhRbC9xkzf3I8IqsXC5kkr-nwY_feVlcM/edit#gid=115155829\">modelled benefits</a> of bednets derive from preventing deaths of under 5s, and around a third from developmental benefits to children. The final third of the modelled benefits derive from preventing deaths of people aged 5 and over. Thus, the vast majority (&gt;66%) of the modelled benefits of bednets accrue to children under the age of 15, though it is unclear what the overall proportion is because GiveWell does not break down the &#x2018;over 5 mortality&#x2019; estimate.</p>\n<p>Paternalism for children is widely agreed to be justified. The concern with bednets and deworming must then stem from the extent to which they are paternalistic with respect to adults.<a title=\"\" href=\"file:///C:/Users/Admin/Documents/My%20docs/Documents/EA%20blogs/Paternalism%20and%20cash%203.docx#_edn2\"><sup><!-- [if !supportFootnotes]--><sup><span>[2]</span></sup><!--[endif]--></sup></a></p>\n<p>In general, this shows that deworming and anti-malaria charities do a small or zero amount of objectionable paternalism. So, paternalism would have to be very very bad to justify donating to GiveDirectly. Moreover, anti-paternalists can play it safe by donating to DtW, which does not target adults at all.&#xA0;</p>\n<p>This alone shows that anti-paternalism provides weak or zero additional reason to donate to cash transfer charities, rather than deworming or anti-malaria charities.</p>\n<p>&#xA0;</p>\n<p>(b) Positive Externalities</p>\n<p>Deworming drugs and bednets probably produce substantial positive externalities. Some of these come in the form of health benefits to others. According to <a href=\"http://www.givewell.org/international/technical/programs/insecticide-treated-nets#Targeted_vs_universal_coverage\">GiveWell</a>, there is pretty good evidence that there are community-level health benefits to bednets: giving A a bednet reduces his malaria risk, as well as his neighbour B&#x2019;s. However, justifying giving A a bednet on the basis that it provides health benefits to B is more paternalistic towards B than giving her the cash, for the reasons outlined in section 1.</p>\n<p>However, by saving lives and making people more productive, deworming and bednets are also likely to produce large <em>monetary</em> positive externalities over the long term. According to a weighted average of GiveWell staffers, for the same money, one can save ~10 equivalent lives by donating to DtW, but ~1 equivalent life by donating to GiveDirectly. (An <a href=\"https://docs.google.com/spreadsheets/d/1KiWfiAGX_QZhRbC9xkzf3I8IqsXC5kkr-nwY_feVlcM/edit#gid=1034883018\">&#x2018;equivalent life&#x2019;</a> is based on the &quot;DALYs per death of a young child averted&quot; input each GiveWell staffer uses. What a life saved equivalent represents will therefore vary between staffers because they are likely to adopt different value assumptions).</p>\n<p>What are the indirect monetary benefits of all the health and mortality benefits that constitute these extra &#x2018;equivalent lives&#x2019;? I&#x2019;m not sure if there&#x2019;s hard quantitative evidence on this, but for what it&#x2019;s worth, <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">GiveWell</a> believes that &#x201C;If one believes that, on average, people tend to accomplish good when they become more empowered, it&#x2019;s conceivable that the indirect benefits of one&#x2019;s giving swamp the first-order effects&#x201D;. What GiveWell is saying here is as follows. &#x201C;Suppose that the direct benefits of a $1k donation are <em>x</em>. If people accomplish good when they are empowered, the indirect benefits of this $1k are plausibly &gt;<em>x</em>.&#x201D; If this is true, then what if the direct benefits are 10*<em>x</em>? This must make it very likely that the indirect benefits &gt;&gt;<em>x</em>.</p>\n<p>So, given certain plausible assumptions, it&#x2019;s plausible that the indirect <em>monetary</em> benefits of deworming and bednets <em>exceed the direct and indirect monetary benefits of cash transfers</em>. DtW and AMF are like indirect GiveDirectlys: they ensure that lots of people receive large cash dividends down the line.</p>\n<p>As I argued in section 1, providing bednets and deworming drugs is autonomy-enhancing relative to no aid: it adds autonomy to the world. If, as I&#x2019;ve suggested, bednets and deworming also produce larger overall cash benefits than GiveDirectly, then bednets and deworming dominate cash transfers in terms of autonomy-production. One possible counter to this is to discount the autonomy-enhancements brought about by future cash. I briefly discuss discounting future autonomy in (c).</p>\n<p>This shows that anti-paternalists should arguably prefer deworming or anti-malaria charities to GiveDirectly, other things equal.</p>\n<p>&#xA0;</p>\n<p>(c) Short-term and long-term autonomy</p>\n<p>Short-term paternalism can enhance not only the welfare but also the long-term autonomy of an individual. For the same amount of money, one can save 10 equivalent lives by donating to DtW vs. 1 equivalent life by donating to GiveDirectly. The morbidity and mortality benefits that constitute these equivalent lives enable people to pursue their own autonomously chosen projects. It&#x2019;s very plausible that this produces more autonomy than providing these benefits only to one person. Anti-paternalists who ultimately aim to maximise overall autonomy therefore have reason to favour deworming and bednets over GiveDirectly.</p>\n<p>Some anti-paternalists may not want to maximise overall autonomy. Rather, they may argue that we should maximise autonomy with respect to some specific near-term choices. When we are deciding what to do with $100, we should maximise autonomy with respect to that $100. So, we should give them $100 rather than using the $100 to buy bednets.</p>\n<p>This argument shows that how one justifies anti-paternalism is important. If you&#x2019;re concerned with the overall long-term autonomy of recipients, you have reason to favour bednets or deworming. If you&#x2019;re especially concerned with near-term autonomy over a particular subset of choices, the case for GiveDirectly is a bit stronger, but still probably defeated by argument (a).</p>\n<p>&#xA0;</p>\n<p>(d) Missing markets</p>\n<p>Deworming charities receive deworming drugs at subsidised prices from drug companies. Deworming charities can also take advantage of economies of scale in order to make the cost per treatment very low &#x2013; around $0.50. I&#x2019;m not sure how much it would cost recipients to purchase deworming drugs at market rates, but it seems likely to be much higher than $0.50. Similar things are likely true of bednets. The market cost of bednets is likely to be much greater than what it would cost AMF to get one. Indeed, GiveWell <a href=\"http://blog.givewell.org/2012/05/30/giving-cash-versus-giving-bednets/\">mentions</a> some anecdotal evidence that the long-lasting insecticide-treated bednets that AMF gives out are simply not available in local markets.</p>\n<p>From the point of view of anti-paternalists, this is arguably important <em>if</em> the following is true: recipients would have purchased bednets or deworming drugs if they were available at the cost that AMF and DtW pay for them. Suppose that if Mike could buy a bednet for the same price that AMF can deliver them &#x2013; about $5 &#x2013; he would buy one, but that they aren&#x2019;t available at anywhere near that price. If this were true, then giving Mike cash would deprive him of an option he autonomously prefers, and therefore ought to be avoided by anti-paternalists. This shows that cash is not <em>necessarily</em> the best way to leave it to the individual &#x2013; it all depends on what you can do with cash.</p>\n<p>However, the limited evidence may <a href=\"http://www.givewell.org/international/technical/programs/insecticide-treated-nets#Free_vs_cost-recovering_distributions\">suggest</a> that most recipients would not in fact buy deworming drugs or bednets even if they were available at the price at which deworming and anti-malaria charities can get them. This may in part be because recipients expect to get them for free. However, Poor Economics outlines a lot of evidence showing that the very poor do not spend their money in the most welfare-enhancing way possible. (<a href=\"https://youtu.be/YUeeIPxOXN8?t=34\">Neither do the very rich</a>). The paper &#x2018;<a href=\"http://www.frbsf.org/economic-research/files/TestingPaternalism_JesseCunha.pdf\">Testing Paternalism</a>&#x2019; presents some evidence in the other direction.</p>\n<p>In sum, for anti-paternalists, concerns about missing markets may have limited force.&#xA0;</p>\n<p>&#xA0;</p>\n<p><u>Conclusion</u></p>\n<p>Deworming and anti-malaria charities target children, probably provide large long-term indirect monetary benefits, and enhance the long-term autonomy of beneficiaries. This suggests that anti-paternalism provides at best very weak reasons to donate to GiveDirectly over deworming and anti-malaria charities, and may favour deworming and anti-malaria charities, depending on how anti-paternalism is justified. Concerns about missing markets for deworming drugs and bednets may also count against cash transfers to some extent.</p>\n<p>Nonetheless, even if GiveDirectly is less cost-effective than other charities, there may be other reasons to donate to GiveDirectly. One could for example argue, as George Howlett <a href=\"https://www.facebook.com/groups/effective.altruists/permalink/1333280470061640/?comment_id=1342281929161494&amp;notif_t=group_comment_mention&amp;notif_id=1490328350700482\">does</a>, that GiveDirectly promises substantial systemic benefits and that its model is a great way to attract more people to the idea of effective charity.</p>\n<p>&#xA0;</p>\n<p>Thanks to Catherine Hollander, James Snowden, Stefan Schubert, Michael Plant for thorough and very helpful comments.</p>\n<p>&#xA0;&#xA0;</p>\n<p>&#xA0;</p>\n<div><!-- [if !supportEndnotes]--><br><hr><!--[endif]-->\n<div>\n<p><a title=\"\" href=\"file:///C:/Users/Admin/Documents/My%20docs/Documents/EA%20blogs/Paternalism%20and%20cash%203.docx#_ednref1\"><span><!-- [if !supportFootnotes]--><span><span>[1]</span></span><!--[endif]--></span></a> See <a href=\"https://plato.stanford.edu/entries/paternalism/\">this</a> excellent discussion of paternalism by the philosopher Gerald Dworkin.</p>\n</div>\n<div>\n<p><a title=\"\" href=\"file:///C:/Users/Admin/Documents/My%20docs/Documents/EA%20blogs/Paternalism%20and%20cash%203.docx#_ednref2\"><span><!-- [if !supportFootnotes]--><span><span>[2]</span></span><!--[endif]--></span></a> It&#x2019;s an interesting and difficult question what we are permitted to do to parents in order to help their children. We can discuss this in the comments.&#xA0;</p>\n</div>\n</div></body></html>", "user": null}, {"_id": "vDsGvWEzoccPnJqDQ", "title": "Informatica: Special Issue on Superintelligence", "postedAt": "2017-05-03T05:05:55.750Z", "htmlBody": "<html><body><p>A special issue on Superintelligence is coming up at the journal <a href=\"http://www.informatica.si/index.php/informatica/pages/view/csi3\">Informatica</a>. The call for proposals is given below. We would welcome submissions from a range of perspectives, including philosophical and other fields that effective altruists may work in.</p>\n<p>----------------------------------------------------------------------------------</p>\n<h3>Introduction</h3>\n<p>Since the inception of the field of artificial intelligence, a prominent goal has been to create computer systems that would reason as capably as humans across a wide range of fields. Over the last decade, this goal has been brought closer to reality. Machine learning systems have come to excel in many signal processing tasks and have achieved superhuman performance in learning tasks including the games of Go and Heads-up Poker. More broadly, we have seen large changes in every pore of our society. This remarkable progress raises the question of how the world may look if the field of artificial intelligence eventually succeeds in creating highly capable general purpose reasoning systems. In particular, it has been hypothesized that such advances may lead to the development of a superintelligent agent &#x2013; one whose capabilities &#x201C;greatly exceed humans across virtually all domains of interest&#x201D;.</p>\n<p>Discussion on superintelligence arose from AI circles, but has spread to other disciplines. It has been hypothesized that superintelligence might emerge not from a human-programmed AI system but from the development of emulations of the human brain, through the use of brain-computer interfaces, or through genetic engineering. If an AI system might become superintelligent, this raises some technical questions about how the system can be made to behave transparently and in alignment with human values. The hypothesis of superintelligence is also an interesting setting in which to examine philosophical questions pertaining to cognition, consciousness and moral reasoning. Pragmatically, social and societal implications of superintelligence appear to be an overwhelmingly important topic. The social sciences have a place in analyzing how the impacts of AI might change as capabilities increase. There is further work yet to be done in assessing the risks and benefits of superintelligence, and in shaping policies, protocols and governance instruments that may accentuate its benefits while mitigating any risks. Superintelligence as an undertaking therefore connects very different researchers into a multidisciplinary endeavor encompassing AI, philosophy, cognitive science, biology, law and more.</p>\n<h3>Objectives</h3>\n<p>The aim of this special issue is to promote research on superintelligence by approaching the topic in the most multidisciplinary and visionary manner possible. This will lead to a thoroughly comprehensive effort of cooperation, which will inform and therefore produce new insights, innovative ideas and purposeful concepts, thereby building new grounds for thinking about superintelligence and related topics. Original research, critical studies and review articles dealing with recommended topics in regards to superintelligence are welcome. Position papers and visionary papers will be evaluated according to the quality of their argumentation. Submissions from both academia and industry are encouraged.</p>\n<h3>Recommended Topics</h3>\n<p>Topics to be discussed in this special issue include (but are not limited to) the following:</p>\n<ul>\n<li>Artificial Superintelligence</li>\n<li>Artificial General Intelligence (AGI)</li>\n<li>Biological Superintelligence</li>\n<li>Brain-computer Interfaces</li>\n<li>Whole Brain Emulation</li>\n<li>Genetic Engineering</li>\n<li>Cognitive Enhancement</li>\n<li>Collective Superintelligence</li>\n<li>Neural Lace-Mediated Empathy</li>\n<li>Technological Singularity</li>\n<li>Intelligence Explosion</li>\n<li>Definition of Life</li>\n<li>Definition of Intelligence</li>\n<li>Machine Ethics &amp; Computational Ethics</li>\n<li>Consciousness</li>\n<li>Human Limitations</li>\n<li>Technological Limitations</li>\n<li>Societal Risk from Machine-generated Alternate Value Systems</li>\n<li>Social Benefit and Existential Risk from Superintelligence</li>\n<li>Policy Options for Superintelligent Artificial Intelligence Development (domestic)</li>\n<li>International Governance Options for Superintelligent Artificial Intelligence Development</li>\n<li>Design Considerations for Superintelligence&apos;s Morality</li>\n<li>Turing Test</li>\n</ul>\n<p>Note that each of these topics has to ultimately be written about in regards to superintelligence, either as a consequence or a precursor for it and its implications.</p>\n<h3>Key Dates</h3>\n<ul>\n<li>Paper Submission Deadline: August 31, 2017</li>\n<li>Author Notification: October 31, 2017</li>\n<li>Final Manuscript Deadline: November 30, 2017</li>\n</ul>\n<p>All submissions and inquiries should be directed to the attention of: matjaz.gams@ijs.si &#xA0;and tine.kole@gmail.com &#xA0;or to the special editors:</p>\n<ul>\n<li>Ryan Carey, ryan@intelligence.org</li>\n<li>Matthijs Maas, matthijs.m.maas@gmail.com&#xA0;</li>\n<li>Nell Watson, nell.watson@su.org&#xA0;</li>\n<li>Roman Yampolskiy, roman.yampolskiy@louisville.edu</li>\n</ul>\n<p>&#xA0;</p></body></html>", "user": {"username": "RyanCarey"}}, {"_id": "qLSygonzMqWbKszXT", "title": "The Life You Can Save's 2016 Annual Report", "postedAt": "2017-04-26T22:46:55.707Z", "htmlBody": "<html><body><p>The Life You Can Save&#x2019;s 2016 Annual Report is out!</p>\n<p>Highlights of the year:</p>\n<ul>\n<li>We moved $2.7 million to our Recommended Nonprofits in 2016, while spending ~$300,000 on our operating expenses. This means that for every $1 we spent, we raised about $9 for our Recommended Nonprofits.</li>\n<li>Growth was strong in key metrics. Total Money Moved was up 72% relative to 2015, while our Net Impact (Money Moved net of expenses) increased by 86%.</li>\n<li>We made significant progress in developing working relationships with value-aligned groups and individuals. These sorts of partnerships amplify our reach and expertise, and are an essential part of our plan to scale. To support our future growth, we made major enhancements to our infrastructure: we overhauled our donation process, codified and strengthened our charity selection process, and upgraded our donor stewardship process.</li>\n</ul>\n<p>Full report:</p>\n<p><a href=\"https://www.thelifeyoucansave.org/Portals/0/Annual%20Report%202016%20-%20The%20Life%20You%20Can%20Save.pdf?ver=2017-04-26-151354-293\">https://www.thelifeyoucansave.org/Portals/0/Annual%20Report%202016%20-%20The%20Life%20You%20Can%20Save.pdf?ver=2017-04-26-151354-293</a></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Jon_Behar"}}, {"_id": "fywZAX74GuLFbfRJP", "title": "The 2017 Effective Altruism Survey - Please Take!", "postedAt": "2017-04-24T21:01:26.039Z", "htmlBody": "<html><body><p>This year, the EA Survey volunteer team is proud to announce the launch of <strong>the 2017 Effective Altruism Survey</strong>.</p>\n<p>-</p>\n<p><strong><a href=\"http://survey.effectivealtruismhub.com/index.php/933193/lang-en?source=ea-forum\">PLEASE TAKE THIS SURVEY NOW! :)</a></strong></p>\n<p>If you&apos;re short on time and you&apos;ve taken the survey in prior years, <a href=\"http://survey.effectivealtruismhub.com/index.php/116472/lang-en?source=ea-forum\">you can take an abridged donations-only version of the survey here</a>.</p>\n<p>If you want to share the survey with others, please use this fancy share link with referral tracking: <a href=\"http://bit.ly/2pbhjQT\">http://bit.ly/2pbhjQT</a></p>\n<p>-</p>\n<p><strong>What is this?</strong></p>\n<p>This is the third survey we&apos;ve done, coming hot off the heels of the <a href=\"/ea/zw/the_2015_survey_of_effective_altruists_results/\">2015 EA Survey (see results and analysis)</a>&#xA0;and the <a href=\"/ea/gb/the_2014_survey_of_effective_altruists_results/\">2014 EA Survey</a>. (We apologize that we didn&apos;t get a 2016 Survey together... it&apos;s hard to be an all volunteer team!)</p>\n<p>We hope this survey will produce very useful data on the growth and changing attitudes of the EA Community. In addition to capturing a snapshot&#xA0;of what EA looks like now, we also intend to do longitudinal analysis to see how our snapshot has been changing.</p>\n<p>We&apos;re also using this as a way to build up the online EA community, such as featuring people on&#xA0;<a href=\"https://eahub.org/map\">a global map of EAs</a>&#xA0;and with a list of&#xA0;<a href=\"https://eahub.org/user/profiles\">EA Profiles</a>. This way more people can learn about the EA community. We will ask you in the survey if you would like to join us, but you do not have to opt-in and you will be opted-out by default.</p>\n<p>&#xA0;</p>\n<p><strong>How does the survey work?</strong></p>\n<p>All questions are optional (apart from one important question to verify that your answers should be counted). Most are multiple choice and the survey takes around 10-30 minutes. We have included spaces for extra comments if there is some extra detail you would like to add (these are strictly optional).</p>\n<p>At the end of the survey there is an &apos;Extra Credit&apos; section with some more informal questions and opportunities for comment&#xA0;- definitely feel free to skip these questions.</p>\n<p>Results will be shared anonymously unless you give your explicit permission otherwise.</p>\n<p>&#xA0;</p>\n<p><strong>Who is behind this?</strong></p>\n<p>The EA Survey is a all-volunteer community project run through .impact, which is soon changing it&apos;s name to &quot;Rethink Charity&quot;. The results will not belong to any one person or organization.</p></body></html>", "user": {"username": "Peter_Hurford"}}, {"_id": "rpFjPmCL4tcfBic6a", "title": "Effective altruism is self-recommending", "postedAt": "2017-04-23T06:11:20.903Z", "htmlBody": "<html><body><p>A parent I know reports (some details anonymized):</p>\n<blockquote>\n<p>Recently we bought my 3-year-old daughter a &quot;behavior chart,&quot; in which she can earn stickers for achievements like not throwing tantrums, eating fruits and vegetables, and going to sleep on time. We successfully impressed on her that a major goal each day was to earn as many stickers as possible.</p>\n<p>This morning, though, I found her just plastering her entire behavior chart with stickers. She genuinely seemed&#xA0;to think I&apos;d be proud of how many stickers she now had.</p>\n</blockquote>\n<p>The Effective Altruism movement has now entered this extremely cute stage of cognitive development. EA is more than three years old, but institutions age differently than individuals.</p>\n<h1>What is a confidence game?</h1>\n<p>In 2009, investment manager and con artist Bernie Madoff&#xA0;<a href=\"https://archives.fbi.gov/archives/newyork/press-releases/2009/nyfo031209.htm\">pled guilty</a>&#xA0;to running a massive fraud, with $50 billion in fake return on investment, having outright embezzled around&#xA0;<a href=\"http://www.cbsnews.com/news/the-madoff-scam-meet-the-liquidator-25-09-2009/\">$18 billion</a>&#xA0;out of the $36 billion investors put into the fund. Only a couple of years earlier, when my grandfather was still alive, I remember him telling me about how Madoff was a genius, getting his investors a consistent high return, and about how he wished he could be in on it, but Madoff wasn&apos;t accepting additional investors.</p>\n<p>What Madoff was running was a classic&#xA0;<a href=\"https://en.wikipedia.org/wiki/Ponzi_scheme\">Ponzi scheme</a>. Investors gave him money, and he told them that he&apos;d gotten them an exceptionally high return on investment, when in fact he had not. But because he promised to be able to do it again, his investors mostly&#xA0;<em>reinvested</em>&#xA0;their money, and more people were excited about getting in on the deal. There was more than enough money to cover the few people who wanted to take money&#xA0;<em>out&#xA0;</em>of this amazing opportunity.</p>\n<p>Ponzi schemes, pyramid schemes, and speculative bubbles are all situations in investors&apos; expected profits are paid out from the money paid in by new investors, instead of any independently profitable venture. Ponzi schemes are centrally managed &#x2013; the person running the scheme represents it to investors as legitimate, and takes responsibility for finding new investors and paying off old ones. In pyramid schemes such as multi-level-marketing and chain letters, each generation of investor recruits new investors and profits from them. In speculative bubbles, there is no formal structure propping up the scheme, only a common, mutually reinforcing set of expectations among speculators driving up the price of something that was already for sale.</p>\n<p>The general situation in which someone sets themself up as the repository of others&apos; confidence, and uses this as leverage to acquire increasing investment, can be called a&#xA0;<strong><em>confidence game</em></strong>.</p>\n<p>Some of the most iconic Ponzi schemes blew up quickly because they promised wildly unrealistic growth rates. This had three undesirable effects for the people running the schemes. First, it attracted too much attention &#x2013; too many people wanted into the scheme too quickly, so they rapidly exhausted sources of new capital. Second, because their rates of return were implausibly high, they made themselves targets for scrutiny. Third, the extremely high rates of return themselves caused their promises to quickly outpace what they could plausibly return to even a small share of their investor victims.</p>\n<p>Madoff was careful to avoid all these problems, which is why his scheme lasted for nearly half a century. He only promised&#xA0;<em>plausibly</em>&#xA0;high returns (<a href=\"http://www.washingtonpost.com/wp-dyn/content/article/2008/12/12/AR2008121203970_2.html?hpid=topnews\">around</a>&#xA0;<a href=\"http://www.nytimes.com/2008/12/13/business/13fraud.html?pagewanted=2\">10% annually</a>) for a successful hedge fund, especially if it was illegally engaged in insider trading, rather than the sort of implausibly high returns typical of more blatant&#xA0;<a href=\"https://web.archive.org/web/20050306163502re_/http://www.ssa.gov:80/history/ponzi.html\">Ponzi schemes</a>. (Charles Ponzi promised to double investors&apos; money in 90 days.) Madoff showed reluctance to accept new clients, like any other fund manager who doesn&apos;t want to get too big for their trading strategy.</p>\n<p>He didn&apos;t plaster stickers all over his behavior chart &#x2013; he put a&#xA0;<em>reasonable</em>&#xA0;number of stickers on it. He played a&#xA0;<strong><em>long game</em></strong>.</p>\n<p>Not all confidence games are inherently bad. For instance, the US national pension system, Social Security, operates as a kind of Ponzi scheme, it is not obviously unsustainable, and many people continue to be glad that it exists. Nominally, when people pay Social Security taxes, the money is invested in the&#xA0;<a href=\"https://www.ssa.gov/OACT/ProgData/describeoasi.html\">social security trust fund</a>, which holds interest-bearing financial assets that will be used to pay out benefits in their old age. In this respect it looks like an ordinary pension fund.</p>\n<p>However, the financial assets are US Treasury bonds. There is no independently profitable venture. The Federal Government of the United States of America is quite literally writing an IOU to itself, and then spending the money on current expenditures, including paying out current Social Security benefits.</p>\n<p>The Federal Government, of course, can write as large an IOU to itself as it wants. It could make&#xA0;<em>all</em>&#xA0;tax revenues part of the Social Security program. It could issue new Treasury bonds and gift them to Social Security. None of this would increase its ability to pay out Social Security benefits. It would be an empty exercise in putting stickers on its own chart.</p>\n<p>If the Federal government loses the ability to collect enough taxes to pay out social security benefits, there is no additional capacity to pay represented by US Treasury bonds. What we have is an implied promise to pay out future benefits, backed by the expectation that the government will be able to collect taxes in the future, including Social Security taxes.</p>\n<p>There&apos;s nothing necessarily wrong with this, except that the mechanism by which Social Security is funded is obscured by financial engineering. However, this misdirection should raise at least some doubts as to the underlying sustainability or desirability of the commitment. In fact, this scheme was adopted specifically to give people the impression that they had some sort of property rights over their social Security Pension, in order to make the program politically difficult to eliminate. Once people have &quot;bought in&quot; to a program, they will be reluctant to treat their prior contributions as sunk costs, and willing to invest additional resources to salvage their investment, in ways that may make them increasingly reliant on it.</p>\n<p>Not all confidence games are intrinsically bad, but dubious programs benefit the most from being set up as confidence games. More generally, bad programs are the ones that benefit the most from being allowed to fiddle with their own accounting. As Daniel Davies writes, in&#xA0;<a href=\"http://blog.danieldavies.com/2004/05/d-squared-digest-one-minute-mba.html\">The D-Squared Digest One Minute MBA - Avoiding Projects Pursued By Morons 101</a>:</p>\n<blockquote>\n<p><strong>Good ideas do not need lots of lies told about them in order to gain public acceptance.</strong>&#xA0;I was first made aware of this during an accounting class. We were discussing the subject of accounting for stock options at technology companies. [&#x2026;] One side (mainly technology companies and their lobbyists) held that stock option grants should not be treated as an expense on public policy grounds; treating them as an expense would discourage companies from granting them, and stock options were a vital compensation tool that incentivised performance, rewarded dynamism and innovation and created vast amounts of value for America and the world. The other side (mainly people like Warren Buffet) held that stock options looked awfully like a massive blag carried out my management at the expense of shareholders, and that the proper place to record such blags was the P&amp;L account.</p>\n<p>Our lecturer, in summing up the debate, made the not unreasonable point that if stock options really were a fantastic tool which unleashed the creative power in every employee, everyone would&#xA0;<em>want</em>&#xA0;to expense as many of them as possible, the better to boast about how innovative, empowered and fantastic they were. Since the tech companies&apos; point of view appeared to be that if they were ever forced to account honestly for their option grants, they would quickly stop making them, this offered decent&#xA0;<em>prima facie</em>&#xA0;evidence that they weren&apos;t, really, all that fantastic.</p>\n</blockquote>\n<p>However, I want to generalize the concept of confidence games from the domain of financial currency, to the domain of social credit more generally (of which money is a particular form that our society commonly uses), and in particular I want to talk about confidence games in the currency of credit for achievement.</p>\n<p>If I were applying for a very important job with great responsibilities, such as President of the United States, CEO of a top corporation, or head or board member of a major AI research institution, I could be expected to have some relevant prior experience. For instance, I might have had some success managing a similar, smaller institution, or serving the same institution in a lesser capacity. More generally, when I make a bid for control over something, I am implicitly claiming that I have enough social credit &#x2013; enough of a track record &#x2013; that I can be expected to do good things with that control.</p>\n<p>In general, if someone has done a lot, we should expect to see an iceberg pattern where a small easily-visible part suggests a lot of solid but harder-to-verify substance under the surface. One might be tempted to make a habit of imputing a much larger iceberg from the combination of a small floaty bit, and promises. But, a small easily-visible part with claims of a lot of harder-to-see substance is easy to mimic without actually doing the work. As Davies continues:</p>\n<blockquote>\n<p><strong>The Vital Importance of Audit</strong>. Emphasised over and over again. Brealey and Myers has a section on this, in which they remind callow students that like backing-up one&apos;s computer files, this is a lesson that everyone seems to have to learn the hard way. Basically, it&apos;s been shown time and again and again; companies which do not audit completed projects in order to see how accurate the original projections were, tend to get exactly the forecasts and projects that they deserve. Companies which have a culture where there are no consequences for making dishonest forecasts, get the projects they deserve. Companies which allocate blank cheques to management teams with a proven record of failure and mendacity, get what they deserve.</p>\n</blockquote>\n<p>If you can independently put stickers on your own chart, then your chart is no longer reliably tracking something externally verified. If forecasts are not checked and tracked, or forecasters are not consequently held accountable for their forecasts, then there is no reason to believe that assessments of future, ongoing, or past programs are accurate. Adopting a wait-and-see attitude, insisting on audits for actual results (not just predictions) before investing more, will definitely slow down funding for good programs. But without it, most of your funding will go to worthless ones.</p>\n<h1>Open Philanthropy, OpenAI, and closed validation loops</h1>\n<p>The Open Philanthropy Project recently&#xA0;<a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/openai-general-support\">announced</a>&#xA0;a $30 million grant to the $1 billion nonprofit AI research organization OpenAI. This is the largest single grant it has ever made. The main point of the grant is to buy influence over OpenAI&#x2019;s future priorities; Holden Karnofsky, Executive Director of the Open Philanthropy Project, is getting a seat on OpenAI&#x2019;s board as part of the deal. This marks the second major shift in focus for the Open Philanthropy Project.</p>\n<p>The first shift (back when it was just called GiveWell) was from trying to find the best already-existing programs to fund (&#x201C;passive funding&#x201D;) to envisioning new programs and working with grantees to make them reality (&#x201C;active funding&#x201D;). The new shift is from funding specific programs at all, to trying to take control of programs without any specific plan.</p>\n<p>To justify the passive funding stage, all you have to believe is that you can know better than other donors, among existing charities. For active funding, you have to believe that you&#x2019;re smart enough to evaluate potential programs, just like a charity founder might, and pick ones that will outperform. But buying control implies that you think you&#x2019;re so much better, that even before you&#x2019;ve evaluated any programs, if someone&#x2019;s doing something big, you ought to have a say.</p>\n<p>When GiveWell moved from a passive to an active funding strategy, it was relying on the moral credit it had earned for its extensive and well-regarded charity evaluations. The thing that was particularly exciting about GiveWell was that they focused on outcomes and efficiency. They didn&apos;t just focus on the size or intensity of the problem a charity was addressing. They didn&apos;t just look at financial details like overhead ratios. They asked the question a consequentialist cares about: for a given expenditure of money, how much will this charity be able to&#xA0;<em>improve outcomes</em>?</p>\n<p>However, when GiveWell tracks its&#xA0;<a href=\"http://www.givewell.org/about/impact\">impact</a>, it does not track objective outcomes at all. It tracks inputs: attention received (in the form of visits to its website) and money moved on the basis of its recommendations. In other words, its estimate of its own impact is based on the level of trust people have placed in it.</p>\n<p>So, as GiveWell built out the Open Philanthropy Project, its story was: We promised to do something great. As a result, we were entrusted with a fair amount of attention and money. Therefore, we should be given more responsibility. We&#xA0;<em>represented</em>&#xA0;our behavior as praiseworthy, and as a result people put stickers on our chart. For this reason, we should be advanced stickers against future days of praiseworthy behavior.</p>\n<p>Then, as the Open Philanthropy Project explored active funding in more areas, its estimate of its own effectiveness grew. After all, it was funding more speculative, hard-to-measure programs, but a multi-billion-dollar donor, which was largely relying on the Open Philanthropy Project&apos;s opinions to assess efficacy (including its own efficacy), continued to trust it.</p>\n<p>What is missing here is any objective track record of benefits. What this looks like to me, is a long sort of confidence game &#x2013; or, using less morally loaded language, a venture with structural reliance on increasing amounts of leverage &#x2013; in the currency of moral credit.</p>\n<h2>Version 0: GiveWell and passive funding</h2>\n<p>First, there was GiveWell. GiveWell&#x2019;s purpose was to find and vet evidence-backed charities. However, it recognized that charities know their own business best. It wasn&#x2019;t trying to do better than the charities; it was trying to do better than the typical charity donor, by being more discerning.</p>\n<p>GiveWell&#x2019;s thinking from this phase is exemplified by co-founder Elie Hassenfeld&#x2019;s&#xA0;<a href=\"http://blog.givewell.org/2011/12/19/6-tips-for-giving-like-a-pro/\">Six tips for giving like a pro</a>:</p>\n<blockquote>\n<p><strong>When you give, give cash &#x2013; no strings attached.&#xA0;</strong>You&#x2019;re just a part-time donor, but the charity you&#x2019;re supporting does this full-time and staff there probably know a lot more about how to do their job than you do. If you&#x2019;ve found a charity that you feel is excellent &#x2013; not just acceptable &#x2013; then it makes sense to trust the charity to make good decisions about how to spend your money.</p>\n</blockquote>\n<p>GiveWell similarly tried to avoid distorting charities&#x2019; behavior. Its job was only to evaluate, not to interfere. To perceive, not to act. To find the best, and buy more of the same.</p>\n<p>How did GiveWell assess its effectiveness in this stage? When GiveWell evaluates charities, it&#xA0;<a href=\"http://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/cost-effectiveness-models\">estimates</a>&#xA0;their cost-effectiveness in advance. It&#xA0;<a href=\"http://www.givewell.org/research/intervention-reports\">assesses the program</a>&#xA0;the charity is running, through experimental evidence of the form of randomized controlled trials. GiveWell also&#xA0;<a href=\"http://www.givewell.org/how-we-work/process#Examining_charities\">audits the charity</a>&#xA0;to make sure they&#x2019;re actually running the program, and figure out how much it costs as implemented. This is an excellent, evidence-based way to generate a&#xA0;<em>prediction</em>&#xA0;of how much good will be done by moving money to the charity.</p>\n<p>As far as I can tell, these predictions are untested.</p>\n<p>One of GiveWell&#x2019;s early top charities was&#xA0;<a href=\"http://www.givewell.org/international/charities/villagereach\">VillageReach</a>, which helped Mozambique with TB immunization logistics. GiveWell estimated that VillageReach could save a life for $1,000. But this charity is no longer recommended. The public page says:</p>\n<blockquote>\n<p>VillageReach (<a href=\"http://www.villagereach.org/\">www.villagereach.org</a>) was our top-rated organization for 2009, 2010 and much of 2011 and it has received over $2 million due to GiveWell&apos;s recommendation. In late 2011, we removed VillageReach from our top-rated list because we felt its project had limited&#xA0;<a href=\"http://www.givewell.org/international/technical/criteria/scalability\">room for more funding</a>. As of November 2012, we believe that that this project may have room for more funding, but we still prefer our current&#xA0;<a href=\"http://www.givewell.org/charities/top-charities\">highest-rated charities</a>&#xA0;above it.</p>\n</blockquote>\n<p>GiveWell reanalyzed the data it based its recommendations on, but hasn&#x2019;t published an after-the-fact retrospective of long-run results. I asked GiveWell about this by email. The response was that such an assessment was not prioritized because GiveWell had found implementation problems in VillageReach&apos;s scale-up work&#xA0;as well as reasons to doubt its original conclusion about the&#xA0;impact&#xA0;of the pilot program. It&apos;s unclear to me whether this has caused GiveWell to evaluate charities differently in the future.</p>\n<p>I don&apos;t think someone looking at GiveWell&apos;s page on VillageReach would be likely to reach the conclusion that GiveWell now believes its original recommendation was likely erroneous. GiveWell&apos;s&#xA0;<a href=\"http://www.givewell.org/about/impact\">impact page</a>&#xA0;continues to count money moved to VillageReach without any mention of the retracted recommendation. If we assume that the point of tracking money moved is to track the benefit of moving money from worse to better uses, then repudiated programs ought to be counted against the total, as costs, rather than towards it.</p>\n<p>GiveWell has recommended the Against Malaria Foundation for the last several years as a top charity. AMF distributes long-lasting insecticide-treated bed nets to prevent mosquitos from transmitting malaria to humans. Its&#xA0;<a href=\"http://www.givewell.org/charities/against-malaria-foundation\">evaluation of AMF</a>&#xA0;does not mention any direct evidence, positive or negative, about what happened to malaria rates in the areas where AMF operated. (There is a discussion of the evidence that the bed nets were in fact delivered and used.) In the&#xA0;<a href=\"http://www.givewell.org/charities/against-malaria-foundation/supplementary-information\">supplementary information</a>&#xA0;page, however, we are told:</p>\n<blockquote>\n<p>Previously, AMF expected to collect data on malaria case rates from the regions in which it funded LLIN distributions: [&#x2026;]&#xA0;In 2016, AMF shared malaria case rate data [&#x2026;] but we have not prioritized analyzing it closely.&#xA0;AMF believes that this data is not high quality enough to reliably indicate actual trends in malaria case rates, so we do not believe that the fact that AMF collects malaria case rate data is a consideration in AMF&#x2019;s favor, and do not plan to continue to track AMF&apos;s progress in collecting malaria case rate data.</p>\n</blockquote>\n<p>The data was noisy, so they simply stopped checking whether AMF&#x2019;s bed net distributions do anything about malaria.</p>\n<p>If we want to know the size of the improvement made by GiveWell in the developing world, we have their&#xA0;<em>predictions</em>&#xA0;about cost-effectiveness, an audit trail verifying that work was performed, and their direct measurement of how much money people gave because they trusted GiveWell. The predictions on the final target &#x2013; improved outcomes &#x2013; have not been tested.</p>\n<p>GiveWell is actually doing unusually well as far as major funders go. It sticks to describing things it&apos;s actually responsible for. By contrast, the Gates Foundation, in a&#xA0;<a href=\"https://www.gatesnotes.com/2017-Annual-Letter\">report</a>&#xA0;to Warren Buffet claiming to describe its impact, simply described overall improvement in the developing world, a very small rhetorical step from claiming credit for 100% of the improvement. GiveWell at least sticks to facts&#xA0;<em>about GiveWell&apos;s own effects</em>, and this is to its credit. But, it focuses on costs it has been able to impose, not benefits it has been able to create.</p>\n<p>The Centre for Effective Altruism&apos;s William MacAskill&#xA0;<a href=\"https://www.givingwhatwecan.org/post/2012/12/some-general-concerns-about-givewell/\">made a related point</a>&#xA0;back in 2012, though he talked about the lack of any sort of formal outside validation or audit, rather than focusing on empirical validation of outcomes:</p>\n<blockquote>\n<p>As far as I know, GiveWell haven&apos;t commissioned a thorough&#xA0;<a href=\"http://www.givewell.org/about/self-evaluation\">external evaluation</a>&#xA0;of their recommendations. [&#x2026;] This surprises me. Whereas businesses have a natural feedback mechanism, namely profit or loss, research often doesn&apos;t, hence the need for peer-review within academia. This concern, when it comes to charity-evaluation, is even greater. If GiveWell&apos;s analysis and recommendations had major flaws, or were systematically biased in some way, it would be challenging for outsiders to work this out without a thorough independent evaluation. Fortunately, GiveWell has the resources to, for example, employ two top development economists to each do an independent review of their recommendations and the supporting research. This would make their recommendations more robust at a reasonable&#xA0;cost.</p>\n</blockquote>\n<p>GiveWell&apos;s&#xA0;<a href=\"http://www.givewell.org/how-we-work/self-evaluation#External_Reviews\">page on self-evaluation</a>&#xA0;says that it discontinued external reviews in August 2013. This page links to an&#xA0;<a href=\"http://blog.givewell.org/2013/03/01/external-evaluation-of-our-research/\">explanation of the decision</a>, which concludes:</p>\n<blockquote>\n<p>We continue to believe that it is important to ensure that our work is subjected to in-depth scrutiny. However, at this time, the scrutiny we&#x2019;re naturally receiving &#x2013; combined with the high costs and limited capacity for formal external evaluation &#x2013; make us inclined to postpone major effort on external evaluation for the time being.</p>\n<p>That said,</p>\n<ul>\n<li>&gt;If someone volunteered to do (or facilitate) formal external evaluation, we&#x2019;d welcome this and would be happy to prominently post or link to criticism.</li>\n<li>We do intend eventually to re-institute formal external evaluation.</li>\n</ul>\n</blockquote>\n<p>Four years later, assessing the credibility of this assurance is left as an exercise for the reader.</p>\n<h2>Version 1: GiveWell Labs and active funding</h2>\n<p>Then there was&#xA0;<a href=\"http://blog.givewell.org/2011/09/08/announcing-givewell-labs/\">GiveWell Labs</a>, later called the Open Philanthropy Project. It looked into more potential&#xA0;<a href=\"http://www.openphilanthropy.org/research/cause-reports\">philanthropic causes</a>, where the evidence base might not be as cut-and-dried as that for the GiveWell top charities. One thing they learned was that in many areas, there simply weren&#x2019;t shovel-ready programs ready for funding &#x2013; a funder has to play a more active role. This shift was described by GiveWell co-founder Holden Karnofsky in his 2013 blog post,&#xA0;<a href=\"http://blog.givewell.org/2013/04/18/challenges-of-passive-funding/\">Challenges of passive funding</a>:</p>\n<blockquote>\n<p>By &#x201C;passive funding,&#x201D; I mean a dynamic in which the funder&#x2019;s role is to review others&#x2019; proposals/ideas/arguments and pick which to fund, and by &#x201C;active funding,&#x201D; I mean a dynamic in which the funder&#x2019;s role is to participate in &#x2013; or lead &#x2013; the development of a strategy, and find partners to &#x201C;implement&#x201D; it. Active funders, in other words, are participating at some level in &#x201C;management&#x201D; of partner organizations, whereas passive funders are merely choosing between plans that other nonprofits have already come up with.</p>\n<p>My instinct is generally to try the most &#x201C;passive&#x201D; approach that&#x2019;s feasible. Broadly speaking, it seems that a good partner organization will generally know their field and environment better than we do and therefore be best positioned to design strategy; in addition, I&#x2019;d expect a project to go better when its implementer has fully bought into the plan as opposed to carrying out what the funder wants. However, (a) this philosophy seems to contrast heavily with how most existing major funders operate; (b) I&#x2019;ve seen multiple reasons to believe the &#x201C;active&#x201D; approach may have more relative merits than we had originally anticipated. [&#x2026;]</p>\n<ul>\n<li>In the nonprofit world of today, it seems to us that funder interests are major drivers of which ideas that get proposed and fleshed out, and therefore, as a funder, it&#x2019;s important to express interests rather than trying to be fully &#x201C;passive.&#x201D;</li>\n<li>While we still wish to err on the side of being as &#x201C;passive&#x201D; as possible, we are recognizing the importance of clearly articulating our values/strategy, and also recognizing that an area can be underfunded even if we can&#x2019;t easily find shovel-ready funding opportunities in it.</li>\n</ul>\n</blockquote>\n<p>GiveWell earned some credibility from its novel, evidence-based outcome-oriented approach to charity evaluation. But this credibility was already &#x2013; and still is &#x2013; a sort of loan. We have GiveWell&apos;s predictions or promises of cost effectiveness in terms of outcomes, and we have figures for money moved, from which we can infer how much we were promised in improved outcomes. As far as I know, no one&apos;s gone back and checked whether those promises turned out to be true.</p>\n<p>In the meantime, GiveWell then leveraged this credibility by extending its methods into more speculative domains, where less was checkable, and donors had to put more trust in the subjective judgment of GiveWell analysts. This was called GiveWell Labs. At the time, this sort of compounded leverage may have been sensible, but it&apos;s important to track whether a debt has been paid off or merely rolled over.</p>\n<h2>Version 2: The Open Philanthropy Project and control-seeking</h2>\n<p>Finally, the Open Philanthropy made its largest-ever single grant to purchase its founder a seat on a major organization&#x2019;s board. This represents a transition from mere active funding to&#xA0;<a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/openai-general-support\">overtly purchasing influence</a>:</p>\n<blockquote>\n<p>The Open Philanthropy Project awarded a grant of $30 million ($10 million per year for 3 years) in general support to OpenAI. This grant initiates a partnership between the Open Philanthropy Project and OpenAI, in which Holden Karnofsky (Open Philanthropy&#x2019;s Executive Director, &#x201C;Holden&#x201D; throughout this page) will join OpenAI&#x2019;s Board of Directors and, jointly with one other Board member, oversee OpenAI&#x2019;s safety and governance work.</p>\n<p>We expect the primary benefits of this grant to stem from our partnership with OpenAI, rather than simply from contributing funding toward OpenAI&#x2019;s work. While we would also expect general support for OpenAI to be likely beneficial on its own, the case for this grant hinges on the benefits we anticipate from our partnership, particularly the opportunity to help play a role in OpenAI&#x2019;s approach to safety and governance issues.</p>\n</blockquote>\n<p>Clearly&#xA0;<a href=\"http://benjaminrosshoffman.com/an-openai-board-seat-is-surprisingly-expensive/\">the value proposition is not increasing available funds for OpenAI</a>, if OpenAI&#x2019;s founders&#x2019;&#xA0;<a href=\"https://blog.openai.com/introducing-openai/\">billion-dollar commitment</a>&#xA0;to it is real:</p>\n<blockquote>\n<p>Sam, Greg, Elon, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services (AWS), Infosys, and&#xA0;<a href=\"https://ycr.org/\">YC Research</a>&#xA0;are donating to support OpenAI. In total, these funders have committed $1 billion, although we expect to only spend a tiny fraction of this in the next few years.</p>\n</blockquote>\n<p>The Open Philanthropy Project is neither using this money to fund programs that have a track record of working, nor to fund a specific program that it has prior reason to expect will do good. Rather, it is buying control, in the hope that Holden will be able to persuade OpenAI not to destroy the world, because he knows better than OpenAI&#x2019;s founders.</p>\n<p>How does the Open Philanthropy Project know that Holden knows better? Well, it&#x2019;s done some active funding of programs it expects to work out. It expects those programs to work out because they were approved by a process similar to the one used by GiveWell to find charities that it expects to save lives.</p>\n<p>If you want to acquire control over something, that implies that you think you can manage it more sensibly than whoever is in control already. Thus, buying control is a claim to have superior judgment - not just over others funding things (the original GiveWell pitch), but over those being funded.</p>\n<p>In a&#xA0;<a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/openai-general-support#footnote3_w9trscz\">footnote</a>&#xA0;to the very post announcing the grant, the Open Philanthropy Project notes that it has historically tried to avoid acquiring leverage over organizations it supports, precisely because it&#x2019;s&#xA0;<em>not</em>&#xA0;sure it knows better:</p>\n<blockquote>\n<p>For now, we note that providing a high proportion of an organization&#x2019;s funding may cause it to be dependent on us and accountable primarily to us. This may mean that we come to be seen as more responsible for its actions than we want to be; it can also mean we have to choose between providing bad and possibly distortive guidance/feedback (unbalanced by other stakeholders&#x2019; guidance/feedback) and leaving the organization with essentially no accountability.</p>\n</blockquote>\n<p>This seems to describe two main problems introduced by becoming a dominant funder:</p>\n<ol>\n<li>People might accurately attribute causal responsibility for some of the organization&apos;s conduct to the Open Philanthropy Project.</li>\n<li>The Open Philanthropy Project might influence the organization to behave differently than it otherwise would.</li>\n</ol>\n<p>The first seems obviously silly. I&apos;ve been trying to correct the imbalance where Open Phil is criticized mainly when it makes grants, by <a href=\"/ea/17e/givewell_and_partial_funding/\">criticizing it for holding onto too much money</a>.</p>\n<p>The second really is a cost as well as a benefit, and the Open Philanthropy Project has been absolutely correct to recognize this. This is the sort of thing GiveWell has consistently gotten right since the beginning and it deserves credit for making this principle clear and &#x2013; until now &#x2013; living up to it.</p>\n<p>But discomfort with being dominant funders seems inconsistent with buying a board seat to influence OpenAI. If the Open Philanthropy Project thinks that Holden&#x2019;s judgment is good enough that he should be in control, why only here? If he thinks that other Open Philanthropy Project AI safety grantees have good judgment but OpenAI doesn&#x2019;t, why not give them similar amounts of money free of strings to spend at their discretion and see what happens? Why not buy people like Eliezer Yudkowsky, Nick Bostrom, or Stuart Russell a seat on OpenAI&#x2019;s board?</p>\n<p>On the other hand, the Open Philanthropy Project is&#xA0;<a href=\"http://benjaminrosshoffman.com/openai-makes-humanity-less-safe/\">right on the merits here</a>&#xA0;with respect to safe superintelligence development. Openness makes sense for weak AI, but if you&#x2019;re building true strong AI you want to make sure you&#x2019;re cooperating with all the other teams in a single closed effort. I agree with the Open Philanthropy Project&#x2019;s assessment of the relevant risks. But it&apos;s not clear to me how often joining the bad guys to prevent their worst excesses is a good strategy, and it seems like it has to often be a mistake. Still, I&#x2019;m mindful of heroes like&#xA0;<a href=\"https://en.wikipedia.org/wiki/John_Rabe\">John Rabe</a>,&#xA0;<a href=\"https://en.wikipedia.org/wiki/Chiune_Sugihara\">Chiune Sugihara</a>, and&#xA0;<a href=\"https://en.wikipedia.org/wiki/Oskar_Schindler\">Oscar Schindler</a>. And if I think someone has a good idea for improving things, it makes sense to reallocate control from people who have worse ideas, even if there&apos;s some potential better allocation.</p>\n<p>On the other hand, is Holden Karnofsky the right person to do this? The case is mixed.</p>\n<p>He listens to and engages with the arguments from principled advocates for AI safety research, such as Nick Bostrom, Eliezer Yudkowsky, and Stuart Russell. This is a point in his favor. But, I can think of other people who engage with such arguments. For instance, OpenAI founder Elon Musk has publicly praised Bostrom&#x2019;s book&#xA0;<em>Superintelligence</em>, and founder Sam Altman has written&#xA0;<a href=\"http://blog.samaltman.com/machine-intelligence-part-1\">two</a>&#xA0;<a href=\"http://blog.samaltman.com/machine-intelligence-part-2\">blog posts</a>&#xA0;summarizing concerns about AI safety reasonably cogently. Altman even asked Luke Muehlhauser, former executive director of MIRI, for feedback pre-publication. He&apos;s met with Nick Bostrom. That suggests a substantial level of direct engagement with the field, although Holden has engaged for a longer time, more extensively, and more directly.</p>\n<p>Another point in Holden&#x2019;s favor, from my perspective, is that under his leadership, the Open Philanthropy Project has funded the most serious-seeming programs for both weak and strong AI safety research. But Musk also&#xA0;<a href=\"https://futureoflife.org/first-ai-grant-recipients/\">managed to (indirectly) fund</a>&#xA0;AI safety research at MIRI and by Nick Bostrom personally, via his $10 million FLI grant.</p>\n<p>The Open Philanthropy Project also says that it expects to learn a lot about AI research from this, which will help it make better decisions on AI risk in the future and influence the field in the right way. This is reasonable as far as it goes. But remember that the case for positioning the Open Philanthropy Project to do this relies on the assumption that the Open Philanthropy Project will improve matters by becoming a central influencer in this field. This move is consistent with reaching that goal, but it is not independent evidence that the goal is the right one.</p>\n<p>Overall, there are good narrow reasons to think that this is a potential improvement over the prior situation around OpenAI &#x2013; but only a small and ill-defined improvement, at considerable&#xA0;<a href=\"http://benjaminrosshoffman.com/openai-makes-humanity-less-safe/#What_if_OpenAI_and_DeepMind_are_not_working_on_problems_relevant_to_superintelligence\">attentional</a>&#xA0;cost, and with the offsetting potential harm of increasing OpenAI&apos;s&#xA0;<a href=\"http://benjaminrosshoffman.com/an-openai-board-seat-is-surprisingly-expensive/\">perceived legitimacy</a>&#xA0;as a long-run AI safety organization.</p>\n<p>And it&#x2019;s worrying that Open Philanthropy Project&#x2019;s largest grant &#x2013; not just for AI risk, but&#xA0;<em>ever</em>&#xA0;(aside from GiveWell Top Charity funding) &#x2013; is being made to an organization at which Holden&#x2019;s housemate and future brother-in-law is a leading researcher. The nepotism argument is not my central objection. If I otherwise thought the grant were obviously a good idea, it wouldn&#x2019;t worry me, because it&#x2019;s natural for people with shared values and outlooks to become close nonprofessionally as well. But in the absence of a clear compelling specific case for the grant, it&#x2019;s worrying.</p>\n<p>Altogether, I&apos;m not saying this is an unreasonable shift, considered in isolation. I&#x2019;m not even sure this is a bad thing for the Open Philanthropy Project to be doing &#x2013; insiders may have information that I don&#x2019;t, and that is difficult to communicate to outsiders. But as outsiders, there comes a point when someone&#x2019;s maxed out their moral credit, and we should wait for results before actively trying to entrust the Open Philanthropy Project and its staff with more responsibility.</p>\n<h1>EA Funds and self-recommendation</h1>\n<p>The Centre for Effective Altruism is actively trying to entrust the Open Philanthropy Project and its staff with more responsibility.</p>\n<p>The concerns of CEA&#x2019;s CEO William MacAskill about GiveWell have, as far as I can tell, never been addressed, and the underlying issues have only become more acute. But CEA is now working to put more money under the control of Open Philanthropy Project staff, through its new&#xA0;<a href=\"https://app.effectivealtruism.org/funds\">EA Funds</a>&#xA0;product &#x2013; a way for supporters to delegate giving decisions to expert EA &#x201C;fund managers&#x201D; by giving to one of four funds:&#xA0;<a href=\"https://app.effectivealtruism.org/funds/global-development\">Global Health and Development</a>,&#xA0;<a href=\"https://app.effectivealtruism.org/funds/animal-welfare\">Animal Welfare</a>,&#xA0;<a href=\"https://app.effectivealtruism.org/funds/far-future\">Long-Term Future</a>, and&#xA0;<a href=\"https://app.effectivealtruism.org/funds/ea-community\">Effective Altruism Community</a>.</p>\n<p>The Effective Altruism movement began by saying that because very poor people exist, we should reallocate money from ordinary people in the developed world to the global poor. Now the pitch is in effect that because very poor people exist, we should reallocate money from ordinary people in the developed world to the extremely wealthy. This is a strange and surprising place to end up, and it&#x2019;s worth retracing our steps. Again, I find it easiest to think of three stages:</p>\n<ol>\n<li>Money can go much farther in the developing world. Here, we&#x2019;ve found some examples for you. As a result, you can do a huge amount of good by giving away a large share of your income, so you ought to.</li>\n<li>We&#x2019;ve found ways for you to do a huge amount of good by giving away a large share of your income for developing-world interventions, so you ought to trust our recommendations. You ought to give a large share of your income to these weird things our friends are doing that are even better, or join our friends.</li>\n<li>We&#x2019;ve found ways for you to do a huge amount of good by funding weird things our friends are doing, so you ought to trust the people we trust. You ought to give a large share of your income to a multi-billion-dollar foundation that funds such things.</li>\n</ol>\n<h2>Stage 1: The direct pitch</h2>\n<p><a href=\"https://www.givingwhatwecan.org/about-us/history/\">At first</a>, Giving What We Can (the organization that eventually became CEA) had a simple, easy to understand pitch:</p>\n<blockquote>\n<p>Giving What We Can is the brainchild of Toby Ord, a philosopher at Balliol College, Oxford. Inspired by the ideas of ethicists Peter Singer and Thomas Pogge, Toby decided in 2009 to commit a large proportion of his income to charities that effectively alleviate poverty in the developing&#xA0;world.</p>\n<p>[&#x2026;]</p>\n<p>Discovering that many of his friends and colleagues were interested in making a similar pledge, Toby worked with fellow Oxford philosopher Will MacAskill to create an international organization of people who would donate a significant proportion of their income to cost-effective&#xA0;charities.</p>\n<p>Giving What We Can launched in November 2009, attracting significant media attention. Within a year, 64 people had joined the society, their pledged donations amounting to $21 million. Initially run on a volunteer basis, Giving What We Can took on full-time staff in the summer of 2012.</p>\n</blockquote>\n<p>In effect, its argument was: &quot;Look, you can do huge amounts of good by giving to people in the developing world. Here are some examples of charities that do that. It seems like a great idea to give 10% of our income to those charities.&quot;</p>\n<p>GWWC was a simple product, with a clear, limited scope. Its founders believed that people, including them, ought to do a thing &#x2013; so they argued directly for that thing, using the arguments that had persuaded them. If it wasn&apos;t for you, it was easy to figure that out; but a surprisingly large number of people were persuaded by a simple, direct statement of the argument, took the pledge, and gave a lot of money to charities helping the world&apos;s poorest.</p>\n<h2>Stage 2: Rhetoric and belief diverge</h2>\n<p>Then, GWWC staff were persuaded you could do even more good with your money in areas other than developing-world charity, such as existential risk mitigation. Encouraging donations and work in these areas became part of the broader Effective Altruism movement, and GWWC&apos;s umbrella organization was named the Centre for Effective Altruism. So far, so good.</p>\n<p>But this left Effective Altruism in an awkward position; while leadership often personally believe the most effective way to do good is far-future stuff or similarly weird-sounding things, many people who can see the merits of the developing-world charity argument reject the argument that because the vast majority of people live in the far future, even a very small improvement in humanity&#x2019;s long-run prospects outweighs huge improvements on the global poverty front. They also often reject similar scope-sensitive arguments for things like animal charities.</p>\n<p>Giving What We Can&apos;s page on&#xA0;<a href=\"https://www.givingwhatwecan.org/get-involved/what-we-can-achieve/\">what we can achieve</a>&#xA0;still focuses on global poverty, because developing-world charity is easier to explain persuasively. However, EA leadership tends to privately focus on things like AI risk. Two years ago many attendees at the EA Global conference in the San Francisco Bay Area were surprised that the conference focused so heavily on AI risk, rather than the global poverty interventions they&#x2019;d expected.</p>\n<h2>Stage 3: Effective altruism is self-recommending</h2>\n<p>Shortly before the launch of the EA Funds I was told in informal conversations that they were a response to demand. Giving What We Can pledge-takers and other EA donors had told CEA that they trusted it to GWWC pledge-taker demand. CEA was responding by creating a product for the people who wanted it.</p>\n<p>This seemed pretty reasonable to me, and on the whole good. If someone wants to trust you with their money, and you think you can do something good with it, you might as well take it, because they&#x2019;re estimating your skill above theirs. But&#xA0;<a href=\"http://benjaminrosshoffman.com/humble-charlie/\">not everyone agrees</a>, and as the Madoff case demonstrates, &quot;people are begging me to take their money&quot; is not a definitive argument that you are doing anything real.</p>\n<p>In practice, the funds are managed by Open Philanthropy Project staff:</p>\n<blockquote>\n<p>We want to keep this idea as simple as possible to begin with, so we&#x2019;ll have just four funds, with the following managers:</p>\n<ul>\n<li>Global Health and Development - Elie Hassenfeld</li>\n<li>Animal Welfare &#x2013; Lewis Bollard</li>\n<li>Long-run future &#x2013; Nick Beckstead</li>\n<li>Movement-building &#x2013; Nick Beckstead</li>\n</ul>\n<p>(Note that the meta-charity fund will be able to fund CEA; and note that Nick Beckstead is a Trustee of CEA. The long-run future fund and the meta-charity fund continue the work that Nick has been doing running the EA Giving Fund.)</p>\n</blockquote>\n<p>It&#x2019;s not a coincidence that all the fund managers work for GiveWell or Open Philanthropy. &#xA0;First, these are the organisations whose charity evaluation we respect the most. The worst-case scenario, where your donation just adds to the Open Philanthropy funding within a particular area, is therefore still a great outcome. &#xA0;Second, they have the best information available about what grants Open Philanthropy are planning to make, so have a good understanding of where the remaining funding gaps are, in case they feel they can use the money in the EA Fund to fill a gap that they feel is important, but isn&#x2019;t currently addressed by Open Philanthropy.</p>\n<p>In past years, Giving What We Can recommendations have largely overlapped with GiveWell&#x2019;s top charities.</p>\n<p>In the comments on the&#xA0;<a href=\"/ea/174/introducing_the_ea_funds/\">launch announcement</a>&#xA0;on the EA Forum, several people (including me) pointed out that the Open Philanthropy Project seems to be having trouble giving away even the money it already has, so it seems odd to direct more money to Open Philanthropy Project decisionmakers. CEA&#x2019;s senior marketing manager&#xA0;<a href=\"/ea/174/introducing_the_ea_funds/a2y\">replied</a>&#xA0;that the Funds were a minimum viable product to test the concept:</p>\n<blockquote>\n<p>I don&apos;t think the long-term goal is that OpenPhil program officers are the only fund managers. Working with them was the best way to get an MVP version in place.</p>\n</blockquote>\n<p>This also seemed okay to me, and I said so at the time.</p>\n<p>[NOTE: I&apos;ve edited the next paragraph to excise some unreliable information. Sorry for the error, and thanks to Rob Wiblin for pointing it out.]</p>\n<p>After they were launched, though, I saw phrasings that were not so cautious at all, instead making claims that this was generally a better way to give. As of writing this, if someone on the effectivealtruism.org website clicks on &quot;Donate Effectively&quot; they will be led directly to a page promoting EA Funds. When I looked at Giving What We Can&#x2019;s <a href=\"https://www.givingwhatwecan.org/top-charities/\">top charities</a> page in early April, it recommended the EA Funds &quot;as the highest impact option for donors.&quot;</p>\n<p>This is not a response to demand, it is an attempt to create demand by using CEA&apos;s authority, telling people that the funds are&#xA0;<em>better</em>&#xA0;than what they&apos;re doing already. By contrast, GiveWell&apos;s&#xA0;<a href=\"http://www.givewell.org/charities/top-charities\">Top Charities</a>&#xA0;page simply says:</p>\n<blockquote>\n<p>Our top charities are evidence-backed, thoroughly vetted, underfunded organizations.</p>\n</blockquote>\n<p>This carefully avoids any overt claim that they&apos;re the highest-impact option available to donors. GiveWell avoids saying that because there&apos;s no way they could know it, so saying it wouldn&apos;t be truthful.</p>\n<p>A marketing email might have just been dashed off quickly, and an exaggerated wording might just have been an oversight. But when I looked at Giving What We Can&#x2019;s&#xA0;<a href=\"https://www.givingwhatwecan.org/top-charities/\">top charities</a>&#xA0;page in early April, it recommended the EA Funds &quot;as the highest impact option for donors.&quot;</p>\n<p>The wording has since been qualified with &#x201C;for most donors&#x201D;, which is a good change. But the thing I&#x2019;m worried about isn&#x2019;t just the explicit exaggerated claims &#x2013; it&#x2019;s the underlying marketing mindset that made them seem like a good idea in the first place. EA seems to have switched from an endorsement of the best things outside itself, to an endorsement of itself. And it&apos;s concentrating decisionmaking power in the Open Philanthropy Project.</p>\n<h1>Effective altruism is overextended, but it doesn&apos;t have to be</h1>\n<p>There is a saying in finance, that was old even back when Keynes said it. If you owe the bank a&#xA0;<strong><em>million</em></strong>&#xA0;dollars, then&#xA0;<strong><em>you</em></strong>&#xA0;have a problem. If you owe the bank a&#xA0;<strong><em>billion</em></strong>&#xA0;dollars, then the&#xA0;<strong><em>bank</em></strong>&#xA0;has a problem.</p>\n<p>In other words, if someone extends you a level of trust they could survive writing off, then they might call in that loan. As a result, they have leverage over you. But if they overextend, putting all their eggs in one basket, and you are that basket, then you have leverage over them; you&apos;re too big to fail. Letting you fail would be so disastrous for their interests that you can extract nearly arbitrary concessions from them, including further investment. For this reason, successful institutions often try to diversify their investments, and avoid overextending themselves. Regulators, for the same reason, try to prevent&#xA0;<em>banks</em>&#xA0;from becoming &quot;too big to fail.&quot;</p>\n<p>The Effective Altruism movement is concentrating decisionmaking power and trust as much as possible, in a way that&apos;s setting itself up to invest ever increasing amounts of confidence&#xA0;to keep the game&#xA0;going.</p>\n<p>The alternative is to keep the scope of each organization narrow, overtly ask for trust for each venture separately, and make it clear what sorts of programs are being funded. For instance, Giving What We Can should go back to its initial focus of global poverty relief.</p>\n<p>Like many EA leaders, I happen to believe that anything you can do to steer the far future in a better direction is much, much more consequential for the well-being of sentient creatures than any purely short-run improvement you can create now. So it might seem odd that I think Giving What We Can should stay focused on global poverty. But, I believe that the single most important thing we can do to improve the far future is&#xA0;<em>hold onto our ability to accurately build shared models</em>. If we use bait-and-switch tactics, we are actively eroding the most important type of capital we have &#x2013; coordination capacity.</p>\n<p>If you do not think giving 10% of one&apos;s income to global poverty charities is the right thing to do, then you can&apos;t in full integrity urge others to do it &#x2013; so you should&#xA0;<em>stop</em>. You might still believe that GWWC ought to exist. You might still believe that it is a positive good to encourage people to give much of their income to help the global poor, if they wouldn&apos;t have been doing anything else especially effective with the money. If so, and you happen to find yourself in charge of an organization like Giving What We Can, the thing to do is write a letter to GWWC members telling them that you&apos;ve changed your mind, and why, and offering to&#xA0;<em>give away the brand</em>&#xA0;to whoever seems best able to honestly maintain it.</p>\n<p>If someone at the Centre for Effective Altruism fully believes in GWWC&apos;s original mission, then that might make the transition easier. If not, then one still has to tell the truth and do what&apos;s right.</p>\n<p>And what of the EA Funds? The&#xA0;<a href=\"https://app.effectivealtruism.org/funds/far-future\">Long-Term Future Fund</a>&#xA0;is run by Open Philanthropy Project Program Officer Nick Beckstead. If you think that it&apos;s a good thing to delegate giving decisions to Nick, then I would agree with you. Nick&apos;s a great guy! I&apos;m always happy to see him when he shows up at house parties. He&apos;s smart, and he actively seeks out arguments against his current point of view. But the right thing to do, if you want to persuade people to delegate their giving decisions to Nick Beckstead, is to&#xA0;<em>make a principled case for delegating giving decisions to Nick Beckstead</em>. If the Centre for Effective Altruism did that, then Nick would almost certainly feel more free to allocate funds to the best things he knows about, not just the best things he suspects EA Funds donors would be able to understand and agree with.</p>\n<p>If you can&apos;t directly persuade people, then maybe you&apos;re wrong. If the problem is inferential distance, then you&apos;ve got some work to do bridging that gap.</p>\n<p>There&apos;s nothing wrong with setting up a fund to make it easy. It&apos;s actually a really good idea. But there is something wrong with the multiple layers of vague indirection involved in the current marketing of the Far Future fund &#x2013; using global poverty to sell the generic idea of doing the most good, then using CEA&apos;s identity as the organization in charge of doing the most good to persuade people to delegate their giving decisions to it, and then sending their money to some dude at the multi-billion-dollar foundation to give away at his personal discretion. The same argument applies to all four Funds.</p>\n<p>Likewise, if you think that working directly on AI risk is the most important thing, then you should make arguments directly for working on AI risk. If you can&apos;t directly persuade people, then maybe you&apos;re wrong. If the problem is inferential distance, it might make sense to imitate the example of someone like Eliezer Yudkowsky, who used indirect methods to bridge the inferential gap by writing extensively on individual human rationality, and&#xA0;<a href=\"/lw/66/rationality_common_interest_of_many_causes/\">did not try to control others&apos; actions</a>&#xA0;in the meantime.</p>\n<p>If Holden thinks he should be in charge of some AI safety research, then he should ask Good Ventures for funds to actually start an AI safety research organization. I&apos;d be excited to see what he&apos;d come up with if he had full control of and responsibility for such an organization. But I don&apos;t think anyone has a good plan to work directly on AI risk, and I don&apos;t have one either, which is why I&apos;m not directly working on it or funding it. My plan for improving the far future is to build&#xA0;<em>human</em>&#xA0;coordination capacity.</p>\n<p>(If, by contrast, Holden just thinks there needs to be <em>coordination</em> between different AI safety organizations, the obvious thing to do would be to work with FLI on that, e.g. by giving them enough money to throw <em>their</em>&#xA0;weight around as a funder. They organized the successful Puerto Rico conference, after all.)</p>\n<p>Another thing that would be encouraging would be if at least one of the Funds were not administered entirely by an Open Philanthropy Project staffer, and ideally an expert who doesn&apos;t benefit from the halo of &quot;being an EA.&quot; For instance, Chris Blattman is a development economist with experience designing programs that don&apos;t just use but&#xA0;<a href=\"http://chrisblattman.com/2016/07/19/14411/\">generate</a>&#xA0;evidence on what works. When people were arguing about whether sweatshops are good or bad for the global poor, he actually&#xA0;<a href=\"http://chrisblattman.com/2016/09/29/14655/\">went and looked</a>&#xA0;by performing a randomized controlled trial. He&apos;s&#xA0;<a href=\"http://chrisblattman.com/2017/01/25/come-work-build-two-huge-new-research-initiatives-violence-reduction-recovery/\">leading two new initiatives with J-PAL and IPA</a>, and expects that directors designing studies will also have to spend time fundraising. Having funding lined up seems like the sort of thing that would let them spend more time actually running programs. And more generally, he seems likely to know about funding opportunities the Open Philanthropy Project doesn&apos;t, simply because he&apos;s embedded in a slightly different part of the global health and development network.</p>\n<p>Narrower projects that rely less on the EA brand and more on&#xA0;<em>what they&apos;re actually doing</em>, and more cooperation on equal terms with outsiders who seem to be doing something good already, would do a lot to help EA grow beyond putting stickers on its own behavior chart. I&apos;d like to see EA grow up. I&apos;d be excited to see what it might do.</p>\n<h1>Summary</h1>\n<ol>\n<li>Good programs don&apos;t need to distort the story people tell about them, while bad programs do.</li>\n<li>Moral confidence games &#x2013; treating past promises and trust as a track record to justify more trust &#x2013; are an example of the kind of distortion mentioned in (1), that benefits bad programs more than good ones.</li>\n<li>The Open Philanthropy Project&apos;s Open AI grant represents a shift from evaluating other programs&apos; effectiveness, to assuming its own effectiveness.</li>\n<li>EA Funds represents a shift from EA evaluating programs&apos; effectiveness, to assuming EA&apos;s effectiveness.</li>\n<li>A shift from evaluating other programs&apos; effectiveness, to assuming one&apos;s own effectiveness, is an example of the kind of &quot;moral confidence game&quot; mentioned in (2).</li>\n<li>EA ought to focus on scope-limited projects, so that it can directly make the case for those particular projects instead of relying on EA identity as a reason to support an EA organization.</li>\n<li>EA organizations ought to entrust more responsibility to outsiders who seem to be doing good things but don&apos;t overtly identify as EA, instead of trying to keep it all in the family.</li>\n</ol>\n<div><br>(Originally posted at <a href=\"http://benjaminrosshoffman.com/effective-altruism-is-self-recommending/\">my personal blog</a> and <a href=\"http://lesswrong.com/r/discussion/lw/ox4/effective_altruism_is_selfrecommending/\">LessWrong</a>. I&apos;ve gotten some encouragement to cross-post this here, so I&apos;m doing so.<br><br>Disclosure: I know many people involved at many of the organizations discussed, and I used to work for GiveWell. I have no current institutional affiliation to any of them. Everyone mentioned has always been nice to me and I have no personal complaints.)</div></body></html>", "user": {"username": "BenHoffman"}}, {"_id": "MsaS8JKrR8nnxyPkK", "title": "Update on Effective Altruism Funds", "postedAt": "2017-04-20T17:20:03.808Z", "htmlBody": "<html><body><p><span>This post is an update on the progress of Effective Altruism Funds. If you&#x2019;re not familiar with EA Funds please check out our </span><a href=\"/ea/17v/ea_funds_beta_launch/\"><span>launch post</span></a><span> and our original </span><a href=\"/ea/174/introducing_the_ea_funds/\"><span>concept post</span></a><span>. The EA Funds website is <a href=\"https://app.effectivealtruism.org/funds\">here.</a></span></p>\n<p>&#xA0;</p>\n<p><span>EA Funds launched on February 28, 2017. In our launch post we said:</span></p>\n<p>&#xA0;</p>\n<blockquote>\n<p><em><span>We only want to focus on the Effective Altruism Funds if the community believes it will improve the effectiveness of their donations and that it will provide substantial value to the EA community. Accordingly, we plan to run the project for the next 3 months and then reassess whether the project should continue and if so, in what form.</span></em></p>\n</blockquote>\n<p>&#xA0;</p>\n<p><span>Our review of the evidence so far has caused us to conclude that EA Funds will continue past the three-month experiment in some form. However, details such as which funds we offer, who manages them, and the content and design of the website may change as a result of what we learn during the three month trial period to the end of May.</span></p>\n<p>&#xA0;</p>\n<p><span>Below we review how EA Funds has performed since launch, we unveil our first round of grant recommendations by the fund managers, highlight some of the mistakes we&#x2019;ve made so far, and outline some of our short-term priorities.</span></p>\n<p>&#xA0;</p>\n<h1><span>Traction of EA Funds so far</span></h1>\n<p><span>In our launch post we said:</span></p>\n<p>&#xA0;</p>\n<blockquote>\n<p><em><span>The main way we will assess if the funds provide value to our community is total recurring donations to the EA Funds and community feedback.</span></em></p>\n</blockquote>\n<p>&#xA0;</p>\n<p><span>We outline our traction on each of these dimensions below.</span></p>\n<p>&#xA0;</p>\n<h2><span>Donations</span></h2>\n<p><span>At the time of writing $672,925 has been donated to EA Funds with an additional $26,861 in monthly recurring donations. Of the total amount donated, $250,000 came from a single new donor that Will met, although EA Funds has received donations from 403 unique donors as well.</span></p>\n<p>&#xA0;</p>\n<p><span>Stats on individual funds are provided below:</span></p>\n<p>&#xA0;</p>\n<div>\n<table><colgroup><col><col><col></colgroup>\n<tbody>\n<tr>\n<td>\n<p><span>Fund Name</span></p>\n</td>\n<td>\n<p><span>Amount Donated</span></p>\n</td>\n<td>\n<p><span>Monthly recurring donations</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Global Health and Development</span></p>\n</td>\n<td>\n<p><span>$311,562</span></p>\n</td>\n<td>\n<p><span>$10,529</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Animal Welfare</span></p>\n</td>\n<td>\n<p><span>$161,824</span></p>\n</td>\n<td>\n<p><span>$4,756</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Long-Term Future</span></p>\n</td>\n<td>\n<p><span>$118,342</span></p>\n</td>\n<td>\n<p><span>$8,151</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>EA Community</span></p>\n</td>\n<td>\n<p><span>$74,704</span></p>\n</td>\n<td>\n<p><span>$3,156</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>&#xA0;</p>\n<p><span>The donation amounts we&#x2019;ve received so far are greater than we expected, especially given that donations typically decrease early in the year after ramping up towards the end of the year. </span></p>\n<p>&#xA0;</p>\n<p><span>We&#x2019;ve also been impressed with the relative lack of slowdown in new donations over time. New projects typically experience a surge in usage and then a significant slowdown (sometimes called the </span><a href=\"http://andrewchen.co/after-the-techcrunch-bump-life-in-the-trough-of-sorrow/\"><span>Trough of Sorrow</span></a><span>). While we&#x2019;ve experienced slowdown since launch, we&#x2019;ve also seen a steady stream of around 5-10 new donations per day to EA Funds.</span></p>\n<p>&#xA0;</p>\n<h2><span>Community feedback</span></h2>\n<p><span>We&#x2019;ve mostly gauged community feedback through a combination of reading comments on our launch post, reading feedback on EA Funds on Facebook, and talking to people outside of CEA whose opinions we trust. While this way of gauging feedback is far from perfect, our impression is that community feedback has been positive overall. (<em>Note: the claim that the community feedback has been positive overall has been disputed in the comments below.</em>)</span></p>\n<p>&#xA0;</p>\n<p><span>In addition, we&#x2019;ve requested feedback from donors to EA Funds and from the community more generally through a Typeform survey. We ask the </span><a href=\"https://en.wikipedia.org/wiki/Net_Promoter\"><span>Net Promoter Score</span></a><span> (NPS) question in both surveys and received an NPS of +56 (which is generally considered excellent according to the NPS Wikipedia page). While we don&#x2019;t take NPS (or our sampling method) too seriously, it provides some quantitative data to corroborate our subjective impression.</span></p>\n<p>&#xA0;</p>\n<p><span><span><span>Some of the areas of concern we&apos;ve received so far include:</span></span><br></span></p>\n<ul>\n<li><span><span><a href=\"/ea/174/introducing_the_ea_funds/a33\">Concerns about principle-agent problems and transparency</a></span></span></li>\n<li><span><span><a href=\"/ea/174/introducing_the_ea_funds/a4h\">Concerns about unsuccessful attempts to do something similar in the past</a></span></span></li>\n<li><span><span><a href=\"/ea/174/introducing_the_ea_funds/a3p\">Concerns about the overall amount of money influences by Nick</a></span></span></li>\n<li><span><span><a href=\"/ea/174/introducing_the_ea_funds/a2m\">Concerns about centralization leading to less diversity in funding and less funding of new projects</a></span></span></li>\n<li><span><span><a href=\"/ea/17v/ea_funds_beta_launch/acj\">Concerns about creating dependency on Open Phil by charities</a></span></span>&#xA0;</li>\n</ul>\n<p><span>One additional area of concern is in donor&#x2019;s response to the following question:</span><span><br></span></p>\n<p>&#xA0;</p>\n<blockquote>\n<p><em><span>How likely is it that your donation to EA Funds will do more good in expectation than where you would have donated otherwise?</span></em></p>\n</blockquote>\n<p>&#xA0;</p>\n<p><span>Responses were on a scale from 0 (not at all likely) to 10 (extremely likely). We only collected 23 responses to this question, but the average score was 7.6 (compared to an average of 8.7 on the NPS above question). Using the NPS scoring system we would get a 0 on this question (same number of promoters as detractors). This could merely represent healthy skepticism of a new project or it could indicate that donors are enthusiastic about features other than the impact of donations to EA Funds. </span></p>\n<p>&#xA0;</p>\n<p><span>Our preference is that donors give wherever they have reason to believe their donation will do the most good. If EA Funds succeeds in getting donations but fails to first convince donors that it is the highest-impact donation option available, we would substantially reevaluate the project and how we communicate about it. We will continue to evaluate this as the project continues and as we gain more data.</span></p>\n<p>&#xA0;</p>\n<h2><span>Conclusion</span></h2>\n<p><span>The evidence so far has led us to conclude that EA Funds should continue after the three month trial period. We&#x2019;ve been impressed with the community response both in terms of feedback and donations and are enthusiastic about the potential to further improve the donation options available over time.</span></p>\n<p>&#xA0;</p>\n<h1><span>Allocations from Fund Managers</span></h1>\n<p><span>We&#x2019;re also excited to announce the first round of grant allocations from the fund managers. Details are provided below.</span></p>\n<p>&#xA0;</p>\n<h2><span>Global Health and Development Fund</span></h2>\n<p><span>By Elie Hassenfeld</span></p>\n<p>&#xA0;</p>\n<p><span>I&apos;m planning to allocate all of the funds in the Global Health and Development fund to the </span><a href=\"http://www.givewell.org/charities/against-malaria-foundation\"><span>Against Malaria Foundation</span></a><span>, consistent with </span><a href=\"http://blog.givewell.org/2017/04/03/allocation-of-discretionary-funds/\"><span>GiveWell&apos;s current recommendation to donors.</span></a></p>\n<p>&#xA0;</p>\n<p><span>AMF&apos;s ability to sign additional agreements to distribute malaria nets is currently hampered by insufficient funding.</span></p>\n<p>&#xA0;</p>\n<p><span>In addition:</span></p>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><span>GiveWell&apos;s </span><a href=\"http://www.givewell.org/research/incubation-grants\"><span>Incubation Grants</span></a><span> program has evaluated and recommended a handful of grants in the last few months. In each case, Good Ventures followed GiveWell&apos;s recommendation, so I continue to believe that GiveWell&apos;s Incubation Grants program is not hampered by insufficient funding.</span></p>\n</li>\n<li>\n<p><span>I don&apos;t currently know of any other global health and development opportunities that I believe are higher impact, in expectation, than AMF.</span></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<p><span>I don&apos;t anticipate either of the above facts changing in the next 6 months, so I&apos;m choosing to allocate all of the funds immediately.</span></p>\n<p>&#xA0;</p>\n<h2><span>Animal Welfare Fund</span></h2>\n<p><span>By Lewis Bollard</span></p>\n<p>&#xA0;</p>\n<p><span>I&#x2019;ve recommended disbursements for the first $180K donated to the fund. I&#x2019;ll likely recommend funding fewer groups in future, but have recommended initial grants to nine groups for a few reasons: </span></p>\n<p>&#xA0;</p>\n<ol>\n<li>\n<p><span>I want to signal to donors the sort of things I&#x2019;m likely to recommend via this fund, and signal groups that I think have (a) additional room for more funding by individual donors and (b) Open Phil can&#x2019;t fully fund because we already account for much of their budgets, e.g. The Humane League and Compassion in World Farming USA. </span></p>\n</li>\n<li>\n<p><span>I&#x2019;m recommending a few new approaches that I&#x2019;m not sure have significantly more room for funding than I&#x2019;m proposing, e.g. the Effective Altruism Foundation and The F&#xF3;rum Nacional. </span></p>\n</li>\n<li>\n<p><span>I&#x2019;m recommending some groups that I anticipate Open Phil may fill the funding of in future, so only want to fund the groups enough to expand in the meantime.</span></p>\n</li>\n<li>\n<p><span>I want to maintain some diversity within this fund so that donors can support a diversity of approaches.</span></p>\n</li>\n</ol>\n<p><strong>&#xA0;</strong></p>\n<p><span>The Humane League </span><span>($30K)</span></p>\n<p><span>Advocacy group. THL is one of two key campaigning groups responsible for the major recent US corporate wins for layer hens and broiler chickens. (The other is Mercy for Animals, which I&#x2019;m not supporting via this Fund because I&#x2019;m confident that major donors, including Open Phil, will fill its funding needs for now.) THL has also played a critical role in the global corporate campaign wins for layer hens, via the Open Wing Alliance, a grouping of 33 campaign groups that it organized. I&#x2019;ve been consistently impressed by THL&#x2019;s management, focus on staff and activist development, and wise use of funds across program areas. Open Phil already accounts for roughly half of THL&#x2019;s budget, so dependence concerns may constrain our ability to fill its funding needs in future. &#xA0;</span></p>\n<p>&#xA0;</p>\n<p><span>Animal Equality </span><span>($30K)</span></p>\n<p><span>Advocacy group. Animal Equality does grassroots activism, corporate campaigning, and undercover investigations across Europe, the Americas, and India. I&#x2019;ve been impressed by its constant updating based on evidence: first moving toward only farm animal welfare work, and later toward a focus on corporate campaigning. I also think that its co-founders Sharon Nunez and Jose Valle have a strong vision for building a grassroots movement globally. I think it has funding needs now that aren&#x2019;t likely to be immediately met.</span></p>\n<p>&#xA0;</p>\n<p><span>New Harvest </span><span>($30K)</span></p>\n<p><span>Clean meat research group. I&#x2019;m not sure what the odds are that we&#x2019;ll ever develop price-competitive clean or cultured meat. The evidence I&#x2019;ve seen has convinced me that we won&#x2019;t have it in the next five years, as some boosters claim. But I think it&#x2019;s plausible that we will in the next 20-50 years, and I think the odds of it ever being developed will depend on the funds invested in it now. I&#x2019;m also excited about the Good Food Institute&#x2019;s work in this space, but I think that big funders (including Open Phil) will fill GFI&#x2019;s funding needs in the medium term. I think New Harvest fulfills an important and complementary role, and has more room for more funding.</span></p>\n<p>&#xA0;</p>\n<p><span>The Effective Altruism Foundation </span><span>($30K)</span></p>\n<p><span>Research on the welfare of animals in natural environments. This grant will fund the research on the welfare of wild animals done by researchers Ozy&#xA0;Brennan and Persis Eskander, which internal changes at EAF have resulted in a loss of funding for. I&#x2019;ve been impressed with their recent research, which focuses on foundational questions like the best scientific methods for measuring the wellbeing of wild animals, and relatively non-controversial potential interventions, like more humane methods of pest control. I view this as an important and highly neglected cause, though I&#x2019;m unsure how tractable it will be and think more research is needed.</span></p>\n<p>&#xA0;</p>\n<p><span>The F&#xF3;rum Nacional de Prote&#xE7;&#xE3;o e Defesa Animal in Brazil </span><span>($20K)</span></p>\n<p><span>Advocacy group. The F&#xF3;rum Nacional is Brazil&#x2019;s largest animal protection network with 120+ affiliated NGOs (mainly companion animal groups). Advocates I trust credit the group with a key role in securing crate-free pledges from Brazil&#x2019;s three largest pork producers, and more recently cage-free pledges from Brazil&#x2019;s three largest mayo producers, amongst others. Open Phil already accounts for roughly half of the F&#xF3;rum Nacional&#x2019;s budget, so dependence concerns may constrain our ability to fill its funding needs in future, and I&#x2019;m less optimistic that other donors will step in than I am for THL or CIWF USA given Brazil&#x2019;s challenging fundraising environment.</span></p>\n<p>&#xA0;</p>\n<p><span>Compassion in World Farming USA </span><span>($10K)</span></p>\n<p><span>Advocacy group. CIWF USA is one of two corporate advocacy groups responsible for the major recent US corporate wins for layer hens and broiler chickens. (The other is the Humane Society of the US Farm Animal Protection campaign, which is harder to support via this fund because of fungibility concerns.) It&#x2019;s now focused almost exclusively on winning further corporate welfare reforms for broiler chickens. Open Phil already accounts for roughly half of CIWF USA&#x2019;s budget, so dependence concerns may constrain our ability to fill its funding needs in future.</span></p>\n<p>&#xA0;</p>\n<p><span>The Albert Schweitzer Foundation in Germany </span><span>($10K)</span></p>\n<p><span>Advocacy group. This group appears to have been instrumental in securing cage-free and other corporate pledges in Germany, as well as in advancing some policy reforms and institutional meat reduction efforts. It currently has funding needs which may be filled in the medium term.</span></p>\n<p>&#xA0;</p>\n<p><span>Animal Charity Evaluators </span><span>($10K)</span></p>\n<p><span>Charity evaluator. I like the work that ACE does to build a more effective farm animal movement through research, charity recommendations, and outreach to donors, researchers, and advocates. When I recommended this initial grant, ACE had significant room for more funding. I&#x2019;m now more confident that funding gap will be filled by large funders, so it&#x2019;s unlikely that I&#x2019;ll direct more funds to ACE this year.</span></p>\n<p>&#xA0;</p>\n<p><span>Otwarte Klatki in Poland </span><span>($10K)</span></p>\n<p><span>Advocacy group. This young grassroots group appears to have helped achieve significant corporate reforms in Poland with a small budget and in a tough political environment. It currently has funding needs, though they may be filled in the medium term.</span></p>\n<p>&#xA0;</p>\n<h2><span>Long-Term Future Fund</span></h2>\n<p><span>By Nick Beckstead</span></p>\n<p>&#xA0;</p>\n<p><span>The Long-Term Future Fund made one grant of $14,838.02 to the </span><a href=\"http://existence.org/\"><span>Berkeley Existential Risk Initiative</span></a><span> (BERI).</span></p>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><span>How I got the idea: Andrew Critch, who created BERI, requested $50,000.</span></p>\n</li>\n<li>\n<p><span>What it is: It is a new initiative providing various forms of support to researchers working on existential risk issues (administrative, expert consultations, technical support). It works as a non-profit entity, independent of any university, so that it can help multiple organizations and to operate more swiftly than would be possible within a university context. For more information, see their website.</span></p>\n</li>\n<li>\n<p><span>Why I provided the funds: Key inputs to my decision include:</span></p>\n</li>\n<ul>\n<li>\n<p><span>The basic idea makes sense to me. I believe that this vehicle could provide swifter, more agile support to researchers in this space and I think that could be helpful.</span></p>\n</li>\n<li>\n<p><span>I know Critch and believe he can make this happen.</span></p>\n</li>\n<li>\n<p><span>I believe I can check in on this a year or two from now and get a sense of how helpful it was. Supporting people to try out reasonable ideas when that seems true is appealing to me.</span></p>\n</li>\n<li>\n<p><span>I see myself as a natural first funder to ask for new endeavors like this, and believe others who would support this would make relatively wise choices with their donations. I therefore did not check much whether someone else could have or would have funded it.</span></p>\n</li>\n<li>\n<p><span>This seemed competitive with available alternatives.</span></p>\n</li>\n</ul>\n<li>\n<p><span>I did not provide the full $50,000 from the Long-Term Future Fund because I didn&apos;t have enough funding yet. I provided all the funding I had at the time. The remainder of the funding was provided by the EA Giving Group and some funds held in a personal DAF. (This illustrates complex issues of fungibility that I plan to discuss at a later date.)</span></p>\n</li>\n<li></li>\n</ul>\n<h2><span>Effective Altruism Community Fund</span></h2>\n<p><em><span>(At the time of writing the EA community fund had not made any grants)</span></em></p>\n<p>&#xA0;</p>\n<h1><span>Mistakes and Updates</span></h1>\n<p><span>Since the launch of EA Funds, we&#x2019;ve made several mistakes which have led to several useful updates. We outline these below.</span></p>\n<p>&#xA0;</p>\n<h2><span>Understatement of EA Funds Risks</span></h2>\n<p><span>How we fell short:</span><span> In our launch post (available </span><a href=\"/ea/17v/ea_funds_beta_launch/\"><span>here</span></a><span> and </span><a href=\"https://app.effectivealtruism.org/funds/why\"><span>here</span></a><span>) we argue that donations to EA Funds are likely to be at least as good as Open Phil&#x2019;s last dollar and that Open Phil&#x2019;s last dollar may be higher value than the lowest-cost alternatively, namely, donating to GiveWell-recommended charities.</span></p>\n<p>&#xA0;</p>\n<p><span>However, this argument did not sufficiently communicate that Open Phil is likely to donate its last dollar many decades in the future which adds a good deal of extra risk that does not exist for an option like donating to GiveWell-recommended charities.</span></p>\n<p>&#xA0;</p>\n<p><span>How we&#x2019;re improving: </span><span>We&#x2019;ve added some additional paragraphs about this issue to the &#x201C;</span><a href=\"https://app.effectivealtruism.org/funds/why\"><span>Why donate to Effective Altruism Funds</span></a><span>&#x201D; page. We also added an additional paragraph to the &#x201C;Why might you choose not to donate to this fund?&#x201D; page for the </span><a href=\"https://app.effectivealtruism.org/funds/animal-welfare\"><span>Animal Welfare</span></a><span>, </span><a href=\"https://app.effectivealtruism.org/funds/far-future\"><span>Long-Term Future</span></a><span>, and </span><a href=\"https://app.effectivealtruism.org/funds/ea-community\"><span>EA Community</span></a><span> funds which address the need to trust Open Phil in making donations to EA Funds. We added a similar, but much shorter paragraph addressing the need to trust GiveWell to the </span><a href=\"https://app.effectivealtruism.org/funds/global-development\"><span>Global Health and Development fund page</span></a><span>.</span></p>\n<p>&#xA0;</p>\n<h2><span>Poor content about EA Funds on the Giving What We Can website</span></h2>\n<p><span>How we fell short:</span><span> Around a month after launch we added some information and recommendations for EA Funds to the Giving What We Can website (</span><a href=\"https://www.givingwhatwecan.org/donate/effective-altruism-funds/\"><span>here</span></a><span>, </span><a href=\"https://www.givingwhatwecan.org/top-charities/\"><span>here</span></a><span>, and </span><a href=\"https://www.givingwhatwecan.org/donate/\"><span>here</span></a><span>). This information endorsed EA Funds without linking to the arguments in favor of it and did not sufficiently highlight our belief that not all donors should give to EA Funds.</span></p>\n<p>&#xA0;</p>\n<p><span>In addition, this recommendation was at odds with our public statement in our </span><a href=\"/ea/17v/ea_funds_beta_launch/\"><span>launch post</span></a><span> that EA Funds was in a three-month test period. Some users were confused as to why we would recommend a project which we were still testing.</span></p>\n<p>&#xA0;</p>\n<p><span>How we&#x2019;re improving</span><span>: We added a link to the &#x201C;Why donate to EA Funds&#x201D; page (or reproduced that content) on all three GWWC pages. We also added a sentence explaining that we do not think EA Funds is likely to be the highest impact for option all donors.</span></p>\n<p>&#xA0;</p>\n<p><span>We&#x2019;re also releasing this update post to explain how we&#x2019;ve updated and why we feel comfortable recommending EA Funds to a wider pool of donors.</span></p>\n<p>&#xA0;</p>\n<h2><span>Potential issues and areas of uncertainty</span></h2>\n<ul>\n<li>\n<p><span>We followed the YC mantra of &#x201C;launch when you&#x2019;re still slightly embarrassed&#x201D; in deciding how quickly to launch EA Funds. This allowed us to move quickly and take EA Funds from concept to launch in less than a month, but also led us to launch a product with some software and content bugs. Since CEA has a more established brand than most startups and since we&#x2019;re dealing with large amounts of money, it might have been appropriate to spend more time refining the product before launch.</span></p>\n</li>\n<li>\n<p><span>We have struggled to find a balance between the desire to be careful and thorough in describing the reasons in favor of donating to EA Funds on the one hand and the desire to be user-friendly and appealing to newer donors on the other hand. Our </span><a href=\"https://app.effectivealtruism.org/funds\"><span>current homepage</span></a><span> likely leans too far in favor of being user-friendly and sparse on argumentation, but our </span><a href=\"/ea/17v/ea_funds_beta_launch/\"><span>launch post</span></a><span> likely leaned too far in the direction of requiring lots of background context to understand. We&#x2019;ll continue to work on striking the appropriate balance as EA Funds evolves including A/B testing some different options to get more information on what&#x2019;s appropriate and useful.</span></p>\n</li>\n<li>\n<p><span>The EA Funds user interface unintentionally nudges users in favor of splitting their donation between the available causes because it shows you all the options simultaneously and asks you to choose your allocation between then. It is an open question if donors should split between plausible options or donate entirely to the option they think is best in expectation. We&#x2019;re currently evaluating options for how to either help donors think through the split versus no-split decision or to make the user interface less biased in favor of donation splitting.</span></p>\n</li>\n</ul>\n<h1>&#xA0;</h1>\n<h1><span>Future plans</span></h1>\n<p><span>Below we highlight some of the near-term priorities for EA Funds. </span></p>\n<p>&#xA0;</p>\n<h2><span>Is the growth of EA Funds dependent on the growth of EA?</span></h2>\n<p><span>The success of EA Funds so far is primarily attributable to the size of the existing EA community. One important question is whether the growth of EA Funds will be dependent on the growth of the EA community or whether EA Funds can grow independently, and perhaps faster than the growth of EA. </span></p>\n<p>&#xA0;</p>\n<p><span>If EA Funds can grow independent of EA, then it likely makes sense to spend a good deal of staff time and money working directly on improving the project and getting more money moving through the platform. If EA Funds primarily grows as EA grows, then it makes sense to spend staff time and money working on growing EA while making sure that the EA community knows about EA Funds.</span></p>\n<p>&#xA0;</p>\n<p><span>We&#x2019;re looking at three general options for growing EA Funds independently of growing the EA community: online marketing, engaging with high net worth donors and partnership development. We&#x2019;ll be looking for low-cost ways to test tactics in each of these domains over the coming months while the organization&#x2019;s main focus will be the EA Community. If none of these options look promising, then we&#x2019;ll likely focus on growing the EA community while maintaining EA Funds as a donation option for EAs.</span></p>\n<p>&#xA0;</p>\n<h2><span>Adding new funds and new fund managers</span></h2>\n<p><span>In our launch post we said:</span></p>\n<p>&#xA0;</p>\n<blockquote>\n<p><span>If we decide to proceed with the EA Funds project after the three month trial, our aim would be to have 50% or less of the Fund Managers be Open Phil Program Officers (although they may manage more than 50% of the money donated).</span></p>\n</blockquote>\n<p><span>This continues to be an important goal for us. Internally we&#x2019;ve discussed some ideas for what funds or fund managers we might add to accomplish this goal, but we haven&#x2019;t settled on any firm plans. We plan to allocate more time to accomplishing this goal over the summer. </span></p>\n<p><span>If you have ideas for funds or fund managers we might add, please fill out </span><a href=\"https://cea-core.typeform.com/to/Mwxftc\"><span>this form</span></a><span> and/or email me at </span><a href=\"mailto:kerry@effectivealtruism.org\"><span>kerry@effectivealtruism.org</span></a><span>.</span></p>\n<p>&#xA0;</p>\n<h2><span>The EA Funds infrastructure as a platform</span></h2>\n<p><span>Behind the scenes, donations to EA Funds go to CEA until the fund manager makes a grant recommendation at which point CEA donates the money to the recipient organization. </span></p>\n<p>&#xA0;</p>\n<p><span>We choose this system over other options like using a separate organization to receive the money, using charity platforms like CauseVox, or setting up an independent donor-advised fund for several reasons. These include less administrative costs for us, more control over the user experience, lower fees with the possibility of negotiating even lower fees in the future, and tax deductibility in the US and UK through the same website and platform. </span></p>\n<p>&#xA0;</p>\n<p><span>This system is designed such that it can scale beyond just collecting donations to EA Funds. For example, we could process donations to individual charities, we could help coordinate donor lotteries, we could process bequests, we could process birthday and holiday fundraisers, and more. In the short-term, we are replacing the Giving What We Can trust with EA Funds because CEA can make the same grants with fewer restrictions (more on this in our </span><a href=\"https://www.centreforeffectivealtruism.org/blog/cea-update-march-2017/#1-test-effective-altruism-funds-as-a-concept\"><span>March update</span></a><span>) and use EA Funds to process donations to individual charities for members. We&#x2019;ll be looking for other ways to use this infrastructure to benefit the EA community.</span></p></body></html>", "user": {"username": "Kerry_Vaughan"}}, {"_id": "pfEpu3gMG5bRMyfee", "title": "Intro to caring about AI alignment as an EA cause", "postedAt": "2017-04-14T00:42:16.065Z", "htmlBody": "<html><body><p>I recently gave a talk at Google on the problem of aligning smarter-than-human AI with operators&apos; goals. Below is a modified transcript, in case people here are interested. I think it provides a decent introduction to &quot;AI risk&quot; as a cause-area worthy of attention from effective altruists. I post it here in case anyone wants to link it to curious EAs. Note that this is a cross-post from the MIRI blog, and that you can also watch the talk <a href=\"https://youtu.be/dY3zDvoLoao\">on YouTube</a>. The talk was inspired by &#x201C;<a href=\"/https:/intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/\">AI Alignment: Why It&apos;s Hard, and Where to Start</a>.&#x201D; The slides of the talk are available <a href=\"https://intelligence.org/files/fundamental-difficulties-transitions2.pdf\">here</a>.</p>\n<p><strong>Outline:</strong></p>\n<blockquote>\n<p>1. <a href=\"#1\">Overview</a></p>\n<p>2. <a href=\"#2\">Simple Bright Ideas Going Wrong</a></p>\n<p>2.1. <a href=\"#task\">Task: Fill a Cauldron</a></p>\n<p>2.2. <a href=\"#subproblem\">Subproblem: Suspend Buttons</a></p>\n<p>3. <a href=\"#3\">The Big Picture</a></p>\n<p>3.1. <a href=\"#priorities\">Alignment Priorities</a></p>\n<p>3.2. <a href=\"#propositions\">Four Key Propositions</a></p>\n<p>4. <a href=\"#4\">Fundamental Difficulties</a></p>\n</blockquote>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<h2>Overview</h2>\n<p>I&apos;m the executive director of the Machine Intelligence Research Institute. Very roughly speaking, we&apos;re a group that&apos;s thinking in the long term about artificial intelligence and working to make sure that by the time we have advanced AI systems, we also know how to point them in useful directions.</p>\n<p>Across history, science and technology have been the largest drivers of change in human and animal welfare, for better and for worse. If we can automate scientific and technological innovation, that has the potential to change the world on a scale not seen since the Industrial Revolution. When I talk about &#x201C;advanced AI,&#x201D; it&apos;s this potential for automating innovation that I have in mind.</p>\n<p>AI systems that exceed humans in this capacity aren&apos;t coming next year, but many smart people are working on it, and I&apos;m not one to bet against human ingenuity. I think it&apos;s likely that we&apos;ll be able to build something like an automated scientist in our lifetimes, which suggests that this is something we need to take seriously.</p>\n<p>When people talk about the social implications of <a href=\"https://intelligence.org/2013/06/19/what-is-intelligence-2/\">general AI</a>, they often fall prey to anthropomorphism. They conflate artificial intelligence with artificial consciousness, or assume that if AI systems are &#x201C;intelligent,&#x201D; they must be intelligent in the same way a human is intelligent. A lot of journalists express a concern that when AI systems pass a certain capability level, they&apos;ll spontaneously develop &#x201C;natural&#x201D; desires like a human hunger for power; or they&apos;ll reflect on their programmed goals, find them foolish, and &#x201C;rebel,&#x201D; refusing to obey their programmed instructions.</p>\n<p>These are misplaced concerns. The human brain is a complicated product of natural selection. We shouldn&apos;t expect machines that exceed human performance in scientific innovation to closely resemble humans, any more than early rockets, airplanes, or hot air balloons closely resembled birds. <sup><a href=\"#footnote1\">1</a></sup></p>\n<p>The notion of AI systems &#x201C;breaking free&#x201D; of the shackles of their source code or spontaneously developing human-like desires is just confused. The AI system is its source code, and its actions will only ever follow from the execution of the instructions that we initiate. The CPU just keeps on executing the next instruction in the program register. We could write a program that manipulates its own code, including coded objectives. Even then, though, the manipulations that it makes are made as a result of executing the original code that we wrote; they do not stem from some kind of ghost in the machine.</p>\n<p>The serious question with smarter-than-human AI is how we can ensure that the objectives we&apos;ve specified are correct, and how we can minimize costly accidents and unintended consequences in cases of misspecification. As Stuart Russell (co-author of <em>Artificial Intelligence: A Modern Approach</em>) <a href=\"https://www.edge.org/conversation/the-myth-of-ai#26015\">puts it</a>:</p>\n<blockquote>\n<p>&#x201C;The primary concern is not spooky emergent consciousness but simply the ability to make <em>high-quality decisions</em>.&#x201D;</p>\n<p>Here, quality refers to the expected outcome utility of actions taken, where the utility function is, presumably, specified by the human designer. Now we have a problem:</p>\n<ol>\n<li>The utility function may not be perfectly aligned with the values of the human race, which are (at best) very difficult to pin down.</li>\n<li>Any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources &#x2013; not for their own sake, but to succeed in its assigned task.</li>\n</ol>\n<p>A system that is optimizing a function of <em>n</em> variables, where the objective depends on a subset of size <em>k</em>&lt;<em>n</em>, will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable.</p>\n</blockquote>\n<p>These kinds of concerns deserve a lot more attention than the more anthropomorphic risks that are generally depicted in Hollywood blockbusters.</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<h2>Simple Bright Ideas Going Wrong</h2>\n<h3>Task: Fill a Cauldron</h3>\n<p>Many people, when they start talking about concerns with smarter-than-human AI, will throw up a picture of the Terminator. I was once quoted in a news article making fun of people who put up Terminator pictures in all their articles about AI, next to a Terminator picture. I learned something about the media that day.</p>\n<p>I think this is a much better picture:</p>\n<p>&#xA0;</p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/vlcsnap-2016-05-04-18h44m30s933-300x225.png\" alt=\"Mickey&apos;s cauldron-filling program\"></p>\n<p>&#xA0;</p>\n<p>This is Mickey Mouse in the movie <em>Fantasia</em>, who has very cleverly enchanted a broom to fill a cauldron on his behalf.</p>\n<p>How might Mickey do this? We can imagine that Mickey writes a computer program and has the broom execute the program. Mickey starts by writing down a scoring function or objective function:</p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/Equation1.png\" alt=\"scoring function\"></p>\n<p>Given some set &#x1D434; of available actions, Mickey then writes a program that can take one of these actions &#x1D44E; as input and calculate how high the score is expected to be if the broom takes that action. Then Mickey can write a function that spends some time looking through actions and predicting which ones lead to high scores, and outputs an action that leads to a relatively high score:</p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/Equation2.png\" alt=\"sorta argmax\"></p>\n<p>The reason this is &#x201C;sorta-argmax&#x201D; is that there may not be time to evaluate every action in &#x1D434;. For realistic action sets, agents should only need to find actions that make the scoring function as large as they can given resource constraints, even if this isn&apos;t the maximal action.</p>\n<p>This program may look simple, but of course, the devil&apos;s in the details: writing an algorithm that does accurate prediction and smart search through action space is basically the whole problem of AI. Conceptually, however, it&apos;s pretty simple: We can describe in broad strokes the kinds of operations the broom must carry out, and their plausible consequences at different performance levels. When Mickey runs this program, everything goes smoothly at first. <a href=\"https://www.youtube.com/watch?v%3DUEYy3osi8Gs%26t%3D3m25s\">Then</a>:</p>\n<p>&#xA0;</p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/vlcsnap-2016-05-04-19h48m12s031.png\" alt=\"cauldron-filling AI\"></p>\n<p>&#xA0;</p>\n<p>I claim that as fictional depictions of AI go, this is pretty realistic.</p>\n<p>Why would we expect a generally intelligent system executing the above program to start overflowing the cauldron, or otherwise to go to extreme lengths to ensure the cauldron is full?</p>\n<p>The first difficulty is that the objective function that Mickey gave his broom left out <a href=\"https://intelligence.org/files/ComplexValues.pdf\">a bunch of other terms</a> Mickey cares about:</p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/Equation3.png\" alt=\"\"></p>\n<p>The second difficulty is that Mickey programmed the broom to make the expectation of its score as large as it could. &#x201C;Just fill one cauldron with water&#x201D; looks like a modest, limited-scope goal, but when we translate this goal into a probabilistic context, we find that optimizing it means driving up the probability of success to absurd heights. If the broom assigns a 99.9% probability to &#x201C;the cauldron is full,&#x201D; and it has extra resources lying around, then it will always try to find ways to use those resources to drive the probability even a little bit higher.</p>\n<p>Contrast this with the limited &#x201C;<a href=\"/=&quot;https:/arbital.com/p/task_goal/&quot;\">task-like</a>&#x201D; goal we presumably had in mind. We wanted the cauldron full, but in some intuitive sense we wanted the system to &#x201C;not try too hard&#x201D; even if it has lots of available cognitive and physical resources to devote to the problem. We wanted it to exercise creativity and resourcefulness within some intuitive limits, but we didn&apos;t want it to pursue &#x201C;absurd&#x201D; strategies, especially ones with large unanticipated consequences. <sup><a href=\"#footnote2\">2</a></sup></p>\n<p>In this example, the original objective function looked pretty task-like. It was bounded and quite simple. There was no way to get ever-larger amounts of utility. It&apos;s not like the system got one point for every bucket of water it poured in &#x2014; then there would clearly be an incentive to overfill the cauldron. The problem was hidden in the fact that we&apos;re maximizing <em>expected</em> utility. This makes the goal open-ended, meaning that even small errors in the system&apos;s objective function will <a href=\"https://arbital.com/p/goodharts_curse/\">blow up</a>.</p>\n<p>There are a number of different ways that a goal that looks task-like can turn out to be open-ended. Another example: a larger system that has an overarching task-like goal may have <a href=\"https://www.gwern.net/Tool%2520AI\">subprocesses</a> that are themselves trying to maximize a variety of different objective functions, such as optimizing the system&apos;s memory usage. If you don&apos;t understand your system well enough to track whether any of its subprocesses are themselves acting like resourceful open-ended optimizers, then <a href=\"https://agentfoundations.org/item?id%3D1220\">it may not matter how safe the top-level objective is</a>.</p>\n<p>So the broom keeps grabbing more pails of water &#x2014; say, on the off chance that the cauldron has a leak in it, or that &#x201C;fullness&#x201D; requires the water to be slightly above the level of the brim. And, of course, at no point does the broom &#x201C;rebel against&#x201D; Mickey&apos;s code. If anything, the broom pursued the objectives it was programmed with <em>too</em> effectively.</p>\n<h3>Subproblem: Suspend Buttons</h3>\n<p>A common response to this problem is: &#x201C;OK, there may be some unintended consequences of the objective function, but we can always pull the plug, right?&#x201D;</p>\n<p>Mickey <a href=\"http://www.youtube.com/watch?v%3DUEYy3osi8Gs%26t%3D4m0s\">tries this</a>, and it doesn&apos;t work:</p>\n<p>&#xA0;</p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/vlcsnap-2016-05-04-19h21m04s349.png\" alt=\"Mickey tries to stop the program\"></p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/vlcsnap-2016-05-04-19h22m09s178.png\" alt=\"Brooms replicate\"></p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/vlcsnap-2016-05-04-19h53m09s315.png\" alt=\"Things go very badly for Mickey\"></p>\n<p>&#xA0;</p>\n<p>And I claim that this is realistic too, for systems that are sufficiently good at modeling their environment. If the system is trying to drive up the expectation of its scoring function and is smart enough to recognize that its being shut down will result in lower-scoring outcomes, then the system&apos;s incentive is to subvert shutdown attempts. The more capable the system is, the likelier it is to find creative ways to achieve that subgoal &#x2014; e.g., by copying itself to the Internet, or by tricking the programmers into thinking it&apos;s safer.</p>\n<p>This isn&apos;t to say that it&apos;s impossible to shut down sufficiently capable AI systems; it&apos;s just that we need to put in the extra work of deliberately designing systems to avoid searching for ways to avoid being shut down. If you&apos;ve written code that would try to resist attempts to shut it down, then you shouldn&apos;t run the code even if you&apos;re convinced it would fail &#x2014; there are better ways to spend those compute cycles than on adversarial policy searches.</p>\n<p>Relevant safety measures that don&apos;t assume we can always outthink and outmaneuver the system will need to modify the system&apos;s objective function in some relevant way. We need to figure out how to formally specify objective functions that don&apos;t automatically place the AI system into an adversarial context with the operators; or we need to figure out some way to have the system achieve goals without optimizing some objective function in the traditional sense. E.g., we can look for ways to limit the system&apos;s capabilities <a href=\"https://arbital.com/p/direct_limit_oppose/\">to a small set of &#x201C;whitelisted&#x201D; domains</a> that don&apos;t include thinking about whether the system will be shut down.</p>\n<p>One obvious solution is to try changing the score function to make reference to a suspend button &#x1D401;:</p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/Equation4.png\" alt=\"Suspend button\"></p>\n<p>This says that when the button is on, the system&apos;s goal changes from &#x201C;fill the cauldron&#x201D; to &#x201C;be suspended.&#x201D;</p>\n<p>It&apos;s not clear how we could actually specify this goal &#x2014; if there are any loopholes in the way we&apos;ve specified &#x201C;suspension,&#x201D; then the system might try to exploit them to hedge its bets. E.g., it could fulfill the <em>technical</em> definition of &#x201C;suspend operations&#x201D; while tasking a child agent it built or subprocess it started with filling the cauldron. This would allow is to achieve both &#x201C;cauldron full&#x201D; and &#x201C;robot suspended&#x201D; at once, just in case it&apos;s wrong about whether the button has been pushed.</p>\n<p>Let&apos;s assume, however, that we&apos;ve found an airtight way to technically specify suspension. The problem we run into now is that we&apos;ve given the system an incentive to care a lot about whether the button is pushed. If it has any way to influence its operators&apos; decision to push the button, or any other way to exercise more control, then it will jump on the opportunity and try to ensure the button is pushed, or ensure that it&apos;s not pushed &#x2014; depending on whether it looks easier to suspend or to fill the cauldron.</p>\n<p>And, surprisingly, the state of the art on this problem isn&apos;t much more advanced than that. There&apos;s a paper co-authored by me, Fallenstein, Yudkowsky, and Armstrong (&#x201C;<a href=\"/=&quot;http:/aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10124/10136&quot;\">Corrigibility</a>&#x201C;), and a related result by Orseau and Armstrong (&#x201C;<a href=\"/=&quot;https:/intelligence.org/2016/06/01/new-paper-safely-interruptible-agents/&quot;\">Safely Interruptible Agents</a>&#x201C;), but not a lot more.</p>\n<p>What we want is a way to combine two objective functions &#x2014; a default function for normal operation, and a suspend function for when we want to suspend the system to disk.</p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/Equation5.png\" alt=\"combine two functions\"></p>\n<p>We want our method for combining the functions to satisfy three conditions: an operator should be able to switch between the functions (say, by pushing a button); the system shouldn&apos;t have any incentives to control which function is active; and if it&apos;s plausible that the system&apos;s normal operations could inadvertently compromise our ability to switch between the functions, then the system should be incentivized to keep that from happening.</p>\n<p>So far, we haven&apos;t found any way to achieve all three goals at once. As an example, we can try to remove any incentive on the system&apos;s part to control whether its suspend button is pushed by giving the system a switching objective function that always assigns the same expected utility to the button being on or off:</p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/Equation6.png\" alt=\"Expected utility on or off\"></p>\n<p>Unfortunately, this approach makes it impossible to satisfy the third condition. It turns out that the above objective function is equivalent to setting the probability of the button being pressed to 0. The system won&apos;t <em>deliberately</em> tamper with the button, but it will happily approve updates that delete any reference to the button as dead code, or build subagents that ignore the button.</p>\n<p>Meeting these intuitively simple constraints turns out to be a nontrivial problem. This is a pattern that runs through a lot of questions in this space: traditional tools and concepts run into immediate safety problems that don&apos;t turn up in conventional capabilities research.</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<h2>The Big Picture</h2>\n<h3>Alignment Priorities</h3>\n<p>Let&apos;s take a step back and talk about what&apos;s needed overall in order to align highly capable AI systems with our interests.</p>\n<p>Here&apos;s a dramatically simplified pipeline: You have some humans who come up with some task or goal or preference set that serves as their intended value function &#x1D61D;. Since our values are complicated and context-sensitive, in practice we&apos;ll need to build systems to learn our values over time, rather than coding them by hand. <sup><a href=\"#footnote3\">3</a></sup> We&apos;ll call the goal the AI system ends up with (which may or may not be identical to &#x1D61D;) &#x1D5E8;.</p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/alignment-priorities.png\" alt=\"alignment priorities\"></p>\n<p>When the press covers this topic, they often focus on one of two problems: &#x201C;What if the wrong group of humans develops smarter-than-human AI first?&#x201D;, and &#x201C;What if AI&apos;s natural desires cause &#x1D5E8; to diverge from &#x1D61D;?&#x201D;</p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/humans-nd.png\" alt=\"media concerns\"></p>\n<p>In my view, the &#x201C;wrong humans&#x201D; issue shouldn&apos;t be the thing we focus on until we have reason to think we could get good outcomes with the <em>right</em> group of humans. We&apos;re very much in a situation where well-intentioned people couldn&apos;t leverage a general AI system to do good things even if they tried. As a simple example, if you handed me a box that was an extraordinarily powerful function optimizer &#x2014; I could put in a description of any mathematical function, and it would give me an input that makes the output extremely large &#x2014; then I do know how to use the box to produce a random catastrophe, but I don&apos;t actually know how I could use that box in the real world to have a good impact. <sup><a href=\"#footnote4\">4</a></sup></p>\n<p>There&apos;s a lot we don&apos;t understand about AI capabilities, but we&apos;re in a position where we at least have a general sense of what progress looks like. We have a number of good frameworks, techniques, and metrics, and we&apos;ve put a great deal of thought and effort into successfully chipping away at the problem from various angles. At the same time, we have a very weak grasp on the problem of how to align highly capable systems with any particular goal. We can list out some intuitive desiderata, but the field hasn&apos;t really developed its first formal frameworks, techniques, or metrics.</p>\n<p>I believe that there&apos;s a lot of low-hanging fruit in this area, and also that a fair amount of the work does need to be done early (e.g., to help inform capabilities research directions &#x2014; some directions may produce systems that are much easier to align than others). If we don&apos;t solve these problems, developers with arbitrarily good or bad intentions will end up producing equally bad outcomes. From an academic or scientific standpoint, our first objective in that kind of situation should be to remedy this state of affairs and at least make good outcomes technologically possible.</p>\n<p>Many people quickly recognize that &#x201C;natural desires&#x201D; are a fiction, but infer from this that we instead need to focus on the other issues the media tends to emphasize &#x2014; &#x201C;What if bad actors get their hands on smarter-than-human AI?&#x201D;, &#x201C;How will this kind of AI impact employment and the distribution of wealth?&#x201D;, etc. These are important questions, but they&apos;ll only end up actually being relevant if we figure out how to bring general AI systems up to a minimum level of reliability and safety.</p>\n<p>Another common thread is &#x201C;Why not just tell the AI system to (insert intuitive moral precept here)?&#x201D; On this way of thinking about the problem, often (perhaps unfairly) associated with Isaac Asimov&apos;s writing, ensuring a positive impact from AI systems is largely about coming up with natural-language instructions that are vague enough to subsume a lot of human ethical reasoning:</p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/intended-values.png\" alt=\"intended values\"></p>\n<p>In contrast, precision is a virtue in real-world safety-critical software systems. Driving down accident risk requires that we begin with limited-scope goals rather than trying to &#x201C;solve&#x201D; all of morality at the outset. <sup><a href=\"#footnote5\">5</a></sup></p>\n<p>My view is that the critical work is mostly in designing an effective value learning process, and in ensuring that the sorta-argmax process is correctly hooked up to the resultant objective function &#x1D5E8;:</p>\n<p><img src=\"https://intelligence.org/wp-content/uploads/2017/04/vl-argmax.png\" alt=\"MIRI&apos;s concerns\"></p>\n<p>The better your value learning framework is, the less explicit and precise you need to be in pinpointing your value function &#x1D61D;, and the more you can offload the problem of figuring out what you want to the AI system itself. Value learning, however, raises <a href=\"https://intelligence.org/files/ValueLearningProblem.pdf\">a number of basic difficulties</a> that don&apos;t crop up in ordinary machine learning tasks.</p>\n<p>Classic capabilities research is concentrated in the sorta-argmax and Expectation parts of the diagram, but sorta-argmax also contains what I currently view as the most neglected, tractable, and important safety problems. The easiest way to see why &#x201C;hooking up the value learning process correctly to the system&apos;s capabilities&#x201D; is likely to be an important and difficult challenge in its own right is to consider the case of our own biological history.</p>\n<p>Natural selection is the only &#x201C;engineering&#x201D; process we know of that has ever led to a generally intelligent artifact: the human brain. Since natural selection relies on a fairly unintelligent hill-climbing approach, one lesson we can take away from this is that it&apos;s possible to reach general intelligence with a hill-climbing approach and enough brute force &#x2014; though we can presumably do better with our human creativity and foresight.</p>\n<p>Another key take-away is that natural selection was maximally strict about <em>only</em> optimizing brains for a single very simple goal: genetic fitness. In spite of this, the internal objectives that humans represent as their goals are not genetic fitness. We have innumerable goals &#x2014; love, justice, beauty, mercy, fun, esteem, good food, good health, &#x2026; &#x2014; that correlated with good survival and reproduction strategies in the ancestral savanna. However, we ended up valuing these correlates directly, rather than valuing propagation of our genes as an end in itself &#x2014; as demonstrated every time we employ birth control.</p>\n<p>This is a case where the external optimization pressure on an artifact resulted in a general intelligence with internal objectives that didn&apos;t match the external selection pressure. And just as this caused humans&apos; actions to diverge from natural selection&apos;s pseudo-goal once we gained new capabilities, we can expect AI systems&apos; actions to diverge from humans&apos; if we treat their inner workings as black boxes.</p>\n<p>If we apply gradient descent to a black box, trying to get it to be very good at maximizing some objective, then with enough ingenuity and patience, we may be able to produce a powerful optimization process of some kind. <sup><a href=\"#footnote6\">6</a></sup> By default, we should expect an artifact like that to have a goal &#x1D5E8; that strongly correlates with our objective &#x1D61D; in the training environment, but sharply diverges from &#x1D61D; <a href=\"https://arbital.com/p/context_disaster/\">in some new environments or when a much wider option set becomes available</a>.</p>\n<p>On my view, the most important part of the alignment problem is ensuring that the value learning framework and overall system design we implement allow us to crack open the hood and confirm when the internal targets the system is optimizing for match (or don&apos;t match) the targets we&apos;re externally selecting through the learning process. <sup><a href=\"#footnote7\">7</a></sup></p>\n<p>We expect this to be technically difficult, and if we can&apos;t get it right, then it doesn&apos;t matter who&apos;s standing closest to the AI system when it&apos;s developed. Good intentions aren&apos;t sneezed into computer programs by kind-hearted programmers, and coming up with plausible goals for advanced AI systems doesn&apos;t help if we can&apos;t align the system&apos;s cognitive labor with a given goal.</p>\n<h3>Four Key Propositions</h3>\n<p>Taking another step back: I&apos;ve given some examples of open problems in this area (suspend buttons, value learning, limited task-based AI, etc.), and I&apos;ve outlined what I consider to be the major problem categories. But my initial characterization of why I consider this an important area &#x2014; &#x201C;AI could automate general-purpose scientific reasoning, and general-purpose scientific reasoning is a big deal&#x201D; &#x2014; was fairly vague. What are the core reasons to prioritize this work?</p>\n<p>First, <a href=\"https://arbital.com/p/orthogonality/\">goals and capabilities are orthogonal</a>. That is, knowing an AI system&apos;s objective function doesn&apos;t tell you how good it is at optimizing that function, and knowing that something is a powerful optimizer doesn&apos;t tell you what it&apos;s optimizing.</p>\n<p>I think most programmers intuitively understand this. Some people will insist that when a machine tasked with filling a cauldron gets smart enough, it will abandon cauldron-filling as a goal unworthy of its intelligence. From a computer science perspective, the obvious response is that you could go out of your way to build a system that exhibits that conditional behavior, but you could also build a system that doesn&apos;t exhibit that conditional behavior. It can just keeps searching for actions that have a higher score on the &#x201C;fill a cauldron&#x201D; metric. You and I might get bored if someone told us to just keep searching for better actions, but it&apos;s entirely possible to write a program that executes a search and never gets bored. <sup><a href=\"#footnote8\">8</a></sup></p>\n<p>Second, <a href=\"https://intelligence.org/2015/11/26/new-paper-formalizing-convergent-instrumental-goals/\">sufficiently optimized objectives tend to converge on adversarial instrumental strategies</a>. Most objectives a smarter-than-human AI system could possess would be furthered by subgoals like &#x201C;acquire resources&#x201D; and &#x201C;remain operational&#x201D; (along with &#x201C;learn more about the environment,&#x201D; etc.).</p>\n<p>This was the problem suspend buttons ran into: even if you don&apos;t explicitly include &#x201C;remain operational&#x201D; in your goal specification, whatever goal you did load into the system is likely to be better achieved if the system remains online. Software systems&apos; capabilities and (terminal) goals are orthogonal, but they&apos;ll often exhibit similar behaviors if a certain class of actions is useful for a wide variety of possible goals.</p>\n<p>To use an example due to Stuart Russell: If you build a robot and program it to go to the supermarket to fetch some milk, and the robot&apos;s model says that one of the paths is much safer than the other, then the robot, in optimizing for the probability that it returns with milk, will automatically take the safer path. It&apos;s not that the system fears death, but that it can&apos;t fetch the milk if it&apos;s dead.</p>\n<p>Third, <a href=\"http://aiimpacts.org/sources-of-advantage-for-artificial-intelligence/\">general-purpose AI systems are likely to show large and rapid capability gains</a>. The human brain isn&apos;t anywhere near the upper limits for hardware performance (or, one assumes, software performance), and there are a number of other reasons to expect large capability advantages and rapid capability gain from advanced AI systems.</p>\n<p>As a simple example, Google can buy a promising AI startup and throw huge numbers of GPUs at them, resulting in a quick jump from &#x201C;these problems look maybe relevant a decade from now&#x201D; to &#x201C;we need to solve all of these problems in the next year.&#x201D; <sup><a href=\"#footnote9\">9</a></sup></p>\n<p>Fourth, <a href=\"https://arbital.com/p/aligning_adds_time/\">aligning advanced AI systems with our interests looks difficult</a>. I&apos;ll say more about why I think this presently.</p>\n<p>Roughly speaking, the first proposition says that AI systems won&apos;t naturally end up sharing our objectives. The second says that by default, systems with substantially different objectives are likely to end up adversarially competing for control of limited resources. The third suggests that adversarial general-purpose AI systems are likely to have a strong advantage over humans. And the fourth says that this problem is hard to solve &#x2014; for example, that it&apos;s hard to transmit our values to AI systems (addressing orthogonality) or <a href=\"https://arbital.com/p/avert_instrumental_pressure/\">avert adversarial incentives</a> (addressing convergent instrumental strategies).</p>\n<p>These four propositions don&apos;t mean that we&apos;re screwed, but they mean that this problem is critically important. General-purpose AI has the potential to bring enormous benefits if we solve this problem, but we do need to make finding solutions a priority for the field.</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<h2>Fundamental Difficulties</h2>\n<p>Why do I think that AI alignment looks fairly difficult? The main reason is just that this has been my experience from actually working on these problems. I encourage you to <a href=\"https://intelligence.org/2017/02/28/using-machine-learning/\">look at some of the problems yourself</a> and try to solve them in toy settings; we could use more eyes here. I&apos;ll also make note of a few structural reasons to expect these problems to be hard:</p>\n<p>First, aligning advanced AI systems with our interests looks difficult for the same reason rocket engineering is more difficult than airplane engineering.</p>\n<p>Before looking at the details, it&apos;s natural to think &#x201C;it&apos;s all just AI&#x201D; and assume that the kinds of safety work relevant to current systems are the same as the kinds you need when systems surpass human performance. On that view, it&apos;s not obvious that we should work on these issues now, given that they might all be worked out in the course of narrow AI research (e.g., making sure that self-driving cars don&apos;t crash).</p>\n<p>Similarly, at a glance someone might say, &#x201C;Why would rocket engineering be fundamentally harder than airplane engineering? It&apos;s all just material science and aerodynamics in the end, isn&apos;t it?&#x201D; In spite of this, empirically, the proportion of rockets that explode is far higher than the proportion of airplanes that crash. The reason for this is that a rocket is put under much greater stress and pressure than an airplane, and small failures are much more likely to be highly destructive. <sup><a href=\"#footnote10\">10</a></sup></p>\n<p>Analogously, even though general AI and narrow AI are &#x201C;just AI&#x201D; in some sense, we can expect that the more general AI systems are likely to experience a wider range of stressors, and possess more dangerous failure modes.</p>\n<p>For example, once an AI system begins modeling the fact that (i) your actions affect its ability to achieve its objectives, (ii) your actions depend on your model of the world, and (iii) your model of the world is affected by its actions, the degree to which minor inaccuracies can lead to harmful behavior increases, and the potential harmfulness of its behavior (which can now include, e.g., deception) also increases. In the case of AI, as with rockets, greater capability makes it easier for small defects to cause big problems.</p>\n<p>Second, alignment looks difficult for the same reason it&apos;s harder to build a good space probe than to write a good app.</p>\n<p>You can find a number of interesting engineering practices at NASA. They do things like take three independent teams, give each of them the same engineering spec, and tell them to design the same software system; and then they choose between implementations by majority vote. The system that they actually deploy consults all three systems when making a choice, and if the three systems disagree, the choice is made by majority vote. The idea is that any one implementation will have bugs, but it&apos;s unlikely all three implementations will have a bug in the same place.</p>\n<p>This is significantly more caution than goes into the deployment of, say, the new WhatsApp. One big reason for the difference is that it&apos;s hard to roll back a space probe. You can send version updates to a space probe and correct software bugs, but only if the probe&apos;s antenna and receiver work, and if all the code required to apply the patch is working. If your system for applying patches is itself failing, then there&apos;s nothing to be done.</p>\n<p>In that respect, smarter-than-human AI is more like a space probe than like an ordinary software project. If you&apos;re trying to build something smarter than yourself, there are parts of the system that have to work perfectly on the first real deployment. We can do all the test runs we want, but once the system is out there, we can only make online improvements if the code that makes the system <em>allow</em> those improvements is working correctly.</p>\n<p>If nothing yet has struck fear into your heart, I suggest meditating on the fact that the future of our civilization may well depend on our ability to write code that <em>works correctly</em> on the first deploy.</p>\n<p>Lastly, alignment looks difficult for the same reason computer security is difficult: systems need to be robust to intelligent searches for loopholes.</p>\n<p>Suppose you have a dozen different vulnerabilities in your code, none of which is itself fatal or even really problematic in ordinary settings. Security is difficult because you need to account for intelligent attackers who might find all twelve vulnerabilities and chain them together in a novel way to break into (or just break) your system. Failure modes that would never arise by accident can be sought out and exploited; weird and extreme contexts can be instantiated by an attacker to cause your code to follow some crazy code path that you never considered.</p>\n<p>A similar sort of problem arises with AI. The problem I&apos;m highlighting here is not that AI systems might act adversarially: AI alignment as a research program is all about finding ways to <a href=\"https://arbital.com/p/nonadversarial/\">prevent adversarial behavior</a> before it can crop up. We don&apos;t want to be in the business of trying to outsmart arbitrarily intelligent adversaries. That&apos;s a losing game.</p>\n<p>The parallel to cryptography is that in AI alignment we deal with systems that perform intelligent searches through a very large search space, and which can produce weird contexts that force the code down unexpected paths. This is because the weird <a href=\"https://arbital.com/p/edge_instantiation/\">edge cases</a> are places of extremes, and places of extremes are often the place where a given objective function is optimized. <sup><a href=\"#footnote11\">11</a></sup> Like computer security professionals, AI alignment researchers need to be very good at thinking about edge cases.</p>\n<p>It&apos;s much easier to make code that works well on the path that you were visualizing than to make code that works on all the paths that you weren&apos;t visualizing. AI alignment needs to work <a href=\"https://arbital.com/p/unforeseen_maximum/\">on all the paths you weren&apos;t visualizing</a>.</p>\n<p>Summing up, we should approach a problem like this with the same level of rigor and caution we&apos;d use for a security-critical rocket-launched space probe, and do the legwork as early as possible. At this early stage, a key part of the work is just to formalize basic concepts and ideas so that others can critique them and build on them. It&apos;s one thing to have a philosophical debate about what kinds of suspend buttons people intuit ought to work, and another thing to translate your intuition into an equation so that others can fully evaluate your reasoning.</p>\n<p>This is a crucial project, and I encourage all of you who are interested in these problems to get involved and try your hand at them. There are <a href=\"http://humancompatible.ai/bibliography\">ample resources online</a> for learning more about the open technical problems. Some good places to start include MIRI&apos;s <a href=\"https://intelligence.org/research/\">research agendas</a> and a great paper from researchers at Google Brain, OpenAI, and Stanford called &#x201C;<a href=\"/=&quot;https:/arxiv.org/abs/1606.06565&quot;\">Concrete Problems in AI Safety</a>.&#x201D;</p>\n<p>&#xA0;</p>\n<hr>\n<p>&#xA0;</p>\n<h2>Footnotes</h2>\n<p><sup><a></a>1</sup> An airplane can&apos;t heal its injuries or reproduce, though it can carry heavy cargo quite a bit further and faster than a bird. Airplanes are simpler than birds in many respects, while also being significantly more capable in terms of carrying capacity and speed (for which they were designed). It&apos;s plausible that early automated scientists will likewise be simpler than the human mind in many respects, while being significantly more capable in certain key dimensions. And just as the construction and design principles of aircraft look alien relative to the architecture of biological creatures, we should expect the design of highly capable AI systems to be quite alien when compared to the architecture of the human mind. <a href=\"#footnote1ret\">&#x21A9;</a></p>\n<p><sup><a></a>2</sup> Trying to give some formal content to these attempts to differentiate task-like goals from open-ended goals is one way of generating open research problems. In the &#x201C;<a href=\"/=&quot;https:/intelligence.org/2016/07/27/alignment-machine-learning/&quot;\">Alignment for Advanced Machine Learning Systems</a>&#x201D; research proposal, the problem of formalizing &#x201C;don&apos;t try too hard&#x201D; is <a href=\"https://arbital.com/p/soft_optimizer/\">mild optimization</a>, &#x201C;steer clear of absurd strategies&#x201D; is <a href=\"https://arbital.com/p/conservative_concept/\">conservatism</a>, and &#x201C;don&apos;t have large unanticipated consequences&#x201D; is <a href=\"https://arbital.com/p/low_impact/\">impact measures</a>. See also &#x201C;avoiding negative side effects&#x201D; in Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man&#xE9;&apos;s &#x201C;<a href=\"/=&quot;https:/arxiv.org/abs/1606.06565&quot;\">Concrete Problems in AI Safety</a>.&#x201D; <a href=\"#footnote2ret\">&#x21A9;</a></p>\n<p><sup><a></a>3</sup> One thing we&apos;ve learned in the field of machine vision over the last few decades is that it&apos;s hopeless to specify by hand what a cat looks like, but that it&apos;s not too hard to specify a learning system that can learn to recognize cats. It&apos;s even more hopeless to specify everything we value by hand, but it&apos;s plausible that we could specify a learning system that can learn the relevant concept of &#x201C;value.&#x201D; <a href=\"#footnote3ret\">&#x21A9;</a></p>\n<p><sup><a></a>4</sup> Roughly speaking, MIRI&apos;s focus is on research directions that seem likely to help us conceptually understand how to do AI alignment in principle, so we&apos;re fundamentally less confused about the kind of work that&apos;s likely to be needed.</p>\n<p>What do I mean by this? Let&apos;s say that we&apos;re trying to develop a new chess-playing programs. Do we understand the problem well enough that we could solve it if someone handed us an arbitrarily large computer? Yes: We make the whole search tree, backtrack, see whether white has a winning move.</p>\n<p>If we didn&apos;t know how to answer the question even with an arbitrarily large computer, then this would suggest that we were fundamentally confused about chess in some way. We&apos;d either be missing the search-tree data structure or the backtracking algorithm, or we&apos;d be missing some understanding of how chess works.</p>\n<p>This was the position we were in regarding chess prior to Claude Shannon&apos;s seminal paper, and it&apos;s the position we&apos;re currently in regarding many problems in AI alignment. No matter how large a computer you hand me, I could not make a smarter-than-human AI system that performs even a very simple limited-scope task (e.g., &#x201C;put a strawberry on a plate without producing any catastrophic side-effects&#x201D;) or achieves even a very simple open-ended goal (e.g., &#x201C;maximize the amount of diamond in the universe&#x201D;).</p>\n<p>If I didn&apos;t have any particular goal in mind for the system, I <em>could</em> write a program (assuming an arbitrarily large computer) that strongly optimized the future in an undirected way, using a formalism like <a href=\"https://arbital.com/p/AIXI/\">AIXI</a>. In that sense we&apos;re less obviously confused about capabilities than about alignment, even though we&apos;re still missing a lot of pieces of the puzzle on the practical capabilities front.</p>\n<p>Our goal is to develop and formalize basic approaches and ways of thinking about the alignment problem, so that our engineering decisions don&apos;t end up depending on sophisticated and clever-sounding verbal arguments that turn out to be subtly mistaken. Simplifications like &#x201C;what if we weren&apos;t worried about resource constraints?&#x201D; and &#x201C;what if we were trying to achieve a much simpler goal?&#x201D; are a good place to start breaking down the problem into manageable pieces. For more on this methodology, see &#x201C;<a href=\"/=&quot;https:/intelligence.org/2015/07/27/miris-approach/&quot;\">MIRI&apos;s Approach</a>.&#x201D; <a href=\"#footnote4ret\">&#x21A9;</a></p>\n<p><sup><a></a>5</sup> &#x201C;Fill this cauldron without being too clever about it or working too hard or having any negative consequences I&apos;m not anticipating&#x201D; is a rough example of a goal that&apos;s intuitively limited in scope. The things we actually want to use smarter-than-human AI for are obviously more ambitious than that, but we&apos;d still want to begin with various limited-scope tasks rather than open-ended goals.</p>\n<p>Asimov&apos;s Three Laws of Robotics make for good stories partly for the same reasons they&apos;re unhelpful from a research perspective. The hard task of turning a moral precept into lines of code is hidden behind phrasings like &#x201C;[don&apos;t,] through inaction, allow a human being to come to harm.&#x201D; If one followed a rule like that strictly, the result would be massively disruptive, as AI systems would need to systematically intervene to prevent <a href=\"https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/#1\">even the smallest risks of even the slightest harms</a>; and if the intent is that one follow the rule loosely, then all the work is being done by the human sensibilities and intuitions that tell us <a href=\"http://lesswrong.com/lw/sp/detached_lever_fallacy/\">when and how to apply the rule</a>.</p>\n<p>A common response here is that vague natural-language instruction is sufficient, because smarter-than-human AI systems are likely to be capable of natural language comprehension. However, this is eliding the distinction between the system&apos;s objective function and its model of the world. A system acting in an environment containing humans may learn a world-model that has lots of information about human language and concepts, which the system can then use to achieve its objective function; but this fact doesn&apos;t imply that any of the information about human language and concepts will &#x201C;leak out&#x201D; and alter the system&apos;s objective function directly.</p>\n<p>Some kind of value learning process needs to be defined where the objective function itself improves with new information. This is a tricky task because there aren&apos;t known (scalable) metrics or criteria for value learning in the way that there are for conventional learning.</p>\n<p>If a system&apos;s world-model is accurate in training environments but fails in the real world, then this is likely to result in lower scores on its objective function &#x2014; the system itself has an incentive to improve. The severity of accidents is also likelier to be self-limiting in this case, since false beliefs limit a system&apos;s ability to effectively pursue strategies.</p>\n<p>In contrast, if a system&apos;s value learning process results in a &#x1D5E8; that matches our &#x1D61D; in training but diverges from &#x1D61D; in the real world, then the system&apos;s &#x1D5E8; will obviously not penalize it for optimizing &#x1D5E8;. The system has no incentive relative to &#x1D5E8; to &#x201C;correct&#x201D; divergences between &#x1D5E8; and &#x1D61D;, if the value learning process is initially flawed. And accident risk is larger in this case, since a mismatch between &#x1D5E8; and &#x1D61D; doesn&apos;t necessarily place any limits on the system&apos;s instrumental effectiveness at coming up with effective and creative strategies for achieving &#x1D5E8;.</p>\n<p>The problem is threefold:</p>\n<ol>\n<li>&#x201C;Do What I Mean&#x201D; is an informal idea, and even if we knew how to build a smarter-than-human AI system, we wouldn&apos;t know how to precisely specify this idea in lines of code.</li>\n<li>If doing what we actually mean is instrumentally useful for achieving a particular objective, then a sufficiently capable system may learn how to do this, and may act accordingly so long as doing so is useful for its objective. But as systems become more capable, they are likely to find creative new ways to achieve the same objectives, and there is no obvious way to get an assurance that &#x201C;doing what I mean&#x201D; will continue to be instrumentally useful indefinitely.</li>\n<li>If we use value learning to refine a system&apos;s goals over time based on training data that appears to be guiding the system toward a &#x1D5E8; that inherently values doing what we mean, it is likely that the system will actually end up zeroing in on a &#x1D5E8; that approximately does what we mean during training but catastrophically diverges in some difficult-to-anticipate contexts. See &#x201C;<a href=\"/=&quot;https:/arbital.com/p/goodharts_curse/&quot;\">Goodhart&apos;s Curse</a>&#x201D; for more on this.</li>\n</ol>\n<p>For examples of problems faced by existing techniques for learning goals and facts, such as reinforcement learning, see &#x201C;<a href=\"/=&quot;https:/intelligence.org/2017/02/28/using-machine-learning/#problem-1&quot;\">Using Machine Learning to Address AI Risk</a>.&#x201D; <a href=\"#footnote5ret\">&#x21A9;</a></p>\n<p><sup><a></a>6</sup> The result will probably not be a particularly human-like design, since so many complex historical contingencies were involved in our evolution. The result will also be able to benefit from a number of large <a href=\"http://aiimpacts.org/sources-of-advantage-for-artificial-intelligence/\">software and hardware advantages</a>. <a href=\"#footnote6ret\">&#x21A9;</a></p>\n<p><sup><a></a>7</sup> This concept is sometimes lumped into the &#x201C;<a href=\"/=&quot;https:/intelligence.org/2013/08/25/transparency-in-safety-critical-systems/&quot;\">transparency</a>&#x201D; category, but standard algorithmic transparency research isn&apos;t really addressing this particular problem. A better term for what I have in mind here is &#x201C;<a href=\"/=&quot;https:/arbital.com/p/understandability_principle/&quot;\">understanding</a>.&#x201D; What we want is to gain deeper and broader insights into the kind of cognitive work the system is doing and how this work relates to the system&apos;s objectives or optimization targets, to provide a conceptual lens with which to make sense of the hands-on engineering work. <a href=\"#footnote7ret\">&#x21A9;</a></p>\n<p><sup><a></a>8</sup> We could <em>choose</em> to program the system to tire, but we don&apos;t have to. In principle, one could program a broom that only ever finds and executes actions that optimize the fullness of the cauldron. Improving the system&apos;s ability to efficiently find high-scoring actions (in general, or relative to a particular scoring rule) doesn&apos;t in itself change the scoring rule it&apos;s using to evaluate actions. <a href=\"#footnote8ret\">&#x21A9;</a></p>\n<p><sup><a></a>9</sup> Some other examples: a system&apos;s performance may suddenly improve when it&apos;s first given large-scale Internet access, when there&apos;s a conceptual breakthrough in algorithm design, or when the system itself is able to propose improvements to its hardware and software. We can imagine the latter case in particular resulting in a <a href=\"https://intelligence.org/files/IEM.pdf\">feedback loop</a> as the system&apos;s design improvements allow it to come up with further design improvements, until all the low-hanging fruit is exhausted. Another important consideration is that two of the main bottlenecks to humans doing faster scientific research are training time and communication bandwidth. If we could train a new mind to be a cutting-edge scientist in ten minutes, and if scientists could near-instantly trade their experience, knowledge, concepts, ideas, and intuitions to their collaborators, then scientific progress might be able to proceed much more rapidly. Those sorts of bottlenecks are exactly the sort of bottleneck that might give automated innovators an enormous edge over human innovators even without large advantages in hardware or algorithms. <a href=\"#footnote9ret\">&#x21A9;</a></p>\n<p><sup><a></a>10</sup> Specifically, rockets experience a wider range of temperatures and pressures, traverse those ranges more rapidly, and are also packed more fully with explosives. <a href=\"#footnote10ret\">&#x21A9;</a></p>\n<p><sup><a></a>11</sup> Consider Bird and Layzell&apos;s <a href=\"https://people.duke.edu/~ng46/topics/evolved-radio.pdf\">example</a> of a very simple genetic algorithm that was tasked with evolving an oscillating circuit. Bird and Layzell were astonished to find that the algorithm made no use of the capacitor on the chip; instead, it had repurposed the circuit tracks on the motherboard as a radio to replay the oscillating signal from the test device back to the test device.</p>\n<p>This was not a very smart program. This is just using hill climbing on a very small solution space. In spite of this, the solution turned out to be outside the space of solutions the programmers were themselves visualizing. In a computer simulation, this algorithm might have behaved as intended, but the actual solution space in the real world was wider than that, allowing hardware-level interventions.</p>\n<p>In the case of an intelligent system that&apos;s significantly smarter than humans on whatever axes you&apos;re measuring, you should by default expect the system to push toward weird and creative solutions like these, and for the chosen solution to be difficult to anticipate. <a href=\"#footnote11ret\">&#x21A9;</a></p></body></html>", "user": {"username": "So8res"}}, {"_id": "LaXGf6hZXXqZ7sj5m", "title": "How accurately does anyone know the global distribution of income?", "postedAt": "2017-04-06T04:49:45.335Z", "htmlBody": "<html><body><p>Cross posted from the <a href=\"https://80000hours.org/2017/04/how-accurately-does-anyone-know-the-global-distribution-of-income/\">80,000 Hours blog</a>.</p>\n<p>&#xA0;</p>\n<p><img src=\"https://80000hours.org/wp-content/uploads/2017/04/1200x-1.png\" alt=\"World income distribution\"></p>\n<p><em>How much should you believe the numbers in charts&#xA0;like this?</em></p>\n<p>People in the effective altruism community often refer to the global income distribution to make various points:</p>\n<ul>\n<li>The richest people in the world are many times richer than the poor.</li>\n<li>People earning professional salaries in countries like the US are usually in the top 5% of global earnings and fairly often in the top 1%. This gives them a disproportionate ability to improve the world.</li>\n<li>Many people in the world live in serious absolute poverty, surviving on as little as one hundredth the income of the upper-middle class in the US.</li>\n</ul>\n<p>Measuring the global income distribution is very difficult and experts who attempt to do so end up with different results. However, these core points are supported by every attempt to measure the global income distribution that we&#x2019;ve seen so far.</p>\n<p>The rest of this post will discuss the global income distribution data we&apos;ve referred to, the uncertainty inherent in that data, and why we believe our bottom lines hold up anyway.</p>\n<p>Will MacAskill had a striking illustration of global individual income distribution in his book <a href=\"https://www.amazon.com/Doing-Good-Better-Effective-Altruism/dp/1592409660\">Doing Good Better</a>, that has ended up in many other articles online, including <a href=\"https://80000hours.org/career-guide/how-much-difference-can-one-person-make/#how-is-this-possible\">our own career guide</a>:</p>\n<p>&#xA0;</p>\n<p>&#xA0; &#xA0;<img src=\"https://80000hours.org/wp-content/uploads/2017/04/00005-1024x934.jpeg\"> &#xA0;</p>\n<p>&#xA0;</p>\n<p>The data in this graph was put together back in 2012 using an approach suggested by Branko Milanovic, at the time lead economist in the World Bank&apos;s research department, and author of <em><a href=\"https://www.amazon.com/Haves-Have-Nots-Idiosyncratic-History-Inequality/dp/0465031412\">The Haves and the Have-Nots</a></em>. Incidentally, Milanovic went on to achieve mainstream fame for the so-called <a href=\"https://www.bloomberg.com/news/articles/2016-06-27/get-ready-to-see-this-globalization-elephant-chart-over-and-over-again\">&#x2018;elephant graph&#x2019;</a>. For the bottom 80% of the income distribution we used World Bank figures from their database &#x2018;PovcalNet&#x2019;. As this data set was not considered reliable for the top 20% of the income distribution, we substituted them with figures from Branko Milanovic&#x2019;s own work compiling national household surveys.</p>\n<p>Some have questioned whether the graph gives a misleading picture of global income inequality. How seriously should we take it?</p>\n<p>One obvious concern is that this distribution is based on income surveys from 2008, and the global income distribution may have changed since then. Strong economic growth in countries like China has improved the lot of people in the middle of the income distribution, while the Great Recession in 2008-10 suppressed incomes in rich countries more than poor countries. <em>Hellebrandt and Mauro</em>&#xA0;attempts <a href=\"https://ourworldindata.org/global-economic-inequality#the-global-income-distribution-in-2003-and-2013ref\">to estimate how the income distribution changed</a> between 2003 and 2013, and finds a quite significant shift.[fn 1]</p>\n<p>Why are we still using numbers from <em>9</em> years ago? Complete and consistent global income distribution estimates arrive infrequently and at a substantial delay, because they rely on surveys trickling in from over 150 countries around the world, and being made comparable. 2008 is still the last year for which we are aware of publicly available and compatible survey figures across the whole distribution. We are working to get access to newer figures that are not yet public, though they will only bring us up to 2011.</p>\n<p>But that&#x2019;s only the beginning of the difficulties. There are lots of ways different organisations might produce different numbers:</p>\n<p>1. <strong>The underlying survey data may be inaccurate or sample differently.</strong> For example, different polling groups will have different ways of trying to question a representative cross-section of people in a country about their income. Needless to say this gets very challenging. Imagine trying to make sure you&#x2019;ve fairly sampled the poorest 20% of people in India, or the Democratic Republic of Congo. The people you want to sample may be in rural areas without access to any telecommunications. How do you know you&#x2019;ve got the right number of people from these groups in your measurements? And how do you measure the equivalent income of people who grow food for their own consumption, rather than receive a salary? <a href=\"http://iresearch.worldbank.org/PovcalNet/methodology.aspx\">PovcalNet is up-front</a> about the serious challenges they face:</p>\n<p><em>More than 2 million randomly sampled households were interviewed in these surveys, representing 96 percent of the population of developing countries. Not all these surveys are comparable in design and sampling methods. Non representative surveys, though useful for some purposes, are excluded from the calculation of international poverty rates. ... No data are ideal. International comparisons of poverty estimates entail both conceptual and practical problems that should be understood by users.</em></p>\n<p>In line with its desire to measure the extent of effective poverty around the world, PovcalNet uses measures of *consumption* where they are available. This means that income people receive which they then save is not counted, while spending funded by past savings *is* counted, and savings this year will appear in future years&apos; consumption. Our World In Data [has more information](https://ourworldindata.org/extreme-poverty/) on how this is done.</p>\n<p>If you&#x2019;d like to learn more about how questionable data coming out of the developing world can be, a good source would be <a href=\"https://www.amazon.com/Poor-Numbers-Development-Statistics-Political/dp/080147860X\">Poor Numbers</a> by Morten Jerven.</p>\n<p>Getting enough data about the <a href=\"https://web.williams.edu/Economics/wp/BakijaColeHeimJobsIncomeGrowthTopEarners.pdf\">top 1% of earners</a> is difficult in a different way: they represent only a small fraction of respondents in surveys, their income sources are more varied, and their incomes differ <em>enormously</em>.</p>\n<p>2. <strong>Different ways of adjusting for &#x2018;purchasing power&#x2019;.</strong> Money goes further in poorer countries, and any sensible attempt to look at global income will account for this. But how do you compare the value of two currencies when the people in the relevant countries are buying very different things? Very few identical products are bought in both rural Kenya and Switzerland, so any attempt to compare the practical purchasing power of Swiss francs and Kenyan shillings is going to be imprecise. Moreover, even within countries people at different parts of the income distribution consume very different bundles of goods, and therefore are affected by different prices. Economists do their best to sample what people are buying in a range of places, how similar their quality is to goods elsewhere, and what they cost - but only so much is possible. In one dramatic case, a revision of purchasing power parity weights by the World Bank in 2005 <a href=\"https://ftalphaville.ft.com/2014/05/02/1842012/china-the-us-and-ppp-a-pretty-poor-parallel/\">cut China&#x2019;s purchasing power parity-adjusted GDP by 40 per cent</a>. Then in 2014 it was <a href=\"https://www.imf.org/external/pubs/ft/weo/2014/update/02/index.htm\">revised back up based on surveys conducted in 2011</a>, suddenly making it the <a href=\"https://www.ft.com/content/d79ffff8-cfb7-11e3-9b2b-00144feabdc0\">largest economy in the world</a>&#xA0;(maybe, anyway).</p>\n<p>Finally, how do people deal with the varied cost of living in different places within a country? I&#x2019;ve never seen these adjustments made. And it&#x2019;s unclear how much they should be made. One way people choose to spend their income to improve their lives is to live in expensive cities!</p>\n<p>3. <strong>Different ways of dealing with household size.</strong> Sometimes income data is given &#x2018;per capita&apos;, and other times it&#x2019;s given &#x2018;per household&#x2019;. This can change the shape of an income distribution, because globally larger families have lower incomes. Larger families also have greater &#x2018;economies of scale&#x2019; (e.g. they might share a single house or car). When economists want to take household income from surveys and &#x2018;individualise&#x2019; the figures to compare across households of different sizes, they use &#x2018;equivalence scales&#x2019;. But estimates of the right equivalence scales <a href=\"http://econlog.econlib.org/archives/2014/02/how_rival_marri.html\">differ a remarkable amount</a>. Using one method, a couple and one child living on $20,000 collectively, have an effective individual income of $15,200. Using a method at the other extreme, the figure is $9,100. You might also just divide total household income by the number of family members and ignore any of the effects of family structure (this is the approach taken in PovcalNet and Milanovi&#x107;&apos;s figures). This creates another source of variation in how the survey data is processed before you see it.</p>\n<p>4. <strong>Different dollar units.</strong> Figures for global income comparisons are usually given in <a href=\"https://en.wikipedia.org/wiki/Geary%E2%80%93Khamis_dollar\">&#x2018;international dollars&#x2019;</a>. Occasionally 1990 international dollars are used for comparison of changes in data over long time periods. Other times you&#x2019;ll find figures in 2000 international dollars, 2011 international dollars, or whatever year the data were released. Inflation between these different time periods can move the figures by 10-40%.</p>\n<p>5. <strong>Are the figures after tax or pre-tax?</strong>&#xA0;<em>Gallup Polling</em>&#xA0;and <em>Hellebrandt and Mauro</em>&#xA0;(2015) report income pre-tax. The Brankovic figures used above are post-tax. PovcalNet doesn&#x2019;t say on their site, but in correspondence I&#x2019;ve been told &#x201C;in principle the figures are post-tax&#x201D; (the World Bank is forced to draw on many varied data sources). This alone could create a 25-50% gap between them.</p>\n<p>Is pre-tax or post-tax the better way to do things? Reasonable people can disagree about this. People in poorer countries pay less tax, which in a sense boosts their spending power. But they also get fewer services from their governments in return, forcing them to buy them out of pocket. On the other hand, if high income earners in a given country are funding financial transfers to people on lower incomes - rather than services they personally receive - it makes more sense to exclude those taxes from their effective income.</p>\n<p>One person sent us <a href=\"http://www.gallup.com/poll/166211/worldwide-median-household-income-000.aspx\">figures from Gallup Polling</a> that seemed to dramatically conflict with our graph - a median income of $9,733 vs the $1,272 we pulled out of the World Bank&#x2019;s PovcalNet. The first big adjustment is that the $10,000 figure is for <em>households</em>, whereas the chart we use gives figures for individuals. The per person income figure from Gallup is the more modest $2,920.</p>\n<p>If that person had looked around, they would have found that <a href=\"https://ourworldindata.org/global-economic-inequality\">other sources</a> give different numbers again. For example, <em>Hellebrandt and Mauro</em>, which I mentioned above, offers a global median income of $2,010 in 2013.[fn 2] Milanovic&#x2019;s estimate was $1,225 for 2005, and $1,480 for 2008.[fn 3]</p>\n<p>A substantial fraction of those differences can be explained by rising incomes over time, and the fact that the two higher numbers are pre-tax, and the lower two post-tax. What explains the remainder? The information required to figure that out isn&#x2019;t publicly available, and answering that question is really a job for an expert in the field rather than a dilettante such as myself. One possibility is that the surveys used by the World Bank go further into poor, dangerous and rural communities than those by Gallup (a private polling company). Evidence fo this is that in their income tables Gallup appears to only have surveyed the capital city of the Democratic Republic of Congo. In addition, as far as I could see Gallup&#x2019;s polling data has not yet been published in an economics journal, so there could be quite a few methodological differences with the rest of the literature.&#xA0;</p>\n<p>All that said, given the range of choices researchers are required to make, a difference of this size isn&apos;t much of a surprise. Political scientist Merle Kling once proposed three &#x2018;iron laws of social science&#x2019;, and they apply here as much as anywhere:</p>\n<p>1. Sometimes it&#x2019;s this way, and sometimes it&#x2019;s that way.<br>2. The data are insufficient.<br>3. The methodology is flawed.</p>\n<p>These figures <em>are</em>&#xA0;approximations. However, having had personal experience with social science data, the rigour here is better than I would have expected going in. As far as I can tell most researchers are making defensible decisions while trying to produce these estimates.</p>\n<p>And despite the challenges, these bottom lines remain in every estimate of the global income distribution I&#x2019;ve seen so far:</p>\n<ul>\n<li>The richest people in the world are many times richer than the poor.</li>\n<li>People earning professional salaries in countries like the US are usually in the top 5% of global earnings and sometimes in the top 1%. This gives them a disproportionate ability to improve the world.</li>\n<li>Many people in the world live in serious absolute poverty, surviving on as little as one hundredth the income of the upper-middle class in the US.</li>\n</ul>\n<p>[fn 1] Hellebrandt, Tomas and Mauro, Paolo (2015) &#x2013; The Future of Worldwide Income Distribution (April 1, 2015). Peterson Institute for International Economics Working Paper No. 15-7. Available at <a href=\"https://ssrn.com/abstract=2593894\">SSRN</a> or <a href=\"http://dx.doi.org/10.2139/ssrn.2593894\">http://dx.doi.org/10.2139/ssrn.2593894</a>. [/fn]</p>\n<p>[fn 2] Incidentally, it&#x2019;s unlikely they could have had global survey data compiled for 2013 by 2015, as individual country distributions for 2013 are only becoming available now. So they probably used modelling assumptions about growth at different parts of the distribution. The more you know! [/fn]</p>\n<p>[fn 3] The former of these is in <a href=\"https://www.amazon.com/Haves-Have-Nots-Idiosyncratic-History-Inequality/dp/0465031412\">The Haves and the Have-Nots</a> vignette 3.2. The latter figure is from personal correspondence. [/fn]</p></body></html>", "user": {"username": "Robert_Wiblin"}}, {"_id": "55giwB2Pmv5YM6JuQ", "title": "How do EA Orgs Account for Uncertainty in their Analysis?", "postedAt": "2017-04-05T16:48:45.220Z", "htmlBody": "<html><body><p><em>This essay was jointly written by&#xA0;Peter Hurford, Kathryn Mecrow, and Simon Beard[1].</em></p>\n<p>&#xA0;</p>\n<p>Effective altruism <a href=\"/ea/9s/effective_altruism_is_a_question_not_an_ideology/\"> is about figuring out how to do the most good</a>; when working with limited resources, whether financial or otherwise, and faced with opportunity costs, how do we measure the good of the impact of a particular program? Using examples from current projects: how good is it to give someone a bednet, hand out a pro-vegetarian leaflet, or do an hour of AI safety research? This article considers the various methods employed by EA and EA associated organizations for dealing with uncertainty in their cost effectiveness analyses. To address this, we conducted a literature review using resources on the websites of EA orgs, and reached out to the different organizations for guidance.</p>\n<p>When striving to answer these questions, we can model various parameters. For example, GiveWell <a href=\"http://blog.givewell.org/2016/07/26/deworming-might-huge-impact-might-close-zero-impact/\"> mentions</a>&#xA0;that the benefit of a deworming program depends on a number of factors that each carry their own uncertainties. Does deworming have short-term health benefits? Is it clear that deworming has long-term positive health or social impacts? How do we evaluate evidence from randomized control trials that differ from modern day deworming programs in a number of important ways? How do we proceed if some analysis suggests that deworming programs are more likely than not to have very little or no impact, but there is some possibility that deworming has a very large impact? In the process of combining these models we can employ ranges or intervals to express our uncertainty and aggregate the various inputs to create a total estimate of impact.</p>\n<p>How do we know that we have all the relevant parameters, that we have aggregated correctly, accurately assessed our uncertainty in each parameter, and made no errors? Given the emphasis of the EA movement on robustness of evidence, how do we ensure that we actually do the most good and retain the credibility of our assessments in the face of poorly researched areas, uncertain future impact scenarios, and multitudes of known and unknown program-specific factors?</p>\n<p>For example, <a href=\"http://www.givewell.org/international/charities/villagereach\"> VillageReach was GiveWell&#x2019;s top charity for three years</a>&#xA0;(2009-2011) with an <a href=\"http://www.givewell.org/charities/top-charities/2009\"> estimated &quot;$200-$1,000 per life saved&quot;</a>. However, <a href=\"http://www.givewell.org/international/top-charities/villagereach/pilot-project-re-analysis\"> a later re-analysis in 2012 found that there was a significant amount of missing data</a>&#xA0;that was not considered in the original review of VillageReach. This data could potentially have a large effect on the endline conclusion of Village Reach&#x2019;s cost-effectiveness. Additionally, GiveWell <a href=\"http://blog.givewell.org/2012/07/26/rethinking-villagereachs-pilot-project/\"> later discovered potential alternative explanations for some of Village Reach&#x2019;s impact</a>, further reducing confidence in the cost-effectiveness figure. Together, this missing data and alternative explanations potentially represents a large source of &#x201C;model uncertainty&#x201D; on the initial estimate.</p>\n<p>This dilemma is not confined to the EA movement. As a now famous example, <a href=\"http://election.princeton.edu/2016/11/08/final-mode-projections-clinton-323-ev-51-di-senate-seats-gop-house/\"> Sam Wang at Princeton gave a 99% chance of Clinton winning the election</a>. While Wang expressed his uncertainty, <a href=\"http://fivethirtyeight.com/features/election-update-why-our-model-is-more-bullish-than-others-on-trump/\"> there were systematic errors in the model</a>&#xA0;that made it more confident than was warranted. Nate Silver argues in <strong><a href=\"https://smile.amazon.com/Signal-Noise-Many-Predictions-Fail-but/dp/0143125087?sa-no-redirect=1\"> The Signal and The Noise</a></strong>&#xA0;that a similar kind of problem with correlated errors contributed to a lot of model uncertainty in financial forecasting, which was ultimately partially responsible for the 2008 Recession.</p>\n<p>&#xA0;</p>\n<h3><strong>Key points from EA orgs</strong></h3>\n<ul>\n<li>\n<p><strong>Clearly describing sources of uncertainty is useful for readers.</strong> It creates positive reinforcement mechanisms for charities that fully disclose the successes, but particularly failures, in program implementation, further increasing the likelihood of further positive implementation of interventions. It also makes it easier for the reader to &#x201C;sanity check&#x201D; the estimate and understand to what degree and under what circumstances the estimate might accurately represent future results.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p><strong>Cost-effectiveness estimates frequently need to include value judgements.</strong> If the cost effectiveness of an intervention depends upon a particular value judgement, e.g., how much one values saving the life of a child under age 5 versus improving the income of a 40-year old adult, this should be acknowledged as a further source of uncertainty and attempts should be made to calculate the implications of making different possible value judgements instead (i.e., we should do sensitivity analyses). When viewing a cost-effectiveness estimate, one should make sure that it reflects one&#x2019;s own values or update the estimate so that it does.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p><strong>Data may be inherently biased, such as confirmation bias in the assessment of cause areas and with information provided by charities.</strong> Unacknowledged biases are a key driver of overconfidence.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p><strong>Cost-effectiveness of the implementation of a program should take into consideration variation in the contexts of implementation.</strong> Different organizations run the same program with varying levels of effectiveness, in a diversity of implementation contexts.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p><strong>A cost-effectiveness estimate should take into consideration all data.</strong> For example, it may be helpful to use a Bayesian framework where the cost-effectiveness estimate is used to perform a Bayesian update on a prior distribution of expected cost-effectiveness. One should also strive to gather data from a variety of sources, when applicable, such as randomized controlled trials.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p><strong>Performing a cost-effectiveness analysis alone (as is typically done) may not be enough to adequately assess a program&#x2019;s effectiveness.</strong> Your confidence in a cost-effectiveness estimate should depend on the quality of the work that has gone into it -- some cost-effectiveness estimates are more robust than others. As GiveWell and many EA orgs state, cost-effectiveness estimates are frequently oversimplified, overly sensitive, and unverified. Other criteria may be essential in building a more informed understanding of an intervention&#x2019;s effectiveness.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p><strong>Transparency is key.</strong> Not revealing in detail how the estimate was made can obscure errors for a long time. Organizations should strive to mention and elaborate on the sources of uncertainty in their models, as well as make the full details of their calculations public. A cost-effectiveness analysis should take into consideration uncertainty and be transparent not only about how its model attempts to deal with it but also that a degree of uncertainty is a reality of programs operating in under-researched or developing areas. People running programs should therefore be prepared to change their estimates based on new evidence without prejudice.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p><strong>It may be less unintentionally confusing to emphasize comparing estimates against a threshold rather than emphasize absolute estimates.</strong> For example, it is a lot easier to accurately assess whether a certain value exceeds a threshold than it is to assess the value precisely, which favors threshold approaches that try to identify &#x201C;top charities&#x201D; but not rank those charities any further. This may go against the virtue of transparency as absolute scores are more informative of the estimate, even if less robust. However, this transparency could unintentionally confuse people into thinking that an estimate that scores higher is automatically better.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p><strong>Consider information quality.</strong> When information quality is good, you should focus on quantifying your different options; when it isn&#x2019;t, you should focus on raising information quality.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p><strong>It can be useful to aggregative uncertainty levels of independent estimates.</strong> There are tools that make this easier, like <a href=\"https://www.getguesstimate.com/\">Guesstimate</a>.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p><strong>One shouldn&#x2019;t treat all areas of uncertainty as equal, or be overly concerned about the mean or median possible outcome.</strong> When risks and benefits are not normally distributed there may be good reason to care disproportionately about the best or worst possible outcomes. It may also be important to consider issues such as fairness and the temporal distribution of costs and benefits across different outcomes as well as their overall cost effectiveness.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p><strong>Consider a range of views.</strong> Giving some weight to <a href=\"http://lesswrong.com/lw/iao/common_sense_as_a_prior/\"> common sense</a>&#xA0;and <a href=\"https://en.wikipedia.org/wiki/Reference_class_forecasting\"> the outside view</a>&#xA0;can be useful when dealing with uncertainty.</p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<h2>GiveWell</h2>\n<p>It&#x2019;s hard to find an organization, in effective altruism or otherwise, that has invested as much time thinking and articulating thoughts about uncertainty as GiveWell. While organizations like UNICEF suggest they can save a life with $1 per dose and that Nothing But Nets argues they can save a life with $10 per bednet, <a href=\"http://www.givewell.org/how-we-work/our-criteria/cost-effectiveness#Charities_frequently_cite_misleading_cost-effectiveness_figures\"> GiveWell&#x2019;s page on cost-effectiveness argues that this is misleading</a>&#xA0;and GiveWell is not inclined to take these estimates literally.</p>\n<p>&#xA0;</p>\n<p><strong>Shortcomings with Cost-Effectiveness Estimates</strong></p>\n<p>As early as 2008, GiveWell pointed out that while they find the idea of a cost-effectiveness estimate to be an attractive way to compare causes, there are shortcomings to these estimates, such as <a href=\"http://blog.givewell.org/2008/08/22/dalys-and-disagreement/\"> incorporating value judgements that may differ dramatically between different people </a> . In 2010, <a href=\"http://blog.givewell.org/2010/03/19/cost-effectiveness-estimates-inside-the-sausage-factory/\"> GiveWell elaborated on more shortcomings: </a></p>\n<ul>\n<li>\n<p>Frequently, cost-effectiveness estimates like DALYs or QALYs are point-value estimates that do not properly express the range of uncertainty involved.</p>\n</li>\n<li>\n<p>These estimates often are based on ideal versions of the program and don&#x2019;t take into account possible errors or the idea that efficacy may decline over time.</p>\n</li>\n<li>\n<p>Estimates of particular interventions (e.g., deworming) do not take into account the large amount of variation that comes from different organizations implementing the same program in different contexts with different abilities.</p>\n</li>\n<li>\n<p>Lastly, these estimates <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\"> frequently ignore indirect effects</a>.</p>\n</li>\n<li>\n<p>The complexity of these estimates provide a large opportunity for model error. For example, in 2011, <a href=\"http://blog.givewell.org/2011/09/29/errors-in-dcp2-cost-effectiveness-estimate-for-deworming/\"> GiveWell found five separate errors in a DCP2 DALY figure for deworming</a>&#xA0;that, combined, made the estimate off by a factor of 100x.</p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<p>GiveWell <a href=\"http://blog.givewell.org/2011/11/04/some-considerations-against-more-investment-in-cost-effectiveness-estimates/\"> summarizes these problems into three core issues</a>: cost-effectiveness estimates are frequently oversimplified (ignoring important indirect and long-term effects), overly sensitive (with small changes in assumptions producing big changes in value), and unverified (with errors persisting unnoticed for years).</p>\n<p>&#xA0;</p>\n<p><strong>Cluster Thinking and Extreme Model Uncertainty</strong></p>\n<p>These discussions led Holden Karnofsky to articulate a mathematical framework for assessing cost-effectiveness estimates that take into account a level of rigor in <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">&quot;Why We Can&#x2019;t Take Expected Value Estimates Literally&quot;</a>, <a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/\">&quot;Maximizing Cost-Effectiveness via Critical Inquiry&quot;</a>, and<a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">&quot;Cluster Thinking vs. Sequence Thinking&quot;</a>, with further clarifications in <a href=\"http://www.givewell.org/modeling-extreme-model-uncertainty\">&quot;Modeling Extreme Model Uncertainty&quot;</a>. In summary, the key insight is that rather than rely on a single, explicit cost-effectiveness estimate, one ought to try to evaluate interventions from many different angles and adjust for one&#x2019;s prior expectation of the intervention and an assumption that an estimate is likely to magnify dramatic effects.</p>\n<p>Mathematically, the framework suggests that when you have a robust estimate of a particular intervention&apos;s cost-effectiveness, the key figure is how good the charity is, according to your estimations. Robustness can be achieved by (among other things) having multiple independent estimates. But when robustness is poor to moderate, variation in robustness can be as important as or more important than the point estimate. More broadly &#x2013; when information quality is good, you should focus on quantifying your different options; when it isn&#x2019;t, you should focus on raising information quality.</p>\n<p>These points arise from the fact that when conducting a cost-effectiveness estimate, one must consider one&#x2019;s prior distribution (i.e., what is predicted for the value of one&#x2019;s actions by other life experience and evidence) and the variance of the estimate error around the cost-effectiveness estimate (i.e., how much room for error the estimate has) to produce a posterior estimate for cost-effectiveness[1].</p>\n<p>The approach of looking at an organization or intervention from many different angles is further sketched out by Holden in what he calls&#xA0;<a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">&quot;cluster thinking&quot;</a>, which is distinct from what he calls &quot;sequence thinking&quot;. With &quot;cluster thinking&quot;, one seeks to evaluate a claim from multiple perspectives (or &quot;clusters&quot;) and take an aggregated approach. In contrast, &quot;sequence thinking&quot; involves combining all factors into a single, sequential line of argument, usually in the form of an explicit expected value calculation.</p>\n<p>&#xA0;</p>\n<p><strong>Communicating Uncertainty</strong></p>\n<p>This forms the basis for a general philosophy toward supporting the charities that have a combination of reasonably high estimated cost-effectiveness and maximally robust evidence. GiveWell now look for &quot;meaningful differences&quot; in modeled cost-effectiveness to determine whether an intervention appears to be meaningfully more cost-effective than GiveDirectly or direct financial grants. They also aim to determine whether there are meaningful differences between other organizations, for example, deworming verses bednets.</p>\n<p>When assessing particular interventions, GiveWell tends to avoid using &quot;error bars&quot; and prefers to display their best guess values and state the reasons for uncertainty qualitatively instead of quantitatively, while urging people to not take the best guess value literally.</p>\n<p>GiveWell does not aim to adjust downwards for uncertainty. Instead they aim to make best guesses about the key parameters that affect the final estimate. For example, <a href=\"https://docs.google.com/spreadsheets/d/1KiWfiAGX_QZhRbC9xkzf3I8IqsXC5kkr-nwY_feVlcM/edit#gid=472531943\"> GiveWell&apos;s evaluation of deworming</a>&#xA0;includes a discount of 100x in the weighting of estimates from a key supporting study, due to concerns about replicability /external validity for this study.</p>\n<p>More generally, GiveWell has noticed a trend that cost-effectiveness estimates frequently become worse (and rarely become better) upon further investigation, and occasionally they adjust estimates downward to try to &#x201C;get ahead&#x201D; of this trend when first looking at new interventions. However, in the <a href=\"http://blog.givewell.org/2016/03/10/march-2016-open-thread/\"> March 2016 open thread</a>, GiveWell expressed that staff have differing opinions on these uncertainty-based downward discounts and that they could appear overly subjective and controversial.</p>\n<p>As an example of some of the uncertainty considered, <a href=\"http://www.givewell.org/charities/schistosomiasis-control-initiative#Sourcesofuncertainty\"> GiveWell wrote an uncertainty section in their 2016 review of SCI&#x2019;s room for more funding</a>, highlighting context specific issues such as political unrest, expiring drug supplies, additional donated drugs becoming available, delays and budget changes due to coordination with other actors, results of disease mapping, and grants from other donors. GiveWell additionally acknowledged that they do not place much weight on the preliminary room for more funding estimates for SCI&apos;s work in 2017-2018. In the consideration of the final estimate of cost per treatment delivered, GiveWell emphasized that their estimate relies on a number of uncertain assumptions.</p>\n<p>&#xA0;</p>\n<h2>Animal Charity Evaluators</h2>\n<p>Just as GiveWell recommends the best non-profits for improving global health, ACE analyzes the effectiveness of different ways to improve the wellbeing of nonhuman animals. This is a task rife with uncertainty as the empirical record for many of the analyzed interventions is sparse or nonexistent. Additionally, ACE has to consider very difficult and unresolved questions like animal consciousness and wild animal suffering. <a href=\"/ea/14g/thoughts_on_the_reducetarian_labs_mturk_study/\">&quot;Thoughts on the Reducetarian Study&quot;</a>&#xA0;contains a review of existing empirical pro-vegetarian intervention research and their limitations.</p>\n<p>ACE rates each charity on <a href=\"https://animalcharityevaluators.org/approach/evaluating-charities/evaluation-criteria-charities/#full-criteria-template\"> seven specific criteria</a>[3] -- an assessment of room for more funding, an explicit cost-effectiveness estimate, a more general assessment of individual intervention work having high implicit cost-effectiveness estimates, and four qualitative assessments of organizational health (e.g., track record, strong leadership, good culture). This could be seen as a form of cluster thinking where ACE looks at an organization in numerous ways to come to a holistic assessment, in which the impact of a cost-effectiveness calculation affects at most three of the seven criteria.</p>\n<p>ACE has <a href=\"https://animalcharityevaluators.org/blog/some-thoughts-on-our-cost-effectiveness-estimates/\"> historically had challenges communicating cost-effectiveness information</a>&#xA0;and now favors expressing their uncertainty about their estimates in the form of a range. For example, an estimated aggregation over all of Mercy for Animal&#x2019;s activities produced an estimate of <a href=\"https://animalcharityevaluators.org/research/charity-review/mercy-for-animals/#c2\"> -4 and 20 years of suffering averted per dollar spent</a>. To construct this estimate, ACE uses a third-party program called <a href=\"https://www.getguesstimate.com/\">Guesstimate</a> that can create and aggregate various confidence intervals, such as <a href=\"https://www.getguesstimate.com/models/7294?token=aYbYkohTp6snq7jhOvTbK9CsKZ7top-LQ204d35AmrlyDKkm9DGGvmXTWlhVgfaDUERNcuRQn67-n6mXQVMZVQ\"> an estimate for all of MFA&#x2019;s activities</a>.</p>\n<p>In addition to using Guesstimate, ACE also provides spreadsheet-driven calculators such as their <a href=\"https://docs.google.com/spreadsheets/d/13cB7afTewFAlx-VdCZ6BPgqqdEpbrMFSWr-GGOTApsk/edit#gid=1263869078\"> social media calculator</a>, that aims to have three point estimates for every parameter -- a pessimistic (conservative) view, a realistic (best guess) view, and an optimistic view. Each category is aggregated individually, creating three final estimates that are also pessimistic, realistic, and optimistic. The Guesstimate blog <a href=\"https://medium.com/guesstimate-blog/the-flaws-of-best-worst-case-analyses-cbb22c46f9ac#.o3vrkhwfc\"> argues that this form of approach leads to inflated ranges</a>, with the pessimistic final view being extra pessimistic and the optimistic final view being extra optimistic, though the central, realistic, best guess view should remain unaffected.</p>\n<p>Currently, ACE&#x2019;s estimates are based almost entirely on short-term effects. ACE <a href=\"https://animalcharityevaluators.org/blog/how-should-we-account-for-the-potential-long-term-impact-of-organizations/\"> has thought a lot about how to include long-term effects</a>, but currently does not include these in their models due to an incommensurable level of bias relative to short-term estimates, a very high degree of uncertainty, and estimates being dominated by subjective judgment calls.</p>\n<p>ACE also considered a robustness adjustment to their estimate (i.e., reducing estimates downward that are less robust), but <a href=\"https://animalcharityevaluators.org/blog/some-thoughts-on-our-cost-effectiveness-estimates/\"> decided not to do this</a>&#xA0;due to concerns about too much subjectivity and thinking that estimating uncertainty of individual parameters and communicating the overall range should be sufficient to account for most of the impacts of robustness.</p>\n<p>&#xA0;</p>\n<h2>80,000 Hours</h2>\n<p><a href=\"https://80000hours.org/about/credibility/research-principles/\"> In the outline of their Research Principles</a>, 80,000 Hours state they strive to employ Bayesian reasoning in their analysis of career decisions, through clarifying a prior guess on an issue, such as the potential benefits of a particular career path, by updating in or out of favour based on the strength of the evidence. They state that Bayesian reasoning is regarded as the best practice for decision-making under high uncertainty. They use their research principles as aspirational goals to inform their programs.</p>\n<p>In the face of uncertainty, 80,000 Hours also uses cluster thinking -- instead of relying upon one or two strong considerations, they consider the question from a broad variety of angles and talk to people with different views, weighing each perspective according to the robustness and importance of the potential consequences. They additionally seek to avoid bias by aiming to make their research transparent and aiming to state their initial position so readers can spot any potential sources of bias and receive feedback from experts on sources of uncertainty.</p>\n<p>In order to develop &#x201C;workable assumptions&#x201D;, 80,000 Hours generally <a href=\"https://80000hours.org/2013/10/linearity-a-useful-assumption-in-evaluating-careers-and-causes/\">adopts an assumption of linearity</a>. For instance, they assume that the value of a resource is likely to be linear when considering changes that are a small fraction of the current supply of that resource. When consuming a resource, the overall effect is therefore very likely to be diminishing through most of the range; and is likely to be increasing only as one comes to control the majority of that resource, and even then only in some cases. For example, a donation of $200 is likely to be twice as good as $100.</p>\n<p>Rob Wiblin emphasized that 80,000 Hours use their research principles as aspirational goals to inform their programs. He additionally drew our attention to the following ideas:</p>\n<ul>\n<li>\n<p>taking a &apos;risk management&apos; approach to existential risk, thinking of it as insurance against the possibility that we really are dealing with an exceptional case here, i.e., the possibility that the <a href=\"https://80000hours.org/2012/12/how-to-judge-your-chances-of-success/\"> inside view</a>&#xA0;is right,</p>\n</li>\n<li>\n<p>giving some weight to common sense,</p>\n</li>\n<li>\n<p>doing things that aren&apos;t going to be a disaster, making sure that nothing they do will be catastrophically bad even if they misunderstood the situation.</p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<h2>Centre for the Study of Existential Risk</h2>\n<p>The Centre for the Study of Existential Risk is an interdisciplinary research centre within the University of Cambridge, dedicated to the study and mitigation of human extinction-level threats[4]. As an academic institution, CSER does not have a corporate position on the evaluation of outcomes and risks, and contains a wide range of views. The following is therefore based on a specific project they are undertaking to develop a policy focused framework for the evaluation of Extreme Technological Risks (ETRs), i.e., technological developments that pose new existential risks. CSER <a href=\"http://cser.org/research/current-projects/\">states that</a>&#xA0;standard cost-benefit analysis has serious deficiencies in the evaluation of ETRs and that the science of ETR management needs to take this into account when drawing conclusions about mitigating ETRs compared to other global priorities. Since much of the difficulty in evaluating ETRs stems from the significant degree of uncertainty about their risks and benefits, much of CSER&#x2019;s work in this area involves developing alternatives to cost-benefit analysis that are better suited to evaluation under uncertainty.&#xA0;</p>\n<p>One problem with using cost-benefit analysis to evaluate ETRs is that existential threats usually emerge only in worst case scenarios. Such scenarios are often very unlikely to occur. However, given that the costs associated with human extinction level threats are many orders of magnitude greater than those associated with the next worse scenario, such as a global catastrophe, they may significantly alter the overall balance of costs and benefits associated with developing a technology. One practical implication of this is that existential risk mitigation involves making predictions and preparing for outcomes that will be much worse than what we would expect to see in most outcomes. In practice, cost-benefit analyses often exclude or ignore such tail risks and most organizations are sensitive to being seen as making predictions that are persistently proven to be incorrect, so CSER is keen to identify when such pessimism is most justified and support those who it views as responding correctly to these kinds of extreme tail risk.</p>\n<p>However, CSER&#x2019;s work also goes beyond such practical concerns. They are also concerned that within the significant degree of uncertainty that surround ETRs and other existential risks there may be other morally salient concerns that go beyond what is captured by standard cost benefit analysis. CSER use a fundamentally normative approach to identify and evaluate these concerns to create a framework that identifies where we have the greatest ethical imperative for precaution in the face of uncertainty, and where a more balanced approach to weighing costs and benefits remains appropriate. Three key issues in this analysis are population ethics, fairness and temporal discounting:</p>\n<p><strong>Population ethics</strong></p>\n<p>Philosophical debates about the value of future lives have thrown up many intriguing axiological theories, implying that one cannot directly derive the value of a life from their level of well-being, let alone the interpretation of these wellbeing levels in monetary terms as required by cost-benefit analysis. Some people, such as <a href=\"http://media.philosophy.ox.ac.uk/moral/HT15_DP.mp3\"> Derek Parfit</a>, have proposed a lexical theory about the value of lives, in which certain goods, such as science, friendship and culture, are morally more significant than any amount of individual welfare on its own. If taken seriously, this view greatly reduces the importance some some kinds of moral uncertainty. For instance, it implies that it does not matter if we do not know what the welfare costs and benefits of a technology will be if it threatens the existence of these &#x2018;perfectionist goods&#x2019;. There are several ways of incorporating such concerns into an evaluative framework, for instance by adopting a form of <a href=\"http://lesswrong.com/lw/kpr/population_ethics_in_practice/\"> critical-level utilitarianism </a> (giving priority to lives that are above some &#x2018;critical level&#x2019; of welfare) or by <a href=\"http://www.patheos.com/blogs/unequallyyoked/2015/03/effective-altruism-ethically-questionable-cookies.html\"> implementing a more pluralist approach to moral value</a>. As a starting point, CSER is analyzing whether possible future scenarios provide the potential resources necessary to foster perfectionist values at all, since this may be morally equivalent to the question of whether they pose an existential threat.</p>\n<p><strong>Fairness</strong></p>\n<p>Sometimes our evaluation of an action is sensitive to more than just its costs and benefits, but also <a href=\"http://cser.org/simon-beard-evaluating-risks/\"> the ways in which these come about and their distribution</a>. This view is common amongst a variety of moral theories, although it can be articulated in many ways. CSER is currently investigating accounts of fairness that allow us to integrate such concerns with a suitably consequentialist and aggregative approach to evaluating risks, for instance the <a href=\"https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjmuIm8qvnSAhWDJcAKHWkoDh0QFggaMAA&amp;url=http%3A%2F%2Feprints.lse.ac.uk%2F55883%2F1%2F__lse.ac.uk_storage_LIBRARY_Secondary_libfile_shared_repository_Content_Voorhoeve%2C%2520A_How%2520should%2520we%2520aggregate_Voorhoeve_How%2520should%2520we%2520aggregate_2014.pdf&amp;usg=AFQjCNFbC4almvLsKvlzmcnvhojw1xfvvQ&amp;sig2=cXpKonUYsTNFFi7gIm1egQ&amp;bvm=bv.150729734,d.ZGg\">&quot;aggregate relevant claims&quot;</a>&#xA0;view. By introducing new evaluative frameworks such views have the potential to remove large amounts of evaluative uncertainty. For instance, on some of these views it is always better to save a single life than cure any number of headaches, rendering any uncertainty over the number of headaches that one might potentially cure morally insignificant. Such views are already being used to evaluate public health choices, but are yet to be studied in the evaluation of technological risks.</p>\n<p><strong>Discounting</strong></p>\n<p>The relationship between uncertainty and the social discount rate (i.e. whether the fact that a cost or benefit will occur in the future makes it less important than if it occurred in the present) may seem less obvious. However, many theories about why we should discount future harms and benefits actually imply that we should use different discount rates for different kinds of costs and benefits. Whilst it seems legitimate to impose quite a high temporal discount rate on future benefits where these take the form of additional wellbeing for those who are already well off, the discount rate should be lower for assessing costs or the wellbeing of those who are worse off and should be <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/1758-5899.12318/abstract\"> even lower, or potentially negative, for costs associated with global catastrophes</a>. This result is in fact well known in theory, but generally gets lost in practice when it is easiest to apply a single social discount rate to all costs and benefits, regardless of when and where they fall. One upshot is that as we move further into the future it may matter less and less just how big certain costs and benefits are, and much more whether or not there could be an extreme event such as human extinction or a global catastrophe.</p>\n<p><br> CSER hopes that by developing these lines of research it will be possible to produce a middle way between Cost Benefit Analysis, which is often far too insensitive to risk and uncertainty, and a more blanket precautionary approach, which tends to overreact to it, yielding irrational results. This will form the basis of their integrated approach to managing extreme technological risks.</p>\n<p>CSER is also interested in uncertainty in other areas that, although unlikely to produce existential threats in themselves, play an important role in framing humanity&apos;s future. One example is the evaluation of scientific research, where they are concerned that a reliance on overly precise risk-benefit assessments of research when there is significant uncertainty about the actual research outcomes produces no real improvement in the quality of research output, but does encourage the perpetuation of <a href=\"https://aeon.co/ideas/science-funding-is-a-gamble-so-lets-give-out-money-by-lottery\"> selection bias and other irrationalities in the kinds of scientific research that is undertaken and promoted</a>.</p>\n<p>&#xA0;</p>\n<h2>Endnotes</h2>\n<p><strong>[1]:</strong> Peter Hurford is an independent researcher who works as a data scientist and is on the board of Charity Science Health, .impact, and Animal Charity Evaluators. Kathryn Mecrow is a member of the Operations Team of the Future of Humanity Institute. Simon Beard is a Research Associate at the Centre for the Study of Existential Risk at the University of Cambridge. We also thank (in no particular order) Michelle Hutchinson at the Oxford Institute for Effective Altruism, Rob Wiblin at 80,000 Hours; Allison Smith at Animal Charity Evaluators; Amanda Askell; and Elie Hassenfeld, Rebecca Raible, and Holden Karnofsky at GiveWell for answering initial questions that allowed us to research this essay and for reviewing a final draft of this essay prior to publication.</p>\n<p><strong>[2]:</strong> As a concrete example, Michael Dickens <a href=\"/ea/vo/expected_value_estimates_you_can_maybe_take/\"> elaborates on how to use this framework to produce cost-effectiveness estimates </a> for various different causes that may be more directly comparable, even across multiple levels of rigor. GiveWell also produces <a href=\"http://www.givewell.org/modeling-extreme-model-uncertainty/example\"> a worked example </a> showing mathematically how one might combine three different models demonstrating uncertainty about investing in a start-up.</p>\n<p><strong>[3]:</strong> Note that one of the three authors, Peter Hurford, serves as the Treasurer of the ACE board and was the original designer of ACE&#x2019;s evaluation criteria as an ACE volunteer. This could introduce potential bias when discussing this section.</p>\n<p><strong>[4]:</strong> Note that one of the three authors, Simon Beard, is a research associate at the Centre for the Study of Existential Risk and works on their project Evaluating Extreme Technological Risks. This could introduce potential bias when discussing this section.</p></body></html>", "user": {"username": "Peter_Hurford"}}, {"_id": "GdWyH3ScsjvSXhK6h", "title": "Surviving Global Catastrophe in Nuclear Submarines as Refuges", "postedAt": "2017-04-05T08:06:31.780Z", "htmlBody": "<html><body><p>Our&#xA0;article about using nuclear submarines as refuges in case of a global catastrophe has been accepted for the <em>Futures</em>&#xA0;journal and its preprint is available online. Preventing global risks or surviving them is good application of EA efforts. Converting existing nuclear submarines into refuges may be cheap intervention with high impact.&#xA0;</p>\n<h1>Aquatic Refuges for Surviving a Global Catastrophe</h1>\n<ul>\n<li><a href=\"http://www.sciencedirect.com/science/article/pii/S0016328716303494?np=y&amp;npKey=6dcc6d35057e4c51bfd8d6933ab62c6d4a1604b5b71a40f060eb49dc7f42c9a1\">Alexey Turchin</a><a title=\"Affiliation: a\" href=\"http://www.sciencedirect.com/science/article/pii/S0016328716303494?np=y&amp;npKey=6dcc6d35057e4c51bfd8d6933ab62c6d4a1604b5b71a40f060eb49dc7f42c9a1#aff0005\"><sup>a</sup></a><sup>,</sup><a title=\"Corresponding author contact information\" href=\"http://www.sciencedirect.com/science/article/pii/S0016328716303494?np=y&amp;npKey=6dcc6d35057e4c51bfd8d6933ab62c6d4a1604b5b71a40f060eb49dc7f42c9a1#cor0005\"></a>, &#xA0;</li>\n<li><a href=\"http://www.sciencedirect.com/science/article/pii/S0016328716303494?np=y&amp;npKey=6dcc6d35057e4c51bfd8d6933ab62c6d4a1604b5b71a40f060eb49dc7f42c9a1\">Brian Patrick Green</a><a title=\"Affiliation: b\" href=\"http://www.sciencedirect.com/science/article/pii/S0016328716303494?np=y&amp;npKey=6dcc6d35057e4c51bfd8d6933ab62c6d4a1604b5b71a40f060eb49dc7f42c9a1#aff0010\"><sup>b</sup></a><sup>,&#xA0;</sup></li>\n</ul>\n<div>\n<h2>Abstract</h2>\n<p>Recently many methods for reducing the risk of human extinction have been suggested, including building refuges underground and in space. Here we will discuss the perspective of using military nuclear submarines or their derivatives to ensure the survival of a small portion of humanity who will be able to rebuild human civilization after a large catastrophe. We will show that it is a very cost-effective way to build refuges, and viable solutions exist for various budgets and timeframes. Nuclear submarines are surface independent, and could provide energy, oxygen, fresh water and perhaps even food for their inhabitants for years. They are able to withstand close nuclear explosions and radiation. They are able to maintain isolation from biological attacks and most known weapons. They already exist and need only small adaptation to be used as refuges. But building refuges is only &#x201C;Plan B&#x201D; of existential risk preparation; it is better to eliminate such risks than try to survive them.</p>\n</div>\n<h2>Keywords</h2>\n<ul>\n<li><span>global catastrophic risk</span>;&#xA0;</li>\n<li><span>existential risk</span>;&#xA0;</li>\n<li><span>refuges</span>;&#xA0;</li>\n<li><span>disaster shelters</span>;&#xA0;</li>\n<li><span>social collapse</span>;&#xA0;</li>\n<li>human extinction</li>\n</ul>\n<div>&#xA0;</div>\n<h2>Highlights</h2>\n<p>&#xA0;</p>\n<dl>\n<dt>&#x2022; Nuclear submarines could be effective refuges from several types of global catastrophes.</dt>\n<dt>&#x2022; Existing military submarines could be upgraded for this function with relatively low cost.</dt>\n<dt>&#x2022; Contemporary submarines could provide several months of surface independence.</dt>\n<dt>&#x2022; A specially designed fleet of nuclear submarines could potentially survive years or even decades under water.</dt>\n</dl>\n<p>&#xA0;</p>\n<dl>\n<dd>\n<p>&#xA0;</p>\n</dd>\n</dl>\n<p>Full text:&#xA0;http://www.sciencedirect.com/science/article/pii/S0016328716303494?np=y&amp;npKey=6dcc6d35057e4c51bfd8d6933ab62c6d4a1604b5b71a40f060eb49dc7f42c9a1&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "turchin"}}, {"_id": "AbrL4M9sPsqZWBio2", "title": "Students for High-Impact Charity Interim Report", "postedAt": "2017-04-03T22:27:29.233Z", "htmlBody": "<html><body><p>SHIC has released an Interim report detailing our progress through 2016. Read the Executive Summary below, or <a href=\"https://drive.google.com/file/d/0B_YyyNcGXfb6ZVljWFVSdThVVjg/view\">view the full report here</a>.</p>\n<p>***</p>\n<h1>SHIC Interim Report: Executive Summary</h1>\n<p>This report comes following the close of our first semester of operation, four months after the organization secured target funding to continue testing pilot programs through to 2017. Since the soft launch in March 2016, we have seen encouraging levels of uptake and largely positive feedback about the influence of our program. While more pilot testing is necessary in order to make definitive judgements on SHIC as a whole, we feel that we have gathered enough data to guide strategic changes to this exceedingly novel project.</p>\n<h2>SHIC programs have covered significant ground.</h2>\n<p>As of December 2016, approximately 750 students have participated in 1500 program hours. More than 200 students have completed the entire SHIC pilot, and the program prompted just over $1500 USD to high-impact charity in its first fundraising cycle. An estimated 50 operations volunteers have put in 3,300 hours of work into bringing SHIC to students worldwide.</p>\n<h2>We&apos;re optimistic about SHIC&apos;s influence.</h2>\n<p>The majority of qualitative responses and self-reported interim survey data indicated notable changes in perspective and intended behavior among SHIC leaders and operations volunteers. This bodes well for our &#x2018;volunteering as outreach&#x2019; strategy. Any trends we&#x2019;ve observed, however, are not yet backed to a meaningful degree of statistical significance. Early data from the before-after participant survey are encouraging and provide an added level of quantitative rigor.</p>\n<h2>Performance metrics reshaped our outlook.</h2>\n<p>We achieved our outreach goals, underestimated program exposure, overestimated potential fundraising totals, stalled on school accreditation, and postponed plans for revenue generation. The novelty of SHIC made for tricky forecasting, and while we feel the program outperformed in many ways, missing some of our metrics warrants a reevaluation.</p>\n<h2>Our approach has adapted with feedback.</h2>\n<p>Quantitative and qualitative feedback has informed several strategic adjustments. We&#x2019;re implementing a more coherent framework for volunteer engagement, restructuring the entire curriculum development process, and weighting direct outreach strategies differently than before.</p>\n<h2>We&apos;re seeing signs of growth potential.</h2>\n<p>The apparent demand for SHIC worldwide has been encouraging thus far. SHIC is currently represented by students in more than a dozen countries, and we&#x2019;ve collected survey data that indicates further demand for this program elsewhere. Roughly half of ambassadors and operations volunteers found out about SHIC through volunteer websites unrelated to the effective altruism community, which speaks to the program&#x2019;s broad appeal.</p>\n<p>***</p>\n<p>Read the rest of the report <a href=\"https://drive.google.com/file/d/0B_YyyNcGXfb6ZVljWFVSdThVVjg/view\">here</a>. Feedback and questions are welcome! Thanks to the EA community for helping us get this far!</p></body></html>", "user": {"username": "baxterb"}}, {"_id": "Rh5a5CHhpP955Winn", "title": "Applications are open for EA Global Boston", "postedAt": "2017-04-01T16:01:18.215Z", "htmlBody": "<html><body><p><a href=\"https://www.eaglobal.org/apply/boston/\">Applications for EA Global Boston are now open</a><span>!</span><br><br><span>Our plan is to respond to applications within seven days of receiving them. If you haven&#x2019;t heard from us by then, please check any filtered inboxes or email us at hello@eaglobal.org.</span><br><br><span>The early bird price for Boston is $334 until April 30. We don&apos;t want cost to keep anyone away, so we plan to have plenty of financial aid available during the registration process. This is true even if you are not low-income but, for example, are prioritizing donation. Please take financial aid rather than stay home!</span><br><br><span>This event is a smaller, focused conference about pushing the frontiers of Effective Altruism. We&#x2019;ll feature speakers on policy and science and explore how to think about speculative topics. Current speakers include:</span></p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/George_M._Church\">George Church</a></li>\n</ul>\n<ul>\n<li><a href=\"https://medium.com/mit-media-lab-digital-currency-initiative/whats-new-at-mit-s-digital-currency-initiative-9d53c39502d3\">Neha Narula</a></li>\n</ul>\n<ul>\n<li><a href=\"http://www.adammarblestone.org/\">Adam Marblestone</a></li>\n</ul>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Jim_O&apos;Neill_(investor)\">Jim O&#x2019;Neill</a></li>\n</ul>\n<ul>\n<li><a href=\"http://philosophy.fas.nyu.edu/object/philo.people.amandaaskell\">Amanda Askell</a></li>\n</ul>\n<ul>\n<li><a href=\"https://industry.datascience.columbia.edu/profile/vikash-mansinghka\">Vikash K. Mansinghka</a></li>\n</ul>\n<p><span>Please submit additional speaker recommendations&#xA0;</span><a href=\"https://eaglobal.typeform.com/to/dg3OxS\">here</a><span>.</span><br><br><span>Applications for EA Global San Francisco (August 11-13) will open shortly. San Francisco will be the larger, more community-focused conference. Applications for EA Global UK (October or November) will follow.</span><br><br><span>We look forward to seeing you at EA Global!</span></p></body></html>", "user": {"username": "AmyLabenz"}}, {"_id": "otwnXitAXq5iQiHPi", "title": "An Effective Altruist Message Test", "postedAt": "2017-04-01T14:45:16.674Z", "htmlBody": "<html><body><p><span>I decided to run an Effective Altruist message on a full population survey I have access to, use bayesian message testing software to analyze the results, and share the results with the EA community on the forum. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>I tested several EA themed messages aimed at increasing respondents&#x2019; interest in donating and effective altruism. For non-control respondents, I presented them with either the claim that AMF can save a life for 3,500 dollars (Facts), a short version of Peter Singer&#x2019;s Pond analogy (Obligation) or a short take on Will MacAskill&#x2019;s opportunity framing (Opportunity).</span></p>\n<p>&#xA0;</p>\n<p><span>I then asked respondents how much they planned to donate in the next 12 months, their interest in EA and gave them the opportunity to click on a link to donate to the Against Malaria Foundation (AMF) or potentially join Giving What We Can (GWWC). I recorded whether they clicked on these links. Additionally, I asked how much they donated in the last 12 months as a &#x201C;prescreen&#x201D; to control for in my analysis, dramatically increasing my precision. The wording of each of the questions described above is in a doc linked to at the bottom of this article.</span></p>\n<p>&#xA0;</p>\n<p><span>Overall, my results indicate that these brief messages do not increase respondents perspective donations, but there is some evidence that the facts and obligation message may increase interest in EA among educated individuals. Below, I discuss my results in more detail.</span></p>\n<p><span>&#xA0;</span></p>\n<p><span>Methods</span></p>\n<p><span>This was embedded in a 1200 person online survey representative of US citizens. Within this survey, I delivered 3 treatments (Facts, Obligation and Opportunity) each with a sample size of ~200 respondents. I compared how interested respondents &#xA0;were in learning more about effective altruism and how much they planned to donate. I examined these relationships overall and among the critical subgroup of those with at least a bachelor&#x2019;s degree (this was my only pre-planned comparison; having a bachelor&#x2019;s degree serves as a rough proxy for the EA target elite audience). Unfortunately, only 10 respondents took substantive action by clicking on the AMF donation or GWWC links, so this sample size was not sufficient to conduct robust analysis on this dependent variable.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>To analyze this relationship, I used Bayesian hierarchical modeling software based in R and Stan. This modeling approach allows us to create a probability distribution of the treatment effects, reduce variance by controlling for other variables in the survey and borrow power from the full sample when estimating effects among subgroups.</span></p>\n<p>&#xA0;</p>\n<p><span>Future Donation Plans</span></p>\n<p><span>To gauge how the EA messages changed people&#x2019;s donations, I predicted the probability that respondents plan to donate at least 6% of their income to charity. Below are treatment effects and probability distributions for the effect of each message. Error bars are one standard error.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span><img src=\"https://lh4.googleusercontent.com/cKSKawTQEsF4Mjyv4kBTapqdQqxNOW2rYVq3kxwchP1DEDy2tSCsmHFTmUdrpjVm8Iafil2z2nCXAUSYol-VUj0j309OZYpRQElNPyKKBJ4HJA3oVy6GEXlaL62LGFQdZMIw6gm7\" alt=\"Screen Shot 2017-03-29 at 9.04.26 PM.png\"></span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span><img src=\"https://lh6.googleusercontent.com/sT3MgyI0VTwXjuFP-Tr8aMD_eYlq1D4te-00IFpUk3ZrVLDM1-0oQw7tkJZy88Up_109nRSYX6NxaOmZOUFPcvkjkfEUW00J2y4fTRAPrOzUulWk6FvAba2jyNKv3ZWmKbszeWB0\" alt=\"Screen Shot 2017-03-29 at 9.03.55 PM.png\"></span></p>\n<p><strong><br><br></strong></p>\n<p><span>As evidenced by the plots above, none of the treatments had a large impact. In fact, the impact such as it is is negative. The impact is minute, with Average Treatment Effects (ATEs) well under 1%. Even in the most optimistic case, few are likely to increase their expected donation behaviors in response to the arguments provided. This is not surprising; it can take a lot for people to change substantial behaviors, or even suggest they may change them in a survey. There&#x2019;s little evidence that any argument is more effective, and will require other means to assess. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Interest in Effective Altruism</span></p>\n<p><span>I also examined interest in Effective Altruism (whether respondents are at least somewhat interested in learning more about EA). Below are treatment effects and probability distributions for the effect of each message.</span></p>\n<p><span><img src=\"https://lh5.googleusercontent.com/-_tCSpeqp-Rl-MrWVrEMVR8MQ9edu_HG2v4YIfO5UE1bLqYuaOvbAGpvEaIChuy9tqFiSoHIu2yW1cjGyAI_isXzfF5mFVbXKh7ps__7n2iO65pCRQbzir9shon6-0nafiTf2Mnl\" alt=\"Screen Shot 2017-03-29 at 9.10.34 PM.png\"></span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span><img src=\"https://lh6.googleusercontent.com/d1ueFcW0PehhB4APu7GQ8DWkbItqDowoeTXoDG_iFkNGhPXbbVW5Abp0Oo8w_YuR2CzLMSDKFG1MmzfksxUP9zfbe1CqGrPWjb0t3Os_KsZUXEvgA_hTCHFQ2y70HZhak_34xGa5\" alt=\"Screen Shot 2017-03-29 at 9.09.51 PM.png\"></span></p>\n<p><span>Unlike perspective donations, interest exhibits substantial variation between treatments. While the opportunity message appears to actively put people off, letting people know they can save lives for $3,500 does appear to get folks interested in effective altruism, and may be a good way to get them in the door. However, the most interesting ATEs appear when breaking the results out by education level. Below are ATEs, probabilities that each message is best, and probabilities of backlash (a negative effect) for each message among those with at least a bachelor&#x2019;s degree.</span></p>\n<p><span><img src=\"https://lh3.googleusercontent.com/3BP0MR6fOv9XBhPo2V7huWjCMS4OuF6n65T503gywtApzCX20i745YKXAkmm0QFalyRTpdmQ6fCkFg8HTmAwDvPjPa5kpzbNKVW92_lIiY3Lt7rCq0eQJMHEhSIjLOsJQbnS-33w\" alt=\"Screen Shot 2017-03-29 at 9.12.33 PM.png\"></span></p>\n<p><span>There&#x2019;s a large gap in the impact these messages have on those with and without a bachelor&#x2019;s degree. For those with a bachelor&#x2019;s degree, every message has a positive ATE, and the effects of obligation and facts are quite substantial (they increase the chance individuals are at least somewhat interested in learning more about EA by over 6 percentage points). However, there&#x2019;s a substantial amount of variance in these effect sizes. The true effect of these interventions could credibly range from slightly negative to over 15 percentage points, but most of the probability mass lies in the middle of this range. Overall, Singer&#x2019;s pond analogy and the scope of impact people can have with their donations appear to be effective methods of building interest in EA; there&#x2019;s around a 90% chance that one of these is the best message (out of the three and the control) that you can use with individuals having at least a bachelor&#x2019;s degree.</span></p>\n<p>&#xA0;</p>\n<p><span>However, the results are much different for those without a bachelor&apos;s degree. Only the factual argument appears to have any positive effect, and they appear to be turned off by the opportunity and obligation framings. This confirms what we already suspected, prospective EAs are likely to be educated elites and it makes sense to target them.</span></p>\n<p>&#xA0;</p>\n<p><span>Discussion and caveats</span></p>\n<p><span>Overall, these findings are mixed, but not surprising. EA messaging does work to get people interested in Effective Altruism. However, EA messaging alone is not enough to get people to even claim that they will increase their donations. This likely takes a much more substantial treatment. But getting educated elites interested in Effective Altruism is the first step. By emphasizing the moral reasoning behind effective altruism and the scale of good we can do in the world, we can encourage people to learn more about Effective Altruism. From there, we can change their behavior.</span></p>\n<p>&#xA0;</p>\n<p><span>Like all research, this is limited. For one, the individuals targeted and the context are not entirely typical. EA messaging will tend to come from friends and acquaintances in person or in discussions online rather than as an anonymous message in a web survey. People may react differently in other situations, but this study does provide an important piece of experimental evidence that can inform how we try to engage people. Additionally, having a bachelor&#x2019;s degree is not enough to be in EA&#x2019;s core audience. EAs as a whole tend to be analytically oriented and are often mathematical in their thinking. This population is not as restrictive as typical perspective EAs. The good news is that both of these differences suggest the true effect size may be larger. A more personal contact to an even better target may be very effective at encouraging people to join EA. Indeed, that may help explain EA&#x2019;s substantial growth. However, in another way, the audience is overly restrictive; only US citizens were included. Different messages may be more effective in other countries.</span></p>\n<p>&#xA0;</p>\n<p><span>This study is a step forward, it provides some evidence on what treatments work best and what we can accomplish with a contact. As always, more research (both observational and experimental) is needed. As our community engages in more trials to test our messaging, we can continue to fine tune it and expand the appeal of EA.</span></p>\n<p>&#xA0;</p>\n<p><span>Thanks to Kerry Vaughan for advice on message choice.</span></p>\n<p>&#xA0;</p>\n<p><a href=\"https://docs.google.com/document/d/1IBm0z_uVDMqS9kwUMdwymJwjT81-7hGpv6wwmbpF4ic/edit?usp=sharing\"><span>Question Wording</span></a></p></body></html>", "user": {"username": "Michael_S"}}, {"_id": "XHuBagT6n7yRKobig", "title": "A Third Take on Trump", "postedAt": "2017-03-31T18:21:49.204Z", "htmlBody": "<html><body><p>Following two <a href=\"/ea/14r/a_different_take_on_president_trump/\">other</a> <a href=\"/ea/146/president_trump_as_a_global_catastrophic_risk/\">posts</a>&#xA0;on Trump, I think there is a third viewpoint which seems more true after observing the first few months.&#xA0;</p>\n<p>The election of Trump could be a net positive vs Clinton, but not because he is a better president.</p>\n<p>This is based on two assumptions:</p>\n<ol>\n<li>The president&#x2019;s party loses seats at every other level of government</li>\n<li>The president doesn&#x2019;t actually have that much power</li>\n</ol>\n<p>For the first point you can see how many losses there have been for Democrats&#xA0;since 2008. Losing the House and the Senate, as well as over a thousand state legislators and going from 29 governors to 16.<a href=\"https://en.wikipedia.org/wiki/Political_party_strength_in_U.S._states\">1</a>&#xA0;This isn&#x2019;t unique to Democrats, since 1944 the president&#x2019;s party has lost, on average, 8 seats in the Senate, 36 in the House and over 450 state legislator seats.<a href=\"http://www.politico.com/magazine/story/2014/12/presidents-bad-for-their-parties-113241\">2</a>&#xA0;</p>\n<p>For the second point there is less hard evidence but even with executive orders, the majority are either statements of intent or actions that will have to be passed through congress otherwise the next president will roll it back instantly.<a href=\"http://www.washingtonexaminer.com/a-guide-to-trumps-first-17-executive-orders/article/2613318\">3</a>&#xA0;The president does have more power than any other person, but not 100% and probably not even 10%. It&#x2019;s not just the other branches of government, it&#x2019;s also state governments, media, lobbyists, industry, the bureaucracy and even the White House is split between various groups.<a href=\"https://fivethirtyeight.com/features/the-eight-power-centers-of-the-trump-administration/\">4</a>&#xA0;</p>\n<p>If we take these two assumptions as plausible and think how the next decade will pan out we can see two very different scenarios. If Clinton had won there would still be a Republican House and Senate and it would be hard to pass legislation on areas Democrats care about. It&#x2019;s likely there would be continued election losses for Democrats as their usual voters would be less motivated to vote and in 2020 the Republicans would take control of every level of government. They could be even more stable with a less divisive figure than Trump and have the presidency until 2028.</p>\n<p>Alternatively, with Trump winning in 2016, there seems to have been a large mobilisation of Democrats, registered voters more likely to turn out, &#xA0;more likely to volunteer, volunteers more likely to organise and go to town halls and stand for election.<a href=\"http://www.politico.com/story/2017/03/democrats-trump-special-elections-235692\">5</a>&#xA0;There is potential for the House to swap in 2018 and for Democrats to sweep more seats in a 2020 election against a figure at least as motivating as Obama and Clinton were for Republicans.</p>\n<p>If Clinton had won, it&#x2019;s likely Democrats would have to wait until 2028 until they control the Presidency and Congress whereas now there is a higher chance it could happen 8 years earlier, rather than waiting nearly two decades since they last held all three in 2010.</p>\n<p>&#xA0;</p>\n<p>What does that mean for individuals interested in effective altruism?</p>\n<p>If you&apos;re passionate about politics and in America, than getting involved now seems like a potentially positive action but it wont be neglected if you align with Democrat positions (but potentially easier to get involved if you are Republican).</p>\n<p>Otherwise it may be more important for politically minded people to focus on countries that have less stable political structures and more potential to improve policy.</p></body></html>", "user": {"username": "DavidNash"}}, {"_id": "nkLNAvyHqkeC6tpBv", "title": "Intuition Jousting: What It Is And Why It Should Stop", "postedAt": "2017-03-30T11:25:30.479Z", "htmlBody": "<html><body><p>Originally posted on&#xA0;<a href=\"http://www.plantinghappiness.co.uk/intuition-jousting/\">my blog</a>.</p>\n<p>Over the past year or so I&#x2019;ve become steadily more aware of and annoyed by a phenomeon I&#x2019;m going to call, for lack of a better term, &#x2018;intuition jousting&#x2019; (&#x2018;IJ&#x2019;). My experience, and obviously I can only speak for my own, is that IJ is a quite a serious phenomenon in the Effective Altruism (&#x2018;EA&#x2019;) community. It also exists amongst academic philosophers, although to much more modest extent. I&#x2019;ll explain what IJ is, why it&#x2019;s bad, why I think it&#x2019;s particularly prevalent in EA and what people should be doing instead.</p>\n<p>Intuition jousting is the act of challenging whether someone seriously holds the intuition they claim to have. The implication is nearly always that the target of the joust has&#xA0;the&#xA0;&#x2018;wrong&#x2019; intuitions. This is typically the last stage in an argument: you&#x2019;ve already discussed the pros and cons of a particular topic and have realised you disagree because you&#x2019;ve just got different starting points. While you&#x2019;ve now exhausted all&#xA0;<em>logical</em>&#xA0;arguments, there is an additional&#xA0;<em>rhetoric</em>&#xA0;move to make: claiming someone&#x2019;s fundamental (moral) instincts are just flawed. I call it &#x2018;jousting&#x2019; because all it involves is testing how firmly attached someone is to their view: you&#x2019;re trying to &#x2018;unhorse&#x2019; them. Intuition jousting is a test of psychological tenacity, not philosophical understanding.</p>\n<p>(It&#x2019;s possible there&#x2019;s already a term for this phenomenon somewhere I&#x2019;ve not come across. I should note it&#x2019;s similar to giving someone whose argument you find absurd an&#xA0;&#x2018;<a href=\"https://en.wikipedia.org/wiki/Modal_realism\">incredulous stare</a>&#x2018;: you don&#x2019;t provide a reason against their position, you just look them in the eye like they&#x2019;re mad. The incredulous stare is one potential move in an intuition joust.)</p>\n<p>To give an common example, lots of philosophers and effective altruists disagree about the value of future people. To some, it&#x2019;s just obvious that future lives have value and the highest priority is fighting existential threats to humanity (&#x2018;X-risks&#x2019;). To others, it&#x2019;s just obvious there is nothing morally good about creating new people and we should focus on present-day suffering. Both views have weird implications, which I won&#x2019;t go into here (see&#xA0;<a href=\"http://users.ox.ac.uk/~mert2255/papers/population-axiology-long.pdf\">Greaves 2015</a>&#xA0;for a summary), but conversation often reaches its finale with one&#xA0;person saying &#x201C;But&#xA0;<strong>hold on:</strong>&#xA0;you think X, so your view entails Y and that&#x2019;s&#xA0;<strong>ridiculous</strong>! You can&#x2019;t possibly think that.&#x201D; Typically at that stage the person will fold his arms (it&#x2019;s nearly always a &#x2018;he&#x2019;) and look around the room for support expecting he&apos;s now won the argument.</p>\n<p>Why do I think intuition jousting is bad? Because it doesn&#x2019;t achieve anything, it erodes community relations and it makes people much less inclined to share their views, which in turn reduces the quality of future discussions and the collective pursuit of knowledge. And frankly, it&apos;s rude to do and unpleasant to receive. I hope it&#x2019;s clear that IJing isn&#x2019;t arguing, it&#x2019;s just disagreeing about who has what intuitions and how good they are. Given that the intuitions are the things you have without reasoning or evidence, IJ has to be pointless. If you reach the stage where someone says&#xA0;&#x201C;yeah, I can&#x2019;t give you any further reasons, I just do find the view&#xA0;plausible&#x201D; and you then decide to tell them those beliefs are stupid, all you&#x2019;re doing is trying to shame or pressure them in admitting defeat so that you can win the argument. Obviously, where intuition jousting occurs and people feel they will get their personal views attacked if they share them, people will be much less inclined to cooperate or work together. There&apos;s also a very real danger of creating accidental group-think and intellectual segregation. This may already be happening: suppose members of some group IJ&#xA0;those they disagree with. Individually, people decide not to participate that group, which people those left in the collective all have similar views and additionally think those views are more commonly held than they really are.</p>\n<p>To be clear, I don&#x2019;t object at all to arguing about things and getting down to what people&#x2019;s base intuitions are. Particularly if they haven&#x2019;t thought about them before, this is really useful. People should understand what those intuitions commit them to and whether they are consistent so they can decide if they like the consequences or want to revise their views.&#xA0;My objection is that, once you&apos;ve worked you way down to someone&apos;s base intuitions, you shouldn&apos;t mock them just because <em>their</em> intuitions are different from <em>yours</em>. It&#x2019;s the&#xA0;<em>jousting</em>&#xA0;aspect I think is wrong.</p>\n<p>I&#x2019;ve noticed IJing happens much more among effective altruists than academic philosophers. I think there are two reasons for this. The first is that the stakes are higher for effective altruists. If you&#x2019;re going to base your entire career&#xA0;on whether view X is right or wrong, getting X right&#xA0;<em>really matters</em>&#xA0;in a way it doesn&#x2019;t if two philosophers disagree over whether Plato&#xA0;<em>really meant</em>&#xA0;A, A*, or A**. The second is that academic philosophers (i.e. people who have done philosophy at university for more than a couple of years) just&#xA0;<em>accept&#xA0;</em>that people will have different intuitions about topics, it&#x2019;s&#xA0;<em>normal</em>&#xA0;and there&#x2019;s&#xA0;<em>nothing you can do about it</em>. If I meet a Kantian and get chatting about ethics, I might believe I&#x2019;m right and he&#x2019;s wrong (again, it&#x2019;s mostly &#x2018;hes&#x2019; in philosophy) but there&#x2019;s no sense fighting over it. I know we&#x2019;re just going to have started from different places. Whilst there are lots of philosophical-types in effective altruists, by no means all EAs are used to philosophical discourse. So when one EA who has strong views runs into another EA who doesn&#x2019;t share his views, it&#x2019;s more likely one or both them will&#xA0;assume there&#xA0;<em>must be</em>&#xA0;a fact of the matter to be found, and one obvious and useful way to settle this fact is by intuition jousting it out until one person admits defeat.</p>\n<p>I admit I&#x2019;ve done my fair share of IJing in my time. I&#x2019;ll hold my lance up high and confess to that. Doing it is fun and I find it a hard habit to drop. That said, I&#x2019;ve increasingly realised it&#x2019;s worth trying to repress my instincts because IJing is counter-productive. Certainly I think the effective altruist community should stop. (I&#x2019;m less concerned about philosophers because 1. they do it less and 2. lots of philosophy is low-stakes anyway).</p>\n<p>What should people do instead? The first step, which I think people should basically always do, is to stop before you start. If you realise you&#x2019;re starting to line yourself up for the charge you should realise this will be&#xA0;pointless. Instead you say &#x201C;Huh, I guess we just disagree about this,&#xA0;<em>how weird</em>&#x201C;. This is the &#x2018;stop jousting&#x2019; option. The second step, which is optional but advised, is to trying to gain understanding by working out why a person has those views: &#x201C;Oh wow. I think about it&#xA0;<em>this</em>&#xA0;way. Why do you think about it&#xA0;<em>that</em>&#xA0;way?&#x201D; This is more the &#x2018;dismount and talk&#x2019; option.</p>\n<p>As Toby Ord has argued, it&#x2019;s possible for people to engage in&#xA0;<a href=\"https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjWyOL7293SAhWELsAKHfUABT4QFggcMAA&amp;url=http%3A%2F%2Fwww.amirrorclear.net%2Ffiles%2Fmoral-trade.pdf&amp;usg=AFQjCNEPZPID7ZmFq0w1b7pnoBOTTL6dAg\">moral trade</a>. The idea is that two people can disagree about what&#x2019;s valuable but it can still be good for both parties to cooperate and help each other reach their respective moral goals. I really wish in the EA community I saw more scenarios where, should an X-risk advocate end up speaking to an animal welfare advocate, rather than each dismissing the other person as being wrong or stupid (either out loud or in their head), or jousting over who supports the&#xA0;<em>right</em>&#xA0;cause, they tried to help the other better&#xA0;achieve their objectives. From what I&#x2019;ve seen philosophers tend to be much better and taking it in turns to develop either others&#x2019; views, even if they don&#x2019;t remotely share them.</p>\n<p>And if we really do feel the need to joust, can&#x2019;t we at least attack the intuitions of&#xA0;those heartless bastards over at Charity Navigator or the Make-a-Wish Foundation instead?*</p>\n<p>*This is a joke, I&#x2019;m sure they are lovely people doing valuable work.</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "MichaelPlant"}}, {"_id": "N95ZziqxBbEbZJWmJ", "title": "Utopia In The Fog", "postedAt": "2017-03-28T02:54:51.490Z", "htmlBody": "<html><body><p><em><a href=\"https://bashibazuk.wordpress.com/2017/03/28/utopia-in-the-fog/\">Cross-posted from my new blog. </a></em></p>\n<p>The last several years have witnessed a strong rise of activity on the topic of AI safety. Institutional and academic support has vindicated several elements of the embryonic Friendly AI research program. However, I believe that the degree of attention it has received is undue when compared to other aspects of artificial intelligence and the far future. It resembles the concept of an &#x201C;availability cascade&#x201D;, defined by <a href=\"https://en.wikipedia.org/wiki/Availability_cascade\">Wikipedia</a> as follows:</p>\n<blockquote>\n<p>An <strong>availability cascade</strong> is a self-reinforcing cycle that explains the development of certain kinds of collective beliefs. A novel idea or insight, usually one that seems to explain a complex process in a simple or straightforward manner, gains rapid currency in the popular discourse by its very simplicity and by its apparent insightfulness. Its rising popularity triggers a chain reaction within the social network: individuals adopt the new insight because other people within the network have adopted it, and on its face it seems plausible. The reason for this increased use and popularity of the new idea involves both the availability of the previously obscure term or idea, and the need of individuals using the term or idea to appear to be current with the stated beliefs and ideas of others, regardless of whether they in fact fully believe in the idea that they are expressing. Their need for social acceptance, and the apparent sophistication of the new insight, overwhelm their critical thinking.</p>\n</blockquote>\n<p>In this post I&#x2019;m going to argue for a different approach which should bring more balance to the futurist ecosystem. There are significant potential problems which are related to AI development but are not instances of value alignment and control, and I think that they are more deserving of additional effort at the margin.</p>\n<h2>The prospects for a single superintelligence</h2>\n<p>Bostrom (2016) says that a recursively self-improving artificial general intelligence with a sufficient lead over competitors would have a decisive strategic advantage that is likely to ensure that it controls the world. While this is plausible, it is not inevitable and may not be the most likely scenario.</p>\n<p>Little argument has been given that this scenario should be our default expectation as opposed to merely plausible. Yudkowsky (2013) presents an argument that the history of human cognitive evolution indicates that an exponential takeoff in intelligence should be expected, though the argument has yet to be formally put together and presented. Computer scientists frequently refer to complexity theory, which implies that getting better at problem solving rapidly becomes very difficult, towards asymptotic limits. In broader economic strokes, Bloom et al (2017) argue that there is a general trend of diminishing returns to research. Both these points suggest that for an agent to acquire a decisive strategic advantage in cognition would either take a very long time or not happen at all.</p>\n<p>It seems to me, intuitively, that if superintelligence is the sort of thing that one agent cannot obtain rapidly enough to outcompete all other agents, then it&#x2019;s also the sort of thing which cannot be obtained rapidly enough by a small subset of agents, like three or four of them. So it will be widespread, or alternatively, it cannot be obtained at all, leaving billions of humans or other agents at the top of the hierarchy. So while I don&#x2019;t think that a true multi-agent scenario (with scores or more agents, as is typically meant by the term in game theory) is inevitable in the event that there is no single superintelligence, I think it&#x2019;s conditionally probable.</p>\n<h2>The Importance of Multi-agent Analysis: Three Scenarios</h2>\n<p><strong>Whole brain emulation and economic competition</strong></p>\n<p>Robin Hanson (2016) writes that the future of human civilization will be a fast-growing economy dominated by whole brain emulations. The future looks broadly good in this scenario given approximately utilitarian values and the assumption that ems are conscious, with a large growing population of minds which are optimized for satisfaction and productivity, free of disease and sickness. Needless to say, without either of the above premises, the em scenario looks very problematic. But other aspects of it would potentially lead to suboptimal utility: social hierarchy, wealth inequality and economic competition. Also, while Hanson gives a very specific picture of the type of society which &#x201C;ems&#x201D; will inhabit, he notes that the conjunction of all his claims is extremely unlikely, so there is room for unforeseen issues to arise. It is plausible to me that the value of an em society is heavily contingent upon how ems are built, implemented and regulated.</p>\n<p>However, the idea of whole brain emulation as a path to general artificial intelligence has been criticized and is a minority view. Bostrom (2016) argues that there seem to be greater technological hurdles to em development than to other kinds of progress in intelligence. The best current AI is far more capable than the best current emulation (OpenWorm). Industry and academia seem to be placing much more effort into even the very speculative strains of AI research than into emulation.</p>\n<p><strong>The future of evolution</strong></p>\n<p>If humans are not superseded by a monolithic race of ems, then trends in technological progress and evolution might have harmful effects upon the composition of the population. Bostrom (2009) writes that &#x201C;freewheeling evolutionary developments, while continuing to produce complex and intelligent forms of organization, lead to the gradual <em>elimination</em> of all forms of being that we care about.&#x201D; With the relaxation of contemporary human social and biological constraints, two possibilities are plausible: a Malthusian catastrophe where the population expands until welfare standards are neutral or negative, and the evolution of agents which outperform existing ones but without the same faculties of consciousness. Either of these scenarios would entail the extinction of most or all that we find valuable.</p>\n<p>Andres Gomez Emilsson also writes that this is a possibility <a href=\"https://qualiacomputing.com/2016/08/20/wireheading_done_right/\">on his blog</a>, saying:</p>\n<blockquote>\n<p>I will define a pure replicator, in the context of agents and minds, to be an intelligence that is indifferent towards the valence of its conscious states and those of others. A pure replicator invests all of its energy and resources into surviving and reproducing, even at the cost of continuous suffering to themselves or others. Its main evolutionary advantage is that it does not need to spend any resources making the world a better place.</p>\n</blockquote>\n<p>Bostrom does not believe that the problem is unavoidable, saying that a &#x2018;singleton&#x2019; could combat this process. By singleton he refers to not just a superintelligence but also to any global governing body or even a set of moral codes with the right properties. He writes that such an institution should implement &#x201C;<span>a coordinated policy to prevent internal developments from ushering it onto an evolutionary trajectory that ends up toppling its constitutional agreement, and doing this would presumably involve modifying the fitness function for its internal ecology of agents.&#x201D;</span></p>\n<p><strong>Augmented intelligence and military competition</strong></p>\n<p>Daniel McIntosh (2010) writes that the near-inevitable adoption of transhuman technologies poses a significant security dilemma due to the political, economic, and battlefield advantages provided by agents with augmented cognitive and physical capabilities. Critics who argue for restraint &#x201C;<span>tend to deemphasize the competitive and hedonic pressures encouraging the adoption of these products.&#x201D; Not only is this a problem on its own, but I see no reason to think that the conditions described above wouldn&#x2019;t apply for scenarios where AI agents turned out to be the primary actors and decisionmakers rather than transhumans or posthumans. </span></p>\n<p><span>Whatever the type of agent, arms races in future technologies would lead to opportunity costs in military expenditures and would interfere with the project of improving welfare. It seems likely that agents designed for security purposes would have preferences and characteristics which fail to optimize for the welfare of themselves and their neighbors. It&#x2019;s also possible that an arms race would destabilize international systems and act as a catalyst for warfare.<br> </span></p>\n<p><span>These trends might continue indefinitely with technological progress. McIntosh rejects the assumption that a post-singularity world would be peaceful:<br> </span></p>\n<blockquote>\n<p>In a post-singularity, fifth-generation world, there would always be the possibility that the economic collapse or natural disaster was not the result of chance, but of design. There would always be the possibility that internal social changes are being manipulated by an adversary who can plan several moves ahead, using your own systems against you. The systems themselves, in the form of intelligences more advanced than we can match, could be the enemy. Or it might be nothing more than paranoid fantasies. The greatest problem that individuals and authorities might have to deal with may be that one will never be sure that war is not already under way. Just as some intelligence analysts cited the rule that &#x201C;nothing is found that is successfully hidden&#x201D; &#x2013; leading to reports of missile gaps and Iraqi WMD &#x2013; a successful fifth generation war would [be] one that an opponent never even realized he lost.</p>\n</blockquote>\n<p>Almost by definition, we cannot precisely predict what will happen in a post-singularity world or develop policies and tools that will be directly applicable in such a world. But this possibility highlights the importance of building robust cooperative systems from the ground up, rather than assuming that technological changes will somehow remove these problems. A superintelligent agent with a sufficient advantage over other agents would presumably be able to control a post-singularity world sufficiently to avoid this, but as has been noted, it&#x2019;s not clear that this is the most likely scenario.</p>\n<h2>Multi-agent systems are neglected</h2>\n<p>The initiatives and independent individuals close to the EA sphere who are working towards developing reliable, friendly AI include the Machine Intelligence Research Institute, the Future of Humanity Institute, Berkeley&#x2019;s Center for Human-Compatible AI, Roman Yampolskiy, and all the effective altruists who are students of AI as far as I can tell. There is less attention towards multi-agent outcomes, as Robin Hanson, Nick Bostrom and Andres Gomez Emilsson seem to be the only ones who have done research on it (and Bostrom seems to be focused on superintelligence), while the Foundational Research Institute has given a general nod towards looking into this direction with its concerns over AI suffering, cooperation, and multipolar takeoffs.</p>\n<p>The disparity is preserved as you look farther afield. Pragmatic industry-oriented initiatives to make individual AI systems safe, ethical and reliable include the Partnership on AI among the six major tech companies, some attention from the White House on the subject, and a notable amount of academic work at universities. The work in universities and industry from researchers on multi-agent systems and game theory seems to be entirely focused on pragmatic problems like distributed computational systems and traffic networks; only a few researchers have indicated the need for analyzing multi-agent systems of the future, let alone actually done so. Finally, in popular culture, Bostrom&#x2019;s <em>Superintelligence</em> has received 319 Amazon reviews to <em>Age of Em&#x2019;s</em> 30 despite being published at a similar time, and the disparity in general media and journalism on the two general topics seems comparably large.</p>\n<p>I do not expect this to change in the future. Multi-agent outcomes are varied and complex, while superintelligence is highly available and catchy. My conclusion is that the former is significantly more neglected than the latter.</p>\n<h2>Is working on multi-agent systems of the future a tractable project?</h2>\n<p>The main point of Scott Alexander&#x2019;s <a href=\"https://slatestarcodex.com/2014/07/30/meditations-on-moloch/\">&#x201C;Meditations on Moloch&#x201D;</a> is essentially that &#x201C;the only way to avoid having all human values gradually ground down by optimization-competition is to install a Gardener over the entire universe who optimizes for human values.&#x201D; In other words, given the problems which have been described above, the only way to actually achieve a really valuable society is to have a singleton which has the right preferences and keeps everyone in line.</p>\n<p>This is not different from what Bostrom argues. But remember that the singleton need not be a superintelligence with a decisive strategic advantage. This is fortunate, since it is plausible that computational difficulties will prevent such an entity from ever existing. Instead, the Gardener of the universe might be a much more complex set of agents and institutions. For instance, Peter Railton and Steve Petersen are (I believe) both working on arguments that agents will be linked via a teleological thread where they accurately represent the value functions of their ancestors. We&#x2019;ll need to think more carefully about how to implement this sort of thing in a way that reliably maximizes welfare.</p>\n<p>This is why analysis in multi-agent game theory and mechanism design is important. The very idea behind game theory in general is that you can find useful conclusions by abstracting away from the details of a situation and only looking at players as abstract entities with basic preferences and strategies. This means that analyses and institutions are likely to be pertinent to a wide range of scenarios of technological progress.</p>\n<p>While ideas of preventing evolution, economic competition and arms races sound extremely difficult, there is some historical precedent for human institutions to install robust regulations and international agreements on this type of issue. Admittedly, none of it has been on nearly the same scale that would be required to solve the problems described above. But due to the preliminary stage of this line of research, I think that additional research, or literature review at minimum, is needed at least to investigate the various possibilities which we might pursue. Also, there is a similar problem with cooperation when it comes to ordinary AI safety anyway (Armstrong et al 2013).</p>\n<h2>Conclusion and proposal</h2>\n<p>I believe I have shown that recent interest in AI and the future of humanity has disproportionately neglected the idea of working on a broader range of futures in which society is not controlled by a single agent. There is still value in AI safety work insofar as alignment and control would help us with building the right agents in multi-agent scenarios, but there are other parts of the picture which need to be explored.</p>\n<p>First, there are specific questions which should be answered. How likely are the various scenarios described above, and how can we ensure that they turn out well? Should we prefer that society is governed by a superintelligence with a decisive strategic advantage, and if so, then how much of a priority is it?</p>\n<p>Second, there are specific avenues where practical work now can uncover the proper procedures and mindsets for increasing the probability of a positive future. Aside from setting precedents for international cooperation on technical issues, we can start steering the course of machine ethics as it is implemented in modern-day systems. Better systems of machine ethics which don&#x2019;t require superintelligence to be implemented (as coherent extrapolated volition does) are likely to be valuable for mitigating potential problems involved with AI progress, although they won&#x2019;t be sufficient (Brundage 2014). Generally speaking, we can apply tools of game theory, multi-agent systems and mechanism design to issues of artificial intelligence, value theory and consciousness.</p>\n<p>Given the multiplicity of the issues and the long timeline from here to the arrival of superhuman intelligence, I would like to call for a broader, multifaceted approach to the long term future of AI and civilization. Rather than having a singleminded focus on averting a particular failure mode, it should be a more ambitious and positive project towards a pattern of positive and self-reinforcing interactions between social institutions and intelligent systems, supported by a greater amount of human and financial capital.</p>\n<h2>References</h2>\n<p>Armstrong, Stuart et al (2016). <em><a href=\"http://link.springer.com/article/10.1007/s00146-015-0590-y\">Racing to the Precipice: a Model of Artificial Intelligence Development</a>.</em> AI &amp; Society.</p>\n<p>Bloom, Nicholas et al (2017). <em><a href=\"http://www-leland.stanford.edu/%7Echadj/IdeaPF.pdf\">Are Ideas Getting Harder To Find?</a></em></p>\n<p>Bostrom, Nick (2009). <em><a href=\"http://www.nickbostrom.com/fut/evolution.html\">The Future of Human Evolution</a></em>. Bedeutung.</p>\n<p>Bostrom, Nick (2016). <a href=\"https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834/ref=sr_1_1?ie=UTF8&amp;qid=1481778144&amp;sr=8-1&amp;keywords=superintelligence\"><em>Superintelligence</em></a>. Oxford University Press.</p>\n<p>Brundage, Miles (2014). <em><a href=\"http://www.milesbrundage.com/uploads/2/1/6/8/21681226/limitations_and_risks_of_machine_ethics.pdf\">Limitations and Risks of Machine Ethics</a>. </em>Journal of Experimental &amp; Theoretical Artificial Intelligence.</p>\n<p>Hanson, Robin (2016). <a href=\"https://www.amazon.com/Age-Em-Work-Robots-Earth/dp/0198754620/ref=cm_cr_arp_d_product_top?ie=UTF8\"><em>Age of Em</em></a>. Oxford University Press.</p>\n<p>McIntosh, Daniel (2010). <em><a href=\"http://jetpress.org/v21/mcintosh.htm\">The Transhuman Security Dilemma</a>.</em> Journal of Evolution and Technology.</p>\n<p>Yudkowsky, Eliezer (2013). <a href=\"http://intelligence.org/files/IEM.pdf\"><em>Intelligence Explosion Microeconomics</em></a>.</p></body></html>", "user": {"username": "Zeke_Sherman"}}, {"_id": "LG6gwxhrw48Dvteej", "title": "Concrete project lists", "postedAt": "2017-03-25T18:12:50.765Z", "htmlBody": "<html><body><p><span>There are lots of important project ideas in EA that people could work on, and I&#x2019;d like to encourage people to </span><a href=\"/ea/170/ea_should_invest_more_in_exploration/\"><span>explore more</span></a><span>. When I was looking for projects to work on, I had difficulty thinking of what needed doing apart from obvious projects like raising money for GiveWell-recommended charities. I even had a sense that all the organisations that needed to exist existed, which is obviously not correct.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Fortunately many people have put together project ideas in important cause areas:</span></p>\n<p><strong><strong>&#xA0;</strong></strong></p>\n<ul>\n<li>\n<p><a href=\"http://www.charityentrepreneurship.com/results.html\"><span>Charity Entrepreneurship: promising new charities</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Health</span></p>\n</li>\n<li>\n<p><span>Charity Entrepreneurship want to find founders to work on these, and will offer extensive guidance</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"http://blog.givewell.org/2015/10/15/charities-wed-like-to-see/\"><span>GiveWell: Global poverty charities we&apos;d like to see</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Poverty</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"http://www.d-prize.org/\"><span>D-Prize: Poverty intervention distribution challenges</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Poverty, health</span></p>\n</li>\n<li>\n<p><span>Successful applicants to these challenges receive seed funding</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"http://transformativetechnologies.org/the-50-breakthroughs-study/\"><span>Institute for Transformative Technologies: 50 breakthroughs needed for sustainable global development</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Health, food and agriculture, human rights, education, water, gender equality, digital inclusion, climate change resilience, and electricity</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"https://www.finddx.org/target-product-profiles/\"><span>FIND: Needed diagnostic devices for the developing world</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Poverty, health</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"/ea/16r/increasing_access_to_pain_relief_an_ea_perspective/\"><span>Lee Sharkey: Increasing Access to Pain Relief in Developing Countries</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Poverty, health</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"http://goodtechnologyproject.org/problems/financial_services_for_the_poor\"><span>Good Technology Project: Financial services for the poor</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Poverty </span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"http://documents.worldbank.org/curated/en/188451468336589650/pdf/903050WP0REPLACEMENT0Box385358B00PUBLIC0.pdf\"><span>World Bank: The Opportunities for Digitising Payments</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Poverty</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"https://medium.com/deepscience/antibiotic-resistance-what-can-you-do-9d8994bab30c#.6yxg91d0p\"><span>Deep Science Ventures: Antibiotic resistance: what can you do?</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Health, catastrophic risk</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"http://goodtechnologyproject.org/problems/biosurveillance\"><span>Good Technology Project: Biosurveillance</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Health, catastrophic risk</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"http://mission-innovation.net/our-work/innovation-challenges/\"><span>Mission Innovation: Climate change innovation challenges</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Climate change</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"http://worrydream.com/ClimateChange/\"><span>Brett Victor: What can a technologist do about climate change?</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Climate change</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"https://arxiv.org/abs/1606.06565\"><span>Concrete problems in AI safety</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>AI safety</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"https://intelligence.org/files/TechnicalAgenda.pdf\"><span>MIRI: Agent Foundations for Aligning Superintelligence with Human Interests</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>AI safety</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"https://intelligence.org/files/AlignmentMachineLearning.pdf\"><span>MIRI: Alignment for Advanced Machine Learning Systems</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>AI safety</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"https://arxiv.org/pdf/1602.03506.pdf\"><span>Research Priorities for Robust and Beneficial Artificial Intelligence</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>AI safety</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"http://lukemuehlhauser.com/some-studies-which-could-improve-our-strategic-picture-of-superintelligence/\"><span>Luke Muehlhauser: How to study superintelligence strategy</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>AI safety</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"http://www.wri.org/sites/default/files/wri13_report_4c_wrr_online.pdf\"><span>World Resources Institute: Creating a sustainable food future</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Catastrophic risk</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"https://www.amazon.co.uk/dp/B00Q2N073O\"><span>Feeding everyone no matter what: managing food security after global catastrophe</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Catastrophic risk</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"https://docs.google.com/document/d/13Hzde8wgfSUO2ij0lAUxztNsOM03RxD9plo5HaaXUmk/edit\"><span>EA Ventures: Promising project ideas</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Mixed</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"https://impact.hackpad.com/Projects-aRiPtncmuKS\"><span>.impact: Projects list</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Mixed</span></p>\n</li>\n</ul>\n<li>\n<p><a href=\"https://docs.google.com/spreadsheets/d/1CBYddAHPR5_tNXJWO4YA1anyfsyo-cR2116gjATQI1I/edit#gid=0\"><span>Good Technology Project: Open projects in EA</span></a></p>\n</li>\n<ul>\n<li>\n<p><span>Mixed</span></p>\n</li>\n</ul>\n</ul>\n<p><strong>&#xA0;</strong></p>\n<p><span>This is far from exhaustive, but it&#x2019;s a start. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>However, it&#x2019;s not clear whether lack of ideas is actually what&#x2019;s stopping people from working on new projects. So I&#x2019;d be interested to know:</span></p>\n<p><strong>&#xA0;</strong></p>\n<ul>\n<li>\n<p><span>What&#x2019;s blocking you from working on an altruistic project?</span></p>\n</li>\n<li>\n<p><span>Are there resources which the community could provide that would help?</span></p>\n</li>\n<li>\n<p><span>Do you have any more project ideas or lists of project ideas to add? - I&apos;ll keep this list updated with what I find.</span></p>\n</li>\n</ul>\n<p><strong>&#xA0;</strong></p>\n<p><span>[This came out of </span><a href=\"/ea/17s/what_should_the_average_ea_do_about_ai_alignment/abe\"><span>this thread</span></a><span> on why things don&#x2019;t get done in the EA community. Thanks to John Maxwell for being a commitment device.]</span></p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Richard_Batty"}}, {"_id": "64yrq2nzwSaCFH7Ei", "title": "Effective altruism: an elucidation and a defence", "postedAt": "2017-03-22T17:06:50.202Z", "htmlBody": "<html><body><p><em><span>By </span><span>John Halstead, Stefan Schubert, Joseph Millum,</span><span> Mark Engelbert,</span><span> Hayden Wilkinson,</span><span> and James Snowden.</span><span> Cross-posted from the </span><a href=\"https://www.centreforeffectivealtruism.org/blog/effective-altruism-an-elucidation-and-a-defence/\"><span>Centre for Effective Altruism blog</span></a><span>. A direct link to the article can be found </span></em><a href=\"https://assets.contentful.com/es8pp29e1wp8/2CTJ1CFOfaqMss4MqYwkmo/2265870850f312a2409cc3a262566301/Gabriel_published.pdf\"><span><em>here</em></span></a><em><span>.</span></em></p>\n<p><span>Abstract</span></p>\n<p><span>In this paper, we discuss <a href=\"https://www.academia.edu/13786913/Effective_Altruism_and_Its_Critics\">Iason Gabriel&#x2019;s recent piece on criticisms of effective altruism</a>.</span><span> Many of the criticisms rest on the notion that effective altruism can roughly be equated with utilitarianism applied to global poverty and health interventions which are supported by randomised control trials and disability-adjusted life year estimates. We reject this characterisation and argue that effective altruism is much broader from the point of view of ethics, cause areas, and methodology. We then enter into a detailed discussion of the specific criticisms Gabriel discusses. Our argumentation mirrors Gabriel&#x2019;s, dealing with the objections that the effective altruist community neglects considerations of justice, uses a flawed methodology, and is less effective than its proponents suggest. Several of the criticisms do not succeed, but we also concede that others involve issues which require significant further study. Our conclusion is thus twofold: the critique is weaker than suggested, but it is useful insofar as it initiates a philosophical discussion about effective altruism and highlights the importance of more research on how to do the most good.</span></p>\n<p><span>&#xA0;</span></p>\n<p><a href=\"https://assets.contentful.com/es8pp29e1wp8/2CTJ1CFOfaqMss4MqYwkmo/2265870850f312a2409cc3a262566301/Gabriel_published.pdf\"><span>Click here to go to the article</span></a><span>.</span></p></body></html>", "user": {"username": "Stefan_Schubert"}}, {"_id": "hAeL4DdrDjqfo98t7", "title": "CEA's strategic update for February 2017", "postedAt": "2017-03-18T21:50:20.356Z", "htmlBody": "<html><body><p>Below is CEA&apos;s strategic update for February 2017. &#xA0;I&apos;m going to take over sharing these updates. If you&apos;d like to receive them by email just comment or contact me to let me know. I will be posting these updates on the <a href=\"https://www.centreforeffectivealtruism.org/blog/\">CEA blog</a> moving forward. I&apos;m sharing this one here for anyone who missed it on our blog and so you know how to find them in future.</p>\n<p>Since&#xA0;<a href=\"/ea/17d/cea_update_updates_from_january_2017/\">our last update</a>, we have&#xA0;<a href=\"/ea/17v/ea_funds_beta_launch/\">launched&#xA0;<span>EA</span>&#xA0;Funds</a>, hosted a team retreat and continued to learn a lot at&#xA0;<span>YC</span>. As a result, we&#x2019;ve updated and clarified our strategy and I&#x2019;d like to share more of our bigger-picture thinking going forward. In these updates, I&#x2019;ll aim to explain more of the why of our overall approach, to provide more context for what we&#x2019;re working<span>&#xA0;</span>on.</p>\n<h2><span><br>CEA</span>&#x2019;s Vision and<span>&#xA0;</span>Mission</h2>\n<p>During our team retreat, we clarified and updated our overall vision and mission. Our vision is to create an optimal world. We don&#x2019;t yet know exactly what an optimal world looks like. There are some things which we think are robustly good, such as ending death from malaria or abolishing factory farming, and other areas where we are highly uncertain. For this reason, we want to see effective altruism do for the pursuit of good what the Scientific Revolution did for the pursuit of truth. We want to build a community focused on figuring out what this optimal world looks like, and how we get<span>&#xA0;</span>there.</p>\n<p>To help make this vision a reality, we plan to first focus on building capacity to do more good in the future. In particular, we need three key resources as a<span>&#xA0;</span>community:</p>\n<ol>\n<li>groundbreaking ideas,</li>\n<li>talented, motivated people</li>\n<li>the money needed to put the best ideas into<span>&#xA0;</span>practice.</li>\n</ol>\n<p>In the past, we&#x2019;ve talked a lot about whether we&#x2019;re bottlenecked by ideas, talent or money, but we&#x2019;ve realised in absolute terms, we need vastly more of all of these key resources. We want to promote and strengthen effective altruism as an idea and a community, with the aim of increasing the total value of all resources effectively aimed at robustly doing good now, and figuring out how to do even more good in the<span>&#xA0;</span>future.</p>\n<h2><span><br>CEA</span>&#x2019;s Objectives for<span>&#xA0;</span>Q1:</h2>\n<p>To better pursue our mission to support the community we are pursuing quarterly goals, focusing on one of the key community resources (ideas, talent or money) for each quarter. This quarter, our primary focus is improving the infrastructure for giving effectively. We developed the&#xA0;<a href=\"https://app.effectivealtruism.org/funds/\"><span>EA</span>&#xA0;Funds</a>&#xA0;concept after speaking to both highly engaged donors, and people who were quite new to the community. Beyond that, this quarter we are also building faster feedback loops to ensure we focus our future efforts where it is most valuable. Next quarter we aim to focus on consolidation and develop better infrastructure for developing and sharing core ideas within effective<span>&#xA0;</span>altruism.</p>\n<p><strong><br>Our specific Q1<span>&#xA0;</span>Objectives:</strong></p>\n<p>Test&#xA0;<a href=\"https://app.effectivealtruism.org/funds/\">Effective Altruism Funds</a>&#xA0;as a concept. We will consider this project a success if feedback from the community is positive, and the amount we have raised for the funds in the first quarter exceeds $1M. We think that the fund managers will be able to recommend a more representative series of grants if they have at least $200K to allocate in the first round. You can keep up with how much money we&apos;ve raised using&#xA0;<a href=\"https://app.effectivealtruism.org/funds/stats\">this dashboard</a>. We hope to raise additional funds through new contacts at&#xA0;<a href=\"https://www.ycombinator.com/about/\">Y Combinator</a>. We chose to focus on money-moved during&#xA0;<span>YC</span>&#xA0;in part because the partners and founders there are particularly good at giving advice in this area. You can read more about why we launched&#xA0;<span>EA</span>&#xA0;Funds in&#xA0;<a href=\"/ea/17v/ea_funds_beta_launch/\">our launch post</a>&#xA0;and if you&#x2019;ve yet to provide feedback on&#xA0;<span>EA</span>&#xA0;Funds we&#x2019;d really appreciate your thoughts in this&#xA0;<a href=\"https://cea-core.typeform.com/to/sqNLhX\">quick<span>&#xA0;</span>survey</a>.</p>\n<p>Establish models of how to evaluate the impact of our activities. This involves both establishing a framework for evaluating Executive Office activities and evaluating our how existing channels (such as our Effective Altruism Global conferences and social media channels) add value. We&#x2019;re building some rough quantitative models to compare these different approaches, as we expect some projects to be many times more effective per dollar than others.<br>Maintain and grow a positive relationship with the broader effective altruism community. This includes providing greater transparency about what&#xA0;<span>CEA</span>&#xA0;is working on and<span>&#xA0;</span>why.</p>\n<p>&#xA0;</p>\n<h2>Changes to&#xA0;<span>CEA</span>&#x2019;s organisational<span>&#xA0;</span>structure.</h2>\n<p>Our renewed focus means some changes to the organisational structure of&#xA0;<span>CEA</span>, in line with our&#xA0;<a href=\"https://www.centreforeffectivealtruism.org/fundraising/#2016-retrospective-and-plans-for-2017\">goal of testing projects</a>, measuring their impact and updating to focus on those which perform<span>&#xA0;</span>best.<br><br><strong>1. Remaining teams in&#xA0;<span>CEA</span>&#xA0;all in one division</strong></p>\n<p>To allow us to better coordinate we&#x2019;ve moved everyone (marketing, events, chapter support, community liaisons and Will MacAskill&#x2019; Executive Office) to one division, working towards agreed metrics and objectives. Find out more about the people who work at&#xA0;<span>CEA</span>&#xA0;on our&#xA0;<a href=\"https://www.centreforeffectivealtruism.org/team/\">team<span>&#xA0;</span>website</a>.</p>\n<p>&#xA0;<br><span>2. The dissolution of our Special Projects<span>&#xA0;</span>Division</span></p>\n<p>As part of&#xA0;<span>CEA</span>&#x2019;s internal reorganization in July 2016, we created a Special Projects Division to house a number of discrete, pre-existing research-related<span>&#xA0;</span>projects:</p>\n<ul>\n<li>Philanthropic advising</li>\n<li>Policy</li>\n<li>The Oxford Institute for Effective Altruism<span>&#xA0;</span>(<span>OIEA</span>)</li>\n<li><a href=\"https://www.centreforeffectivealtruism.org/fundraising#fundamentals-research\">Fundamentals research</a></li>\n</ul>\n<p>In line with our aim to&#xA0;<a href=\"https://www.centreforeffectivealtruism.org/fundraising/\">narrow&#xA0;<span>CEA</span>&#x2019;s focus</a>, we&#x2019;ve been reviewing which of the projects in this division to scale up, scale down or discontinue. We have therefore decided to do the<span>&#xA0;</span>following:</p>\n<p>discontinue the philanthropic advising project (while shifting some of that work to&#xA0;<a href=\"https://founderspledge.com/\">Founders Pledge</a>);<br>move our policy work on existential and technological risks to the Future of Humanity Institute (<span>FHI</span>);<br>move&#xA0;<span>OIEA</span>&#xA0;fully into Oxford University; and<br>allow the fundamentals research team to operate independently as part of a collaborative Oxford-based research community that includes&#xA0;<span>OIEA</span>&#xA0;and<span>&#xA0;</span><span>FHI</span>.</p>\n<p>In light of these changes, the need for a Special Projects Division at&#xA0;<span>CEA</span>&#xA0;has run its course. Although&#xA0;<span>CEA</span>&#xA0;will continue to sponsor the fundamentals research stream and facilitate the development of&#xA0;<span>OIEA</span>, its organizational focus will be on developing and strengthening the effective altruism<span>&#xA0;</span>community.</p>\n<p>Below is more information on the next steps for each of the research projects that previously formed part of the Special Project<span>&#xA0;</span>Division.</p>\n<p><br><strong>Philanthropic advising</strong></p>\n<p>The philanthropic advising project, which was originally part of Giving What We Can, has focused on (i) research into new, effective giving opportunities, and (ii) providing tailored giving recommendations to wealthy individuals and foundations. We recently decided to discontinue this project at&#xA0;<span>CEA</span>&#xA0;for the following<span>&#xA0;</span>reasons:</p>\n<p>We moved less money through wealthy individuals and foundations than we expected. Although we had hoped that Founders Pledge members would provide a reliable stream of clients, we underestimated the inefficiencies to Founders Pledge of relying on a third-party for consulting services. We believe it would be more effective for Founders Pledge to provide these services itself. Marinella Capriati, formerly of our philanthropic advising team, will be joining Founders Pledge to help it develop its own philanthropic advising<span>&#xA0;</span>capacity.</p>\n<p>Our research wasn&#x2019;t able to add enough value beyond GiveWell and the Open Philanthropy Project. Our model involved conducting research into areas that GiveWell/Open Philanthropy Project had not fully explored and were unlikely to explore anytime soon. Our team&#x2019;s areas of expertise overlapped considerably with those of GiveWell/Open Philanthropy Project, however. Without venturing well beyond our areas of expertise, there were fewer opportunities to provide value here than we expected. Although this might not always be the case, we believe that research that is within the focus areas of GiveWell/Open Philanthropy Project is most efficiently conducted within those organizations. James Snowden, formerly of our philanthropic advising team, will be joining GiveWell, where we believe his research will have a&#xA0;greater<span>&#xA0;</span>impact.</p>\n<p>Our philanthropic advising work was insufficiently complementary to&#xA0;<span>CEA</span>&#x2019;s core strategy. In the philanthropy domain,&#xA0;<span>CEA</span>&#x2019;s plan is to develop, and move money through, the new&#xA0;<a href=\"https://app.effectivealtruism.org/funds\"><span>EA</span>Funds platform</a>. We believe this platform will accomplish several of the goals we had for the philanthropic advising team and therefore reduces the value of&#xA0;<span>CEA</span>&#xA0;doing its own charity<span>&#xA0;</span>research.</p>\n<p><br><strong>Policy</strong></p>\n<p>As we mentioned in last month&#x2019;s update, Seb Farquhar, who led our policy advising work (previously as the Executive Director of Global Priorities Project), is moving across the hall to&#xA0;<span>FHI</span>, where he will continue his work on existential and technological risk policy. We discussed our decision not to expand our policy focus in our&#xA0;<a href=\"https://www.centreforeffectivealtruism.org/fundraising/\">year-end<span>&#xA0;</span>review</a>.</p>\n<p>&#xA0;<br><strong>Oxford Institute for Effective<span>&#xA0;</span>Altruism</strong></p>\n<p><span>OIEA</span>&#xA0;is an academic institute, founded by Hilary Greaves and Will MacAskill, that we expect to go live in fall 2017. During its initial, grant-writing stage,&#xA0;<span>OIEA</span>&#xA0;has been housed within&#xA0;<span>CEA</span>, which has funded its first employees (Michelle Hutchinson and Jon Courtney). Having received its first (small) grant,&#xA0;<span>OIEA</span>&#xA0;is now at the stage where it can operate as part of Oxford University, independent of<span>&#xA0;</span><span>CEA</span>.</p>\n<p><br><strong>Fundamentals research</strong></p>\n<p>The remaining research team within&#xA0;<span>CEA</span>&#xA0;will focus on pursuing research that helps to improve the intellectual community around effective altruism. This may<span>&#xA0;</span>include:</p>\n<ul>\n<li>searching for insights that could help improve understanding of object-level questions about movement norms or<span>&#xA0;</span>strategy;</li>\n<li>understanding issues that cut across different cause areas, or relate to how they might work together;<span>&#xA0;</span>and</li>\n<li>producing resources that help individuals engage more thoughtfully with effective<span>&#xA0;</span>altruism</li>\n</ul>\n<p>This team consists of two full-time researchers (Stefan Schubert and Max Dalton), and one part-time researcher (Ben Garfinkel). In addition, Owen Cotton-Barratt will be joining part-time to lead research direction. This team will work closely with&#xA0;<span>FHI</span>&#xA0;and&#xA0;<span>OIEA</span>&#xA0;as part of a collaborative Oxford-based research<span>&#xA0;</span>community.</p></body></html>", "user": {"username": "TaraMacAulay"}}, {"_id": "eyDDjYrG3i3PRGxtc", "title": "Hard-to-reverse decisions destroy option value ", "postedAt": "2017-03-17T17:54:34.688Z", "htmlBody": "<p><span>This post is co-authored with Ben Garfinkel. It is cross-posted from <a href=\"https://www.centreforeffectivealtruism.org/blog/hard-to-reverse-decisions-destroy-option-value/\">the CEA blog</a>. A PDF version can be found <a href=\"https://assets.contentful.com/es8pp29e1wp8/33dKJ09kCcaOUsIS4eAkEy/9d5f64a1b056517f16aa4f346926cb9c/Reversiblestrategiespreserveoptionvaluepost.pdf\">here</a>.</span></p>\n<p><span>Summary:</span><span> Some strategic decisions available to the effective altruism movement may be difficult to reverse. One example is making the movement\u2019s brand explicitly political. Another is growing large. Under high uncertainty, there is often reason to avoid or delay such hard-to-reverse decisions.</span></p>\n<p>&nbsp;</p>\n<p><strong><span>Table of contents</span></strong></p>\n<p><span>Introduction</span></p>\n<p><span>What is reversibility?</span></p>\n<p><span><span>How to choose</span></span></p>\n<p><span>Fundamental considerations</span></p>\n<p><span>Secondary considerations</span></p>\n<p><span><span>Where do all the social movements go?</span></span></p>\n<p>&nbsp;</p>\n<h3><span>Introduction</span></h3>\n<p><span>The importance of option value is widely appreciated within the effective altruism movement. In an uncertain world, keeping multiple paths open can be very valuable.</span></p>\n<p><span>One aspect of option value is </span><span>reversibility</span><span>. </span><span>When we consider a change from the </span><span>status quo</span><span> to a new state of affairs, we risk losing option value if the decision is difficult to reverse. If we have a white cloth, we can dye it black at any time. However, once we have dyed it black, it would take much more work to make it white again.</span></p>\n<p><span>The effective altruism movement faces many similar strategic situations, where it is easier to leave the </span><span>status quo</span><span> than to get back. For instance:(1)</span></p>\n<ul>\n<li>\n<p><span>The effective altruism movement is currently not strongly associated with any political party, or with any of the mainstream political ideologies. In that sense, the effective altruism brand is relatively apolitical. This could easily change, e.g., if the movement allied itself with a certain political party. However, once that step has been taken, it might be hard to go back to an apolitical brand.</span></p>\n</li>\n</ul>\n<ul>\n<li>\n<p><span>The effective altruism movement is currently quite small. The movement may try to grow big, but if it does, it will arguably be hard to reverse that decision. Shrinking the movement in a way which does not cause serious damage is presumably very difficult.</span></p>\n</li>\n</ul>\n<ul>\n<li>\n<p><span>The effective altruism movement has invested in acquiring a reputation for integrity, rigour, friendliness, and other kinds of prosocial behaviour. It may, however, decide that such a reputation is too costly to uphold, and that some level of dishonesty, lack of rigour, or unfriendliness is acceptable. Since it is easier to destroy a good reputation than to build one, it is plausibly hard to reverse such a decision.(2)</span></p>\n</li>\n</ul>\n<h3><span>What is reversibility?</span></h3>\n<p><span>To understand the notion of reversibility, let us first look at the notion of option value. It can be defined as follows. </span></p>\n<p>&nbsp;</p>\n<p><span>Option value: </span><span>The </span><span>option value </span><span>associated with a possible choice is the expected value of having this choice available.</span></p>\n<p><span>To use a standard example from the literature, it can be in your self-interest to support a tax for the maintenance of Sequoia National Park even if you are not sure that you will ever visit. The fact that you might some day choose to visit the park, and can expect the visit to be worthwhile if you do, gives this possible choice option value.(3)</span></p>\n<p><span>The concept of option value is frequently applied within the effective altruism movement. For example, </span><a href=\"http://www.openphilanthropy.org/blog/worldview-diversification#Capacity_building_and_option_value\"><span>the\u2002Open\u2002Philanthropy\u2002Project\u2002has\u2002argued</span></a><span> that working on many different causes gives it the option value of being able to focus on any of these causes in the future. </span></p>\n<p><span>Now suppose that one is considering leaving some state A (e.g., being an apolitical movement) in order to enter some state B (e.g., being a political movement). While one is in state A, one derives option value from the possible choice to enter state B. One loses this option value, of course, by actually entering B. But one may also gain option value from the possible choice to re-enter A.</span></p>\n<p><span>The right decision in this case depends in part on how much option value one would gain. A good heuristic here is to ask how </span><span>reversible </span><span>the decision to leave state A for state B would be.</span></p>\n<p><span>We define:</span></p>\n<p><span>Reversibility:</span><span> The </span><span>reversibility </span><span>of a decision to leave state A for state B is the reciprocal (i.e. inverse) of the direct cost of returning to A.(4)</span></p>\n<p><span>To better understand the significance of reversibility, let us consider a concrete case.</span></p>\n<p><span>Suppose, again, that the decision to become a political movement has both a low direct cost and a very low reversibility. Then many future opportunities that will be available to political movements (such as the opportunity to partner with an influential activist group) could in practice be available to apolitical movements too, since the direct cost of becoming political is low. In contrast, future opportunities that will be available to apolitical movements (such as the opportunity to attract a wide range of recruits) will not in practice be available to political movements, since the direct cost of becoming apolitical again will be too high.</span></p>\n<p><span>Note that reversibility is not the only determinant of option value. However, it is one particularly significant determinant, which it is easy to conceptualize and make snap judgements of.</span></p>\n<p><span>In this article, we are focusing on decision situations where the decision to exit the </span><span>status quo </span><span>is easier to make than reverse. However, this choice of focus is not arbitrary. In many situations where we might consider two different strategies, one much easier to switch out from than the other, the question of which strategy we should follow will only be a live one if we are currently following the strategy that it is easy to switch out of. Due to this selection effect, we will less often be considering situations in which it is easier to re-enter the </span><span>status quo</span><span> than to exit it.(5)</span></p>\n<h3><span>How to choose? </span></h3>\n<p><span>Let us now have a closer look at how to evaluate a potential decision by a social movement to switch strategies &nbsp;(e.g., to become explicitly political). We will first look at five fundamental considerations, before turning to eight secondary considerations.(6)</span></p>\n<p>&nbsp;</p>\n<h4><span><span>Fundamental considerations</span></span></h4>\n<p>&nbsp;</p>\n<p><span>Core expected value</span><span>. We define a strategy\u2019s </span><span>core expected value</span><span> to be its expected value given that one does not switch out of it. A large core expected value can obviously trump concerns having to do with reversibility. For instance, if a large movement has much greater core expected value than a small movement, then it may be worth growing large even if the decision is not reversible.</span><span><br></span></p>\n<p>&nbsp;</p>\n<p><span>Reversibility</span><span>. Lower reversibility normally counts against a decision to adopt a new strategy (though see </span><span>uncertainty about core expected value</span><span>), since this implies that we will not be able to derive much option value from the possibility of switching back. </span></p>\n<p><span>Direct cost</span><span>.</span> <span>As also discussed in endnote 4, we define the </span><span>direct cost</span><span> of a decision to switch strategies as the cost of the decision itself, rather than the opportunity cost of no longer following the initial strategy. For example, the direct cost of switching from a small movement to a large movement might include time and money spent on outreach and the loss of any members who are alienated by the growing process (but not the new size of the movement itself). A large direct cost would of course count against a decision.</span></p>\n<p><span>Uncertainty about core expected value</span><span>. The reason reversibility can be so useful is that the core expected value of many decisions is very uncertain. For instance, it is highly uncertain how valuable it would be for the effective altruism movement to grow large. If the movement could reverse that strategy once it had embarked on it, uncertainty would be less of a problem.</span></p>\n<p><span>Conversely, if we are certain of how valuable our strategies are, reversibility does not matter. But neither does it matter if we believe that there is no way for us to learn more about how valuable they are. Under such radical uncertainty, having the option to reverse your decision is of no avail. </span></p>\n<p><span>However, the most common scenario is that of more moderate uncertainty. For instance, if we embark on a certain strategy, we tend to improve our estimates of its core expected value. If we learn that it is lower than we thought, having the option to reverse that strategy can be crucial. </span></p>\n<p><span>Uncertainty can also diminish prior to the launch of a strategy, either automatically or as a result of research and testing (see </span><span>researching and testing strategies</span><span>). If we expect uncertainty to be reduced in the future, this can be a strong reason to delay a relatively irreversible decision to embark on a new strategy.</span></p>\n<p><span>Uncertainty about reversibility and direct cost</span><span>. In addition to being uncertain about expected value, we are also often uncertain about reversibility and direct cost. For instance, it seems very hard to assess how reversible a decision not to have norms of integrity actually is. This may be a reason against prematurely leaping onto paths which we suspect can be highly irreversible. We may want to wait until we have got a more resilient estimate of reversibility and direct cost, e.g., thanks to research or testing (cf. </span><span>researching and testing strategies</span><span>).</span></p>\n<p><span><span>Secondary considerations</span></span></p>\n<p><span>Risk aversion.</span><span> Risk aversion is normally a reason not to make a hard-to-reverse decision. Since reversibility gives you the option to switch course if your strategy underperforms, it normally reduces the risk of a truly bad outcome. Note, though, that the standard view within the effective altruism movement </span><a href=\"https://concepts.effectivealtruism.org/concepts/risk-aversion/\"><span>seems to be that altruists should not be risk-averse</span></a><span>.</span></p>\n<p><span>Focus on long time horizons</span><span>. If the effective altruism movement remains active for decades, and significant opportunities to do good continue to exist, then it will probably be faced with a large array of opportunities, some of which will only be available to the movement if it is pursuing a particular strategy (such as having a reputation for integrity). Longer time horizons also leads to greater uncertainty about which opportunities may eventually arise or become valuable (cf. </span><span>uncertainty about core expected value</span><span>). This means that it may be crucial to keep our options open, by avoiding hard-to-reverse decisions. On the other hand, if the effective altruism movement will be short-lived, or if the best opportunities to do good are fleeting, then option value considerations may not be very significant. Option value can be thought as a kind of capacity, which the movement may or may not take advantage of in the future. </span></p>\n<p><span>Cause-neutrality and option value</span><span>. One of the key features of effective altruism is </span><span>cause-neutrality</span><span>: the notion that we should not prejudge what cause to invest in, but rather compare all causes impartially.</span><span>(7) If the movement finds new and more valuable causes in the future, it can pursue them. This gives the movement much greater option value compared to cause-partial groups, which are set on pursuing certain causes. It is not implausible to believe that most of the effective altruism movement\u2019s expected value derives from this option value. However, some hard-to-reverse decisions may make it significantly harder to pursue some causes. For instance, turning political may make it harder to work on promoting bipartisan civility. This means that making hard-to-reverse decisions on key questions may deprive the effective altruism movement significant proportions of the value that cause-neutrality gives it.</span></p>\n<p><span>Correlation between irreversibility and uncertainty</span><span>. First, we saw that a question of what strategy to pursue on a certain issue often ceases to be a live one if we take a hard-to-reverse decision. Second, we are normally more certain of the expected value, direct costs, and reversibility of strategies that we already have pursued. Together, these two premises entail a negative correlation between reversibility and uncertainty: if a decision to pursue a certain strategy is hard to reverse, we typically have not pursued it previously, which normally means that we are uncertain about its expected value, direct costs, and reversibility.(8)</span><span> If so, that can strengthen the case against hard-to-reverse decisions to adopt new strategies (cf. </span><span>uncertainty about core expected value </span><span>and </span><span>uncertainty about reversibility and direct cost</span><span>).</span></p>\n<p><span>Side effects on other decisions</span><span>. Deviations from the </span><span>status quo</span><span> on one issue are likely to affect the core expected value, reversibility, and direct cost of other decisions in ways which are hard to predict. For instance, growing the effective altruism movement may affect the core expected value, reversibility, and direct cost of the possible decision to become explicitly political in unpredictable ways. That may be a reason not to make several hard-to-reverse decisions at once.</span></p>\n<p><span>Researching and testing strategies</span><span>. Often, it is possible to learn more about the core expected value, reversibility, and direct cost of a decision to adopt a new strategy prior to embarking on it. This can be done through research, or through testing the strategy on a small scale. Whether to research and test a specific strategy depends on costs and expected information value. The information value is, in turn, dependent on estimated reversibility. Everything else being equal, a low level of reversibility is a reason to invest more resources in researching and testing a strategy prior to pursuing it.</span></p>\n<p><span>Overconfidence</span><span>. Humans tend to be biased towards overconfidence. That may make us underestimate the actual uncertainty of core expected value and reversibility and therefore the importance of reversibility. In particular, we may underestimate the number of doors that a hard-to-reverse decision closes. We often employ inside-view thinking (cf. </span><a href=\"http://www.overcomingbias.com/2007/07/beware-the-insi.html\"><span>Robin\u2002Hanson</span></a><span>) to model plausible future scenarios in terms of specific causal pathways. When doing so, we often overestimate the extent to which the pathways we have identified exhaust the space of plausible scenarios. We often miss important ways in which the future could pan out, and in some of those, it may be very valuable to be in the state (e.g., being a small movement) that is difficult to re-enter into.</span></p>\n<p><span>The unilateralist\u2019s curse</span><span>. Many hard-to-reverse decisions are instances of the </span><a href=\"https://concepts.effectivealtruism.org/concepts/unilateralists-curse/\"><span>unilateralist\u2019s curse</span></a><span>; a concept described in a </span><a href=\"http://www.nickbostrom.com/papers/unilateralist.pdf\"><span>paper</span></a><a href=\"http://www.openphilanthropy.org/blog/worldview-diversification#Capacity_building_and_option_value\"><span>\u2002</span></a><a href=\"http://www.nickbostrom.com/papers/unilateralist.pdf\"><span>by</span></a><a href=\"http://www.openphilanthropy.org/blog/worldview-diversification#Capacity_building_and_option_value\"><span>\u2002</span></a><a href=\"http://www.nickbostrom.com/papers/unilateralist.pdf\"><span>Bostrom,</span></a><a href=\"http://www.openphilanthropy.org/blog/worldview-diversification#Capacity_building_and_option_value\"><span>\u2002</span></a><a href=\"http://www.nickbostrom.com/papers/unilateralist.pdf\"><span>Douglas</span></a><a href=\"http://www.openphilanthropy.org/blog/worldview-diversification#Capacity_building_and_option_value\"><span>\u2002</span></a><a href=\"http://www.nickbostrom.com/papers/unilateralist.pdf\"><span>and</span></a><a href=\"http://www.openphilanthropy.org/blog/worldview-diversification#Capacity_building_and_option_value\"><span>\u2002</span></a><a href=\"http://www.nickbostrom.com/papers/unilateralist.pdf\"><span>Sandberg</span></a><span>. The \u201ccurse\u201d appears in situations where a group\u2019s decision may effectively be determined by a unilateral action from a single member of the group. For instance, if one person decides to tell the object of a surprise party in advance, they have effectively made the decision for the whole group. In particular, the curse predicts that the more members the group has, the more likely it is that the decision will be determined by unilateral action, regardless of whether it is the right one.</span><span><br></span><span><br></span><span>All of the examples discussed in the paper by Bostrom et al., such as the case of the spoiled surprise party, are also examples of hard-to-reverse decisions. However, there are also many hard-to-reverse decisions which cannot be undertaken unilaterally.</span><span>(9) For instance, it is probably hard for a single effective altruist organization to grow the movement by itself, in the face of resistance from other parts of the movement. </span></p>\n<p><span>Out of the three discussed examples, decisions regarding norms of, e.g., honesty and integrity are probably the most susceptible to the unilateralist\u2019s curse. For such decisions, low reversibility and the unilateralist\u2019s curse have compounding effects. That a small group can unilaterally make a decision which irreversibly harms the whole movement may pose a serious risk.</span><span><br></span><span><br></span><span>Bostrom et al. suggest that the unilateralist\u2019s curse can be lifted through deliberation or deference to other actors: what they call </span><span>the principle of conformity</span><span>.(10)</span><span> In short, they argue, members of a group should agree to a code of conduct that makes it unlikely that any individual member will take the unilateral decision against the wishes of the group. </span></p>\n<hr>\n<p><span>It is hard to say in the abstract which of these considerations are most important, but in our view, what one should look at first is </span><span>core expected value, reversibility, </span><span>and </span><span>direct cost</span><span>. Some of the secondary considerations are also quite important. These include </span><span>focus on long time horizons</span><span>, </span><span>correlation between irreversibility and uncertainty</span><span>, and </span><span>side effects on other decisions</span><span>. It could be useful to reflect on them, especially because they are less obvious than the fundamental considerations. </span></p>\n<h3><span>Where do all the social movements go?</span></h3>\n<p><span>To put the concept of reversibility in perspective, let us note that it could help to explain and predict the trajectory of social movements. Suppose that:</span></p>\n<p>&nbsp;</p>\n<ol>\n<li>\n<p><span>A social movement has an ongoing or recurring opportunity to make a hard-to-reverse decision</span></p>\n</li>\n<li>\n<p><span>Given that the movement survives, the probability that it takes the hard-to-reverse decision at any particular point in time never dips below some (potentially very low) lower bound.</span></p>\n</li>\n</ol>\n<p><span>Then:</span></p>\n<ol>\n<li>\n<p><span>If the movement exists for long enough, it will almost certainly take the hard-to-reverse decision eventually.</span>(11)<span><br><br></span></p>\n</li>\n</ol>\n<p><span>Figure</span><span>: A social movement facing a recurrent opportunity to make a hard-to-reverse decision. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995663/mirroredImages/eyDDjYrG3i3PRGxtc/pwnojkf5qz77atxssf0p.png\"></span></p>\n<p><span>This means that if it is very difficult to reverse the decisions to grow large, to go explicitly political, or to give up on norms of honesty and integrity, we may expect most social movements capable of entering these states to end up in them. In particular, we may expect that the effective altruism movement will do so by default.</span></p>\n<p>&nbsp;</p>\n<p><span>This means, in turn, that the effective altruism movement should think through carefully whether those end-states are indeed desirable. That depends on host of considerations. We have addressed some of them here, but individual strategic decisions must be decided on a case-by-case basis. If the movement decides that a particular end-state is undesirable, it should reflect on what can be done to prevent us from ending up in it, e.g., through unilateral action.</span></p>\n<h3><span>Notes</span></h3>\n<ol>\n<li><span><span><span>Note that the primary point of these examples is to illustrate the notion of reversibility. There might be reasonable disagreement on how reversible these strategies are.</span></span></span></li>\n<li>\n<p><span>Although our main focus here is on strategic choices for the effective altruism movement as a whole, it is worth noting that reversibility is often salient in the decisions made by individuals within the movement as well. For instance, the choice to leave a high status career for a lower status one may be difficult to reverse. It can also be very difficult for individuals to reverse a reputation for being a bad apple. </span></p>\n</li>\n<li>\n<p><span>See Richard C. Bishop\u2019s </span><a href=\"https://www.jstor.org/stable/3146073\"><span>Option</span></a><a href=\"http://www.openphilanthropy.org/blog/worldview-diversification#Capacity_building_and_option_value\"><span>\u2002</span></a><a href=\"https://www.jstor.org/stable/3146073\"><span>Value:</span></a><a href=\"http://www.openphilanthropy.org/blog/worldview-diversification#Capacity_building_and_option_value\"><span>\u2002</span></a><a href=\"https://www.jstor.org/stable/3146073\"><span>An</span></a><a href=\"http://www.openphilanthropy.org/blog/worldview-diversification#Capacity_building_and_option_value\"><span>\u2002</span></a><a href=\"https://www.jstor.org/stable/3146073\"><span>Exposition</span></a><a href=\"http://www.openphilanthropy.org/blog/worldview-diversification#Capacity_building_and_option_value\"><span>\u2002</span></a><a href=\"https://www.jstor.org/stable/3146073\"><span>and</span></a><a href=\"http://www.openphilanthropy.org/blog/worldview-diversification#Capacity_building_and_option_value\"><span>\u2002</span></a><a href=\"https://www.jstor.org/stable/3146073\"><span>Extension</span></a><span> (1982) for a more thorough discussion of this example and the concept of option value.</span></p>\n</li>\n<li>\n<p><span>This definition can be made more precise. By the \u201cdirect cost\u201d of switching states we mean the cost of switching itself, rather than the opportunity cost of no longer being in the initial state. For instance, in the case of Sequoia National Park, the direct cost of moving from a developed to an undeveloped state would include the financial cost of demolishing buildings, replanting trees, importing animals, and so on, but would not include the lost tax revenue from any businesses displaced. Note also that for cases where it is in fact impossible to switch states, the direct cost is infinite. </span></p>\n</li>\n<li>\n<p><span>Much of our analysis is applicable to cases where the more reversible decision is not the </span><span>status quo </span><span>as well. This includes situations where we face two new options which differ in terms of reversibility.</span></p>\n</li>\n<li>\n<p><span>The fundamental considerations are the ones that we believe it is most important to take into account when choosing whether to make a hard-to-reverse decision. The secondary considerations are ones that we believe are either less important or useful mainly insofar as they help us to think more clearly about the fundamental considerations. However, this distinction is quite rough.</span></p>\n</li>\n<li>\n<p><span>Thus, we use the term \u201ccause-neutral\u201d in the sense that one of us, Stefan, calls \u201ccause-impartiality\u201d in his article </span><a href=\"https://www.centreforeffectivealtruism.org/blog/understanding-cause-neutrality/\"><span>Understanding cause-neutrality</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>It should be said, however, that in some cases we may have firm knowledge of the value of hard-to-reverse strategies from other sources. For instance, deciding to change your career from earning to give to academia may be hard to reverse, but you can still get a fair estimate of its value through looking at other people\u2019s careers.</span></p>\n</li>\n<li>\n<p><span>Conversely, there are some decisions prone to the curse which are not hard to reverse. For instance, suppose that a member of a group can veto a decision to take a certain offer. Suppose also that the offer will not cease to be given. That is a unilateralist\u2019s curse situation, and yet the decision is not hard to reverse.</span></p>\n</li>\n<li>\n<p><span>There may be an empirical correlation between a refusal to adopt the principle of conformity, and a tendency to rashly take irreversible decisions. This could be mediated by overconfidence in one\u2019s own present judgements of the best course of action. However, this is merely a conjecture. The question should be studied further.</span></p>\n</li>\n<li>\n<p><span>Though note Keynes\u2019s quip that \u201cin the long run we will all be dead\u201d.</span></p>\n</li>\n</ol>", "user": {"username": "Stefan_Schubert"}}, {"_id": "q36uhfREa98tec5XM", "title": "'Crucial Considerations and Wise Philanthropy', by Nick Bostrom", "postedAt": "2017-03-17T06:48:47.986Z", "htmlBody": "<p>On July 9th, 2014, Nick Bostrom gave a talk on Crucial Considerations and Wise Philanthropy at Good Done Right, a conference on effective altruism held at All Souls College, Oxford. I found the talk so valuable that I decided to transcribe it.</p><h2>Full transcript</h2><p>This talk will build on some of the ideas that <a href=\"http://www.nickbeckstead.com/\">Nick Beckstead</a> was <a href=\"https://docs.google.com/presentation/d/1nLiOh5ISjJUXmfXyH7L4piFaNK9QHZNxCAfc4yP_gwc/edit#slide=id.p1\">talking about</a> before lunch. By contrast with his presentation, though, this will not be a well-presented&nbsp;presentation.</p><p>This is very much a work in progress, so there\u2019s going to be some jump cuts, some of the bits will be muddled, etc. But I\u2019ll look forward to the discussion part of&nbsp;this.</p><h2><strong>What is a crucial&nbsp;consideration?</strong></h2><p>So I want to talk about this concept of a <a href=\"https://concepts.effectivealtruism.org/concepts/the-importance-of-crucial-considerations/\">crucial consideration</a>, which comes up in the work that we\u2019re doing a lot. Suppose you\u2019re out in the forest and you have a map and a compass, and you\u2019re trying to find some destination. You\u2019re carrying some weight, maybe you have a lot of water because you need to hydrate yourself to reach your goal and carry weight, and trying to fine-tune the exact direction you\u2019re going. You\u2019re trying to figure out how much water you can pour out, to lighten your load without having too little to reach your&nbsp;destination.</p><p>All of these are normal considerations: you\u2019re fine-tuning the way you\u2019re going to make more rapid progress towards your goal. But then you look more closely at this compass that you have been using, and you realize that the magnet part has actually come loose. This means that the needle might now be pointing in a completely different direction that bears no relation to North: it might have rotated some unknown number of laps or parts of a&nbsp;lap.</p><p>With this discovery, you now completely lose confidence in all the earlier reasoning that was based on trying to get the more accurate reading of where the needle was pointing. This would be an example of a crucial consideration in the context of orienteering. The idea is that there could be similar types of consideration in more important contexts, that throw us off completely. So a crucial consideration is a consideration such that if it were taken into account, it would overturn the conclusions we would otherwise reach about how we should direct our efforts, or an idea or argument that might possibly reveal the need not just for some minor course adjustment in our practical endeavors, but a major change of direction or&nbsp;priority.</p><p>Within a utilitarian context, one can perhaps try to explicate it as follows: a crucial consideration is a consideration that radically changes the expected value of pursuing some high-level subgoal. The idea here is that you have some evaluation standard that is fixed, and you form some overall plan to achieve some high-level subgoal. This is your idea of how to maximize this evaluation standard. A crucial consideration, then, would be a consideration that radically changes the expected value of achieving this subgoal, and we will see some examples of this. Now if you stop limiting your view to some utilitarian context, then you might want to retreat to these earlier more informal formulations, because one of the things that could be questioned is utilitarianism itself. But for most of this talk we will be thinking about that&nbsp;component.</p><p>There are some related concepts that are useful to have. So a <i>crucial consideration component</i> will be an argument, idea, or datum which, while not on its own amounting to a crucial consideration, seems to have a substantial probability of maybe being able to serve a central role within a crucial consideration. It\u2019s the kind of thing of which we would say: \u201cThis looks really intriguing, this could be important; I\u2019m not really sure what to make of it at the moment.\u201d On its own, maybe it doesn\u2019t tell us anything, but maybe there\u2019s another piece that, when combined, will somehow yield an important result. So those kinds of crucial consideration components could be useful to&nbsp;discover.</p><p>Then there\u2019s the concept of a <i>deliberation ladder</i>, which would be a sequence of crucial considerations regarding the same high-level subgoal, where the considerations hold in opposing directions. Let\u2019s look at some examples of these kinds of crucial consideration ladders that help to illustrate the general&nbsp;predicament.</p><h2><strong>Should I vote in the national&nbsp;election?</strong></h2><p>Let\u2019s take this question: \u201cShould I vote in the national election?\u201d At the sort of \u201clevel one\u201d of reasoning, you think, \u201cYes, I should vote to put a better candidate in office.\u201d That clearly makes&nbsp;sense.</p><p>Then you reflect some more: \u201cBut, my vote is extremely unlikely to make a difference. I should not vote, but put my time to better&nbsp;use.\u201d</p><p>(These examples are meant to illustrate the general idea. It\u2019s not so much I want a big discussion as to these particular examples; they\u2019re complicated. But I think they will serve to illustrate the general&nbsp;phenomenon.)</p><p>So here we have gone from \u201cYes, we should vote,\u201d making a plan to get to the polling booth, etc. And then, with the consideration number two, we switch to \u201cNo, I should not vote. I should do something completely&nbsp;different.\u201d</p><p>Then you think, \u201cWell, although it\u2019s unlikely that my vote will make a difference, the stakes are very high: millions of lives are affected by the president. So even if the chance that my vote will be decisive is one in several million, the expected benefit is still large enough to be worth a trip to the polling station.\u201d I'd just went back to the television and turned on the football game, but now it turns out I should vote, so we have a reversed&nbsp;direction.</p><p>Then you continue to think, \u201cWell, if the election is not close, then my vote will make no difference. If the election <i>is</i> close, then approximately half of the votes will be for the wrong candidate, implying either that the candidates are exactly or almost exactly of the same merit, so it doesn\u2019t really matter who wins, or typical voters\u2019 judgment of the candidates\u2019 merits is extremely unreliable, and carries almost no signal, so I should not bother to&nbsp;vote.\u201d</p><p>Now you sink back into the comfy sofa and bring out the popcorn or whatever, and then you think, \u201cOh, well, of course I\u2019m a much better judge of the candidates\u2019 merits than the typical voter, so I should&nbsp;vote.\u201d</p><p>Then you think, \u201cWell, but psychological studies show that people who tend to be overconfident almost everybody believes themselves to be above average, but they are as likely to be wrong as right about that. If I am as likely to vote for the wrong candidate as is the typical voter, then my vote would have negligible information to the selection process, and I should not&nbsp;vote.\u201d</p><p>Then we go&nbsp;on\u2026</p><p>\u201cOkay, I\u2019ve gone through all of this reasoning that really means that I\u2019m special, so I should&nbsp;vote.\u201d</p><p>But then, \u201cWell, if I\u2019m so special, then the opportunity cost\u2026\u201d (This is why I warned you all against becoming&nbsp;philosophers.)</p><p>So, I should do something more important. But if I don\u2019t vote my acquaintances will see that I have failed to support the candidates that we all think are best, they would think me weird and strange, and disloyal. Then that would maybe diminish my influence, which I could otherwise have used for good ends, so I should vote after&nbsp;all.</p><p>But it\u2019s important to stand up for one\u2019s convictions, to stimulate fruitful discussion. They might think that I'm really sophisticated if I explain all this complicated reasoning for voting, and that might increase my influence, which I can then invest in some good cause. Et certera, et cetera, et&nbsp;cetera.</p><p>There is no reason to think that the ladder would stop there; it\u2019s just that we ran out of steam at that point. If you end at some point, you might then wonder \u2013 maybe there are further steps on the ladder? How much reason do you really think you have for the conclusion you\u2019ve temporarily reached at that&nbsp;stage?</p><h2><strong>Should we favor more funding for x-risk tech&nbsp;research?</strong></h2><p>I want to look at one other example of a deliberation ladder more in the context of technology policy and x-risk. This is a kind of argument that can be run with regard to certain types of technologies, whether we should try to promote them or get more funding&nbsp;from.</p><p>The technology here is nanotechnology; this is in fact the example where this line of reasoning originally came up. Some parts of this hearken back to Eric Drexler\u2019s book <a href=\"https://www.amazon.com/Engines-Creation-Nanotechnology-Library-Science/dp/0385199732\"><i>Engines of Creation</i></a>, where he actually advocated this line of thinking (ch. 12). So we should fund nanotechnology \u2014 this is the \u201clevel one\u201d reasoning \u2014 because there are many potential future applications: medicine, manufacturing, clean energy, etc. It would be really great if we had all those&nbsp;benefits.</p><p>But it also looks like nanotechnology could have important military applications, and it could be used by terrorists etc., to create new weapons of mass destruction that could pose a major existential threat. If it\u2019s so dangerous, maybe we shouldn\u2019t really fund&nbsp;it.</p><p>But if this kind of technology is possible, it will almost certainly be developed sooner or later, whether or not we decide to pursue it. (\u2018We\u2019 being maybe the people in this room or the people in Britain or other Western democracies.) If responsible people refrain from developing it, then it will be developed by irresponsible people, which would make the risks even greater, so we should fund&nbsp;it.</p><p>(You can see that the same template could be relevant for evaluating other technologies with upsides and downsides, besides&nbsp;nanotechnology.)</p><p>But we are already ahead in its development, so extra funding would only get us there sooner, leaving us less time to prepare for the dangers. So we should not add funding: the responsible people can get there first even without adding funding to this&nbsp;endeavor.</p><p>But then you look around and see virtually no serious effort to prepare for the dangers of nanotechnology \u2014 and this is basically Drexler\u2019s point back in <i>Engines</i> \u2014 because serious preparation will begin only after a massive project is already underway to develop nanotechnology. Only then will people take the prospect&nbsp;seriously.</p><p>The earlier a serious Manhattan-like project to develop nanotechnology is initiated, the longer it will take to complete, because the earlier you start, the lower the foundation from which you begin. The actual project will then run for longer, and that will then mean more time for preparation: serious preparation only starts when the project starts, and the sooner the project starts, the longer it will take, so the longer the preparation time will be. And that suggests that we should push as hard as we can to get this product launched immediately, to maximize time for&nbsp;preparation.</p><p>But there are more considerations that should be taken into account. The level of risk will be affected by factors other than the amount of serious preparation that has been made, specifically to counter the threat from nanotechnology. For instance, machine intelligence or ubiquitous surveillance might be developed before nanotechnology, eliminating or mitigating the risks of the latter. Although these other technologies may pose great risks of their own, those risks would have to be faced anyway. And there\u2019s a lot more that can be&nbsp;said.</p><p>Nanotechnology would not really reduce these other risks, like the risks from AI, for example. The preferred sequence is that we get superintelligence or ubiquitous surveillance before nanotechnology, and so we should oppose extra funding for nanotechnology even though superintelligence and ubiquitous surveillance might be very dangerous on their own, including posing existential risk, given certain background assumptions about the <a href=\"http://www.nickbostrom.com/papers/future.pdf\">technological completion conjecture</a> \u2014 that in the fullness of time, unless civilization collapses, all possible general useful technologies will be developed \u2014 these dangers will have to be confronted, and all our choice really concerns is the sequence in which we confront them. And it\u2019s better to confront superintelligence before nanotechnology because superintelligence can obviate the nanotechnology risk, but not <i>vice&nbsp;versa</i>.</p><p>However, if people oppose extra funding for nanotechnology, then people working in nanotechnology will dislike those people who are opposing it. (This is also a point from Drexler\u2019s book.) But other scientists might regard these people who oppose funding for nanotechnology as being anti-science and this will reduce our ability to work with these scientists, hampering our efforts on more specific issues\u2014efforts that stand a better chance of making a material difference to any attempt on our part to influence the level of national funding for nanotechnology. So we should not oppose nanotechnology. That is, rather than opposing nanotechnology \u2014 we may try to slow it down a little bit. But we are a small group and we can\u2019t make a big difference \u2014 we should work with the nanotechnology scientists, be their friends, and then maybe try to influence on the margin, so that they develop nanotechnology in a slightly different way or add some safeguards, and stuff like&nbsp;that.</p><p>Again, there is no clear reason to think that we have reached the limit of the level of deliberation that we could apply to this. It\u2019s disconcerting because it looks like the practical upshot keeps switching back and forth as we look more deeply into the search tree, and we might wonder why this is so. I think that these deliberation ladders are particularly likely to turn up when one is trying to be a thoroughgoing utilitarian and really take the big-picture question&nbsp;seriously.</p><h2><strong>Crucial considerations and&nbsp;utilitarianism</strong></h2><p>Let\u2019s consider some possible reasons for why that might be. If we compare, for example, the domain of application of utilitarianism to another domain of application, say if you have an ordinary human preference function\u2014you want a flourishing life, like a healthy family, a successful career and some relaxation, like typical human values \u2014 if you\u2019re trying to satisfy those, it looks less likely that you will encounter a large number of these crucial considerations. Why might that&nbsp;be?</p><p>One possible explanation is that we have more knowledge and experience of human life at the personal level. Billions of people have tried to maximize an ordinary human utility function and have received a lot of feedback and a lot of things have been tried out. So we already know some of the basics like, if you want to go on for decades, it\u2019s a good idea to eat, things like&nbsp;that.</p><p>They\u2019re not something we need to discover. And maybe our preferences in the first place have been shaped to more or less fit the kind of opportunities we can cognitively exploit in the environment by evolution. So we might not have some weird preference that there was no way that we could systematically satisfy. Whereas with utilitarianism, the utilitarian preference extends far and wide beyond our familiar environment, including into the cosmic commons and billions of years into the future and super advanced civilizations: what they do matters from the utilitarian perspective, and matters a lot. Most of what the utilitarian preference cares about is stuff that we have no familiarity&nbsp;with.</p><p>Another possible source of crucial considerations with regard to utilitarianism is difficulties in understanding the goal itself. For example, if one tries to think about how to apply utilitarianism to a world that has a finite probability of being infinite, one will run into difficulties in terms of how to measure different infinite magnitudes and still seeing how we could possibly make any difference to the world. I have a <a href=\"http://www.nickbostrom.com/ethics/infinite.pdf\">big paper</a> about that, and we don\u2019t need to go into it. There are some other issues that consist in actually trying to articulate utilitarianism to deal with all these possible&nbsp;cases.</p><p>The third possible reason here is that one might think that we are kind of close, not super close, but close to some pivot point in history. That means we might have special opportunities to influence the long-term future now. And we\u2019re still far away from this: it\u2019s not obvious what we should do to have the maximally beneficial impact on the future. But we're still close enough that we can maybe begin to perceive some contours of the apparatus that will shape the future. For example, you may think that superintelligence might be this pivot point, or one of them (there may be x-risk pivot points as well) that we will confront in this century. In that case, it might just be that we are barely just beginning to get the ability to think about those things, which introduces a whole set of new considerations that might be very&nbsp;important.</p><p>This could affect the personal domain as well. It\u2019s just like with an ordinary person\u2019s typical utility function: they probably don\u2019t place a million times more value on living for a billion years than living for a hundred years, or a thousand times more value on raising a thousand children than on raising one child. So even though the future still exists, it just doesn\u2019t weigh as heavily in a normal human utility function as it does for&nbsp;utilitarians.</p><p>Fourthly, one might also argue that we have recently discovered some key exploration tools that enable us to make these very important discoveries about how to be a good utilitarian. And we haven\u2019t yet run the course with these tools, so we keep turning up like fundamental new important discoveries using these exploration tools. That\u2019s why there seem to be so many crucial considerations being discovered. We might talk a little bit about some of those later in the&nbsp;presentation.</p><h2><strong>Evaluation functions</strong></h2><p>Now let me come at this from a slightly different angle. In chess, the way you would ideally play is you would start by thinking about the possible moves that you could make, then the possible responses that your opponent could make, and your responses to those responses. Ideally, you would think that through all the way to the end state, and then just try to select a first move that would be best from the point of view of winning when it could calculate through the entire game tree. But that\u2019s computationally infeasible because the tree branches too much: you have an exponential number of moves to&nbsp;consider.</p><p>So what you instead have to do is to calculate explicitly some number of plays ahead. Maybe a dozen plays ahead or something like that. At that point, your analysis has to stop, and what you do is to have some evaluation function which is relatively simple to compute, which tries to look at the board state that could result from this sequence of six moves and countermoves, and in some rough and ready way try to estimate how good that state is. A typical chess evaluation function might look something like&nbsp;this.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/z0fby67ya6dsghossv5q\" alt=\"crucial considerations1\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/z0fby67ya6dsghossv5q 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/a1g6ysp8httlkarliydg 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/mq6jmisow8v8xqdo7li1 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/nroohpkxdqxinhzcycmd 992w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/z96o2sqavyh9d5oek5pa 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/nkr5xnaxahmvkowlvkk9 1800w\"></figure><p>You have some term that evaluates how much material we have, like having your queen and a lot of pieces is beneficial. The opponent having few of those is also beneficial. We have some metric like a pawn is worth one and queen is worth, I don\u2019t know, 11 or something like&nbsp;that.</p><p>So you weigh that up \u2014 that\u2019s one component in the evaluation function. Then maybe consider how mobile your pieces are. If they\u2019re all crammed in the corner, that\u2019s usually an unpromising situation, so you have some term for that. King safety - center control adds a bit of value: if you control the middle of the board, we know from experience that tends to a good&nbsp;position.</p><p>So what you do is calculate explicitly some number of steps ahead and then you have this relatively unchanging evaluation function that is used to figure out which of these initial games that you could play would be resulting in the most beneficial situation for you. These evaluation functions are mainly derived from some human chess masters who have a lot of experience playing the game. The parameters, like the weight you assign to these different features, might also be learned by machine&nbsp;intelligence.</p><p>We do something analogous to that in other domains. Like a typical traditional public policy, social welfare economists might think that you need to maximize some social welfare function which might take a form like&nbsp;this.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/zofczu7vyagboaqao8vv\" alt=\"crucial considerations2\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/zofczu7vyagboaqao8vv 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/eamg5rgfhi9skjax4gdh 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/acrlzhthxbfpcmkoztte 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/xrkivltuzx2jsvlvtuh2 992w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/vj4ufj3yq3xzqa0qtems 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/domqthx24kau9befdcsa 1800w\"></figure><p>GDP? Yes, we want more GDP, but we also have to take into account the amount of unemployment, maybe the amount of equality or inequality, some factor for the health of the environment. It might not be that whatever we write there is exactly the thing that is equivalent to moral goodness fundamentally considered. But we know that these things tend to be good, or we think&nbsp;so.</p><p>This is a useful approximation of true value that might be more tractable in a practical decision-making context. One thing I can ask, then, is if there is something similar to that for moral goodness. You want to do the morally best thing you can do, but to calculate all of these out from scratch just looks difficult or impossible to do in any one situation. You need more stable principles that you can use to evaluate different things you could do. Here we might look at the more restricted version of utilitarianism. We can wonder what we might put in&nbsp;there.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/tdaanzfsdceangxlbowf\" alt=\"crucial considerations3\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/tdaanzfsdceangxlbowf 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/noonjpgxtpuhs4dmm4sj 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/igjtik0sbmzsiduha1up 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/xhqp8ihwigr65ywncb5l 992w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/lp4ep1aw8vl27epw3266 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/bs0fdh3birohrapof0zr 1800w\"></figure><p>Here we can hark back to some of the things Beckstead talked about. If we plot capacity, which could be level of economic development and technological sophistication, stuff like that, on one axis and time on the other, my view is that the human condition is a kind of metastable region on this capability&nbsp;axis.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/jnhpybyuaedrgjq53avg\" alt=\"crucial considerations4\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/jnhpybyuaedrgjq53avg 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/i8pixviehnjrtoka5jmv 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/ryeklujbbrlfpr1qt4bx 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/xprwwe9cfjitck5n9pag 992w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/in3p9tzqbht1byz0zrl6 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/tjdgvvswcujwxcbygyer 1800w\"></figure><p>You might fluctuate inside for a while, but the longer the time scale you\u2019re considering, the greater the chance that you will exit that region in either the downwards direction and go extinct\u2014if you have too few resources below the minimum viable population size, you go extinct (that\u2019s one attractor state: once you\u2019re extinct, you tend to stay extinct) \u2014 or in the upwards direction: we get through to technological maturity, start colonization process and the future of earth-originating intelligent life might just then be this bubble that expands at some significant fraction of the speed of light and eventually accesses all the cosmological resources that are in principle accessible from our starting point. It\u2019s a finite quantity because of the positive cosmological constant: looks like we can only access a finite amount of stuff. But once you\u2019ve started that, once you\u2019re an intergalactic empire, it looks like it could just keep going with high probability to this natural&nbsp;vision.</p><p>We can define the concept of an <a href=\"http://www.nickbostrom.com/existential/risks.html\">existential risk</a> as one that fails to realize the potential for value that you could gain by accessing the cosmological commons, either by going extinct or by maybe accessing all the cosmological commons but then failing to use them for beneficial purposes or something like&nbsp;that.</p><p>That suggests this Maxipok principle that Beckstead also mentioned: <i>Maximize the probability of an OK outcome</i>. That\u2019s clearly, at best, a rule of thumb: it\u2019s not meant to be a valid moral principle that\u2019s true in all possible situations. It\u2019s not that. In fact, if you want to go away from the original principle you started with to something practically tractable, you have to make it contingent on various empirical assumptions. That\u2019s the trade-off there: you want to make as weak assumptions as you can and still move it as far as possible towards being tractable as you&nbsp;can.</p><p>I think this is something that makes a reasonable compromise there. In other words, take the action that minimizes the integral of existential risk that humanity will confront. It will not always give you the right answer, but it\u2019s a starting point. There are different things to the ones that Beckstead mentioned, there could be other scenarios where this would give the wrong answer: if you thought that there was a big risk of hyper existential catastrophe like some hell scenario, then you might want to increase level of existential risks slightly in order to decrease the risk that there would not just be an existential catastrophe but hyper existential catastrophe. Other things that could come into it are trajectory changes that are less than drastic and just shift&nbsp;slightly.</p><p>For present purposes, we could consider the suggestion of using the Maxipok rule as our attempt to define the value function for utilitarian agents. Then the question becomes, If you want to minimize existential risk, what should you do? That is still a very high-level objective. We still need to do more work to break that down into more tangible&nbsp;components.</p><p>I\u2019m not sure how well this fits in with the rest of the&nbsp;presentation.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/mqexfaf5xsjsdexu6rtm\" alt=\"crucial considerations5\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/mqexfaf5xsjsdexu6rtm 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/e2mgnspoi6fvq2guuk4d 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/mzpbstew3jzvtdjjkxuo 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/scrwzwdgj6lwqus4abli 992w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/xzciopvm6a5zxvqhj7eu 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/odbkn1qemmwxswaecue2 1800w\"></figure><p>I have this nice slide from another presentation. It\u2019s a different way of saying some of what I just said: instead of thinking about sustainability as is commonly known, as this static concept that has a stable state that we should try to approximate, where we use up no more resources than are regenerated by the natural environment, we need, I think, to think about sustainability in dynamical terms, where instead of reaching a state, we try to enter and stay on a trajectory that is indefinitely sustainable in the sense that we can contain it to travel on that trajectory indefinitely and it leads in a good&nbsp;direction.</p><p>An analogy here would be if you have a rocket. One stable state for a rocket is on the launch pad: it can stand there for a long time. Another stable state is if it\u2019s up in space, it can continue to travel for an even longer time, perhaps, if it doesn\u2019t rust and stuff. But in mid-air, you have this unstable system. I think that\u2019s where humanity is now: we\u2019re in mid-air. The static sustainability concept suggests that we should reduce our fuel consumption to the minimum that just enables us to hover there. Thus, maybe prolong the duration in which we could stay in our current situation, but what we perhaps instead should do is maximize the fuel consumption so that we have enough thrust to reach escape velocity. (And that\u2019s not a literal argument for burning as much fossil fuel as possible. It\u2019s just a&nbsp;metaphor.)</p><p>The point here is that to have the best possible condition, we need super advanced technology: to be able to access the cosmic commons, to be able to cure all the diseases that plague us, etc. I think to have the best possible world, you\u2019ll also need a huge amount of insight and wisdom, and a large amount of coordination so as to avoid using high technology to wage war against one another, and so forth. Ultimately, we would want a state where we have huge quantities of each of these three variables, but that leaves open the question of what we want more from our consideration. It might be, for example, that we would want more coordination and insight before we have more technology of a certain type. So that before we have various powerful technologies, we would first want to make sure that we have enough peace and understanding to not use them for warfare, and that we have enough insight and wisdom not to accidentally blow ourselves up with&nbsp;them.</p><p>A superintelligence, clearly, seems to be something you want in utopia \u2014 it\u2019s a very high level of technology \u2014 but we might want a certain amount of insight before we develop superintelligence, so we can develop it in the correct&nbsp;way.</p><p>One can begin to think about, as in analogy with the computer test situation, if there are different features that one could possibly think of as components of this evaluation function for the utilitarian, the&nbsp;Maxipok.</p><p>This <a href=\"https://en.wikipedia.org/wiki/Differential_technological_development\">principle of differential technological development</a> suggests that we should retard the development of dangerous and harmful technologies\u2014ones that raise existential risk, that is\u2014 and accelerate technologies that reduce existential&nbsp;risk.</p><p>Here is our first sketch, this is not a final answer, but one may think that we want a lot of wisdom, we want a lot of international peace and cooperation, and with regards to technologies, it gets a little bit more complicated: we want faster progress in some technology areas, perhaps, and slower in others. I think those are three broad kinds of things one might want to put into the one\u2019s evaluation&nbsp;function.</p><p>This suggests that one thing to be thinking about in addition to interventions or causes, is the signature of different kinds of things. An intervention should be sort of high leverage, and a cause area should promise high leverage interventions. It\u2019s not enough that something you could do would do good, you also want to think hard about how much good it could do relative to other things you could do. There is no point in thinking about causes without thinking about how do you see all the low hanging fruit that you could access. So a lot of the thinking is about&nbsp;that.</p><p>But when we\u2019re moving at this more elevated plane, this high altitude where there are these crucial considerations, then it also seems to become valuable to think about determining the sign of different basic parameters, maybe even where we are not sure how we could affect them. (The sign being, basically, Do we want more or less of it?) We might initially bracket questions as to leverage here, because to first orient ourselves in the landscape we might want to sort of postpone that question a little bit in this context. But a good signpost \u2014 that is a good parameter of which we would like to determine the signature \u2014 would have to be visible from afar. That is, if we define some quantity in terms that still make it very difficult for any particular intervention to say whether it contributes positively or negatively to this quantity that we just defined, then it\u2019s not so useful as a signpost. So, \u201cmaximize expected value\u201d, say, is the quantity they could define. It just doesn\u2019t help us very much, because whenever you try to do something specific you\u2019re still virtually as far away as you had been. On the other hand, if you set some more concrete objective, like maximize the number of people in this room, or something like that, we can now easily tell like how many people there are, and we have ideas about how we could maximize it. So any particular action we think of we might easily see how it fares on this objective of maximizing the people in this room. However, we might feel it\u2019s very difficult to get strong reasons for knowing whether more people in this room is better, or whether there is some inverse relationship. A good signpost would strike a reasonable compromise between being visible from afar and also being such that we can have strong reason to be sure of its&nbsp;sign.</p><h2><strong>Some tentative signposts</strong></h2><p>Here are some very tentative signposts: they\u2019re tentative in my own view, and I guess there might also be a lot of disagreement among different people. So these are more like areas for investigation. But it might be useful just to show how one might begin to think about&nbsp;it.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/wi7b6zu98v7xudbeqsa3\" alt=\"crucial considerations6\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/wi7b6zu98v7xudbeqsa3 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/xkhtavb4eqhp7var617y 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/ih3qxhcppcr3vjse731j 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/ulded0nneuslbqdksnqz 992w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/b7nh5dad1f7wvxhdwf8s 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/pf6wdowjxejvnrqlldke 1800w\"></figure><p><i>Do we want faster progress in computer hardware or slower progress?</i> My best guess there is that we want slower progress. And that has to do with the risks from the machine intelligence transition. Faster computers would make it easier to make AI, which (a) would make them happen sooner probably, which seems perhaps bad in itself because it leaves less time for the relevant kind of preparation, of which there is a great need; and (b) might reduce the skill level that would be required to produce AI: with a ridiculously large amount of computing power you might be able to produce AI without really knowing much about what you\u2019re doing; when you are hardware-constrained you might need more insight and understanding, and it\u2019s better that AI be created by people who have more insight and&nbsp;understanding.</p><p>This is not by any means a knockdown argument, because there are other existential risks. If you thought that we are about to go extinct anytime soon, because somebody will develop nanotechnology, then you might want to sort of try the AI wildcard as soon as possible. But all-things-considered this is my current best guess. These are the kinds of reasoning that one can engage&nbsp;in.</p><p><i>Whole brain emulation?</i> We did a <a href=\"http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf\">long, big analysis</a> of that. More specifically, not whether we want to have whole brain emulation, but whether we want to have more or less funding for whole brain emulation, more or fewer resources for developing that. This is one possible path towards machine superintelligence, and for complicated reasons, my guess is \u201cNo\u201d, but that\u2019s even more uncertain, and we have a lot of different views in our research group on that. (In the discussion, if anybody is interested in one particular one, we can zoom in on&nbsp;that.)</p><p><i>Biological cognitive enhancement of humans?</i> My best guess there is that we want faster progress in that&nbsp;area.</p><p>So with these three \u2014 I talk more about them in <a href=\"https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies\">the book</a> \u2014 and AI as&nbsp;well.</p><p><i>AI</i> I think we want AI probably to happen a little bit slower than it\u2019s likely to do by&nbsp;default.</p><p>Another question is:</p><p><i>If there is one company or project or team that will develop the first successful AI, how much ahead does one want that team to be to the second team that is trying to do it?</i> My best guess is that we want it to have a lot of lead, many years ideally, to enable them to slow down at the end to implement more safety measures, rather than being in the tight tech&nbsp;race.</p><p><i>Solutions to the </i><a href=\"https://en.wikipedia.org/wiki/AI_control_problem\"><i>control problem for AI</i></a><i>?</i> I think we want faster progress in that, and that\u2019s one of our focus areas, and some of our friends from the <a href=\"http://intelligence.org/\">Machine Intelligence Research Institute</a> are here, also working hard on&nbsp;that.</p><p><i>The </i><a href=\"http://effectivealtruism.org/\"><i>effective altruism movement</i></a><i>?</i> I think that looks very good in many ways, robustly good, to have faster, better growth in&nbsp;that.</p><p><i>International peace and cooperation?</i> Looks&nbsp;good.</p><p><i>Synthetic biology?</i> I think it looks bad. We haven\u2019t thought as carefully about that, so that could change, but it looks like there could be x-risks from that, although it may also be beneficial. Insofar as it might enable improvements in cognitive enhancement, there\u2019ll be a kind of difficult&nbsp;trade-off.</p><p><i>Nanotechnology?</i> I think it looks bad: we want slower progress towards&nbsp;that.</p><p><i>Economic growth?</i> Very difficult to tell the sign of that, in my view. And within a community of people have thought hard about that are, again, different guesses as to the sign of&nbsp;that.</p><p><i>Small and medium scale catastrophe prevention?</i> Also looks good. So global catastrophic risks falling short of existential risk. Again, very difficult to know the sign of that. Here we are bracketing leverage at all, even just knowing whether we would want more or less, if we could get it for free, it\u2019s non-obvious. On the one hand, small-scale catastrophes might create an immune response that makes us better, puts in place better safeguards, and stuff like that, that could protect us from the big stuff. If we\u2019re thinking about medium-scale catastrophes that could cause civilizational collapse, large by ordinary standards but only medium-scale in comparison to existential catastrophes, which are large in this context, again, it is not totally obvious what the sign of that is: there\u2019s a lot more work to be done to try to figure that out. If recovery looks very likely, you might then have guesses as to whether the recovered civilization would be more likely to avoid existential catastrophe having gone through this experience or&nbsp;not.</p><p>So these are the parameters that one can begin to think about. One doesn\u2019t realize just how difficult it is, even some parameters that from an ordinary common-sense point of view seem kind of obvious, actually turn out to be quite non-obvious once you start to think through the way that they\u2019re all supposed to fit&nbsp;together.</p><p>Suppose you\u2019re an administrator here in Oxford, you\u2019re working in the Computer Science department, and you\u2019re the secretary there. Suppose you find some way to make the department run slightly more efficiently: you create this mailing list so that everybody can, when they have an announcement to make, just email it to the mailing list rather than having to put in each person individually in the address field. And that\u2019s a useful thing, that\u2019s a great thing: it didn\u2019t cost anything, other than one-off cost, and now everybody can go about their business more easily. From this perspective, it\u2019s very non-obvious whether that is, in fact, a good thing. It might be contributing to AI\u2014that might be the main effect of this, other than the very small general effect on economic growth. And it might probably be that you have made the world worse in expectation by making this little efficiency improvement. So this project of trying to think through this it\u2019s in a sense a little bit like the Nietzschean <a href=\"https://en.wikipedia.org/wiki/Transvaluation_of_values\">Umwertung aller Werte</a> \u2014 the revaluation of all values\u2014project that he never had a chance to complete, because he went mad&nbsp;before.</p><h2><strong>Possible areas with additional crucial&nbsp;considerations</strong></h2><p>So, these are some kinds of areas\u2014I\u2019m not going to go into all of these, I\u2019m just giving examples of the kinds of areas where today it looks like there might still be crucial considerations. This is not an exhaustive list by any means, and we can talk more about some of those. They kind of go from more general and abstract and powerful, to more specific and understandable by ordinary&nbsp;reasoning.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/pkovqcvkhshdpvcwk6r3\" alt=\"crucial considerations7\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/pkovqcvkhshdpvcwk6r3 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/nqvjgmglwvnbe1zguitk 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/hgwho9rozuehbflzwbls 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/v1ol4r9na9q8plzkpvf9 992w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/a6xgyeabwgt0ijakwv5j 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/q36uhfREa98tec5XM/exgrop8kjueen4awikbj 1800w\"></figure><p>To just pick an example: <i>insects</i>. If you are a classical utilitarian, this consideration arises within the more mundane\u2014we\u2019re setting aside the cosmological commons and just thinking about here on Earth. If insects are sentient then maybe the amount of sentience in insects is very large because there are so very, very many of them. So that maybe the effect of our policies on insect well-being might trump the effect of our policies on human well-being or animals in factories and stuff like that. I\u2019m not saying it does, but it\u2019s a question that is non-obvious and that could have a big&nbsp;impact.</p><p>Or take another&nbsp;example:</p><p><i>Subroutines</i>. With certain kinds of machine intelligence there are processes, like reinforcement learning algorithms and other subprocesses within the AI, that could turn out to have moral status in some way. Maybe there will be hugely large numbers of runs of these subprocesses, so that if it turns out that some of these kinds of things count for something, then maybe the numbers again would come to&nbsp;dominate.</p><h1><strong>Some partial remedies</strong></h1><p>Each of these is a whole workshop on its own, so it\u2019s not something we can go into. But what can one do if one suspects that there might be these crucial considerations, some of them not yet discovered? I don\u2019t have a crisp answer to that. Here are some <i>prima facie</i> plausible things one might try to do a little bit&nbsp;of:</p><ul><li><i>Don\u2019t act precipitously, particularly in ways that are&nbsp;irrevocable.</i></li><li><i>Invest in more analysis to find and assemble missing crucial considerations.</i> That\u2019s why I\u2019m doing the kind of work that I\u2019m doing, and the rest of us are also involved in that&nbsp;enterprise.</li><li><i>Take into account that expected value changes are probably smaller than they appear.</i> If you are a utilitarian, let\u2019s say you think of this new argument that has this radical implication for what you should be doing, the first instinct might be to radically change your expected utility of different practical policies in light of this new insight. But maybe when you reflect on the fact that there are new crucial considerations being discovered every once in awhile, maybe you should still change your expected value, but not as much as it seems you should the first time. You should reflect on this at the meta&nbsp;level.</li><li><i>Take into account fundamental moral uncertainty.</i> If we widen our purview to not just consider utilitarianism, as we should consider things from a more general unrestricted normative perspective, then something like the <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">Parliamentary Model</a> for taking normative uncertainty into account looks fairly robust. This is the idea that if you are unsure as to which moral theory is true, then you should assign probabilities to different moral theories and imagine that there were a parliament where each moral theory got to send delegates to that parliament in proportion to their probability. Then in this imaginary parliament, these delegates from the different moral theories discuss and compromise and work out what to do. And then you should do that what that moral parliament of yours would have decided, as a sort of metaphor. The idea is that, other things equal, the more probability a moral theory has, the greater its say in determining your actions, but there might also be these trades between different moral theories which I think <a href=\"http://www.tobyord.com/\">Toby Ord</a> talked about in <a href=\"https://soundcloud.com/gooddoneright/toby-ord-moral-trade\">his presentation</a>. This is one metaphor for how to conceive of those traits. It might not be exactly the right way to think about fundamental normative uncertainty, but it seems to be close in many situations, and it seems to be relatively robust in the sense of being unlikely to have a totally crazy&nbsp;implication.</li><li><i>Focus more on near-term and convenient objectives.</i> To the extent that one is despairing about having any coherent view about how to go about maximizing aggregative welfare in this cosmological context, the greater it seems the effective voice of other types of things that one might be placing weight. So if you\u2019re partly an egoist and partly an altruist, then if you say that the altruistic component is on this kind of deliberation ladder then maybe you should go more with the egoistic part, until and unless you can find stability in your altruistic&nbsp;deliberations.</li><li><i>Focus on developing our capacity as a civilization to wisely deliberate on these types of things.</i> To build up our capacity, rather than pursuing very specific goals, and by capacity in this context it looks like perhaps we should focus less on powers and more on the propensity to use powers as well. This is still quite vague, but something in that general direction seems to be robustly desirable. Certainly, you could have a crucial consideration that\u2019s turned up to show that that was the wrong thing to do, but it still looks like a reasonable&nbsp;guess.</li></ul><p>That\u2019s it. Thanks.</p><p>&nbsp;</p><p>&nbsp;</p><p><i>This work is licensed under a&nbsp;</i><a href=\"https://creativecommons.org/licenses/by/4.0/\"><i>Creative Commons Attribution 4.0 International License</i></a><i>.</i></p>", "user": {"username": "Pablo_Stafforini"}}, {"_id": "THXP2gy5TqxF8nJkd", "title": "Open Thread #36", "postedAt": "2017-03-15T03:39:05.959Z", "htmlBody": "<html><body><p>Hello, EA Forum! Here is an open thread.</p>\n<p><br>I will kick it&#xA0;off by asking what thoughts people have on saving for retirement while donating more than a set 10% of income.</p>\n<p>I am likely to have a relatively high paying job within a few months and don&apos;t plan on spending most of that income. I plan to divide the rest between retirement savings and donations to x-risk charities, but I don&apos;t have a coherent framework for balancing&#xA0;the creation of passive income with helping preserve the world.&#xA0;</p>\n<p>Ideas on utilizing less-taxed retirement accounts would be appreciated as well. Are there&#xA0;any advantages over DAFs?</p></body></html>", "user": {"username": "ZachWeems"}}, {"_id": "wnzophSMnp49ui3eD", "title": "Ethical Reaction Time: What it is and why it matters", "postedAt": "2017-03-12T02:34:30.078Z", "htmlBody": "<html><body><p>By Matthew Gentzel and Ben Hoskin, cross posted <a href=\"https://theconsequentialist.wordpress.com/2017/03/04/ethical-reaction-time-what-it-is-and-why-it-matters/\">here</a>.</p>\n<p>People often dismiss the practical application of philosophical ideas such as the&#xA0;<a href=\"https://www.youtube.com/watch?v=bOpf6KcWYyw\">trolley problem</a>. In the problem an out of control trolley is racing down a track toward five workers who will not be able to get out of the way in time. If you flip a switch, the trolley will go onto another track, with only one worker who is unable to get out of the way. Flipping the switch sacrifices one to save five, which leads to a hard choice for some people.&#xA0;<br><br>Those who dismiss the usefulness of thought experiments like the trolley problem reason that such situations are unlikely to actually occur in their life, and they wouldn&#x2019;t have enough time to think and make the right choice anyway, because there are many things to account for. This concern is valid: in the trolley problem you don&#x2019;t even have enough time to warn the workers, let alone engage in philosophical speculation! If you&#x2019;re not familiar with the situation, and there&#x2019;s no time to think, how can you be sure you&#x2019;ll make the right choice? &#xA0;To ensure you make the right decision you need to do moral thinking&#xA0;<strong><em>ahead</em></strong>&#xA0;of time and know the situation you are in already when the threat becomes apparent. You need&#xA0;<a href=\"https://wiki.lesswrong.com/wiki/Heuristic\">heuristics</a>&#xA0;for how to act in a variety of situations; deliberation is too slow once an urgent problem arises. But if you can do all that deliberation ahead of time and establish fast habits for doing good, you might end up being able to prevent many trolley problem type dilemmas in the first place.<br><br>Cases as severe as the trolley problem rarely happen in day-to-day life. Less intense situations do arise often: there are many opportunities to influence decisions, prevent accidents, save dozens of people time, and develop relationships where having good heuristics and habits for how to act ahead of time can be valuable. These situations often go unnoticed or unacted upon due to the&#xA0;<a href=\"https://en.wikipedia.org/wiki/Bystander_effect\">bystander effect</a>&#xA0;and&#xA0;<a href=\"https://en.wikipedia.org/wiki/Diffusion_of_responsibility\">diffusion of responsibility</a>. But if we widen our horizons to a global scale, moral dilemmas as bad as the trolley problem happen all the time. Beyond our direct gaze, people are&#xA0;<a href=\"https://en.wikipedia.org/wiki/Preventable_causes_of_death\">dying</a>&#xA0;<a href=\"http://www.who.int/mediacentre/news/releases/2014/air-pollution/en/\">preventable</a>&#xA0;<a href=\"http://www.who.int/malaria/media/world_malaria_report_2013/en/\">deaths</a>&#xA0;- preventable by us, in&#xA0;<a href=\"https://www.givingwhatwecan.org/blog/2015-10-19/reaching-greater-impact-through-us-legislation\">many</a>&#xA0;<a href=\"http://www.givewell.org/international/top-charities/amf\">cases</a>&#xA0;- but the opportunity we have to intervene is easily overlooked because the deaths happen far away or in the future. Many times you only find out about a threat in hindsight when it&#x2019;s already too late to do anything about it. But other times you will find out about a threat or an opportunity just in time to do something about it: you can prevent some moral dilemmas if you act&#xA0;<strong><em>fast</em></strong>.&#xA0;<br><br>This is the main idea behind ethical reaction time: if your ethics involve outcomes and actually helping people, being competently reactive helps! Sometimes it&#x2019;s only possible to do the right thing if you do it quickly; at other times the sooner you act, the better the consequences. This is similar to the idea behind the&#xA0;<a href=\"https://80000hours.org/2012/04/the-haste-consideration/\">haste consideration</a>; what you do with your time now is likely to be more important than what you do in the future, because it influences what you and others will actually be able to do in the future. This compounding effect means that generally, acting sooner is better. Of course, the future is hard to predict, so often the best course of action might only become apparent just before you have to make a decision - hence the need for quick reactions.<br><br>The usefulness of reaction time is demonstrated well in real time strategy games, where the first minutes of the game and speed matter a lot, often measured with metrics such as time to reach a certain technology and&#xA0;<a href=\"https://www.youtube.com/watch?v=YbpCLqryN-Q\">APM</a>&#xA0;(Actions Per Minute). Players with higher meaningful APM can beat players with better general strategy skills simply by being many steps ahead. This speed allows them to build a larger economy faster (more&#xA0;<a href=\"https://80000hours.org/career-guide/career-capital/\">career capital</a>), micro manage units to keep them alive (keep options open), and correct mistakes rapidly. In the military, the concept of the&#xA0;<a href=\"https://en.wikipedia.org/wiki/OODA_loop\">OODA loop</a>&#xA0;(Observe Orient Decide Act) is very similar:</p>\n<p><br><img src=\"https://theconsequentialist.files.wordpress.com/2017/03/ooda-boyd-svg.png?w=680\" alt=\"OODA.Boyd.svg\"><br><em>&#x201C;</em><em>An entity (whether an individual or an organization) that can process this cycle quickly, observing and reacting to unfolding events more rapidly than an opponent, can thereby &quot;get inside&quot; the opponent&apos;s decision cycle and gain the advantage.&#x201D;</em><br><br>What this means is that the faster entity is changing course while the opponent is still deciding what to do about the entity&#x2019;s prior state. This is being &quot;inside&quot; an opponent&#x2019;s decision cycle, the opponent can&#x2019;t catch up, or use superior power to defeat you. Another useful thing about the OODA loop is that it encourages information gathering (observe) and processing it (orient) quickly in a way that just emphasizing reaction time alone does not. Just reacting to situations quickly might be a good thing, but could also involve speeding toward irreversible mistakes.&#xA0;<br><br>Any time doing good takes place in an adversarial environment, this concept is likely to apply. For example while many harmful&#xA0;<a href=\"https://en.wikipedia.org/wiki/Vaccine_controversies\">memes</a>&#xA0;have been quite thoroughly debunked, because they are shared faster than their hosts dismiss them, they can spread anyway and come to rest in minds where they will never be debunked. Likewise, more reactive political organizations also can accumulate power disproportionately fast with respect to their size and cost.&#xA0;<br><br>In summary, a person acting in accordance with the idea of ethical reaction time would do the following:</p>\n<ul>\n<li>Pay attentive to things expected to be relevant to ethical outcomes</li>\n<li>Develop career capital toward positions where you can take action in hard situations</li>\n<li>Develop general rules and heuristics for doing good when there is little time to think</li>\n<li>Train reflexes for acting based on these rules and&#xA0;<a href=\"http://acritch.com/bystander-tactics/\"><strong>eliminate bystander effect</strong></a></li>\n<li>Continually update these rules as you see how they work in the real world</li>\n</ul>\n<p><br><strong>End Notes:</strong><br><br>Initially, this post mostly laid out the idea of ethical reaction time abstractly. With the following examples I hope illustrate the point better:</p>\n<p><strong>Policy:</strong></p>\n<ul>\n<li>\n<p>One can imagine the Reach Every Mother and Child Act might have passed last year if a few congressmen were more responsive in adjusting it to get past partisan objections (left wing opposing funding religious groups, right wing opposing funding contraception). That likely would have saved a few thousand lives, and <a href=\"/ea/pk/how_to_support_and_improve_the_reach_every_mother/\">possibly millions</a> according to USAID. My model of the political constraints on the Reach Act may be wrong here though.</p>\n</li>\n<li>\n<p>Any regulation of technology that starts occurring in response to the technology rapidly coming into existence: Uber executed its plans faster than it could be banned, which was probably good. We don&#x2019;t necessarily want the same to be true for certain tech risks in AI and biology, which makes it important to figure out quickly the correct way to regulate things.</p>\n</li>\n<li>\n<p>Anything on the U.S. Federal Register that is going poorly: if you don&#x2019;t notice a new rule come up relevant to your area of interest (easy to imagine this with animal welfare and new tech) and respond within the comment period, your concerns aren&#x2019;t going to inform the regulation (if you put in relevant research, and get ignored, you can sue the federal agency and win: this happened when the FDA first failed to ban trans fat). This is also a sort of situation that may actually require you to do research under a lot of time pressure.</p>\n</li>\n</ul>\n<p><strong>Donor coordination:</strong></p>\n<ul>\n<li>If your organization is not prepared to accept/talk to donors, there are often times you will lose a lot that you&#x2019;d otherwise get. This I think is one of the reasons David Goldberg with Founder&#x2019;s Pledge would carry a backpack with him containing everything needed for someone to legally commit some % of their venture cash out value to effective charities.</li>\n</ul>\n<p><strong>Start-ups:</strong></p>\n<ul>\n<li>If initially you need partnerships/funding and others are competing for those partnerships/funding then OODA loop applies (but less for funding since there can be more sources).</li>\n</ul>\n<p><strong>Salary negotiations:</strong></p>\n<ul>\n<li>It makes a lot of sense to <a href=\"http://haseebq.com/my-ten-rules-for-negotiating-a-job-offer/\">know how you are going to negotiate ahead of time</a>, or to be very quick in thought. Saying the wrong thing could cost you thousands of dollars, and if you are donating that to AMF, that&#x2019;s likely costing lives.</li>\n</ul>\n<p><strong>Grants/research opportunities:</strong></p>\n<ul>\n<li>\n<p>Grant opportunities = competitive = OODA loop. Less true when there is a deadline that is far away however.</p>\n</li>\n<li>\n<p>In 2015, there were several EA organizations that had the opportunity to get free research from grad students who were interested in Effective Altruism from the School of Public Policy at the University of Maryland. In order to get free research, an organization would have had to submit a general/rough research proposal (&lt;1 page), in which they could enumerate some of the means by which a study or literature review would be undertaken to maintain rigor/ guidelines for the advisor monitoring grad students. No one was able to react within the month after solicitation, so other non-EA organizations got free research instead for the grad student projects class. It does seem reasonable that EA orgs may have had better priorities, and that there is reason to be skeptical of the grad students, but it would have been a good way to get a bunch of students going into policy more bought into EA even if they didn&#x2019;t produce work at a level we&#x2019;d accept. This is also partly my fault, since I could have informed groups earlier, though not by a lot.</p>\n</li>\n</ul>\n<p><strong>Handling the throughput vs. responsiveness trade off:</strong></p>\n<ul>\n<li>If you set up systems so that you can be reactive without draining as much of your attention, you can get more things done generally. Dropping responsiveness and responding to emails once per day or less may make sense if you are a coder/researcher, but it doesn&#x2019;t make sense if you are an information node between organizations that need to coordinate. Adopting simple algorithms like writing down everything in my working memory before taking a call has made me both a lot more willing to take calls, and sped up my ability to get right back to work after interruption.</li>\n</ul>\n<p><strong>Time sensitive opportunities:</strong></p>\n<ul>\n<li>If factory farming and or malaria are going to be gone at some point in the next 20 years due to development/economics, then there won&#x2019;t be the same opportunity to reduce suffering/save lives in the future that there is now. That being said, donations don&#x2019;t require prompt reaction to opportunity the way policy opportunities with respect to these do.</li>\n</ul></body></html>", "user": {"username": "Gentzel"}}, {"_id": "6F6ix64PKEmMuDWJL", "title": "Understanding cause-neutrality", "postedAt": "2017-03-10T17:43:51.345Z", "htmlBody": "<html><body><p>I&apos;m pleased to be able to share <a href=\"https://www.centreforeffectivealtruism.org/blog/understanding-cause-neutrality/\">Understanding cause-neutrality</a>, a new working paper produced by the research team at the Centre for Effective Altruism. (<a href=\"https://assets.contentful.com/es8pp29e1wp8/6Hso7DwFs406oOOw2umIsO/8f51ff68148a248dea9dfc9b41584350/Understandingcause-neutralitypublish.pdf\">PDF version</a>.)</p>\n<h2>Executive summary</h2>\n<p>The term &#x201C;cause-neutrality&#x201D; has been used for at least four concepts. The first aim of this article is to define those concepts.</p>\n<p><em><strong>Cause-impartiality</strong></em> means to select causes based on impartial estimates of impact. This is the concept most frequently associated with the term &#x201C;cause-neutrality&#x201D;. Cause-impartiality can either be seen as entailing <em><a href=\"https://plato.stanford.edu/entries/impartiality/\">moral</a><a href=\"https://en.wikipedia.org/wiki/Streetlight_effect\">&#x2002;</a><a href=\"https://plato.stanford.edu/entries/impartiality/\">impartiality</a></em>, or as pure <em>means-impartiality</em>: choosing the means (e.g., charity evaluation, policy work) to reach one&#x2019;s moral ends impartially.</p>\n<p><em><strong>Cause-agnosticism</strong></em> means uncertainty about how investments (direct work, donations) in different causes compare in terms of impact.</p>\n<p><em><strong>Cause-general investments</strong></em> have a wide <em>scope</em>. They yield capacity which can affect any cause. Cause-general capacity fall into two categories. Cause-flexible capacity (e.g., money) can be flexibly re-allocated across causes. Broad impact capacity (e.g., good epistemics) affect multiple causes without having to be re-directed.</p>\n<p><strong><em>Cause-divergent investments</em></strong> are cause-specific investments in multiple causes (e.g., global poverty, existential risk).</p>\n<p><strong>Figure 1</strong>: Decision process for altruistic investments (the four concepts&#x2019; antonyms in black).</p>\n<p><img src=\"https://images.contentful.com/es8pp29e1wp8/6I7d0kgmxaeUMaOaysYiOu/d89da0bc4dcea18cca2e00fe61e00002/Cause_neutrality_1.png?w=100&amp;q=80\" alt=\"Cause neutrality 1\"></p>\n<p>My second aim is to give a survey of considerations on the value of cause-impartiality, cause-agnosticism, cause-generality, and cause-divergence. In these sections, I among other things discuss the relations between the four concepts.</p>\n<p>Though cause-impartiality is sometimes mixed up with the other three concepts, it does not entail any of them. Cause-agnosticism can be a reason for cause-divergent and cause-general investments. Cause-divergent and cause-flexible investments can substitute for each other, whereas cause-divergent and broad impact investments can complement each other. Recruiting cause-impartial individuals amounts to a cause-flexible investment.</p></body></html>", "user": {"username": "Stefan_Schubert"}}]