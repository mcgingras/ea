[{"_id": "esbPsw4hBoWxHfKqe", "postedAt": "2024-03-27T16:46:35.378Z", "postId": "GrHE8WLtLSmJDWNZJ", "htmlBody": "<p>I think these articles are very good.&nbsp;</p><p>In my opinion, the singularity hypothesis is the widely held EA belief that is the least backed by evidence or even argumentation. People will just throw out claims like \"AGI will solve cold fusion in 6 months\", or \"AGI will cure death within our lifetime\" without feeling the need to provide any caveats or arguments in their favor, as if the imminent arrival of god-AI is obvious common knowledge.&nbsp;</p><p>I'm not saying you can't believe this stuff (although I believe you will be wrong), but at least treat them like the extraordinary claims they are.&nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "njtfW56WQQtczaYsc", "postedAt": "2024-03-27T16:56:02.904Z", "postId": "GrHE8WLtLSmJDWNZJ", "htmlBody": "<p>I support people poking at the foundations of these arguments. And I especially appreciated the discussion of bottlenecks, which I think is an important topic and often brushed aside in these discussions.</p><p>That said, I found that this didn't really speak to the reasons I find most compelling in favour of something like the singularity hypothesis. Thorstad says in the second blog post:</p><blockquote><p>If each doubling of intelligence is harder to bring about than the last, then even if all AI research is eventually done by recursively self-improving AI systems, the pace of doubling will steadily slow.</p></blockquote><p>I think this is wrong. (Though the paper itself avoids making the same mistake.) There are lots of coherent models where the effective research output of the AI systems is growing faster than the difficulty of increasing intelligence, leading to accelerating improvements despite each doubling of intelligence getting harder than the last. These are closely analogous to the models which can (depending on some parameter choices) produce a singularity in economic growth by assuming endogenous technological growth.</p><p>In general I agree with Thorstad that the notion of \"intelligence\" is not pinned down enough to build tight arguments on it. But I think that he goes too far in inferring that the arguments aren't there. Rather I think that the strongest versions of the arguments don't directly route through an analysis of intelligence, but something more like the economic analysis. If further investments in AI research drive the price-per-unit-of-researcher-year-equivalent down in fast enough, this could lead to hyperbolic increases in the amount of effective research progress, and this could in turn lead to rapid increases in intelligence -- however one measures that. I agree that this isn't enough to establish that things will be \"orders of magnitude smarter than humans\", but for practical purposes the upshot that \"there will be orders of magnitude more effective intellectual labour from AI than from humans\" does a great deal of work.</p><p>On the argument that extraordinary claims require extraordinary evidence, I'd have been interested to see Thorstad's takes on the analyses which suggest that long-term historical growth rates <i>are</i> hyperbolic, e.g. <a href=\"https://www.openphilanthropy.org/research/modeling-the-human-trajectory/\">Roodman (2020)</a>. I think of that as one of the more robust long-term patterns in world history. The hypothesis which says \"this pattern will approximately continue\" doesn't feel to me to be extraordinary. You might say \"ah, but that doesn't imply a singularity in <i>intelligence</i>\", and I would agree -- but I think that if you condition on this kind of future hyperbolic growth in the economy, the hypothesis that there will be a very large accompanying increase in intelligence (however that's measured) also seems kind of boring rather than extraordinary.</p>", "parentCommentId": null, "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "eXGpikv46qkoAKudz", "postedAt": "2024-03-27T17:13:38.791Z", "postId": "GrHE8WLtLSmJDWNZJ", "htmlBody": "<p>Just noting that these are possibly much stronger claims than \"AGI will be able to completely disempower humanity\" (depending on how hard it is to solve cold fusion a-posteriori).</p>\n", "parentCommentId": "esbPsw4hBoWxHfKqe", "user": {"username": "Nick K."}}, {"_id": "egRTzDeXxFmX5fezv", "postedAt": "2024-03-28T04:57:50.089Z", "postId": "GrHE8WLtLSmJDWNZJ", "htmlBody": "<p>Here's the talk version for anyone who finds it easier to listen to videos:&nbsp;</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=dlDcFVBXqKQ\"><div><iframe src=\"https://www.youtube.com/embed/dlDcFVBXqKQ\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure>", "parentCommentId": null, "user": {"username": "David Thorstad"}}, {"_id": "LnkDnSHwJmQKaaJSx", "postedAt": "2024-03-28T19:15:27.687Z", "postId": "GrHE8WLtLSmJDWNZJ", "htmlBody": "<blockquote><p>Circuits\u2019 energy requirements have massively increased\u2014increasing costs and overheating.[6]</p></blockquote><p><br>I'm not sure I understand this claim, and I can't see that it's supported by the cited paper.&nbsp;<br><br>Is the claim that energy costs have increased faster than computation? This would be cruxy, but it would also be incorrect.&nbsp;</p>", "parentCommentId": null, "user": {"username": "ElliotJDavies"}}, {"_id": "LQy7rrGdehPTz2HHp", "postedAt": "2024-03-28T19:47:47.241Z", "postId": "GrHE8WLtLSmJDWNZJ", "htmlBody": "<blockquote><p><strong>Intelligence Explosion:</strong>&nbsp;For a <i>sustained</i>&nbsp;period<br><br>[...]<br><br><strong>Extraordinary claims require extraordinary evidence:</strong>&nbsp;Proposing that exponential or hyperbolic growth will occur for a <i><strong>prolonged</strong></i><strong>&nbsp;period </strong>[Emphasis mine]</p></blockquote><p>&nbsp;</p><ul><li>I'm not sure why \"prolonged period\" or \"sustained\" was used here?</li><li>I am also not sure what is meant by prolonged period? 5 years? 100 years?&nbsp;<ul><li>For the answer to the above, why do you believe would this be required?&nbsp;</li></ul></li></ul><p>Just to help nail down the crux here, I don't see why more than a few days of an intelligence explosion is required for a singularity event.<br><br>&nbsp;</p>", "parentCommentId": null, "user": {"username": "ElliotJDavies"}}, {"_id": "2fCiArbPDass7r9GA", "postedAt": "2024-03-28T20:00:19.438Z", "postId": "GrHE8WLtLSmJDWNZJ", "htmlBody": "<p>I feel this claim is disconnected with the definition of the singularity given in the paper:&nbsp;</p><blockquote><p>The singularity hypothesis begins with the supposition that artificial agents will gain the ability to improve their own intelligence. From there, it is claimed that the intelligence of artificial agents will grow at a rapidly accelerating rate, producing an intelligence explosion in which artificial agents quickly become orders of magnitude more intelligent than their human creators. The result will be a singularity, understood as a fundamental discontinuity in human history beyond which our fate depends largely on how we interact with artificial agents</p></blockquote><p>Further in the paper you write:&nbsp;</p><blockquote><p>The singularity hypothesis posits a <strong>sustained period</strong> of accelerating growth in the general intelligence of artificial agents.</p></blockquote><p>[Emphasis mine]. I can't see any reference for either the original definition and later addition of \"sustained\".&nbsp;</p>", "parentCommentId": "LQy7rrGdehPTz2HHp", "user": {"username": "ElliotJDavies"}}, {"_id": "eNLcByzzCKqadnFXe", "postedAt": "2024-03-28T21:57:39.294Z", "postId": "GrHE8WLtLSmJDWNZJ", "htmlBody": "<p>Ah - that comes from the discontinuity claim. If you have accelerating growth that isn't sustained for very long, you get something like population growth from 1800-2000, where the end result is impressive but hardly a discontinuity comparable to crossing the event horizon of a black hole.&nbsp;</p><p>(The only way to go around the assumption of sustained growth would be to post one or a few discontinuous leaps towards superintelligence. But that's harder to defend, and it abandons what was classically taken to ground the singularity hypothesis, namely the appeal to recursive self-improvement).&nbsp;</p>", "parentCommentId": "2fCiArbPDass7r9GA", "user": {"username": "David Thorstad"}}, {"_id": "MXC3fxJqeLG5KFXMy", "postedAt": "2024-03-28T22:02:37.435Z", "postId": "GrHE8WLtLSmJDWNZJ", "htmlBody": "<p>Here's a gentle introduction to the kinds of worries people have (https://spectrum.ieee.org/power-problems-might-drive-chip-specialization). Of the cited references \"the chips are down for moore's law\" is probably best on this issue, but a little longer/harder. There's plenty of literature on problems with heat dissipation if you search the academic literature. I can dig up references on energy if you want, but with Sam Altman saying we need a fundamental energy revolution even to get to AGI, is there really much controversy over the idea that we'll need a lot of energy to get to superintelligence?&nbsp;</p>", "parentCommentId": "LnkDnSHwJmQKaaJSx", "user": {"username": "David Thorstad"}}, {"_id": "jxReDofynGQKfDZHD", "postedAt": "2024-03-29T09:15:46.887Z", "postId": "GrHE8WLtLSmJDWNZJ", "htmlBody": "<p>As you write:&nbsp;</p><blockquote><p>The result will be a singularity, understood as a fundamental discontinuity in human history beyond which our fate depends largely on how we interact with artificial agents</p></blockquote><p>The discontinuity is a result of humans no longer being the smartest agents in the world, and no longer being in control of our own fate. After this point, we've entered an event horizon where the output is almost entirely unforeseeable.&nbsp;</p><blockquote><p>If you have accelerating growth that isn't sustained for very long, you get something like population growth from 1800-2000</p></blockquote><p>If, after surpassing humans, intelligence \"grows\" exponentially for another 200 years, do you not think we've passed an event horizon? I certainly do!<br><br>If not, using the metric of single agent intelligence (i.e. not the sum of intelligence in a group of agents), at what point during an exponential growth curve that intersects human level intelligence, &nbsp;would you defining as crossing the event horizon?&nbsp;</p>", "parentCommentId": "eNLcByzzCKqadnFXe", "user": {"username": "ElliotJDavies"}}]