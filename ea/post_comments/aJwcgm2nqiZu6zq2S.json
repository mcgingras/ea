[{"_id": "sJECTe5obAaofPQQJ", "postedAt": "2023-02-23T20:02:45.319Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>I notice that I am surprised and confused.</p>\n<p>I'd have expected Holden to contribute much more to AI existential safety as CEO of Open Philanthropy (career capital, comparative advantage, specialisation, etc.) than via direct work.</p>\n<p>I don't really know what to make of this.</p>\n<p>That said, it sounds like you've given this a lot of deliberation and have a clear plan/course of action.</p>\n<p>I'm excited about your endeavours in the project!</p>\n", "parentCommentId": null, "user": {"username": "Dragon God"}}, {"_id": "ujetQgTruG3c46z9E", "postedAt": "2023-02-23T20:26:02.944Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>Is it at all fair to say you\u2019re shifting your strategy from a \u201cmarathon\u201d to a \u201csprint\u201d strategy? I.e. prioritising work that you expect to help soon instead of later.</p>\n<p>Is this move due to your personal timelines shortening?</p>\n", "parentCommentId": null, "user": {"username": "GMcGowan"}}, {"_id": "YtXhivT26ECi8zay7", "postedAt": "2023-02-23T20:47:17.396Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>As AI heats up, I'm excited and frankly somewhat relieved to have Holden making this change. While I agree with \ud835\udd6e\ud835\udd8e\ud835\udd93\ud835\udd8a\ud835\udd97\ud835\udd86's comment below that Holden had a lot of leverage on AI safety in his recent role, I also believe he has an vast amount of domain knowledge that can be applied more directly to problem solving. We're in shockingly short supply of that kind of person, and the need is urgent.<br><br>Alexander has my full confidence in his new role as the sole CEO. I consider us incredibly fortunate to have someone like him already involved and and prepared to of succeed as the leader of Open Philanthropy.</p>", "parentCommentId": null, "user": {"username": "Dustin Moskovitz"}}, {"_id": "thEmZf5HZovZKJ2it", "postedAt": "2023-02-23T21:48:46.220Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>RE direct work, I would generally think of the described role as still a form of \"leadership\" \u2014 coordinating actors in the present \u2014 unlike &nbsp;\"writing research papers\" or \"writing code\". I expect Holden to have a strong comparative advantage at leadership-type work.</p>", "parentCommentId": "sJECTe5obAaofPQQJ", "user": {"username": "catherio"}}, {"_id": "XwztLgjatxYFMftvM", "postedAt": "2023-02-23T23:00:22.971Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>I've been meaning to ask: Are there plans to turn your Cold Takes posts on AI safety and The Most Important Century into a published book? I think the posts would make for a very compelling book, and a book could reach a much broader audience and would likely get much more attention. (This has pros and cons of course, as you've discussed in your posts.)</p>\n", "parentCommentId": null, "user": {"username": "iarwain"}}, {"_id": "XxCb2nfnWDghSBHj2", "postedAt": "2023-02-23T23:14:22.980Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>Yes, it would be very different if he'd said \"I'm going to skill up on ML and get coding\"!</p>\n", "parentCommentId": "thEmZf5HZovZKJ2it", "user": {"username": "Michael_PJ"}}, {"_id": "FvuAaqfz7ezcmjRXs", "postedAt": "2023-02-24T01:06:17.307Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>I'd love to chat with you about directions here, if you're interested. I don't know anyone with a bigger value of &nbsp;p(survival | West Wing levels of competence in major governments) - p(survival | leave it to OpenAI and DeepMind leadership). I've published technical AI existential safety research at top ML conferences/journals, and I've gotten two MPs in the UK onside this week. You can see my work at <a href=\"http://www.michael-k-cohen.com\">michael-k-cohen.com</a>, and you can reach me at michael.cohen@eng.ox.ac.uk.</p>", "parentCommentId": null, "user": {"username": "Michael_Cohen"}}, {"_id": "oEsMbprF8LMunNkMg", "postedAt": "2023-02-24T01:26:30.122Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>I think your first priority is promising and seemingly neglected (though I'm not familiar with a lot of work done by governance folk, so I could be wrong here). I also get the impression that MIRI folk believe they have an unusually clear understanding of risks, would like to see risky development slow down and are pessimistic about their near-term prospects for solving technical problems of aligning very capable intelligent systems and generally don't see any clearly good next steps. It appears to me that this combination of skills and views positions them relatively well for developing AI safety standards. I'd be shocked if you didn't end up talking to MIRI about this issue, but I just wanted to point out that from my point of view there seems to be a substantial amount of fit here.</p>", "parentCommentId": null, "user": {"username": "David Johnston"}}, {"_id": "pXv4YLTakLdMxnhJD", "postedAt": "2023-02-24T08:06:53.679Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>In your recent Cold Takes <a href=\"https://www.cold-takes.com/what-ai-companies-can-do-today-to-help-with-the-most-important-century/\">post</a> you disclosed that your wife owns equity in both OpenAI and Anthropic. (She was <a href=\"https://forum.effectivealtruism.org/posts/qvEo8kzn3zMEriGtN/questions-about-op-grant-to-helena?commentId=TGSdLorgvJ3ovFdjQ\">appointed</a> to a VP position at OpenAI, as was her sibling, after you joined OpenAI's board of directors<sup class=\"footnote-ref\"><a href=\"#fn-yNkwF6o9zNeWu2hxP-1\" id=\"fnref-yNkwF6o9zNeWu2hxP-1\">[1]</a></sup>). In 2017, under your leadership, OpenPhil decided to generally <a href=\"https://forum.effectivealtruism.org/posts/EooyY6XeebdtSaXz5/does-open-philanthropy-have-a-public-document-regarding?commentId=tnbaAfRQRMPJvExsj\">stop publishing</a> \"relationship disclosures\". How do you intend to handle conflicts of interest, and transparency about them, going forward?</p>\n<p>You wrote here that the first intervention that you'll explore is AI safety standards that will be \"enforced via self-regulation at first, and potentially government regulation later\". AI companies can easily end up with \"self-regulation\" that is mostly optimized to appear helpful, in order to avoid regulation by governments. Conflicts of interest can easily influence decisions w.r.t. regulating AI companies (mostly via biases and self-deception, rather than via conscious reasoning).</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-yNkwF6o9zNeWu2hxP-1\" class=\"footnote-item\"><p>EDIT: you <a href=\"https://www.openphilanthropy.org/grants/openai-general-support/\">joined</a> OpenAI's board of directors as part of a deal between OpenPhil and OpenAI that involved recommending a $30M grant to OpenAI. <a href=\"#fnref-yNkwF6o9zNeWu2hxP-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "parentCommentId": null, "user": {"username": "ofer"}}, {"_id": "cCLEqFR7AGztpZWvs", "postedAt": "2023-02-24T09:39:53.508Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>Can Holden clarify if and if so what proportion of those shares in OpenAI and Anthropic are legally pledged for donation?</p>", "parentCommentId": "pXv4YLTakLdMxnhJD", "user": {"username": "anolderea"}}, {"_id": "etJ7mFoaPWg2F4BBA", "postedAt": "2023-02-24T09:40:15.052Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<blockquote>\n<p>MIRI folk believe they have an unusually clear understanding of risks</p>\n</blockquote>\n<p>\"Believe\" being the operative word here. I really don't think they do.</p>\n", "parentCommentId": "oEsMbprF8LMunNkMg", "user": {"username": "Guy Raveh"}}, {"_id": "ARrptjQBx7iY9tqcQ", "postedAt": "2023-02-24T11:02:19.509Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>Amazon: <a href=\"https://www.amazon.com/Most-Important-Century-Holden-Karnofsky/dp/B09SBNJSKM\">The Most Important Century Paperback \u2013 February 12, 2022 </a>by <a href=\"https://www.amazon.com/s/ref=dp_byline_sr_book_1?ie=UTF8&amp;field-author=Holden+Karnofsky&amp;text=Holden+Karnofsky&amp;sort=relevancerank&amp;search-alias=books\">Holden Karnofsky</a></p>", "parentCommentId": "XwztLgjatxYFMftvM", "user": {"username": "WilliamKiely"}}, {"_id": "NtnGF4vpwxQqqkoQv", "postedAt": "2023-02-24T19:36:51.224Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>I don't think Holden agrees with this as much as you might think. For example, he spent a lot of his time in the last year or two <a href=\"https://cold-takes.com/\">writing a blog</a>.</p>", "parentCommentId": "thEmZf5HZovZKJ2it", "user": {"username": "Buck"}}, {"_id": "8FvWXsbKDpBGJkF6o", "postedAt": "2023-02-24T22:44:24.394Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>(I work at Open Phil, speaking for myself)<br><br>FWIW, I think this could also make a lot of sense. I don't think Holden would be an individual contributor writing code forever, but skilling up in ML and completing concrete research projects seems like a good foundation for ultimately building a team doing something in AI safety.</p>", "parentCommentId": "XxCb2nfnWDghSBHj2", "user": {"username": "Ajeya"}}, {"_id": "BBNJnHAg6te33eTpG", "postedAt": "2023-02-24T23:20:02.355Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>Neat! Cover jacket could use a graphic designer in my opinion. It's also slotted under engineering? Am I missing something?</p>", "parentCommentId": "ARrptjQBx7iY9tqcQ", "user": {"username": "JoelMcGuire"}}, {"_id": "cdeP7GaZDT6A4HBD5", "postedAt": "2023-02-25T09:15:49.908Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>My understanding is that Alexander has different views from Holden in that he prioritises global health and wellbeing over longtermist cause areas. Is there a possibility that Open Phil's longtermist giving decreases due to having a \"non-longtermist\" at the helm?</p>", "parentCommentId": "YtXhivT26ECi8zay7", "user": {"username": "jackmalde"}}, {"_id": "s4qAB2gRMoSKJfTcs", "postedAt": "2023-02-25T12:04:09.713Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>I'm not sold on how well calibrated their predictions of catastrophe are, but I think they have contributed a large number of novel &amp; important ideas to the field.</p>", "parentCommentId": "etJ7mFoaPWg2F4BBA", "user": {"username": "David Johnston"}}, {"_id": "9NB2uW4qd9oCxZqfp", "postedAt": "2023-02-25T16:29:31.149Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>I believe that\u2019s an oversimplification of what Alexander thinks but don\u2019t want to put words in his mouth.</p>\n<p>In any case this is one of the few decisions the 4 of us (including Cari) have always made together so we have done a lot of aligning already. My current view, which is mostly shared, is we\u2019re currently underfunding x-risk even without longtermism math, both because FTXF went away and because I\u2019ve updated towards shorter AI timelines in the past ~5 years. And even aside from that, we weren\u2019t at full theoretical budget last year anyway. So that all nets out that to expected increase, not decrease.</p>\n<p>I\u2019d love to discover new large x-risk funders though and think recent history makes that more likely.</p>\n", "parentCommentId": "cdeP7GaZDT6A4HBD5", "user": {"username": "Dustin Moskovitz"}}, {"_id": "MMRB5hj4FjJgivHzP", "postedAt": "2023-02-25T16:48:08.376Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>OK, thanks for sharing!</p><p>And yes I may well be oversimplifying Alexander's view.</p>", "parentCommentId": "9NB2uW4qd9oCxZqfp", "user": {"username": "jackmalde"}}, {"_id": "kJTaiKgwXnWZThHgf", "postedAt": "2023-03-01T07:18:29.807Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>I don't think they would claim to have significantly better predictive models in a positive sense, they just have far stronger models of what isn't possible and cannot work for ASI, and it constrains their expectations about the long term far more. (I'm not sure I agree with, say, Eliezer about his view of uselessness of governance, for example - but he has a very clear model, which is unusual.) I also don't think their view about timelines or takeoff speeds is really a crux - they have claimed that even if ASI is decades away, we still can't rely on current approaches to scale.</p>", "parentCommentId": "etJ7mFoaPWg2F4BBA", "user": {"username": "Davidmanheim"}}, {"_id": "XToooKNB7k5Fxrbnh", "postedAt": "2023-03-01T07:19:33.822Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<blockquote><p>are pessimistic about their near-term prospects for solving technical problems of aligning very capable intelligent systems and generally don't see any clearly good next steps</p></blockquote><p>I don't think they claim to have better longer-term prospects, though.</p>", "parentCommentId": "oEsMbprF8LMunNkMg", "user": {"username": "Davidmanheim"}}, {"_id": "udbr2LGhuESNvccRa", "postedAt": "2023-03-01T09:18:09.561Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>I think they do? Nate at least says he\u2019s optimistic about finding a solution given more time</p>\n", "parentCommentId": "XToooKNB7k5Fxrbnh", "user": {"username": "David Johnston"}}, {"_id": "wztsboRoDPsJjeZHW", "postedAt": "2023-03-03T16:14:05.098Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>You may have already thought of this, but one place to start exploring what AI standards might look like is exploring what other safety standards for developing risky new things do in fact look like. The one I'm most familiar with (but not at all an expert on) is DO-178C Level A, the standard for developing avionics software where a bug could crash the plane. \"Softer\" examples worth looking at would include the SOC2 security certification standards.</p><p>I wrote a related thing here as a public comment to the NIST regulation framework developers, who I presume are high on your list to talk to as well: https://futuremoreperfect.substack.com/p/ai-regulation-wonkery</p>", "parentCommentId": null, "user": {"username": "Nicholas Weininger"}}, {"_id": "5cax7hBKqendp2Qwr", "postedAt": "2023-03-20T16:17:03.434Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>I\u2019m in no position to judge how you should spend your time all things considered, but for what it\u2019s worth, I think your blog posts on AI safety have been very clear and thoughtful, and I frequently recommend them to people (<a href=\"https://www.reddit.com/r/ControlProblem/comments/11pvrl9/comment/jc08pre/?utm_source=share&amp;utm_medium=web2x&amp;context=3\">example</a>). For example, I\u2019ve started using the phrase <a href=\"https://www.cold-takes.com/ai-safety-seems-hard-to-measure/\">\u201cThe King Lear Problem\u201d</a> from time to time (<a href=\"https://twitter.com/steve47285/status/1610690304629407745\">example</a>).</p><p>Anyway, good luck! And let me know if there\u2019s anything I can do to help you. \ud83d\ude42</p>", "parentCommentId": null, "user": {"username": "steve2152"}}, {"_id": "i2bSqyj9a3mFLJ4DJ", "postedAt": "2023-03-21T02:11:28.716Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>I threw that book together for people who want to read it on Kindle, but it\u2019s quite half-baked. If I had the time, I\u2019d want to rework the series (and a more recent followup series at <a href=\"https://www.cold-takes.com/tag/implicationsofmostimportantcentury/\">https://www.cold-takes.com/tag/implicationsofmostimportantcentury/</a>) into a proper book, but I\u2019m not sure when or whether I\u2019ll do this.</p>\n", "parentCommentId": "BBNJnHAg6te33eTpG", "user": {"username": "HoldenKarnofsky"}}, {"_id": "iC3sD9kzKuJgCLQLD", "postedAt": "2023-03-21T02:12:44.392Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>I wouldn\u2019t say I\u2019m in \u201csprinting\u201d mode - I don\u2019t expect my work hours to go up (and I generally work less than I did a few years ago, basically because I\u2019m a dad now).</p>\n<p>The move is partly about AI timelines, partly about the opportunities I see and partly about Open Philanthropy\u2019s stage of development.</p>\n", "parentCommentId": "ujetQgTruG3c46z9E", "user": {"username": "HoldenKarnofsky"}}, {"_id": "JJhnJWZo3e4jmBMjf", "postedAt": "2023-03-22T22:40:28.474Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>For context, my wife is the President and co-founder of Anthropic, and formerly worked at OpenAI.</p>\n<p>80% of her equity in Anthropic is (not legally bindingly) pledged for donation. None of her equity in OpenAI is. She may pledge more in the future if there is a tangible compelling reason to do so.</p>\n<p>I plan to be highly transparent about my conflict of interest, e.g. I regularly open meetings by disclosing it if I\u2019m not sure the other person already knows about it, and I\u2019ve often mentioned it when discussing related topics on Cold Takes.</p>\n<p>I also plan to discuss the implications of my conflict of interest for any formal role I might take. It\u2019s possible that my role in helping with safety standards will be limited to advising with no formal powers (it\u2019s even possible that I\u2019ll decide I simply can\u2019t work in this area due to the conflict of interest, and will pursue one of the other interventions I\u2019ve thought about).</p>\n<p>But right now I\u2019m just exploring options and giving non-authoritative advice, and that seems appropriate. (I\u2019ll also note that I expect a lot of advice and opinions on standards to come from people who are directly employed by AI companies; while this does present a conflict of interest, and a more direct one than mine, I think it doesn\u2019t and can\u2019t mean they are excluded from relevant conversations.)</p>\n", "parentCommentId": "cCLEqFR7AGztpZWvs", "user": {"username": "HoldenKarnofsky"}}, {"_id": "zXGsxAMmGyhhc6mXq", "postedAt": "2023-03-29T07:04:48.970Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>Thanks for the clarification.</p>", "parentCommentId": "JJhnJWZo3e4jmBMjf", "user": {"username": "anolderea"}}, {"_id": "htiJtJMhzpdJfhfpj", "postedAt": "2023-05-25T21:42:02.014Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>For what it's worth, I don't see an option to buy a kindle version on Amazon - <a href=\"https://imgur.com/a/lY3sxUK\">screenshot here</a></p>\n", "parentCommentId": "i2bSqyj9a3mFLJ4DJ", "user": {"username": "DanielFilan"}}, {"_id": "fakLESYWq7T3ub2oL", "postedAt": "2023-06-02T22:51:16.085Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p>I think this was a goof due to there being a separate hardcover version, which has now been removed - try again?</p>\n", "parentCommentId": "htiJtJMhzpdJfhfpj", "user": {"username": "HoldenKarnofsky"}}, {"_id": "Jov3x6CuhnSNNxWyE", "postedAt": "2023-06-05T18:54:08.981Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": "<p><a href=\"https://www.amazon.com/most-important-century-blog-Takes-ebook/dp/B09GXJ4GJ3/\">This link</a> works.</p>\n", "parentCommentId": "fakLESYWq7T3ub2oL", "user": {"username": "DanielFilan"}}, {"_id": "bqqifmZzcmdiEWLc9", "postedAt": "2023-03-26T04:26:12.524Z", "postId": "aJwcgm2nqiZu6zq2S", "htmlBody": null, "parentCommentId": null, "user": null}]