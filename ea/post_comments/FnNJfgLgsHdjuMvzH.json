[{"_id": "gwdbBRdEKmdyE6EnN", "postedAt": "2023-12-06T04:06:18.284Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I suppose one of the main factors to take into consideration is what percent of donors want to fund cause agnostic EA projects vs. what percent want to fund any kind of EA-adjacent community building and want the fund managers to figure out what is most impactful with that.</p>\n", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "pk25oo9npB7wYThoW", "postedAt": "2023-12-06T04:18:47.332Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>Scattered first impressions:</p><ul><li>I feel generally very positively about this update and have personally felt confused about the scope of EAIF when referring other people to it.</li><li>There are wide grey areas when attempting to delineate principles-first EA from cause-specific EA and the effective giving examples in this post stand out to me as one thorny area. I think it may make sense not to fund an AI-specific or an animal-specific effective giving project through EAIF (and the LTTF and AWF are more appropriate), but an effective giving project that e.g. takes a longtermist approach or is focused on near-term human <strong>and</strong> nonhuman welfare seems different to me. Put differently: How do you think about projects that don't cover <i>all</i> of EA, but also aren't limited to <i>one</i> cause area?</li><li>For this out-of-scope example in particular, I'm not sure where I would route someone to pursue alternative funding in a timely fashion:</li></ul><blockquote><p>Funding a very promising biology PhD student to attend a one-month program run by a prestigious US think tank to understand better how the intelligence community monitors various kinds of risk, such as biological threats ($6,000)</p></blockquote><p>Maybe Lightspeed? But I worry there isn't currently other coverage for funding needs of this sort.</p><ul><li>I'm worried about people couching cause-specific projects as principles-first, but there is already a heavy tide pushing people to couch principles-first projects as x-risk-specific, so this might not be a concern.</li><li>I'm really happy to see you thinking about digital minds and (seemingly) how to grow s-risk projects.</li></ul>", "parentCommentId": null, "user": {"username": "Rockwell Schwartz"}}, {"_id": "dhpKS2kddQ7wxwcJK", "postedAt": "2023-12-06T04:20:47.443Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>Thanks for your comment. I\u2019m not able to respond to the whole comment right now but I think the bio career grant is squarely in the scope of the LTFF.</p>\n", "parentCommentId": "pk25oo9npB7wYThoW", "user": {"username": "calebp"}}, {"_id": "KHirakF8dgZTQR3gB", "postedAt": "2023-12-06T04:28:36.283Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>Makes sense, thank you! Maybe my follow-up questions would be: How confident would they need to be that they'd use the experience to work on biorisk vs. global health before applying to the LTFF? And if they were, say, 75:25 between the two, would EAIF become the right choice -- or what ratio would bring this grant into EAIF territory?</p>\n", "parentCommentId": "dhpKS2kddQ7wxwcJK", "user": {"username": "Rockwell Schwartz"}}, {"_id": "CgrRZtrQMPNHxd2ow", "postedAt": "2023-12-06T11:23:36.003Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>Very excited about this, both about the clarification of scope and the scope itself.</p>\n<p>I strongly agree there is currently a gap in terms of principles-first EA funders, and also largely agree with the way you've outlined \"principles-first EA\" here. I think this new scope will make me seriously consider becoming a donor to the EAIF in the new year.</p>\n", "parentCommentId": null, "user": {"username": "SjirH"}}, {"_id": "HxW3XEBh5kE7DiNXp", "postedAt": "2023-12-06T11:39:15.833Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I echo this view and think it's really exciting. I expect many people in the meta-funding space will be positive about this idea. However, I also anticipate that many of the donors will need to see a round or two of this idea executed and observe the resulting grants before donating to the fund.</p>", "parentCommentId": "CgrRZtrQMPNHxd2ow", "user": {"username": "Joey"}}, {"_id": "a7sQek7kqzozcJApZ", "postedAt": "2023-12-06T16:22:54.616Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>Agreed (though personally I might be willing to make a bet if e.g. fund manager selection is done well)</p>\n", "parentCommentId": "HxW3XEBh5kE7DiNXp", "user": {"username": "SjirH"}}, {"_id": "JXSTC8P7tvdtycYvS", "postedAt": "2023-12-06T16:56:26.599Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p><i>I wrote the following on a draft of this post. For context, I currently do (very) part-time work at EAIF</i></p><p>Overall, I\u2018m pretty excited to see EAIF orient to a principles-first EA. Despite recent challenges, I continue to believe that the EA community is doing something special and important, and is fundamentally worth fighting for. With this reorientation of EAIF, I hope we can get the EA community back to a strong position. I share many of the uncertainties listed - about whether this is a viable project, how EAIF will practically evaluate grants under this worldview, or if it\u2019s even philosophically coherent. Nonetheless, I\u2019m excited to see what can be done.</p>", "parentCommentId": null, "user": {"username": "22tom"}}, {"_id": "8kaobCsysC8kHjt88", "postedAt": "2023-12-06T17:36:41.109Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>This makes a lot of sense generally, but I see one issue that seems potentially significant.</p><p>I have a fairly good understanding of what will happen to more cause-area-specific yet \"meta\" grants in the x-risk/longtermism and animal-welfare domains. The view that the LTFF and AWF are better suited to funding these opportunities seems fairly compelling. The issue I see is that the EA Funds' Global Health and Development Fund (GHDF) seems to have focused on larger grants to more established organizations; this makes sense given its strong connection to GiveWell's work. That doesn't feel like a good fit for opportunities like the ones described by (4) and (5) of your examples of out-of-scope projects. According to its <a href=\"https://funds.effectivealtruism.org/funds/global-development\">website</a>, GHDF isn't even accepting applications. Thus, while these sorts of projects are not formally outside of GHDF's scope -- e.g., it has granted to One for the World -- it seems that they may be inaccessible as a practical matter.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8eqsk8k3xd6\"><sup><a href=\"#fn8eqsk8k3xd6\">[1]</a></sup></span></p><p>Perhaps the ideal solution would be for GHDF to start taking applications that would previously have been within EAIF's scope, so that there is a relatively seamless transition for potential and established grantees. I'm not sure if that is practicable for GHDF, though?</p><p>A second possibility would be for EAIF to retain the global-health/development scope for a stated time period, but (for donations received after a specified date in 2024) only out of donor funds that have been designated for that specific scope. That would allow more clarity of scope for EAIF donors while providing a conduit for donors who feel strongly about global-health/development meta work.</p><p>Finally, the exit strategy could be slowed down for global health/development specifically, in recognition of the lack of an obvious alternative fund for these sorts of grants. Although exit grants would soften existing grantees' landing for projects receiving ongoing support, it seems plausible that potential grantees may have done significant groundwork for new projects or expansions based on the funding universe as it existed prior to this plan being made public. Moreover, even if one expects other grantmakers would eventually step in to fill the void, this would likely take time. Thus, 1Q 2024 may be too soon for phasing out the current version of EAIF, at least where global health/development meta activity is concerned.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8eqsk8k3xd6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8eqsk8k3xd6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'm aware of Open Phil's <a href=\"https://www.openphilanthropy.org/research/new-grantmaking-program-supporting-the-effective-altruism-community-around-global-health-and-wellbeing/\">work in global health/wellbeing community building</a>, but as you note one of the objectives here is to move toward \"a more community-funded model, in contrast to . . . previous reliance on significant institutional donors like Open Phil.\" A plan in which Open Phil picks up responsibility for funding these sorts of grants in global health/wellbeing seems like a step backward from this objective.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Jason"}}, {"_id": "a9eka7bPPRhgZ5Hf9", "postedAt": "2023-12-06T19:27:44.140Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>Nice points on GHDF, Jason! I will publish a related post in the next few days following up of <a href=\"https://forum.effectivealtruism.org/posts/PTHskHoNpcRDZtJoh/gwwc-s-evaluations-of-evaluators?commentId=RKxJTK6zistF9coo3\">this</a> comment I made recently. Update: <a href=\"https://forum.effectivealtruism.org/posts/CDt5ShpdABZRn8Tvi/my-quick-thoughts-on-donating-to-ea-funds-global-health-and\">published</a>!</p>", "parentCommentId": "8kaobCsysC8kHjt88", "user": {"username": "vascoamaralgrilo"}}, {"_id": "Tdd8zyzKrPRJJokgD", "postedAt": "2023-12-06T21:51:43.478Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I\u2019m pretty new to the EA movement and community, so please take what I have to say with a grain of salt. With that said, I really think this is the right direction. Effective Altruism is about common moral values and ethical vision, and at the end of the day this has to be our main focus. Recruitment or \u201cwinning converts\u201d isn\u2019t the point\u2014a \u201cbig tent\u201d movement without any substance behind it is of low value to the well-being of sentient life on the planet, in the short term and especially in the long. I don\u2019t think we can afford \u201cmission creep\u201d right now, especially not after the FTX events.</p>\n", "parentCommentId": null, "user": {"username": "Hayven Jackson"}}, {"_id": "4cAJ8ZnnjqitN3ixf", "postedAt": "2023-12-06T21:52:04.399Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I think this is a great idea, and will protect against the worry Will McAskill raised of AI Safety \u201ceating\u201d EA</p>\n", "parentCommentId": null, "user": {"username": "freedomandutility"}}, {"_id": "YwXvQxYwjHLGmCLLW", "postedAt": "2023-12-07T00:15:59.133Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>(own views only) Thank you Jason; I think you've correctly nailed the most important (short-term) issue with the changed scope.&nbsp;</p><p>I think there are two huge uncertainties with trying to do grants in global health and development meta. The first is that I'm not sure this is what donors want. The second is that I'm not sure there are good grantmakers who are willing to work in this area.</p><p>For the first confusion, I don't have survey results or anything, but I think many GHDF donors will feel betrayed if they learned that a significant fraction of their money goes to funding ambiguously meta activities<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefomw69l5cuoq\"><sup><a href=\"#fnomw69l5cuoq\">[1]</a></sup></span>.&nbsp;</p><p>I do think GHDF donors with high risk tolerance are currently poorly served by the current ecosystem (and may have to either handpick projects themselves to support, or donate to a meta fund with a large cause split). I don't have a good sense of how large this population of donors actually is.</p><p>For the second confusion, as an empirical matter I believe it's been difficult to find grantmakers excited about evaluating GHD meta. Even if donors are on board, I don't think the current EAIF is set up well to do this, nor is the current GHDF.&nbsp;</p><p>(In the medium- to long- term, I don't necessarily expect grantmakers to be a significant bottleneck in itself. Having enough assured funding + us focusing more time on hiring might be enough to solve that problem.)<br><br>Longer term, I think it probably makes sense for some fund to do global health and development meta<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpslrwiy1rdm\"><sup><a href=\"#fnpslrwiy1rdm\">[2]</a></sup></span>&nbsp;(It might even be under EA Funds!) I just don't think it's a good choice right now for either EAIF or GHDF.</p><p>I like your exit strategy suggestion and will probably bring it up with the team (note that I don't have any direct decision-making power for EAIF).</p><p>Again, these are just my own views. Caleb and other fund managers might disagree, and provide their own input.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnomw69l5cuoq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefomw69l5cuoq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I think many people give to GHDF because they want something that's maybe 10-20% more risky than GiveWell's All Grants Fund. Whereas I expect many meta activities, particularly projects with a longer chain of impact than, say, paying for a fundraiser, to be <i>much </i>more risky. &nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpslrwiy1rdm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpslrwiy1rdm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I do think having a non-OP source of funding is good here. In addition to greater independence as you've noted, I think OP GHD community building is just quite conservative, e.g. more inclined to fund things with \"one step of meta\" and clear metrics like fundraisers that counterfactually raise more money than they cost, or incubate GH charities that are on track to become future GiveWell top charities. Whereas I think people should be excited about the types of programs that <a href=\"https://forum.effectivealtruism.org/users/agb\">originally got people like AGB</a> to donate to global health, or fund neglected interventions research, even when the payoffs are not immediate.</p></div></li></ol>", "parentCommentId": "8kaobCsysC8kHjt88", "user": {"username": "Linch"}}, {"_id": "LuBwRe3p3Z6AEZxxo", "postedAt": "2023-12-07T00:22:43.509Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>Thanks Hayven! I'm glad you like this direction. The challenge remaining, from my perspective is how we can practically build a robust community. Particularly one that's not directly tied to singular short-term object-level metrics<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefghjc2ui858b\"><sup><a href=\"#fnghjc2ui858b\">[1]</a></sup></span>&nbsp;like lives saved or money donated or people who work in impactful jobs, without being overly inward-facing and losing track of why we're here in the first place.</p><p>We want the community to be neither a factory nor a social club.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnghjc2ui858b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefghjc2ui858b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Because judging a community too closely on specific object-level metrics risks biasing a specific worldview, plus might be long-term unhealthy for a community.&nbsp;</p></div></li></ol>", "parentCommentId": "Tdd8zyzKrPRJJokgD", "user": {"username": "Linch"}}, {"_id": "aj2zGcq7t9y6tXaZ5", "postedAt": "2023-12-07T01:14:36.513Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I feel pretty good about surveying donors and allocating some proportion of funding based on that. Ultimately, I don't think it's low integrity or misleading for us to change directions towards meta work on the GHDF if we are still appealing to the values on our website - though I think the specifics of the arrangement matter a lot.</p>\n<p>The main issue (imo) is that it's unclear that meta GHDF work is competitive with just donating to GiveWell charities. Conversations with Open Phil GHW have made me a bit less enthusiastic about this direction.</p>\n<p>What is the ToC for meta Global Health work?</p>\n<p>** Find excellent people who can work at existing direct orgs? **\nGHD doesn't seem particularly leveraged career-wise right now. Most career opportunities for people in high-income countries (where EA is most prevalent) seem fairly unexciting (particularly) junior roles. I could imagine mid/late career meta work is pretty exciting, but I haven't seen many fundable projects in this area. If you are excited about working on mid/late field building in any cause area, please apply to the EAIF!</p>\n<p>** Find people who can start new fundraising orgs?**\nOpen Phil is currently funding projects in this area; EAIF also funds projects in this area (and will continue to do so if they work in multiple cause areas).</p>\n<p>** Find people who can start new direct charities?**\nI am most compelled by meta work for Animal Welfare, where it seems like new initiatives could beat the best animal interventions we know. To the best of my knowledge, I don't think that new GHW charities have had much luck beating the best GiveWell charities (by a GiveWell-type view's lights). Ofc, you could disagree with GiveWell's worldview; I have some disagreements, though I haven't seen well-reasoned improvements.</p>\n", "parentCommentId": "YwXvQxYwjHLGmCLLW", "user": {"username": "calebp"}}, {"_id": "Q5GGE6RhjK2GZR5KP", "postedAt": "2023-12-07T01:19:24.421Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I think this is pretty unclear; I think we'd mostly be looking for people who are using EA principles to guide their career decision-making (scope sensitivity, impartiality etc.) as opposed to thinking primarily about future cause areas. I agree it's fuzzy, though I don't want to share concrete criteria. I am excited about here out of worries of goodharting.</p>\n<p>Ultimately, we can transfer apps between funds, so it's not a huge deal. I think at 75:25 should probably apply to EAIF (my very off-the-cuff view).</p>\n", "parentCommentId": "KHirakF8dgZTQR3gB", "user": {"username": "calebp"}}, {"_id": "QCxtv2BENozTjCdhA", "postedAt": "2023-12-07T01:25:14.268Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>(A few more responses to your comment)</p>\n<blockquote>\n<p>There are wide grey areas when attempting to delineate principles-first EA from cause-specific EA and the effective giving examples in this post stand out to me as one thorny area. I think it may make sense not to fund an AI-specific or an animal-specific effective giving project through EAIF (and the LTTF and AWF are more appropriate), but an effective giving project that e.g. takes a longtermist approach or is focused on near-term human and nonhuman welfare seems different to me. Put differently: How do you think about projects that don't cover all of EA, but also aren't limited to one cause area?</p>\n</blockquote>\n<p>I think it's fine for us to evaluate projects that don't cover all of EA. I think the thing we want to avoid is funding things that are clearly focused on a specific cause area. We can always transfer grants to other funds in EA Funds if it's a bit confusing for the applicant. In the examples that you gave, the LTFF would evaluate the AI-specific thing, but the EAIF is probably a better fit for the neartermist cross-cause fundraising.</p>\n<blockquote>\n<p>Maybe Lightspeed? But I worry there isn't currently other coverage for funding needs of this sort.</p>\n</blockquote>\n<p>I don't think this is open right now, and it's not clear when it will be open again.</p>\n<blockquote>\n<p>I'm worried about people couching cause-specific projects as principles-first, but there is already a heavy tide pushing people to couch principles-first projects as x-risk-specific, so this might not be a concern.</p>\n</blockquote>\n<p>Yes, I'm worried about this too.</p>\n", "parentCommentId": "pk25oo9npB7wYThoW", "user": {"username": "calebp"}}, {"_id": "yrdghEyywrJm6BSgC", "postedAt": "2023-12-07T03:49:39.464Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>(Epistemic status: <i>speculative</i>)</p><p>&nbsp;</p><p>ETA a TL;DR -- it may lie in using relatively small amounts of EA funding to counterfactually multiply the positive effect of non-EA resources, or to counterfactually move substantial non-EA funding toward much more effective charities (even if not GiveWell's best).</p><blockquote><p>What is the ToC for meta Global Health work?</p></blockquote><p>It could lie in a few places. As an example, one could provide very low operational funding to student volunteer-led organizations. Having even a small external budget can be a real force multiplier for a student organization, making existing resources (e.g., student volunteer time, access to campus resources, access to a population reflecting on its values with time to hear a good speaker) significantly more effective.&nbsp;</p><p>Drawing on my own life, I went to something like an <a href=\"https://www.oxfamamerica.org/take-action/oxfam-hunger-banquet/\">Oxfam Hunger Banquet</a> as an option toward fulfilling requirements for the freshman seminar class in college. I think that event had a meaningful effect on my own views about effectiveness and global priorities. If one could counterfactually give a similar, even mildly-EA flavored experience to college freshmen for a few dollars each, I speculate that the ROI would be quite good (e.g., in promoting effective giving). That only works if the funding acts as a force multiplier -- you'd need many of the inputs to be provided for \"free\" by non-EA sources. But as in my Hunger Banquet example, I don't think that is necessarily implausible.</p><blockquote><p>** Find people who can start new direct charities?** &nbsp;. . . . To the best of my knowledge, I don't think that new GHW charities have had much luck beating the best GiveWell charities (by a GiveWell-type view's lights).&nbsp;</p></blockquote><p>I don't think we should assume that the new charities will only donations from EA sources. If a GHW meta grantmaker provides startup funding to a new charity, and as a result that charity ends up diverting $1MM a year from ~ineffective charities to ~0.5X GiveWell work, the value is equivalent to donating ~$500K/year to a GiveWell top charity. Many potential donors are pre-committed to a specific subfield (e.g., mental health), or find diffuse interventions like bednets unappealing for whatever reasons. So their dollars were never in play for GiveWell top charities anyway.</p><p>In addition to providing startup funds, one could argue for funding a meta organization that -- e.g., -- helps carefully selected 98th-percentile-effectiveness organizations write convincing grant pitches to governments and non-EA foundations. I guess that comes back to force multipliers too -- it's not very effective to fund these organizations' operating expenses on a long term basis, but the right strategic investments might help them leverage enough non-EA monies to create a really good ROI.</p>", "parentCommentId": "aj2zGcq7t9y6tXaZ5", "user": {"username": "Jason"}}, {"_id": "KhonSTKnquKYz7ruy", "postedAt": "2023-12-07T05:07:26.851Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>Makes sense to me, thanks for sharing! It seems pretty plausible that the tighter remit is a good choice, both operationally and in terms of communication with donors. And I appreciate the clear examples of what falls inside and out.</p><p>One question, not intended as a criticism: you point out that EAIF would no longer function as a catch-all donor-of-last resort to random projects, which makes sense. But I do come across people working on such projects, and it does feel like there should be somewhere I can refer them, where they will be evaluated by cause-agnostic generalist EA evaluators (even if their prior probability of beating AI/GHW/Animals was low). Are you aware of such a venue?</p>", "parentCommentId": null, "user": {"username": "Larks"}}, {"_id": "LrCtf9T9Ph9Pu4AJw", "postedAt": "2023-12-07T08:10:08.488Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I\u2019m hugely in favour of principles first as I think it builds a more healthy community. However, my concern is that if you try too hard to be cause neutral, you end up artificially constrained. For example, Global Heath and Wellbeing is often a good introduction point to the concept of effectiveness. Then once people are focused on maximisation, it\u2019s easier to introduce Animal Welfare and X-Risk.</p>\n", "parentCommentId": null, "user": {"username": "Grayden"}}, {"_id": "bvpoY5zsrr5y2aigf", "postedAt": "2023-12-07T10:59:41.153Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I\u2019m a grant writer and fundraiser by trade, but in the past I haven\u2019t provided services to any charities that were affiliated with EA or met GiveWell\u2019s effectiveness standards. They\u2019re mostly the typical single-cause, single-location organizations run by people who really mean well but are running on emotion or \u201cfaith\u201d alone. These are good people who just aren\u2019t used to using an effective lens, even using much more conventional program evaluation methods.</p>\n<p>There\u2019s only so much I can do as an independent worker in this field, but I do like the idea of selecting those 98th percentile orgs you mentioned and am intrigued by the approach of applying a small amount of EA money to them (epistemic status: uncertain, ~40%).</p>\n<p>My concern would be that such organizations would only be tangentially aligned with EA values, and so essentially EA Infrastructure would be funding organizations with very different values, which I don\u2019t think matches EA\u2019s core vision.</p>\n<p>Of course, I\u2019m still new to the movement, so I don\u2019t really feel all that comfortable speaking definitively about this.</p>\n", "parentCommentId": "yrdghEyywrJm6BSgC", "user": {"username": "Hayven Jackson"}}, {"_id": "qhHaLFyYubaJ5jv9w", "postedAt": "2023-12-07T11:03:27.119Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I was wondering the same thing, especially for projects that are managed by EAs or intended to meet EA principles. Are there any EA sources out there that fund such projects?</p>\n", "parentCommentId": "KhonSTKnquKYz7ruy", "user": {"username": "Hayven Jackson"}}, {"_id": "dK94DWB7SH6qHkYEs", "postedAt": "2023-12-07T12:36:20.831Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I also think an emphasis on principles-first EA will help protect us against the failure mores of</p>\n<ol>\n<li>\n<p>not reprioritising between cause areas as interventions by EAs make certain cause areas smaller in scale or less neglected</p>\n</li>\n<li>\n<p>not reprioritising between cause areas based on new and better cause prioritisation research</p>\n</li>\n</ol>\n", "parentCommentId": null, "user": {"username": "freedomandutility"}}, {"_id": "HNFc3MPf6p4KfD8Pi", "postedAt": "2023-12-07T16:20:31.337Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>Agreed, and I'm not exactly sure what this looks like. I'm not comparing EA to religious ideologies, but in the past religion has been the main institution that has tried to fulfill the purpose of \"robust moral community.\" Maybe taking a page from some of the ways these organizations have done community building (e.g., focused talks and meetings, social events, big gatherings with talks and exhibits, personal meditation / focus on values) would be a good idea? (epistemic status -- low, ~30%)</p>", "parentCommentId": "LuBwRe3p3Z6AEZxxo", "user": {"username": "Hayven Jackson"}}, {"_id": "5asCbfAdwogmJ7oE6", "postedAt": "2023-12-07T20:20:52.751Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I haven't come across any good non-EA GHD student groups. Remember that they need to beat the bar of current uni EA groups (that can get funding from Open Phil) from a GHD perspective - which I think is somewhat of a high bar.</p>\n<blockquote>\n<p>If a GHW meta grantmaker provides startup funding to a new charity, and as a result that charity ends up diverting $1MM a year from ~ineffective charities to ~0.5X GiveWell work, the value is equivalent to donating ~$500K/year to a GiveWell top charity.</p>\n</blockquote>\n<p>I don't think this reasoning checks out. GiveWell interventions also get lots of money from non-EA sources (e.g. AMF). It might be the case that top GiveWell charities are unusually hard to fundraise for from non-EA sources relative to 98% charities, though I'm not sure why that would be the case, and a 98th% intervention could end up being much less cost-effective in real terms.</p>\n", "parentCommentId": "yrdghEyywrJm6BSgC", "user": {"username": "calebp"}}, {"_id": "j8NH2RKHkNHLpsk9M", "postedAt": "2023-12-07T20:27:39.231Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I agree with the overall point, though I am not I've seen much empirical evidence for the GHD as a good starting point claim (or at least I think it's often overstated). I got into EA stuff though GHD, but,  this may have just been because there were a lot more GHD/EA intro materials at the time. I think that the eco-system is now a lot more developed and I wouldn't be surprised if GHD didn't have much of an edge over cause first outreach (for AW or x-risk).</p>\n<p>Maybe our analysis should be focussed on EA principles, but the interventions themselves can be branded however they like? E.g. We're happy to fund GHD giving games because we believe that they contribute to promoting caring about impartiality and cost-effectiveness in doing good - but they don't get much of a boost or penalty from being GHD giving games (as opposed to some other suitable cause area).</p>\n", "parentCommentId": "LrCtf9T9Ph9Pu4AJw", "user": {"username": "calebp"}}, {"_id": "sZcvPGweiZnXBojCe", "postedAt": "2023-12-07T22:46:07.395Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>This is very exciting. A key point in our draft strategy for 2024 was the apparent lack of principles-first EA funding (beyond CEA\u2019s CBG programme). This is quite the update, I\u2019m glad you posted it when you did!</p>\n", "parentCommentId": null, "user": {"username": "James Herbert"}}, {"_id": "Y7AGrgLLyNB3DhYiB", "postedAt": "2023-12-07T23:42:18.652Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>Thanks! To be clear, this is a 'plan' instead of something we are 100% committed to delivering on in the way it's presented below. I think there are some updates to be made here, but I would feel bad if you made large irreversible decisions based on this post. We will almost certainly have a more official announcement if we do decide to commit to this plan.</p>\n", "parentCommentId": "sZcvPGweiZnXBojCe", "user": {"username": "calebp"}}, {"_id": "zm4drq3e3Ms8fubn2", "postedAt": "2023-12-08T04:54:55.519Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I agree that GHW is an excellent introduction to effectiveness and we should watch out for the practical limitations of going too meta, but I want to flag that seeing GHW as a pipeline to animal welfare and longtermism is problematic, both from a common-sense / moral uncertainty view (it feels deceitful and that\u2019s something to avoid for its own sake) and a long-run strategic consequentialist view (I think the EA community would last longer and look better if it focused on being transparent, honest, and upfront about what most members care about, and it\u2019s really important for the long term future of society that the core EA principles don\u2019t die).</p>\n", "parentCommentId": "LrCtf9T9Ph9Pu4AJw", "user": {"username": "romanhauksson"}}, {"_id": "kTeGXgByAw5kfLT4A", "postedAt": "2023-12-08T11:33:20.442Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>Thanks for making that clear!&nbsp;</p>", "parentCommentId": "Y7AGrgLLyNB3DhYiB", "user": {"username": "James Herbert"}}, {"_id": "yGS5W4gSWTrFkyCcY", "postedAt": "2023-12-09T02:28:14.882Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p><a href=\"https://manifund.org/\">Manifund</a> comes to mind, though they're more of a platform than a grantmaking agency.&nbsp;</p>", "parentCommentId": "KhonSTKnquKYz7ruy", "user": {"username": "Linch"}}, {"_id": "zm5GLpLddJrBNoAb9", "postedAt": "2023-12-09T16:59:33.869Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>\"We want the community to be neither a factory nor a social club.\"</p><p>It is not immediately needed but I would really appreciate some further elaboration of your thoughts on this topic as I reckon many people(including me) are grappling with the same problem for their work outside of EAIF.</p>", "parentCommentId": "LuBwRe3p3Z6AEZxxo", "user": {"username": "emre kaplan"}}, {"_id": "5xKKCS26y9Mov3jz8", "postedAt": "2023-12-09T23:43:38.958Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>\"The EA Infrastructure Fund will fund and support projects that build and empower the&nbsp; community of people trying to identify actions that do the greatest good from a scope-sensitive and impartial welfarist view.\"</p><p>I'm curious how EA Funds incorporates moral uncertainty into its decision making given its mandate is 100% welfarist. To be clear, I don't think running one project that is 100% welfarist necessarily contradicts with plausible views on moral uncertainty. I think welfarism is massively underrepresented in most people's decision making and to compensate for that one might run a 100% welfarist project despite having credence in multiple theories.</p><p>I know this is not within the scope EAIF but I think this example from animal welfare illustrates a trade-off well. Some countries have passed legislation to ban the culling of male chicks in the egg industry. Male chicks won't be born in those countries. Working on these bans is a moral priority if you think acts of killing are intrinsically bad. If you think welfare is all that matters then working on this issue is far lower in priority since male chicks live for three days at most and their life experiences are dwarfed by the life experiences of other animals. Would EA Funds prefer people coming into EA to be 100% welfarist with respect to projects they choose to work on?</p><p>I had similar conundrums when drafting a vision and mission for my organisation, ie. how to keep our edge while being clear about taking moral uncertainty seriously. So I'm curious about how EA Funds thinks about this issue.</p>", "parentCommentId": null, "user": {"username": "emre kaplan"}}, {"_id": "HdaRjCS3YrXbiKTFA", "postedAt": "2023-12-10T13:03:28.206Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>Not to detract from the general point, but there are welfarist views that can accommodate chick culling being very bad, like critical level utilitarianism. I don't think they're very popular, though.</p>\n", "parentCommentId": "5xKKCS26y9Mov3jz8", "user": {"username": "MichaelStJules"}}, {"_id": "geD6w5sGPA7bzidEN", "postedAt": "2023-12-11T19:58:37.637Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>Could you give me some examples of these kinds of projects? I think, as Linch said, Manifund is probably their best bet, or posting on the EA forum asking for funding from individual donors.</p>\n", "parentCommentId": "KhonSTKnquKYz7ruy", "user": {"username": "calebp"}}, {"_id": "k2cRW9Md7MoBp5aWP", "postedAt": "2023-12-11T20:15:24.640Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I'm not too worried about this kind of moral uncertainty. I think that moral uncertainty is mostly action-relevant when one moral view is particularly 'grabby' or the methodology you use to analyse an intervention seems to favour one view over another unfairly.</p>\n<p>In both cases, I think the actual reason for concern is quite slippery and difficult for me to articulate well (which normally means that I don't understand it well). I tend to think that the best policy is to maximise the expected outcomes of the overall decision-making policy (which involves paying attention to decision theory, common sense morality, deontological constraints etc. ).</p>\n<p>In any case, most of my moral uncertainty worry comes from maximising very hard on a narrow worldview (or set of metrics) - but I think that \"welfarism\" is sufficiently broad and the mandate and track record of the EAIF is sufficiently varied that I am not particularly worried about this class of concerns.</p>\n", "parentCommentId": "5xKKCS26y9Mov3jz8", "user": {"username": "calebp"}}, {"_id": "ajTDeTMDDhagM8PsW", "postedAt": "2023-12-13T00:26:32.748Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>I'm excited to see the EAIF share more about their reasoning and priorities. Thank you for doing this!</p><p>I'm going to give a few quick takes\u2013 happy to chat further about any of these. TLDR: <strong>I recommend (1) getting rid of the \"principles-first\" phrase &amp; (2) issuing more calls for proposals focused on the specific projects you want to see </strong>(regardless of whether or not they fit neatly into an umbrella term like \"principles-first\")<strong>.&nbsp;</strong></p><ul><li>After skimming the post for 5 minutes, I couldn't find a clear/succinct definition of what \"principles-first\" actually means. I think it means something like \"focus more on epistemics and core reasoning\" and \"focus less on specific cause areas\". But then some of the examples of the projects that Caleb is excited about are basically just like \"get people together to think about a specific cause area\u2013 but not one of the mainstream ones, like one of the more neglected ones.\"</li><li>I find the \"principles-first\" frame a bit icky at first glance. Something about it feels... idk... just weird and preachy or something. Ok, what's actually going on there?<ul><li>Maybe part of it is that it seems to imply that people who end up focusing on specific cause areas are not \"principles-first\" people, or like in the extreme case they're not \"good EAs\". And then it paints a picture for me where a \"good EA\" is one who spends a bunch of time doing \"deep reasoning\", instead of doing cause-area-specific work. Logically, it's pretty clear to me that this isn't what the posters are trying to say, but I feel like that's part of where the system 1 \"ick\" feeling is coming from.</li></ul></li><li>I worry that the term \"principles-first EA\" might lead to a bunch of weird status things and a bunch of unhelpful debates. For me, the frame naturally invokes questions like \"what principles?\" and \"who gets to decide what those principles are?\" and all sorts of \"what does it truly mean to be an EA?\" kinds of questions. Maybe the posters think that, on the margin, more people should be asking these questions. But I think that should be argued for separately\u2013 if EAIF adopts this phrase as their guiding phrase, I suspect a lot of people will end up thinking \"I need to understand what EAIF thinks the principles of EA are and then do those things\".</li><li>Personally, I don't think the EAIF needs to have some sort of \"overarching term\" that summarizes what it is prioritizing. I think it's quite common for grantmaking organizations to just say \"hey, here's a call for proposals with some examples of things we're excited about.\"</li><li>Personally, I'm very excited about the projects that Caleb listed in the appendix. Some of these don't really seem to me to fall neatly under the \"principles-first\" label (a bunch of them just seem like \"let's do deconfusion work or make progress in specific areas that are important and highly neglected.\"</li><li>Historically, my impression is that EAIF hasn't really done many calls for proposals relating to specific topics. It has been more like \"hey anyone with any sort of meta idea can apply.\" I'm getting the sense from this post that Caleb wants EAIF to have a clearer focus. Personally, <strong>I would encourage EAIF to do more \"calls for proposals\" focused on specific projects that they want to see happen in the world. </strong>As an example, EAIF could say something like \"we are interested in seeing proposals about acausal trade and ethics of digital minds. Here are some examples of things you could do.\"<ul><li>I think there are a lot of \"generally smart and agentic people\" around who don't really know what to do, and some guidance from grantmakers along the lines of \"here are some projects that we want to see people apply to\" could considerably lower the amount of agency/activation energy/confidence/inside-viewness that such people need.</li><li>On the flip side, we'd want to avoid a world in which people basically just blindly defer to grantmakers. I don't suspect calls for proposals to contribute to that too much, and I also suspect there's a longer conversation that could be had about how to avoid these negative cultural externalities.</li></ul></li></ul>", "parentCommentId": null, "user": {"username": "Akash"}}, {"_id": "qmFGkbyzKrqKAjmfj", "postedAt": "2023-12-19T01:12:36.813Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<blockquote><p>I'm excited to see the LTFF share more about their reasoning and priorities. Thank you for doing this!</p></blockquote><p>Just noting that this is EAIF, not LTFF.</p>", "parentCommentId": "ajTDeTMDDhagM8PsW", "user": {"username": "Linch"}}, {"_id": "9F4uBJtEvwPaeZtGG", "postedAt": "2023-12-19T18:58:46.090Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>(Oops, fixed!)</p>", "parentCommentId": "qmFGkbyzKrqKAjmfj", "user": {"username": "Akash"}}, {"_id": "8Sc4csdF4dtKyLGBr", "postedAt": "2023-12-31T01:54:00.682Z", "postId": "FnNJfgLgsHdjuMvzH", "htmlBody": "<p>People who like this new vision may want to consider <a href=\"https://www.givingwhatwecan.org/en-US/funds/effective-altruism-funds\">donating to EAIF</a>!&nbsp;</p><p>Now until the end of January is an unusually good time to donate, as it allows you to take advantage of <a href=\"https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching\">2:1 Open Phil matching</a> ($2 from them for $1 from you)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0vlfyfv2ai2\"><sup><a href=\"#fn0vlfyfv2ai2\">[1]</a></sup></span>. My best guess is that (unlike with LTFF), the EAIF match <i>will not</i> be filled by default without significant changes in more donors chipping in (<a href=\"https://dashboard.effectivealtruism.org/public/dashboard/d76c3f83-e183-49aa-ab37-dcfab89740a3\">see dashboard here</a>)</p><p>Let me know if you have any questions! We'll also likely post an AMA up on the forum in early January.&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0vlfyfv2ai2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0vlfyfv2ai2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note that this is less good that it sounds to the extent that you think OP's marginal dollar is almost as good and/or better than EAIF's marginal $.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Linch"}}]