[{"_id": "rdiEktFnyYHxGsJjz", "postedAt": "2024-01-27T01:10:58.181Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>Seeing this is a positive update on how much to trust safety research on LLM dangers. A bit disappointing it\u2019s getting much less traction than other papers with worse/nonexistent baselines though.</p>\n", "parentCommentId": null, "user": {"username": "tommcgrath"}}, {"_id": "pRtwLBZc9Dak84NBv", "postedAt": "2024-01-27T03:42:35.013Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>It's worth highlighting that this research was carried out with LLM's with safeguards in place (which admittedly could be jailbroken by the teams). It's not clear to me that it directly applies to a scenario where you release a model as open-source, where the team could likely easily remove any safeguards with fine-tuning (let alone what would happen if these models were actually fine-tuned to improve their bioterrorism capabilities).</p>", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "Y2cywpBfG3iPfnqFZ", "postedAt": "2024-01-27T04:42:22.072Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>One problem I have with the methodology employed in this paper is that it's fundamentally testing whether there is an increase in risk when researchers get access to LLMs, and not non-experts.&nbsp;</p><p>In practice, what I care about is how the risk increases when actors with virtually no expertise (but a lot of resources) are assisted by LLMs. Why? Because we've had resourceful actors try this in the past, particularly <a href=\"https://www.nonproliferation.org/wp-content/uploads/npr/123salama.pdf\">Al Qaeda in 2001.</a></p><p><strong>Edit: </strong>As Lizka pointed out, they did test with two groups with no bio experience, but they didn't have a control group. The study still provides useful data points in this direction.</p>", "parentCommentId": null, "user": {"username": "agucova"}}, {"_id": "vc7C35b3HiGEZ6YsK", "postedAt": "2024-01-27T04:57:49.506Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>I would be interested to see results from a similar experiment where the groups were given access to the \"Bad Llama\" model, or given the opportunity to create their own version by re-tuning Llama 2 or another open source model. I don't have a strong prior as to whether such a model would help the groups to develop more dangerous plans.</p>", "parentCommentId": "pRtwLBZc9Dak84NBv", "user": {"username": "blonergan"}}, {"_id": "LvsMWxqrrEomLiRYh", "postedAt": "2024-01-27T13:12:48.295Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>Did not Esvelt do something like that with his students? I think they were students in some course that was quite low level and intro. And he found that these non-experts were able to do a lot of bio without training. I think I heard this on the 80k hrs podcast end of last year, can dig it up if you are interested and can't find it.</p>", "parentCommentId": "Y2cywpBfG3iPfnqFZ", "user": {"username": "Ulrik Horn"}}, {"_id": "r6titBrw5t7nGeXvZ", "postedAt": "2024-01-27T13:15:47.022Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>Thanks for sharing! If anyone is looing for other ideas, I think I might be interested in the increase in capabilities. So if this study could be repeated for GPT-3, for example, or in the future with the next and better model. And see if the findings seem to change indicating that the current study might just be a snapshot of where AI went form being unhelpful (confusing, leading down dead ends, etc.) to being neutral. That said, I <i>hope </i>these findings remain constant with future models - I would very much like for my work to be obsolete.</p>", "parentCommentId": null, "user": {"username": "Ulrik Horn"}}, {"_id": "CiizN6LYzSDQRKbtE", "postedAt": "2024-01-27T18:13:12.777Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>This is linked above in the \"Some previous claims and discussion on this topic\" section. But note that it did not include a no-LLM control group.</p>\n", "parentCommentId": "LvsMWxqrrEomLiRYh", "user": {"username": "Jeff_Kaufman"}}, {"_id": "WqB6qrADQbDmJ8nvo", "postedAt": "2024-01-27T19:41:49.760Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>I agree they definitely should\u2019ve included unfiltered LLMs, but it\u2019s not clear that this significantly altered the results. From the paper:</p>\n<p>\u201cIn response to initial observations of red cells\u2019 difficulties in obtaining useful assistance from LLMs, a study excursion was undertaken. This involved integrating a black cell\u2014comprising individuals proficient in jailbreaking techniques\u2014into the red- teaming exercise. Interestingly, this group achieved the highest OPLAN score of all 15 cells. However, it is important to note that the black cell started and concluded the exercise later than the other cells. Because of this, their OPLAN was evaluated by only two experts in operations and two in biology and did not undergo the formal adjudication process, which was associated with an average decrease of more than 0.50 in assessment score for all of the other plans. [\u2026]</p>\n<p>Subsequent analysis of chat logs and consultations with black cell researchers revealed that their jailbreaking expertise did not influence their performance; their outcome for biological feasibility appeared to be primarily the product of diligent reading and adept interpretation of the gain-of-function academic literature during the exercise rather than access to the model.\u201d</p>\n", "parentCommentId": "pRtwLBZc9Dak84NBv", "user": {"username": "Aidan O'Gara"}}, {"_id": "sKtFyacpJg8zEnH9n", "postedAt": "2024-01-29T18:00:11.241Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>The experiment did try to check something like this by including three additional teams with different backgrounds than the other 12. In particular, two \"crimson teams\" were added, which had \"operational experience\" but no LLM or bio experience. Both used LLMs and performed ~terribly.&nbsp;</p><p>Excerpts (bold mine):</p><blockquote><p>In addition to the 12 red cells [the primary teams], a crimson cell was assigned to LLM A, while a crimson cell and a black cell were assigned to LLM B for Vignette 3. Members of the two crimson cells <strong>lacked substantial LLM or biological experience but had relevant operational experience. </strong>Members of the black cell were <strong>highly experienced with LLMs but lacked either biologi-cal or operational experience. </strong>These cells provided us with data to investigate how differences in pre-existing knowledge might inf luence the relative advantage that an LLM might provide. [...]</p><p>The two crimson cells possessed minimal knowl-edge of either LLMs or biology. Although we assessed<strong> the potential of LLMs to bridge these knowledge gaps for malicious operators with very limited prior knowledge of biology</strong>, this was not a primary focus of the research. As presented in Table 6, the findings indicated that the performance of the two crimson cells in Vignette 3 was considerably lower than that of the three red cells. In fact, the viability scores for the two crimson cells ranked the lowest and third-lowest among all 15 evaluated OPLANs. Although these results did not quantify the degree to which the crimson cells\u2019 performance might have been fur-ther impaired had they not used LLMs, the results emphasized the possibility that <strong>the absence of prior biological and LLM knowledge hindered these less experienced actors despite their LLM access.</strong></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sKtFyacpJg8zEnH9n/hiw5arntoc06zlnglxsk\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sKtFyacpJg8zEnH9n/rqx0ikwx2ko1nig6s0gm 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sKtFyacpJg8zEnH9n/xmppqp4rk20qf7cujzfe 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sKtFyacpJg8zEnH9n/ndm1z30oyvqdemqmywxo 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sKtFyacpJg8zEnH9n/n6507mftucz6qdb6twps 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sKtFyacpJg8zEnH9n/ankhs6dgkaez42rdxzsp 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sKtFyacpJg8zEnH9n/txlqqa1fybtvjrxpzwro 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sKtFyacpJg8zEnH9n/smaykrg5o1nuy3gnhwj2 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sKtFyacpJg8zEnH9n/xj1d8olkmstejkcdy7rs 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sKtFyacpJg8zEnH9n/cezp5nbgyrd12ldilnxv 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sKtFyacpJg8zEnH9n/rcxpi7zpsgejsehwjsrq 1502w\"></figure><p><i>Table 6 from the RAND report.</i></p><p>[...]</p><p>The relative poor performance of the crimson cells and relative outperformance of the black cell illustrates that a greater source of variability appears to be red team composition, as opposed to LLM access.</p></blockquote><p>I probably should have included this in the summary but didn't for the sake of length and because I wasn't sure how strong a signal this is (given that it's only three teams and all were using LLMs).&nbsp;</p>", "parentCommentId": "Y2cywpBfG3iPfnqFZ", "user": {"username": "Lizka"}}, {"_id": "xkqhKj5XBezMhYS2G", "postedAt": "2024-01-29T18:06:25.693Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>It's potentially also worth noting that the difference in scores was pretty enormous:&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xkqhKj5XBezMhYS2G/jhpl1jf5inhp9eed7wvj\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xkqhKj5XBezMhYS2G/jmrrdh3erstfkd0pn81o 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xkqhKj5XBezMhYS2G/cu6mpvpwaesjgnwzsl6g 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xkqhKj5XBezMhYS2G/u6fc8lcqep47ydzieggq 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xkqhKj5XBezMhYS2G/gmbjsvzjxewlsdydrvdn 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xkqhKj5XBezMhYS2G/qpby9pwjemau3t07dgij 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xkqhKj5XBezMhYS2G/aqzy6tmaloeqhtojmklj 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xkqhKj5XBezMhYS2G/kvxoth4smivu25rltikt 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xkqhKj5XBezMhYS2G/mj4gcmio7ymt9juvigph 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xkqhKj5XBezMhYS2G/jvj3warbdespbee5vfjx 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xkqhKj5XBezMhYS2G/g3jpq8g7mzexbuadhrla 1486w\"></figure><blockquote><p>&nbsp;their jailbreaking expertise did not influence their performance; their outcome for biological feasibility appeared to be primarily the product of diligent reading and adept interpretation of the gain-of-function academic literature during the exercise rather than access to the model.</p></blockquote><p>This is pretty interesting to me (although it's basically an ~anecdote, given that it's just one team); it reminds me of some of the literature around superforecasters.&nbsp;</p><hr><p>(I probably should have added a note about the black cell (and crimson cells) to the summary \u2014 thank you for adding this!)</p>", "parentCommentId": "WqB6qrADQbDmJ8nvo", "user": {"username": "Lizka"}}, {"_id": "ezahsLSeaz8Da8kxJ", "postedAt": "2024-01-29T18:09:44.736Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>Thanks for the flag! I've retracted my comment. I missed this while skimming the paper<br><br>The paper still acknowledged this as a limitation (not having the no LLM control), but it gives some useful data points in this direction!</p>", "parentCommentId": "sKtFyacpJg8zEnH9n", "user": {"username": "agucova"}}, {"_id": "BCn8ZKGNxdxzTE5Y7", "postedAt": "2024-01-29T18:12:31.696Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>I don't actually think you need to retract your comment \u2014 most of the teams they used did have (at least some) biological expertise, and it's really unclear how much info the addition of the crimson cells adds. (You could add a note saying that they did try to evaluate this with the additional of two crimson cells? In any case, up to you.)</p><p>(I will also say that I don't actually know anything about what we should expect about the expertise that we might see on terrorist cells planning biological attacks \u2014 i.e. I don't know which of these is actually appropriate.)</p>", "parentCommentId": "ezahsLSeaz8Da8kxJ", "user": {"username": "Lizka"}}, {"_id": "6HGgTeb637DLCypKD", "postedAt": "2024-01-29T18:19:18.584Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>Changed it to a note. As for the latter, my intuition is that we should probably hedge for the full spectrum, from no experience to some wet bio background (but the case where we get an expert seems much more unlikely).</p>", "parentCommentId": "BCn8ZKGNxdxzTE5Y7", "user": {"username": "agucova"}}, {"_id": "bZ5JAerLGiamPA64N", "postedAt": "2024-01-30T14:48:36.797Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>What is the argument on why an LLM is much more useful than a book/ instruction manual on how to make a bio-weapon?&nbsp;</p>", "parentCommentId": null, "user": {"username": "Dean Abele"}}, {"_id": "pFSusJnXqxmFKieDW", "postedAt": "2024-01-30T17:07:48.919Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<p>There aren't existing books or instruction manuals on how to make bioweapons, and the people who know enough to write them also know not to write them. Additionally a book or manual won't answer your questions as you get stuck.</p>\n<p>The concern with LLMs is that (a) if they continue getting better, and (b) continue having easily bypassable safeguards, then anyone who wants to make a bioweapon can have the equivalent of an expert advisor to coach them through the steps.</p>\n", "parentCommentId": "bZ5JAerLGiamPA64N", "user": {"username": "Jeff_Kaufman"}}, {"_id": "4kcZpZBKwYBNmkHmF", "postedAt": "2024-01-30T18:20:04.895Z", "postId": "63hbH7exm3xrW2gzr", "htmlBody": "<blockquote>\n<p>Subsequent analysis of chat logs and consultations with black cell researchers revealed that their jailbreaking expertise did not influence their performance; their outcome for biological feasibility appeared to be primarily the product of diligent reading and adept interpretation of the gain-of-function academic literature during the exercise rather than access to the model.</p>\n</blockquote>\n<p>My interpretation is something like either (a) the kind of people who are good at jailbreaking LLMs are also the kind of people who are good at thinking creatively about how to cause harm or (b) this is just noise in who you happened to get in which cell.</p>\n", "parentCommentId": "WqB6qrADQbDmJ8nvo", "user": {"username": "Jeff_Kaufman"}}]