[{"_id": "zZiJBFaZfSueTHQJL", "postedAt": "2023-03-24T23:26:06.319Z", "postId": "FCkchmXcSCQtJ9PZA", "htmlBody": "<blockquote><p>First,&nbsp;<i>predicting the values of our successors&nbsp;</i>\u2013 what John Danaher (<a href=\"https://philarchive.org/rec/DANAFT-2\"><u>2021</u></a>) calls&nbsp;<i>axiological futurism</i> \u2013 in worlds where these are meaningfully different from ours doesn\u2019t seem intractable at all. Significant progress has already been made in this research area and there seems to be room for much more (see the next section and the Appendix).</p></blockquote><p>Could you point more specifically to what progress you think has been made? As this research area seems to have only existed since 2021 we can't have yet made successful predictions about future values so I'm curious what has been achieved.</p>", "parentCommentId": null, "user": {"username": "Joseph Miller"}}, {"_id": "HA2rNrjCfy4oGS7wY", "postedAt": "2023-03-25T02:45:37.431Z", "postId": "FCkchmXcSCQtJ9PZA", "htmlBody": "<p>I see some parallel between this project of predicting future (hopefully wiser and better-informed) values for moral antirealists and just doing moral philosophy to work out facts of the matter in ethics for moral realists. Both projects seem pretty hard. I expectantly await future posts!</p>\n", "parentCommentId": null, "user": {"username": "Oscar Delaney"}}, {"_id": "QsdCRmbuYbKLK8LSv", "postedAt": "2023-03-25T05:45:49.636Z", "postId": "FCkchmXcSCQtJ9PZA", "htmlBody": "<blockquote>\n<p>Our descendants are unlikely to have values that are both different from ours in a very significant way and predictable. Either they have values similar to ours or they have values we can\u2019t predict. Therefore, trying to predict their values is a waste of time and resources.</p>\n</blockquote>\n<p>I'm strongly drawn to that response. I remain so after reading this initial post, but am glad that you, by writing this sequence, are offering the opportunity for someone like me to engage with the arguments/ideas a bit more! Looking forward to upcoming installments!</p>\n<p><s>Wrote this on my phone and wasn't offered the option to format the paragraph as a quote (and I don't know what the command is); might come back to edit and fix it later</s></p>\n", "parentCommentId": null, "user": {"username": "Sarah Weiler"}}, {"_id": "R27pnAmkWawXuqkQY", "postedAt": "2023-03-25T09:13:39.206Z", "postId": "FCkchmXcSCQtJ9PZA", "htmlBody": "<p>Yeah so Danaher (<a href=\"https://philarchive.org/rec/DANAFT-2\"><u>2021</u></a>) coined the term <i>axiological futurism</i>, but research on this topic has existed long before that. For instance, I find those two pieces particularly insightful:</p><ul><li>Robin Hanson (1998)&nbsp;<a href=\"http://mason.gmu.edu/~rhanson/filluniv.pdf\"><u>Burning the Cosmic Commons: Evolutionary Strategies for Interstellar Colonization</u></a></li><li>Nick Bostrom (2004)&nbsp;<a href=\"https://nickbostrom.com/fut/evolution\"><u>The Future of Human Evolution</u></a></li></ul><p>They explore how compassionate values might be selected against because of evolutionary pressures, and be replaced by values more competitive for, e.g., space colonization races. In <a href=\"https://ageofem.com/\"><u>The Age of Em</u></a>, Robin Hanson forecasts what would happen if whole brain emulation comes before <i>de novo</i> AGI, and arrives at similar conclusions.<br><br>I don't think we can say they made \"successful predictions\" and settled the debate, but it seems like they came up with quite important considerations.<br><br>I intend to elaborate more on this kind of work in future posts within this sequence. :)</p>", "parentCommentId": "zZiJBFaZfSueTHQJL", "user": {"username": "Jim Buhler"}}, {"_id": "ywGySkpDsWEjScAoA", "postedAt": "2023-03-25T09:21:41.901Z", "postId": "FCkchmXcSCQtJ9PZA", "htmlBody": "<p>Thanks Sarah, that's motivating!&nbsp;</p>", "parentCommentId": "QsdCRmbuYbKLK8LSv", "user": {"username": "Jim Buhler"}}, {"_id": "eTcDupkBBniNfFkng", "postedAt": "2023-03-25T09:27:58.581Z", "postId": "FCkchmXcSCQtJ9PZA", "htmlBody": "<blockquote>\n<p>Wrote this on my phone and wasn't offered the option to format the paragraph as a quote (and I don't know what the command is); might come back to edit and fix it later</p>\n</blockquote>\n<p>You can try \"&gt; paragraph\"</p>\n", "parentCommentId": "QsdCRmbuYbKLK8LSv", "user": {"username": "Lorenzo Buonanno"}}, {"_id": "Rea7Jd5rExXjHNHpZ", "postedAt": "2023-03-25T09:39:09.218Z", "postId": "FCkchmXcSCQtJ9PZA", "htmlBody": "<p>Thanks Oscar!</p><blockquote><p>predicting future (hopefully wiser and better-informed) values for moral antirealists</p></blockquote><p>Any reason to believe moral realists would be less interested in this empirical work? You seem to assume the goal is to update our values based on those of future people. While this can be a motivation (this is among those of <a href=\"https://philarchive.org/rec/DANAFT-2\">Danaher 2021</a>), we might also worry -- independently from whether we are moral realists or antirealists -- that the expected future evolution of values doesn't point towards something wiser and better-informed (since that's not what evolution is \"optimizing\" for; relevant examples in <a href=\"https://philarchive.org/rec/DANAFT-2\">this comment</a>), and want to change this trajectory.<br><br>Anticipating what could happen seems instrumentally useful for anyone who has long-term goals, no matter their take on meta-ethics, right?</p>", "parentCommentId": "HA2rNrjCfy4oGS7wY", "user": {"username": "Jim Buhler"}}, {"_id": "rRaGzYaSc5xEoJ4Pb", "postedAt": "2023-03-25T10:57:34.690Z", "postId": "FCkchmXcSCQtJ9PZA", "htmlBody": "<p>Ah OK, yes that seems right. I think the main context I have considered the values of future people previously is in trying to frontrun moral progress and get closer to the truth (if it exists) sooner than others, so that is where my mind most naturally went. But yes, if for instance, we were more in a <a href=\"https://slatestarcodex.com/2014/07/30/meditations-on-moloch/\">Moloch</a> style world where value was slowly disappearing in favour of ruthless efficiency then indeed that is good to know before it has happened so we can try to stop it.</p>", "parentCommentId": "Rea7Jd5rExXjHNHpZ", "user": {"username": "Oscar Delaney"}}, {"_id": "JWAT9cTzbgt3w6Qqt", "postedAt": "2023-03-30T01:11:36.727Z", "postId": "FCkchmXcSCQtJ9PZA", "htmlBody": "<p>I love your writing style here, and am very excited for future posts in this sequence.</p><p>:two-cents: I would make it more clear that this is the start of a sequence, so that readers will be able to more easily figure out why there's no linked paper.</p>", "parentCommentId": null, "user": {"username": "jpaddison"}}, {"_id": "LXxFNbRvpaRctnYwq", "postedAt": "2023-03-30T13:20:26.010Z", "postId": "FCkchmXcSCQtJ9PZA", "htmlBody": "<p>Thanks a lot :)</p>", "parentCommentId": "JWAT9cTzbgt3w6Qqt", "user": {"username": "Jim Buhler"}}]