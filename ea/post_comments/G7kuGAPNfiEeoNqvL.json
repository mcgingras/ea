[{"_id": "XW5Dv6SEegLzzkpCP", "postedAt": "2023-11-28T13:00:24.885Z", "postId": "G7kuGAPNfiEeoNqvL", "htmlBody": "<p><strong>Executive summary</strong>: This post summarizes 29 project proposals for the 2024 AI Safety Camp, listing the goals, desired skills, and teams for each one. The projects span a variety of alignment methods like debate, constitutional AI, and asymmetric control.</p><p><strong>Key points:</strong></p><ol><li>Many projects focus on restricting uncontrollable AI through methods like operational design domains, data laundering injunctions, and congressional messaging.</li><li>Multiple projects aim to improve mechanistic interpretability of LLMs through analysis of toy models, activation engineering, and out-of-context learning.</li><li>Evaluating and steering LLMs towards alignment is another theme, with projects on reflectivity benchmarks, situational awareness datasets, tiny model evals, steering techniques, and more.</li><li>Additional areas include agent foundations like actuation spaces, optimization and agency, and detecting agents.</li><li>Miscellaneous alignment methods being explored include non-maximizing agents, debate improvements, personalized fine-tuning, self-other overlap, and asymmetric control.</li><li>Supplementary projects address policy-based model access, economic safety nets for AGI deployment, and organizing virtual AI safety unconferences.</li></ol><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}]