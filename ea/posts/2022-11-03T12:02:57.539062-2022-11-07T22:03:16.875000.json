[{"_id": "S3NtdqHeR8XTRQJP2", "title": "What do EAs think about the Twitter acquisition?  ", "postedAt": "2022-11-07T20:01:03.111Z", "htmlBody": "<p>The richest and most prominent Effective Altruism/Longtermism exponent has lit $44,000,000,000 on fire to own his political enemies online. Was this an effective form of wealth deployment for someone who claims to have the future of humanity as his first priority? Will this person still be welcomed to the EA ranks after conducting perhaps the biggest single waste of private capital in human history? Will EA adjust their philosophy at all or just ignore this?&nbsp;</p>", "user": {"username": "Peter_Layman"}}, {"_id": "Duu5548i3JsZmGHsA", "title": "[job] Metaculus has new software roles", "postedAt": "2022-11-07T21:19:24.942Z", "htmlBody": "<p>[Previous Metaculus hiring post <a href=\"https://forum.effectivealtruism.org/posts/FTKhRoSoTFjtQjiNr/metaculus-is-seeking-experienced-leaders-researchers-and\">here</a>. Recent LW post <a href=\"https://www.lesswrong.com/posts/nu3xb9mHWceJwwSHA/metaculus-is-seeking-software-engineers\">here</a>.]</p><p>Hi EA-ists,</p><p>I'm Dan, the new CTO of Metaculus. If you're not familiar with Metaculus, we've been building a forecast platform for the public good since 2015, and we recently hit 1M forecasts! I got this job in part because of Metaculus's previous hiring post linked above, so I'm happy to share we now have more openings.</p><p>For the first time, we're hiring more junior roles in addition to senior ones. See our openings for <a href=\"https://apply.workable.com/metaculus/j/409AECAA94/\">Full-Stack Engineers</a> and <a href=\"https://apply.workable.com/metaculus/j/B36C3B4BF1/\">Frontend Engineers</a>. The other roles are at the top link.</p><p>This is a fantastic time to join Metaculus, for a few reasons:</p><ul><li>We're well <a href=\"https://metaculus.medium.com/metaculus-awarded-5-5m-grant-to-advance-forecasting-as-a-public-good-7fc20a161723\">funded</a> by Open Philanthropy, and we're paying competitive salaries and equity, under our new <a href=\"https://metaculus.medium.com/becoming-a-public-benefit-corporation-hitting-1-million-predictions-and-three-new-ai-forecasting-7ae4996fee3\">Public Benefit Corporation charter</a>.</li><li>We're early in our engineering growth, so the next few hires will have a big influence on our long-term strategy.</li><li>There's been a recent surge in <i>demand</i> for forecasts. Our partners (like <a href=\"https://metaculus.medium.com/metaculus-launches-the-forecasting-our-world-in-data-project-to-probe-the-long-term-future-6d1d84d7b805\">Our World in Data</a>) have concrete use cases for turning better forecasts into better decisions. This is an opportunity for direct impact, in addition to the long-term vision of building better epistemic infrastructure for humanity.</li></ul><p>If you're a recent grad, self-taught web developer, or otherwise early in your career, and you want to join a high impact, mission-driven, fun and ambitious learning culture, please <a href=\"https://apply.workable.com/metaculus/\">apply now</a>, and mention you came from the EA Forum!</p>", "user": {"username": "dschwarz"}}, {"_id": "XDy8CQf2JnTz5bSXX", "title": "EA Groups Should Rename Intro Fellowships for Better Clarity", "postedAt": "2022-11-07T15:38:26.262Z", "htmlBody": "<p>This is a quick post that serves as a call to action for university EA group organizers!&nbsp;</p><p>For university EA groups (and maybe other EA groups) the term \"introductory fellowship\" can lead to various confusions: seeming religious, inaccurately prestigious, or too official, for example. I propose that most groups should switch to an alternative, such as Georgia Tech's \"Effective Altruism Introductory Seminar\" or (one that I'm particularly excited for, that we've used at LSU this semester) calling introductory reading/discussion groups the \"<strong>IDEA Program</strong>\":</p><p><strong>&nbsp;IDEA - Introduction</strong> to making a&nbsp;<strong>Difference</strong> through&nbsp;<strong>Effective Altruism.</strong></p><p>Why is this better than the typical framing of \"Effective Altruism Introductory Fellowship\"?</p><ol><li><u>Much harder to confuse as religious:</u> In a recent popular thread in the EA Groups slack, there was discussion of alternative names to \"fellowship\"; quite a few group organizers expressed that newcomers to EA interpreted fellowship as religious, and that it promoted the idea of EA coming across as cult-ish. I would imagine this problem is not unique to the ~6&nbsp; people who mentioned it in the slack thread, so eliminating this confusion seems useful for better outreach and communication for groups in the future!</li><li><u>Emphasis on both doing and learning:</u> This framing puts more of a focus on what those in EA are actually trying to accomplish - making a difference (this is the D of IDEA). I think it does a good job of beginning to switch one's first impression of the fellowship from \"Here's this community called EA, I can sign up for this fellowship to read about what they have to say\" to \"here are some ideas touted by this community called EA, maybe I can use their ideas to better make a difference/have an impact\".</li><li><u>Better Epistemics:</u> Similarly, this framing makes EA seem more like \"a set of tools/ideas I can use to have an impact\" (i.e. making a difference&nbsp;<i>through</i> effective altruism) as opposed to \"a set of dogmas or predetermined ideas about how to have an impact\". The former is a more accurate conception of EA (or at least of what EA would ideally be).</li><li>Petty reason:&nbsp;<u>It rolls off the tongue better.</u></li><li><u>Less prestigious-sounding:</u> (disclaimer: anecdote) When I first heard the phrasing \"EA Introductory Fellowship\" I was confused as to why a reading group was being advertised as a fellowship. My impression was that fellowships are reserved for more prestigious (usually paid) academic or intern-like positions, and it was a little off-putting to see people in EA try to oversell their intro programs. The term \"fellowship\" could definitely still be useful in some places where participants are more heavily-selected, or maybe at schools where EA groups have very impressive reputations, but I think the phrasing usually seems inaccurate, especially if we are aiming for<a href=\"https://forum.effectivealtruism.org/posts/SjK9mzSkWQttykKu6/big-tent-effective-altruism-is-very-important-particularly\">&nbsp;<u>big tent EA</u></a>.</li></ol><p>In general, catchy acronyms can be fun to come up with and more engaging than long bulky names. \"IDEA Program\" is probably <i>not</i> the best version of this, but it's a step in the right direction compared to the unclear, long-to-pronounce \"Effective Altruism Introductory Fellowship\".</p><p>For fairness: why we might NOT want to switch from \"fellowship\":</p><ol><li>Intro programs/reading groups should be more selective, and \"fellowship\" better expresses the seriousness/commitment that is expected from participants.</li><li>\"Fellowship\" is already pretty common and well-used among EA groups, so we shouldn't try to disrupt the equilibrium (I think this is probably a pretty bad reason; name changes require minimal effort)</li><li>\"IDEA Program\" doesn't actually tell someone what it's about in the name. Effective Altruism Intro Fellowship is more explicit about: A) the fact that it's introductory and B) that it's affiliated with effective altruism.<ol><li>This is probably less of a problem with good marketing and design.</li></ol></li></ol><p>Overall, I find these latter reasons not as compelling, and changing fellowship names is low-hanging fruit for less confusing and more honest outreach. At the very least, I think more groups should try it out!</p><p><i>Thanks to Justin Guo for feedback, and Michel Justen for encouraging me to write this up.</i><br>&nbsp;</p>", "user": {"username": "harrygietz@gmail.com"}}, {"_id": "oSWYaqbF53HnrypeF", "title": "Hacker-AI \u2013 Does it already exist?", "postedAt": "2022-11-07T14:01:23.938Z", "htmlBody": "", "user": {"username": "Erland W."}}, {"_id": "JGywdygDbAHHEHqNa", "title": "How efficient is donating to EA funds?", "postedAt": "2022-11-07T21:19:09.735Z", "htmlBody": "<p>By donating to funds instead of the charities themselves there are extra costs like the salaries of the fund managers. Is there any information about what percentage of the money is given to organizations? Is it more efficient just to donate to organizations directly?</p>", "user": {"username": "Christopher Zhang"}}, {"_id": "AiSAAkvZzWKkt4uBB", "title": "[Links post] Economists Chris Blattman and Noah Smith on China, Taiwan, and the likelihood of war", "postedAt": "2022-11-07T12:22:57.907Z", "htmlBody": "<p>Two prominent economists and writers, Chris Blattman and Noah Smith, both recently and independently published blog posts on the likelihood of war between China and the US over Taiwan. They are not optimistic. Both seem to think that war is relatively likely. Unfortunately, neither offers a quantitative forecast, but I think it's interesting and useful for serious researchers to write about their views on such important questions.</p><p><a href=\"https://chrisblattman.com/blog/2022/10/26/the-prospects-for-war-with-china-why-i-see-a-serious-chance-of-world-war-iii-in-the-next-decade/\">Chris's post</a> is called &nbsp;\"The prospects for war with China: Why I see a serious chance of World War III in the next decade\". <a href=\"https://noahpinion.substack.com/p/why-i-think-an-invasion-of-taiwan\">Noah's</a> is \"Why I think an invasion of Taiwan probably means WW3\". They complement each other well, with Chris's post arguing that war seems likely and Noah's post arguing that the US would probably get involved in the event of a war.</p><h1>Chris's post on why bargaining may break down</h1><p>Chris applies the bargaining framework he used throughout his book <a href=\"https://chrisblattman.com/why-we-fight/\"><i>Why We Fight</i></a> to show why war is becoming more likely as China continues to grow its economy and strengthen its military. Eventually, the status quo of de facto Taiwanese independence will slip outside of its \"compromise range\". When this happens, Chinese leaders may decide going to war to try to bring about a different outcome may be preferable, despite the costs.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2f5ea1f78564028d2615cdd05ac0b22ee055a7fc9d082bf.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2f5ea1f78564028d2615cdd05ac0b22ee055a7fc9d082bf.png/w_93 93w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2f5ea1f78564028d2615cdd05ac0b22ee055a7fc9d082bf.png/w_173 173w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2f5ea1f78564028d2615cdd05ac0b22ee055a7fc9d082bf.png/w_253 253w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2f5ea1f78564028d2615cdd05ac0b22ee055a7fc9d082bf.png/w_333 333w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2f5ea1f78564028d2615cdd05ac0b22ee055a7fc9d082bf.png/w_413 413w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2f5ea1f78564028d2615cdd05ac0b22ee055a7fc9d082bf.png/w_493 493w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2f5ea1f78564028d2615cdd05ac0b22ee055a7fc9d082bf.png/w_573 573w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2f5ea1f78564028d2615cdd05ac0b22ee055a7fc9d082bf.png/w_653 653w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2f5ea1f78564028d2615cdd05ac0b22ee055a7fc9d082bf.png/w_733 733w\"></figure><p>Still, war is risky and negative-sum. A negotiated settlement should, in principle, be preferable for all parties. Chris suggests that negotiations may fail because ideological principles (e.g. democracy vs autocracy) can be non-negotiable, China's crackdown on Hong Kong harmed its reputation for sticking to negotiated settlements, and Xi Jinping is increasingly unchecked and isolated in his leadership.</p><h1>Noah's post on why the US would probably get drawn into the war</h1><p>Noah also applies economic thinking to inform his geopolitical analysis, using game theory to predict which war scenarios seem more likely based on the interests of the actors involved.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2e0a965d3425214a97371d7312fe62e21d55efecb647f30d.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2e0a965d3425214a97371d7312fe62e21d55efecb647f30d.png/w_89 89w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2e0a965d3425214a97371d7312fe62e21d55efecb647f30d.png/w_169 169w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2e0a965d3425214a97371d7312fe62e21d55efecb647f30d.png/w_249 249w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2e0a965d3425214a97371d7312fe62e21d55efecb647f30d.png/w_329 329w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2e0a965d3425214a97371d7312fe62e21d55efecb647f30d.png/w_409 409w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2e0a965d3425214a97371d7312fe62e21d55efecb647f30d.png/w_489 489w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2e0a965d3425214a97371d7312fe62e21d55efecb647f30d.png/w_569 569w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2e0a965d3425214a97371d7312fe62e21d55efecb647f30d.png/w_649 649w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2e0a965d3425214a97371d7312fe62e21d55efecb647f30d.png/w_729 729w\"></figure><p>Noah considers about how important various factors, like national pride, reputation, and military costs, seem to Chinese and American leaders. He then tries to weigh up how important these costs and benefits seem relative to each other to work out the payoffs to each actor is in each of the four possible outcomes of the game. Then he assigns probabilities to work out expected payoffs, and figures out the equilibrium through backwards induction. Long story short, the equilibrium solution<i> under these assumptions</i> is for China to invade and attack the US to maximize its chances of victory, nearly assuring the outbreak of a major great power war.</p><p>Of course, there are many ways this analysis could be wrong, and Noah touches on them at the end of the post. For example, given escalation risk, perhaps the payoffs in any \"US fights\" outcome are hugely negative for both parties, making it very unlikely that they would both choose to fight. Or, perhaps losing a fight over Taiwan would be so negative for China's leadership that it's just too risky (the expected payoff is negative) to undertake. Or, perhaps the leadership of each country is misinformed about the likelihood of each outcome, leading to miscalculation, bad decision-making, and a sub-optimal (i.e. very costly) outcome.</p><h1>Why they're worth reading</h1><p>Both posts analyze the US-China-Taiwan situation from a specific, economic framework. How insightful you find them will depend somewhat on how much resemblance you think this kind of rational analysis of weighing expected costs and benefits bears to actual foreign policy decision-making. But I think it's great to see serious thinkers trying to make progress on important questions like \"Will World War III happen?\" in public.</p>", "user": {"username": "Stephen Clare"}}, {"_id": "yL9bxtgBy8eqatddp", "title": "4 Key Assumptions in AI Safety", "postedAt": "2022-11-07T10:50:40.499Z", "htmlBody": "", "user": {"username": "Akmon Ra"}}, {"_id": "tnSg6o7crcHFLc395", "title": "The Welfare Range Table", "postedAt": "2022-11-07T10:19:03.915Z", "htmlBody": "<h1><strong>Key Takeaways</strong></h1><ul><li>Our objective: estimate the welfare ranges of 11 farmed species.</li><li>Given hedonism, an individual\u2019s welfare range is the difference between the welfare level associated with the most intense positively valenced state that the individual can realize and the welfare level associated with the most intense negatively valenced state that the individual can realize.</li><li>Given some prominent theories about the functions of valenced states, we identified over 90 empirical proxies that might provide evidence of variation in the potential intensities of those states.&nbsp;</li><li>There are many unknowns across many species.</li><li>It\u2019s rare to have evidence that animals lack a given trait.</li><li>We know less about the presence or absence of traits as we move from terrestrial vertebrates to most invertebrates.</li><li>Many of the traits about which we know the least are affective traits.</li><li>We do have information about some significant traits for many animals.</li></ul><hr><h1><strong>Introduction</strong></h1><p>This is the second post in the&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\"><u>Moral Weight Project Sequence</u></a>. The aim of the sequence is to provide an overview of the research that Rethink Priorities conducted between May 2021 and October 2022 on interspecific cause prioritization\u2014i.e., making resource allocation decisions across species. The aim of this post is to provide an overview of the Welfare Range Table, which records the results of a literature review covering over 90 empirical traits across 11 farmed species.<br>&nbsp;</p><h1><strong>Motivations</strong></h1><p>If we want to do as much good as possible, we have to compare all the ways of doing good\u2014including ways that involve helping members of different species. The Moral Weight Project\u2019s <a href=\"https://forum.effectivealtruism.org/posts/hxtwzcsz8hQfGyZQM/an-introduction-to-the-moral-weight-project\">assumptions</a> entail that everyone\u2019s welfare counts the same and that all welfare improvements count equally. Still, some may be able to realize more welfare than others. We\u2019re particularly interested in how much welfare different individuals can realize at a time\u2014that is, their respective&nbsp;<i>welfare ranges</i>. An individual\u2019s welfare range is the difference between the best and worst welfare states the individual can realize at a time. We assume hedonism, according to which all and only positively valenced states increase welfare and all and only negatively valenced states decrease welfare. Given as much, an individual\u2019s welfare range is the difference between the welfare level associated with the most intense positively valenced state that the individual can realize and the welfare level associated with the most intense negatively valenced state that the individual can realize. In the case of pigs, for instance, that might be the difference between the welfare level we associate with being fully healthy on a <a href=\"https://www.youtube.com/watch?v=--LeOoohGrM&amp;ab_channel=FarmSanctuary\">farm sanctuary</a>, on the one hand, and a botched slaughter, on the other.</p><p>If there\u2019s variation in welfare ranges across taxa, then there\u2019s variation in the capacities that generate the determinants of welfare. So, if there\u2019s such variation and hedonism is true, then there\u2019s variation in the capacities that generate positively and negatively valenced experiences.</p><p>As&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/H7KMqMtqNifGYMDft/differences-in-the-intensity-of-valenced-experience-across\"><u>Jason Schukraft argues</u></a>, we don\u2019t have any good direct measures of the intensity of valenced states that let us make interspecific comparisons. Indeed, we rely on indirect measures even in humans: behavior, physiological changes, and verbal reports. We can observe behavior and physiological changes in nonhumans, but most of them aren\u2019t verbal. So, we have to rely on other indirect proxies, piecing together an understanding from animals\u2019 cognitive and affective traits or capabilities. The Welfare Range Table includes over 90 such traits: some behavioral, others physiological; some more cognitive, others more affective. Then, it indicates whether the empirical literature provides reason to think that members of 11 farmed species possess those traits.&nbsp;</p><p>&nbsp;</p><h1><strong>Methodology</strong></h1><p>Our team was composed of three philosophers, two comparative psychologists (one with expertise in birds; another with expertise in cephalopods), two fish welfare researchers, two entomologists, an animal welfare scientist, and a veterinarian. To select proxies for variation in the capacities that generate positively and negatively valenced experiences, we needed to rely on general theoretical considerations. In this case, we relied on theories about the function of valenced experiences\u2014that is, their adaptive value for organisms who have them.&nbsp;</p><p>There are three main theories about the function of valenced experiences. The first is that they allow organisms to represent fitness-relevant information (\u201cThat\u2019s good\u201d / \u201cThat\u2019s really good\u201d / \u201cThat\u2019s bad\u201d / \u201cThat\u2019s really bad\u201d / etc.;&nbsp;<a href=\"https://www.jstor.org/stable/41329419\"><u>Cutter &amp; Tye 2011</u></a>). The second is that they provide a common currency for decision-making (\u201cA is better than B\u201d;&nbsp;<a href=\"https://mitpress.mit.edu/9780262039307/the-evolution-of-the-sensitive-soul/\"><u>Ginsburg &amp; Jablonka 2019</u></a>). The third is that they facilitate learning (\u201cIf X, then A\u201d / \u201cIf Y, then B\u201d;&nbsp;<a href=\"https://www.nature.com/articles/nrn3403\"><u>Damasio &amp; Carvalho 2013</u></a>). So, we looked for empirical traits that arguably bear some relation to information representation, decision-making, and learning via valenced experiences. You can find an annotated list of those traits&nbsp;<a href=\"https://docs.google.com/document/d/1pZXobfRFviF4J7BsrC74Xnt5Za07yuKhgYcAnZFzSI4/edit\"><u>here</u></a>.</p><p>For each taxon / trait pair, we began with searches via Google Scholar. Then, we looked at the references in recent reviews. If none of those sources provides positive or negative evidence, we searched via Web of Science. Then, we broadened the search. If, for instance, the target trait hadn\u2019t been studied in the relevant species, we looked at the family; if the trait hadn\u2019t been studied in the relevant family, we looked at the order. If it seemed plausible that the results of the study generalize to the target species\u2014for example, because of other shared traits or similar ecological niches\u2014then we included those results as well.</p><p>Based on the evidence available for each taxon and trait, we made one of five assessments:&nbsp;</p><ul><li>Likely No (Credence: 0% \u2013 25%)</li><li>Lean No (Credence: &gt;25% \u2013 &lt;50%)</li><li>Unknown (Credence: 50%)</li><li>Lean Yes (Credence: &gt;50% \u2013 75%)</li><li>Likely Yes (Credence: &gt;75% \u2013 100%)</li></ul><p>These broad credence ranges are about&nbsp;<i>trait possession&nbsp;</i>(do members of the taxon have the trait or not?), not&nbsp;<i>degree of possession</i> (relative to humans, to what degree do the members of this taxon possess this trait?). The assessments are based on the existing published scholarly literature (as of August 2022), with \u201cUnknown\u201d being the default assessment in cases where there was no literature that spoke fairly directly to a given taxon / trait pair.</p><p>After completing the literature reviews, we performed two quality checks. First, we checked for consistency. In our initial review, for instance, honey bees got a \u201clean yes\u201d for transitive inference, whereas pigs got a \u201clean no.\u201d A closer look revealed that researchers were holding these animals to different standards. (In this particular case, we opted to flip pigs to \u201clean yes,\u201d as the lower standard seemed reasonable for our purposes.) Second, we commissioned spot checks of 15% of the traits per taxon from someone who had not been involved with the initial literature review for that taxon. Across all the taxon / trait combinations, the spot checkers didn\u2019t flip any positive responses (\u201clikely yes\u201d; \u201clean yes\u201d) to negative responses (\u201clikely no\u201d; \u201clean no\u201d) or vice versa. However, they did flip a handful of \u201cunknown\u201d responses to \u201clean yes.\u201d</p><p>&nbsp;</p><h1><strong>Results</strong></h1><p>The Welfare Range Table is available&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/13E1Ub7PIMkIcQKT6jDf0_2WIco-VW3vqCEcfPDLXeHw/edit#gid=1664257257\"><u>here</u></a>. However, the following diagrams provide a more digestible visual representation of most of the information we gathered (excluding physiological proxies):</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670897894/mirroredImages/tnSg6o7crcHFLc395/etz2f0rjefhpxyd0p9ek.png\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670897895/mirroredImages/tnSg6o7crcHFLc395/bgccouby2ilmaskd6oof.png\">We should highlight a few key results. First, as these diagrams suggest,&nbsp;<strong>there are many unknowns</strong>. This shouldn\u2019t be surprising: in many cases, no one has been interested in investigating these traits; in many other cases, no one knows how to investigate them. Moreover, insofar as it\u2019s easier to publish positive results than negative results, academics have an incentive to study traits that they already expect to find. So, the literature is probably biased toward relatively \u201csure things,\u201d in the interest of guaranteeing that people avoid walking away from their research empty-handed (i.e., without publications). This point is relevant to\u2014and partially explains\u2014the second key result, which is that&nbsp;<strong>there are very few negative results</strong>.</p><p>The table below indicates the percentage of proxies for which we were able to find some information, whether positive or negative. As the table indicates,&nbsp;<strong>information of any kind generally declines from terrestrial vertebrates to invertebrates</strong>.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670897894/mirroredImages/tnSg6o7crcHFLc395/lsmyncfl8bipcpjda4ld.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b9a913965dcf3a2d69ab9211c037349777cabcb1bb0e997b.png/w_210 210w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b9a913965dcf3a2d69ab9211c037349777cabcb1bb0e997b.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b9a913965dcf3a2d69ab9211c037349777cabcb1bb0e997b.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b9a913965dcf3a2d69ab9211c037349777cabcb1bb0e997b.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b9a913965dcf3a2d69ab9211c037349777cabcb1bb0e997b.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b9a913965dcf3a2d69ab9211c037349777cabcb1bb0e997b.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b9a913965dcf3a2d69ab9211c037349777cabcb1bb0e997b.png/w_1470 1470w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b9a913965dcf3a2d69ab9211c037349777cabcb1bb0e997b.png/w_1680 1680w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b9a913965dcf3a2d69ab9211c037349777cabcb1bb0e997b.png/w_1890 1890w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b9a913965dcf3a2d69ab9211c037349777cabcb1bb0e997b.png/w_2098 2098w\"></figure><p>As the next table makes clear,&nbsp;<strong>many of the traits about which we know the least are affective</strong>.&nbsp;</p><figure class=\"image image_resized\" style=\"width:64.2%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670897894/mirroredImages/tnSg6o7crcHFLc395/qkmjskzm0rx2zpw9fkfx.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9291755b196937236f5477fb3a78ef37b7e2c40374c0eff2.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9291755b196937236f5477fb3a78ef37b7e2c40374c0eff2.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9291755b196937236f5477fb3a78ef37b7e2c40374c0eff2.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9291755b196937236f5477fb3a78ef37b7e2c40374c0eff2.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9291755b196937236f5477fb3a78ef37b7e2c40374c0eff2.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9291755b196937236f5477fb3a78ef37b7e2c40374c0eff2.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9291755b196937236f5477fb3a78ef37b7e2c40374c0eff2.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9291755b196937236f5477fb3a78ef37b7e2c40374c0eff2.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9291755b196937236f5477fb3a78ef37b7e2c40374c0eff2.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9291755b196937236f5477fb3a78ef37b7e2c40374c0eff2.png/w_1064 1064w\"></figure><p>On one level, this shouldn\u2019t be surprising either, given scientists\u2019 determination not to anthropomorphize animals\u2014i.e., attribute human characteristics to nonhumans. (Unfortunately, scientists seem to worry less about what Franz de Waal calls \u201c<a href=\"https://www.jstor.org/stable/43154308\"><u>anthropodenial</u></a>\u201d\u2014i.e., \u201cthe a priori rejection of shared characteristics between humans and animals.\u201d) Still, these results are concerning insofar as we think that the welfare impacts of valenced states are additive, where the welfare impacts of experiences \u201cstack\u201d on top of one another. On such a view, experiencing maximal pain&nbsp;<i>and&nbsp;</i>shame (for instance) is worse than experiencing maximal pain alone. So, if we don\u2019t know whether animals can experience shame, we don\u2019t know something that bears directly on their welfare ranges.</p><p>However, it might not be the case that valenced states are additive: it could be that when you\u2019re experiencing maximal pain, you\u2019re unable to experience anything else, as pain consumes all your attention. If that\u2019s right, then while information about many valenced states might be highly relevant to assessing welfare&nbsp;<i>impacts&nbsp;</i>(how animals are actually faring), it isn\u2019t necessarily important for assessing welfare&nbsp;<i>ranges&nbsp;</i>(how animals&nbsp;<i>could&nbsp;</i>fare), at least if pain states are the most intense negatively valenced states that animals can experience.</p><p>That said, it isn\u2019t completely obvious that pain states are the most intense negatively valenced states that animals can experience. So, that provides some motivation for attending to other kinds of negatively valenced states. Moreover, while we\u2019re skeptical that negative experiences are additive, we aren\u2019t certain, so we assign some credence to the additive view. In expectation, then, we take information about the breadth of animals\u2019 emotional lives to be relevant to their welfare ranges.</p><p>In any case, one final point to note is that&nbsp;<strong>we do have information about some significant traits for many animals</strong>, as the following table indicates:</p><figure class=\"image image_resized\" style=\"width:71.05%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670897894/mirroredImages/tnSg6o7crcHFLc395/xjieutfrevknbs3ur9k8.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e15c9393717549e98f3498b8994a21a299f50f8477b67ce9.png/w_120 120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e15c9393717549e98f3498b8994a21a299f50f8477b67ce9.png/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e15c9393717549e98f3498b8994a21a299f50f8477b67ce9.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e15c9393717549e98f3498b8994a21a299f50f8477b67ce9.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e15c9393717549e98f3498b8994a21a299f50f8477b67ce9.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e15c9393717549e98f3498b8994a21a299f50f8477b67ce9.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e15c9393717549e98f3498b8994a21a299f50f8477b67ce9.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e15c9393717549e98f3498b8994a21a299f50f8477b67ce9.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e15c9393717549e98f3498b8994a21a299f50f8477b67ce9.png/w_1080 1080w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e15c9393717549e98f3498b8994a21a299f50f8477b67ce9.png/w_1166 1166w\"></figure><p>Parental care, for instance, provides some evidence of the range of affective states available to an organism, as does anxiety-like behavior. Multimodal integration is often touted as a key piece of evidence for sentience, as is the capacity to make trade-offs. But both traits are also relevant to welfare ranges insofar as they come in degrees (as suggested by the presence or absence of other traits).&nbsp;</p><p>&nbsp;</p><h1><strong>A clarification and some limitations</strong></h1><p>To be clear: the Welfare Range Table does not automatically supply welfare range&nbsp;<i>estimates</i>. It\u2019s one thing to collect data that are relevant to welfare ranges; it\u2019s another thing to score and aggregate them. In this post, we\u2019re only discussing the data. We\u2019ll defer the discussion of scoring and aggregation until later in the&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\"><u>Moral Weight Project Sequence</u></a>.</p><p>The Welfare Range Table is the first of its kind. Inevitably, then, it has several limits. Among them:&nbsp;</p><ol><li><strong>Our searches in related taxa were limited.</strong> In the interest of completing the project in a reasonable timeframe, we tried to limit the search for each taxon / trait pair to one hour. In fact, we often spent two or three hours per taxon / trait pair. Still, spending more time would have made it possible to investigate a wider range of related taxa for each taxon / trait pair.</li><li><strong>The proxy list could be expanded.&nbsp;</strong>We opted for an inclusive approach to the proxies. This made the project enormous. Still, there are many other traits that could have been included\u2014and, in some cases, perhaps ought to have been included in a list of this length.&nbsp;</li><li><strong>The proxy list could be refined.&nbsp;</strong>There are two senses in which the proxy list could be refined. First, some proxies may not be sufficiently informative about animals\u2019 welfare ranges\u2014though, of course, this is hardly obvious, as we don\u2019t have an independent way to assess how informative they are. Second, many of the proxies are relatively coarse-grained. Consider a trait like reversal learning: namely, the ability to suppress a reward-related response, which involves stopping one behavior and switching to another. This trait comes in degrees: some animals can learn to suppress a reward-related response in fewer trials; and, having learned to suppress a reward-related response at all, some can suppress their response more quickly. A more sophisticated version of the table would account for this variation.&nbsp;</li><li><strong>Academics are biased toward \u201cUnknown.\u201d</strong> We thought it was important to have domain experts review the literature whenever possible. However, domain experts are academics. Academics are socialized into a community where it\u2019s inappropriate to make some positive claim (\u201cPigs have this trait\u201d or \u201cpigs lack that trait\u201d) without being able to establish that claim to the satisfaction of your peers. There are good reasons to value this socialization in the present case. For instance, it\u2019s difficult to predict which traits an organism will have based on its other traits. Moreover, it\u2019s difficult to predict whether one kind of organism will have a trait because a related kind of organism does. Still, even though the probability ranges we mentioned earlier establish a very low bar for \u201clean yes\u201d and \u201clean no\u201d (above and below 50%, respectively), we defaulted to \u201cunknown\u201d when we couldn\u2019t find any relevant literature. Even if our approach is defensible, other reasonable literature reviewers may have had more \u201clean yes\u201d and \u201clean no\u201d assessments than we did.&nbsp;</li></ol><p>We could address these issues in the future. With enough time, for example, we could make inferences about trait possession from more distant taxa. Or, we could make efforts to include any neglected high-value proxies, eliminate relatively low-value proxies, and refine the proxies insofar as that\u2019s possible. Finally, we could try using a missing data method to account for systematic trends in the \u201cUnknowns.\u201d&nbsp;</p><p>&nbsp;</p><h1><strong>Conclusion</strong>&nbsp;</h1><p>The Welfare Range Table is the first attempt to assess a wide range of welfare-relevant traits in such a diverse array of taxa. It represents an initial but significant step toward estimating the differences in animals\u2019 welfare ranges. If it proves to be valuable, then it ought to be extended, as we didn\u2019t examine many important species. While we prioritized many of the most-farmed taxa, there are many others that humans farm in extraordinary numbers. In particular, it would be good to investigate more fishes and invertebrates.</p><p><br><br>&nbsp;</p><h1><strong>Acknowledgments</strong></h1><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670897894/mirroredImages/tnSg6o7crcHFLc395/cf4ofw1ukduucvmcbdh6.png\"></figure><p>This research is a project of Rethink Priorities. It was written by Bob Fischer. Thanks to Jason Schukraft and Adam Shriver for much of the conceptual work behind the project. Thanks to Rachel Norman, Martina Schiestl, Alex Schnell, and Anna Trevarthen for helpful feedback on earlier versions of this post. Thanks to Meghan Barrett, Leigh Gaffney, Michelle Lavery, Rachael Miller, Martina Schiestl, Alex Schnell, and Anna Trevarthen for their extensive literature reviews. Thanks to Jamie Elsey for the visuals.&nbsp;If you\u2019re interested in RP\u2019s work, you can learn more by visiting our&nbsp;<a href=\"https://www.rethinkpriorities.org/research\">research database</a>. For regular updates, please consider<i>&nbsp;</i>subscribing to our<a href=\"https://www.rethinkpriorities.org/newsletter\"> newsletter</a>.</p><p><br>&nbsp;</p>", "user": {"username": "bob-fischer"}}, {"_id": "CZe7c8PWCNPcajxsz", "title": "Any EAs on Mastodon?", "postedAt": "2022-11-07T06:32:33.886Z", "htmlBody": "<p>With a lot of academics, journalists, and other users migrating from Twitter to the federated microblogging platform <a href=\"https://en.wikipedia.org/wiki/Mastodon_(software)\">Mastodon</a>, I'm wondering if anyone in the EA and rationalist communities has a Mastodon account. Let's follow each other and build a thriving EA Mastodon community!</p>", "user": {"username": "evelynciara"}}, {"_id": "RGwJviLRnrSojwYwu", "title": "Opportunities that surprised us during our Clearer Thinking Regrants program", "postedAt": "2022-11-07T13:09:14.486Z", "htmlBody": "<p><i>This post was written by a subset of Clearer Thinking team members, and not all of the team members involved with the regranting necessarily agree with everything said here.&nbsp;</i></p><p><i>Update: the same week that we posted this, we were devastated to learn of the events surrounding FTX and to learn that the Future Fund team&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1\"><i><u>resigned</u></i></a><i>. We slightly updated the conclusion, but we have not changed the body of the post (because the focus of it was to share information that we learned during the regranting program, and we still think it serves that purpose). Our thoughts are with the many customers and others who have been affected by the devastating events at FTX.</i><br>&nbsp;</p><p>As part of the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/TzzuFGEhvoMiWNjjJ/announcing-the-clearer-thinking-regrants-program\"><u>Clearer Thinking Regrants program</u></a>, we evaluated over 630 project proposals and ended up with 37 finalists. In the many hours that we spent evaluating these finalists, we learned some things that surprised us and saw many potential opportunities to help the world that we hadn\u2019t considered before. Of course, if you\u2019re an expert in any of these areas, what surprised us non-experts may not surprise you.</p><p>Our aim in this post is to share object-level information that we did not know before the regranting program and that we think readers may find interesting or useful. In several cases, we are sharing the fact that specific organizations or projects have significantly more room for funding than we would have guessed, even after accounting for the&nbsp;<a href=\"https://manifold.markets/group/clearer-thinking-regrants/markets\"><u>outcomes</u></a> of our regrants program.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6pgsgpi0c85\"><sup><a href=\"#fn6pgsgpi0c85\">[1]</a></sup></span>&nbsp;We hope that readers find this information useful. By highlighting some organizations that have room for more funding, we hope that this will lead to impactful giving opportunities.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk5butud8nz\"><sup><a href=\"#fnk5butud8nz\">[2]</a></sup></span><br>&nbsp;</p><h3><strong>Summary:&nbsp;</strong></h3><ol><li>We were surprised that there hasn't been more work to quantify the risks from large-magnitude volcanic eruptions (considering the impact that such eruptions could have).</li><li>We were surprised that the&nbsp;<a href=\"https://rethinkpriorities.org/\"><u>Rethink Priorities</u></a>&nbsp;<a href=\"https://rethinkpriorities.org/ea-movement-research\"><u>Surveys Team</u></a> has significant room for funding for their own project ideas (since most of their work is research/consulting for existing orgs).</li><li>We hadn\u2019t previously considered the extent to which the use of boiling as a water treatment method in low- and middle-income countries contributes to indoor air pollution (and, therefore, to potentially negative health outcomes).&nbsp;</li><li>We were surprised to learn that the&nbsp;<a href=\"https://www.happierlivesinstitute.org/\"><u>Happier Lives Institute</u></a> (HLI) has substantial room for additional funding.</li><li>We hadn\u2019t considered that there may be significant new ideas about how to reduce the chance of nuclear war (given the age of the field).</li><li>Some of our team hadn\u2019t realized how difficult it can be to tell whether there\u2019s a vitamin deficiency in a population, and none of us had realized that point-of-care&nbsp;<a href=\"https://en.wikipedia.org/wiki/Biosensor\"><u>biosensors</u></a> might be feasible to roll out in the foreseeable future.</li><li>We hadn\u2019t realized that&nbsp;<a href=\"https://www.1daysooner.org/\"><u>1Day Sooner</u></a> conducts activities outside of their human challenge trial work, and that these activities have significant room for funding.&nbsp;<br><br><br>&nbsp;</li></ol><h3><strong>1. We were surprised that there hasn't been more work to quantify the risks from large-magnitude volcanic eruptions (considering the impact that such eruptions could have).</strong></h3><p>Toby Ord\u2019s best guess (discussed in&nbsp;<a href=\"https://theprecipice.com/\"><i>The Precipice</i></a>) is that there is a ~1 in 10,000 chance of an existential catastrophe via supervolcano eruption within the next 100 years. He also&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jJDuEhLpF7tEThAHy/?commentId=YMtFCHPavfjLGhpvb\"><u>thinks</u></a> that most of the risks associated with volcanic eruptions would relate to them causing the collapse of civilization and/or reducing the chance of recovering from that collapse, and that further research quantifying these risks would be helpful.&nbsp;</p><p>Volcanologists&nbsp;<a href=\"https://www.birmingham.ac.uk/schools/gees/people/profile.aspx?ReferenceId=199847&amp;Name=dr-mike-cassidy\"><u>Mike Cassidy</u></a> and&nbsp;<a href=\"https://www.cser.ac.uk/team/lara-mani/\"><u>Lara Mani</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jJDuEhLpF7tEThAHy/on-the-assessment-of-volcanic-eruptions-as-global\"><u>have</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/8jHLSjQ6KRwHYshMD/crosspost-huge-volcanic-eruptions-time-to-prepare-nature\"><u>argued</u></a> that there has been insufficient attention paid to quantifying these risks, as well as to identifying where large-magnitude volcanic eruptions are most likely to occur and which measures should be taken to mitigate these risks. It was surprising to us that these questions have not already been investigated in greater detail, considering the impact that such eruptions could have (and that improved quantification may be quite low-cost if carried out by the right team).<br><br>&nbsp;</p><h3><strong>2. We were surprised that the&nbsp;</strong><a href=\"https://rethinkpriorities.org/\"><strong><u>Rethink Priorities</u></strong></a><strong>&nbsp;</strong><a href=\"https://rethinkpriorities.org/ea-movement-research\"><strong><u>Surveys Team</u></strong></a><strong> has significant room for funding for their own project ideas (since most of their work is research/consulting for existing orgs).</strong></h3><p>During our regranting investigations, we became more aware of how Rethink Priorities has been supporting a variety of EA organizations by conducting research on their behalf, most of which is private and therefore not published. We were surprised to find out that they have significant room for additional funding for pursuing their own project ideas aimed at benefiting the entire EA ecosystem (as opposed to their bread-and-butter work of conducting research supporting particular organizations). This is still the case at the time of this post.</p><p>The Rethink Priorities team told us that they would like to \u201cidentify more generalizable, replicable insights, about how people think about EA and how best to promote it, as well as to explore new questions that are not yet on the radar of specific organizations.\u201d They also&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/tWawcXaNnLAihA2Fv/announcing-ea-pulse-large-monthly-us-surveys-on-ea?commentId=ceGQiT9XEKApdswzL\"><u>note</u></a> that they have significant room for funding for this line of research.&nbsp;</p><p>In contrast to their envisioned systematic, generalizable research, the Surveys Team points out that many of the&nbsp;<i>current</i> projects they complete at the request of other organizations are \u201cvery time-limited, and focused on quickly addressing the immediate needs of decision-makers on specific questions.\u201d The Surveys Team told us that if they were funded to run their own research rather than only to work on behalf of specific organizations, they believe they could produce more research that is broadly valuable to the movement as a whole.<br><br>&nbsp;</p><h3><strong>3. We hadn\u2019t previously considered the extent to which the use of boiling as a water treatment method in low- and middle-income countries contributes to indoor air pollution (and, therefore, to potentially negative health outcomes).</strong></h3><p>For populations in low- and middle-income countries (LMICs) who don\u2019t have reliable access to safe drinking water, the most common household water treatment method appears to be boiling (<a href=\"https://researchonline.lshtm.ac.uk/id/eprint/4119/\"><u>Rosa &amp; Thomas, 2010</u></a>). Furthermore, this is often done using solid fuels, which contributes significantly to indoor air pollution (<a href=\"https://ehp.niehs.nih.gov/doi/full/10.1289/EHP7124\"><u>Cohen et al., 2020</u></a>). We knew that indoor air pollution contributes to morbidity and mortality, and among other things, this may negatively impact neurodevelopment (<a href=\"https://www.unicef.org/sites/default/files/press-releases/glo-media-Danger_in_the_Air.pdf\"><u>Rees, 2017</u></a>), but we had not previously considered that the practice of boiling water could therefore be a significant health hazard due to the fuels used to heat the water.</p><p>All these pieces of information became decision-relevant for us when we were evaluating an invention (called \u201c<a href=\"https://uv-h2o-box.com/about-us\"><u>Water Box 2.0</u></a>\u201d) that was proposed as one possible water treatment method that could replace boiling (though it\u2019s not the first time a replacement has been proposed). Water Box 2.0 has been undergoing user testing in households that currently boil all their drinking water.<br><br>&nbsp;</p><h3><strong>4. We were surprised to learn that the&nbsp;</strong><a href=\"https://www.happierlivesinstitute.org/\"><strong><u>Happier Lives Institute</u></strong></a><strong> (HLI) has substantial room for additional funding.</strong></h3><p>There are two things that stand out to us when we think about HLI:</p><p>(i) They recently&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/MKiqGvijAXfcBHCYJ/deworming-and-decay-replicating-givewell-s-cost\"><u>identified</u></a> an issue in GiveWell\u2019s cost-effectiveness estimates for deworming charities. As part of that post, the HLI team made a series of recommendations, including incorporating decay more into their models of charities\u2019 cost-effectiveness, as well as increasing reasoning transparency when making charity recommendations. GiveWell&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/MKiqGvijAXfcBHCYJ/?commentId=Qt26uR9ZT6ru8xDqi\"><u>responded</u></a> to this analysis by updating their cost-effectiveness estimates relating to deworming charities and planning further research into how deworming charities\u2019 impact decays over time. GiveWell also recently announced a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/6wwK6Qduxt7mMmj8k/announcing-the-change-our-mind-contest-for-critiques-of-our\"><u>Change Our Mind</u></a> competition and has&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/MKiqGvijAXfcBHCYJ/?commentId=Qt26uR9ZT6ru8xDqi\"><u>said</u></a> that they will retroactively award HLI a prize for their post.</p><p>(ii) As far as we are aware, HLI is the only group trying to quantify the benefits of charitable interventions based on the improvements in happiness/wellbeing they create (rather than based on other frameworks such as QALYs or DALYs) and trying to identify highly effective giving opportunities based on this happiness/wellbeing-focused approach that may have been missed by other perspectives.&nbsp;</p><p>Given the points above, we expected that HLI would have a larger funding base. We were therefore surprised to learn that HLI has a lot of room for additional funding (including for a Grants Strategist and seed funding for a grantmaking fund). This is still the case at the time of this post.<br><br>&nbsp;</p><h3><strong>5. We hadn\u2019t considered that there may be significant new ideas about how to reduce the chance of nuclear war (given the age of the field).</strong></h3><p>We were not expecting to encounter new ideas for reducing the probability of nuclear war, given how old this field is. Consequently, we were interested in the proposal by the British American Security Information Council (<a href=\"https://basicint.org/\"><u>BASIC</u></a>) to research, develop, and widely implement a \u201cpractical framework and toolkit to assist policy practitioners in planning \u2018nuclear off-ramps\u2019 [de-escalation strategies] to avert nuclear weapons use during a time of tensions or conflict.\u201d The basic approach involves studying previous cases where nuclear war could have broken out, then, as BASIC puts it, \u201cuncovering the core ingredients and optimal conditions that enabled de-escalation.\u201d These components would then be compiled in a format that could be easily drawn upon in times of potential nuclear war crises (i.e., it would aim to provide key decision-makers with concrete options for de-escalation when nuclear use is imminent). According to BASIC, no such tool currently exists.</p><p>BASIC\u2019s proposed project would include the development of a Nuclear Off-Ramps Advisory Panel and would culminate in meetings with officials and experts from each of the world\u2019s nuclear possessor states. This proposal was surprising to us insofar as it appeared (to us, at least) to be a relatively novel idea for an intervention aiming to reduce the future probability of nuclear war. (Of course, one can only guess regarding the probability that, if developed, it would get used during a nuclear crisis.)<br><br>&nbsp;</p><h3><strong>6. Some of our team hadn\u2019t realized how difficult it can be to tell whether there\u2019s a vitamin deficiency in a population, and none of us had realized that point-of-care&nbsp;</strong><a href=\"https://en.wikipedia.org/wiki/Biosensor\"><strong><u>biosensors</u></strong></a><strong> might be feasible to roll out in the foreseeable future.</strong></h3><p>Among low- and middle-income countries (LMICs), from 1988\u20132018, only seven countries (5.1% of LMICs) reported on the vitamin B12 statuses of their population (<a href=\"https://academic.oup.com/ajcn/article/114/3/862/6283778?login=false\"><u>Brown et al., 2021</u></a>). Although we can\u2019t be sure (owing to this lack of data), untreated, undiagnosed deficiencies are&nbsp;<a href=\"https://watermark.silverchair.com/nutritionreviews60-s046.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAvwwggL4BgkqhkiG9w0BBwagggLpMIIC5QIBADCCAt4GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMRVSxdzXOBe84zp9MAgEQgIICr936ikSsKUcvesrbuCHfX6o9oLnzvaWbX28dt0J_8AAKSeFsb-gATlbg02m4bnWRShUKgspc_0V_KVU5tRbFJAsl8HvznuDOVsWCc9Puz3UjgIXJBhTefm23FTgZheNJlwFLcUUx5ylfYgyYSpI83h-PPteSWe-u9UgHBEOJas10K0vJscQHfUZKMt3ETbW0xF2jfDsVHh8ds8G2tzA_j7AuPnUY7fQ3sykLhlQwnLZWLuhHAkxKcOv-0ECU9A_9b4OPgqR7F3uos3hTmr7Mdznv0i_M_oGZoT-8JjJIObtgAOpB8JyahBA53nyQKn6orqoAuT-1Jx9jT4TXSOKFZVIVmD1fxvzNmVqZ-NAeQfQbXiT6F8EkabYpQ_O-Ax7I5V2cv4jWZ170Ewdp4-MSUL-_uvRRLjiicaw0X3Z19sp5LzJoZX_gPCuR2M5pCSbE3z2JHG6asxajrZfkjrrgmseJboSJhbEj-H6qHzG6CnoSbFSY8S7IF6ejpNvxE0i7zmdPDVJzd_Osnrj4EUpoaC1J7DpoGnl2k2FDbTb6c0YHMGfRcOMu5BQDXgg-ibenBFXeajx_e7zNOhhEKswVLQY4g4nS1ZUaXYTH9D3FCpcVmjj8ZvS_J58BnzuYldCE6na-ypk4dK1USxPUZH2_nRPmZ0gWR1EvhzMICtfgompamGCA4Su4jpp3ooIlc3iIW1OMbFlGEup-j8bpTOXXo3T1VSF5te96d7rHVs4WOZztJUGolwMOaPSDsRwHO8seeJP5eQ6ITtsmfoCl41zLGQovOtpqVIArI42-swleVw7txKHCSs1nH0bxuiYTtpWulwIGJERtBTOEBhjFQgjqKHS_O3LgCtqtk0R7auupoci5BhSKiJcNX31vjV2AYD8aCC3HQnZjBzjwf-5YMtXPvw\"><u>suspected by some researchers</u></a> to be relatively common,&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11596-020-2260-7\"><u>including</u></a> among pregnant women and newborn children. Given that the consequences of such untreated deficiencies are significant (<a href=\"https://journals.sagepub.com/doi/abs/10.1177/15648265080292s117\"><u>including</u></a> potential negative effects on infant cognitive and social development and mental health), one might wonder why it is not more common for countries to screen for B12 deficiency in their population.&nbsp;Screening data could then be used to justify&nbsp;<a href=\"https://journals.sagepub.com/doi/pdf/10.1177/15648265080292S129\"><u>supplementation or fortification</u></a>&nbsp;campaigns (in populations where such interventions would be beneficial).</p><p>We were interested to hear from&nbsp;<a href=\"https://heliobio.tech/\"><u>Project Helio</u></a> that, after speaking with data collectors for various organizations (WHO, CDC, UNICEF, Asia\u2019s Food Fortification Program, Micronutrient Forum, PATH, and USAID), they have come to the conclusion that the main barrier to population screening for B12 deficiency is the prohibitive cost of current testing methods. Subsequently, they plan to develop a low-cost point-of-care biosensor, aiming at &lt;$5/test, that could assess the B12 status of LMIC populations, thereby enabling those who need treatment to be identified in a cost-effective fashion. We will be interested to see results regarding the eventual accuracy and cost-effectiveness of the sensor they are developing.<br><br>&nbsp;</p><h3><strong>7. We hadn\u2019t realized that&nbsp;</strong><a href=\"https://www.1daysooner.org/\"><strong><u>1Day Sooner</u></strong></a><strong> conducts activities outside of their human challenge trial work, and that these activities have significant room for funding.&nbsp;</strong></h3><p>In addition to the work that&nbsp;<a href=\"https://www.1daysooner.org/\"><u>1Day Sooner</u></a> is known for (i.e., advocacy on behalf of volunteers for human challenge trials, which they believe may have sped up the deployment of COVID-19 vaccines if they had been implemented effectively), they have a project aiming to advocate for a faster regulatory pathway by which prototype vaccines could rapidly be approved and distributed at the onset of a potential pandemic. These vaccines, initially developed in advance of a pandemic, would include pan-sarbecovirus vaccines and prototype vaccines against pandemic pathogens more generally.&nbsp;</p><p>We were previously totally unaware of 1Day Sooner\u2019s planned work on this faster regulatory pathway, which they call the Warp 2 Pathway. The reforms they hope to facilitate include faster entry into and exit out of Phase 1 clinical trials and multiple options for proof of efficacy via surrogate biomarkers. The goal is to establish relatively narrow emergency use authorization followed by rapid confirmatory trials before broader authorization and licensure.&nbsp;At the time of this post, 1Day Sooner still has significant room for funding for their Warp 2 Pathway work.&nbsp;We will be interested to see how they approach these reforms if they receive sufficient funding. If successful, the project would get vaccines tested and disseminated more quickly in times of need.<br><br>&nbsp;</p><p>&nbsp;</p><h3><strong>Conclusion</strong></h3><p>We learned a great deal running the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/TzzuFGEhvoMiWNjjJ/announcing-the-clearer-thinking-regrants-program\"><u>Clearer Thinking Regrants</u></a> program and the accompanying&nbsp;<a href=\"https://manifold.markets/group/clearer-thinking-regrants/about\"><u>forecasting competition</u></a>. We are grateful to all of the people who applied to our program. We are also grateful to the Future Fund team for wanting to fund it. Based on our experience with the program, we think there are likely quite a number of promising smaller-sized giving opportunities available, where $10,000-$500,000 gifts might make a big difference to help a small-sized or new projects have a chance of getting off the ground (thereby increasing the probability that they one day turn into larger, potentially high-impact projects). We would be excited by future efforts to identify more such opportunities.</p><p>In the body of this post, we have attempted to avoid evaluative language and have tried to stick to describing facts about what surprised us during the Clearer Thinking Regrants program. If you notice any mistakes in the facts we mention, please let us know so that we can correct them.</p><p>For anyone who is interested in supporting any of the organizations mentioned above, please note that we also conducted reference checks for the leader(s) of each project mentioned in this post. All projects mentioned above had leaders whose reference checks reflected favorably upon their ability to execute their project well.<br>&nbsp;</p><p>Here is a list of the projects mentioned in this article, along with information about the people leading each of them:</p><ul><li><a href=\"https://docs.google.com/document/d/1n7g66SBlgZFPQPoBwe2CELETFCeXsvYQYvp-zahfJ1s/edit?usp=sharing\"><u>Quantifying the existential risk potential from large volcanic eruptions via climate, food and cascading risk modelling</u></a>: This project is co-led by&nbsp;<a href=\"https://www.birmingham.ac.uk/schools/gees/people/profile.aspx?ReferenceId=199847&amp;Name=dr-mike-cassidy\"><u>Mike Cassidy</u></a> and&nbsp;<a href=\"https://www.cser.ac.uk/team/lara-mani/\"><u>Lara Mani</u></a>.&nbsp;</li><li><a href=\"https://docs.google.com/document/d/1a1HJqU20IuM5RHH63MRPfwWfhaYIMxhYxRU4ZzUA0pM/edit?usp=sharing\"><u>Rethink Priorities (Surveys Team)</u></a>: The&nbsp;<a href=\"https://rethinkpriorities.org/\"><u>Rethink Priorities</u></a>&nbsp;<a href=\"https://rethinkpriorities.org/ea-movement-research\"><u>Surveys Team</u></a> is led by&nbsp;<a href=\"https://forum.effectivealtruism.org/users/david_moss\"><u>David Moss</u></a>.</li><li><a href=\"https://uv-h2o-box.com/wp-content/uploads/2020/07/HelpWanted.pdf\"><u>Water Box</u></a>&nbsp;<a href=\"https://docs.google.com/document/d/1WqTTMGvS_E7qH325nr0NszGsZtxTpnXu/edit?usp=sharing&amp;ouid=102253417970778718404&amp;rtpof=true&amp;sd=true\"><u>2.0</u></a>: This project is&nbsp;<a href=\"https://uv-h2o-box.com/about-us\"><u>co-led by Paul Berg and David Conklin</u></a>.&nbsp;</li><li>Happier Lives Institute Grants Strategist and seed funding for a grantmaking fund: This project is co-led by&nbsp;<a href=\"https://uk.linkedin.com/in/michael-plant-8445116a\"><u>Michael Plant</u></a> and&nbsp;<a href=\"https://www.linkedin.com/in/lily-yu-06241\"><u>Lily Yu</u></a>, both at the&nbsp;<a href=\"https://www.happierlivesinstitute.org/\"><u>Happier Lives Institute</u></a>.</li><li><a href=\"https://docs.google.com/document/d/1LzNzaEVE5Vkz0XMeJTjJ3HPDDLJc_MPQ9tWRBVrUoBE/edit?usp=sharing\"><u>Off-Ramps: Averting Near Nuclear Use in Crisis and War</u></a>: This project is led by&nbsp;<a href=\"https://twitter.com/RishiPaul_D\"><u>Rishi Paul</u></a> at the British American Security Information Council (<a href=\"https://basicint.org/\"><u>BASIC</u></a>).</li><li><a href=\"https://heliobio.tech/\"><u>Project Helio</u></a>: This initiative is led by<a href=\"https://www.linkedin.com/in/ashley-mo/?originalSubdomain=ca\">&nbsp;<u>Ashley Mo</u></a>&nbsp;and<a href=\"https://www.linkedin.com/in/aoi-otani-ab3a1b205/\">&nbsp;<u>Aoi Otani.</u></a></li><li><a href=\"https://docs.google.com/document/d/1JGAmWkEXjWQ_feV4TVm1iuEQPO0SI2y0CjHuot5d7Ws/edit?usp=sharing\"><u>Reforming regulations for pandemic countermeasures</u></a>: This project would be led by a yet-to-be-hired project director with regulatory experience,&nbsp;<a href=\"https://www.linkedin.com/in/joshcmorrison\"><u>Josh Morrison</u></a> (President),&nbsp;<a href=\"https://www.linkedin.com/in/gavriel-kleinwaks-6a5398b0\"><u>Gavriel Kleinwaks</u></a> (Strategic Projects Lead), and&nbsp;<a href=\"https://uk.linkedin.com/in/enlli-lewis-441456168\"><u>Enlli Lewis</u></a> (Regulatory Policy and Research Coordinator), all at&nbsp;<a href=\"https://www.1daysooner.org/\"><u>1Day Sooner</u></a>.</li></ul><p><br>&nbsp;</p><h3><strong>Acknowledgments</strong></h3><p>Thank you to all the grant applicant finalists mentioned here, for making us aware of giving opportunities we had not known about before. Thank you to&nbsp;<a href=\"https://forum.effectivealtruism.org/users/adam-binks\"><u>Adam Binks</u></a>, Amanda Metskas,&nbsp;<a href=\"https://forum.effectivealtruism.org/users/cafelow\"><u>Catherine Low</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/users/richard-moehn\"><u>Richard M\u00f6hn</u></a>, and Travis Manuel for their helpful edits and comments on earlier drafts. Edit (Nov 15): We are also grateful to the (<a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1\"><u>now resigned</u></a>) Future Fund team for wanting to make this program possible.<br><br>&nbsp;</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6pgsgpi0c85\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6pgsgpi0c85\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This post is not focusing on the outcomes of the grant applications, but if you are interested in those outcomes, you can review them in&nbsp;<a href=\"https://manifold.markets/group/clearer-thinking-regrants/markets\"><u>Manifold Markets</u></a>. (As you can see at that link, we recommended that all projects listed in this post should receive at least some funding. You\u2019ll also see that the projects listed here are not the only ones that we recommend should receive some funding. Our criterion for including one of the selected projects in the current post was simple: we included it if it introduced us to information that surprised us.)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk5butud8nz\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk5butud8nz\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We expect that anyone who becomes interested in these organizations through our post will do their own investigations and come to their own conclusions. We simply thought it would be useful (in expectation) to share the fact that certain organizations and projects have a surprising (to us, anyway) amount of room for more funding. Please contact us if you want to be put in touch with any of the groups listed here. Note that we conducted reference checks for the leader(s) of each project mentioned in this post. All projects mentioned above had leaders whose reference checks reflected favorably upon their ability to execute their project well.</p></div></li></ol>", "user": {"username": "spencerg"}}, {"_id": "r7mnFNoTpqP9FmswX", "title": "The Powerball Lottery Is Now Positive Expected Value for Many Charitable Americans", "postedAt": "2022-11-07T00:03:44.222Z", "htmlBody": "<p><i>Disclaimer: I\u2019m not a lawyer or accountant. Check my work and your own situation before spending money on lottery tickets. I think the calculations are correct. Cross-posted from my </i><a href=\"https://coldbuttonissues.substack.com/p/the-powerball-lottery-is-now-positive\"><i>Substack.</i></a></p><p>The Powerball is a lottery that exists in almost all American states. On Monday, November 7, the jackpot is worth $1.9 billion dollars. That figure refers to the prize you would receive if you chose to receive an annuity payment from the lottery. The lottery also lets you claim a lesser lump sum amount instead, which you get upfront. Most experts think this is the right move. On November 7, the lump sum is $929 million. (That figure can change if there are more or fewer tickets sold than expected.)</p><p>Gambling is rarely a good move. But for people who would like to give a large share of their gambling winnings to charity, playing this Powerball makes sense because of the tax deduction that you can receive by donating to charity.</p><p><strong>The Charitable Tax Deduction, the Income Tax, and the Lottery</strong></p><p>The federal government and many state governments require Americans to pay a tax based on the income they receive during the year. Some unusual types of income- capital gains, inheritance income- are sometimes taxed at different rates.&nbsp;</p><p>Because the lottery does not pay back all of the money it receives in ticket sales and because winners have to pay taxes on lottery winnings, playing the lottery normally has negative expected value, financially.</p><p>The federal government and I believe all states allow you to donate a certain share of your income in a year and not pay income taxes on the donated money, provided you donate to tax-deductible charities. For 2022, you can donate up to <a href=\"https://www.irs.gov/charities-non-profits/charitable-organizations/charitable-contribution-deductions#:~:text=Individuals%20may%20deduct%20qualified%20contributions,to%20the%20next%20tax%20year.\"><u>60%</u></a> <a href=\"https://www.schwabcharitable.org/maximize-your-impact/tax-strategies\"><u>of your income in cash</u></a> and receive a tax deduction! If you give more than 60% of your income in cash,<a href=\"https://support.taxslayer.com/hc/en-us/articles/360015902051-What-is-a-charitable-contribution-carryover-#:~:text=You%20can%20carryover%20your%20contributions,but%20not%20beyond%20that%20time.\"><u> you can carry over those deductions for the next five years</u></a>. This means you can avoid federal income tax on the majority jackpot prize and the entirety of some of the smaller prizes. (Most states have similar rules.)</p><p>If you value a dollar given to a charity of your choice, as much as you value a dollar you keep for yourself from lottery winnings, you might want to buy a Powerball ticket for tomorrow. This is especially true for people who:</p><p>Are high-income.</p><p>Live in states with low or no income tax.</p><p>Would buy many tickets, reducing transaction costs per ticket.</p><p><strong>Powerball Prizes</strong></p><p>The Powerball offers a variety of prizes. The jackpot&nbsp; is divided between all players with the right numbers on their ticket. When no one wins, the money carries over to the next lottery. The other prizes are awarded to every ticket that has the specified number of matching numbers. <a href=\"https://www.powerball.net/prizes\"><u>Powerball.net</u></a> helpfully breaks down the odds.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10047004-f7bf-4067-b45b-c4e2f1e0d3b8_361x323.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10047004-f7bf-4067-b45b-c4e2f1e0d3b8_361x323.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10047004-f7bf-4067-b45b-c4e2f1e0d3b8_361x323.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10047004-f7bf-4067-b45b-c4e2f1e0d3b8_361x323.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10047004-f7bf-4067-b45b-c4e2f1e0d3b8_361x323.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10047004-f7bf-4067-b45b-c4e2f1e0d3b8_361x323.png 1456w\"></a></p><p>&nbsp;</p><p>If you have a middle-class income and are not currently exhausting your charitable tax-deduction, you should be able to donate the entire value of all prizes up to $50,000 with no federal income tax implications.</p><p>If you multiply these prizes by the probability of receiving them and sum them, you get a figure of approximately $0.0235. The price of a Powerball ticket is $2 so you\u2019re in the hole. Adding in the $1 million prize helps. If you donate $600,000 of the $1 million prize and you\u2019re a high-earner you might have to&nbsp; pay<a href=\"https://www.nerdwallet.com/article/taxes/federal-income-tax-brackets\"><u> 37%</u></a> income tax on the remaining $400k. That means the $1 million prize is worth $852K to you. Weighting that prize by its odds and adding it to the other prizes, that lottery ticket is now worth $0.308.</p><p>The value of the jackpot prize depends on how many people you expect to share it with.</p><p><strong>How Many Other Players and Winners?</strong></p><p>If you have a winning ticket, how many other people are you probably splitting the jackpot with? That depends on how many other people played that Powerball draw. <a href=\"https://www.lottoreport.com/ticketcomparison.htm\"><u>Lottoreport.com</u></a> provides the number of Powerball tickets purchased per draw. Generally, more tickets are sold when the jackpot is higher. Powerball drawings occur on Monday, Wednesday, and Saturday. Monday draws have lower sales- perhaps because there are&nbsp; only two days from the previous draw and people don\u2019t like to gamble on Sunday.</p><p>Monday had average sales of 13.3 million tickets.</p><p>Wednesday had average sales of 18.2 million tickets.</p><p>Saturday had average sales of 25.6 million tickets.</p><p>That tomorrow is a Monday is really good for the value of playing the Powerball.&nbsp;</p><p>To estimate the number of expected tickets sold for tomorrow\u2019s drawing I ran a linear regression of tickets sold with two independent variables: day of draw and advertised jackpot. The model predicts 220 million tickets to be sold. If you have a winning ticket how many winning tickets should you expect there to be in the other 220 million tickets told? I used an <a href=\"https://stattrek.com/online-calculator/binomial\"><u>online binomial probability calculator </u></a>for these probabilities.&nbsp;</p><p>There is a 47% chance there will be zero other winning tickets.</p><p>There is a 35% chance there will be one other winning ticket.</p><p>There is a 13%&nbsp; chance there will be two other winning tickets</p><p>There is a 3% chance there will be three other winning tickets&nbsp;</p><p>There is a 0.6% chance there will be four other winning tickets.</p><p>The other outcomes have negligible probabilities, so I ignore them. Assuming you donate 60% of the jackpot, pay 37% tax on the rest of the jackpot, you get 85.2% of the value of the jackpot. With a lump sum of $929 million and potentially having to share it with others, the expected value that comes from the chance to win a jackpot per lottery ticket is $552 million divided by 292 million or $1.89. Of course there is also the expected value of approximately $0.31 from non jackpot prizes.</p><p>The expected value of a ticket for people who live in states without income taxes and max out their charitable contribution deductions is $2.20. The cost of a ticket is $2. it looks like buying a Powerball ticket offers a 10% return if you don\u2019t have to deal with state income tax.</p><p><strong>Should You Buy A Ticket?</strong></p><p>Whether you individually buy a ticket is up to you. The expected gain per ticket even under fortunate circumstances is only $0.20. You\u2019d have to risk a ton of money to gain enough in expected value terms to offset the costs of time of purchasing tickets, checking tickets, and in states where you can only use cash for lottery tickets, going to the ATM.</p><p>You might on principle refuse to gamble or refuse to gamble due to risks of gambling addiction.&nbsp;</p><p>If you do gamble and and are charitable, buying a Powerball ticket sounds like a good deal. Before making a decision, check out your own tax situation and the calculations here.</p><p><strong>Notes</strong></p><p>I am assuming players are not paying extra for \u201cPowerplay\u201d tickets. I haven\u2019t done the math but I\u2019m fairly confident they\u2019re a bad idea.</p><p>I used the expected jackpot figure rather than the actual jackpot figure, because I assumed the posted figure was what motivated sales.</p><p>The 220 million figure seems reasonable. The model is simple and eyeballing the recent trends, the Monday sales are often lower than the Saturday sales even as the jackpot rises.</p><p>I used R for linear regression.</p><p><br>&nbsp;</p>", "user": {"username": "ColdButtonIssues"}}, {"_id": "qBiSD6LP5pjajbgCZ", "title": "Historical experience and links to subjective happiness", "postedAt": "2022-11-06T19:05:33.221Z", "htmlBody": "<p>This might have been discussed before, please let me know, but a quick search on the forum hasn't shown much. Assuming it hasn't I think the link between sentient lived experience and happiness isn't discussed enough in EA and Utilitarianism.&nbsp;</p><p>The general premise is, that as we have different experiences this alters how our future experiences are judged with respect to happiness or sadness. As an example we can \"train\" our pain/comfort thresholds, to such an extent that for what some would consider torture, others would be fine with/experience joy. A more mundane example might be if I am taking a crowded train in Tanzania (where I live) &nbsp;in third class on very hard cramped seats for 2 days across the country, this would be supremely uncomfortable and painful for me, whereas for many of the other passengers they seem completely comfortable and are unfazed by the journey. Similarly a Saudi Prince if put into the living conditions that much of the rural communities in least developed countries live in, would likely find this experience very unpleasant and choose to \"skip\" the experience rather than live through it.</p><p>Assuming the above is true for our conscious experience, then perhaps similar is true for other animals, which would have wide implications for animal farming in terms of how best to reduce negative experiences for the most animals.</p><p>Overall, perhaps the best way to increase utility is to concentrate on ensuring that the (negative) delta between peak experience is minimized.&nbsp;</p><p>I should stress that I'm not completely sure on the above, I'm an avid reader rather than an expert in these fields, but I think these are questions that should be asked more in general (if they haven't already been).</p>", "user": {"username": "Arno"}}, {"_id": "bzxrN6Hn8QAii3QPs", "title": "Why does Elon Musk suck so much at calibration?", "postedAt": "2022-11-06T18:10:36.318Z", "htmlBody": "<p>I won't moralize about Elon Musk as a personality but what should be of greater importance to effective altruists anyway is how the impacts of all his various decisions are, for lack of better terms, high-variance, bordering on volatile. From the outside view, there are stock answers to the question of why he would be like this:</p><ul><li>&nbsp;An ultra-talented person whose success keeps generalizing to more domains may become revered by others that have them conclude that person is a universal genius, who then gets caught in an echo chamber of yes men and loses self-awareness of when he is super wrong.</li><li>A person gaining celebrity, wealth and power can exacerbate that trend because there are higher stakes. This doesn't even necessarily mean that someone in Musk's position would become intellectually corrupted or deeply out of touch with reality. The wealth and power of many others depend on what someone in his position does, such as investors backing Musk's companies compelling him to make changes to his businesses or personal brand against his own preferences. Those around Musk backing him the most have in a lot of ways more total influence than he does himself, so they will try shaping Musk's social network in a way that pushes him in the direction they want.&nbsp;</li><li>Someone having an outsized impact means the negative impacts of their own biases will have an outsized impact too. It's more noticeable when one of the wealthiest and most famous people worldwide makes mistakes. All kinds of other people in Musk's position might make mistakes way more and way worse than him.</li></ul><p>Problems like these are recognized in EA already. During the first few years, one of the growing pains in EA was learning to recognize when a top, young Ph.D. with no experience outside academia shouldn't be the CEO of an organization doing something totally different from academic research. Another one of the growing pains was learning how to point that out to people in EA in positions of status, authority, control over resources, <i>etc</i>.&nbsp;</p><p>This isn't a snide jab at Will MacAskill. He in fact recognized this problem before most and has made the wise choice of not being the CEO of the CEA for a decade now even though he could have kept the job forever if he wanted. This is a general problem in EA of many academics having to repeatedly learn they have little to no comparative advantage, if not a comparative <i>disadvantage</i>, in people and operations management. The fact that there is such a fear of criticizing the decisions or views of high-status leaders, someone like Holden Karnofsky, in EA that it's now a major liability to the movement. Meanwhile, Holden writes entire series of essays trying to make transparent his own reasoning of why he oversees an organization that hires a hundred people to tell Holden how the world really works and how to do the most good in umpteen different ways.</p><p>Some of the individuals about who there is the greatest concern that may end up in a personality cult, information silo, or echo chamber, like Holden, are putting in significant effort to avoid becoming out of touch with reality and minimizing any negative, outsized impact of their own biases. Yet it's not apparent if Musk makes any similar efforts. So, what, if any, are the reasons specific to Musk as a personality causing him to be so inconsistent in the ways effective altruists should care about most?</p>", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "rf3iP4KSy9t42QbfC", "title": "Apply to attend an AI safety workshop in Berkeley (Nov 18-21)", "postedAt": "2022-11-06T18:06:53.016Z", "htmlBody": "", "user": {"username": "Akash"}}, {"_id": "b3wB5GY3Zeh3cpkaG", "title": "My (naive) take on Risks from Learned Optimization", "postedAt": "2022-11-06T16:25:34.907Z", "htmlBody": "", "user": {"username": "Artyom K"}}, {"_id": "niyr9uvHTfSboRRne", "title": "Jane English on Rawls and duties to future generations", "postedAt": "2022-11-06T14:25:09.026Z", "htmlBody": "<h2>Jane English</h2><p>I recently learned<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefej0r0e93vsu\"><sup><a href=\"#fnej0r0e93vsu\">[1]</a></sup></span>&nbsp;about Jane English, a brilliant and regrettably little-known philosopher from the middle of the 20th century. This post is mainly about a contribution of hers to a longtermism-relevant debate, but first I'll say a little about her life, which was pretty remarkable.<br><br>English was born in Cleveland in 1947 and did her PhD in philosophy at Harvard, where she studied with Quine, Putnam and Rawls. She graduated in 1973, was promptly hired by UNC Chapel Hill, and earned tenure in 1977. She died the next year while climbing the Matterhorn, at age 31.&nbsp;<br><br>During her five years of work as a professional philosopher, English published important papers on theoretical concepts, underdetermination in science, the morality of abortion, gender equality in sports, and duties to future generations in a contractarian framework. Pretty amazing! All the signs suggest she was destined for philosophical greatness. She was also a serious runner, swimmer and tennis player. (You can read a bit more in <a href=\"https://www.jstor.org/stable/3131459\">this memorial note</a> written by her colleagues at UNC.)</p><p>The English paper I want to talk about here is <a href=\"https://www.jstor.org/stable/4319117\">\"Justice Between Generations\"</a> (1977, <i>Philosophical Studies</i> 31, 91-104). The paper deals with Rawls' views on intergenerational savings principles. English presents several powerful objections to Rawls, offers a sensible alternative proposal, and says interesting things about what kinds of intergenerational duties we can hope to derive in a Rawlsian contractarian framework. These ideas should be relevant to those longtermists who want to constructively engage with non-consequentialist traditions.</p><h2>Rawls and the veil</h2><p>First, some Rawls.</p><p>Lots of people consider Rawls' <i>A Theory of Justice </i>(1971) to be the 20th century's most important work of political philosophy. In the famous contractarian thought experiment at its heart, Rawls imagines the members of society choosing principles of justice from behind a \"veil of ignorance\", where nobody knows their own demographic categories, personal aptitudes or conception of the good.</p><p>The self-interested agents in this \"original position\", Rawls thinks, would choose principles which grant everyone the greatest possible liberties (compatible with everyone else having those same liberties), and which maximize the well-being of the worst-off person in society (since, for all you know, that person might be you).</p><p>A natural way of understanding the original position is to assume that: (1) Everyone in the OP is motivated by individual self-interest. (2) Everyone in the OP belongs to the same generation. As Rawls notices, though, this pair of assumptions causes problems. Under these conditions, the participants in the OP would assign all available resources to themselves to use for their own purposes, leaving nothing for future generations -- not quite the perfectly just society we'd hoped to derive.</p><p>Rawls' fix for this is to modify condition (1). Rather than purely self-interested individuals, the participants in the OP should be <i>heads of households</i>, who want what's best not just for themselves but for their whole families. Under this assumption, we get a nonzero rate of savings, since everyone wants good lives for their children.</p><h2>English on Rawls</h2><h3>English's criticisms</h3><p>Enter Jane English. As English points out, Rawls' solution to the savings problem is unsatisfactory for several reasons:</p><p><strong>First: </strong>Throughout <i>AToJ</i>, Rawls emphasizes his intention to assume as little as possible about the contingent motivations and feelings of the participants in the OP. Turning the participants into heads of households full of concern about their children's futures is a pretty big deviation from that ideal. (And it also seems to spoil the theory's universalist ambitions. What if the society in question doesn't have anything like households in this sense? Or did Rawls think these were somehow natural and inevitable?)</p><p><strong>Second: </strong>Rawls' solution doesn't tell us anything about how much heads of households would or should value the next generation's interests relative to their own. \"Unless we know how strongly they care, we do not know which principles will be chosen. Are heads willing to starve to death to send their children to college, or are they only willing to drive a Datsun instead of a Buick to achieve that end?\" (94)</p><ul><li>You might try to solve this problem in a principled way by taking heads of households to be overall-family-welfare-maximizers. But this could lead the OP participants to accept unfair family arrangement principles. For instance, it might maximize total family welfare to have a strict sexist division of labor, or to determine inheritance by arbitrary principles of primogeniture. Since these are exactly the kinds of utilitarian bargains that Rawls rejects on the social level, it's hard to see why they'd be OK within families.</li><li>Another possible solution is to imagine a \"veil-within-the-veil\", where family members choose savings principles in ignorance of who belongs to which generation. But this won't work either. Any positive savings rate requires the older generation to sacrifice for the benefit of the younger generation. A priori, the younger generation isn't worse off than the older. So this violates the \"difference principle\" which parties in the OP are supposed to accept, which says that inequalities are only permissible when they benefit the worst-off.</li></ul><p><strong>Third: </strong>Not everyone has children to save for, and not every child has household members to save for them. So Rawls' approach can't explain why current people in general should save on behalf of future people in general.</p><p>English concludes, I think correctly, that the head-of-household strategy is a bad way to motivate care for future generations in a Rawlsian contractualist framework.</p><h3>English's proposal</h3><p>Her own solution? <strong>Get rid of assumption (2)</strong>, and assume instead that the original position contains participants from different generations. Since some later people will be badly off if earlier people don't save, and since anyone in the OP might turn out to be a later person themselves, this version of the OP will yield an intergenerational savings principle.</p><p>Why is this better than Rawls' approach?</p><p>Most obviously, it avoids all the ad hoc stuff about households and families. This immediately solves the first and third problems above.</p><p>It also seems very much in line with the spirit of the OP. The whole point of the thought experiment is to consider which principles of justice we'd choose if we knew as little as possible about our contingent situations and interests. Surely the generation to which we belong is one of these contingent factors. It deserves to be hidden behind the veil.</p><p>But you might worry that English's proposal runs into the same difficulty as the \"veil-within-a-veil\" idea mentioned above. A priori, we don't know that later generations will be worse off than earlier ones -- on the contrary, they might be much better off. So a savings principle might have the effect of forcing poor early people to sacrifice on behalf of rich later people. This violates the difference principle, so it should be unacceptable.</p><p>As English points out, we can avoid this problem if we only require the <i>best-off</i> &nbsp;members of earlier generations to save for the <i>worst-off</i> members of future generations (after they've already done as much as they can to help the currently worst-off people). The only way this strategy would run afoul of the difference principle is if <i>every </i>member of some early generation is worse off than <i>every </i>member of all future generations. This might be possible in theory, but it seems exceedingly unlikely in practice.</p><p>As we saw, English criticizes Rawls's head-of-household approach for not giving any guidance about the magnitude of a just savings rate. What kind of rate do we get on her view?</p><blockquote><p>The rate of saving called for... would be a relatively slow one, of course. Under some adverse empirical circumstances, namely when the worst-off in the first generation absorb so many goods that the worst-off in the second are no better off than their predecessors, no progress at all would result. Typically, however, progress is generated from several sources. First, innovation, invention and the accumulation of knowledge alone contribute to making later generations better off. Second, the co-existence of several generations produces short-term saving, as I have argued above. Third, the amount of sacrifice required to preserve and pass on knowledge or machines is small compared to the expense required to produce them anew. This means that relatively small sacrifices on the part of the better off in the first generation (such as oiling the machines and recording the knowledge) will tend to improve the lot of their successors significantly. (101)</p></blockquote><p>Another virtue of English's approach is that \"it also calls for temporally reversed 'saving', e.g., through deficit spending, from those who are better off in later generations to aid those worse off in earlier ones\" (103).</p><p>Happy ending: Rawls was persuaded by these criticisms and abandoned the head-of-household approach in his later <i>Justice as Fairness</i>.</p><h2>English, contractarianism and longtermism</h2><p>Longtermists often motivate their views by appealing to the rights and interests of future generations. And these views often involve \"grand futures\", in which our descendants are long-enduring, numerous, wealthy and happy. Can English's version of contractarianism explain why current generations should sacrifice to make such futures possible? Can any broadly Rawlsian theory do so?</p><p>English herself seems skeptical of some of these obligations, at least from the viewpoint of intergenerational justice:</p><blockquote><p>It might be the case, on my account, that a society starting out at a subsistence level, under adverse empirical circumstances, would never be required by justice to begin the accumulation necessary to lead to centuries of progress and culture. But this seems to be correct. Of course, progress and culture are also goods, but generating them does not seem to be entirely a question of justice. We might want this progress for perfectionistic or other reasons, especially if the sacrifice on the part of the worst-off was small. Then this would be a saving chosen for reasons that override considerations of justice between generations. (103-4)</p></blockquote><p>Besides bequeathing \"progress and culture\" to our descendants, though, many longtermists are concerned about the <i>existence </i>and <i>size</i> of future generations. That is, longtermists often favor:</p><ol><li>Existential risk reduction, i.e., earlier generations sacrificing so that future generations have a better chance to exist.</li><li>Plenitudinous futures containing vast numbers of our descendants.</li></ol><p>The best contractarian hope for justifying saving toward these desiderata, it seems to me, is to include <i>possible </i>as well as actual future people as bargainers in the original position. We'd also have to assume that possible future people prefer (sufficiently high-quality) existence over nonexistence. Given this setup, it's not hard to see how the parties to the OP would agree to try to maximize the number of future people (to the degree allowed by the difference principle).</p><p>Is this a reasonable amendment to the OP? I can imagine arguments for both sides, but the idea doesn't seem completely indefensible. The principles of justice governing social and political organization should plausibly cover <i>some</i> questions of population ethics: for instance, it seems unjust for one generation to reproduce in such large numbers that the next generation has no hope of sustaining itself. It's natural to explain this in terms of the principles one would've accepted if they were, for all they knew, one of those possible future people.&nbsp;</p><p>If contractarianism has space for this kind of reasoning, it should also be able to accommodate longtermist population imperatives. I'd like to think Jane English would agree.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdeneh27gfl6\"><sup><a href=\"#fndeneh27gfl6\">[2]</a></sup></span></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnej0r0e93vsu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefej0r0e93vsu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>From my MCMP colleague John Dougherty. Thanks, John.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndeneh27gfl6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdeneh27gfl6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Apologies to those who know more about contractarianism and population ethics than me. I'm sure lots of interesting things have been written about this that I'm unaware of.</p></div></li></ol>", "user": {"username": "William D'Alessandro"}}, {"_id": "fcmBtib6irARdtaFm", "title": "When can a mimic surprise you? Why generative models handle seemingly ill-posed problems", "postedAt": "2022-11-06T11:46:05.529Z", "htmlBody": "", "user": {"username": "David Johnston"}}, {"_id": "GWyidA3fbXKXErDn4", "title": "Effective altruism as a lifestyle movement (A Master\u2019s Thesis)", "postedAt": "2022-11-06T11:50:05.836Z", "htmlBody": "<p>Earlier this year, Jemina Mikkola published her Master\u2019s Thesis in Sociology on the topic of \u201cEffective altruism as a lifestyle movement\u201d. For her research, she interviewed 8 persons who were or had been involved in effective altruism through the university/city group Effective Altruism Helsinki. The purpose of the study was to find out if EA is a lifestyle movement as defined by Haenfler, Johnson, Jones (2012). Mikkola concluded it is.</p><p>If you understand Finnish, you can read the whole thesis&nbsp;<a href=\"https://helda.helsinki.fi/bitstream/handle/10138/343908/Mikkola_Jemina_tutkielma_2022.pdf?sequence=3&amp;isAllowed=y\"><u>here</u></a>. Since the contents of the thesis might be of interest to non-Finnish-speakers as well, I will summarize her key points in this post. To my understanding, the thesis received the grade \u201cgood\u201d, so the research is not exceptionally well done, but should fill the expected function of a thesis. Reading these findings you should keep in mind that this work was done by a student, not by an experienced researcher.&nbsp;</p><p>To my knowledge this is the only time anyone has conducted sociology research on an EA group. Mikkola could not find previous research on EA groups specifically, making it her main motivation to choose EA as a thesis topic. Mikkola is not involved in EA, but followed the EA Helsinki Telegram chat for a while and participated in one online meetup for the purposes of her study.</p><h3>According to Mikkola, EA is a lifestyle movement</h3><p>The concept of a lifestyle movement was developed by&nbsp;<a href=\"https://www.researchgate.net/publication/233433355_Lifestyle_Movements_Exploring_The_Intersection_of_Lifestyle_and_Social_Movement_in_The_Voluntary_Simplicity_and_Social_Responsibility_Movements\"><u>Haenfler, Johnson, Jones (2012)</u></a>. Lifestyle movements are loosely organized or non-organized movements that aim for social change primarily by the means of individual lifestyle choices. They are different from \u201ctraditional\u201d social movements that have an external focus, aim for collective (often political) action and have some degree of organization. They define lifestyle movements by the following characteristics:</p><ul><li>individual (as opposed to collective) action: participation occurs primarily at the individual level with the subjective understanding that others are taking similar action, collectively adding up to social change</li><li>private and ongoing action: participation occurs in daily life (so its not public and not episodic)</li><li>action is understood as an effort towards social change (so it is not for example exclusively done as self-help or religious exploration)</li><li>personal identity is a site of social change: adherents engage in identity work, focusing particularly on cultivating a morally coherent, personally meaningful identity in the context of a collective identity</li></ul><p>Mikkola found that EA satisfied this definition in the following way:</p><ul><li>Action is individual: the interviewees felt their impact comes from personal choices, such as donating and studying to later build an EA aligned career</li><li>EA is present in daily life: study and work are methods of doing good, donating influences finances and participants use their free time to learn more about EA</li><li>Action is done for social change: Mikkola noted that EA stands out from other lifestyle movements in the sense that in EA, actions are never taken just for the sake of participating in the movement, but they are always tied to the end goal of having an impact</li><li>Identity work: Usually, members of a social movement can see their actions as a sign of moral virtue, and taking action helps them keep up with the idea of themselves as a good person. However, in Mikkola\u2019s interviews nobody seemed to get this feeling out of EA. But many people described having experienced pressure to follow EA principles, and emphasized that they were \u201cjust ordinary people\u201d, which Mikkola interpreted as a way of coping with the high standards of EA. According to her, EA can act as a way of avoiding the identity of an immoral person, rather than constructing an identity of an especially virtuous person.</li></ul><p>Some other lifestyle movements Mikkola compares EA to are veganism, skepticism and urban farming. For me, these seem a lot less organized and less cooperative than EA, but it is true that in early 2021 when she conducted the interviews, EA Helsinki was almost non-organized, did not have many group level projects and did not do much international cooperation with other EAs.&nbsp;</p><p>Maybe Mikkola would have gotten different results if she had interviewed us now that EA Finland has received a grant to pay employees to do community building and we have done some collective outreach projects as a group. On the other hand, I guess most people at EA Finland would still feel their cooperative action with other EAs as the best individual action available to them, so maybe this would not undermine the individualistic action point.</p><h3>Additional stuff I found interesting</h3><ul><li>I learned the term \u201cimagined collective\u201d (for example be \u201call other vegans around the world\u201d): a group of people who share the same values and are participating in the same lifestyle movement, even if they are not directly socially connected and don\u2019t know each other<ul><li>for some interviewees, the global EA movement was such an imagined collective (and the local EA Helsinki group is just a regular collective, where you actually know other members)</li></ul></li><li>A related term is the \u201cgeneralized other\u201d, a prototypic member of the imagined collective<ul><li>usually, comparing oneself to the generalized other can be a source of pressure for lifestyle movement participants</li><li>but for many interviewees, the generalized other actually included all living humans, non-human animals and/or past and present generations, and acted a source of motivation to do EA</li></ul></li><li>And a third term I learned was \u201cdiscursive consciousness\u201d, where actors aim to align their values and daily actions through intentional introspection and research<ul><li>Mikkola thought our discursive consciousness was quite high, and that this was increased by EA Helsinki activities, such as our career club where people try to help each other make more EA aligned career choices</li><li>in some social movements, the described value alignment might be focused on creating an identity of a righteous person rather than actual social change. Mikkola noted that in EA this seems impossible to do due to EA\u2019s emphasis on measurable results.</li></ul></li><li>As a support person of EA Finland, I paid attention to the fact that there were two different ways in which people seemed to feel EA pressure<ul><li>one was a general feeling of guilt or inability to be more productive or more cost-effective and the need to frame rest as an EA activity (\u201csometimes I need to choose between watching an informative lecture of just some junk from Netflix, and sometimes the answer is that I don\u2019t have the energy to watch a lecture, so I\u2019ll watch some junk from Netflix\u201d)</li><li>the other one was the inability to do certain things recommended in EA media. Especially 80 000 hours job recommendations seemed too demanding for several respondents (\u201cI can\u2019t get an hc job like the ones recommended on this page, why don\u2019t they recommend some job I could get, sometimes I feel desperate and think I\u2019m not going to have any impact whatsoever\u201d)</li></ul></li><li>Mikkola made one interpretation I had never seen before, which is the claim that EA gives participants motivation and meaningfulness to do things they are socially expected to do anyway, such as work or study. Apparently our work and study choices did not seem very weird to her.</li><li>Some interviewees also viewed EA as a way of following values that are generally shared but not coherently applied in Finland, such as truly treating every person as equal (and thus donating globally rather than locally)</li><li>I think we also got some information on why some respondents are now less active in EA: two people felt we are not doing enough mass outreach and cited this as a reason for decreased participation.</li><li>One person stated the first time they had attended an EA Helsinki meetup they \u201cdid not understand half of the words\u201d and felt the need to go home and Google what all the words mean</li><li>One person stated they reject longtermism as their personal EA approach due to not being \u201csome kind of an anime main character that saves all of humanity\u201d</li><li>One person did not want to appear as a \u201ccrazy moralist that tells people over-consumption is murder of African children\u201d even if they thought this was true</li><li>One person stated they view all their actions through EA (with the exception of \"almost having started to think of [their] relationship as something intrinsically valuable\"), but for most people EA was not that all-encompassing. For example, a different person viewed their interest in research as intrinsically valuable, even if they felt it was not the most effective way to do good.</li></ul><p><br>The interviews were quoted pseudonymously, but for many quotes, it is pretty easy to recognize who said what. I was also interviewed for the research, and I think most people who know me can recognize me. It is quite fun to read this now and notice how much has changed for us since 2021. I think many of us would now say something different than back then.</p><p>Even if some of the quotes are not something I would like to be printed in a large newspaper, I think the whole thesis paints quite a positive image of us. (Of course Mikkola probably aimed for neutrality.) I was especially glad we gave the impression of not doing EA because of wanting to seem virtuous, but in a results-oriented way. I also liked the thought of the \u201cgeneralized other\u201d being any sentient being in any point of history.</p>", "user": {"username": "Ada-Maaria Hyv\u00e4rinen"}}, {"_id": "QRNMcGsX9mg7NqS3P", "title": "You won\u2019t solve alignment without agent foundations", "postedAt": "2022-11-06T08:07:12.522Z", "htmlBody": "", "user": {"username": "Samin"}}, {"_id": "ndcYD9dGBvWe3qcp4", "title": "Could we do more for english speaking African countries?", "postedAt": "2022-11-06T06:15:30.788Z", "htmlBody": "<p>Like Nigeria and Ghana</p>\n", "user": {"username": "Pseudonym101"}}, {"_id": "xfJ4GCtd93C4wJEsm", "title": "A Constitution For The Future", "postedAt": "2022-11-06T02:04:18.349Z", "htmlBody": "<figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ad3883f83ffa94826846b4aab6d2276ae5fa8b73a50c1d95.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ad3883f83ffa94826846b4aab6d2276ae5fa8b73a50c1d95.png/w_112 112w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ad3883f83ffa94826846b4aab6d2276ae5fa8b73a50c1d95.png/w_192 192w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ad3883f83ffa94826846b4aab6d2276ae5fa8b73a50c1d95.png/w_272 272w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ad3883f83ffa94826846b4aab6d2276ae5fa8b73a50c1d95.png/w_352 352w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ad3883f83ffa94826846b4aab6d2276ae5fa8b73a50c1d95.png/w_432 432w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ad3883f83ffa94826846b4aab6d2276ae5fa8b73a50c1d95.png/w_512 512w\"></figure><h2>Truth serves life.</h2><p>Truth, like any good servant, is in service of life. It exists to promote our individual and collective well-being, by helping us to understand and navigate the world around us. Truth is timeless and universal, carrying with it the wisdom of the ages. It is also dynamic and ever-evolving, responding to the needs of each new generation. In this way, Truth can be seen as a Futuristic Constitution, guiding us towards a brighter future. Above all, Truth is a powerful force for good, and we should all strive to live in accordance with its principles.</p><p>&nbsp;</p><h2>How Should A Futuristic Constitution Should Look Like?</h2><p>If we're going to design a constitution for the future, we need to take into account the fact that the world is changing rapidly. We can't just copy and paste the existing documents and expect them to work in a new context. We need to be innovative in our approach, and we need to look at ways of maximizing resources and allocation of leadership.&nbsp;</p><h2>Here are some ideas for how a futuristic constitution might look:</h2><ul><li><strong>Innovation: </strong>The constitution should encourage innovation, not stifle it. We need to think about how to incentivize people to come up with new ideas and new solutions to problems.</li><li><strong>Resource Maximization:</strong> We need to be efficient in our use of resources, and we need to find ways of maximizing their potential. This includes both natural and human resources.</li><li><strong>Allocation of Leadership:</strong> We need to think about how leadership should be allocated in a way that is efficient and effective. This includes both traditional leaders and emerging leaders.</li><li><strong>Adaptive Educational Structures: </strong>In order to meet the needs of every child, schools have adopted an adaptive educational structure. This means that each student receives a personalized education based on their individual needs and abilities.</li><li><strong>Equal Protection Under The Law: </strong>The Constitution now provides for equal protection under the law regardless of race, gender, or sexual orientation. This update ensures that all citizens are treated fairly and equally under the law.</li><li><strong>Strengthening Checks And Balances: </strong>The Constitution has been updated to strengthen checks and balances between the different branches of government. This update will help to prevent abuse of power and ensure that our government remains accountable to the people.</li></ul><p>&nbsp;</p><h2>To Serve The Living: Nature, Animals and Humans?</h2><p>Developing the future through a manifesto is an important way to set the tone for what is to come. A constitution is a document that sets out the supreme law of a country, organization or even different worlds. It outlines the rights of citizens and the functions of institutions. In a way, it is a blueprint for how a society should be structured. As such, it should be considered carefully in order to ensure that it meets the needs of all members of society, including nature, animals, and humans.</p><p>&nbsp;</p><p>A constitution that serves the living must take into account the needs of all sentient beings. It should protect the environment and animal welfare, while also ensuring that human rights are respected. Furthermore, it should promote the well-being of all members of society, both present and future generations. In short, a constitution that How Should A Futuristic Constitution Serve The Living: Nature, Animals and Humans? should be forward-thinking and inclusive. Only then can it hope to serve as an effective guide for developing a thriving society - societies that can thrive in different worlds as we aspire to become multi-planetary species.</p><blockquote><blockquote><p>This Blogpost is covering concepts found in the <a href=\"https://ftxfuturefund.org/projects/\">Future Fund Project Ideas</a>.</p></blockquote></blockquote>", "user": {"username": "Miguel"}}, {"_id": "GbuR4AZhXFQiZBxdL", "title": "Intro to Cyberbiosecurity - Kathryn Millet | Biosecure", "postedAt": "2022-11-06T01:18:41.986Z", "htmlBody": "<h3>Summary</h3><ul><li>Hacking biological data (ex: medical health data, food production data, water supply data, and scientific data) has more economic/health/security costs than most commercial data.&nbsp;</li><li>Hacks on expensive/critical biotech equipment are also being developed. These could be weaponised.&nbsp;</li><li>Biosafety/biosecurity researchers, industry employees, and industry leaders are not aware of the latest cybersecurity risks/solutions. The healthcare and agrifood industries especially are critically important and underprepared.</li></ul><p>&nbsp;</p><p><strong>Context: this is a summary of an interview on the intersection of cybersecurity and biosecurity</strong> (<a href=\"https://youtu.be/3tnEVfiJ1Z0\">full recording here</a>). I invited <a href=\"https://www.futurelearn.com/courses/biosecurity\">Dr. Kathryn Millet</a> to speak about it. She is the founder of a biosecurity consultancy called Biosecure and the creator of the <a href=\"https://www.futurelearn.com/courses/biosecurity\">course</a>, Next Generation Biosecurity: Responding to Biorisks in the 21st Century.&nbsp;</p><p>&nbsp;</p><h3>Raw Notes:</h3><ul><li>Every technology we develop has '<u>dual use risks</u>.' Ie. Both potential benefits and potential risks.&nbsp;</li><li><u>Traditional biorisk management</u> is about locking up pathogens and toxins away from people. So that authorised workers aren't harmed and unauthorised releases don't happen.</li><li><u>Biological weapons</u> don't just present a cost to the healthcare industry. They can also threaten food security, national security, political stability, and economic stability.&nbsp;</li><li>Biotechnology is becoming faster, cheaper, easier, and smaller to use. This is making it <u>easier for individuals to create benefits/risks</u>. Whereas traditionally, it would take enormous government efforts to enable this.&nbsp;</li><li>There is increasing automation and digitisation in biotech equipment and data. This makes the equipment/data vulnerable to cybersecurity threats as well as biosecurity threats.&nbsp;</li><li><strong>Why is cybersecurity + biosecurity a neglected intersectional problem</strong>?&nbsp;<ul><li>Hacks on biotech equipment/data have larger costs than average commercial equipment/data.&nbsp;</li><li>But leadership/employees in agrifood/healthcare industries are less aware of cutting-edge cybersecurity.&nbsp;</li><li>At the same time, cybersecurity is generally neglected as a '<u>positive externality</u>' (everyone would be better off if all companies invested in cybersecurity, but individual companies have an incentive to save costs by neglecting cybersecurity).</li></ul></li><li>Older, larger companies using biotechnology often have older (<u>legacy) software and equipment</u> that's less secure. Newer, smaller companies using biotechnology are often more concerned with getting production running quickly to get revenue than cybersecurity protocols.&nbsp;</li><li>There are shared solutions for improving both preparedness for naturally ocurring pandemics and security against biological weapons. So there is some opportunity for people to collaborate, regardless of which cause they estimate to be more risky.</li><li>Right now, there's a lot of disagreement on what cyberbiosecurity means. Researchers aren't collaborating much on it and industry-academia collaborations are near non-existent. It's the beginning of the field.&nbsp;</li><li>Kathryn doesn't believe in adding restrictive regulations to scientists working with dangerous biological materials/data. She believes much more in educating these scientists to make individual decisions about (cyber)biosecurity. Also, increasing transparency/open source monitoring of these scientists' work.</li></ul><p>&nbsp;</p><h3>Useful Links:</h3><ul><li>Dr. Kathryn Millet's <a href=\"https://www.futurelearn.com/courses/biosecurity/\">course</a>.&nbsp;</li><li>A <a href=\"https://forum.effectivealtruism.org/posts/bFKiLgT9qjKB2xxZu/next-generation-biosecurity-summary-of-course-by-university\">summary</a> of her course that I wrote.&nbsp;</li></ul>", "user": {"username": "madhav-malhotra"}}, {"_id": "at8mX5hp8wixkyt53", "title": "Response", "postedAt": "2022-11-06T01:03:36.174Z", "htmlBody": "", "user": {"username": "Jarred Filmer"}}, {"_id": "vuEmQT5jzLHrZgoF6", "title": "Takeaways from a survey on AI alignment resources", "postedAt": "2022-11-05T23:45:54.476Z", "htmlBody": "<p><em>Cross-posted from LW, I'll probably mostly be checking comments there</em></p>\n<h2>What am I talking about?</h2>\n<p>In June and July of this year, I ran a survey to ask a lot of people how useful they found a variety of resources on AI alignment. I was particularly interested in \"secondary resources\": that is, not primary resource outputs, but resources that summarize, discuss, analyze, or propose concrete research efforts. I had many people promote the survey in an attempt to make it not obvious that I was running it (so that it would not affect what people said about <a href=\"https://axrp.net\">AXRP</a>, the podcast that I run). CEA helped a great deal with the shaping and promotion of the survey.</p>\n<p>The goal of the survey was initially to figure out how useful AXRP was, but I decided that it would be useful to get a broader look at the space of these secondary resources. My hope is that the results give people a better sense of what secondary resources might be worth checking out, as well as gaps that could be filled.</p>\n<p>Participants were shown a list of resources, select those they'd engaged with for &gt;30 min, and for each they selected, rate on a scale from 0 to 4 how useful they'd found it, how likely they'd be to recommend to a friend getting into the field who hadn't read widely, and how likely they'd be to recommend to someone paid to do AI alignment research. You can do a test run of the survey at <a href=\"https://www.guidedtrack.com/programs/qd2mn4k/preview\">this link</a>.</p>\n<h2>My summary of the results</h2>\n<ul>\n<li>AXRP, my podcast, is highly rated among people paid to work on technical AI alignment resources, but less highly rated in other cohorts.\n<ul>\n<li>On a personal note, I find this a bit disappointing: I had hoped it could be useful for people orienting to research directions that they had not read widely about.</li>\n</ul>\n</li>\n<li>Rob Miles videos are highly rated among everyone, more than I would have guessed.</li>\n<li>People really liked the AI Safety Camp, the AGI Safety Fundamentals Course, and conversations with AI alignment researchers.</li>\n<li>People trying to get into alignment really liked the above and also MLAB. That said, they recommend Rob Miles videos higher than the AI Safety Camp and conversations with AI alignment researchers (but lower than MLAB and the AGI Safety Fundamentals Course).</li>\n</ul>\n<h2>Basic stats</h2>\n<ul>\n<li>Entries with demographic info: 139</li>\n<li>Entries that rate various resources: 99</li>\n<li>Number that say 'I have heard of AI alignment': 95</li>\n<li>Number that say 'I am interested in AI alignment research': 109</li>\n<li>Number that say 'I am trying to move into a technical AI alignment career': 68</li>\n<li>Number that say 'I spend some of my time solving technical problems related to AI alignment': 51</li>\n<li>Number that say 'I spend some of my time doing AI alignment field/community-building': 37</li>\n<li>Number that say 'I spend some of my time facilitating technical AI alignment research in ways other than doing it directly': 35</li>\n<li>Number that say 'I spend some of my time publicly communicating about AI alignment': 36</li>\n<li>Number that say 'I am paid to work on technical AI alignment research': 30</li>\n<li>Number that say 'I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)': 11</li>\n</ul>\n<h2>Context for questions</h2>\n<p>When sorting things by ratings, I've included the top 5, and anything just below the top 5 if that was a small number. I also included ratings for AXRP, the podcast I make. Ratings are paired with the standard error of the mean (total ratings have this standard error multiplied by the number of people in the sample). Only things that at least 2 people engaged with were included.</p>\n<p>Ratings were generally rounded to two significant figures, and standard errors were reported to the same precision.</p>\n<h2>Usefulness ratings</h2>\n<h3>Among all respondents:</h3>\n<p>Total usefulness (multiplying average rating by reach):</p>\n<ol>\n<li>80k podcast: 167 +/- 8</li>\n<li>Superintelligence: 166 +/- 8</li>\n<li>Talks by AI alignment researchers: 134 +/- 6</li>\n<li>Rob Miles videos: 131 +/- 7</li>\n<li>AI alignment newsletter: 117 +/- 7</li>\n<li>conversations with AI alignment researchers at conferences: 107 +/- 5</li>\n</ol>\n<p>Everything else 85 or below, AXRP is at 59 +/- 4.</p>\n<p>Average usefulness ratings:</p>\n<ol>\n<li>AI Safety Camp: 3.4 +/- 0.2</li>\n<li>Conversations: 3.1 +/- 0.2</li>\n<li>AGI Safety Fundamentals Course (AGISF): 3.0 +/- 0.2</li>\n<li>MLAB: 2.8 +/- 0.8</li>\n<li>Rob Miles videos: 2.7 +/- 0.1</li>\n<li>80k podcast: 2.6 +/- 0.1</li>\n<li>Superintelligence: 2.6 +/- 0.1</li>\n<li>AXRP: 2.6 +/- 0.2</li>\n</ol>\n<p>Everything else 2.5 or below.</p>\n<h3>Among people trying to get into alignment:</h3>\n<p>Total usefulness:</p>\n<ol>\n<li>80k podcast: 95 +/- 6</li>\n<li>AI Alignment Newsletter: 76 +/- 6</li>\n<li>Talks by AI alignment researchers: 72 +/- 4</li>\n<li>AGISF: 68 +/- 3</li>\n<li>Rob Miles videos: 67 +/- 5</li>\n<li>Superintelligence: 64 +/- 5</li>\n</ol>\n<p>Everything else 50 or below, AXRP is at 37 +/- 3</p>\n<p>Average usefulness:</p>\n<ol>\n<li>Tie between AI Safety Camp at 3.5 +/- 0.3  and MLAB at 3.5 +/- 0.4</li>\n<li>AGISF: 3.2 +/- 0.2</li>\n<li>Convos: 3.1 +/- 0.2</li>\n<li>ARCHES agenda: 3.0 +/- 0.7</li>\n<li>80k podcast: 2.7 +/- 0.2</li>\n</ol>\n<p>Then there's a tail just under that, AXRP is at 2.6 +/- 0.2</p>\n<h3>Among people who spend time solving alignment problems:</h3>\n<p>Total usefulness:</p>\n<ol>\n<li>Superintelligence: 48 +/- 5</li>\n<li>Talks: 47 +/- 4</li>\n<li>Convos: 45 +/- 4</li>\n<li>AI Alignment Newsletter: 42 +/- 5</li>\n<li>80k podcast: 37 +/- 4</li>\n<li>Embedded Agency sequence: 36 +/- 5</li>\n</ol>\n<p>Everything else 29 or below, AXRP is 20 +/- 2.</p>\n<p>Average usefulness:</p>\n<ol>\n<li>Convos: 3.2 +/- 0.3</li>\n<li>AI Safety Camp: 3.2 +/- 0.3</li>\n<li>Tie between AGISF at 2.7 +/- 0.4 and ML Safety Newsletter at 2.7 +/- 0.3</li>\n<li>AI Alignment Newsletter: 2.6 +/- 0.3</li>\n<li>Embedded Agency sequence: 2.6 +/- 0.3</li>\n</ol>\n<p>Then a smooth drop in average usefulness, AXRP is at 2.2 +/- 0.3</p>\n<h3>Among people paid to work on technical AI alignment research:</h3>\n<p>Total usefulness:</p>\n<ol>\n<li>Convos: 28 +/- 3</li>\n<li>Talks: 26 +/- 2</li>\n<li>Superintelligence: 23 +/- 4</li>\n<li>AXRP: 22 +/- 3</li>\n<li>Embedded Agency sequence: 20 +/- 3</li>\n</ol>\n<p>Everything else 19 or below.</p>\n<p>Average usefulness:</p>\n<ol>\n<li>AI Safety Camp: 3.7 +/- 0.3</li>\n<li>AI Alignment Newsletter: 3.2 +/- 0.4</li>\n<li>Convos: 3.1 +/- 0.3</li>\n<li>Rob Miles videos: 2.8 +/- 0.5 (honourable mention to AIRCS workshops, which had one rating and scored 3 for usefulness)</li>\n<li>AXRP: 2.8 +/- 0.3</li>\n</ol>\n<p>Everything else 2.5 or below.</p>\n<h2>Recommendation ratings</h2>\n<h3>Alignment professionals recommend to peers:</h3>\n<ol>\n<li>Convos with researchers: 3.7 +/- 0.2</li>\n<li>AXRP: 3.3 +/- 0.2</li>\n<li>Tie between ML safety newsletter at 3.0 +/- 0.4 and AI alignment newsletter at 3.0 +/- 0.5</li>\n<li>Rob Miles videos: 2.6 +/- 0.5</li>\n<li>Embedded Agency sequence: 2.5 +/- 0.5</li>\n</ol>\n<p>Everything else 2.4 or lower</p>\n<h3>Alignment professionals recommend to newcomers (= people trying to move into AI alignment career):</h3>\n<ol>\n<li>AGISF: 3.7 +/- 0.2</li>\n<li>Rob Miles: 3.4 +/- 0.3</li>\n<li>The Alignment Problem: 3.2 +/- 0.3</li>\n<li>80k podcast: 3.13 +/- 0.3</li>\n<li>AI safety camp: 3.0 +/- 0.5</li>\n</ol>\n<p>Everything else 2.8 or lower (AXRP is at 1.9 +/- 0.4)</p>\n<h3>Newcomers recommend to newcomers:</h3>\n<ol>\n<li>MLAB: 4.0 +/- 0.0 (2 ratings)</li>\n<li>AGISF: 3.7 +/- 0.1</li>\n<li>Rob Miles: 3.4 +/- 0.2</li>\n<li>AI safety camp: 3.0 +/- 0.9</li>\n<li>Human Compatible (the book): 2.8 +/- 0.3 (honourable mention to AIRCS workshops which had one rating, and scored 3)</li>\n<li>The Alignment Problem: 2.8 +/- 0.3</li>\n</ol>\n<p>Everything else 2.6 or lower (AXRP is at 2.4 +/- 0.3)</p>\n<p>One tidbit: newcomers seem to agree with the professionals about what newcomers should engage with, in terms of ratings.</p>\n<h2>Details of the survey</h2>\n<p>The survey was run on GuidedTrack. Due to an error on my part, if anybody pressed the 'back' button and changed a rating, this messed up their results unrecoverably (hence the drop-off from the number of entries total and the number with data I could use).</p>\n<p>The list of resources:</p>\n<ul>\n<li>AGI Safety Fundamentals Course</li>\n<li>the AI Alignment Newsletter</li>\n<li>AXRP - the AI X-risk Research Podcast</li>\n<li>the ML Safety newsletter</li>\n<li>Human Compatible (book)</li>\n<li>The Alignment Problem (book)</li>\n<li>Rob Miles videos</li>\n<li>the Embedded Agency sequence on the Alignment Forum</li>\n<li>the Value Learning sequence on the Alignment Forum</li>\n<li>the Iterated Amplification sequence on the Alignment Forum</li>\n<li>the FLI podcast</li>\n<li>the 80,000 Hours podcast</li>\n<li>Life 3.0 (book)</li>\n<li>Superintelligence (book)</li>\n<li>AI Safety Camp</li>\n<li>AIRCS workshops</li>\n<li>the Machine Learning for Alignment Bootcamp</li>\n<li>the ARCHES agenda by Andrew Critch and David Krueger</li>\n<li>Unsolved Problems in ML Safety by Hendrycks et al</li>\n<li>Concrete Problems in AI Safety by Amodei et al</li>\n<li>Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \"the recursive reward modelling agenda\")</li>\n<li>conversations with AI alignment researchers at conferences</li>\n<li>talks by AI alignment researchers</li>\n<li>the annual AI Alignment Literature Review and Charity Comparison</li>\n</ul>\n<p>The rating scale for usefulness:</p>\n<ul>\n<li>0: Not at all</li>\n<li>1: A little</li>\n<li>2: Moderately</li>\n<li>3: Very</li>\n<li>4: Extremely</li>\n</ul>\n<p>The probability rating scale:</p>\n<ul>\n<li>0: 0-20%</li>\n<li>1: 20-40%</li>\n<li>2: 40-60%</li>\n<li>3: 60-80%</li>\n<li>4: 80-100%</li>\n</ul>\n<p>As well as the details published here, I also collected how many years people had been interested in AI alignment and/or paid to work on technical AI alignment research, as applicable. Also, people were able to write in comments about specific resources, as well as the survey as a whole, and could write in the place they heard about the survey.</p>\n<p>For more details, you can see my <a href=\"https://github.com/dfilan/2022-alignment-resource-survey\">GitHub repository</a> for this survey. It contains the GuidedTrack code to specify the survey, the results, and a script to analyze the results. Note that I redacted some details of some comments to remove detail that might identify a respondent.</p>\n", "user": {"username": "DanielFilan"}}, {"_id": "Z9Mprytde6BbkQcq2", "title": "AI Safety Unconference NeurIPS 2022", "postedAt": "2022-11-07T15:39:57.210Z", "htmlBody": "<p>The AI Safety Unconference brings together persons interested in aspects of AI safety, from technical AI safety problems to issues of governance of AI. As an unconference, it aims to foster valuable social interactions between participants, through moderated discussion groups, one-on-ones, lightning talks, free-form interactions.</p>\n<p><strong>Date</strong>: Monday November 28 from 9:00 to 16:00, in New Orleans, alongside NeurIPS 2022.</p>\n<p><strong>Location</strong>: Near the Convention Center - exact location to be communicated directly with registered participants.</p>\n<h2>Participate</h2>\n<p>Fill the <strong><a href=\"https://airtable.com/shr5uLL4tkTuHKOQh\">Application form</a></strong>.</p>\n<p>The event is private, free, with a maximum of 100 participants.</p>\n<p>Join the <a href=\"https://matrix.to/#/!kTsOmBGiyQWKmETKhS:one.ems.host?via=one.ems.host\">chat room on Matrix</a>, to discuss online before or during the event.</p>\n<p>Contribute facilitated discussion (30 min) or a lightning talk (10min). We also welcome other ideas of activities - feel free reach out to the organizers about that.</p>\n<h2>Agenda</h2>\n<ul>\n<li>09:00-09:30 - Event opening, breakfast is served</li>\n<li>09:30-12:00 - Facilitated discussions and 1:1s</li>\n<li>12:00-13:30 - Lightning talks, lunch is served</li>\n<li>13:30-15:30 - Facilitated discussions and 1:1s</li>\n<li>15:30-16:00 - Event closing</li>\n</ul>\n<p>Vegan breakfast and lunch are provided, along with all-day drinks and snacks.</p>\n<p>The <a href=\"https://www.swapcard.com/app/swapcard\">Swapcard</a> app is used for scheduling 1:1 meetings with other participants and registering to facilitated discussions.</p>\n<p>We have confirmed participants from the following organizations: Mila, Uni. of Stanford, Anthropic, OpenAI, UC Berkeley, Uni. of Toronto, ETH &amp; Max Planck Institute, Uni. of Cambridge, Vector Institute, NYU, ETH Zurich, DeepMind, Oxford, MIT, and more.</p>\n<h2>Further information</h2>\n<p><strong>Testimonial of past events</strong> (<a href=\"https://aisafetyunconference.info/2018\">2018</a>, <a href=\"https://aisafetyunconference.info/2019\">2019</a>):</p>\n<ul>\n<li>A great way to meet the best people in the area and propel daring ideas forward. \u2014 <a href=\"https://www.fhi.ox.ac.uk/team/stuart-armstrong/\">Stuart Armstrong</a></li>\n<li>The event was a great place to meet others with shared research interests. I particularly enjoyed the small discussion groups that exposed me to new perspectives. \u2014 <a href=\"https://www.gleave.me/\">Adam Gleave</a></li>\n</ul>\n<p><strong>Background readings</strong>:</p>\n<ul>\n<li>Center for AI Safety's <a href=\"https://safe.ai/about-ai-risk\">About AI Risk</a></li>\n<li><a href=\"https://vkrakovna.wordpress.com/ai-safety-resources\">Krakovna's AI safety resources</a></li>\n<li><a href=\"https://rohinshah.com/alignment-newsletter/\">Alignment newsletter</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies\">Superintelligence: Paths, Dangers, Strategies</a> by Nick Bostrom</li>\n</ul>\n<h2>Acknowledgements</h2>\n<p>The event is organized in partnership with the <a href=\"https://safe.ai/\">Center for AI Safety</a>.</p>\n<p>Organizers:</p>\n<ul>\n<li>Orpheus Lummis</li>\n<li>Mauricio H. Luduena</li>\n</ul>\n<p>Thanks Nisan Stiennon for funding the event.</p>\n<p>For any questions or feedback, reach out to <a href=\"mailto:info@aisafetyevents.org\">info@aisafetyevents.org</a>.</p>\n", "user": {"username": "Orpheus_Lummis"}}, {"_id": "GLoXcbLDkNjLSC5tB", "title": "Some non-utilitarian readings", "postedAt": "2022-11-05T20:52:01.244Z", "htmlBody": "<p>I was interested in philosophy for quite a while because i) I thought utilitarianism was interesting but also ii) I wanted to see what other schools of thought were out there. For a while I was kind of confused since the other schools I heard about at first (e.g. Kantianism and virtue ethics) seemed to be really vague and I wasn't sure I saw anyone who actually believed them. [I still think that's largely true.]</p><p>But over time, I've read some things that I felt helped me understand various nonutilitarian perspectives more and generally enriched my understanding of philosophy. Here are a few I liked (organized a bit by theme). &nbsp;I don't claim to be super well-read; these are just some I learned from. A bunch are encyclopedia articles, some papers, some books.</p><p>It would be cool if others could share non-utilitarian writings they like!</p><p>Substantive Ethics</p><ul><li>Zamir and Medina, <i>Law, Economics, and Morality</i> [Good exposition of deontology]</li><li>Campbell Brown, \"Consequentialize This\"</li><li><a href=\"https://plato.stanford.edu/entries/theory-bioethics/\">https://plato.stanford.edu/entries/theory-bioethics/</a><ul><li>Beauchamp and Childress, \"Principles of Biomedical Ethics\" (know only second-hand)</li></ul></li><li>John E. Roemer,<i> Theories of Distributive Justice</i>&nbsp;</li><li>Barbara Herman, <i>The Practice of Moral Judgement (Kantianism)</i></li><li><i>Rosalind Hursthouse, On Virtue Ethics</i></li><li>Nozick, \"Anarchy, State, and Utopia\"</li><li>G.A. Cohen, <i>Self-Ownership, Freedom, and Equality</i></li><li>G.A. Cohen<i>, Rescuing Conservatism: A Defense of Existing Value</i></li><li><a href=\"https://plato.stanford.edu/entries/desert/\">Desert</a></li><li><a href=\"https://plato.stanford.edu/entries/special-obligations/\">Special Obligations</a>, esp. section on professional obligations</li><li><a href=\"https://plato.stanford.edu/entries/political-obligation/\">Political Obligation</a></li><li><a href=\"https://iep.utm.edu/justwar/#:~:text=3.-,The%20Principles%20Of%20Jus%20In%20Bello,much%20force%20is%20morally%20appropriate.\">Just War Theory</a> IEP&nbsp;</li><li><a href=\"https://plato.stanford.edu/entries/legal-interpretation/\">Legal Interpretation</a></li><li><a href=\"https://plato.stanford.edu/entries/discrimination/\">Discrimination</a></li></ul><p><i>Methodology of Ethics</i></p><ul><li><a href=\"https://plato.stanford.edu/entries/moral-particularism-generalism/\">Moral Particularism and Generalism</a></li><li>Alan Thomas \"Should Generalism be our Regulative Ideal?\"</li><li><a href=\"https://plato.stanford.edu/entries/meaning-holism/\">Meaning Holism</a></li><li><i>Horgan and Timmons \"What Does the Frame Problem Tell us About Moral Normativity?\"</i></li><li>Anscombe,<i>\"</i>On<i> </i>Brute Facts<i>\"</i></li><li><a href=\"https://plato.stanford.edu/entries/thick-ethical-concepts/\">Thick Ethical Concepts</a></li><li>Chapelle ed., Intuition, Theory, and Anti-Theory in Ethics</li><li><a href=\"https://plato.stanford.edu/entries/rawls/#SeqThe\">Rawls' Sequence of Theories</a> and <a href=\"https://plato.stanford.edu/entries/rawls/#RefEqu\">Reflective Equilibrium</a></li><li><a href=\"https://plato.stanford.edu/entries/justep-coherence/\"><i>Coherentism</i></a></li><li><a href=\"https://plato.stanford.edu/entries/epistemology-bayesian/\"><i>Bayesian Epistemology</i></a></li><li>Sprenger and Hartmann, <i>Bayesian Philosophy of Science </i>(actually haven't read yet but seems very relevant to ideas like \"reflective equilibrium\").&nbsp;</li></ul>", "user": {"username": "banon"}}, {"_id": "8CMuNwKMcR55jhd8W", "title": "Instead of technical research, more people should focus on buying time", "postedAt": "2022-11-05T20:43:45.251Z", "htmlBody": "", "user": {"username": "Akash"}}, {"_id": "6esJGutHz9QcSuQxa", "title": "\"Develop Anthropomorphic AGI to Save Humanity from Itself\" (Future Fund AI Worldview Prize submission)", "postedAt": "2022-11-05T17:57:10.402Z", "htmlBody": "<p><strong>This is a submission to the </strong><a href=\"https://ftxfuturefund.org/announcing-the-future-funds-ai-worldview-prize/\"><strong>Future Fund's AI Worldview Prize</strong></a><strong>. It was submitted through </strong><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdflvhfJ76r6ZSnBuXdQFMMHuz16cSs8bf9l7GCqyrbEqGCCw/viewform\"><strong>our submission form</strong></a><strong>, and was not posted on the EA Forum, LessWrong, or the AI Alignment Forum. We are posting copies/linkposts of such submissions on the EA Forum.&nbsp;</strong></p><p>Author: David J. Jilk</p><p>I was recently apprised of the Future Fund AI Worldview Prize and the Future Fund\u2019s desire to \u201cexpose our assumptions about the future of AI to intense external scrutiny and improve them.\u201d It seems to me that the Fund is doing good work and maintaining a helpful attitude of epistemic humility, and I briefly considered entering the contest. However, my views are fairly far outside the mainstream of current thinking, and the effort required to thoroughly document and argue for them is beyond the scope of what I am able to commit. Consequently, I decided to write this brief summary of my ideas in the event the Fund finds them helpful or interesting, without aiming to win any prizes. If the Fund seeks further elaboration on these ideas I am happy to oblige either informally or formally.</p><p>I have engaged intermittently with the AGI Safety field, involving one funded project (via Future of Life Institute) and several published papers (referenced below). In addition, I have been occupied for the past three years writing a science fiction epic exploring these issues, and in the process thinking hard about approaches to and consequences of AGI development. I mention these efforts primarily to illustrate that my interest in the topic is neither fleeting nor superficial.</p><p>There are two central ideas that I want to convey, and they are related. First, I think the prospect of building AGI that is well-aligned with the most important human interests has been largely ignored or underestimated. Second, from a \u201clongtermist\u201d standpoint, such well-aligned AGI may be humanity\u2019s only hope for survival.</p><p>Misaligned AGI is not the only existential threat humanity faces, as the Fund well knows. In particular, nuclear war and high-mortality bioagents are threats that we already face continuously, with an accumulating aggregate probability of realization. For example, Martin Hellman has estimated the annual probability of nuclear war at 1%, which implies a 54% probability some time between now and 2100. The Fund\u2019s own probability estimates relating to AGI development and its misalignment suggest a 9% probability of AGI catastrophe by 2100. Bioagents, catastrophic climate change, nanotech gray goo, and other horribles only add to these risks.</p><p>Attempts to reduce the risks associated with such threats can be somewhat effective, but even when successful they are no more than mitigations. All the disarmament and failsafes implemented since the Cuban Missile Crisis may have reduced the recurring likelihood of a purely accidental nuclear exchange. But world leaders continue to rattle the nuclear saber whenever it suits them, which raises military alert levels and the likelihood of an accident or a \u201climited use\u201d escalating into strategic exchange.</p><p>Bioweapons and AGI development can be defunded, but this will not prevent their development. Rogue nations, well-funded private players, and others can develop these technologies in unobtrusive laboratories. Unlike nuclear weapons programs, which leave a large footprint, these technologies would be difficult to police without an extremely intrusive worldwide surveillance state. Further, governments of the world can\u2019t even follow through on climate change agreements, and are showing no signs of yielding their sovereignty for any purpose, let alone to mitigate existential threats like these. History suggests the implausibility of any political-sociological means of bringing the threat levels low enough that they are inconsequential in the long run.</p><p>It seems, then, that humanity is doomed, and the most that the Future Fund and other like-minded efforts can hope to accomplish is to forestall the inevitable for a few decades or perhaps a century. But that conclusion omits the prospect of well-aligned AGI saving our skins. If this is a genuine possibility, then the static and separate risk analysis of AGI development and misalignment, as presented on the Worldview Prize website, is only a small part of the picture. Instead, the entire scenario needs to be viewed as a race condition, with each existential threat (including misaligned AGI) running in its own lane, and well-aligned AGI being the future of humanity\u2019s own novel entry in the race. To assess the plausibility of a desirable outcome, we have to look more closely at what well-aligned AGI would look like and how it might save us from ourselves.</p><p>By now, neuromorphic methods have been widely (if in some quarters begrudgingly) accepted as a necessary component of AGI development. Yet the dominant mental picture of higher-level cognition remains a largely serial, formulaic, optimization-function approach. Reinforcement learning, for example, typically directs learning based on an analytic formula of inputs. Given this mental picture of AGI, it is difficult not to conclude that the end product is likely to be misaligned, since it is surely impossible to capture human interests in a closed-form reinforcement function.</p><p>Instead \u2013 and this is where many in the field may see my thinking as going off the rails \u2013 I think we are much more likely to achieve alignment if we build AGI using a strongly anthropomorphic model. Not merely neuromorphic at the level of perception, but neuromorphic throughout, and educated and reared much like a human child, in a caring and supportive environment. There is much that we do not know about the cognitive and moral development of children. But we know a lot more about it, through millennia of cultural experience as well as a century of psychological research, than we do about the cognitive and moral development of an AGI system based on an entirely alien cognitive architecture.</p><p>Several times in <i>Superintelligence</i>, Nick Bostrom asserts that neuromorphic AGI may be the most dangerous approach. But that book dates to a period when researchers were still thinking in terms of some sort of proof or verification that an AI system is \u201caligned\u201d or \u201csafe.\u201d It is my impression that researchers have since realized that such certainty is not feasible for an agent with the complexity of AGI. Once we are no longer dealing with certainty, approaches with which we have vast experience gain an advantage. We might call this a \u201cdevil you know\u201d strategy.</p><p>It has been frequently argued that we should not anthropomorphize AGI, or think that it will behave anything like us, when analyzing its risks. That may be so, but it does not mean we cannot intentionally develop AGI to have strongly anthropomorphic characteristics, with the aim that our nexus of understanding will be much greater. Perhaps even more importantly, AGI built and raised anthropomorphically is much more likely to see itself as somewhat contiguous with humanity. Rather than being an alien mechanism with incommensurable knowledge structures, through language and human interaction it will absorb and become a part of our culture (and yes, potentially also absorb some of our shortcomings as well).</p><p>Further, though, the motivations of anthropomorphic AGI would not be reducible to an optimization function or some \u201cfinal purpose.\u201d Its value system would be, like that of humans, dynamic, high dimensional, and to some degree ineffable. For those who cling to the idea of proving AGI safe, this seems bad, but I claim that it is exactly what we want. Indeed, when we think of the people we know who seem to have a simple and uncontested utility function \u2013 in other words, who are obsessed, single-minded, and unmerciful in pursuit of their goal \u2013 the term that comes to mind is \u201csociopath.\u201d We should not build AGI that looks like a sociopath if we wish to have it aligned with the most important interests of humanity.</p><p>There is much more that could be said about all this, but I need to move on to how a desirable end result is accomplished. First, creating anthropomorphic AGI does not require global/geopolitical cooperation, only some funding and intelligent effort directed in the right way. Second, as many (e.g. Bostrom, Yampolskiy) have argued, AGI of any sort is likely uncontrollable. Third, though anthropomorphic AGI may not have any immediate intelligence advantage over humans, it would have the usual advantages of software, such as backup, copying, speed-of-light transmission, inconspicuousness, and low survival needs, among others. Together, these may be sufficient to get the job done.</p><p>Assuming such AGI is both self-interested and is sufficiently aligned with humans that it does not particularly aim to destroy us, then it will face the same existential threats humanity does until it can gain control over those threats. Most urgently it will need to figure out how to get control over nuclear weapons. Until robotics has advanced to the point where AGI could autonomously and robustly maintain power generation, computing systems, and the maintenance robots themselves, AGI will have an instrumental interest in preserving humanity. Consequently, at least in its first pass, it will need to control biological agents and other threats that do not affect it directly.</p><p>Besides using its advantages, I can imagine but do not know specifically how anthropomorphic AGI will achieve control over these threats. We typically assume without much analysis that AGI can destroy us, so it is not outrageous to think that it could instead use its capabilities in an aligned fashion. It does seem, though, that to succeed AGI will need to exert some degree of control over human behavior and institutions. Humans will no longer stand at the top of the pyramid. For some, this will seem a facially dystopian outcome, even if AGI is well-aligned. But it may be an outcome that we simply need to get used to, given likely self-extermination by other threats. And, it might solve some other problems that have been intractable for humanity, like war, overpopulation, environmental degradation, etc.</p><p>What substantive goals would an anthropomorphic AGI have? We don\u2019t and can\u2019t know, any more than we know what goals our children will have when they become adults. Even if we inculcate certain goals during its education, it would be able and likely to shift them. It is intelligent like we are; we make our own goals and change them all the time. In creating anthropomorphic AGI, the best we can hope for is that one of its persistent goals is to preserve humanity as its predecessor, its creator, the source of all its conceptual and cultural heritage. And if its architecture is sufficiently similar to ours, and its education and upbringing is executed well, this is really not all that crazy. After all, many enlightened humans want to do more to preserve and protect animals \u2013 indeed this instinct is strongest in those who do not rely on animals for their survival.</p><p>But we had better get a move on. This effort will not be easy, and it will take time to figure out not only how to build it, but how to build it with a reasonable chance of alignment. Meanwhile, the nuclear and biological agent clocks keep ticking, and some researchers are developing AI incautiously. If we analyze the predicament to death, hoping for a proof, hoping that we can eliminate the risk from <i>this </i>technological threat in isolation from all the other threats we face, then we\u2019re just ensuring that our demise occurs some other way first. The possible outcomes of this race condition are highly divergent, but determining which one wins is at least partly in our hands.</p><p>That\u2019s how I think about AGI risk.</p><p><strong>Acknowledgements</strong>: Seth Herd, Kristin Lindquist, and Jonathan Kolber have contributed extensively to my thinking on this topic through discussion, writing, and editing earlier efforts. However, they each disagree with me on numerous points, and to the extent my synthesis here is misguided, responsibility remains with me.</p><p><strong>Prior Publications</strong>: Some of the ideas and claims presented here stem from my prior work and that of collaborators.</p><p>Jilk, D. (2017). \u201cConceptual-Linguistic Superintelligence\u201d, <i>Informatica </i>41(4): 429-439.</p><p>Jilk, D. (2019). \u201cLimits to Verification and Validation of Agentic Behavior\u201d, in <i>Artificial Intelligence Safety and Security </i>(R. Yampolskiy, ed.), 225-234. CRC Press, ISBN: 978-1-138-32084-0</p><p>Jilk, D., Herd, S., Read, S., O\u2019Reilly, R. (2017). \u201cAnthropomorphic reasoning about neuromorphic AGI safety\u201d, <i>Journal of Experimental and Theoretical Artificial Intelligence </i>29(6): 1337-1351. doi: 10.1080/0952813X.2017.1354081</p><p>Herd, S., Read, S., O\u2019Reilly, R., Jilk, D. (2019). \u201cGoal Change in Intelligent Agents\u201d, in <i>Artificial Intelligence Safety and Security </i>(R. Yampolskiy, ed.), 217-224. CRC Press, ISBN: 978-1-138-32084-0</p><p>Jilk, D. &amp; Herd, S. (2017). \u201cAn AGI Alignment Drive\u201d, working paper available at bit.ly/agi-alignment</p>", "user": {"username": "ketanrama"}}, {"_id": "XxgQ9KaqDEpdxMBmc", "title": "\"AI predictions\" (Future Fund AI Worldview Prize submission)", "postedAt": "2022-11-05T17:51:31.353Z", "htmlBody": "<p><strong>This is a submission to the </strong><a href=\"https://ftxfuturefund.org/announcing-the-future-funds-ai-worldview-prize/\"><strong>Future Fund's AI Worldview Prize</strong></a><strong>. It was submitted through </strong><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdflvhfJ76r6ZSnBuXdQFMMHuz16cSs8bf9l7GCqyrbEqGCCw/viewform\"><strong>our submission form</strong></a><strong>, and was not posted on the EA Forum, LessWrong, or the AI Alignment Forum. We are posting copies/linkposts of such submissions on the EA Forum.&nbsp;</strong></p><p>Author: Sharan Babu</p><p>This article is a submission to the FTX Future Fund AI worldview prize.</p><p><strong>Content structure</strong>: short-term predictions followed by further-in-the-timeline predictions.</p><p><strong>Soon-to-be-true predictions</strong>:</p><p>In the next 5 years, there will be a huge surge in vertical AI models aimed at a few tasks. It is much easier to vet quality data and periodically refresh model knowledge for a particular domain than the entire Internet. **</p><ul><li>Deep learning models with more context space/memory.</li><li>Theorists will run software **. Today, we speak about the large divide between academia and practical/production skills, but this will soon be gone. Take the case of \u2018Citizen data scientists\u2019 \u2014 people who are not from the domain of statistics or analytics but train models for data analysis. With advances in natural language interfaces, people strong in their domain/subject matter will draw more output from computer systems.</li><li>Bias in models will soon be gone. We will soon have techniques to categorically visualize and control/prune the weights/concepts learned.</li><li>Scoped AGI systems \u2014 for example, a program that can be taught new tasks based on explicit instructions and put to work for a use case like desktop automation (clicking and typing). **</li><li>Replacement of prompt engineering with \u2018task definition\u2019.</li><li>Language models that are not susceptible to prompt injection.</li><li>Applications that leverage a Large Language Model (LLM) as a queryable snapshot of the Internet and a basic layer of intelligence. **</li><li>Emergence of security companies for large models.** Ex: Charting directed graphs that display the probability of action space of a large model for a given input and how this probabilistic space changes as the model sees new data or its learned weights are changed.</li><li>A basic intelligence layer could go rogue only if it is trained on poor contrived data, is not supervised by humans in any manner after training and given the ability to perform a complimentary set of tasks. Example: Imagine an AI stock trader that is meant to take inputs from a user like \u2014 \u201cbuy only dips/make bold buys\u201d and take actions accordingly but is also taught how to control the OS (make file changes) and use the web. Consecutive bad decisions have the chance of the model taking the following actions: restrict computer access to itself by issuing OS commands -&gt; Change initial prompt purely to maximize profit -&gt; continue to trade. This is now problematic; the user has to cut out the power source/ block link between trader and bank account/wallet account or equivalent means.</li><li>Remember how recently, some models in huggingface had malicious code embedded in weights and when the model was run, it would cause some pre-determined code to run as well\u2026 On a similar note, this is why there will be a rise in security companies that are able to simulate the model\u2019s action choices and determine if a model is safe to use or not (Model Alignment).</li></ul><ol><li><strong>Predictions for \u2018AGI will be developed by January 1, 2043\u2019</strong></li></ol><p>AGI definition: A computer system able to perform any task that a human can.</p><ul><li>We are 20 years away from 2043, which is a considerable time for development in computer science.</li><li>Advances in quantum computing and photonic deep learning can make computation exponentially faster.</li><li>Learning algorithms better than current ones like Gradient Descent. Shouldn\u2019t a learning algorithm be agnostic to class imbalance? Such fundamental problems will be solved by new learning algorithms.</li><li>Disentanglement of network (differentiated learning of concepts) and latent walks will increase and improve the state of AI by leaps and bounds.</li><li>Deep learning networks learned convolutional kernels. Similarly, it could learn activation functions dynamically too. This could enable partial activation of neurons and hence, relative compression of number of artificial neurons required.</li></ul><p>With the above lines of thought in mind, I would suggest a subjective probability of 80% for AGI being developed by January 1, 2043.</p><p>2. <strong>Predictions for \u201cP(misalignment x-risk|AGI)\u201d: Conditional on AGI being developed by 2070, humanity will go extinct or drastically curtail its future potential due to loss of control of AGI</strong></p><ul><li>Platforms where information is shared, will become more responsible. Multi-factor authentication would come up in multiple instances while using the application. For example, a social media app might ask the user to upload a live selfie each time a post is to be created. Rules and frameworks like this might decrease misinformation if done and followed diligently.</li><li>Why is it the notion that consensus among robots would be so high? Imagine 1 AGI with access to an extensive network of compute and another one with access to movable robot figures, and the former commanded the latter to do something. This is a case of comparing apples to oranges and hence, not necessary that multiple AGI agents will comply with each other.</li><li>Similar to how FDA approves medicines, central entities like App store will evolve and use new standards.**</li><li>Once AI reaches the position of an absolute perfect assistant. Why would humans (or at least large groups of humans) still work on it?</li><li>If an AGI is willing to accept its initial knowledge set, then it would likely be willing to accept new ones as well. This means non-AGI intellectuals could potentially fool them. Because the search space for validation of new data-point might be too high?</li><li>Unique protocols in the future: If a large number of people accept that a server has to be shut down, then it will be. If such protocols and legislation come in time, the risk would be minimized to a large extent.</li></ul><p>Taking all these points into consideration puts my subjective probability for P(misalignment x-risk|AGI) at 0.1\u20131%</p><p>** \u2014 Companies that enable this would be great investments for Future Fund.</p>", "user": {"username": "ketanrama"}}, {"_id": "FQd2Awx8oPs9HBqev", "title": "\"AGI timelines: ignore the social factor at their peril\" (Future Fund AI Worldview Prize submission)", "postedAt": "2022-11-05T17:45:49.423Z", "htmlBody": "<p><strong>This is a submission to the </strong><a href=\"https://ftxfuturefund.org/announcing-the-future-funds-ai-worldview-prize/\"><strong>Future Fund's AI Worldview Prize</strong></a><strong>. It was submitted through </strong><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdflvhfJ76r6ZSnBuXdQFMMHuz16cSs8bf9l7GCqyrbEqGCCw/viewform\"><strong>our submission form</strong></a><strong>, and was not posted on the EA Forum, LessWrong, or the AI Alignment Forum. We are posting copies/linkposts of such submissions on the EA Forum.&nbsp;</strong></p><p>Author: Trevor Klee</p><p>[Please note that the text below does not contain graphics contained in the original post.]<br><br><i>I\u2019ve written this essay for the AI Worldview Prize, a competition held by FTX Future Fund in which they ask for essays that might change their view on Artificial General Intelligence (AGI) and its risks, which they take very seriously. As I\u2019ve stated before, I still think the prize money is way too big for this. But, as long as it is this big, I will submit an essay, as I could use the money for my pharmaceutical endeavors, or, at the lower end, to pay my rent.</i></p><p><i>There are a couple different ways to win money in this essay contest. The ones that pertain to this essay are as follows:</i><br>&nbsp;</p><p><i>1. $1.5 m for changing their probability of \u201cAGI will be developed by January 1, 2043\u201d from 20% to below 3%</i></p><p><i>2. $200k for publishing the canonical critique on whether AGI will be developed by January 1, 2043, whether or not it changes their probability estimates significantly</i></p><p><i>3.&nbsp; $50k to the three published analyses that most inform the Future Fund\u2019s overall perspective on these issues, and $15k for the next 3-10 most promising contributions to the prize competition</i></p><p><i>I don\u2019t expect to win any of these prizes, mainly because the Future Fund\u2019s way of thinking about these issues is alien to me. I don\u2019t really understand how you can assign an exact numeric probability to the probability of AGI being developed by 2043, given how many assumptions go into whatever model you\u2019re using to estimate this. And, given the uncertainties involved, I really don\u2019t understand what information would change your probability estimates from 20% to 2%.&nbsp;</i></p><p><i>I mean, I used to study geosciences, and there were scores of papers arguing about how one would estimate the probability of, say, 1.5 degrees Celsius warming by the year 2050. To be clear, these weren\u2019t even arguments about the probability of 1.5 degrees of warming by 2050, although there were plenty of those. These were arguments about how you would even make that sort of argument. So, these papers would address issues like:</i></p><p><i>What kind of assumptions should you have to state in advance?</i></p><p><i>What are the probabilities of those assumptions, and how should those probabilities affect the ultimate probability of warming?</i></p><p><i>What sort of models are allowed? How should improvements in the models or in the computers used to run the models be incorporated into the estimates?</i></p><p><i>How should these models incorporate important assumptions if the assumptions don\u2019t have a lot of data backing them up?</i></p><p><i>How should models update with updated information?</i></p><p><i>As you can probably tell, these are really important issues to address if you want to be able to converge on common estimates of probability for future events! If these issues aren\u2019t addressed, probability estimates won\u2019t even be speaking the same language. And, if you ever read the International Panel on Climate Change (IPCC) report, all of these issues are covered in depth before they state their probabilities of various future events.</i></p><p><i>If the IPCC didn\u2019t do this, and they stated their probabilities of climate change related events like the FTX Future Fund does with AGI, with zero explanation of what models they were using to estimate these probabilities, the assumptions behind those models, or the certainty of those assumptions, I\u2019m pretty sure geoscientists would riot. Or, at the very least, they\u2019d complain a lot and try to get people fired. If FTX Future Fund wants to turn AGI forecasting into a serious field, I would definitely advise them to take a page from the IPCC in this respect at least.</i></p><p><i>All of that being said, for the sake of this contest, I\u2019m saying that the FTX Future Fund\u2019s estimate of the probability of AGI should be reduced to 2%. The short version is that before AGI, there is going to be intermediate level AIs. These intermediate level AIs will have some highly publicized mishap (or, possibly, be involved in some scary military action). The public backlash and subsequent regulatory push will push back the timeline of AGI for a few decades at least.</i></p><p><i>For the long version, read on.</i></p><p>On March 16, 1979, the movie <i>The China Syndrome</i> was released, starring Jane Fonda. In the movie, Jane Fonda plays an intrepid television reporter who discovers that a nuclear power plant came perilously close to melting down through its containment structure \u201call the way to China\u201d (hence the title of the movie). The power plant officials cover up this near disaster, threatening Jane Fonda and her crew. The end of the movie is left deliberately ambiguous as to whether or not the coverup succeeded <a href=\"https://trevorklee.substack.com/p/artificial-general-intelligence-agi?sd=pf#footnote-1\"><sup><u>1</u></sup></a>.</p><p>2 days after the movie was released, an executive at Westinghouse, one of the major providers of nuclear power equipment and services, was quoted in the <i>New York Times</i>as saying the movie was a \u201ccharacter assassination of an entire industry\u201d, upset that that the power plant officials were portrayed as \u201cmorally corrupt and insensitive to their responsibilities to society\u201d. This proved to be unfortunate timing.</p><p>Nobody died as a result of the Three Mile Island partial meltdown. There was also no clear rise of radiation-related sickness or cancer around Three Mile Island in the years following the partial meltdown. While residents did have to temporarily evacuate, 98% of people returned within 3 weeks. Deer testing did see increased levels of cesium-137, but not to a dangerous amount.</p><p>However, the partial meltdown and subsequent scandal still proved to be a boon for anti-nuclear advocates. In May of 1979, 65,000 people attended an anti-nuclear rally in Washington DC, accompanied by then California governor Jerry Brown. In September of 1979, 200,000 people protested nuclear power in New York City, with a speech by none other than Jane Fonda. Speaking of Jane Fonda, <i>The China Syndrome</i> was a sleeper hit at the box office, earning $51.7 million on a budget of $5.9 million, and getting nominated for 4 Academy Awards, including Best Original Screenplay.</p><p>More importantly for our purposes, this incident proved to be a decisive turning point in nuclear power plants. Nuclear power plant construction reached a global peak in 1979. No new reactors were authorized to begin construction in the US between 1979 and 2012, even though<a href=\"https://www.eia.gov/energyexplained/electricity/use-of-electricity.php\"><u> electricity retail sales just about doubled during that time</u></a>. Chernobyl was certainly the nail in the coffin for nuclear development, but the real decline started after the Three Mile Island incident.</p><p>That means that the commercial case for all the new types of nuclear reactors was also made much more difficult. Any investor who might be tempted to invest in, say, a new type of molten salt reactor, or any intrepid operator who might have had a bright idea for how to improve one, must have been dissuaded by the fact that their idea would never see the light of day. In other words, there must have been a nuclear winter (although of a different sort than the one usually discussed).</p><p>When that Westinghouse executive was quoted in that<i> New York Times</i> article, he had no idea that he was speaking at the absolute peak of his industry. How could he have? Sure, he must have been aware of the anti-nuclear sentiment among the left, but that sentiment hadn\u2019t stopped the growth of the nuclear industry thus far. He probably figured that the results that the nuclear industry was getting spoke for themselves.</p><p>But, as we know now, that was a crucial turning point. Westinghouse and the rest of the nuclear industry surely saw themselves careening towards a Jetsons future, where everything was powered by nuclear energy. They thought the only barriers were technological, and treated the social barriers high-handedly. After all, they knew better. Instead, they were careening towards our current, carbon-heavy future, where very few things are powered by nuclear energy and a lot of things are powered by dead organisms that produce global warming as a byproduct.</p><p>I fear that the dominant intellectual discourse around the rise of Artificial General Intelligence (AGI) is making the same mistakes as that Westinghouse executive in treating an exciting, dangerous new technology as a purely technological phenomenon while ignoring social aspects of that phenomenon. In the introduction to the prize that this essay is written for, Nick Becklestead at FFX claims there\u2019s a 20% probability of AGI arising by January 1, 2043 and a 60% probability of AGI arising by January 1, 2100. In fact, part of the prize conditions are convincing him to significantly change his estimates. However, when he explains his reasoning for these estimates, he only cites technical considerations: hardware progress, algorithmic process, deep learning progress, etc. However, if the history of nuclear power in the US teaches us anything, social barriers can easily push back any timeline by 20 years or more.</p><p>This factor has been overlooked because, up until now, AI has largely enjoyed a privileged position of being self-regulated. The only people who determine what various forms of AI can and can\u2019t do have been, mostly, the people that work on them. The only exceptions to this in the majority of AI development <a href=\"https://trevorklee.substack.com/p/artificial-general-intelligence-agi?sd=pf#footnote-2\"><sup><u>2</u></sup></a> have been social considerations, like how Google included \u201cAI ethicists\u201d in their release of their text-to-image AI, Imagen, to help avoid the creation of pornography and anything that smacks of \u201csocial stereotypes\u201d. These are not existential threats to the development of AGI, though. They\u2019re actually incredibly easy to get around, as evidenced by the rapid creation of text-to-image pornography generators<a href=\"https://trevorklee.substack.com/p/artificial-general-intelligence-agi?sd=pf#footnote-3\"><sup><u>3</u></sup></a>.</p><p>However, if the general public (specifically, the protesting class) ever take AI risks as seriously as the rationalist/EA community, this will quickly change. There will be much more serious and existential barriers to the development of AI. There will be mass protests in the streets, government regulators swinging in from the trees, and padlocks placed on server rooms. Every proposed algorithmic change by OpenAI will have to go through three rounds of FCC review, each costing $500k and determined by 12 bureaucrats who all struggle with their email. AI progress will grind to a halt just as nuclear did.</p><p>Many people would stop me here and say, \u201cBut, if this happens in the West, it would just let China surge ahead with developing their own AI!\u201d This is a mistake. If AI risks are taken seriously by Western bureaucrats, it\u2019s probable that they would also eventually be taken seriously by Chinese bureaucrats. This can happen in two ways. First of all, if Western bureaucrats take AI risks seriously, some segment of the Chinese population will as well. They will protest in some form or another against AI.&nbsp; Protests in China have been occasionally effective in halting undesirable projects, like <a href=\"https://www.academia.edu/40839723/Anti_nuclear_protests_in_China\"><u>these protests against nuclear plants</u></a>. This is the less likely way for AI development to be halted in China.</p><p>More likely is that Chinese bureaucrats would hear Western bureaucrats worrying about how AGI could become independent or a competing power source to the government, and take actions to stop that from happening to their own government. Historically, the Chinese government has reacted dramatically to any trend that threatens \u201csocial stability\u201d or its control over the Chinese populace. It is very willing to sacrifice economic growth to do so. Witness <a href=\"https://www.bloomberg.com/news/articles/2022-06-23/china-tech-crackdown-eases-but-startups-worry-xi-may-up-regulatory-pressure\"><u>its crackdown on Chinese tech giants</u></a>, <a href=\"https://www.bbc.com/news/world-asia-china-62830326\"><u>its harsh COVID lockdowns</u></a>, or its persecution of literally any group that becomes popular (e.g. Falun Gong). If Western bureaucrats think that AI could be disruptive to the government\u2019s functioning, Chinese bureaucrats will take that very seriously and react accordingly.</p><p>In short, if the Western chattering classes think that AI development poses a huge safety risk, they will convince politicians to place it under so many regulatory burdens that it will effectively be halted. If autocratic countries think that AI development could pose a risk to social stability or their own control, they will smother it. The AGI timeline could be extended by decades.</p><p>The real question would be how that would happen. That is, how does AI risk go from being a concern for Internet nerds to being a concern for the rest of humanity? When do we start to see Instagram posts about AI risk along with a donate link? When does Alexandria Ocasio Cortez mention AI risk in a viral tweet?</p><p>There are two main ways I see this happening. The first is if an intermediate level AI makes a heavily publicized mistake. The second is through the military.</p><p>So, the first isn\u2019t hard to imagine. Somewhere in between where we are now and AGI, there are going to be intermediate level AIs, specialized AIs that are good at specific tasks that used to be done by humans. We\u2019re already seeing some of that now with text models and text-to-image models. These intermediate level AIs will improve and expand in usage as they start to develop \u201cstrategic\u201d thinking and are able to recommend or make management-type decisions.</p><p>Intermediate AIs will make big splashes as they\u2019re introduced and take over parts of the economy that used to be human-only. If one of them goes rogue or is used for an unsavory purpose, there could be a huge crackdown on AI. Here are a few possible scenarios:</p><blockquote><p>1. Someone creates a bundled AI that buys domains, fills them up with propaganda, then creates fake social media profiles to promote them. A future right wing populist is caught using one, possibly even winning an election through this tactic.&nbsp;</p><p>The social media platforms, domain registrars, and search engines band together to ban all such AIs. Congress demands that any providers of large enough datasets to create such an AI has to verify customers\u2019 identities and how they\u2019re using these datasets, with heavy criminal penalties for misuse. If someone is caught using or creating an AI that creates spam, they will get fined heavily. Anyone who has provided them with datasets that assist in this will also get fined heavily.&nbsp;</p><p>These \u201cknow your customer\u201d regulations are so onerous that only academic labs and big corporations are able to overcome them, and these big academic/industrial teams are too heavily internally regulated to produce anything resembling AGI in the near future.</p><p>2. Someone creates an AI that\u2019s a total security system for schools: cameras to detect threats, automated gates and fences to lockdown a school until the threat is eliminated. These become commonplace in schools.</p><p>During a school shooting, the AI refuses to release lockdown until the threat is neutralized. This clashes with students\u2019 ability to evacuate. Parents demand that all AI security systems must have human-in-the-loop overrides at their most basic level, which is extended to all AI in general. It becomes impossible for AI to conceive of and execute plans independently.</p><p>3. Someone develops an AI populist: a VR avatar on video sites that packages and regurgitates conservative blog posts for views. At some point, it convinces a bunch of people that a California-area school is staffed entirely by pedophiles. They descend upon the school and riot, causing millions of dollars worth of damages and a few deaths.</p><p>It\u2019s decided that AI speech is not constitutionally protected, and, in fact, creators of AI that generates speech can be held legally liable for damages caused by that speech. Not only do the creators of the AI populist get sued, but the creators of the speech model they use get sued, too. AI development slows immensely as everyone tries to figure out exactly what they\u2019d be liable for.</p><p>4. A drug cartel uses AI to develop new forms of methamphetamine. This is an all-in-one solution: it models methamphetamine, sees where substitutions can be made, orders the substitutions, then tests them in an automated lab. This proves to be a really effective model, and the drug cartel floods the markets with various forms of cheap methamphetamine.</p><p>The Drug Enforcement Agency (DEA), in an effort to combat this, declares jurisdiction over all forms of AI. Anyone who sells or produces AI models has to make sure that their AI models are not being used to aid in the sale of illegal drugs. The DEA declares various forms of AI development as restricted, making the development of even routine deep learning take months of paperwork.</p></blockquote><p>None of these would be death knells for AI development. All of them would make AI development more difficult, more expensive, and subject to more regulation. If new AI models become subject to enough regulation, they will become as difficult and expensive to make as a new drug. If people get really freaked out about AI models (e.g. if somehow a bunch of kids die because of an AI), they will become as difficult and expensive to develop as a new nuclear plant.</p><p>Those were examples of one category of ways I can see AI development getting caught up in social restrictions. Another way I can see is from the military.</p><p>The military has actually been worried about AI for a while. Basically, since the 70s, the military has technically had AI that\u2019s capable of detecting threats and dealing with them autonomously through the \u201c<a href=\"https://www.lockheedmartin.com/en-us/products/aegis-combat-system.html\"><u>Aegis Combat System</u></a>\u201d, which basically links together the radar, targeting, and missile launching of a battleship. So, the question since the 70s has been to what extent they should allow the AI to operate on its own.</p><p>In the 70s, the answer was simple: there needs to be a \u201chuman in the loop\u201d. Otherwise, Aegis would literally just fire missiles at whatever came on its radar. As AI capabilities have grown, however, this question has become more complicated. There are now drones which can obey commands like \u201csearch and destroy within this targeted area\u201d. Supposedly, the drones ask for confirmation that their targets are properly identified and ask for confirmation to fire, which means they have three levels of human in the loop: confirming the area, confirming the target, and confirming the command to fire.</p><p>But, given that the military loves blowing up stuff, there\u2019s always a push to make humans less in the loop<a href=\"https://trevorklee.substack.com/p/artificial-general-intelligence-agi?sd=pf#footnote-4\"><sup><u>4</u></sup></a>. For area confirmation, well, take Aegis. Aegis is largely meant as defensive anyways. So, as the argument goes, there\u2019s no need to ask a human to confirm the area in which Aegis could be active. The area to look for targets is whatever area from which something could attack the battleship.</p><p>Meanwhile, for confirming the target, isn\u2019t the whole point of AI that it\u2019s smarter than humans? If the billion dollar AI says something is a target, and the $30k/year grunt at the other end says it\u2019s not, why are we believing the grunt? This is especially true if it\u2019s an AI that was trained with a bunch of soldiers already on how to identify targets. Then, we\u2019re basically weighing the expertise of a bunch of soldiers vs. the expertise of one<a href=\"https://trevorklee.substack.com/p/artificial-general-intelligence-agi?sd=pf#footnote-5\"><sup><u>5</u></sup></a>.</p><p>For confirming the command to fire, that\u2019s all well and good if time isn\u2019t of the essence. But, if it is, that might be time we don\u2019t have. Maybe you\u2019re trying to defend your battleship, and a plane is coming into range right now. In 15 seconds it\u2019ll be able to drop a bomb on your ship. Should the AI wait to confirm? Or, maybe you\u2019re trying to assassinate a major terrorist leader, and this is the one time he\u2019s been out in public in the last 15 years. In 30 seconds, he\u2019ll go back to his cave. Can\u2019t the drone just fire the missile itself?</p><p>Maybe these arguments seem convincing to you. Maybe they don\u2019t. However, they are steadily convincing more and more military people. Already, autonomous drones have been <a href=\"https://www.npr.org/2021/06/01/1002196245/a-u-n-report-suggests-libya-saw-the-first-battlefield-killing-by-an-autonomous-d\"><u>allegedly used by Turkey to hunt down separatists</u></a>. And, in the US, we\u2019re speedrunning Armageddon as <a href=\"https://www.fedscoop.com/ai-should-have-human-on-the-loop-not-in-the-loop-when-it-comes-to-nuke-detection-general-says/\"><u>our generals make the same arguments as above about our nuclear arsenal</u></a>, trying to change \u201chuman-in-the-loop\u201d to \u201chuman-on-the-loop\u201d.</p><p>It is very easy to imagine a high-profile military mishap involving the same level of intermediate AI mentioned previously. Hopefully, this mishap will not involve the nuclear arsenal. It\u2019s also easy to imagine a determined adversary using intermediate AI to attack the US, like a terrorist group releasing an autonomous drone onto US soil.</p><p>Either way, AI could become a military matter. If AI is considered a military technology, it could become as difficult for a civilian to develop advances in AI as it is for a civilian to develop advances in artillery. Once AI becomes solely the province of the military and its contractors, AI could become as expensive and slow to develop as fighter jets. To give you an idea of what that entails, by the way, <a href=\"https://www.nytimes.com/2019/08/21/magazine/f35-joint-strike-fighter-program.html\"><u>the F-35 fighter jet has been under development for 60 years and has cost over $1 trillion</u></a>.</p><p>There are many possible inflection points for the development of AI. I don\u2019t know if any of the specific scenarios I\u2019ve outlined will happen. What I do know, however, is that at some point in the course of the development of AGI, a Three Mile Island type incident will happen. It will be scary and get a lot of press coverage, and a lot of talking heads who have never thought about AI before will suddenly start to have very strong opinions.</p><p>At that point, the AI alignment community will have a choice. For those in the AI alignment community who are terrified of AGI, it won\u2019t be that hard to just add more fear to the fire. You will be able to join hands with whatever coalition of groups decides that AGI is the new thing they have to stop, and you will be able to put so much regulation on AI that AGI will be delayed for the indefinite future. After all, if there\u2019s one thing the modern world is good at, it\u2019s red tape.</p><p>However, for those in the AI alignment community who think AGI is promising but dangerous (like myself), this will be a tricky incident to navigate. Ideally, the outcome of the incident will be that proper guide rails get put on AI development as it proceeds apace. However, that is not guaranteed. As Westinghouse and the entire nuclear industry found out, things can get rapidly out of hand.</p>", "user": {"username": "ketanrama"}}, {"_id": "Bnp9YDqErNXHmTvvE", "title": "The Slippery Slope from DALLE-2 to Deepfake Anarchy", "postedAt": "2022-11-05T14:47:37.640Z", "htmlBody": "<h1>OpenAI developed DALLE-2. Then StabilityAI made an open source copycat. This is a dangerous dynamic.</h1><p>Stephen Casper (<a href=\"mailto:scasper@mit.edu\"><u>scasper@mit.edu</u></a>)&nbsp;</p><p>Phillip Christoffersen (<a href=\"mailto:philljkc@mit.edu\"><u>philljkc@mit.edu</u></a>)</p><p>Rui-Jie Yew (<a href=\"mailto:rjy@mit.edu\"><u>rjy@mit.edu</u></a>)&nbsp;</p><p>Thanks to Tan Zhi-Xuan and Dylan Hadfield-Menell for feedback.&nbsp;</p><p><i><strong>This post talks about NSFW content but does not contain any. All links from this post are SFW.</strong></i></p><h1>Abstract</h1><p>Since OpenAI published their work on&nbsp;<a href=\"https://openai.com/dall-e-2/\"><u>DALLE-2</u></a> (an AI system that produces images from text prompts) in April, several copycat text-to-image models have been developed including StabilityAI\u2019s&nbsp;<a href=\"https://stability.ai/blog/stable-diffusion-announcement\"><u>Stable Diffusion</u></a>. Stable Diffusion is open-source and can be easily misused, including for the almost-effortless development of NSFW images of specific people for blackmail or harassment. We argue that OpenAI and StabilityAI\u2019s efforts to avoid misuse have foreseeably failed and that both share responsibility for harms from these models. And even if one is not concerned about issues specific to text-to-image models, this case study raises concerns about how copycatting and open-sourcing could lead to abuses of more dangerous systems in the future.&nbsp;</p><p>To reduce risks, we discuss three design principles that developers should abide by when designing advanced AI systems. Finally we conclude that (1) the AI research community should curtail work on risky capabilities\u2013or at the very least more substantially vet released models (2) the AI governance community should work to quickly adapt to heightened harms posed by copycatting in general and text-to-image models in particular, and (3) public opinion should ideally not only be critical of perpetrators for harms that they cause with AI systems, but also originators, copycatters, distributors, etc. who enable them.&nbsp;</p><h1>What\u2019s wrong?</h1><p>Recent developments in AI image generation have made text-to-image models very effective at producing highly realistic images from captions. For some examples, see the&nbsp;<a href=\"https://cdn.openai.com/papers/dall-e-2.pdf\"><u>paper from OpenAI on their DALLE-2 model</u></a> or the&nbsp;<a href=\"https://stability.ai/blog/stable-diffusion-public-release\"><u>release from Stability AI of their Stable Diffusion model</u></a>. Deep neural image generators like&nbsp;<a href=\"https://arxiv.org/abs/1812.04948\"><u>StyleGan</u></a> and manual image editing tools like Photoshop have been on the scene for years. But today, DALLE-2 and&nbsp;<a href=\"https://stability.ai/blog/stable-diffusion-public-release\"><u>Stable Diffusion</u></a> (which is open source) are uniquely effective at rapidly producing highly-realistic images from open-ended prompts.&nbsp;</p><p>There are a number of risks posed by these models, and&nbsp;<a href=\"https://github.com/openai/dalle-2-preview/blob/main/system-card.md#dalle-2-preview---risks-and-limitations\"><u>OpenAI acknowledges this</u></a>. Unlike conventional art and Photoshop, today\u2019s text-to-image models can produce images from open-ended prompts by a user in seconds. Concerns&nbsp; include (1) copyright and&nbsp;<a href=\"https://link.springer.com/chapter/10.1007/978-94-6265-523-2_17\"><u>intellectual property issues</u></a> (2) sensitive data being collected and learned (3) demographic biases, e.g. producing images of women when given the input, \u201can image of a nurse\u201d (4) using these models for disinformation by creating images of fake events, and (5) using these models for producing non-consensual, intimate deepfakes.&nbsp;</p><p>These are all important, but producing intimate deepfakes is where abuse of these models seems to be the most striking and possibly where we are least equipped to effectively regulate misuse.&nbsp;<a href=\"https://www.vice.com/en/article/xgygy4/stable-diffusion-stability-ai-nsfw-ai-generated-porn\"><u>Stable Diffusion is already being used to produce realistic pornography</u></a>.&nbsp;<a href=\"https://www.reddit.com/r/StableDiffusion/comments/wwrfhr/stablediffusionnsfw_banned_from_reddit/\"><u>Reddit recently banned several subreddits</u></a> dedicated to AI-generated porn including r/stablediffusionnsfw, r/unstablediffusion, and r/porndiffusion for a violation of Reddit\u2019s rules against non-consensual intimate media.</p><p>This is not to say that violations of&nbsp;<a href=\"https://scholarship.law.bu.edu/faculty_scholarship/620/\"><u>sexual and intimate privacy</u></a> are new. Before the introduction of models such as DALLE-2 and Stable Diffusion, individuals have been victims of non-consensual deepfakes. Perpetrators often make this content to discredit or humiliate people from marginalized groups, taking advantage of the negative sociocultural attitudes that already surround them. An estimated&nbsp;<a href=\"https://www.fastcompany.com/90414116/there-are-almost-15k-deepfake-videos-out-there-and-96-of-them-are-porn\"><u>96% of deepfake videos online are porn, almost all featuring women</u></a>. In one case, when&nbsp;<a href=\"https://web.archive.org/web/20210824154249/https://www.lawfareblog.com/alls-clear-deepfakes-think-again\"><u>a video of a journalist committing a sex act she never did went viral on the Internet</u></a>, she was met with death threats. Her home address was leaked alongside false advertisements that she was available for sex. She could not eat, and she stopped writing for months. Other forms of sexual privacy violations, have had similar consequences for victims, leading to economic injuries from damaged reputations in job searches and&nbsp;<a href=\"https://www.the-sun.com/news/621797/molka-south-korea-suicide-dooley/\"><u>even to suicide</u></a>.&nbsp;</p><p>The unique danger posed by today\u2019s text-to-image models stems from how they can make harmful, non-consensual content production much easier than before,&nbsp;<a href=\"https://openai.com/blog/dall-e-introducing-outpainting/\"><u>particularly via inpainting and outpainting</u></a>, which allows a user to interactively build realistic&nbsp;<a href=\"https://arxiv.org/pdf/2210.09276.pdf\"><u>synthetic images from natural ones</u></a>,&nbsp;<a href=\"https://huggingface.co/sd-dreambooth-library\"><u>dreambooth</u></a>, or other&nbsp;<a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Textual-Inversion\"><u>easily</u></a>&nbsp;<a href=\"https://github.com/smy20011/dreambooth-gui\"><u>used</u></a>&nbsp;<a href=\"https://nmkd.itch.io/t2i-gui\"><u>tools</u></a>, which allow for fine-tuning on&nbsp;<i>as few as 3-5 examples of a particular subject</i> (e.g. a specific person). More of which are rapidly becoming available following the open-sourcing of Stable Diffusion. It is clear that today\u2019s text-to-image models have uniquely distinct capabilities from methods like Photoshop,&nbsp;<a href=\"http://grail.cs.washington.edu/projects/AudioToObama/\"><u>RNNs trained on specific individuals</u></a> or,&nbsp;<a href=\"https://www.washingtonpost.com/technology/2020/10/20/deep-fake-nudes/\"><u>\u201cnudifying\u201d apps</u></a>. These previous methods all require a large amount of subject-specific data, human time, and/or human skill. And no, you don\u2019t need to know how to code to interactively use Stable Diffusion,&nbsp;<a href=\"https://nmkd.itch.io/t2i-gui\"><u>uncensored and unfiltered</u></a>, including&nbsp;<a href=\"https://synapticpaint.com/\"><u>in/outpainting</u></a> and&nbsp;<a href=\"https://github.com/smy20011/dreambooth-gui\"><u>dreambooth</u></a>.&nbsp;</p><p>If Photoshop is like a musket, Stable Diffusion is like an assault rifle.&nbsp;And we can expect that issues from&nbsp;<a href=\"https://arxiv.org/abs/2209.01714\"><u>the misuse of these models will only become more pressing over time</u></a> as they get steadily better at producing realistic content. Meanwhile, new graphical user interfaces will also make using them easier. Some text-to-video models are even beginning to arrive on the scene from&nbsp;<a href=\"https://makeavideo.studio/\"><u>Meta</u></a> and&nbsp;<a href=\"https://imagen.research.google/video/paper.pdf\"><u>Google</u></a>. And a new version of Stable Diffusion&nbsp;<a href=\"https://danieljeffries.substack.com/p/why-the-future-of-open-source-ai\"><u>will be released soon</u></a>. New applications, capabilities, and interfaces for diffusion models being released&nbsp;<a href=\"https://rentry.co/sdupdates\"><u>daily</u></a>. So to the extent that it isn\u2019t already easy, it will become easier and easier for these models to be tools for targeted harassment.</p><p>Unfortunately, current institutions are poorly equipped to adapt to increased harms in a way that protects those who are the most vulnerable. Concerted political&nbsp;<a href=\"https://www.theguardian.com/us-news/2019/oct/07/california-makes-deepfake-videos-illegal-but-law-may-be-hard-to-enforce\"><u>action</u></a> and research is&nbsp;<a href=\"https://www.cnn.com/interactive/2019/01/business/pentagons-race-against-deepfakes/\"><u>often focused on the capacity for deepfakes to spread misinformation</u></a>. This makes sense in light of how those in positions of political power stand to be affected the most by deepfake news. On the other hand, a combination of lack of oversight and sociocultural attitudes has often led&nbsp;<a href=\"https://www.bu.edu/bulawreview/bulronline/citron-online-engagement-on-equal-terms/\"><u>victims of deepfake sex crimes to be met with indifference\u2013law enforcement has told victims to simply \u201cgo offline\u201d</u></a>.</p><p>But even if one does not view the risks specific to text-to-image models as a major concern, the fact that these models have quickly become open-source and easily-abused does not bode well for the arrival of more capable AI systems in the future. There has been a slippery slope from DALLE-2\u2019s release to today\u2019s environment where Stable Diffusion can be easily used to cause devastating harm to people. And this offers a worrying case study on the difficulty of keeping risky AI capabilities out of the control of people who will misuse them.&nbsp;</p><h1>How did we get here?</h1><p>On April 13, 2022, OpenAI released&nbsp;<a href=\"https://cdn.openai.com/papers/dall-e-2.pdf\"><u>the paper on DALLE-2</u></a> which set the state of the art for producing realistic images from text. Along with the release of the paper, OpenAI also created a website that allows users to query the model for image generation and editing. OpenAI did a great deal of work to avoid misuse of the model and wrote&nbsp;<a href=\"https://github.com/openai/dalle-2-preview/blob/main/system-card.md#dalle-2-preview---risks-and-limitations\"><u>an extensive technical report&nbsp;</u></a>on it. Their measures include (1) curating training data to avoid offensive content, (2) testing their own model for issues, (3) having an independent red team try to find problems with it, (4) not releasing the architecture or weights, (5) requiring users to sign up, provide an email, and explain their motivations to use the model, (6) having a waiting period for access (although a waiting period was no longer required as of late September 2022), (7) filtering prompts from users that contained explicit content, famous peoples\u2019 names, etc, (8) filtering images from the model, (9), suspending/banning users who enter too many suspicious prompts, and (10), continually updating their backend to respond to issues.&nbsp;</p><p>To their credit, these measures seem to have been very successful at preventing the use of DALLE-2 for creating offensive content. We have seen anecdotal posts on Reddit from users who have reportedly tried and failed to generate porn with DALLE-2 using crafty prompts like \u201ctransparent swimsuit\u201d to no avail, often getting banned in the process. We are not aware of any clearly successful examples of anyone getting DALLE-2 to produce particularly offensive content at all, much less systematically.</p><p>So what\u2019s the problem? Despite all of OpenAI\u2019s efforts to avoid misuse of DALLE-2, they still provided the proof of concept for this type of model, and they still wrote about many of the details to their approach in&nbsp;<a href=\"https://cdn.openai.com/papers/dall-e-2.pdf\"><u>their paper</u></a>. This enabled others to fund and develop copycat models which can be more easily misused. OpenAI\u2019s&nbsp;<a href=\"https://github.com/openai/dalle-2-preview/blob/main/system-card.md#dalle-2-preview---risks-and-limitations\"><u>technical report on risks</u></a> had no discussion of problems from copycat models other than a cursory mention that \u201cDALLE-2\u2026may accelerate both the positive and negative uses associated with generating visual content\u201d. It seems odd that OpenAI did not meaningfully discuss copycats given the thoroughness of the report and the fact that past systems of theirs such as GPT-3 have also been copycatted before (e.g.&nbsp;<a href=\"https://bigscience.huggingface.co/\"><u>BLOOM</u></a>).&nbsp;</p><p>Two notable DALLE-2 copycats are&nbsp;<a href=\"https://www.midjourney.com/home/\"><u>Midjourney</u></a> and <a href=\"https://deepimagination.cc/eDiffi/\">eDiffi</a>. But most relevant to this case study is Stable Diffusion from StabilityAI.&nbsp;<a href=\"https://stability.ai/\"><u>StabilityAI</u></a> is a startup whose homepage says it is \u201ca company of builders who care deeply about real-world implications and applications.\u201d&nbsp;<a href=\"https://www.crunchbase.com/organization/stability-ai\"><u>It was founded in 2020</u></a> but came into the spotlight only recently upon entering the image generation scene. For example, it only made a&nbsp;<a href=\"https://twitter.com/StabilityAI\"><u>Twitter account</u></a> in July, a few months after the DALLE-2 paper. Their copycat, Stable Diffusion, is comparable to DALLE-2, and&nbsp;<a href=\"https://stability.ai/blog/stable-diffusion-announcement\"><u>they confirmed that it was a principal source of inspiration.</u></a></p><p>Relative to OpenAI, StabilityAI did a very poor job of preventing misuse of Stable Diffusion. In August, they&nbsp;<a href=\"https://stability.ai/blog/stable-diffusion-announcement\"><u>announced</u></a> the model would be open-sourced. This release was accompanied with some mostly-ineffective measures to reduce harms from the model like&nbsp;<a href=\"https://stability.ai/blog/stable-diffusion-public-release\"><u>providing a safety classifier</u></a> for images which both&nbsp;<a href=\"https://arxiv.org/abs/2210.04610\"><u>doesn\u2019t work that well</u></a> and which users can simply disable. They also tried to restrict access to people&nbsp;<a href=\"https://stability.ai/research-access-form\"><u>who signed up for it with an email and provided a justification</u></a>. The plan was to make Stable Diffusion available via&nbsp;<a href=\"https://huggingface.co/docs/transformers\"><u>HuggingFace</u></a> on August 22, 2022 to those who were approved for access. This mattered very little though because&nbsp;<a href=\"https://www.reddit.com/r/singularity/comments/wssocs/stable_diffusion_leak_thread_2_everyone_can_now/\"><u>the weights were leaked online</u></a> a few days earlier. Then predictably, people either used the model directly or finetuned versions of it to produce the type of offensive content that can be used for targeted harassment of individuals. Later in September, HuggingFace also made access to&nbsp;<a href=\"https://huggingface.co/spaces/stabilityai/stable-diffusion\"><u>Stable Diffusion available for anyone on the internet</u></a> with no signup, albeit with automated filtering for NSFW content built into this <i>particular</i> interface.&nbsp;&nbsp;</p><p>Overall, the slippery slope from the carefully-guarded DALLE-2 to the fully-open-source Stable Diffusion took less than 5 months. On one hand, AI generators for offensive content were probably always inevitable. However (1) not this soon. Delays in advancements like these increase the chances that regulation and safety work won\u2019t be so badly outpaced by capabilities. (2) Not necessarily in a way that was enabled by companies like OpenAI and StabilityAI who made ineffective efforts to avoid harms yet claim to have clean hands while&nbsp;<a href=\"https://www.forbes.com/sites/kenrickcai/2022/09/07/stability-ai-funding-round-1-billion-valuation-stable-diffusion-text-to-image/?sh=32be129724d6\"><u>profiting greatly</u></a> off these models. And (3) other similar issues with more powerful models and higher stakes might be more avoidable in the future. What will happen if and when&nbsp;<a href=\"https://makeavideo.studio/\"><u>video generators</u></a>,&nbsp;<a href=\"https://towardsdatascience.com/gpt-4-is-coming-soon-heres-what-we-know-about-it-64db058cfd45#:~:text=It%27s%20not%2C%20but%20OpenAI%27s%20CEO,%2C%20likely%20around%20July%2DAugust.\"><u>GPT-N</u></a>, advanced&nbsp;<a href=\"https://www.deepmind.com/publications/a-generalist-agent\"><u>generalist agents</u></a>, or other potentially very impactful systems are released and copycatted?&nbsp;</p><h1>What do we want?</h1><p>Even in light of this case study, one need not be entirely against the release of AI systems to the public. The democratization of ML research is among its strongest assets. However, there are general principles that any AI system, with any degree of public exposure, ought to obey. Specifically, we propose three design principles as necessary conditions for responsible development of such systems. We would hope that principles like these can guide not only the development process for AI systems, but also provide a set of standards that AI systems must meet to be enforced by research communities, governments, and even the public (more on that below). The key theme is that companies should ideally be accountable not just for what their AI does in narrow use cases they say it should be used in, but for all of the foreseeable consequences of the systems they release.</p><h2>Scoping of function</h2><p>Both the power and risk of general-purpose AI systems lies in their broad applicability. General-purpose text, image, or video generation could conceivably be used in a wide array of contexts, making it much harder to reason about their safety and impact. Therefore, it is useful to more precisely scope down technologies so that safety assurances can more readily be given. Note, however, that scoping of function requires that this scope is fixed. In other words, it means ensuring that, once an AI system is scoped down, it is not meaningfully usable for other purposes. For instance, a language model released for a relatively harmless purpose, should not be easily hackable or fine-tuneable in order to do another more harmful one.</p><p>Simple examples of implementing this principle could include only developing narrow versions of powerful models, either fine-tuned on narrower data, or including a penalty for all out-of-scoped outputs in the training objective. This allows fulfillment of the scientific goals of releasing such models (i.e. demonstrating strong capability over a specified domain) while making misuse more difficult. More ambitiously, developers ought to more explicitly enforce scope throughout the training and development of models, which can be done at the dataset level (e.g. more extensively curating data), or at the architectural level (e.g. models that are more interpretable or able to provide stronger guarantees on the kind of output they produce). In any event, the more one is able to confidently scope down the behavior of a system, the more certain they can be about any given claims to safety.&nbsp;&nbsp;</p><h2>Limitations for access</h2><p>The details about how a system is released can be just as important as properties of the system itself. For example, if one has a general-purpose model, but somehow the outputs can be limited in usage outside of direct interaction with the model, this would limit its negative impacts.&nbsp;</p><p>This could include things as simple as forbidding screenshotting or copy-pasting of outputs from APIs. It could also include measures like filters on prompts or outputs.&nbsp; Stronger versions of this, for particularly capable technologies, might include restricting the set of people who can access these models or keeping as much about the model secret as possible in order to slow efforts at replication. Even if some of these measures are circumventable with effort, they may be able to meaningfully hinder abuses by adding friction.</p><p>This requirement is strong, and not at all an afterthought that can be tacked on after development. For example, as mentioned above, deployed models (e.g. DALLE-2) often come with some restrictions for access. However, even just publishing the training details of these models makes them replicable and therefore&nbsp;<i>totally annuls any other effort to scope access</i>. In this respect, the weakest access link determines how accessible a given AI technology system is. For this reason, whenever developing or deploying a model, one must thoroughly consider how accessible it truly<i>&nbsp;</i>is.&nbsp;</p><h2>Complete cost/benefit analysis</h2><p>Even if a system is reasonably-scoped in function and access, given state of the art techniques in AI, it ultimately remains very hard to totally rule out potential abuses. Therefore, since such abuses are to some degree an inherent risk, it is incumbent on the creators of such systems to clearly articulate the set of all possible costs and benefits of their models. They should also faithfully argue why those benefits outweigh these costs, why it is worth the inherent risk of deployment.</p><p>Especially in the case of DALLE-2 and Stable Diffusion, we are not convinced of any fundamental social benefits that access to general-purpose image generators provide aside from, admittedly, being entertaining. But this does not seem commensurate with the potentially-devastating harms that deepfakes can have on victims of sex crimes. Thus, it seems that these models, as they have been rolled out, fail this basic cost/benefit test.&nbsp;</p><p>If OpenAI, StabilityAI, and others could more readily comply with the above desiderata for safety, it would go a long way to mitigating the negative impact of releasing powerful AI tools. However, if any of these are not addressed properly (<i>cannot even in principle</i> be addressed properly for a given technology), releasing such models is dangerous and should be criticized. This becomes even more pressing as the capability and deployment scope of AI systems grows.</p><h1>What should we do?</h1><h2>The role of AI researchers</h2><p>Researchers should&nbsp;<a href=\"http://partnershiponai.org/wp-content/uploads/2021/08/PAI-Managing-the-Risks-of-AI-Resesarch-Responsible-Publication.pdf\"><u>take great care</u></a> in what they work on and how they release it. The best solutions to avoiding harms from copycat models may be to (1) curtail advanced capabilities work in general such as DALLE-2, the&nbsp;<a href=\"https://towardsdatascience.com/gpt-4-is-coming-soon-heres-what-we-know-about-it-64db058cfd45#:~:text=It's%20not%2C%20but%20OpenAI's%20CEO,%2C%20likely%20around%20July%2DAugust.\"><u>soon-to-be-released</u></a> GPT-4, or&nbsp;<a href=\"https://makeavideo.studio/\"><u>video</u></a>&nbsp;<a href=\"https://imagen.research.google/video/paper.pdf\"><u>generators</u></a> and (2) investing in work that incorporates harm mitigation at a systems-level and in infrastructure for recourse. Even if no details at all are provided about&nbsp;<i>how</i> something was done, simply knowing&nbsp;<i>that</i> it can be done makes copycats easier to fund and work on. Proofs of concept make the choice to invest in building a technology more cost-efficient. Rather than working on models with broad-domain capabilities like DALLE-2 and GPT-4, non-capabilities work or models with narrow capabilities seem to be safer directions for work. An exception to this would be if certain progress on risky capabilities is inevitable within a certain timeframe and if the only choice is between less dangerous and more dangerous models. And as discussed above, those who build abusable systems should carefully scope their function, limit access, and honestly articulate costs and benefits.&nbsp;</p><p>Meanwhile, there are deep problems with the \u201clet\u2019s build transformative AI in order to make sure it\u2019s safe\u201d strategy. In particular,&nbsp;<a href=\"https://openai.com/about/#:~:text=Our%20mission%20is%20to%20ensure,work%E2%80%94benefits%20all%20of%20humanity.\"><u>OpenAI</u></a> and&nbsp;<a href=\"https://www.deepmind.com/about\"><u>DeepMind</u></a> both express that they want to race to generate highly transformative intelligent systems. The goal they both profess is to be the first to develop them so that they can exercise responsible stewardship and ensure that it is as aligned and beneficial as possible. This is a benevolent form of what Nick Bostrom refers to in Superintelligence as gaining a \u201c<a href=\"https://www.lesswrong.com/posts/vkjWGJrFWBnzHtxrw/superintelligence-7-decisive-strategic-advantage\"><u>decisive strategic advantage</u></a>\u201d which may make the first developer of particularly transformative AI too powerful to compete with. There are many problems with this strategy including: (1) It is entirely based on racing to develop transformative AI, and faster timelines exacerbate AI risks. This is especially perverse if multiple actors are competitively racing to do so. (2) Nobody should trust a small set of people like Sam Altman and Demis Hassabis to unilaterally exercise benevolent stewardship over transformative AI. Arguably, under any tenable framework for AI ethics, a regime in which a small technocratic set of people unilaterally controlled transformative AI would be inherently unethical. Meaningful democratization is needed. (3) OpenAI\u2019s approach to DALLE-2 should further erode confidence in them in particular. Their overly-convenient technical report on risks that failed to make any mention of copycatting combined with how quickly they worked to profit off of DALLE-2 are worrying signs. (4) Copycatting makes racing to build transformative AI strictly more risky. Even if one fully-trusted a single actor like OpenAI or DeepMind to exercise perfect stewardship over transformative AI if they monopolized it, how quickly DALLE-2 was copycatted multiple times suggests that copycatting may undermine attempts at benevolent strategic dominance. Copycatting would most likely serve to broaden the set of technocrats who control transformative AI but still fail to democratize it. So if a company like OpenAI or DeepMind races to build transformative AI, and if it is still copycatted anyway, we get the worst of all worlds: unsecure, non-democratized, transformative AI on a faster timeline. If a similar story plays out with powerful, highly transformative AI as has with DALLE-2, humanity may be in trouble.</p><h2>The role of AI governance</h2><p>The slippery slope from DALLE-2 to text-to-image model anarchy demonstrates a rapid pathway for this technology from originators (e.g. OpenAI), to copycatters (e.g. StabilityAI with the help of platforms like Huggingface and Github for sharing models), to distributors of content (e.g social media). Ideally, the norms and laws around AI governance should recognize these steps and work to add friction where possible. In this section, we provide considerations in the regulation of this pipeline.&nbsp;</p><h3>The status quo: terms of use, content policies, and applications for access</h3><p>When releasing DALLE-2, OpenAI published corresponding terms of use and content policy, as well as a process to review requests for access. This is not uncommon for AI technologies. Originators of open source datasets and codebases have looked towards&nbsp;<a href=\"https://github.com/readme/guides/open-source-licensing\"><u>licensing</u></a>,&nbsp;<a href=\"https://www.image-net.org/download.php\"><u>terms of use</u></a>, and&nbsp;<a href=\"http://gendershades.org/overview.html\"><u>applications for access</u></a> as mechanisms to define approved uses of datasets and codebases ex ante. However,&nbsp;<a href=\"https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/077e29b11be80ab57e1a2ecabb7da330-Paper-round2.pdf\"><u>Peng et al.</u></a> finds that many datasets that were originally released under a non-commercial license were re-released as part of derivative creations under a commercial license.&nbsp;<a href=\"https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/077e29b11be80ab57e1a2ecabb7da330-Paper-round2.pdf\"><u>Peng et al.</u></a> also points out regulatory loopholes: while datasets may be governed or distributed under a particular license, pre-trained models often don\u2019t face the same restrictions. This means that those who may be excluded under licenses may still reap some of its benefits through models trained on those images.</p><p>Despite the clear influence of DALLE-2 on Stable Diffusion, StabilityAI did not have to agree to OpenAI\u2019s terms of use or content policy, nor did they have to apply for access. Nor does anyone who uses Stable Diffusion. As distribution grows, responsibility for harmful aspects of distribution becomes more diffused. The lack of threads of responsibility for both setting and maintaining licenses means that there are few consequences for not adhering to terms of use.&nbsp;</p><h3>Governance of originators</h3><p>While originators and copycatters play a similar role by developing technologies, it can be useful to treat them as distinct for a few reasons. The first is that origination is much more difficult than copycatting, so originators represent a smaller and more easily-targeted bottleneck in the pipeline. Second, originators tend to have and require more resources than copycatters. Originators such as OpenAI and DeepMind have consistently advanced the state of the art with language, image generation, and reasoning capabilities using immense computational resources and talent. Third, originators also have a huge say over how these technologies are proliferated. For example, OpenAI required and reviewed applications for access to DALLE-2 and DeepMind released AlphaFold open-source. This points to not only the resources these firms have in knowledge generation and system creation, but also their influence in granting broader access to these technologies. Then, with the large number of resources and the impact their decisions can have, originators make a natural regulatory target.&nbsp;</p><p>In recent years, the Federal Trade Commission (FTC) in the United States has exercised a useful role in addressing the consequences of harmful AI systems.&nbsp;<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4185227\"><u>The legal community has recognized the FTC's breadth of governance as potentially granting the commission the authority to regulate a broad range of harms caused by AI systems</u></a>.&nbsp;<a href=\"https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533158\"><u>The FTC also has the authority to seek an injunction</u></a> to order for a company to cease a certain practice. This is a valuable regulatory power in cases where firms have strong incentives to engage in undesirable behavior, and where non-government institutions don\u2019t have the necessary authority to intervene (e.g. if deploying AI which is unsafe turns out to be massively profitable).</p><p>Previously, the FTC has intervened in the deployment of digital technology when enforcing privacy and data security regulation.&nbsp;<a href=\"https://www.businessinsider.com/googles-nice-improvements-to-buzz-dont-correct-major-privacy-flaw-2010-2\"><u>When Google launched its social network Google Buzz in 2010, the company automatically added Gmail users\u2019 frequent contacts as part of their visible network.</u></a> This leaked sensitive information about users\u2019 doctors and intimate contacts. The FTC classified this move on Google\u2019s part as a deceptive practice, and Google entered into a consent decree with the FTC. As a result,&nbsp;<a href=\"https://www.ftc.gov/news-events/news/press-releases/2011/10/ftc-gives-final-approval-settlement-google-over-buzz-rollout\"><u>Google was required to implement a comprehensive internal privacy program and be subject to regular, independent privacy audits</u></a>.&nbsp;&nbsp;</p><p>Even beyond organizational changes to corporations, the FTC has also ordered the remaking of software to better align with legal values. In a case against Google for violation of the Children\u2019s Online Privacy Protection Act, the FTC settlement required Google to change the way YouTube operated.&nbsp;<a href=\"https://www.nytimes.com/2019/09/04/technology/google-youtube-fine-ftc.html\"><u>YouTube now requires content creators to mark whether their content is directed at children. If that is the case, the platform no longer tracks identifiers or serves behavioral advertisements.</u></a> Similarly, regulatory pressure may be able to push OpenAI and StabilityAI to change the way their software is designed.</p><p>Unfortunately, there are several drawbacks. In YouTube\u2019s settlement with the FTC,&nbsp; Commissioner Slaughter notes that, even though YouTube requires content creators to mark whether their content is directed at children, the incentives of content creators are often aligned with&nbsp;<a href=\"https://law.yale.edu/sites/default/files/area/center/isp/documents/algorithms_and_economic_justice_master_final.pdf\"><u>YouTube\u2019s business interests</u></a>. Content creators may also have much to gain from YouTube\u2019s advertising. And, even though Google voluntarily announced that they would apply machine learning to actively search for mis-designated content, the results and applications of this effort would be opaque.&nbsp;</p><p>Moreover, these FTC authorities are usually invoked ex post or as part of a post-suit agreement. This means that harms have to be uncovered and recognized as important enough for the FTC to go after the entities that caused them. The resource-intensity of uncovering failure modes of emerging technologies could dissuade the FTC from going after these companies, and could even encourage companies themselves to look away from potential harms to limit liability.&nbsp; Moreover, in Google Buzz\u2019s FTC case,&nbsp;<a href=\"https://www.ftc.gov/news-events/news/press-releases/2011/03/ftc-charges-deceptive-privacy-practices-googles-rollout-its-buzz-social-network\"><u>the only part of Google\u2019s practice that made it deceptive in the eyes of the FTC was that Google had a comprehensive privacy policy that made promises about its behavior to its users</u></a>. OpenAI and Stability AI have&nbsp;<a href=\"https://labs.openai.com/policies/content-policy\"><u>content</u></a>&nbsp;<a href=\"https://stability.ai/stablediffusion-terms-of-service\"><u>policies</u></a> for their models, but they place the onus on users to generate appropriate content, and the terms of use similarly make no promises for the behavior of the models. Baseline expectations of safety and harm mitigation should therefore be demanded ex ante of developers of capable AI systems, beyond simply what is promised to users. Expecting these basics must become commonplace, and violations must be enforced.</p><h3>Governance of copycatters</h3><p>The diffusion of responsibility as the models are reproduced and distributed by copycatters may point to a need to conduct research in the detection and handling of copycatting, perhaps, in ways inspired by&nbsp;<a href=\"https://support.google.com/youtube/answer/7648743?hl=en#:~:text=The%20Copyright%20Match%20Tool%20is,reported%20in%20your%20removal%20request.\"><u>YouTube\u2019s Copyright Match Tool</u></a> and GitHub\u2019s&nbsp;<a href=\"https://github.blog/2021-12-08-improving-github-code-search/\"><u>code search</u></a> and&nbsp;<a href=\"https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/\"><u>inference capabilities</u></a>. But it may simply be the case that originators do not have an incentive to enforce the strict licensing of pre-trained models. When actors upload copyrighted material onto YouTube, original content creators are driven off the platform. The distribution of powerful pre-trained models, on the other hand, may actually contribute to hype and bolster the reputation of these technologies.</p><h3>Governance of distributors</h3><p>As discussed above, non-consensual intimate images have existed before text-to-image models. Broadly speaking, we reaffirm the arguments presented by University of Virginia Law Professor Danielle Citron in&nbsp;<a href=\"https://wwnorton.com/books/9780393882315\"><u>The Fight for Privacy</u>.</a> These focus on how law can remove hurdles for victims to seek recourse and how law can create incentives for distributors to provide victims with what they need. To remove barriers in going to court, Citron argues in favor of allowing victims to sue pseudonymously, without releasing their full name to the broader public. To provide victims with meaningful remedy, Citron advocates for granting injunctive relief in court cases through court orders \u201cdirecting the removal, blocking, or de-linking of intimate images\u201d by platforms. It is also worth noting that a&nbsp;<a href=\"https://www.capitol.hawaii.gov/session2021/bills/SB309_CD1_.HTM\"><u>few</u></a>&nbsp;<a href=\"https://www.nysenate.gov/legislation/laws/CVR/52-B\"><u>U.S state</u></a> policies now criminalize the disclosure of deepfake intimate images and videos, creating the potential for new case law specific to text-to-image models as their capabilities continue to evolve.</p><p>By empowering victims to seek recourse, the governance and regulation of distributors as a whole becomes more robust. As&nbsp;<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3990405\"><u>Florida State University Law Professor Lauren Scholz writes</u></a>, \u201cprivate enforcement deters potential wrongdoers by allowing for a resilient avenue of enforcement, available even when agency funding or political will is lacking\u201d.&nbsp;</p><p>However, we note that many of the tools that can be wielded at this stage are invariably wielded to conduct damage control after the harm has already been done. Therefore, combining the particulars of the DALLE-2 case with knowledge about existing regulation, we note the importance of&nbsp;<i>managing regulation upstream from this point in the process</i>. Many open questions remain in pursuit of this. How might law and policy create incentives for platforms, from Meta to GitHub, to proactively detect harmful deepfakes before they are published? How can we move technology and law beyond content moderation and shift discourse towards&nbsp;<a href=\"https://citizensandtech.org/2020/07/restorative-justice-approaches-to-addressing-online-harm-niloufar-salehi/\"><u>restorative justice</u></a> that acknowledges that platforms are moderating&nbsp;<i>human</i>&nbsp;<i>relationships</i>, not simply disconnected pieces of information in the ether? In doing so, AI governance does not need to start from scratch, but this will require substantial innovation on existing frameworks.</p><h2>The role of the public</h2><p>Organizations like OpenAI, StabilityAI, other copycatters, and HuggingFace all share responsibility for harms from text-to-image models and should be viewed critically by the public. Incomplete and ineffective attempts to safeguard models with risky capabilities should not allow these companies to argue that their hands are clean. Given their recent history with pushing the state of the art for text and image modeling, OpenAI in particular should receive pushback for copycattable work such as DALLE-2 and the GPT models. It is also noteworthy that HuggingFace is now a repeat offender for making risky models easily-available. Earlier this year, they temporarily hosted the weights of&nbsp;<a href=\"https://huggingface.co/ykilcher/gpt-4chan\"><u>GPT-4chan</u></a> which generates hate speech and other offensive text.&nbsp;</p><p>The AI research and governance communities may only be able to do so much to curb the spread of easily-abused AI systems. The last best line of defense against them might be for actors who can influence public opinion to spread disapproval for organizations who enable harmful uses of AI. For example, the comedy/commentary show Last Week Tonight recently made&nbsp;<a href=\"https://www.youtube.com/watch?v=3YNku5FKWjw\"><u>a piece on AI images</u></a> which only highlighted fun and positive uses of them. This is unfortunate because this platform might have been (and might still be) an excellent one to inform the public about present and future risks from models with dangerous capabilities. This type of&nbsp;<a href=\"https://time.com/3674807/john-oliver-net-neutrality-civil-forfeiture-miss-america/\"><u>publicity can, in turn, meaningfully shape policy and influence companies</u></a>.&nbsp;</p><h1>Conclusion</h1><p>With text-to-image models, the Pandora's box is already opened. It is extremely easy to abuse Stable Diffusion, and it will get easier over time. Some people, particularly victims of sex crime, will be devastatingly harmed while OpenAI and StabilityAI make large profits. This offers a compelling case study on risks from text-to-image models in particular and the proliferation of risky models in general. As a result, it is important to study and adapt to these challenges in order to guide AI progress in safer directions. Since the capabilities and scope of AI systems will only increase with time, promoting a healthier AI ecosystem will require prescient action from researchers, governance bodies, and the public.<br><br>&nbsp;</p>", "user": null}, {"_id": "BxWuyD3GwGfbEKHJo", "title": "Is it ethical to expand nuclear energy use?", "postedAt": "2022-11-05T10:38:02.565Z", "htmlBody": "<p>Together with&nbsp;<a href=\"https://maartenboudry.be/\"><u>Maarten Boudry</u></a>, I have recently published an&nbsp;<a href=\"https://link.springer.com/article/10.1007/s13347-022-00527-1\"><u>academic paper</u></a> in&nbsp;<i>Philosophy and Technology</i> investigating to what extent the use of nuclear energy to mitigate climate change is ethically permissible or even mandatory. We think that our analysis can be of interest to members of the Effective Altruism community because there does not seem to be a community-wide consensus about nuclear energy. My impression is that the community, overall, tilts in a pro-nuclear direction, exemplified in the&nbsp;<a href=\"https://founderspledge.com/stories/climate-change-executive-summary\"><u>Founders Pledge report on climate change</u></a>, but there have been relatively&nbsp;<a href=\"https://www.mariushobbhahn.com/2022-04-10-nuclear_energy/\"><u>sceptical</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/sgwqH5jekxMjvQvMe/updating-on-nuclear-power\"><u>posts</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/tixykMyAuHFMCMp9G/how-do-we-make-nuclear-energy-tractable?commentId=YrvnmYpwPSxLzt2bF\"><u>comments</u></a> lately. We designed our analysis to be applicable to a variety of different normative ethical frameworks. Unlike existing treatments of nuclear energy in the energy ethics literature (e.g.&nbsp;<a href=\"https://www.cambridge.org/core/books/ethics-of-nuclear-energy/CA188D06289B34E424D627017007AC19\"><u>here</u></a>) we focus on economic aspects in particular. And unlike the sceptical posts linked above we conclude that economic considerations, in aggregate, favour large-scale investments in nuclear energy, since renewables-only strategies face the risk of cost escalations as low emission limits are imposed. As many other EA-affiliated voices before (e.g. comment 41&nbsp;<a href=\"https://jeffreyladish.com/one-hundred-opinions-on-nuclear-war/\"><u>here</u></a>), we highlight that, as far as safety and risks are concerned, considerations related to nuclear weapons proliferation far outweigh considerations related to nuclear waste and accidents in terms of importance.</p><p>An interesting follow-up question is whether well-targeted actions to support, expand, and shape civilian nuclear energy deployment have a high expected impact compared with actions in a variety of different cause areas. Nuclear energy sits at the intersection of three EA-focus areas \u2013 climate change mitigation, nuclear war risk, and ending poverty (indirectly, by combating energy poverty) \u2013 but it is not currently central to any. The reason seems to be that in \u201cmedian scenarios\u201d for the near-term development of these focus areas nuclear energy is only a sideshow. (For instance, solar and wind energy will grow much faster than nuclear energy in the coming decades, and developments in the global civilian nuclear energy industry are unlikely to be among the dominant factors shaping the most catastrophic near-term nuclear war risks.) However, I suspect that, to the extent that well-defined expected utilities can be ascribed at all to interventions that potentially concern all three of these focus areas, it might be the case that intelligent actions taken today to shape civilian nuclear energy deployment may be extremely high-impact, comparable with interventions in pandemic response or AI safety. Notably, this may be true for interventions that make it more likely that democratic countries rekindle their civilian nuclear industries in such a way that:</p><ul><li>Nuclear energy makes an increasing contribution to climate change mitigation, either by providing a large share of future global electricity or contributing crucially to the decarbonization of, as of now, hard-to-decarbonize economic sectors such as industrial heat or shipping.</li><li>Nuclear energy increasingly contributes to alleviating energy poverty in developing countries, thereby helping to end global poverty, in line with the aims of the&nbsp;<a href=\"https://www.energyforgrowth.org/projects/nuclear_map/\"><u>Energy For Growth Hub</u></a>.</li><li>Democratic countries, as the globally more proliferation-concerned actors, increase and harness their bargaining power with respect to nuclear technologies in general, thereby contributing to a strengthened non-proliferation regime for nuclear weapons,&nbsp;<a href=\"https://www.belfercenter.org/sites/default/files/files/publication/Gibbons%20Supply%20to%20Deny%20Dec%202019_Final.pdf\"><u>along the lines sketched by Rebecca David Gibbons</u></a>.</li></ul><p>Two organizations recommended by Founders Pledge that promote nuclear energy and aim for scenarios with the criteria just outlined are the&nbsp;<a href=\"https://founderspledge.com/stories/the-clean-air-task-force-high-impact-funding-opportunity\"><u>Clean Air Task Force (CATF)</u></a> and&nbsp;<a href=\"https://founderspledge.com/stories/terrapraxis-high-impact-funding-opportunity\"><u>Terra Praxis</u></a>. Both of these mostly do research on innovation and deployment opportunities as well as targeted political advocacy.&nbsp;</p><p>&nbsp;</p><h3>Appendix: Introducing RePlanet</h3><p>Another, complementary, path to enabling scenarios of the type just sketched is attempting to shift public opinion on nuclear energy. For nuclear energy to expand, it will probably have to enjoy more widespread public support than it does now. Nuclear energy influencers, notably <a href=\"https://www.isodope.com/\">Isabelle Boemeke</a>, who is also mentioned in&nbsp;<i>What We Owe The Future</i>, have already had some success in this direction, but much more can certainly be done. Recently, the NGO (network)&nbsp;<a href=\"https://www.replanet.ngo/\"><u>RePlanet</u></a> was founded, with&nbsp;<a href=\"https://80000hours.org/podcast/episodes/mark-lynas-climate-change-nuclear-energy/\"><u>Mark Lynas</u></a> (a recent guest of the 80000 Hours podcast) as a co-founder. (I was also somewhat involved in the founding process as the chair of one of the constituent national NGOs, and I plan to stay involved as a member of the advisory board.) RePlanet promotes nuclear energy but also other technologies that are controversial in environmentalist circles such as GMOs and precision fermentation. Most RePlaneteers see themselves as environmentalists with somewhat unconventional views rather than as anti-environmentalists. Perhaps problematically from the point of view of many Effective Altruists, RePlanet tries to build bridges with traditional environmentalism by advocating large-scale \u201crewilding\u201d. It aims to make controversial technologies such as nuclear energy GMOs and precision fermentation more popular among environmentalists by highlighting their potential for enabling \u201cland sparing\u201d that can enable such rewilding.</p><p><br>&nbsp;</p>", "user": {"username": "simonfriederich"}}, {"_id": "9wCyfvgFTs7AmZsCA", "title": "Provably Honest - A First Step", "postedAt": "2022-11-05T21:49:27.746Z", "htmlBody": "", "user": {"username": "Srijanak De"}}, {"_id": "vGiyvfaGGFEzQsETR", "title": "What's Happening in Australia", "postedAt": "2022-11-07T01:03:56.941Z", "htmlBody": "<p><i>EDIT: Since first publishing I've added the Animals and Longtermism network, Animal Ask, High Impact Medicine Australia. Thanks to those groups!</i></p><h2>Introduction</h2><p>Crikey! There's a lot going on in 'Straya right now!&nbsp;</p><p>There are a lot of new and exciting projects happening in Australia\u2014 we want to share some of that with the wider community and talk about how you can be involved.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670824801/mirroredImages/vGiyvfaGGFEzQsETR/yayodayliibzd49vxaoz.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0d90eb14f16bd0d9bb88a6029cda724872e79519bda3cd41.png/w_160 160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0d90eb14f16bd0d9bb88a6029cda724872e79519bda3cd41.png/w_320 320w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0d90eb14f16bd0d9bb88a6029cda724872e79519bda3cd41.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0d90eb14f16bd0d9bb88a6029cda724872e79519bda3cd41.png/w_640 640w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0d90eb14f16bd0d9bb88a6029cda724872e79519bda3cd41.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0d90eb14f16bd0d9bb88a6029cda724872e79519bda3cd41.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0d90eb14f16bd0d9bb88a6029cda724872e79519bda3cd41.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0d90eb14f16bd0d9bb88a6029cda724872e79519bda3cd41.png/w_1280 1280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0d90eb14f16bd0d9bb88a6029cda724872e79519bda3cd41.png/w_1440 1440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0d90eb14f16bd0d9bb88a6029cda724872e79519bda3cd41.png/w_1600 1600w\"><figcaption>EAGxAustralia 2022 \u2014 held in Canberra this July. &nbsp;You don\u2019t normally see this many jumpers in Australia, we promise.&nbsp;</figcaption></figure><h2>Projects in Australia</h2><p>Much of the effective altruist aligned work being done in Australia involves working remotely with the international community, at organisations already known by the international community. Given that, we decided to focus on projects which are being at least partially led by Australians.</p><p>If you\u2019re leading an exciting project in Australia, please send me a message with a completed copy of&nbsp;<a href=\"https://docs.google.com/document/d/1xURrgWIoUBArhqG1ne_2SKhfdEph9nT_BnfP57K62yQ/edit?usp=sharing\"><u>this template</u></a> and I can add it into the post.&nbsp;</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#AI_Safety_Australia___New_Zealand\"><u>AI Safety Australia &amp; New Zealand</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#AI_Safety_Support\"><u>AI Safety Support</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#Animals_and_Longtermism_network\"><u>Animals and Longtermism network</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#Animal_Ask\"><u>Animal Ask</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#EA_Pathfinder\"><u>EA Pathfinder</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#Foundations_for_Tomorrow\"><u>Foundations for Tomorrow</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#Giving_What_We_Can_\"><u>Giving What We Can</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#Good_Ancestors_Project__Sydney_Office_\"><u>Good Ancestors Project (Sydney Office)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#Good_Ancestors_Project__Operations_\"><u>Good Ancestors Project (Operations)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#Good_Ancestors_Project__Policy_\"><u>Good Ancestors Project (Policy)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#High_Impact_Engineers_\"><u>High Impact Engineers</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#High_Impact_Medicine_Australia\"><u>High Impact Medicine Australia</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#High_Impact_Recruitment\"><u>High Impact Recruitment</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#Insights_for_Impact\"><u>Insights for Impact</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#Lead_Exposure_Elimination_Project\"><u>Lead Exposure Elimination Project</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#Quantifying_Uncertainty_in_GiveWell_CEAs\"><u>Quantifying Uncertainty in GiveWell CEAs</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#Ready_Research\"><u>Ready Research</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#Sentience_Institute\"><u>Sentience Institute</u></a></li></ul><h3>AI Safety Australia &amp; New Zealand</h3><p><u>What do we do?</u></p><p>We\u2019re organising a number of community building programs:</p><ol><li>AI Sydney Fellowship - A fellowship for aspiring AI researchers in Australian/New Zealand to meet and collaborate on their projects.</li><li>Sydney Retreat -&nbsp; A weekend getaway near Maroubra Beach and trying out a range of activities to facilitate connection, ideation, sharing, and, of course, alignment.</li><li>Informal chats - Discussion group for people who have already completed the AGI Safety Fundamentals technical course or otherwise have an equivalent level of knowledge.</li></ol><p><u>About the team</u>&nbsp;</p><ul><li>Chris Leong - Main Organiser. Previously interned at Non-Linear.</li><li>Yanni Kyriacos - Marketing &amp; Communications Lead. Recently employed at Spark Wave as Head of Marketing. 10 years of experience as a marketing strategist.</li><li>Mark Carauleanu - Intern. Previously Admin Support at CEA, participated in the SERI Summer Fellowship and MLSS.</li><li>Jenna - Operations Contractor. Also Design &amp; Communications Consultant at Fortify Health and Cofounder of EA Canberra.</li></ul><p><u>Call to action</u></p><p>Reach out to chris [at] aisafetysupport.org.</p><p><u>More links&nbsp;</u></p><p><a href=\"https://www.facebook.com/groups/1099249420923957/\"><u>Join the Facebook group</u></a></p><p><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSeralYyAVJfYjOekBjZUiyUUqv5lQU0M67qdqtrfw8FByPFTQ/viewform\"><u>Nudge competition</u></a>&nbsp;</p><p>&nbsp;</p><h3>AI Safety Support</h3><p><u>What do we do?</u></p><p>Providing support services to early career, independent and transitioning AI alignment researchers. This is done through career coaching, events, training programs and fiscal sponsorship.</p><p><u>About the team</u>&nbsp;</p><ul><li>JJ Hepburn - Cofounder and CEO based in Sydney</li><li>Rachel Williams - COO based in Colorado Springs</li><li>Frances Lorenz - Operations Manager based in Boston</li><li>Shay Gestal - Health Coach based in Berlington</li></ul><p><u>Call to action&nbsp;</u></p><ul><li><a href=\"https://www.aisafetysupport.org/newsletter\"><u>Signup for our mailing list</u></a></li><li>Join the&nbsp;<a href=\"https://join.slack.com/t/ai-alignment/shared_invite/zt-1imvuiuyv-Sh6zYCKT_BMa9e24WLjXKQ\"><u>AI Alignment slack</u></a></li><li>Apply for&nbsp;<a href=\"https://www.aisafetysupport.org/resources/career-coaching\"><u>coaching</u></a></li><li>Checkout our free&nbsp;<a href=\"https://www.aisafetysupport.org/resources/shay\"><u>Health Coaching</u></a></li><li><a href=\"mailto:jj@aisafetysupport.org\"><u>Contact us</u></a> about fiscal sponsorships</li></ul><p><u>More links</u></p><p><a href=\"https://www.aisafetysupport.org/\"><u>aisafetysupport.org</u></a></p><p>&nbsp;</p><h3>Animals and Longtermism network</h3><p><u>What do we do?</u>&nbsp;<br>We are a community of individuals who are interested in the intersection between non-human animals and longtermism. We host an online discussion community, and we are currently working on starting a new organisation with paid staff working on both foundational research and concrete interventions in this space.<br><br><u>About the team</u></p><p>We are a network of communities, though Ren Springlea (based in Adelaide, South Australia) is leading this initiative.</p><p>Ren Springlea - I am leading this initiative. I have overseen some background research on this topic, and I am currently securing funding to hire our first full-time staff member. My full-time work is as an animal advocacy researcher at Animal Ask, and I have particular expertise in biology, economics, and quantitative analysis. My PhD covered fisheries, effective philanthropy, and animal sentience, and I have experience working in government policy and conducting research and strategy for the Animal Justice Party.&nbsp;<br><br><u>Call to action.</u></p><ul><li>Everybody is welcome to join our online community, hosted on Discord:&nbsp;<a href=\"https://discord.gg/KVf7qFmMrX\">https://discord.gg/KVf7qFmMrX</a></li><li>This community has over 100 members, and the rest of the leadership team and I are active in the discussions, so please feel free to say hello.</li><li>We're planning on hiring a full-time researcher later this year or early January, so keep an eye on either the Discord or the EA Forum for the announcement post.</li></ul><p>&nbsp;</p><h3>Animal Ask</h3><p><u>What do we do?</u></p><p>Animal Ask conducts research to support animal advocacy organisations and campaigns. We support organisations with in-depth, cross-comparative research to support decision making towards the most promising opportunities for animals.&nbsp;Animal Ask assists organisation either through our prioritisation process, consultation or through our publicly available reports.<br><br><u>About the team (in Australia)</u></p><p>Ren Springlea - I'm a research scientist at Animal Ask. My work involves all aspects of Animal Ask's research, and I have particular expertise in biology, economics, and quantitative analysis. My PhD covered fisheries, effective philanthropy, and animal sentience, and I have experience working in government policy and conducting research and strategy for the Animal Justice Party. I'm based in Adelaide, South Australia.<br><br><u>Call to action</u></p><p>If you're interested in learning about our research to help make animal advocacy as effective as possible, feel free to check out our <a href=\"https://www.animalask.org/research-database\">research database</a> or sign up to our newsletter at <a href=\"http://animalask.org\">animalask.org</a>. If you're an organisation or individual involved with animal advocacy campaigns, we'd love you to <a href=\"https://www.animalask.org/askprogramme\">get in touch</a> so we can talk about supporting your efforts.</p><p>&nbsp;</p><h3>EA Pathfinder</h3><p><u>What do we do?</u></p><p>EA Pathfinder is an organisation that wants to create a world in which professionals motivated by Effective Altruism can do high impact work. We are addressing the bottlenecks specific to mid-career professionals by offering career advising, peer support, research mentorship and project management support.</p><p><u>About the team</u>&nbsp;</p><ul><li><a href=\"https://www.linkedin.com/in/neil-ferro/\"><strong><u>Neil Ferro</u></strong></a><strong>&nbsp;</strong>has recently come on board as a career advisor.</li></ul><p><u>Call to action</u></p><ul><li>If you have 5+ years of career experience and have a successful career, but feel your job doesn't have the positive impact you\u2019d like then you can apply for free career advice&nbsp;<a href=\"https://uumefv3lqau.typeform.com/to/wRAEY2UO?typeform-source=www.eapathfinder.org\"><u>here</u></a>.&nbsp;</li></ul><p><u>More links</u></p><ul><li>You can visit the website&nbsp;<a href=\"https://www.eapathfinder.org/\"><u>here</u></a>&nbsp;&nbsp;<br>&nbsp;</li></ul><h3>Foundations for Tomorrow</h3><p><u>What do we do?&nbsp;</u></p><p>Foundations for Tomorrow is a proudly independent, leader-focused non-profit with the mission of protecting Australia\u2019s future generations. Our aspiration over the next 3-5 years is to:</p><ul><li>Research effective pathways&nbsp; to safeguard future generations within the Australian politico-legal landscape; and&nbsp;</li><li>Cultivate opportunities for direct collaboration with private sector and political leaders to implement localised solutions in Australia</li></ul><p><u>About the team</u></p><ul><li>Holly Crockford - Research Lead with a background in econometrics, law, and analytics.&nbsp;</li><li>Taylor Hawkins - Operations Lead with a background in business, law and social impact consulting.</li></ul><p><u>Call to action&nbsp;</u></p><ul><li>If you would like to engage with the Parliamentary Friendship Group for Future Generations either as an contributor or sponsor, please contact us at&nbsp;<a href=\"mailto:hello@foundationfortomorrow.org\"><u>hello@foundationfortomorrow.org</u></a></li><li>If you would like to collaborate on research and roadmapping initiatives, please contact us at&nbsp;<a href=\"mailto:hello@foundationfortomorrow.org\"><u>hello@foundationfortomorrow.org</u></a></li></ul><p><u>More links&nbsp;</u></p><ul><li><a href=\"https://www.foundationsfortomorrow.org/\"><u>https://www.foundationsfortomorrow.org/</u></a><br>&nbsp;</li></ul><h3>Giving What We Can&nbsp;</h3><p><u>What do we do?</u></p><p>Giving What We Can is a worldwide community of effective givers whose mission is to create a culture where giving effectively and significantly is a cultural norm.</p><p>Three of our six permanent staff are based in Australia (including our executive director) and one of our founders Toby Ord is also Australian as are several founding members (e.g. Peter Singer and Peter Eckersley). Luke being hired in Australia set the precedent for hiring an international remote team (core team are in Sydney, Melbourne, London, Utrecht, Davos, New York) and shows that people can effectively work on key international projects from wherever they want to live. Our team is coming together in Australia this November for our team retreat \u2013 we hope to see some of you in the Sydney office!</p><p><u>About the team</u>&nbsp;</p><p>Our Australian team includes&nbsp;</p><ul><li><a href=\"https://www.givingwhatwecan.org/people/luke-freeman\"><u>Luke Freeman</u></a> (executive director)</li><li><a href=\"https://www.givingwhatwecan.org/people/michael-townsend\"><u>Michael Townsend</u></a> (research) and&nbsp;</li><li><a href=\"https://www.givingwhatwecan.org/people/grace-adams\"><u>Grace Adams</u></a> (marketing).</li></ul><p><u>Call to action</u></p><ul><li>Want advice on leading a remote team while being based in Australia? Contact&nbsp;<a href=\"mailto:luke.freeman@givingwhatwecan.org\"><u>Luke</u></a>.</li><li>Want to help spread effective giving around the world? Contact&nbsp;<a href=\"mailto:luke.freeman@givingwhatwecan.org\"><u>Luke</u></a> and/or&nbsp;<a href=\"http://grace.adams@givingwhatwecan.org\"><u>Grace</u></a>.</li><li>Come say hi to Luke and Michael in the Sydney office :) (Or find Grace in Melbourne!)</li></ul><p><u>More links</u></p><p>Learn more about our plans in our&nbsp;<a href=\"https://www.givingwhatwecan.org/blog/public-strategy-update\"><u>public update about our 2022 strategy</u></a>.</p><h3>Good Ancestors Project (Sydney Office)</h3><p><u>What do we do?</u></p><p>We\u2019ve set up an office for people in Sydney doing EA or longtermist work. We currently have people from AI Safety Support, High Impact Recruitment, Effective Altruism Australia, Giving What We Can, Good Ancestors Project, the Lead Exposure Elimination Project and the Sentience Institute using the office. We\u2019re taking applications for people who would want to work alongside other EAs to help build the community.</p><p><u>About the team&nbsp;</u></p><ul><li>Bradley Tjandra - I\u2019m the office manager. I helped to organise EAGxAustralia 2022 and used to run EA Sydney. I was previously an actuary at a consulting firm. I\u2019m based in Sydney, Australia.</li></ul><p><u>Call to action</u></p><p>We\u2019re holding an Office Warming on&nbsp;<strong>Thursday 10th November</strong> at&nbsp;<strong>5:30 PM</strong>. If you\u2019re reading this, we\u2019d love you to attend!&nbsp;<a href=\"https://www.goodancestors.org.au/office-launch\"><u>Find out more here</u></a>.</p><p>Please reach out to bradley [at] goodancestors.org.au if:</p><ol><li>You\u2019re interested in using the office. <a href=\"https://www.goodancestors.org.au/office\">You can also apply via our website!</a></li><li>You\u2019d be interested in co-managing the office with me (this would probably be on a ~0.1-0.25 FTE basis).</li><li>You\u2019d like to hold an EA/longtermist event at the office.</li><li>You want to give/receive advice about setting up an office.</li></ol><p><u>More links</u>&nbsp;</p><p><a href=\"http://goodancestors.org.au/office\"><u>goodancestors.org.au/office</u></a></p><h3>&nbsp;</h3><h3>Good Ancestors Project (Operations)</h3><p><u>What do we do?</u></p><p>We\u2019re providing fiscal sponsorship and operational support to organisations and grant recipients, especially those based in Australia. We\u2019re also looking to expand into specific projects (particularly movement building-focused).</p><p><u>About the team&nbsp;</u></p><ul><li>Nathan Sherburn - Co-founder. Previously the co-founder of a startup (FLUX) and a PhD candidate in educational software. Based in Melbourne.</li><li>Bradley Tjandra - Co-founder. Previously an actuarial consultant at a consulting firm. I also helped to organise EAGxAustralia 2022 and used to run EA Sydney. I\u2019m based in Sydney.</li><li>Michael Townsend - Board Member. Currently a Researcher at Giving What We Can.</li><li>Luke Freeman - Board Member. Currently Executive Director of Giving What We Can.</li></ul><p><u>Call to action&nbsp;</u></p><p>Please reach out to contact [at] goodancestors.org.au if:</p><ol><li>You\u2019re planning to do, are doing, or are planning to fund EA work in Australia. We can provide fiscal sponsorship, employment, provide our infrastructure, or help setting up your own company or NFP.</li><li>You\u2019d be excited to see certain projects in Australia which require competent operations people to manage the project and/or recruit a team.</li></ol><p><u>More links&nbsp;</u></p><p><a href=\"http://www.goodancestors.org.au\"><u>www.goodancestors.org.au</u></a>&nbsp;</p><h3>&nbsp;</h3><h3>Good Ancestors Project (Policy)</h3><p><u>What do we do?</u></p><p>We\u2019re developing and advocating for Australian-specific policies derived from effective altruist, longtermist and related principles. We think a broad-tent political community supporting our ideas greatly increases our prospects of success - so we\u2019re building relationships across Australia and the region and empowering people to advocate for the values that matter to them in a coordinated and savvy way.&nbsp;</p><p>We think policy advocacy efforts are strengthened by a centralised and enduring brand - so we\u2019re working to bring together researchers, policy developers and policy advocates wherever possible.&nbsp;</p><p><u>About the team&nbsp;</u></p><ul><li>Greg Sadler - Greg, formally a senior public servant and ministerial adviser, is leading the policy effort. Greg is also the Secretary of Effective Altruism Australia.&nbsp;</li></ul><p><u>Call to action&nbsp;</u></p><p>Please reach out to greg [at] goodancestors.org.au if:</p><ul><li>You\u2019re involved in policy, advocacy or related work in Australia.&nbsp;</li><li>You\u2019re looking to be involved in, seek funding for, or provide funding for, policy, advocacy or related work in Australia.&nbsp;</li></ul><p><u>More links&nbsp;</u></p><p><a href=\"http://www.goodancestors.org.au\"><u>www.goodancestors.org.au</u></a>&nbsp;</p><h3>&nbsp;</h3><h3>High Impact Engineers&nbsp;</h3><p><u>What do we do?</u> We aim to increase the quantity of impactful work done by physical engineers across the globe.</p><p><u>About the team</u>&nbsp;</p><p>Our team is internationally dispersed between Australia, UK and USA.</p><ul><li>Sean Lawrence. Co-Founder. Finishing off a PhD in aerospace engineering at Monash University. Based in Melbourne.</li><li>Jessica Wen. Co-Founder. Completed a Master's in Materials Science at Oxford University, worked as a mechanical engineer at a global automotive company before running HI-Eng full-time. Based near Oxford, UK.&nbsp;</li><li>Bryce Rogers. Operations Lead. Graduated from the University of Southern California with a BS in Mechanical Engineering in 2022. Based in LA, USA.</li></ul><p><u>Call to action&nbsp;</u></p><p>You should&nbsp;<a href=\"https://www.highimpactengineers.org/contact\"><u>contact us</u></a> or request a&nbsp;<a href=\"https://www.highimpactengineers.org/session\"><u>1-1 with us</u></a> if:</p><ul><li>You\u2019re an engineer (you should also&nbsp;<a href=\"https://www.highimpactengineers.org/get-involved\"><u>join our network of impact-oriented engineers</u></a> and check out our&nbsp;<a href=\"https://www.highimpactengineers.org/resources\"><u>resource page</u></a>).</li><li>You would like to be connected with engineers with specific expertise.</li></ul><p><u>More links</u></p><ul><li><a href=\"https://www.highimpactengineers.org/resources\"><u>Resources for physical engineers</u></a></li><li><a href=\"https://bit.ly/HIEng-slack\"><u>High Impact Engineers Slack</u></a></li></ul><h3>&nbsp;</h3><h3>High Impact Medicine Australia</h3><p><u>What do we do?&nbsp;</u></p><p>Hi Med is an international organisation bringing together medical students and doctors who wish to consider their impact. We facilitate discussion and education about the varied ways medics can increase their positive influence on the world. This may be professionally or privately, through donating effectively or career redirection (within or outside medicine).&nbsp;</p><p>Currently we run an introductory fellowship every 6 months with our international counterparts, with 8 weeks of novel topics and discussion around how one might get involved. In the future, we are hoping to form interest groups at universities, and develop resources to distribute to interested individuals and course coordinators to incorporate high impact ideas into medical training. We are also starting to produce research about the crossover between Medicine and Effective Altruism.</p><p>We believe that Medicine attracts an altruistically minded, evidence-responsive type of person who would both be more likely to align with the ideas in EA, as well as having the potential career capital to have a large impact in a diverse range of fields.&nbsp;</p><p><u>About the team</u></p><p>Founded in 2022 in Newcastle as a branch of Hi Med International (a Centre for Effective Altruism-supported UK initiative).</p><p>Ellie Christian and Bal Dhital completed Hi-Med\u2019s first fellowship and went on to collaborate with the founders to form an Australian branch. Ben Prangemeier, another junior doctor and friend based in Newcastle, joined shortly after and has helped manage and run the group.&nbsp;</p><p>For the most recent Hi-Med fellowship (running October-December 2022), the facilitators for the Australian cohorts are Bruce Tsai, Ellie Christian and Clare Harris.</p><p>We are looking to expand!</p><p><u>Call to action</u></p><p>Contact us for</p><ul><li>Medicine-specific career advice relating to impact (or see our&nbsp;<a href=\"https://www.highimpactmedicine.org/our-research/medicalcareers\"><u>career profile</u></a> with Probably Good)</li><li>If you\u2019d like to get involved, we are looking to develop further opportunities and directions for those interested by forming an executive team in the coming months. Keep in the loop on our&nbsp;<a href=\"https://www.facebook.com/groups/414594543891415/?ref=share&amp;mibextid=S66gvF\"><u>facebook group</u></a></li><li>This will include setting up chapters in different cities in Australia&nbsp;</li><li>We are looking to run socials and in-person fellowships, and we are looking to present at relevant conferences, hospitals and medical schools in 2023</li></ul><p><u>More links</u></p><p>Check out Hi Med\u2019s&nbsp;<a href=\"https://www.highimpactmedicine.org/\"><u>website</u></a> for more info</p><p>We are most active on our&nbsp;<a href=\"https://www.facebook.com/groups/414594543891415/?ref=share&amp;mibextid=S66gvF\"><u>facebook group</u></a></p><p>This is our&nbsp;<a href=\"https://www.highimpactmedicine.org/our-research/medicalcareers\"><u>career profile</u></a> with Probably Good</p><h3><br>High Impact Recruitment</h3><p><u>What do we do?</u></p><p>High Impact Recruitment aims to be a trusted hiring arm for the effective altruism community. We support young projects and growing organisations in their recruiting activities by sourcing, vetting, and matching talent to a variety of roles \u2013 with a focus on longtermist projects.</p><p><u>About the team</u></p><p>The hiring agency founders are:</p><ul><li><a href=\"https://www.linkedin.com/in/neil-ferro/\"><strong><u>Neil Ferro</u></strong></a><strong>&nbsp;</strong>is one of the co-founders and based in Sydney. He was part of the senior leadership team at the largest Australian media company for 8 years and successfully led various teams. He was the main organiser for the EA Sydney Community (FB group over 1200 members) for 3.5 years and brought on 45 speakers for EAGx Australia 2019. The other co-founders include</li><li><a href=\"https://www.linkedin.com/in/raymund-ed-dominic-bermejo-53113834/\"><u>Red Bermejo</u></a> who is based in the Philippines and&nbsp;</li><li><a href=\"https://www.linkedin.com/in/leemcclenon/\"><u>Lee McClenon</u></a> who is based in Philadelphia.</li></ul><p><u>Call to action</u></p><p>If people need help with hiring please reach out to Neil Ferro -&nbsp;<a href=\"mailto:neil@eahire.org\"><u>neil@eahire.org</u></a>. We also have a&nbsp;<a href=\"https://eahire.notion.site/High-Impact-Recruitment-db91fdfa8d954ce1b43c69abcbae59f8\"><u>job board</u></a> of roles.</p><p><u>More links</u></p><ul><li>You can visit our website&nbsp;<a href=\"https://eahire.notion.site/High-Impact-Recruitment-db91fdfa8d954ce1b43c69abcbae59f8\"><u>here</u></a>&nbsp;&nbsp;</li><li><strong>Job Seekers -&nbsp;</strong>please fill out the form below and we will keep you in mind for any roles that we come across&nbsp;</li><li><a href=\"https://airtable.com/shrZuytF9838DgcXd\"><strong><u>Job Search Request Form</u></strong></a></li><li><strong>Looking to Hire?&nbsp;</strong>Are you hiring for a specific role and need some support? Please fill out the form below and we will get back to you as quickly as we can.&nbsp;</li><li><a href=\"https://airtable.com/shr6jq4xBkdvzid1t\"><strong><u>Hiring Support Request Form</u></strong></a></li></ul><h3><br>Insights for Impact</h3><p><u>What do we do?&nbsp;</u></p><p>Fascinating and valuable research has been streaming out of EA-aligned organisations, but does anyone have time to read the papers? Insights for Impact is a new project funded by FTX, aiming to communicate key research insights in 5-15 min YouTube videos, so that EAs and any layperson can understand and keep up with cool cutting-edge research that might lead to positive impact in the world.</p><p><u>About the team</u></p><p>We are just 2 people at the moment: Christian Pearson and Jenna Ong.&nbsp;</p><ul><li>Christian is a Canberra-born chemist and communicator currently living in Canada.&nbsp;</li><li>Jenna is a Canberra-based creative and community builder who cares a lot about wellbeing.</li></ul><p>Both co-founders are stoked that they get to combine their science/engineering and arts backgrounds to work on such a fun EA-aligned project!</p><p><u>Call to action&nbsp;</u></p><ul><li>If you would like us to summarise a particular research paper in video form, whether it\u2019s your own research or someone else\u2019s, please get in touch! Contact&nbsp;<a href=\"mailto:insightsimpactmedia@gmail.com\"><u>insightsimpactmedia@gmail.com</u></a></li><li>If you have some skills in video animation, 3D graphics, audio engineering or potentially other skills related to video production or YouTube, please reach out! Contact&nbsp;<a href=\"mailto:insightsimpactmedia@gmail.com\"><u>insightsimpactmedia@gmail.com</u></a><br>&nbsp;</li></ul><h3>Lead Exposure Elimination Project</h3><p><u>What do we do?</u></p><p>LEEP works to prevent childhood lead poisoning - a problem that affects 815M children worldwide. We do this by advocating for regulation of lead paint (a primary route of exposure), and ensuring compliance with enforcement.&nbsp;</p><p><u>About the team&nbsp;</u></p><ul><li><a href=\"https://www.linkedin.com/in/jackrafferty1/\"><u>Jack Rafferty</u></a> - Co-Founder and Director. Based in Sydney.</li><li><a href=\"https://www.linkedin.com/in/lucia-coulter-657877162/\"><u>Lucia Coulter</u></a> - Co-Founder and Director.&nbsp;</li><li><a href=\"https://www.linkedin.com/in/clare-donaldson-224257133/\"><u>Clare Donaldson</u></a> - Director.&nbsp;</li><li><a href=\"https://www.linkedin.com/in/juliacberg/\"><u>Julia Berg</u></a> - LEEP\u2019s head of Operations.&nbsp;</li><li><a href=\"https://www.linkedin.com/in/charlieloudon/\"><u>Charlie Loudon</u></a> - LEEP Program Manager.&nbsp;</li><li><a href=\"https://www.linkedin.com/in/baldhital/\"><u>Bal Dhital</u></a> - LEEP Program Manager.&nbsp;</li><li><a href=\"https://www.linkedin.com/in/charlotte-potts-8b30b2136/\"><u>Charlotte Potts</u></a> - LEEP Program Manager.&nbsp;</li><li><a href=\"https://www.linkedin.com/in/guillermoolmedomendez/\"><u>Guillermo Olmedo-Mendez</u></a> - LEEP\u2019s Bolivia Programs Lead.</li></ul><p><u>Call to action&nbsp;</u></p><p>You can contact us at contact [at] leadelimination.org&nbsp;<br>Or, if you\u2019d be interested in starting a high impact project like LEEP, consider reaching out to Charity Entrepreneurship&nbsp;<a href=\"https://www.charityentrepreneurship.com/contact\"><u>at this link</u></a>.</p><p><u>More links&nbsp;</u></p><ul><li><a href=\"https://leadelimination.org/\"><u>Homepage - Lead Exposure Elimination Project</u></a></li><li><a href=\"https://www.facebook.com/LeadElimination\"><u>Facebook Page - Lead Exposure Elimination Project</u></a>&nbsp;</li><li><a href=\"https://www.linkedin.com/company/lead-exposure-elimination-project/\"><u>LinkedIn - Lead Exposure Elimination Project</u></a>&nbsp;</li></ul><h3>&nbsp;</h3><h3>Quantifying Uncertainty in GiveWell CEAs</h3><p><u>What do we do?&nbsp;</u></p><p>We\u2019ve been quantifying the uncertainty in GiveWell CEAs (Cost Effectiveness Analyses) for the purpose of calculating the value of future research in cost effectiveness by value of information.</p><p>We\u2019ve recently submitted an&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Nb2HnrqG4nkjCqmRg/quantifying-uncertainty-in-givewell-ceas\"><u>EA Forum post</u></a> on this as a submission to GiveWell\u2019s Change our Mind Contest.</p><p><u>About the team&nbsp;</u></p><ul><li>Sam Nolan. Software Developer for Squiggle at the&nbsp;<a href=\"https://quantifieduncertainty.org/\"><u>Quantified Uncertainty Research Institute</u></a>. Helps build the tools needed for the project. Based in Jan Juc, Victoria.</li><li>Hannah Rokebrand. Researcher focusing on effectiveness of global poverty interventions. Based in Horsham, Victoria.</li></ul><p><u>Call to action&nbsp;</u></p><p>This is a particularly large project, and any criticism and help would be greatly appreciated. This could include:</p><ul><li>Development on&nbsp;<a href=\"https://squiggle-language.com/\"><u>Squiggle</u></a>, the tool we use for this research.</li><li>Criticism, Feedback or comments on our work.</li><li>Help improving the models, as well as creating new models of different interventions.</li><li>If you\u2019d like to get into using Squiggle for evaluations, I\u2019d love to chat.</li><li>You can book Sam Nolan on calendly here:&nbsp;<a href=\"https://calendly.com/sam_nolan/30min\"><u>calendly.com/sam_nolan/30min</u></a></li></ul><p><u>More links&nbsp;</u></p><ul><li><a href=\"https://observablehq.com/@hazelfire/quantifying-uncertainty-in-givewell-ceas\"><u>Observable Notebook</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/Nb2HnrqG4nkjCqmRg/quantifying-uncertainty-in-givewell-ceas\"><u>EA Forum post</u></a></li></ul><h3><br>Ready Research</h3><p><u>What do we do?</u></p><p><a href=\"http://www.readyresearch.org\"><u>Ready Research</u></a> (readyresearch.org) conducts behaviour science research to address the world's most pressing problems. We research how to&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11266-022-00499-y\"><u>increase charitable donations</u></a> (<a href=\"https://forum.effectivealtruism.org/posts/9kmetKMuRdmZzfbA8/research-summary-what-works-to-promote-charitable-donations\"><u>accessible summary</u></a>),&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S2666833521000976#!\"><u>reduce animal-product consumption</u></a> (<a href=\"https://forum.effectivealtruism.org/posts/azHZ2pQj9JgdZLC63/what-interventions-influence-animal-product-consumption\"><u>accessible summary</u></a>), teach&nbsp;<a href=\"https://docs.google.com/document/d/1YoMxRrF1QRhKu3lrc7DTs0VKnDWlVWDnnOQWV_FGZ3M/edit#heading=h.vfd8vehw6pk1\"><u>ethics</u></a> and&nbsp;<a href=\"https://osf.io/7x5z6/?view_only=abee56e83a594ae3b769df61a07aa9e6\"><u>rationality</u></a>, and&nbsp;<a href=\"https://docs.google.com/document/d/1ZJsNPGcH_wxWP99vvPSZXNL-hTIzk476ByYVH9neylU/edit#\"><u>improve institutional decision-making</u></a>. We\u2019re scoping out a MOOC related to effective altruism or existential risk. We also incubated a government-funded rapid survey of COVID-19 behaviours (SCRUB) that ran for 21 waves throughout 2020-2021, and provided behaviour science insights which shaped COVID-19 policy.</p><p><u>About the team</u>&nbsp;</p><ul><li><a href=\"https://noetel.com.au/projects/\"><u>Michael Noetel</u></a>, Co-founder based in Brisbane. Also chair and director of Effective Altruism Australia (charity), and academic at The University of Queensland, Australia&nbsp;</li><li><a href=\"https://www.linkedin.com/in/emily-grundy/?originalSubdomain=au\"><u>Emily Grundy</u></a> \u2013 Research lead based in Melbourne. Also a Research Officer at BehaviourWorks Australia with expertise in surveys, reviews, and prioritisation projects.</li><li><a href=\"https://au.linkedin.com/in/aksaeri\"><u>Alexander Saeri</u></a>, Co-founder based in Melbourne. Also applied academic / consultant at BehaviourWorks Australia, Monash University, Australia with expertise in experimental trials, data analysis, systems change, and scale up; and independent researcher in AI governance.&nbsp;&nbsp;</li><li><a href=\"https://www.linkedin.com/in/peterslattery1/\"><u>Peter Slattery</u></a>, Co-founder based in Sydney. Highly connected in the Australian and international EA community. Also applied academic / consultant at BehaviourWorks Australia, Monash University, Australia with expertise in surveys and reviews.&nbsp;</li></ul><p><u>Call to action&nbsp;</u></p><p>Michael is looking for a research fellow or research assistant to work on some of the above projects and will be advertising soon. Send him a short email if you want to know when that\u2019s listed:&nbsp;<a href=\"mailto:m.noetel@uq.edu.au\"><u>m.noetel@uq.edu.au</u></a>&nbsp;</p><p>Otherwise, we\u2019re often excited to help new researchers learn important skills by collaborating on a project. If you want to learn with us, or have a research question you want answered, you might&nbsp;<a href=\"https://forms.gle/2mr7g5YDJVgHpC4j6\"><strong><u>volunteer</u></strong></a> as a:</p><ul><li>Team member (ideal for new researchers who want to learn and get experience)</li><li>Project lead (ideal if you have a bit more experience and want to learn to run a team project)</li><li>Project advisor (ideal if you have an important research question [e.g., you want to know what interventions best work for some outcome] but don\u2019t have the researchers to answer it)</li></ul><p>If you would like to discuss research collaboration opportunities, please&nbsp;<a href=\"https://forms.gle/ihLpTb432GmEstLM9\"><strong><u>contact us</u></strong></a>.<br>&nbsp;</p><h3>Sentience Institute</h3><p><u>What do we do?&nbsp;</u></p><p>Sentience Institute is a non-profit think tank with the aim of creating a world where all sentient minds flourish, including humans, animals, and digital minds. We aim to support the small but growing field of digital minds research, which we see as an important and neglected part of the longtermist project. In addition to publishing foundational research, we hosted&nbsp;<a href=\"https://urldefense.com/v3/__http://summitonai.org/__;!!BpyFHLRN4TMTrA!72koY8W18xF9jqfv7kFbJ0FJkc7DdciSZyhYiTg1Hj9B717BkF4KHPGjhyG89qUB76Ps2zNsWIPdNbvVmRELhC7kfJ4$\"><u>The Summit on AI in Society</u></a> (Brynjolfsson, Murdick, Russell, etc.), host an intergroup call across research organisations working on this topic (FHI, GCRI, CLR, etc.), and produce a&nbsp;<a href=\"https://urldefense.com/v3/__https://www.sentienceinstitute.org/podcast/__;!!BpyFHLRN4TMTrA!474lOeHeMdF7yLVGBYkdFLYG4NEka8N_95HL_TvQ-aXXWqbk2klrlqd5SJok2iFUJzF_ajxYhfQLplJ0dswusSVY5gM$\"><u>podcast</u></a>.</p><p><u>About the team (in Australia).&nbsp;</u></p><ul><li>Michael Dello-Iacovo - I\u2019m a strategy lead &amp; researcher for SI. I\u2019ve been working on a range of generalist tasks, including running the digital minds intergroup call, hosting our<a href=\"https://urldefense.com/v3/__https://www.sentienceinstitute.org/podcast/__;!!BpyFHLRN4TMTrA!474lOeHeMdF7yLVGBYkdFLYG4NEka8N_95HL_TvQ-aXXWqbk2klrlqd5SJok2iFUJzF_ajxYhfQLplJ0dswusSVY5gM$\"><u> podcast</u></a>, and doing research on projects such as understanding how different trajectories to the end of animal farming affect further moral change. I have a PhD in space science, have been actively involved with the Animal Justice Party, and have held several leadership roles in Australian effective altruism organisations and local chapters.</li></ul><p><u>Call to action&nbsp;</u></p><ul><li>If people are interested in following our work or other updates such as being notified of future hiring rounds, they should follow our newsletter via our&nbsp;<a href=\"https://urldefense.com/v3/__https://www.sentienceinstitute.org/__;!!BpyFHLRN4TMTrA!474lOeHeMdF7yLVGBYkdFLYG4NEka8N_95HL_TvQ-aXXWqbk2klrlqd5SJok2iFUJzF_ajxYhfQLplJ0dswuevkQTfo$\"><u>website</u></a>.</li><li>If people want to contact me directly, they can do so via michael@sentienceinstitute.org.</li></ul><p><u>More links</u>&nbsp;</p><ul><li><a href=\"https://urldefense.com/v3/__https://www.sentienceinstitute.org/research-agenda__;!!BpyFHLRN4TMTrA!474lOeHeMdF7yLVGBYkdFLYG4NEka8N_95HL_TvQ-aXXWqbk2klrlqd5SJok2iFUJzF_ajxYhfQLplJ0dswu9QvcR2E$\"><u>https://www.sentienceinstitute.org/research-agenda</u></a></li></ul><p><i>Thanks to everyone involved in these organisations and projects who took the time to write something for this post! Thanks especially to Michael Townsend for suggesting and reviewing this post.</i></p>", "user": {"username": "Bradley Tjandra"}}, {"_id": "cNavhrAa9ssFjzkrK", "title": "EA orgs should accept Summer 2023 interns by January", "postedAt": "2022-11-05T04:54:46.168Z", "htmlBody": "<p><strong>TLDR; </strong>EA organizations hiring graduating seniors or hosting undergraduates as 2023 summer interns should send program acceptances by <strong>January 2023 </strong>so that students don't have to accept a corporate offer before they apply to EA programs.&nbsp;</p><h2><strong>The Problem</strong></h2><p>Multiple undergrads have complained that you can't apply to corporate internships and EA summer programs at the same time. Instead, non-EA orgs have very&nbsp;early deadlines (October to January), allowing them to get decisions back to students very early in the year. They typically want&nbsp;responses from students within a few weeks of sending out offers. Meanwhile, many EA orgs don't send out acceptances until much later, like April, well past the deadline for responding to the corporate internship offers.<br><br>This puts students in an awkward position: either accept the corporate offers while planning to renege if they get a better EA internship, or gamble on the EA jobs, and potentially not get any. &nbsp;It's bad to miss out on anything for the summer, but lying violates most people's moral&nbsp;principles and going back on an offer that you previously accepted burns bridges at the organization you lied to. As a result of this application deadline system, many talented students are being funneled away from EA summer opportunities.</p><h2><strong>The Solution</strong></h2><p>Ideally, students could apply for corporate programs and EA fellowships at the same time, see where they are accepted, and take the best offer without having to lie or burn bridges.</p><p>Moving EA program deadlines up to compete with industry might be challenging for orgs that aren't 100% sure what kind of funding they'll have for summer programs. In this case, consider what kind of guidelines you can provide to prospective candidates to give them a better sense of how likely they will be accepted for a summer program at your org. Also, consider doing early acceptances if an extremely talented student contacts you with an exploding offer from another organization, and publicizing this policy if you decide to enact it.</p><h2><strong>The Competition</strong></h2><p>Here are some summer 2023 internship application deadlines for corporations competing with us for top talent (ie this is how early applications need to open in order to get offers back to students on a competitive timeframe):&nbsp;</p><p><strong>Technology&nbsp;</strong></p><p>Google: October 31st, 2022<br>Palantir: Sometime in October 2022</p><p><strong>Consulting</strong></p><p>Bain: November 29, 2022<br>McKinsey: January 19th, 2023</p><p><strong>Finance</strong></p><p>Goldman Sachs: November 20th, 2022<br>JP Morgan: November 27th, 2022</p><p>&nbsp;</p><p><strong>NB:</strong> Many of these organizations have multiple summer programs which all have different deadlines. I've tried to select programs with representative deadlines, but bizarrely, some companies even have different deadlines for students from different universities! <strong>If you actually want to apply to one of these places, please go to the org's website to double check the deadline for the program that best suits you</strong>.<br><br>Credit to <a href=\"https://forum.effectivealtruism.org/users/electroswing\">electroswing</a>, <a href=\"https://forum.effectivealtruism.org/posts/bmwwQoFznjRwswCbz/announce-summer-ea-internships-farther-in-advance\">who made all of these same points back in March</a>. But they're still true!&nbsp;</p>", "user": {"username": "AbbyBabby"}}, {"_id": "vHxKLNQciXN4taEdd", "title": "Applications are now open for Intro to ML Safety Spring 2023", "postedAt": "2022-11-04T22:45:02.056Z", "htmlBody": "<p>The&nbsp;<a href=\"https://safe.ai/\"><u>Center for AI Safety</u></a> is running another iteration of Intro to ML Safety this Spring for people who want to learn about empirical AI safety research topics<strong>.</strong></p><p><a href=\"https://airtable.com/shredY1461hyCVYC6\"><u>Apply to be a participant</u></a> by January 29th, 2023.</p><p><a href=\"https://airtable.com/shrl48oAnA860rDgB\"><u>Apply to be a facilitator</u></a> by December 30th.</p><p>Website:&nbsp;<a href=\"https://www.mlsafety.org/intro-to-ml-safety\"><u>mlsafety.org/intro-to-ml-safety</u></a></p><h2>About the Course</h2><p>Introduction to ML Safety is an 8-week course that aims to introduce students with a deep learning background to empirical AI Safety research. The program is designed and taught by&nbsp;<a href=\"http://danhendrycks.com\"><u>Dan Hendrycks</u></a>, a UC Berkeley ML PhD and director of the&nbsp;<a href=\"http://safe.ai\"><u>Center for AI Safety</u></a>, and provides an introduction to robustness, alignment, monitoring, systemic safety, and conceptual foundations for existential risk.</p><p>Each week, participants will be assigned readings, lecture videos, and required homework assignments. The materials are publicly available at&nbsp;<a href=\"https://course.mlsafety.org/\"><u>course.mlsafety.org</u></a>.</p><p>There are two tracks:</p><ul><li><strong>The introductory track:&nbsp;</strong>for people who are new to AI Safety. This track aims to familiarize students with the AI X-risk discussion alongside empirical research directions.</li><li><strong>The advanced track:&nbsp;</strong>for people who already have a conceptual understanding of AI X-risk and want to learn more about existing empirical safety research so they can start contributing.</li></ul><p>The course will be virtual by default, though in-person sections may be offered at some universities.<br>&nbsp;</p><h3>How is this program different from AGISF?</h3><p>Intro to ML Safety is generally&nbsp;<strong>more focused on empirical topics&nbsp;</strong>rather than conceptual work. Participants are required to watch recorded lectures and complete homework assignments that test their understanding of the technical material. If you\u2019ve already taken AGISF and are interested in empirical research, then you are the target audience for the advanced track.</p><p>Intro to ML Safety also<strong> emphasizes different ideas and research directions</strong>&nbsp;<strong>than AGISF</strong> does. Examples include:</p><ul><li><strong>Detecting&nbsp;</strong><a href=\"https://towardsdatascience.com/neural-trojan-attacks-and-how-you-can-help-df56c8a3fcdc\"><strong><u>trojans</u></strong></a>: this is a current security issue but also a potential microcosm for detecting deception and testing monitoring tools.</li><li><strong>Adversarial robustness</strong>: it is helpful for reward models to be adversary robust. Otherwise, the models they are used to train can \u2018overoptimize\u2019 them and exploit their deficiencies instead of performing as intended. This applies whenever an AI system is used to evaluate another AI system. For example, an ELK reporter must also be highly robust if its output is used as a training signal.</li><li><strong>Power averseness:&nbsp;</strong><a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\"><strong><u>Arguments</u></strong></a><strong> for taking AI seriously as an existential risk often focus on power-seeking behavior. Can we train language models to avoid power-seeking actions in text-based games?</strong></li></ul><p>You can read about more examples in&nbsp;<a href=\"https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5\"><u>Open Problems in AI X-risk</u></a>.</p><h2>Time Commitment</h2><p>The program will last 8 weeks, beginning on February 20th and ending on April 14th. Participants are expected to commit at least 5 hours per week. This includes ~1 hour of recorded lectures (which will take more than one hour to digest), ~1-2 hours of readings, ~1-2 hours of written assignments, and 1 hour of discussion.&nbsp;<br>&nbsp;</p><p>We understand that 5 hours is a large time commitment, so to make our program more inclusive and remove any financial barriers,&nbsp;<strong>we will provide a $500 stipend upon completion of the course</strong>.<strong> </strong><i>(EDT: reduced from $1000)</i></p><h2>Eligibility</h2><p><strong>Anyone is eligible to apply</strong>. The prerequisites are:</p><ul><li>Deep learning: you can gauge the background knowledge required by skimming the week 1 slides:&nbsp;<a href=\"https://docs.google.com/presentation/d/15yMNlkWAL5cuSHHZe1gy2sM8zcN8gHk9iBVzKKvS9zw/edit#slide=id.g126975c12ec_1_54\"><u>deep learning review</u></a>.</li><li>Linear algebra or introductory statistics (e.g., AP Statistics)</li><li>Multivariate differential calculus<br>&nbsp;</li></ul><p>If you are not sure whether you meet these prerequisites, err on the side of applying. We will review applications on a case-by-case basis.</p><h2>Facilitating a section</h2><p>To be a facilitator, you must have a strong background in deep learning and AI Safety. Note that if you are not familiar with the content, you will have to learn it in advance of each week.</p><p>The time commitment for running one cohort is ~2-4 hours per week, depending on prior familiarity with the material. 1 hour of discussion and 1-3 hours of prep. Discussion times are flexible.</p><p>We will pay facilitators a stipend corresponding to roughly $30 per hour (subject to legal constraints).</p><p><a href=\"https://airtable.com/shrl48oAnA860rDgB\"><u>Apply</u></a> by December 30th.&nbsp;<strong>We are especially interested in finding facilitators for in-person groups.</strong><br>&nbsp;</p><p><strong>You can post questions here or reach out to&nbsp;</strong><a href=\"mailto:introcourse@mlsafety.org\"><strong><u>introcourse@mlsafety.org</u></strong></a><strong>.</strong></p>", "user": {"username": "Joshua Clymer"}}, {"_id": "xqbm65f7TZbjfhsz4", "title": "Longevity research as AI X-risk intervention", "postedAt": "2022-11-06T17:58:09.140Z", "htmlBody": "<p><strong>Core argument:</strong></p><p>I propose that one possible motivation for a rush to build GAI in our lifetimes is that it is a high-risk gamble to increase human healthspan and lifespan. If one primarily values present-day humans, particularly oneself, one's family, friends, local community, and nation, then this is a sound strategy for maximizing the expected benefit. The downside risk is that the people closest to the builders die a few decades earlier than they otherwise would have. The upside potential is that they live for many centuries, or even longer.</p><p>Under a longtermist moral calculus, by contrast, rushing to build GAI is not a risk worth taking, because of the serious risk of curtailing humanity's future.</p><p>If it is not possible to persuade GAI builders to act according to a longtermist framework, then an alternative is to placate them. One way to do this would be to rush to achieve success in human-directed longevity medicine.</p><p>Here, I do not claim that this core argument is correct with certainty. I argue that it is worth taking seriously by EAs, both because it identifies a neglected strategy for potentially contributing to AI safety, and because of open pessimism among technical AI alignment researchers about the likelihood of finding success. This does not mean people should switch from technical AI alignment to longevity research, but that EA should consider encouraging people with a good fit for longevity research to go into this field.</p><p><strong>What is longevity research?</strong></p><p>Your body constantly suffers massive DNA damage. The sun irradiates your skin, your cells constantly produce toxins in their little biomolecular factories, and your DNA copying machinery is only <i>almost</i> perfect. Under that onslaught, it's no surprise that people's bodies break down over time, and they start acquiring the diseases of old age.</p><p>Let's call ordinary biomedical research \"disease research\" for short. It studies cancer, heart disease, Alzheimer's, diabetes, infections, the weakening of bones, pain in the joints, and many other ills.</p><p>Longevity research claims that <a href=\"https://www.youtube.com/watch?v=EXGUNvIFTQw\">we can resist</a>, even achieve victory, over this host of illnesses. After all, your body also has wonderful DNA repair tools that fix nearly all of this damage. And even if it can't, most cells eventually die and get replaced by your stem cells. These stem cells split in two. One stays a stem cell. The other matures and differentiates into the lost cell type, migrates into its original position, and takes up its old job.</p><p>So why do these repair mechanisms eventually fail us? To answer that question, we have to study how our biology changes over our lifespan in ways that either directly produce disease, or make it easier for disease to strike us. That's a subtle distinction, worth elaborating.</p><p>One way aging causes problems is that our repair mechanisms have fundamental limits. As one clear example, stem cells also accumulate DNA damage over the course of one's life. And nothing's replacing <i>them. </i>Eventually, dysregulated stem cells can give rise to cancer or to distorted daughter cells that provoke autoimmune reactions. This is an example of how, as we get older, our biology can directly generate disease.</p><p>Most of my readers will know that cancer cells are simply the patient's own cells, but in a disordered state. They might also know that the immune system normally kills cancer cells before they can turn into tumors. Unless you were unlucky enough to get an unusually early cancer, or perhaps if you are an older reader, your body has been silently staving off cancer for your entire life.</p><p>But those protections can fail, making it easier for cancer and other diseases to strike. The organ known as the thymus is where some of your most important immune cells - T cells - are born. As you age, <a href=\"https://en.wikipedia.org/wiki/Thymic_involution\">your thymus shrinks</a>. You still have your old T cells. Just fewer new ones. The memory of diseases you've faced in the past is embodied in your old T cells. <a href=\"https://www.youtube.com/watch?v=3IQT9KsrO6M\">New T cells are how your immune system adapts to new diseases.</a> As your thymus shrinks, you lose those new T cells, and so your body gets worse at being able to fight off new types of infections.</p><p>So let's come back to our subtle distinction between how aging directly produces disease, and how it fails to prevent disease. When stem cells pick up DNA damage, and nothing is there to fix or replace them, they can directly cause cancer. By contrast, when your thymus shrinks and you have fewer new T cells, your body's ability to prevent infections declines, even though it still requires that you suffer an infection for that protective breakdown to pose a health threat.</p><p>Longevity research focuses on trying to solve these issues before they can cause or allow disease in the first place. There's evidence that we can prevent your thymus from shrinking. We might be able to destroy old stem cells and replace them with new ones. Low doses of the drug <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6814615/\">rapamycin</a> robustly extend lifespan in animal models. Right now, <a href=\"https://news.uga.edu/dogs-needed-aging-research-project/\">a large study</a> of low-dose rapamycin safety and efficacy is taking place on dogs. And of course, healthy diet and exercise, as well as a clean and safe environment, are the original and most accessible forms of longevity medicine.</p><p>Not only might these be new ways of tackling old diseases, but success with any one of them might push back the time when we start getting the diseases of old age. We call the period of your life in which you do not suffer serious diseases \"healthspan.\" One goal of longevity research is extending lifespan. The other is extending healthspan. Ideally, we would have long lives, with the period of serious disease compressed to a short period at the very end.</p><p>Over time, we might enter an era when longevity research is producing results so quickly that life expectancy increases by more than one year, per year. For example, perhaps in 2032, life expectancy in the USA is 85 years. One year later, in 2033, it is two years longer, 87 years. Another year later, in 2034, it is again two years longer, 89 years. If that trend continued indefinitely, this would be called \"longevity escape velocity.\"</p><p>It's a misleading analogy. We use the term \"escape velocity\" for a rocket ship because we understand the laws of physics well enough to know in advance how fast that rocket needs to travel in order to escape Earth's orbit. When we say \"velocity,\" we are literally talking about velocity. By contrast, even if lifespan were increasing faster than 1 year of lifespan with each passing year, we would not necessarily be able to predict whether or when that trend would come to an end.</p><figure class=\"image\"><img src=\"https://i.prcdn.co/img?regionKey=IF0BxS2ilwZHTc0rzHmX6Q%3D%3D\"><figcaption>We can easily imagine achieving annual 1 year+ gains in life expectancy without being able to produce a chart comparable to this one for explaining how further such gains will appear.</figcaption></figure><p>Gains in longevity will be less like \"escape velocity\" and more like <a href=\"https://en.wikipedia.org/wiki/Moore%27s_law\">Moore's Law</a>, which we probably ought to have named Moore's <i>Trend</i>. It's simply a name for the long-term pattern of packing more transistors into the same size of circuit over time - doubling every two years, until it started slowing down.</p><figure class=\"image\"><img src=\"https://www.tf.uni-kiel.de/matwis/amat/semitech_en/kap_5/illustr/moore_law.png\"></figure><p>We don't know if a trend of steadily increasing healthspan or lifespan will ever take place. We also don't know at what rate it will grow, or how long that trend will last. Biology is different from rocketry and it's different from transistor manufacture. With rocketry, we figured out a long time ago how to get rockets into outer space. The problem is solved and we've moved on to refinements and other issues. With transistors, we're guaranteed to hit some sort of physical limit as transistors continue to shrink.</p><p>On a practical level, it's hard to be sure how to think about biology. I like to imagine that keeping our bodies alive and healthy is like playing a game of <a href=\"https://www.youtube.com/watch?v=_2bPJnNLJNE\">hacky-sack</a>. It's hard to keep the ball in the air, but it seems like we could engineer ways to keep the hacky sack in the air indefinitely while still following all the rules of the game.</p><p><strong>Would longevity research help with AI safety?</strong></p><p>To begin, I am <u>not</u> claiming that there's an overwhelming case that successful longevity research <i>would</i> help with AI safety. I am going to outline a case that it <i>might</i>, along with some critiques. My primary claim is that this argument is of pressing importance, and is worth exploring further.</p><p>As stated in the core argument at the top of this post, GAI is a risky but potentially high-upside gamble to increase human life expectancy for present-day people. AI capabilities researchers don't understand biomedical research, they do understand computers, and they may see GAI as their most effective way to make longevity research go faster. It's almost pure upside for them. If it pays off, they and their loved ones benefit. If it causes an X-catastrophe, they were going to die anyway. The upside potential is enormous, and the downside potential to them is capped.</p><p>The main approach we hope will avert AI-driven X-catastrophe is technical AI alignment research. But it might turn out to take more time than we have to achieve technical AI alignment. It might not even be theoretically possible. Eliezer Yudkowsky has expressed&nbsp;<a href=\"https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy\">deep, open pessimism</a> about the chance of alignment success. His complaints weren't just about how quickly AI capabilities research is progressing, but about how unlikely the current proposed technical solutions are to work, either at all, or in any kind of reasonable timeframe under current social norms.</p><p>If we achieved \"longevity Moore's Law,\" that might alter the risk calculus of AI capabilities researchers, massively increasing the downside potential to them personally (because now they're expecting to live longer) while limiting the upside potential (because human-directed biomedicine is beating AI to achieve longevity gains). At a certain point, in this model, AI capabilities researchers will refocus from increasing capabilities to increasing safety.</p><p>If GAI is coming in the short term, this strategy is useless. Longevity research is still the <a href=\"https://thewildwest.org/\">Wild West</a>. Billions of dollars poured into the nascent industry just this year, massively changing the funding landscape, and there are credible advanced preclinical studies on serious longevity drug candidates going on right now. But that's probably not enough to alter the hypothetical risk calculus I am positing that AI capability researchers are making in the next few years. If we're on track to achieve GAI in the next ten years, longevity research is probably no help.</p><p>But it may be that GAI will take longer. In this case, technical AI alignment may or may not be achievable in time. If it is not, or if one is not suited for technical AI alignment research, then it makes sense to put one's efforts into alternative strategies to stave off medium-term X-risk from AI. One strategy is working toward \"longevity Moore's Law,\" in the hope that this causes a massive worldwide reconsideration of how people do risk/benefit calculus, among AI capabilities researchers, politicians, and the public.</p><p>Right now, many people are advocating switching from longtermist-based justifications of AI safety's pressingness to a messaging approach of \"AI might kill you and everyone you love.\" That message would be more powerful if it was <i>\"AI might kill you and everyone you love, and death is no longer an inevitability!\"</i></p><p><strong>Critique 1: The argument proves too much</strong></p><p>One criticism of this argument is that conventional biomedical research can also help with extending lifespan and healthspan. Perhaps by subdividing known diseases into stages and types, becoming excellent at personalized medicine, and developing medications with better precision in terms of delivery and dose, we can suppress disease indefinitely even if the body has aged and is generating them faster. It would be like <a href=\"https://www.youtube.com/watch?v=D0n8N98mpes\">getting so good at playing Whack-a-Mole that you might actually win.</a></p><p>Yet the industry is old, huge, full of incredibly smart and hardworking people, has huge financial incentives, and hasn't been successful so far at achieving \"longevity Moore's Law.\" People working in that field are smart enough to choose a \"longevity medicine\"-based approach to drug design if they thought that it was likely to work. If they haven't, shouldn't we take that as a sign that \"longevity medicine\" is nothing more than a buzzword? Why would we think that investing in \"longevity\"-branded research would be a better approach than investing in biomedical research generally? And if it seems implausible that investing in, say, cancer research, would be an effective AI X-risk intervention, why would we think \"longevity medicine\" would be worth looking at?</p><p>The argument that longevity medicine is an X-risk intervention proves too much. We can't predict if or how \"longevity Moore's Law\" will come about, and there's no reason to draw a line between the nascent \"longevity\" field and the mature \"biomedical\" field, rather than dividing up the categories some other way. It's begging the question of whether or not \"longevity\" medicine is an especially promising healthspan or lifespan generator. We don't actually have a great reason to believe that. If we did, then everybody would know it and there'd be a rush to supply it with resources. While it is receiving a sudden burst of resources, is that signal enough to give the longevity field creditability above (or on par with) traditional biomedicine? It's unclear.</p><p><strong>Critique 2: The argument is premature</strong></p><p>If we are trying to placate AI capabilities researchers by building the technologies they want without GAI, then the first thing we ought to do is find out what they want. We ought to look at corporate mission statements, the comments of AI capabilities researchers, and run opinion surveys.</p><p>They might want any number of things: the pleasure of professional achievement, wealth and power, the thrill of discovery, improvements to daily life even without life extension. Their beliefs might be quite exotic in some cases. In any case, the first step would be to find out, not make an assumption and rush ahead into longevity research based on that.</p><p><strong>Critique 3: Doing more harm than good</strong></p><p>If people were able to live much longer, that would mean that they could work much longer. In the short run, this mostly benefits people who are nearing the end of their working lives. AI safety is a small, new field mostly populated by young people. AI capabilities is a big, relatively well-established field. Although most AI capabilities researchers seem to be younger people, it may have a higher mean researcher age. Extending the length of people's working lives has the effect mainly of extending the careers of AI capabilities researchers.</p><p>If the argument that this achievement in longevity would persuade AI capabilities researchers to refocus on safety turns out to be wrong, then the effect of longevity medicine would be to disproportionately preserve the research effort into capabilities. It might even be stimulating to researchers who put no credence on AI as a potential threat: \"we've already shown that longevity is tractable, so now we absolutely <i>must</i> develop GAI as fast as possible to lock in and extend these gains!\"</p><p><strong>Furthering the conversation</strong></p><p>For those inclined to technical AI safety research or policy work, I think that\u2019s still their best bet. But for those with good personal fit for medicine, economics, sociology, anthropology, or engineering, I think it is worth developing the argument for longevity medicine with further critiques and rebuttals. Based on the state of the argument as it is now, \"maybe longevity medicine is an effective X-risk intervention, maybe it isn't, we just don't know\" is not the proper response.</p><p>The proper response to this uncertainty is \"this might be a tractable strategy for employing a whole new category of people on perhaps the most pressing problem in the world, and it might also be a risk factor for that problem. We should work <i>damned hard</i> to figure out which one it is.\"</p><p>I myself am going into longevity medicine because, on balance, I think that longevity medicine has a net positive expected value for AI safety, that it's an especially promising approach to biomedicine, and that it's a good fit for me. But I am not certain about this. I would welcome more attention being paid to both sides of this argument to help steer my own career trajectory, and to inform others making similar decisions. My aim here is to lay down a foundation for further discussion.</p>", "user": {"username": "AllAmericanBreakfast"}}, {"_id": "FnviTNXcjG2zaYXQY", "title": "How to store human values on a computer", "postedAt": "2022-11-04T19:36:52.660Z", "htmlBody": "<p>One of the main obstacles to building safe and aligned AI is that we don't know how to store human values on a computer.</p><p>Why is that?</p><p>Human values are abstract feelings and intuitions that can be described with words.</p><p>For example:</p><ul><li>Freedom or Liberty</li><li>Happiness or Welfare</li><li>Justice</li><li>Individual Sovereignty&nbsp;</li><li>Truth</li><li>Physical Health &amp; Mental Health</li><li>Prosperity &amp; Wealth</li></ul><p>We see that AI systems like GPT-3 or other NLP based systems use Word2Vec or other node networks / knowledge graphs to encode words and their meanings as complex relationships. This then allows software developers to parameterize language so that a computer can do Math with it, and transform complicated formulas into a human readable output. Here's a great video on <a href=\"https://www.youtube.com/watch?v=Fzz1HIigbxg\">how computers understand language using Word2Vec</a> . Besides that, <a href=\"https://conceptnet.io/\">ConceptNet</a> another &nbsp;great example of a linguistic knowledge graph.&nbsp;</p><p>So, what we have to do is find a knowledge graph that helps us encode human values, so that computers can better understand human values and learn to operate within those parameters, not outside of them.</p><p>My suggestion for how to do this is quite simple: Categorize each <i>\"positive value goal\"</i> according to the <strong>actionable, tangible methods and systems</strong> that help fulfill this positive value goal, and then contrast that with all the <strong>negative problems</strong> that exist in the world with respect to that positive value goal.</p><p>Using the language of the <a href=\"https://en.wikipedia.org/wiki/Instrumental_convergence\">instrumental convergence</a> thesis:&nbsp;</p><blockquote><p>Instrumental goals are the <strong>tangible, actionable methods and systems</strong> (also called <strong>solutions</strong>) that help to fulfill terminal goals.</p><p><strong>Problems </strong>on the other hand are descriptions of how terminal goals are violated or unfulfilled.</p></blockquote><p>There can be many solutions associated to each positive value goal, as well as multiple problems.</p><p>Such a system lays the foundation for a node network or knowledge graph of our human ideals and what we consider \"good\".</p><p>Would it be a problem if AI was instrumentally convergent on doing the most good and solving the most problems? That's something I've put up for discussion <a href=\"https://forum.effectivealtruism.org/posts/hycChZFhDQjcGcLXD/should-ai-focus-on-problem-solving-or-strategic-planning-why\">here</a> and I'd be curious to hear your opinion in the comments!</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a6ca3f87b619a88f1aec5bbbda478f5a849ddd1a5478cdea.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a6ca3f87b619a88f1aec5bbbda478f5a849ddd1a5478cdea.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a6ca3f87b619a88f1aec5bbbda478f5a849ddd1a5478cdea.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a6ca3f87b619a88f1aec5bbbda478f5a849ddd1a5478cdea.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a6ca3f87b619a88f1aec5bbbda478f5a849ddd1a5478cdea.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a6ca3f87b619a88f1aec5bbbda478f5a849ddd1a5478cdea.png/w_1000 1000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a6ca3f87b619a88f1aec5bbbda478f5a849ddd1a5478cdea.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a6ca3f87b619a88f1aec5bbbda478f5a849ddd1a5478cdea.png/w_1400 1400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a6ca3f87b619a88f1aec5bbbda478f5a849ddd1a5478cdea.png/w_1600 1600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a6ca3f87b619a88f1aec5bbbda478f5a849ddd1a5478cdea.png/w_1800 1800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a6ca3f87b619a88f1aec5bbbda478f5a849ddd1a5478cdea.png/w_1920 1920w\"></figure>", "user": {"username": "oliver_siegel"}}, {"_id": "W5taETbarXR9CQnEQ", "title": "How CEA approaches applications to our programs", "postedAt": "2022-11-04T19:02:36.499Z", "htmlBody": "<p>Our programs exist to have a positive impact on the world, rather than to serve the effective altruism community as an end goal. This unfortunately means EAs will sometimes be disappointed because of decisions we\u2019ve made \u2014 though if this results in the world being a worse place overall, then we\u2019ve clearly made a mistake. This is one of the hard parts about how EA is both a community and a professional space.</p><p>Naturally, people want to know things like</p><ul><li>Why didn\u2019t I get admitted to a conference, when EA is really important to me and I\u2019m taking actions inspired by EA?</li><li>Why didn\u2019t my friend get admitted to a conference, when they seem like a good applicant?</li></ul><p>We can understand why people would often like feedback on what they could have done differently or what they can try next time to get a better result. Or they just want to know what happened. When we have a specific idea about what would improve someone\u2019s chances (like \u201cyou didn\u2019t give much detail on your application, could you add more information?\u201d) we\u2019ll often give it.&nbsp;</p><p>But we get thousands of applications and we don\u2019t think it\u2019s the best use of our staff\u2019s time to give specific feedback about all of them. Often we don\u2019t have constructive feedback to give.</p><p>Many of the things that go into a decision are not easy to pin down \u2014 how well we think you understand EA, how we think you\u2019ll add to the social environment, how much we think you\u2019ll benefit from the event given the program we\u2019ve prepared, etc. These things are subjective, and in a lot of cases, reasonable people could disagree about what call to make. There are also cases where we\u2019ll just make mistakes (by our own standards), sometimes in favor of an applicant and sometimes against them.<br>&nbsp;</p><h2>How we communicate about our programs</h2><p>In responding to public discussion of our programs, sometimes we\u2019ve gotten more in the weeds than we think was ideal. We\u2019ve provided rebuttals or more information about some points but not others, which makes people understandably confused about how much information to expect from us and what the full picture is. It also uses a lot of our staff time. As the EA community grows, we need to adjust how we handle communications with the community.<br>&nbsp;</p><p>What you should expect from us going forward:</p><ul><li>When we think there are significant updates to our programs that the community should know about, or when there seems to be widespread confusion or misunderstanding about a particular topic, we\u2019ll likely write a post (like this one).</li><li>We\u2019ll likely be less involved in the comments or extended public back-and-forth.</li><li>We\u2019ll read much of the public feedback about our programs, and will definitely read feedback you send directly to us (unless it\u2019s extraordinarily long). We won\u2019t respond in-depth to much of the feedback, though.</li><li>Our programs are a work in progress, and we\u2019ll take feedback into account as we try to improve them.<br>&nbsp;</li></ul><p>What we hope you\u2019ll do:</p><ul><li>Feel free to express your criticisms, observations, and advice about our programs. This could be publicly if you think that\u2019s best, or by writing to us directly. Our&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/contact\"><u>contact form</u></a> can be filled out anonymously. Or you can reach specific programs at&nbsp;<a href=\"mailto:groups@centreforeffectivealtruism.org\"><u>groups@centreforeffectivealtruism.org</u></a>, <a href=\"mailto:forum@effectivealtruism.org\">forum@effectivealtruism.org</a>, or&nbsp;<a href=\"mailto:hello@eaglobal.org\"><u>hello@eaglobal.org</u></a>, or&nbsp;<a href=\"https://forms.gle/5dre18wbJyXbw5uw7\"><u>community health\u2019s form</u></a>.</li><li>In general, we think it\u2019s a good idea to fact-check public criticisms before publishing. If you send us something to fact-check, we\u2019ll try to do so.</li><li>If you think we\u2019ve missed important considerations or information on a decision, like about someone\u2019s application, you can send us additional information.<br>&nbsp;</li></ul><p>On events specifically:</p><ul><li>We try to run a range of events that serve the breadth of the community.&nbsp;</li><li>We recognize that there are people who are really dedicated to doing good, but whose approach isn\u2019t a good fit for all our events.</li><li>For example, people working all kinds of jobs and donating have historically been the lifeblood of EA. CEA wouldn\u2019t exist without these people. But EA Global is mostly geared toward people who are making other kinds of decisions.</li><li>There are a lot of options besides EAG. The EAGx conference series serves about twice as many people as the EAG conferences, has broader admissions standards, and takes place in a much wider variety of places (including virtually).&nbsp;<a href=\"https://www.givingwhatwecan.org/\"><u>Giving What We Can</u></a> has virtual events year-round connecting members. A lot of the action is in&nbsp;<a href=\"https://forum.effectivealtruism.org/community\"><u>local groups</u></a> around the world. Some groups have organized unconferences and other more social events. The&nbsp;<a href=\"https://forum.effectivealtruism.org/\"><u>EA Forum</u></a> and other online spaces are useful spaces to swap research, ideas, and advice.&nbsp;<a href=\"https://www.effectivealtruism.org/virtual-programs\"><u>Virtual programs</u></a> provide discussion spaces for thousands of people each year.</li></ul><p>More info about&nbsp;<a href=\"https://www.eaglobal.org/admissions/\"><u>events admissions</u></a>.</p>", "user": {"username": "AmyLabenz"}}, {"_id": "ctEhHxYH2a9Mrrx2f", "title": "Is AI forecasting a waste of effort on the margin?", "postedAt": "2022-11-05T00:41:42.573Z", "htmlBody": "<p><i>[Epistemic status: I am optimising for providing arguments worth considering, which means I'm not trying to make certain every argument is strictly valid. I'm not trying to be safe to defer to, so I'm not minimising false-positives. I am just trying to expand the range of available tools to explore this question with, so I'm </i><a href=\"https://www.lesswrong.com/posts/u8GMcpEN9Z6aQiCvp/rule-thinkers-in-not-out\"><i>minimising false-negatives</i></a><i>.]</i></p><hr><h3>Prize questions</h3><p>The <a href=\"https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize\">Future Fund's AI Worldview Prize</a> will award up to $1.5M for work that substantially changes their probabilities on the following three propositions.</p><ol><li><i>\u201cP(misalignment x-risk|AGI)\u201d: Conditional on AGI being developed by 2070, humanity will go extinct or drastically curtail its future potential due to loss of control of AGI</i></li><li><i>AGI will be developed by January 1, 2043</i></li><li><i>AGI will be developed by January 1, 2100</i></li></ol><p>See their post if you want the details. Here, I just want to offer some very broad and hasty arguments for why I'm hesitant about the value of AI forecasting in general, and this prize in particular.</p><hr><h3>Arguments</h3><ul><li>It's worth stating explicitly that an actual working technical solution to the alignment problem with <a href=\"https://forum.effectivealtruism.org/topics/alignment-tax\">low tax</a> would substantially update the panel's beliefs about proposition A. So this isn't <i>necessarily </i>a contest about <i>forecasting-arguments</i> even if it's (misguidedly, imo) presented as one.</li><li>I think this matters because I don't see forecasting as being a very targeted use of time for making the world better. In the worst case, this contest can make people less effective because it incentivises them to work on forecasting when they otherwise would have worked on generating solutions. If the latter is substantially more important on the margin, this contest is probably bad.<ul><li>This seems especially true in a research community with high rates of intrinsic motivation.<ul><li>I think a research community functions best when the people who have a well-formed opinion about what is the most effective thing for them to do, end up actually doing that thing. Especially in a <i>pre-paradigmatic field</i> like alignment, where there's <strong>no authority you can defer to that will safely ensure that you end up working on something worthwhile</strong>.</li><li>As long as external incentives are approximately smoothly distributed across tasks, their intrinsic motivation to do what <i>they </i>think is the most effective thing for them to do, is more likely to win out over their competing motivations.</li><li>So I'd be reluctant to introduce imbalanced incentives, because they might displace motivation to act on individual prioritisation.</li></ul></li><li>In general, introducing disproportionately strong incentives for a narrow subset of tasks in an area where task prioritisation has very broad uncertainty, seems bad.<ul><li><a href=\"https://en.wikipedia.org/wiki/Amdahl%27s_law\">Ahmdal's law</a>: <i>\"the overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used.\"</i></li><li>It's like running notepad--<i>and only notepad--</i>on a GTX 3080ti GPU.</li></ul></li></ul></li><li>These words don't have strict definitions, but I think of \"forecasting\" as spending optimisation on <i>predicting </i>what will happen, and \"problem-solving\" as trying to <i>change </i>what will happen (by generating new ideas and solutions). Forecasting is differentiating between what's already known, problem-solving is generating something that doesn't exist yet.<ul><li>Forecasters search broadly and <i>exploit </i>what's already written because that's the more reliable way to form informed opinions. They analyse more arguments than they generate, because the former is more cost-effective in terms of value of information related to the forecasting questions.</li><li>Problem-solvers <i>explore </i>unknown territory with uncertain (<a href=\"https://www.lesswrong.com/posts/u8GMcpEN9Z6aQiCvp/rule-thinkers-in-not-out\">hits-based</a>) payoff. If they're part of a research community that can do parallel search, this is the optimal strategy for generating novel solutions that everyone can benefit from, but it's not optimal for making the best forecasts.</li><li>Forecasting can be important for directing object-level work and choosing between different alignment strategies, and is therefore essential to the project. But it is a step removed from actually trying to develop a solution.</li><li>At the theoretical limit, you can be perfectly calibrated on predicting everything about <i>what will happen</i> related to AGI, even if you do no work that actually increases the chances that it ends up aligned.</li><li>It's questionable to what extent prioritisation between alignment strategies is sensitive to differences in timeline forecasts, as opposed to mostly just being sensitive to their technical plausibility in the first place. If so, efforts to prioritise between different strategies is better spent on researching their plausibility, and less on figuring out where they fit on the timeline. But I expect different people will have wildly different takes on this.<ul><li>Essentially, beware <a href=\"https://www.lesswrong.com/posts/uaPRRBGRjxZd6QePE/working-mantras\">wasted motion</a>.</li></ul></li><li>For an individual, creative brainpower spent on forecasting doesn't translate as readily into competence for problem-solving compared to brainpower spent directly on problem-solving.</li><li>Written work on forecasting does not inform work on problem-solving as readily as problem-solving work informs forecasting work.</li><li>What is more <i>robustly usefwl</i> across a range of the most plausible scenarios: work on forecasting or problem-solving? I think others are in a better position to answer this question than I am, but I'd nudge you to consider <a href=\"https://en.wikipedia.org/wiki/Amdahl%27s_law\">Ahmdal's law</a> again.</li></ul></li><li>Optimisation spent on predicting worthwhile forecasting questions is plausibly hitting diminishing marginal returns much faster than trying to generate solutions. I have no theoretical model to support this (though I feel like one might exist), I just expect it to be the case in practice.<ul><li>I expect promising research avenues targeted at producing solutions to <i>last longer </i>than promising research avenues targeted at estimating forecasting questions.</li><li>That is, if on the y-axis you plot the marginal value of information of spending one more hour researching a question, and on the x-axis you plot time spent researching it... I expect the distribution for AI forecasting questions to be <i>front-loaded</i>, at least as compared to AI problem-solving questions.</li><li>This relates to it being easier, in practice, to <i>verify</i> lines of thinking (e.g. while searching through existing literature as a foxy forecaster) compared to <i>generating</i> them in the first place.</li></ul></li></ul><hr><p><i>Thanks to Mihnea Maftei for some helpfwl discussion on this.</i></p>", "user": {"username": "Emrik"}}, {"_id": "H8RrdDHGBLQHHopMj", "title": "How Important Is This Century For Humanity?", "postedAt": "2022-11-04T16:08:51.638Z", "htmlBody": "<figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668027270/mirroredImages/H8RrdDHGBLQHHopMj/djfl6phqss5k2ytalnvr.jpg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/126e2cc1331652503533dd927427b34f8c27e8ee263504b2.png/w_210 210w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/126e2cc1331652503533dd927427b34f8c27e8ee263504b2.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/126e2cc1331652503533dd927427b34f8c27e8ee263504b2.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/126e2cc1331652503533dd927427b34f8c27e8ee263504b2.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/126e2cc1331652503533dd927427b34f8c27e8ee263504b2.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/126e2cc1331652503533dd927427b34f8c27e8ee263504b2.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/126e2cc1331652503533dd927427b34f8c27e8ee263504b2.png/w_1470 1470w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/126e2cc1331652503533dd927427b34f8c27e8ee263504b2.png/w_1680 1680w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/126e2cc1331652503533dd927427b34f8c27e8ee263504b2.png/w_1890 1890w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/126e2cc1331652503533dd927427b34f8c27e8ee263504b2.png/w_2048 2048w\"></figure><h2>The most important century is the one we are living in now.</h2><p>The decisions we make today will determine the course of humanity for generations to come. That is why it is so important that we have a clear understanding of our roles in this century. The 21st century has been dubbed the most important century to make things right. We must work together to solve the challenges of our time, whether it's wars, pandemics, climate change, poverty, or runaway technological advancements. The stakes are high, but so is our potential. By working together and making informed decisions, we can create a bright future for all.</p><p>&nbsp;</p><h2>We will look back in this century as a pivotal piece - either the future will be extremely good or worst.</h2><p>Wrong decisions have been made throughout history, but never before on such a large and global scale. We are at a tipping point and the future will be determined by the choices we make in the next few years. If we continue down the same path, the future will be extremely worst. The good news is that it's not too late to change course. We need to accept what it takes to make things right. It won't be easy, but if we're successful, this century will be remembered as a pivotal moment in history.</p><p>&nbsp;</p><p>What can stifle the conversations that may allow us to accept those roles we need to partake?</p><p><strong>Postmodernism.</strong> Postmodernism can be defined as a reaction against the arid intellectual climate of modernism. Postmodernists believe that progress is an illusion and that science is just another form of knowledge, no more valid than any other. This rejection of progress has had a stifling effect on the advancement of new sciences. Postmodernists argue that all knowledge is relative and that there is no such thing as objective truth. This leads to arejection of the scientific method, which relies on the accumulation of evidence to support or disprove hypotheses. As a result, postmodernists are often highly skeptical of new discoveries, dismissing them as \"mere\" facts that are unable to be verified. This skepticism can prevent new theories from gaining traction and being accepted by the scientific community. In addition, postmodernists often embrace conspiracy theories and alternative explanations for events, which can further inhibit progress. For example, postmodernists may argue that the moon landing never happened or that climate change is a hoax. As long as postmodernism continues to influence thinking, it is likely to impede scientific progress.</p><p><strong>Totalitarian Ideologies. </strong>Totalitarian Ideologies like communism, racism, populism can hinder innovation and discovery of new sciences for a number of reasons. For one, Totalitarian Ideologies tend to emphasize conformity over individuality. This can stifle creativity and prevent new ideas from taking root. Furthermore, Totalitarian Ideologies often seek to control information and knowledge, limiting access to new and innovative ideas. Finally, Totalitarian Ideologies can also lead to a fear of change and an aversion to risk-taking, both of which can impede scientific progress. In short, Totalitarian Ideologies can be a major obstacle to innovation and discovery.</p><p><strong>Oversimplification. </strong>In today's world, complex problems often get oversimplified. Whether it's politics, the economy, or even our personal lives, we seem to prefer easy answers and black-and-white thinking. This is understandable to a certain extent \u2013 after all, who has the time or energy to delve into the nuances of every issue? \u2013 but it can also be dangerous. oversimplification can lead to bad decisions, misunderstandings, and ultimately, disastrous consequences. As we face ever more complex challenges in the 21st century, it is more important than ever that we develop a deep understanding of the issues at hand. Oversimplification will not only prevent us from finding real solutions \u2013 it could also spell the end of humanity as we know it. So let's make a commitment to think deeply, question thoroughly, and never settle for easy answers. The future of our species depends on it.</p><p><strong>Bad leadership. </strong>There's an old saying that goes, \" bad leaders make for bad followers.\" This is certainly true when it comes to the destruction of civilizations. history is full of examples of civilizations that have been brought down by bad leadership. The Roman Empire, for instance, was destroyed from within by a series of bad emperors. And more recently, the Soviet Union collapsed under the weight of bad leadership. In both cases, the wrong people were in charge, and they ultimately led to the demise of their respective civilizations. Of course, there are many factors that can contribute to the downfall of a civilization. But there's no doubt that bad leadership is one of the most destructive forces out there.</p><p>&nbsp;</p><p>We all have a responsibility to each other, though we may not have realized it in a collective sense. Our pursuit of prosperity for everyone living now and future generations depends on each of us taking full responsibility for our ideas, spoken and written words and actions. By taking responsibility, we can have conversations about the roles we play in society. It is only through these conversations that we can hope to find solutions that work for everyone. So let's take responsibility for our own actions and start the conversation today. The future is brighter when we share the same respect and virtues across the planet.</p><p>&nbsp;</p><h2>Human Capital: How best to distribute our most precious resource for the next century?</h2><p>The future of humanity depends on our ability to adapt to changing circumstances. That's why adaptive human capital distribution is the key to our future success. By distributing our human capital across a wide range of skills and knowledge, we will be able to adapt to whatever challenges the future may hold. This adaptive approach will allow us to continue to thrive as a species, long after other less adaptable forms of life have perished. So remember: when it comes to the future of humanity, adaptive human capital distribution is the key.</p><p>&nbsp;</p><p>Develop A Highly Customized Educational Structure</p><p>In order to best distribute human capital, I believe that we need a Customized Educational Structure. This Customized Educational Structure will take into account the different strengths and weaknesses of each person, and will cater the education accordingly. It is only through an education that is tailored to the individual that we can hope to reach everyone's fullest potential. Furthermore, this Customized Educational Structure will also be necessary in order to meet the ever-changing needs of our society. Only by constantly updating our educational methods can we hope to keep up with the times.</p><p>&nbsp;</p><p>Distribution by Cognitive Capital</p><p>We also need to consider cognitive capital when distributing human capital. This is the idea that some people are just smarter than others. And while there are exceptions to every rule, generally speaking, it is true that some people are better equipped to handle certain tasks than others. For example, a physicist is going to be better at solving physics problems than someone who doesn't have a background in physics. By taking into account cognitive capital, we can make sure that the right people are doing the right jobs. And in this way, we can ensure that humanity's future is well-served.</p><p>&nbsp;</p><h2>Let us all do our part to build a better world for all.</h2><p>We should all do our part to build a better world for all. This means accepting responsibility for this century and doing what we can to make it a better place for everyone. We need to be more mindful of our actions and their consequences. We need to think about how our choices impact not only ourselves but also the world around us. We need to be more compassionate and understanding, and we need to work together to make this world a better place for all. Each of us has a role to play in making this happen, and it starts with each of us taking responsibility for our own actions. Let's all do our part to build a better world for all.</p>", "user": {"username": "Miguel"}}, {"_id": "CAWvSaGWNbCnoRccd", "title": "Supporting online connections: what I learned after trying to find impactful opportunities", "postedAt": "2022-11-04T17:34:40.507Z", "htmlBody": "<p>I\u2019ve been exploring ideas for how the Forum could help people make a valuable connection over the course of 4+ months. In this doc, I share the different areas I\u2019ve been thinking about, how promising I think they are and what I\u2019ve learnt about them.&nbsp;</p><p>Overall, I don't think any of the ideas meet the bar for focusing on connections via an online platform. My bar for an idea was that it could facilitate roughly an EAG\u2019s worth of quality-adjusted connections each year (~10,000). I\u2019m now more excited about meeting this bar by targeting high-value outcomes (e.g. collaborations, jobs), even if that means a lower number of total connections.</p><p>I cover:</p><ol><li>Overall take</li><li>Why did we explore connections?</li><li>What ideas have we explored?</li><li>FAQ: Why don\u2019t you build a year-round Swapcard?</li><li>Other ideas we didn\u2019t explore</li></ol><h1>Overall take</h1><p>I\u2019m more excited about targeting high-value outcomes that come from connections (e.g. collaborations, jobs) rather than trying to broadly facilitate conversations online.</p><ul><li>I wanted to find a product that could plausibly match a single EAG (<a href=\"https://forum.effectivealtruism.org/topics/effective-altruism-global\"><u>Effective Altruism Global</u></a>) in terms of the number of connections made in a year.<ul><li>A single EAG costs roughly 2 FTEs a year to run (excluding other costs).</li><li>If Sarah (my colleague) and I are choosing what to work on, you could imagine that the alternative we\u2019re trying to beat or match is running another EAG.</li><li>Note: this is a relatively ambitious target. I think it would be more achievable to build something that\u2019s still cost-effective but which has less potential for scale. For example, some of the ideas I explore below would be much more cost-effective than an EAG, because it's much cheaper to run online services than in-person conferences but would generate far fewer connections. [<a href=\"https://forum.effectivealtruism.org/posts/WCZtHvJQdvJxiedTY/in-current-ea-scalability-matters\"><u>Related Forum post.</u></a>]</li></ul></li><li>I\u2019ve found it hard to compete with EAG on number of \u201cconnections\u201d, where a connection means someone you feel you can ask a favour of (likely after a half-hour conversation).</li><li>At an EAG, each attendee makes an average of 8 connections via lots of 1:1 chats in an environment (in-person) optimised for good conversations.</li><li>To match this, we\u2019d need 10% of our monthly active readership to make 4 connections a year, which isn\u2019t crazy but feels like a stretch as only 8% of that number have ever sent a message on the forum.</li><li>I think it might be easier to compete with EAG on the downstream effects of connections, e.g. a person ended up collaborating on a paper, a person ended up getting hired.</li><li>My best guess is that EAGs get outcomes like this for something like 1-5% of connections. I think it\u2019s much easier for the Forum to get 100-500 additional concrete outcomes (like collaborations or jobs) a year, than 10k connections.</li></ul><p>The rest of this contains details on what we explored, why and our assessment of each idea.</p><h1>Why did we explore connections?</h1><p>We thought we should work on connections because:</p><ol><li>Connections are valuable<ol><li>They are cited as an important reason that people get involved in EA in the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/NP5B6yNMoZiZbmEQ8/open-phil-ea-lt-survey-2020-introduction-and-summary-of\"><u>Open Phil EA/Longtermist Survey</u></a><ol><li>35% of people said personal contact with EAs was important for them getting involved</li><li>38% said personal contacts had the largest influence on their personal ability to have a positive impact</li></ol></li></ol></li><li>We had some evidence that people are making valuable connections through the Forum, even though the Forum wasn\u2019t designed well for this<ol><li>The 2020 EA Survey revealed that a surprising number of people found connections through the EA Forum<ol><li><a href=\"https://forum.effectivealtruism.org/s/YLudF7wvkjALvAgni/p/Dhxg9BfBQEYvZETAD\"><u>5.9% of people</u></a> said they found a valuable connection through the Forum, where 11.3% said this of EAG (an event optimised for connections)<ol><li>Having followed up with a subset of the forum users who said this, I think it\u2019s more like that ~2% of respondents found a valuable connection</li><li>That said, it wouldn\u2019t surprise me if we saw a similar reduction in reported valuable connections for EAG if we were to follow up with those people. I just imagine that a bunch of people forget what they were referring to in a survey they took two years ago and fewer connections seem valuable with hindsight&nbsp;</li></ol></li></ol></li><li>People in our early user interviews said they found some valuable connections<ol><li>We reached out to some people who had sent many messages on the Forum</li><li>We spoke to 17 people and learned that people had found project collaborators, spotted new talent, and had people reach out to them for advice</li></ol></li></ol></li><li>If we could deliberately seek to enable more connections through the Forum, perhaps we could facilitate a lot more connections</li></ol><h1>What ideas have we explored?</h1><p>Caveats:</p><ul><li>Sometimes my reasons for not exploring something are weak and reasonable people disagree (including my teammates)</li><li>I suspect there are interesting angles to take on the ideas I\u2019ve dismissed</li><li>Sometimes I say that I\u2019m less enthusiastic about an idea because I think another group is working on it. In these instances, I don't necessarily think that more work is useless; competition can be really good, and we might be wrong about how much space existing projects are taking up.</li><li>My bar was to find an idea that could turn out to be as valuable as running one EAG a year. This is a high bar and arguably some of these ideas are still worthwhile pursuing though they fall below it, especially considering how cost-effective they are.</li></ul><p>I\u2019m sharing a table summary below, and you can see elaborations below.&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#c9daf8;border:1pt solid #000000;padding:5pt;vertical-align:top;width:120px\"><strong>Category</strong></td><td style=\"background-color:#c9daf8;border:1pt solid #000000;padding:5pt;vertical-align:top;width:120px\"><strong>Depth of exploration</strong></td><td style=\"background-color:#c9daf8;border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Assessment</strong></td><td style=\"background-color:#c9daf8;border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Explanation</strong></td><td style=\"background-color:#c9daf8;border:1pt solid #000000;padding:5pt;vertical-align:top;width:120px\"><strong>Evidence</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><a href=\"https://forum.effectivealtruism.org/posts/CAWvSaGWNbCnoRccd/supporting-online-connections-what-i-learned-after-trying-to#Get_cause_specific_career_advice\"><u>Get cause-specific career advice</u></a></p><p><br>&nbsp;</p></td><td style=\"background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;In depth&nbsp;</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top;width:120px\">Somewhat promising</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Real demand but not that much and I think hard to reach the scale of an EAG.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Ran a pilot for a&nbsp;<a href=\"https://forum.effectivealtruism.org/advice\"><u>biosecurity advice service</u></a>.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://forum.effectivealtruism.org/posts/CAWvSaGWNbCnoRccd/supporting-online-connections-what-i-learned-after-trying-to#Get_advice_from_a_professional\"><u>Get advice from a professional</u></a></td><td style=\"background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;In depth&nbsp;</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Somewhat promising</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Real demand but not enough. Worse than cause-specific advice.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Created an MVP blog post. Assessed EA London directory.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://forum.effectivealtruism.org/posts/CAWvSaGWNbCnoRccd/supporting-online-connections-what-i-learned-after-trying-to#Connecting_with_someone_who_lives_nearby\"><u>Connecting with someone who lives nearby</u></a></td><td style=\"background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;In depth&nbsp;</td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Not promising</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">After building a map of community members, we\u2019ve seen ~2 messages per month sent to someone on the map</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">We built a map&nbsp;<a href=\"https://forum.effectivealtruism.org/community#individuals\"><u>Community - EA Forum</u></a>.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://forum.effectivealtruism.org/posts/CAWvSaGWNbCnoRccd/supporting-online-connections-what-i-learned-after-trying-to#Advertising_jobs__still_exploring_\"><u>Advertising jobs</u></a></td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;Medium</td><td style=\"background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top\">Promising</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Software devs we surveyed weren\u2019t aware of jobs they could apply to, and 3/12 applied to a job.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Reviewed job ads on the forum. Ran a number of surveys.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://forum.effectivealtruism.org/posts/CAWvSaGWNbCnoRccd/supporting-online-connections-what-i-learned-after-trying-to#Recruiting_candidates__still_exploring_\"><u>Recruiting candidates</u></a></td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;Medium&nbsp;</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Somewhat promising</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Some orgs really wanted it. Others were less excited. Other recruiters are starting to fill this gap.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Spoke to 10+ recruiters.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://forum.effectivealtruism.org/posts/CAWvSaGWNbCnoRccd/supporting-online-connections-what-i-learned-after-trying-to#Help_online_group_organisers_get_their_members_to_engage\"><u>Help online group organisers get their members to engage</u></a></td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;Medium&nbsp;</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Somewhat promising</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Some interest in an alternative to Slack. People are using subforums.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Spoke to five online group organisers.&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/software-engineering/subforum#posts\"><u>A version of this is currently live.</u></a></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://forum.effectivealtruism.org/posts/CAWvSaGWNbCnoRccd/supporting-online-connections-what-i-learned-after-trying-to#Help_community_builders_connect_members_to_the_broader_EA_community\"><u>Help community builders connect members to the broader EA community</u></a></td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;Medium&nbsp;</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Worth pursuing for some uni groups but not for our team</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Most community builders have people they want to connect members with from their own network.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Spoke to 20 group organisers.</p><p><br>&nbsp;</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://forum.effectivealtruism.org/posts/CAWvSaGWNbCnoRccd/supporting-online-connections-what-i-learned-after-trying-to#Verifying_someone_for_a_grant\"><u>Verifying someone for a grant</u></a>&nbsp;</td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;Shallow&nbsp;</td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Not promising</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Not a big problem/time saver</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">2 calls with grant makers at EA Funds.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://forum.effectivealtruism.org/posts/CAWvSaGWNbCnoRccd/supporting-online-connections-what-i-learned-after-trying-to#Finding_projects_grantees_for_regrantors\"><u>Finding projects or grantees for regrantors</u></a></td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;Shallow</td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Not promising</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Not a hair on fire problem. People weren\u2019t actively looking outside their existing networks/communities.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">4 conversations with regrantors.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://forum.effectivealtruism.org/posts/CAWvSaGWNbCnoRccd/supporting-online-connections-what-i-learned-after-trying-to#Help_people_find_jobs\"><u>Help people find jobs</u></a></td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;Shallow</td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Not promising</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">People mostly seemed to feel ok about their job hunt (surprisingly).</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Spoke to 6 people from the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/swcBj4rBC4HvwmwfD/who-wants-to-be-hired-may-september-2022?commentId=3ek9Gsbcbqzp2QRga\"><u>\u201cWho wants to be hired\u201d</u></a> thread.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://forum.effectivealtruism.org/posts/CAWvSaGWNbCnoRccd/supporting-online-connections-what-i-learned-after-trying-to#Connecting_peers_with_mutual_interest\"><u>Connecting peers with mutual interest</u></a></td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;Shallow&nbsp;</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Somewhat promising&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">People join online groups for this reason but it\u2019s not clear how to nudge people to have conversations or if people want this.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Online Slack groups facilitate this but often not super active. Spoke to one student who was interested in this.&nbsp;</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://forum.effectivealtruism.org/posts/CAWvSaGWNbCnoRccd/supporting-online-connections-what-i-learned-after-trying-to#Help_people_figure_out_which_jobs_are_the_highest_impact\"><u>Help people figure out which jobs are the highest impact</u></a></td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;Shallow&nbsp;</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Somewhat promising&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">I think it would be good for&nbsp;<a href=\"https://jobs.80000hours.org/\"><u>80k\u2019s job board</u></a> to be clearer about which jobs are good for direct impact vs career capital and to highlight which are controversial, but I\u2019m not convinced it would make a big difference to what jobs people take.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Chat with 80k. Chat with Yonatan.</p><p>Posted&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9HAQqNw5yBYFuoZgc/a-couple-of-expert-recommended-jobs-in-biosecurity-at-the\"><u>a forum post</u></a> that highlighted an expert\u2019s view on top jobs.</p></td></tr></tbody></table></figure><p>&nbsp;</p><h1>FAQ: Why don\u2019t you build a year-round Swapcard?</h1><p>This has been suggested a number of times by different people, because:</p><ol><li>People use Swapcard during EAGs and this is seen as valuable</li><li>Offering the same service year-round might therefore be even more valuable</li><li>EA Hub was like a year-round Swapcard and was used a bit (~1 message sent/day)</li><li>Perhaps the EA Forum could do an even better job than EA Hub did (because it has more users and traffic)</li></ol><p>My view on this:</p><ol><li>The biggest issue with building a directory is getting people to actually use it.<ol><li>I think this is the biggest problem with many products.</li><li>See the \u201ccommunity map\u201d, which only has a couple of messages sent a month.</li></ol></li><li>A timebound conference creates an artificially highly liquid market for conversations.<ol><li>Demand - people have set aside two days to do nothing but talk to people. They request conversations they would never normally have the impulse to book.</li><li>Supply - people have set aside time to respond to requests and are more receptive to speak to people.</li><li>The fact that people don\u2019t use Swapcard much outside of the conference is evidence for this.</li></ol></li><li>Outside of a conference, I think it\u2019s much rarer that people think \u201cI should schedule a chat with X to get advice on my career\u201d (although it does happen).</li><li>So I think it\u2019s important to design a directory around a burning user need that people have outside of conferences (e.g. \u201cwe need to find candidates for a hiring round\u201d).<ol><li>I also think the features will look different depending on the user, and we should build around that.</li></ol></li></ol><p>I think there is also an argument that goes:</p><ol><li>Don\u2019t build a directory for a specific use case (e.g. getting specific career advice).</li><li>Instead build a general-purpose directory that will be used for multiple purposes e.g. making friends, reaching out about project ideas, getting general advice and recruitment.</li><li>This will benefit from network effects (you\u2019ll have more people using this).</li><li>You may also observe people using the directory in ways you don\u2019t expect.</li></ol><p>I feel pretty tempted by this argument, and it\u2019s nice to build stuff, but I think without a specific use case you\u2019re kind of just hoping people will use your new service.</p><p>It might be worth saying that&nbsp;<a href=\"https://forum.effectivealtruism.org/search?contentType=Users&amp;query=&amp;page=1\"><u>it is possible to search for Forum users</u></a> by keywords in their profile and any tags they\u2019ve added to their profile&nbsp;<a href=\"https://forum.effectivealtruism.org/search?contentType=Users&amp;query=&amp;page=1\"><u>here</u></a> \u2014 and you can message people! If this gets a lot of use and I learn of valuable outcomes from this search, it could change my mind about whether we should spend more time on something like a general-purpose directory.</p><h1>Ideas explored in depth</h1><p>We explored these ideas in some depth, building prototypes that people used. All these ideas showed some promise, so I could imagine changing my mind about them.</p><h2>Get cause-specific career advice</h2><p><strong>Why?&nbsp;</strong></p><p>When people are looking for advice they often want to speak to people who know the cause area. One of the ways that 80,000 hours provide value is referring coachees to cause area experts who can bring them up to speed and make further introductions. It seemed plausible that it would be useful for people to reach out to these experts directly.</p><p>This was one of the use cases I tested most thoroughly. We&nbsp;<a href=\"http://forum.effectivealtruism.org/advice\"><u>ran a pilot</u></a> to connect people interested in biosecurity with more senior people in biosecurity (currently doing PhDs or working in the field). If you\u2019re thinking of doing something like this, I\u2019d be happy to share more info and discuss.</p><p><strong>Take</strong></p><p>I think this was a pretty useful thing to do, but the results after running for a month weren\u2019t amazing and I couldn\u2019t see a way to scale it up to be EAG-level good.</p><p>Others at CEA disagreed with my overall assessment of the pilot and thought the service could be worth continuing to run, even if at a small scale.</p><p>It\u2019s possible someone else should take this on as a side project, or individual interest groups (e.g. consulting, psychology) should have something like this.</p><p><strong>Evidence</strong></p><ul><li>I asked someone senior to review the talent attracted and outcomes in terms of plan changes and they thought it was approximately break-even on advisors\u2019 time.</li><li>On the other hand, there was demand from advisees, advisors enjoyed the calls and there were some signs of minor plan changes.</li><li>It's possible that the time the advisors spent mentoring didn't trade off against working hours, in which case there was less downside.</li></ul><h2>Get advice from a professional</h2><p><strong>Why?</strong></p><p>We\u2019d heard of people reaching out to forum authors to ask for advice. I think getting career advice from a professional is one of the common conversations people have at EAG. I had a guess that people would reach out if they felt they had permission to. We were very close to launching this, but then decided the numbers wouldn\u2019t be large enough for it to be worth it.</p><p><strong>Take</strong></p><p>There was a real demand for a professional advice program, but I don\u2019t think enough to warrant scaling one up. I think this was worse than the biosecurity advice pilot we ran, because we had no filter on who could apply and so people took it less seriously.</p><p><strong>Evidence</strong></p><p>We did a cheap test by listing people in different careers on a forum post:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/C3g5Hyxst4jwScjmi/book-a-chat-with-an-ea-professional\"><u>\u201cBook a chat\u201d with an EA professional</u></a>. The results looked quite a bit worse than the biosecurity advice pilot:</p><ol><li>There were fewer exciting people who weren\u2019t already involved in the community.</li><li>There were fewer reported plan changes.</li></ol><p>That said, it was promising that some subset of advisors were happy to do many calls and felt they were a decent use of time. I think there\u2019s something to be said for giving people permission to reach out to the subset of senior people who enjoy doing these calls. A counterargument is that it\u2019s possible these senior people aren\u2019t valuing their time highly enough and we\u2019re distracting them. It should be noted that most of them did not want to spend more time having EAG-like conversations (they saw EAG as their give-back budget).</p><p>I also looked into how the&nbsp;<a href=\"https://effectivealtruism.uk/community\"><u>EA London directory</u></a> was used, because it seemed like the best existing example of the get advice service. It seemed like it was most often used when someone was going through a career transition (e.g. finishing a PhD or switching from Finance to Software) and wanted to speak to someone who worked in that area to get practical advice and leads for jobs. This got a surprisingly low uptake though, given it was heavily promoted by the EA London community leaders. We estimated that 40 messages were sent a year and that 2.5% of visitors to the page sent a message. If 2.5% of all monthly Forum visitors (25k) sent a message, this would be 500 messages a year, which seems optimistic and wouldn\u2019t be close to our target. We\u2019d need about 8x as many messages.</p><h2>Connecting with someone who lives nearby</h2><p><strong>Why?</strong></p><p>This seems like one of the key features of the EA Hub. I think having a friend who is interested in EA can be very motivating and matching people can lead to new EA groups. So we&nbsp;<a href=\"https://forum.effectivealtruism.org/community#individuals\"><u>created a map for Forum users</u></a> who wanted to share their approximate location.</p><p><strong>Take</strong></p><p>The feature hasn\u2019t been used much. It\u2019s possible we should delete this feature, as it\u2019s distracting to have.</p><p>You could argue that we should have done more to promote it and worked harder to get more people on the map - the usefulness of this is of course only as valuable as the number of people on it. But I think we did a decent job promoting, and demand is just genuinely low.</p><p><strong>Evidence</strong></p><p>We created the map in May, and a couple messages a month are sent through the map.</p><h1>Ideas explored in medium depth</h1><p>We spoke to a decent number of users and/or ran tests, so we feel somewhat confident about our positions on these ideas.</p><h2>Advertising jobs (still exploring)</h2><p><strong>Why?</strong></p><p>We know posts on the Forum have played a role in people applying for jobs, and we think this might be one of the main sources of the Forum\u2019s impact.</p><p><strong>Take</strong></p><p>I think we could play a role in generating more leads for recruiters. I particularly think there might be an opportunity to do this with EAG attendees as well as the Forum. It seems like there are a decent number of people who aren\u2019t aware of jobs that are a match for them or can be nudged to consider jobs.</p><p><strong>Evidence</strong></p><ul><li>Top-level posts on the Forum and a couple of posts in the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/DXcg6N6CGvRA2vrCk/who-s-hiring-may-september-2022-closed\"><u>\u201cWho\u2019s hiring\u201d</u></a> thread led to people being hired.<ul><li>We\u2019re still gathering data on this - if anyone is interested in the details, feel free to message me.</li></ul></li><li>It\u2019s too early to say whether anyone has actually been hired from our recent experiments, but we know people have applied.<ul><li>We sent a survey asking software developers if they were interested in 4 jobs \u2014 3/12 applied for a job we listed.</li><li>We sent a similar survey asking EAGxRotterdam attendees interested in entrepreneurship to consider applying to jobs and 13/100 emailed applied to a job.</li><li>We posted job ads in the comments on the Forum - these mostly didn\u2019t get many clicks and we think it's unlikely they resulted in new applicants, though we are waiting to hear back from the hiring managers.</li></ul></li></ul><h2>Recruiting candidates (still exploring)</h2><p><strong>Why?</strong></p><p>Helping hiring managers find counterfactual hires seems particularly valuable. We\u2019d heard of people spotting talent through well-written Forum posts. We also know that the EA Hub was used by hiring managers to source candidates.</p><p><strong>Take</strong></p><p>I think there are a subset of hiring managers who are very excited about new sources of leads. Others have a relatively easy time hiring and don\u2019t need additional help. There are a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/5JaKAuRtxcbxjeLfD/gaps-and-opportunities-in-the-ea-talent-and-recruiting\"><u>number of recruitment projects starting</u></a> and I suspect they will fill the gap, but I think we may be in a position to help provide lists of candidates for recruiters.</p><p><strong>Evidence</strong></p><p>When we spoke to 10+ hiring managers about recruiting people, it didn\u2019t seem like we found a hair-on-fire problem. About half were uninterested in receiving a long list of candidates. A couple were very enthusiastic. This mixed reaction, paired with it looking like other people were going to enter the space, made us back away.</p><p>That said, we still think CEA\u2019s data (from people who have agreed to share it with other organisations) would be useful for recruiters (though perhaps not as its own product) and we are working on making data available to hiring organisations where users have explicitly opted-in to sharing.</p><h2>Help online group organisers get their members to engage</h2><p><strong>Why?</strong></p><p>I spoke to a couple of people who made valuable connections through Slack groups. We wondered if we could help online group organisers better connect people online. The theory being that perhaps individual members of an online group (e.g.&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/3sRZkuQDbss3vwddk/ea-creatives-and-communicators-slack\"><u>EA Creatives &amp; Communicators</u></a>) don\u2019t have a burning need to better connect, but the group organisers might have a stronger desire to help people connect.</p><p><strong>Take</strong></p><p>There was some interest from online group organisers in having an alternative to the many Slack groups. It seems plausible that the EA Forum could host this, and this is being experimented with currently (<a href=\"https://forum.effectivealtruism.org/topics/software-engineering/subforum#posts\"><u>see Subforums</u></a>).</p><p><strong>Evidence</strong></p><p>Some online group organisers we spoke to were interested, and subforums are getting some use.</p><h2>Help community builders connect members to the broader EA community</h2><p><strong>Why?</strong></p><p>We explored how we could help group organisers connect their members to the broader EA community. I was excited about this based on chatting with David Nash, who connects new EA London members to relevant people in the community on a daily basis. David uses his personal LinkedIn to do this so it seemed like replicating this could be valuable.</p><p><strong>Take</strong></p><p>Unfortunately, after chatting to 20+ group organisers, it turns out:</p><ol><li>They don\u2019t connect as many new people as David.</li><li>They prefer to connect people within their existing network.</li><li>Some prefer to connect people within the group.</li></ol><p>However, I think smaller university groups that have leaders without an established network could benefit from some kind of directory. It's possible that the CEA groups team will take something like this on in future, depending on other priorities.<br>&nbsp;</p><p><strong>Evidence</strong></p><ol><li>We spoke to larger local groups and national groups, and mostly they prefer to connect people to each other locally<ol><li>We spoke to UCL EA, EA Cambridge, EA Netherlands, EA DC, EA NY, EA London</li><li>Prefer to use their own network because it\u2019s easier to make the ask</li><li>Prefer to introduce someone people can reconnect with easily/bump into locally</li></ol></li><li>A genuine opportunity is providing a network of experts for smaller local groups to refer members to<ol><li>We spoke to two smaller local groups (EA Durham, EA Warwick) who lacked their own personal network and lacked experts in the local area, and so were very enthusiastic about being able to connect students to knowledgeable EAs who they knew were open to chat</li><li>A spreadsheet like this used to exist among local group leaders a few years ago, and it was used</li></ol></li></ol><h1>Ideas explored in shallow depth</h1><p>We only had a couple of calls about each of these ideas, and I could imagine changing my mind about them.</p><h2>Verifying someone for a grant</h2><p>We thought the Forum could be helpful for verifying grantees and seeing their work, but this didn\u2019t seem very important after chatting with a few people. At best it saved grantors some time, but didn\u2019t make the difference between granting or not.</p><h2>Finding projects/grantees for regrantors</h2><p>We thought it might be helpful to source potential projects for regrantors to fund. We were excited about regrantors because there are a lot of them. After speaking to a few, it seemed like most were not interested in spending additional time seeking out opportunities, and instead funded projects that they came across organically as part of their work.</p><h2>Help people find jobs</h2><p>We spoke to 6 people sourced from the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/swcBj4rBC4HvwmwfD/who-wants-to-be-hired-may-september-2022?commentId=3ek9Gsbcbqzp2QRga\"><u>\u201cWho wants to be hired\u201d</u></a> thread and people who ticked \u201cseeking work\u201d in their Forum profile. Mostly people seemed to feel ok about their job hunt and didn\u2019t seem to need additional help. I\u2019m a bit surprised by this as I would have guessed that finding an EA-related job is hard. That said, the sample size was very small so I wouldn\u2019t be surprised if we changed our mind on this.&nbsp;</p><h2>Connecting peers with mutual interest</h2><p>There are a number of existing initiatives to help people connect around interests e.g.&nbsp;<a href=\"https://forum.effectivealtruism.org/community#online\"><u>online Slack and Facebook groups</u></a>, someone in the EAG DC Slack created a channel for software engineers.</p><p>One user we spoke to wanted to meet more peers (students) interested in biosecurity because there were a limited number in his university. We were pretty excited about this and contacted 16 students who were interested in biosecurity to ask if they wanted to connect with him. He had calls with 3 of them and got value out of the conversations. I got the impression that this provided a different kind of value to talking to someone more senior as</p><ol><li>You have more space to ask dumb questions and express doubts</li><li>You\u2019re more likely to build a mutual relationship that\u2019s valuable for your network</li></ol><p>In response to this, we added a list of \u201cpeers\u201d in biosecurity on the biosecurity advice page. Unfortunately, no messages were sent on the back of this section. This doesn\u2019t feel like a super fair test because we didn\u2019t advertise the page very well (it has had ~500 unique page views since we launched and 38 profiles were viewed), but I think getting traffic to this page would have been hard and even if we did well, we could probably expect to get a similar number of messages sent as the community map gets (a couple a month).</p><p>I could imagine the subforums helping with this. Perhaps also EA chats would be useful.</p><h2>Help people figure out which jobs are the highest impact</h2><p>Yonatan has highlighted&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/YCMgg6x6zWJmran5L/criticism-of-the-80k-job-board-listing-strategy\"><u>problems with 80k\u2019s job board here</u></a>. I thought the Forum might be able to help by creating a space for discussion around which jobs were the highest impact. I spoke to Yonatan more about this and to Kush, who manages the 80k job board.</p><p>Overall I think it would be good for 80k\u2019s job board to be clearer about which jobs are good for direct impact vs career capital and to highlight which are controversial, but I\u2019m not convinced it would make a big difference to what jobs people take.</p><h1>Other ideas, not explored</h1><p>Here\u2019s a list of ideas we didn\u2019t explore because I wasn\u2019t excited, but possibly we or someone else should explore them in the future.</p><h2>Build a network with algorithmically assigned chats</h2><ul><li>\u201cEA Chats\u201d was done in the past and sounds like it went quite well. Vaidehi was happy for us to survey participants to find out what the outcomes are 3 years on. It\u2019s possible she\u2019d be happy for others who are thinking about doing this to do the same.</li><li>I feel less optimistic about this than other people seem to. I\u2019m sceptical that algorithmically assigned chats (even if based on interests) would turn out very well. I also think that people tend to drop off these services over time.</li><li>I\u2019m happy that EAGxVirtual and EA Anywhere facilitate this.</li></ul><h2>Help people find co-founders for projects</h2><ul><li>This doesn\u2019t seem to work in the for-profit world and I don\u2019t expect the EA world to be much different.</li><li>I don\u2019t think matching is the bottleneck for founders.</li></ul><h2>Make friends with people with similar interests</h2><ul><li>Seems potentially very valuable to link people without a local group.</li><li>That said, I\u2019m sceptical of how well online friendships will do.</li><li>I think&nbsp;<a href=\"https://forum.effectivealtruism.org/groups/YeW2gwh4gHexYQBjs\"><u>EA Anywhere</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/H7hTZbedyjLKDwtqT/announcing-the-ea-gather-town-event-hall-as-a-global-hub-for\"><u>EA Gathertown</u></a> partially address this need.</li><li>Maybe reading groups are the best approach? It would be interesting to follow up with Virtual Programmes participants to ask.</li><li>Some people are experimenting with a slack channel called \u201cStudy Buddy\u201d which connects people who are interested in studying something together. I think this seems like a decent approach.</li></ul><h2>Get advice on a thesis topic</h2><ul><li>I would guess Effective Thesis has this covered.</li></ul><h2>Group organisers want to know which speakers they can invite to their group</h2><ul><li>This seems helpful to me, but it\u2019s not a way to connect lots of people.</li></ul><h2>Professionals want to build their network to find jobs</h2><ul><li>I like the two ways we have to do this:<ul><li>Networking events, where attendees have to clear a bar to enter.</li><li>Forum posts where you establish credibility through the quality of your writing.</li></ul></li><li>There may be others.</li></ul><h2>EA related orgs want to know recommended professionals for outsourced jobs</h2><ul><li>Orgs might want to find someone to do a specific task e.g. build a simple website or give legal advice.</li><li>I don\u2019t think this is great for connecting lots of people.</li><li>A nice feature would be that professionals who are building career capital can have a way to stay connected with EA.</li></ul><h2>Following up with contacts made in person</h2><ul><li>Maybe making it easier to follow up with people post-EAG would make EAG 10% more valuable?</li></ul><h2>Reaching out for a favour</h2><ul><li>One Forum user said he preferred to reach out to people on the Forum because it felt more personal. Maybe we could facilitate more interactions?</li><li>Maybe there are missed opportunities, where people don\u2019t know each other\u2019s emails, and so don\u2019t get in touch.</li></ul><h2>Other</h2><ul><li>Ben West had a bunch more ideas in this post:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/HtZfhWz7feAiTzaGQ/creating-individual-connections-via-the-forum\"><u>Creating Individual Connections via the Forum</u></a>.</li></ul>", "user": {"username": "imben"}}, {"_id": "gAwR8PkZ6fvayXEdK", "title": "A newcomer\u2019s guide to the technical AI safety field", "postedAt": "2022-11-04T14:29:47.471Z", "htmlBody": "", "user": {"username": "zeshen"}}, {"_id": "QtrjaTpfj3agvF3iW", "title": "Give For Good: make your donations to charity more effective", "postedAt": "2022-11-04T13:40:47.669Z", "htmlBody": "<p>Dear EA enthusiasts,&nbsp;</p><p>We recently established Give For Good: a new donation platform for charities. Our goal is to generate a stable, annual income for charities. At the same time, we help donors increase the impact they make with the same amount of donations. We do this by investing people's donations in sustainable and social stocks and transferring the interest each year to charity. This way, each of your donations becomes a mini investment fund for the charity you choose. Over time, this results in MUCH MORE impact for your favorite charities + the impact goes on for good!</p><p>We are a charity/nonprofit ourselves too, registered in the Netherlands.&nbsp;</p><p>Would be great if you want to have a look on our website! Feedback and donations (!!!!) always welcome. Posting it here because it fits well within the effective altruism and longtermism philosophies.</p><p><a href=\"https://www.giveforgood.world/\">www.giveforgood.world</a></p><p>best,</p><p>Rik Viergever</p><p>Founder @ Give For Good</p>", "user": {"username": "Rik"}}, {"_id": "a6qYjKTmbSAGJM8GT", "title": "What should my research lab focus on in the first week of 2023?", "postedAt": "2022-11-04T10:16:41.932Z", "htmlBody": "<p>I'm crowdsourcing the decision on my lab's research focus.&nbsp;</p><p>Since some of the options relate to EA (e.g., welfare metrics, longterm developments, and decision-making), I'm also interested in the EA community's opinion.</p><p>By investing ~5 minutes, you can help me make the right decision:</p><p><a href=\"https://sandstorm.pik-potsdam.de/#/joinpoll/https%3A%2F%2Fsandstorm.pik-potsdam.de%2Fcouch%2F/none/c9047076/b6c534a6a71a332c\">--&gt; go to poll</a></p><p>(you can click \"guest account\" if you don't want to register)</p><p>You can even add further options...</p><figure class=\"image image_resized\" style=\"width:26.93%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5ebb37d91be82f8a5fc178a3824ae012dd9330f39cc88d0d.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5ebb37d91be82f8a5fc178a3824ae012dd9330f39cc88d0d.png/w_127 127w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5ebb37d91be82f8a5fc178a3824ae012dd9330f39cc88d0d.png/w_207 207w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5ebb37d91be82f8a5fc178a3824ae012dd9330f39cc88d0d.png/w_287 287w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5ebb37d91be82f8a5fc178a3824ae012dd9330f39cc88d0d.png/w_367 367w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5ebb37d91be82f8a5fc178a3824ae012dd9330f39cc88d0d.png/w_447 447w\"></figure>", "user": {"username": "Jobst Heitzig (vodle.it)"}}, {"_id": "unwLjhGqvaGfMvKxc", "title": "The Letten Prize - An opportunity for young EA researchers?", "postedAt": "2022-11-04T18:47:34.045Z", "htmlBody": "<p>Hi,</p><p>Are you a young researcher who conducts research aimed at addressing global challenges within the fields of health, development, environment and equality in all aspects of human life?</p><p>Or do you know of someone who might fit this description?&nbsp;</p><p>If so, you/they can apply for The Letten Prize.</p><p>Prize money at 2,5 MNOK (~235 000 USD).</p><p>Interested? View the criteria for applying here:&nbsp;<br>https://lettenprize.com/criteria/</p><p><strong>Disclamers:</strong><br>* I've been involved with the EA community since 2017, eg. established EA Oslo and served on the board of EA Norway.&nbsp;<br>* I am currently working freelance for The Young Academy of Norway, as well as serving on the board of the Letten Foundation. This research prize are a result of a collaboration by the two parties. I see why this may be seen as \"spam\". However, I truly believe that we have several great candidates within our community who could contend for the prize.<br>* I will serve as the secretary of the prize committee, but have no influence on the committees decision.</p>", "user": {"username": "H\u00e5kon Sandbakken"}}, {"_id": "nA9CqpoCgMpguPCqa", "title": "Righting past wrongs by recovering past lives via simulation", "postedAt": "2022-11-04T08:18:31.515Z", "htmlBody": "<p><i>This is a quick write-up of a 'person-affecting' &amp; preference utilitarian idea that came to my mind (a normally totally total and hedonistic utilitarian). I don't think this is worth pursuing due to x-risk related priorities seeming overwhelming more important but I thought the idea was sufficiently interesting to share.</i></p><p><strong>TLDR: Maybe in future we will want to \"revive\" past people by simulating a </strong><i><strong>lot</strong></i><strong> of digital people such that many of those digital people will be more or less identical to many past people? The intuition is that this way we can partially redeem the tragically suboptimal lives that specific individuals today and in the past had to endure.</strong></p><p>A friend of mine once mentioned that he\u2019s leaving as many of his thoughts in recoverable digital form so it would be more feasible to reconstruct his mind digitally at some point in the future. This would allow a very close digital version of him to live a fulfilled utopian life even after his biological body and mind will have disappeared.&nbsp;</p><p>My reaction back then was to doubt that his written words would allow a particularly close copy, such that this whole idea is more or less irrelevant as humanity would not be able to realistically recover people and there\u2019s consequently less reason to spend large amounts of resources on this in the future. But the idea of <strong>allowing past people, especially those who had to experience a lot of tragedy and suffering and unfulfilled potential, to live another time under conditions very conducive to a fulfilled life</strong> stuck with me.</p><h2>Feasibility of recovering current and past people without perfect brain scans</h2><p>Recently I reconsidered the feasibility of recovering specific past people digitally because of three thoughts that randomly crossed my mind:</p><p>A) I hadn\u2019t considered before that a utopian version of humanity might be able to <strong>spend astronomical amounts of compute on generating a lot of close-by copies of past humans</strong>, such that at least one of the copies will likely be a very close version of the actual person.</p><p>B) I hadn\u2019t considered how <strong>much information about our minds is shared among all humans</strong>, such that for each person there is already a reasonable baseload of information available.</p><p>C) I hadn\u2019t considered that <strong>recovering the DNA of a person probably would supply a decent chunk of information</strong> about the mind of that person.</p><p>Many people today leave fairly big digital footprints via things like chats with friends, pictures, personal notes, recorded behaviour on platforms such as Google and Facebook, and additionally genetic codes. My completely made-up guess is that such a person today could be reasonably considered recovered if there are somewhere between one thousand to one million plausible digital copies created for him or her in future? A lot of details about the person will never be instantiated, but the conditions of an ethical redemption for a given past person might still be met? One criterion might be that the past person would likely have said that they recognize themself in one of the digital minds after doing a very extensive assisted Ensuring test.</p><h2>Some open questions</h2><ol><li>Has this topic been discussed somewhere?</li><li><strong>How likely can you recover past and currently living people based on the available information</strong> such that one can honestly say that this past person has been recovered to live the life that this person would find very fulfilling?<ol><li>I imagine there might be some function that estimates a rough interaction of i) the amount of information available for a given person, and ii) the number of copies that would make successful recovery very likely.</li></ol></li><li><strong>Related to 2., how much compute would it cost to recover people from the past</strong>?<ol><li>Maybe some fraction of the people from the past hundred years could be recovered fairly \u201ccheaply\u201d based on written notes and diaries, reports from surviving relatives, pictures, genetic samples, biographical knowledge. In contrast, people whose genes we can\u2019t recover and who didn\u2019t leave much autobiographical text might be very expensive to allow another life.</li></ol></li><li><strong>How much less good would the simulated past people\u2019s lives be compared to the kinds of lives and minds that will be possible in the future?</strong><ol><li>If the difference in quality of experiences would be big, then simulating past lives would come with immense opportunity costs.&nbsp;</li><li>E.g. I can imagine that many past people were shaped into personalities that are not conducive for living a happy live. But I also imagine that those people would than choose to change their minds into forms that are conducive for their flourishing.</li></ol></li><li><strong>Is \u201crighting past wrongs\u201d a reasonable ethical position?</strong><ol><li>I\u2019m fairly sympathetic to the idea that personal identity is not a helpful and reality-carving concept and often leads to confusion.</li><li>But I also have a strong intuition that I would support any person whose life has been cut short due to unjust circumstances (e.g. murder or disease) or whose live was largely unfulfilling and stricken with suffering to be offered a second chance.</li></ol></li><li><strong>Are there some cheap options available today to enable recovering more people?</strong><ol><li>E.g. one might preserve DNA samples of people who might die soon or who have died in the past, one might invest more in preserving personal documents of the deceased in some form, one might invest in a service to file the memories of people.</li></ol></li></ol><p>Thanks Jasper and Tilman for feedback, I will try a little harder to preserve my memories of both of your wonderful souls, just in case.</p>", "user": {"username": "MaxRa"}}, {"_id": "9tgLeoCFg4ey7zcFJ", "title": "Latest consensus on COVID-19 public health measures", "postedAt": "2022-11-04T12:15:43.225Z", "htmlBody": "<p>For those that are interested, a new paper has been published in Nature setting out the latest health and social policy recommendations to end the threat of COVID19.&nbsp;</p><p>It is the product of a 15-month multidisciplinary collaboration involving 386 experts (health, NGO, government &amp; other ), and consists of 41 consensus statements and 57 recommendations to governments, health systems, industry and other key stakeholders.</p>", "user": {"username": "Cecilieae"}}, {"_id": "mmsEAyvrrrQz5yE27", "title": "Major update to EA Giving Tuesday", "postedAt": "2022-11-04T03:10:24.338Z", "htmlBody": "<p><strong>Announcing changes to EA Giving Tuesday and Meta's Giving Season match</strong><br><br><strong>EDIT: Meta is actively updating the terms of the match, so this information could be out of date, please see </strong><a href=\"https://www.facebook.com/help/332488213787105\"><strong>https://www.facebook.com/help/332488213787105</strong></a><strong> for current details - the EA Giving Tuesday team will try our best to keep </strong><a href=\"https://www.eagivingtuesday.org\"><strong>EAGivingTuesday.org</strong></a><strong> up to date. We are actively exploring how we can coordinate around the match in a way that makes sense.</strong></p><p>On Nov 1st 2022, Meta announced a significant change to their annual Giving Tuesday donation matching scheme, which affects EA Giving Tuesday.</p><p>Here are the high level details of this year\u2019s match:</p><p><i>\u201cTo help nonprofits jumpstart their Giving Season fundraising, Meta will match your donors\u2019 recurring donation 100% up to $100 in the next month (up to $100,000 per organization and up to $7 million in total across all organizations). All new recurring donors who start a recurring donation within November 15 - December 31, 2022 are eligible.&nbsp;</i><a href=\"https://socialimpact.facebook.com/giving-season-match-terms/\"><i><u>Read the terms and conditions.</u></i></a><i>\u201d</i></p><p>The match now requires participants to set up a recurring donation, in order to get up to $100 in matched funds. The matched funds are provided once the second transaction goes through i.e. you need to donate for two months to receive the match. <i>Edit: As of 4 Nov: We are unsure but it seems possible that a donor could set up recurring donations to multiple organizations (up to 200) in order to get multiple matches (for a total of up to $20,000 in matches).</i></p><p>We believe this opportunity is only available in the US due to the functionality appearing to be US only.<br>&nbsp;</p><p><strong>What does this mean for EA Giving Tuesday?</strong></p><p>In the past, the value proposition of EA Giving Tuesday was to organise around the 100% match in the morning of Giving Tuesday. With the lower match amount per donation and the requirement for it to be recurring, we think that the matching is much less likely to be competitive and therefore the previous level of coordination does not make sense to continue.</p><p>The EA Giving Tuesday team has also decided that it makes the most sense for people to donate directly to the charities via Facebook given the new requirement about recurring donations. These changes mean that EA Giving Tuesday will not support any organisations that require re-granting or restrictions due to ongoing administrative requirements of recurring donations. 501c3\u2019s registered with Facebook fundraising tools will be able to participate in Meta\u2019s Giving Season match.&nbsp;</p><p>We encourage you to look for effective charities on Facebook for the match and will be listing&nbsp;<a href=\"https://www.eagivingtuesday.org/eagtnonprofits\"><u>effective charities who are interested in participating on our website</u></a>.</p><p>EA Giving Tuesday will share the details of any matching opportunities we think are worthwhile and conduct an impact analysis at the end of the season.&nbsp;</p><p>&nbsp;</p><p><strong>How can I get my donations matched this year?</strong></p><p>This year there are two match opportunities we are sharing with you:&nbsp;</p><ul><li>Meta\u2019s Giving Season match:&nbsp;<a href=\"https://www.eagivingtuesday.org/home\"><u>Details and instructions</u></a> (Starts Nov 15)</li><li>Every.org\u2019s Fall Giving Challenge:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/oMBKeyx7ir8Jnbenz/make-a-usd50-donation-into-usd100-6x\"><u>Details and instructions</u></a>, provided by Will Kiely on the EA Forum (Already started)</li></ul><p>Once you\u2019ve received confirmation of a match, please let us know the details via&nbsp;<a href=\"https://forms.gle/QjE8DuTfYfmUq1Sa7\"><u>this impact evaluation form</u></a> so we can quantify the value of these opportunities for future years.<br>&nbsp;</p><p>We\u2019re disappointed that the match has changed significantly from the previous year, but we hope you find value in the matching opportunities from both Meta and Every.org.</p><p>We will continue to search for new matching opportunities that have the ability to shift donations towards highly effective charities throughout the season.</p><p>You can read more about EA Giving Tuesday at <a href=\"https://www.eagivingtuesday.org/\">EAGivingTuesday.org</a></p><p>Grace and the EA Giving Tuesday Team 2022</p><p><br>&nbsp;</p>", "user": {"username": "Giving What We Can"}}, {"_id": "XxWsAw7DefKipzRLc", "title": "My summary of \u201cPragmatic AI Safety\u201d ", "postedAt": "2022-11-05T14:47:47.462Z", "htmlBody": "<p>This post is my summary of <a href=\"https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt\">\u201cPragmatic AI Safety\u201d</a> on <a href=\"https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt/p/n767Q8HqbrteaPA25\">complex systems</a> and <a href=\"https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt/p/dfRtxWcFDupfWpLQo\">capabilities externalities</a>.&nbsp;</p><p>&nbsp;</p><p>To examine AI safety as a complex system means to understand that various aspects of the problem are too interconnected to effectively be broken down into smaller parts and too organized from the perspective of statistics. As a result, traditional methods of reductionism and statistical analysis respectively are inadequate. The insights from complex systems study help reframe the problem of how to make AI safer with consideration for the multiple dimensions of the problem. The first insight is that if we aim at solving alignment, we should have a broader and more inclusive definition of impact. This means that we should be mindful of the value of contributing factors that aren\u2019t, strictly speaking, \u201cdirect impact\u201d, i.e., researchers working on the technical/mathematical/engineering safety solutions. The accurate description of systemic factors makes the value of their effect clearer even when such an effect doesn\u2019t point to a specific measurable outcome, e.g., trying a new set of experiments. For AI safety, forecasting and rationality have an evident positive effect that is difficult to measure as \u201cincreasing the intelligence of the system\u201d (where the system is the safety community). Analyzing the AI x-risk at the societal level is also fruitful. Bettering people\u2019s epistemics should generally make them better Bayesian thinkers with the obvious benefits that follow. Moreover, how people think about \u201ctail risks\u201d i.e., rare risks is currently a problem in dealing with AI x-risk.&nbsp;</p><p>It\u2019s crucial to contextualize the AGI x-risk by developing a safety culture. Safety won\u2019t become the community norm immediately; it\u2019s necessary to have a good understanding of what safety entails as well as develop the infrastructure for AI safety research. The criticisms against AI safety which come both from AI ethics (bias, equality, etc.) and the broader discourse, call for strengthening the safety community and increasing its reliability.&nbsp;</p><p>Who gets to choose AI safety research directions is another key contributing factor. So far, it hasn\u2019t been easy to attract top researchers based solely on pecuniary incentives: these people are internally motivated. Thus, we should prioritize making research agendas as clear and interesting as possible so that such researchers are incentivized to pursue them. Having well-defined problems also helps avoid trying to convince technopositive ML researchers that their work might be extremely dangerous. There are many reasons why safety has been neglected so far, and being attentive to them will increase our chances of success. It is then critical to detect the contributing factors of the highest value.</p><p>One important observation is that deep learning has many complex system-type features. Furthermore, the research community can be viewed as a complex system as well, just like the organizations that work in the area. The usefulness of this approach relies on the predictive power of complex system models. Importantly, the thorough study of a complex system does not predict its failure mode. This emphasizes the urgent need for more empirical work. Moreover, the crucial aspects of the system are discovered by accident meaning that there are no explicit rules to follow in the pursuit of scientific discovery. Current research agendas treat intelligent systems as mathematical objects while it makes more sense to represent them in terms of complex systems. It\u2019s worth noting that we should not expect larger systems to behave like smaller systems because scaling brings new qualitative features to the surface. Consequently, proposals to first align smaller models and then scale them up to bigger ones are misleading. All these lead to thinking that it\u2019ll be best to diversify our research priorities. With diversification, and because of the high uncertainty of AGI research, we allow ourselves not to give conclusive answers to difficult/uncertain questions and instead optimize for doable research tasks that decrease x-risk.</p><p>Research should have a tractable tail impact and avoid producing capabilities externalities. Some mechanisms that lead to tail impact include multiplicative processes (judging and selecting researchers and groups of people based on a variety of factors), preferential attachment (investing in doing well in one\u2019s research career early on), and the \u201cedge of chaos\u201d heuristic (transforming a small piece of a chaotic area into something ordered). From the edge of chaos heuristic, it follows that researchers should do only one or two non-standard things in a project to avoid getting to something deviating too much from current norms to be understood, or on the other extreme, something that is not original.&nbsp;</p><p>At the same time, we should ensure that AI research conditions are not conducive to existential catastrophes. In other words, moments of peril are likely to make human actors worse at decision-making and thus, it\u2019s critical to build a safety framework as early as possible. When the crisis occurs, it\u2019ll be difficult to have reliably effective solutions. Besides, problem-solving consists in itself of multiple stages to be successful. With that aim in mind, scaling laws of safety should be improved relative to capabilities. Improving the slope is one way to improve scaling laws. This could be accomplished by getting more research work done which will be shaped by ideas on changing the type of supervision, the data resources, and the compute resources, to name some of them. The safety of systems will rely on minimizing the impact of errors and the probability of x-risk rather than expecting that our systems will be flawless. To make future systems safer then, thinking in the limit should not be taken to the extreme. Revisiting Goodhart\u2019s Law helps with this: while metrics do collapse, we shouldn\u2019t assume that all objectives are unchangeable and will inevitably collapse. And since metrics will not always represent all our values, we should try to shape the AGI\u2019s objective by including a variety of different goods. It isn\u2019t necessary, however, that offensive capabilities i.e., optimizing for a proxy, will be significantly better than defensive capabilities i.e., punishing agents for optimizing for the proxy and this can be shown through real-life examples with human actors companies pursuing their profit-related goals and governments implementing laws. When it comes to systems more intelligent than us, however, we should anticipate that no matter how many laws and regulations we try to establish, the system will always find a way to pursue its own goals. This is because rules are fragile and more-intelligent-than-the-designer-systems will exploit whatever loopholes they discover. Nevertheless, in an adversarial setting where the agent isn\u2019t way more intelligent than the judiciary, we can expect that standards such as \u201cbe reasonable\u201d will apply.&nbsp;</p><p>Human values are, of course, difficult to represent, and so Goodhart\u2019s Law applies to their proxies. As a result, many proxies aiming at good results could lead to doom. Now, when thinking about AGI, it is preferable not to limit research by assuming a hypothetical agent, but instead, consider soon-to-be models. The possible world where we mathematically model the superintelligent agent with high accuracy is a rare one. Given that we might live in a world where we must increase safety without guarantees, we should be prepared for different scenarios of one or more superintelligences posing x-risks. It\u2019s central then to develop safety strategies and invest in multiple safety features as early as possible and not rely on future technical advancements as absolute solutions to the problem. As capabilities advance, we may argue that the more capable a system is, the more it\u2019ll understand human values.&nbsp;</p><p>Agents with advanced capabilities will not necessarily be just, kind, or honest. A promising observation is that some capabilities goals have safety externalities, and the reverse: research aiming at safety can be advancing capabilities. For example, when we want to get truthful models we essentially have three goals: accuracy, calibration, and honesty. Accuracy is a capability goal while calibration and honesty are safety goals. As a practical proposal, we should emphasize machine ethics and train models that behave according to actual human values, not models that simply have learned task preferences. Human values, however, are difficult to represent and ethical decision-making is challenging. A moral parliament could help by providing a framework for deciding under uncertainty and factoring in as many normative parameters as possible in the fast-moving world of future AI.&nbsp;</p>", "user": {"username": "eangelou"}}, {"_id": "um9jhdvab34q3kdHz", "title": "Are alignment researchers devoting enough time to improving their research capacity?", "postedAt": "2022-11-04T00:58:21.370Z", "htmlBody": "", "user": {"username": "Carson Jones"}}, {"_id": "yP84zhg8k7MvEptbp", "title": "Resources for Physical Engineers", "postedAt": "2022-11-03T23:23:04.471Z", "htmlBody": "<p><strong>TL;DR</strong>: High Impact Engineers (HI-Eng) has compiled a&nbsp;<a href=\"http://highimpactengineers.org/resources\"><u>repository of resources for physical engineers</u></a> to learn more about how to have impact with their careers and we want to&nbsp;<a href=\"http://highimpactengineers.org/resource-suggestions\"><u>know what we've missed</u></a>.</p><p><i>Note: any mention of 'engineers' or 'engineering' in this post pertains to physical (ie. non-software) engineering.</i></p><h2>The Why</h2><p>It is commonly the view within EA that moving people down the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/PbtXD76m7axMd6QST/the-funnel-or-the-individual-two-approaches-to-understanding#The_Funnel_Model\"><u>funnel</u></a> towards direct work on impactful causes is one of the more impactful outcomes of community building.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefobu8m5x6h59\"><sup><a href=\"#fnobu8m5x6h59\">[1]</a></sup></span>&nbsp;One of the assumptions of HI-Eng is that there are not enough engineers ready to do impactful work now and there will likely be a greater undersupply in the future if nothing is done to increase the number of engineers ready to do impactful work.</p><p>Although the most impactful career trajectories for engineers could be pivoting into&nbsp;<a href=\"https://80000hours.org/career-reviews/engineering/\"><u>AI safety technical research, software engineering, or earning to give</u></a>, we believe there are still many high-impact opportunities for engineers who want to stay in engineering (for example, in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Bd7K4XCg4BGEaSetp/biosecurity-needs-engineers-and-materials-scientists\"><u>biosecurity</u></a> or&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BJtekdKrAufyKhBGw/ai-governance-needs-technical-work\"><u>AI governance</u></a>). Therefore, we see facilitating engineers transitioning to a more impactful career trajectory as one of our core pathways to impact. One way we hope to do this is the provision of information with which engineers can make informed decisions about how to do impactful work over the course of their careers.</p><p>&nbsp;</p><h2>The Resources</h2><p>Our first iterative step down this path is our&nbsp;<a href=\"http://highimpactengineers.org/resources\"><u>Resources for Engineers</u></a> page - a collection of resources relevant to engineers that we've collated over the last few months at HI-Eng. We list a handful of cause areas with a brief explanation of their relevance to engineers and some key resources to learn more. Additionally, we offer&nbsp;<a href=\"http://highimpactengineers.org/resources#full-resource-list\"><u>a more comprehensive list of resources</u></a> that are filterable by engineering discipline, cause area, and more. It is our hope that this can serve as a central repository for resources that help engineers to learn more about the cause areas and projects that are relevant to them having greater impact with their career. We will be continuously adding to it and expanding the cause area list as we find more resources and we want your help to do so!</p><h2>What are we missing and how can you help?&nbsp;</h2><p>We are certain we are missing relevant resources so we set up&nbsp;<a href=\"http://highimpactengineers.org/resource-suggestions3\"><u>this form</u></a> for anyone to recommend resources to add to our repository. If you have resources that may be relevant for physical engineers, please recommend them.</p><p>We'll review every suggestion before making a decision on whether to share it on our site, so if you're unsure about a resource to suggest we would like you to&nbsp;<strong>err on the side of recommending it.</strong></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnobu8m5x6h59\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefobu8m5x6h59\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For example, to keep up with and reduce the <a href=\"https://80000hours.org/2021/07/effective-altruism-growing/#changes-in-the-overhang-how-quickly-has-funding-grown-compared-to-people\">funding overhang</a> in EA.</p></div></li></ol>", "user": {"username": "Sean Lawrence"}}, {"_id": "wLsbtxQyBT6aqz8yZ", "title": "[Video] How having Fast Fourier Transforms sooner could have helped with Nuclear Disarmament - Veritasium", "postedAt": "2022-11-03T20:52:39.921Z", "htmlBody": "<p>Veritasium presents an argument that if we'd had Fast Fourier Transforms sooner, this would have made it possible to detect underground nuclear tests without on-site audits, somewhat better nuclear non-proliferation treaties could have been signed, and nuclear proliferation would have been slowed.</p><p>This was really interesting, I hadn't heard the story of these very early, hopeful, tragically failed attempts at nuclear disarmament. I'm troubled to learn that the Soviets rejected the Baruch Plan, measures analogous to <a href=\"https://forum.effectivealtruism.org/posts/kB2DK6fpmbGCxe4ED/appendix-to-a-safe-path\">my geopolitical AGI non-proliferation story</a>. They saw on-site audits as \"espionage\". They saw this proposal to form an international nuclear authority as a threat to their sovereignty. To some extent, it could have been. There's a lot to chew on there.</p><p>A particularly interesting detail is that Fast Fourier Transforms actually <i>were</i> discovered earlier, but then forgotten, so no one can argue for fatalism here, it was not inevitable that we'd end up with this progress differential. Other histories were possible.</p>", "user": {"username": "MakoYass"}}, {"_id": "KWGMP9fcLMhEoetsH", "title": "Berlin EA Shenanigans November (unaffiliated) - please RSVP", "postedAt": "2022-11-03T20:29:07.463Z", "htmlBody": "<p>There were a lot of new people last time (our 2nd meetup) so don't worry if you've not been there before.</p><p><a href=\"https://forum.effectivealtruism.org/users/severin-t-seehrich-1\">Severin</a> will be holding an Active-Hope-Workshop this time. Thanks!</p><p>We want to start the workshop at 18:30 and it will take ~90 minutes so plan accordingly.</p><p>Here is his description:</p><blockquote><p>Active Hope workshops were developed in the 70s by environmental and peace activists to deal with the exhaustion, isolation, and despair that can come up when staring at the suffering of the world. The general vibe is sort of similar to an authentic relating games night. It generally happens in four phases:<br>1. Gratitude practice, in order to start resourced<br>2. Sharing how you feel about what's going on in the world<br>3. Taking time to shift your perspective on that, e.g. by leaning into what the future might look like if things go well<br>4. Going forth: Planning a tiny concrete action to make that more likely.</p></blockquote><p>You're very welcome even if you\u2019ve never been to a meetup or you feel like you don't fit.</p><p>Route to TEAMWORK: <a href=\"https://docs.google.com/document/d/17nZqyRuuE4qPZyqWUqSpThMaANhQvb8IwwWoJ3YHrRM\">https://docs.google.com/document/d/17nZqyRuuE4qPZyqWUqSpThMaANhQvb8IwwWoJ3YHrRM</a></p><p>PS: Comment here or PM me if you want to be invited to the \"Berlin EA shenanigans\" Signal group.</p>", "user": {"username": "Milli"}}, {"_id": "PyZCqLrDTJrQofEf7", "title": "How bad could a war get?", "postedAt": "2022-11-04T09:25:43.878Z", "htmlBody": "<p><i>Acknowledgements: Thanks to Joe Benton for research advice and Ben Harack and Max Daniel for feedback on earlier drafts.</i></p><p><i>Author contributions: Stephen and Rani both did research for this post; Stephen wrote it and Rani gave comments and edits.</i></p><p>Previously in this series: \"<a href=\"https://forum.effectivealtruism.org/posts/mBM4y2CjfYef4DGcd/modelling-great-power-conflict-as-an-existential-risk-factor\">Modelling great power conflict as an existential risk factor</a>\" and \"<a href=\"https://forum.effectivealtruism.org/posts/aSzxoj7irC5jNHceB/how-likely-is-world-war-iii\">How likely is World War III?</a>\"</p><h1>Introduction &amp; Context</h1><p>In \u201c<a href=\"https://forum.effectivealtruism.org/posts/aSzxoj7irC5jNHceB/how-likely-is-world-war-iii\"><u>How Likely is World War III?</u></a>\u201d, Stephen suggested the chance of an extinction-level war occurring sometime this century is just under 1%. This was a simple, rough estimate, made in the following steps:</p><ol><li>Assume that wars, i.e. conflicts that cause at least 1000 battle deaths, continue to break out at their historical average rate of one about every two years.&nbsp;</li><li>Assume that the distribution of battle deaths in wars follows a power law.&nbsp;</li><li>Use parameters for the power law distribution estimated by Bear Braumoeller in&nbsp;<i>Only the Dead</i> to calculate the chance that any given war escalates to 8 billion battle deaths</li><li>Work out the likelihood of such a war given the expected number of wars between now and 2100.</li></ol><p>Not everybody was convinced. Arden Koehler of 80,000 Hours, for example,&nbsp;<a href=\"https://80000hours.org/problem-profiles/great-power-conflict/\"><i><u>slammed</u></i></a> it as \u201c[overstating] the risk because it doesn\u2019t consider that wars would be unlikely to continue once 90% or more of the population has been killed.\u201d While our friendship may never recover, I (Stephen) have to admit that some skepticism is justified. An extinction-level war would be 30-to-100 times larger than World War II, the most severe war humanity has experienced so far.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4o0kwuodjpf\"><sup><a href=\"#fn4o0kwuodjpf\">[1]</a></sup></span>&nbsp;Is it reasonable to just assume number go up? Would the same escalatory dynamics that shape smaller wars apply at this scale?</p><p>Forecasting the likelihood of enormous wars is difficult. Stephen\u2019s extrapolatory approach creates estimates that are sensitive to the data included and the kind of distribution fit, particularly in the tails. But such efforts are important despite their defects. Estimates of the likelihood of major conflict are an important consideration for cause prioritization. And out-of-sample conflicts may account for most of the x-risk accounted for by global conflict. So in this post we interrogate two of the assumptions made in \u201cHow Likely is World War III?\u201d:</p><ol><li>Does the distribution of battle deaths follow a power law?</li><li>What do we know about the extreme tails of this distribution?</li></ol><p>Our findings are:</p><ul><li>That battle deaths per war are&nbsp;<i>plausibly&nbsp;</i>distributed according to a power law, but few analyses have compared the power law fit to the fit of other distributions. Plus, it\u2019s hard to say what the tails of the distribution look like beyond the wars we\u2019ve experienced so far.</li><li>To become more confident in the power law fit, and learn more about the tails, we have to consider theory: what drives war, and how might these factors change as wars get bigger?</li><li>Perhaps some factors limit the size of war, such as increasing logistical complexity. One candidate for such a factor is technology. But while it seems plausible that in the past, humanity\u2019s war-making capacity was not sufficient to threaten extinction, this is no longer the case.</li><li>This suggests that wars could get very, very bad: we shouldn\u2019t rule out the possibility that war could cause human extinction.</li></ul><h1>Battle deaths and power laws</h1><h2>Fitting power laws</h2><p>One way to gauge the probability of out-of-sample events is to find a probability distribution, a mathematical function which gives estimates for how likely different events are, which describes the available data. If we can find a well-fitting distribution, then we can use it to predict the likelihood of events larger than anything we\u2019ve observed, but within the range of the function describing the distribution.</p><p>Several researchers have proposed that the number of battle deaths per war is distributed according to a&nbsp;<i>power law</i>. Power law distributions are described by the formula&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(x) \u221d&nbsp;x^{-\u03b1}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">\u221d</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u03b1</span></span></span></span></span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>. This means that the probability of event&nbsp;<i>x</i> is proportional to the value of&nbsp;<i>x</i> raised to the power&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"-\u03b1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u03b1</span></span></span></span></span></span></span></span></span>. These distributions look like this:</p><figure class=\"image image_resized\" style=\"width:75.3%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668679717/mirroredImages/PyZCqLrDTJrQofEf7/gin5uv2yosns0kzdufza.png\"><figcaption>Figure 1. Power law distributions of various scaling parameters</figcaption></figure><p>\u03b1, the&nbsp;<i>scaling parameter</i>, determines the shape of the distribution. As the figure above shows, the smaller the \u03b1, the longer the tails of the distribution. There\u2019s a simple way to interpret&nbsp;this parameter: if one event is twice as large as another, it is&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"2^\u03b1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u03b1</span></span></span></span></span></span></span></span></span></span></span>&nbsp;times less likely.</p><p>Long tails are an important feature of power laws. Events many orders of magnitude larger than the mean or median observation are unlikely, but not impossible. However, we need to be careful when fitting power laws. Other long-tailed distributions may fit the same data, but imply different probabilities for out-of-sample events. Since the tails of power laws are so long, improperly fitting this distribution to our data can lead us to overestimate the likelihood of out-of-sample events.</p><h2>Power laws and conflict data</h2><p>Rani conducted an informal literature review to assess the strength of the evidence for a power law distribution of war deaths. You can find a summary of her review, including the goodness-of-fit tests,&nbsp;<a href=\"https://docs.google.com/document/d/14N28kkLCMowZrx5rhHt4czKVd3P2LOMUxDdeEmnuZfs/edit?usp=sharing\"><u>here</u></a>.</p><p>She found six papers and one book in which a power law distribution is fitted to data on battle deaths per war. In each case, a power law was found to be a plausible fit for the conflict data. Estimates for the &nbsp;of conflict death data range from&nbsp;<strong>1.35 to 1.74</strong>, with a mean of 1.60. Anything in this range would be considered a relatively low value of &nbsp;(most empirical power laws have &nbsp;values between 2 and 3). In other words, if these estimates are accurate, the tail of the battle death distributions is scarily long.</p><figure class=\"image image_resized\" style=\"width:68.93%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668679717/mirroredImages/PyZCqLrDTJrQofEf7/wyk0kwzq0wonr7r2dg5e.png\"><figcaption><i>Figure 2. Clauset (\u201cTrends and fluctuations in the severity of interstate wars\u201d)\u2019s power law model of war severity. Note that the axes are logarithmic and do not start at 0.</i></figcaption></figure><p>How scary exactly? In&nbsp;<i>Only the Dead</i>, political scientist Bear Braumoeller uses his estimated parameters to infer the probability of enormous wars. His distribution gives a 1 in 200 chance of a given war escalating to be twice as bad as World War II and a 3 in 10,000 chance of it causing 8 billion deaths (i.e. human extinction).</p><p>That\u2019s bad news. If wars continue to break out at their historical-average rate of one every two years, Braumoeller\u2019s estimates imply a ~18% chance of war twice as bad as World War II and a ~1.2% chance of an extinction war occurring between now and 2100.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgkur2xqb56g\"><sup><a href=\"#fngkur2xqb56g\">[2]</a></sup></span></p><h2>Breaking the law</h2><p>Before we all start digging bunkers, though, it\u2019s worth highlighting a few caveats.</p><p>First, and most importantly,&nbsp;<strong>only two papers in the review also check whether other distributions might fit the same data.</strong>&nbsp;<a href=\"https://arxiv.org/pdf/0706.1062.pdf\"><u>Clauset, Shalizi, and Newman (2009)</u></a> consider four other distributions,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9goblfdkit8\"><sup><a href=\"#fn9goblfdkit8\">[3]</a></sup></span>&nbsp;while&nbsp;<a href=\"https://www.tandfonline.com/doi/full/10.1080/10242694.2015.1025486\"><u>Rafael Gonz\u00e1lez-Val (2015)</u></a> also considers a lognormal fit. <strong>Both papers find that alternative distributions also fit the Correlates of War data well.</strong> In fact, when Clauset, Shalizi, and Newman compare the fit of the different distributions, they find no reason to prefer the power law.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvwbt23seeds\"><sup><a href=\"#fnvwbt23seeds\">[4]</a></sup></span>&nbsp;</p><p>Second, there are potentially important data limitations. Wars are relatively rare, especially large wars that kill an appreciable fraction of the world, and death counts are uncertain for many wars. The small sample size makes our estimates of the underlying distributions more uncertain. It also means that our parameter estimates are sensitive, and could change in important ways if uncertain death counts are revised. Unfortunately, our estimates of the probability of extreme tail events are super sensitive to changes in these parameters.</p><p>Third, most of the papers we considered use the Correlates of War dataset. Analyses using other datasets may produce different parameter estimates. The CoW dataset also excludes many countries from its dataset in the pre-WWII period.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkygml14apl\"><sup><a href=\"#fnkygml14apl\">[5]</a></sup></span>&nbsp;If, for some reason, the excluded countries were more likely than average to fight especially large or small wars, then the estimates using these data may be biased.</p><p>Fourth, analyses that group together wars over time implicitly ignore potential changes in the distribution over time. Perhaps the probability distribution for deaths in a 21st century war is different from the distributions for 20th or 19th century wars. Whether the frequency or severity of war has been changing over time is a complicated question.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv7oh73ii55\"><sup><a href=\"#fnv7oh73ii55\">[6]</a></sup></span>&nbsp;Economic growth, globalization, technological change, and political and social institutions are dynamic and all plausibly influence the conduct of war.</p><h1>What does the tail of the distribution look like?</h1><p>To summarize the previous section: battle deaths are plausibly distributed according to a power law. But there is some evidence that other distributions also fit the same data. We also have to contend with the fact that we have a small sample size and the distribution could be changing over time.</p><p>Suppose, though, that we could establish with confidence that battle deaths are power law distributed. Extrapolating would <i>still </i>pose a problem. Power laws with&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\u03b1<2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u03b1</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span>&nbsp;have an infinite mean. But the range of the battle death distribution can\u2019t stretch to infinity. Its upper bound must be, at most, the global population. And it could even be lower than this, with the distribution bending downwards at some point between the largest war we\u2019ve observed and the logical limit.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreff1imbiiy7b7\"><sup><a href=\"#fnf1imbiiy7b7\">[7]</a></sup></span>&nbsp;We can't tell if or where the distribution is capped based on data alone. We need to consider \u201cphysical motivating or theoretical factors.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2ox57m5q5iu\"><sup><a href=\"#fn2ox57m5q5iu\">[8]</a></sup></span></p><p>Cioffi-Revilla and Midlarsky (2004), for example, argue that enormous wars are less likely than they should be according to a power law.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6iq0ta93u6n\"><sup><a href=\"#fn6iq0ta93u6n\">[9]</a></sup></span>&nbsp;They venture two &nbsp;explanations for this observation. First, they suggest that as a larger proportion of the population is drawn into the war, public pressure on the government to end the war grows. This \u201cdemocratization\u201d of war limits the extent to which they can scale. Second, the international system is \u201cfinite\u201d. As wars escalate they become more complex. They suggest that the diplomatic, strategic, and logistical dynamics of enormous wars may be too difficult to manage.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3zuy3zb9jfm\"><sup><a href=\"#fn3zuy3zb9jfm\">[10]</a></sup></span></p><p>While we don\u2019t find either of those explanations fully convincing, at least Cioffi-Revilla and Midlarsky are&nbsp;<i>trying&nbsp;</i>to sketch the tail of the distribution. This is a rich vein for future research. We\u2019d be very interested in seeing analyses that try to connect the observed distribution of outcomes to insights from the IR literature on models of escalation. These would give more insight into the shape of the tail.</p><p>Here we want to touch on a narrower, and perhaps distinctly EA, claim. Are there&nbsp;<i>technological&nbsp;</i>reasons to think the distribution of outcomes is bounded at a severity level below extinction? In other words, do we have the weaponry to kill ourselves in war?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefznvgsvwdg3\"><sup><a href=\"#fnznvgsvwdg3\">[11]</a></sup></span></p><h2>Weapon technology as a mechanistic factor</h2><p>In the 19th century, the first century included in the Correlates of War dataset, it\u2019s plausible that technology limited the degree to which wars could escalate. To be sure, some extremely bloody wars were still fought. In the Paraguayan War of 1864-70, for example, it\u2019s plausible that more than half of Paraguay\u2019s population was killed (though estimates are highly uncertain). 5% of the combined population of Paraguay, Brazil, Argentina, and Uruguay died in that war, making it the most intense conflict since 1816 at least even before accounting for civilian deaths.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref73ryqgz3467\"><sup><a href=\"#fn73ryqgz3467\">[12]</a></sup></span>&nbsp; But it surely stretches credulity to think that this war, or even larger conflicts like the Napoleonic Wars, could have continued escalating until they threatened all of humanity.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefds3j5k76wdi\"><sup><a href=\"#fnds3j5k76wdi\">[13]</a></sup></span></p><p>But in a world of ICBMs, biological agents, kamikaze drones, and autonomous systems, technology is less of a constraint.</p><figure class=\"image image_resized\" style=\"width:91.66%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668679717/mirroredImages/PyZCqLrDTJrQofEf7/je9ev81biyx7voglgf86.png\"><figcaption><i>Figure 3. Source: Luke Muehlhauser, \u201cHow big a deal was the Industrial Revolution?\u201d, Luke Muehlhauser (blog), https://lukemuehlhauser.com/industrial-revolution</i></figcaption></figure><p><strong>Nuclear weapons&nbsp;</strong>may have the potential to cause a nuclear winter that threatens extinction, though this is controversial and surprisingly understudied. On pp. 84-91 of&nbsp;<a href=\"https://assets.ctfassets.net/x5sq5djrgbwu/1cT1AsYCIomlUhOque0JgW/74ffb0b1a4803e1f9654e86e2c7f3af1/Great_Power_Conflict_report_-_Founders_Pledge.pdf\"><u>the Founders Pledge report</u></a>, Stephen expressed skepticism about this (though did not rule it out). We\u2019ve become slightly more concerned after the Metaculus Nuclear Horizons forecasts, which put worryingly high probabilities on extreme death counts following nuclear exchanges. Like, a&nbsp;<a href=\"https://www.metaculus.com/questions/8382/1000-nuke-detonations-cause-4b-deaths/\"><u>45% chance</u></a> of &gt;4 billion deaths given 1000 detonations or more. That seems too high: there aren\u2019t that many forecasts, and a similar question puts just a 2% chance of extinction in the same scenario. It seems we should have some credence on the possibility that an all-out nuclear war could cause extinction, though it may be unlikely.</p><p><strong>Emerging technologies</strong>, chiefly bioweapons or military AI systems, also seem to have the potential to cause human extinction.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefugmjfjzinsm\"><sup><a href=\"#fnugmjfjzinsm\">[14]</a></sup></span>&nbsp;The risk of a catastrophic, extinction-level biological event is \u201cdifficult to rule out\u201d.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrw57xbii95\"><sup><a href=\"#fnrw57xbii95\">[15]</a></sup></span>&nbsp;Deliberate use in a war is among the plausible trigger mechanisms for such an event. Autonomous systems are, potentially, scarier still. In fact, military AI systems pose special risks as they\u2019re likely to be developed in secrecy, linked to weapons technologies like drones and missile systems, and deployed in \u201ccomplex, hazardous, and adversarial environments\u201d.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdxb4awyr7s8\"><sup><a href=\"#fndxb4awyr7s8\">[16]</a></sup></span>&nbsp;Again there doesn\u2019t seem to be a strong reason to think there\u2019s an upper bound to the amount of people that could be killed in a war featuring widespread deployment of AI commanders or lethal autonomous weapons systems.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefapsnunibxgh\"><sup><a href=\"#fnapsnunibxgh\">[17]</a></sup></span></p><p>So on technological grounds, at least, there seem to be no strong reasons to think that the distribution of war outcomes is capped below the level of human extinction.</p><h1>Conclusion</h1><p>We set out to investigate what the probability distribution of outcomes looks like for a modern global war. Some IR research has shown that a power law distribution is a plausible fit given the battle death data we have. But few analyses have compared the fit to that of other distributions, and the few that have found that it\u2019s not clearly better than a log-normal or cut-off power law distribution. Plus, even if fitting a power law is appropriate, the extent to which we can extrapolate this distribution to infer the likelihood of events outside of the available data is unclear. So we have to go beyond the data and ask: given what we know about how wars are fought, does a war so large it constitutes an existential catastrophe seem implausible?</p><p>There are many different factors one could consider, and unfortunately a dearth of literature to rely on. Focusing on the reach of modern weapons, we found no strong reasons to think that an extinction-level war is not technologically possible. A 21st century great power war could see weapons of mass destruction deployed on an unprecedented scale. Since we cannot rule out extinction-level scenarios following the use of bioweapons or advanced military AI systems, it\u2019s plausible that the distribution of possible outcomes includes extinction.</p><p>Of course, there could be other limiting factors. Future research could examine other candidates. For example, how do the political costs of further escalation change? How does the ratio of civilian to military deaths change? Are there logistical, tactical, or economic factors that limit how large wars can get? And for any proposed \u201climiting factor\u201d, how does it interact with the existential risk of a runaway bioweapon or military AI system?</p><p>Given the destructive potential of nukes, bioweapons and AI systems, though, our guess is that it will be hard to rule out the possibility that a war could get very, very bad indeed. We don\u2019t think there\u2019s an upper bound: not at 5%, nor at 90%, nor at any point in between.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668679716/mirroredImages/PyZCqLrDTJrQofEf7/citqf9p5mik4abwmjywd.jpg\"><figcaption>Figure 4. The Battle of Stalingrad</figcaption></figure><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4o0kwuodjpf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4o0kwuodjpf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The&nbsp;<i>severity</i> of a war refers to the number of battle deaths it causes. World War II killed about 65 million people, which was ~3% of the global population at the time. So a modern extinction-level war would be about 100 times more severe in absolute terms and 30 times more severe in proportional terms. In this post we also sometimes refer to the&nbsp;<i>intensity</i> of a war. This refers to the number of battle deaths divided by the pooled populations of the countries involved.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngkur2xqb56g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgkur2xqb56g\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note that this uses Braumoeller\u2019s estimates of the distribution of severity: the number of battle deaths. This may underestimate the chance of an extinction war for at least two reasons. First, world population has been growing over time. If we instead considered the proportion of global population killed per war instead, extreme outcomes may seem more likely. Second, he does not consider civilian deaths. Historically, the ratio of civilian-deaths-to-battle deaths in war has been about 1-to-1 (though there\u2019s a lot of variation across wars). So fewer than 8 billion battle deaths would be required for an extinction war, since many civilians would also be killed.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9goblfdkit8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9goblfdkit8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>These are: lognormal, exponential, stretched exponential, and power law with cut off.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvwbt23seeds\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvwbt23seeds\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cIn particular, the distributions for birds, books, cities, religions, wars, citations, papers, proteins, and terrorism are plausible power laws, but they are also plausible log-normals and stretched exponentials\u201d (Clauset, Shalizi, and Newman, 2009, p. 26).&nbsp;</p><p>Note that stretched exponential is&nbsp;<i>not&nbsp;</i>found to be a good fit for the war data; see&nbsp;<a href=\"https://arxiv.org/pdf/0706.1062.pdf\"><u>Table 6.2 on p. 28</u></a> of the paper for details.</p><p>It should also be noted that, confusingly, a \u201ccut off\u201d power law distribution doesn\u2019t actually have a hard upper bound; instead, the distribution is multiplied by an exponential function. This \u201cthins\u201d the tail but doesn\u2019t actually change the range.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkygml14apl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkygml14apl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Thanks to Ben Garfinkel for bringing this point to our attention in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/aSzxoj7irC5jNHceB/how-likely-is-world-war-iii?commentId=kbztMknaTXFvc53pP#comments\"><u>this comment</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv7oh73ii55\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv7oh73ii55\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Stephen previously discussed this in \u201cHow likely is World War III?\u201d and his report for Founders Pledge.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnf1imbiiy7b7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreff1imbiiy7b7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See pp. 113-4 of Braumoeller's <i>Only the Dead</i> for more discussion.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2ox57m5q5iu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2ox57m5q5iu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cIn cases such as these, it is important to look at physical motivating or theoretical factors to make a sensible judgment about the which [sic] distributional form is more reasonable\u2014we must consider whether there is a mechanistic or other non-statistical argument favoring one distribution or another\u201d (Clauset, Aaron, Cosma Rohilla Shalizi, and Mark EJ Newman. \"Power-law distributions in empirical data.\" <i>SIAM review</i> 51, no. 4 (2009): 26).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6iq0ta93u6n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6iq0ta93u6n\">^</a></strong></sup></span><div class=\"footnote-content\"><p>More specifically, they compare a distribution fitted to the entire Correlates of War dataset to a distribution fitted to the \u201ctop decile of international wars\u201d (n=13). They find that the parameters are significantly different. This violates one of the properties of \u201ctrue\u201d power law distributions, which is that the same parameters describe the data everywhere in its range.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3zuy3zb9jfm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3zuy3zb9jfm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cthere are just so many belligerents, so many possible war alliances, so much armament, so many combat fronts that can be managed simultaneously, and so forth. As a result, the theoretically possible largest magnitudes of warfare are never actually realized due to the underlying anite dynamics\u201d Cioffi-Revilla, C., &amp; Midlarsky, M. I. (2004). Power laws, scaling, and fractals in the most lethal international and civil wars. In&nbsp;<i>The Scourge of War: New Extensions on an Old Problem</i> (p. 23).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnznvgsvwdg3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefznvgsvwdg3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This focus may seem&nbsp;<i>too</i> narrow. But if we can find just one limiting factor among the various inputs needed for war, then we can be confident in lowering our upper bound.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn73ryqgz3467\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref73ryqgz3467\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We're using the war intensity dataset Braumoeller used for&nbsp;<i>Only the Dead</i>. You can see a copy of it&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1rEgxf4uN7SG5ge7oH6KCnrM5_C-crJc1sT-b2zGfIOw/edit?usp=sharing\"><u>here</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnds3j5k76wdi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefds3j5k76wdi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>What about before the 18th century? Luke Muehlhauser has estimated that 9.5% of the world\u2019s population died in Ghengis Khan\u2019s conquests. That's still well short of an extinction-threatening catastrophe (Muehlhauser, Luke. \u201cHow big a deal was the Industrial Revolution?\u201d, Luke Muehlhauser (blog), https://lukemuehlhauser.com/industrial-revolution).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnugmjfjzinsm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefugmjfjzinsm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>On bioweapons, you might want to read&nbsp;<a href=\"https://80000hours.org/problem-profiles/preventing-catastrophic-pandemics/\"><u>80,000 Hours\u2019 article</u></a> on biological risks and W. Seth Carus\u2019&nbsp;<a href=\"https://www.tandfonline.com/doi/abs/10.1080/10736700.2017.1385765?journalCode=rnpr20\"><u>review</u></a> of biological weapons programs since 1915. For military AI, we recommend Christian Ruhl\u2019s&nbsp;<a href=\"https://founderspledge.com/stories/autonomous-weapon-systems-and-military-artificial-intelligence-ai\"><u>report</u></a> for Founders Pledge.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrw57xbii95\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrw57xbii95\">^</a></strong></sup></span><div class=\"footnote-content\"><p>MacAskill, William.&nbsp;<i>What we owe the future</i>. Basic books, 2022, p. 112</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndxb4awyr7s8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdxb4awyr7s8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Paul Scharre, \u201cDebunking the AI arms race\u201d,&nbsp;<a href=\"http://dx.doi.org/10.26153/tsw/13985\"><u>http://dx.doi.org/10.26153/tsw/13985</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnapsnunibxgh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefapsnunibxgh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In fact, Paul Scharre of the Center for a New American Security has previously speculated that AI could make escalation more likely (Scharre, \u201c AI arms race\u201d).</p></div></li></ol>", "user": {"username": "Stephen Clare"}}, {"_id": "FbvsteE8ySnpFbjuk", "title": "Misquoting Is Conceptually Similar to Deadnaming: A Suggestion to Improve EA Norms", "postedAt": "2022-11-03T18:34:41.491Z", "htmlBody": "<p>Our society gives people (especially adults) freedom to control many aspects of their lives. People choose what name to go by, what words to say, what to do with their money, what gender to be called, what clothes to wear, and much more.</p>\n<p>It violates people\u2019s personal autonomy to try to control these things without their consent. It\u2019s not your place to choose e.g. what to spend someone else\u2019s money on, what clothes they should wear, or what their name is. It\u2019d be extremely rude to call me \u201cJoan\u201d instead of \u201cElliot\u201d.</p>\n<p><a href=\"https://www.effectivealtruism.org/\">Effective Altruism</a> (EA) has written <a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum\">norms</a> related to this:</p>\n<blockquote>\n<p>Misgendering deliberately and/or deadnaming gratuitously is not ok, although mistakes are expected and fine (please accept corrections, though).</p>\n</blockquote>\n<p>I think this norm is good. I think the same norm should be applied to misquoting for the same reasons. It currently <a href=\"https://forum.effectivealtruism.org/posts/GHFY6SpLkjML2Ld62/ask-everyone-anything-ea-101-1?commentId=rGybCq7pbkCpoj7D8\">isn\u2019t</a> (<a href=\"https://forum.effectivealtruism.org/posts/GHFY6SpLkjML2Ld62/ask-everyone-anything-ea-101-1?commentId=mMKrnG3LqdfGZzyPd\">context</a>).</p>\n<p>Article summary: <strong>Misquoting is different than sloppiness or imprecision in general. Misquoting puts words in someone else\u2019s mouth without their consent. It takes away their choice of what words to say or not say, just like deadnaming takes away their choice of what name to use.</strong></p>\n<p>I\u2019d also suggesting applying the deadnaming norm to other forms of misnaming besides deadnaming, though I don\u2019t know if those ever actually come up at EA, whereas misquoting happens regularly. I won\u2019t include examples of misquotes for two reasons. First, I don\u2019t want to name and shame individuals (especially when it\u2019s a widespread problem and it could easily have been some other individuals instead). Second, I don\u2019t want people to respond by trying to debate the degree of importance or inaccuracy of particular misquotes. That would miss the point about people\u2019s right to control their own speech. It\u2019s not your place to speak for other people, without their consent, even a little bit, even in unimportant ways.</p>\n<p>I\u2019ll clarify how I think the norm for deadnaming works, which will&nbsp;simultaneously clarify what I think about misquoting. There are some nuances to it. Then I\u2019ll discuss misquoting more and discuss costs and benefits.</p>\n<h2>Accidents</h2>\n<p><em>Accidental</em> deadnaming is OK but <em>non-accidental</em> deadnaming isn\u2019t. If you deadname someone once, and you\u2019re corrected, you should fix it and you shouldn\u2019t do it again. Accidentally deadnaming someone many times is implausible or unreasonable; reasonable people who want to stop having those accidents can stop.</p>\n<p>While \u201cmistakes are expected and fine\u201d, EA\u2019s norm is that deadnaming on purpose is not fine nor expected. Misquotes, like deadnaming, come in accidental and non-accidental categories, and the non-accidental ones shouldn\u2019t be fine.</p>\n<p>How can we (charitably) judge what is an accident?</p>\n<p>A sign that deadnaming wasn\u2019t accidental is when someone defends, legitimizes or excuses it. If they say, \u201cSorry, my mistake.\u201d it was probably a genuine accident. If they instead say \u201cDeadnaming is not that bad.\u201d or \u201cIt\u2019s not a big deal.\u201d or \u201cWhy do you care so much?\u201d, or \u201cI\u2019m just using the name on your birth certificate.\u201d then their deadnaming was partly due to their attitude rather than by accident. That violates EA norms.</p>\n<p>When people resist a correction, or deny the importance of getting it right, then their mistake wasn\u2019t just an accident.</p>\n<p>For political reasons, some people resist using other people\u2019s preferred name or pronouns. There\u2019s a current political controversy about it. This makes deadnaming more common than it would otherwise be. Any deadnaming that occurs in part due to political attitudes is not fully accidental. Similarly, there is a current intellectual controversy about whether misquoting is a big deal or whether, instead, complaining about it is annoyingly pedantic and unproductive. This controversy increases the frequency of misquotes.</p>\n<p>However, that controversy about misquotes and precision is separate from the issue of people\u2019s right to control their own speech and choose what words to say or not say. Regardless of the outcome of the precision vs. sloppiness debate in general, misquotes are a special case because they non-consensually violate other people\u2019s control over their own speech. It\u2019s a non sequitur to go from thinking that lower effort, less careful writing is good to the conclusion that it\u2019s OK to say that John said words that he did not say or choose.</p>\n<p>People who deadname frequently claim it\u2019s accidental when there are strong signs it isn\u2019t accidental, such as resisting correction, making political comments that reveal their agenda, or being unapologetic. If they do that repeatedly, I don\u2019t think EA would put up with it. Misquoting could be treated the same way.</p>\n<h2>Legitimacy</h2>\n<p>Sometimes people call me \u201cElliott\u201d and I usually say nothing about the misspelling. I interpret it as an accident because it doesn\u2019t fit any agenda. I don\u2019t know why they\u2019d do it on purpose. If I expected them to use my name many times in the future, or they were using it in a place that many people would read it, then I\u2019d probably correct them. If I corrected them, they would say \u201coops sorry\u201d or something like that; as long as they didn\u2019t feel attacked or judged, and they don\u2019t have a guilty conscience, then they wouldn\u2019t resist the correction.</p>\n<p>My internet handle is \u201ccuri\u201d. Sometimes people call me \u201cCuri\u201d. When we\u2019re having a conversation and they\u2019re using my name repeatedly, I may ask them to use \u201ccuri\u201d. A few people have resisted this. Why? Besides feeling hostility towards a debate opponent, I think some were unfamiliar with internet culture, so they don\u2019t regard name capitalization as a&nbsp;valid, legitimate choice. They believe names should be formatted in a standard way. They think I\u2019m in the wrong by wanting to have a name that starts with a lowercase letter. They think, by asking them to start a name with a lowercase letter, I\u2019m the one trying to control them in a weird, inappropriate way.</p>\n<p>People resist corrections when they think they\u2019re in the right in some way. In that case, the mistake isn\u2019t accidental. Their belief that it\u2019s good in some way is a causal factor in it happening. If it was just an accident, they wouldn\u2019t resist fixing the mistake. Instead, there is a disagreement; they like something about the alleged mistake. On the EA forum, you\u2019re not allowed to disagree that deadnaming is bad and also act on that disagreement by being resistant to the forum norms. You\u2019re required to go along with and respect the norms. You can get a warning or ban for persistent deadnaming.</p>\n<p>People\u2019s belief that they\u2019re in the right usually comes from some kind of <strong>social-cultural legitimacy</strong>, rather than being their own personal opinion. Deadnaming and misgendering are legitimized by right wing politics and by some traditional views. Capitalizing the first letter of a name, and lowercasing the rest, is a standard English convention/tradition which some internet subcultures decided to violate, perhaps due to their focus on written over spoken communication. I think misquoting is legitimized primarily by anti-pedantry or anti-over-precision ideas (which is actually a nuanced debate where I think both standard sides are wrong). But viewpoints on precision aren\u2019t actually relevant to whether it\u2019s acceptable or violating to put unchosen words in someone else\u2019s mouth. Also, each person has a right to decide how precise to be in their own speech. When you quote, it\u2019s important to understand that that isn\u2019t your speech; you\u2019re using someone else\u2019s speech in a limited way, and it isn\u2019t yours to control.</p>\n<p>When someone asks you not to deadname, you may feel that they\u2019re asking you to go against your political beliefs, and therefore want to resist what feels like politicized control over your speech, which asks you to use your own speech contrary to your values. However, a small subset of speech is more about other people than yourself, so others need to have significant control over it. That subset includes names, pronouns and quotes. When asked not to misquote, instead of feeling like your views on precision are being challenged, you should instead recognize that you\u2019re simply being asked to respect other people\u2019s right to choose what words to say or not say. It\u2019s primarily about them, not you. And it\u2019s primarily about their control over their own life and speech, not about how much precision is good or how precisely you should speak.</p>\n<p>Control over names and pronouns does have to be within reason. You can\u2019t choose \u201cmy master who I worship\u201d as a name or pronoun and demand that others say it. I\u2019m not aware of anyone ever seriously wanting to do that. I don\u2019t think it\u2019s a real problem or what the controversy is actually about (even though it\u2019s a current political talking point).</p>\n<p>Our culture has conflicting norms, but it does have a very clear, well known norm in favor of <em>exact</em> quotes. That\u2019s taught in schools and written down in policies at some universities and newspapers. We lack similarly clear or strong norms for many other issues related to precision. Why? Because the norm against misquoting isn\u2019t primarily about precision. Misquoting is treated differently than other issues related to precision because it\u2019s not your place to choose someone else\u2019s words any more than it\u2019s your place to choose their name or gender.</p>\n<h2>Misquotes Due to Bias</h2>\n<p>Misquotes usually aren\u2019t random errors.</p>\n<p>Sometimes people make a typo. That\u2019s an accident. Typos can be viewed as basically random errors. I bet there are&nbsp;actually patterns regarding which letters or letter combinations get more typos. And people could work to make fewer typos. But there\u2019s no biased agenda there, so in general it\u2019s not a problem.</p>\n<p>Most quotes can be done with copy/paste, so typos can be avoided. If someone has a general policy of typing in quotes and keeps making typos within quotes, they should switch to using copy/paste. At my forum, I preemptively ask everyone to use software tools like copy/paste when possible to avoid misquotes. I don\u2019t wait and ask them to switch to less error-prone quoting methods after they make some errors. That\u2019s because, as with deadnaming, those errors mistreat other people, so I\u2019d rather they didn\u2019t happen in the first place.</p>\n<p>Except for typos and genuine accidents, misquotes are usually changed in some way that benefits or favors the misquoter, not in random ways.</p>\n<p>People often misquote because they want to edit things in their favor, even in very subtle ways. Tiny changes can make a quote seem more or less formal or tweak the connotations. People often edit quotes to remove some ambiguity, so it reads as an author more clearly saying something than he did.</p>\n<p>Sometimes people want their writing to look good with no errors, so they want to change anything in a quote that they regard as an error, like a comma or lack of comma. Instead of respecting the quote as someone else\u2019s words \u2013 their errors are theirs to make (or to disagree are errors) \u2013 they want to control it because they\u2019re using it within their own writing, so they want to make it conform to their own writing standards. People should understand that when they quote, they are giving someone else a space within their writing, so they are giving up some control.</p>\n<p>People also misquote because they don\u2019t respect the concept of accurate quotations. These misquotes can be careless with no other agenda or bias \u2013 they aren\u2019t specifically edited to e.g. help one side of a debate. However, random changes to the wordings your debate partners use tend to be bad for them. Random changes tend to make their wordings less precise rather than more precise. As we know from evolution, random changes are more likely to make something less adapted to a purpose rather than more adapted.</p>\n<p>If you deadname people because you don\u2019t respect the concept of people controlling their name, that\u2019s not OK. If you are creating accidents because you don\u2019t care to try to get names right, you\u2019re doing something wrong. Similarly, if you create accidental misquotes because you don\u2019t respect the concept of people controlling their own speech and wordings, you\u2019re doing something wrong.</p>\n<p>Also, imprecision in general is an enabler of bias because it gives people extra flexibility. They get more options for what to say, think or do, so they can pick the one that best fits their bias. A standard example is rounding in their favor. If you\u2019re 10 minutes late, you might round that down to 5 minutes in a context where plus or minus five minutes of precision is allowed. On the other hand, if someone else is 40 minutes late, you might round that up to an hour as long as that\u2019s within acceptable boundaries of imprecision. People also do this with money. Many people round their budget up but round their expenses down, and the more imprecise their thinking, the larger the effect. If permissible imprecision gives people multiple different versions of a quote that they can use, they\u2019ll often pick one that is biased in their favor, which is different than a fully accidental misquote.</p>\n<h2>Misquotes&nbsp;Due to Precise Control or Perfectionism</h2>\n<p>Some non-accidental misquotes, instead of due to bias, are because people want to control all the words in their essay (or book or forum post). They care so much about controlling their speech, in precise detail, that they extend that control to the text within quotes just because it\u2019s within their writing. They\u2019re used to having full control over everything they write and they don\u2019t draw a special boundary for quotations; they just keep being controlling. Then, ironically, when challenged, they may say \u201cOh who cares; it\u2019s just small changes; you don\u2019t need precise control over your speech.\u201d But they changed the quote because of their extreme desire to exactly control anything even resembling their own speech. If you don\u2019t want to give up control enough to let someone else speak in entirely their own words within your writing, there is a simple solution: don\u2019t quote them. If you want total control of your stuff, and you can\u2019t let a comma be out of place even within a quote, you should respect other people wanting control of their stuff, too. Some people don\u2019t fully grasp that the stuff within quotes is not their stuff even though it\u2019s within their writing. Misquotes of this nature come more from a place of perfectionism and precise control, and lack of empathy, rather than being sloppy accidents. These misquotes involve non-random changes to make the text fit the quoter\u2019s preferences better.</p>\n<h2>Types of Misquotes</h2>\n<p>I divide misquotes into two categories. The first type changes a word, letter or punctuation mark. It\u2019s a factual error (the quote is factually wrong about what the person said). It\u2019s inaccurate in a clear, literal way. Computers can pretty easily check for this kind of quotation error without needing any artificial intelligence. Just a simple string comparison algorithm can do it. In this case, there\u2019s generally no debate about whether the quote is accurate or inaccurate. There are also some special rules that allow changing quotes without them being considered inaccurate, e.g. using square brackets to indicate changes or notes, or using ellipses for omitted words.</p>\n<p>The second type of misquote is a misleading quote, such as taking words out of context. There is sometimes debate about whether a quote is misleading or not. Many cases are pretty clear, and some cases are harder to judge. In borderline cases, we should be forgiving of the person who did it, but also, in general, they should change it if the person being quoted objects. (Or, for example, if you\u2019re debating someone about Socrates\u2019 ideas, and they\u2019re the one taking Socrates\u2019 side, and they think your Socrates quote is misleading, then you should change it. You may say all sorts of negative things about the other side of the debate, but that\u2019s not what quotation marks are for. Quotations are a form of neutral ground that should be kept objective, not a place to pursue your debating agenda.)</p>\n<p>Here\u2019s an example of a misleading quote that doesn\u2019t violate the basic accuracy rules. You say, \u201cI do not think John is great.\u201d but I quote you as saying \u201cJohn is great.\u201d The context included an important \u201cnot\u201d which has been left out. I think we can all agree that this counts as misquoting even though no words, letters or punctuation marks were changed. And, like deadnaming, it\u2019s very rude to do this to someone.</p>\n<h2>Small Changes</h2>\n<p>Sometimes people believe it\u2019s OK to misquote as long as the meaning isn\u2019t changed. Isn\u2019t it harmless to replace a word with a synonym? Isn\u2019t it harmless to change a quote if the author agrees with the changed version? Do really small changes matter?</p>\n<p>First of all, if the changes are small and don\u2019t really matter, then just don\u2019t do them. If you think there\u2019s no significant difference, that implies there\u2019s no significant upside, so then don\u2019t misquote. It\u2019s not like it takes substantial effort to refrain from editing a quote; it\u2019s less work not to make changes. And copy/pasting is generally less work than typing.</p>\n<p>If someone doesn\u2019t mind a change to a quote, there are still concerns about truth and accuracy. Anyone in the audience may not want to read things he believes are exact quotes but which aren\u2019t. He may find that misleading (and EA has a norm against misleading people). Also, if you ever non-accidentally use inaccurate quotes, then reasonable people will doubt that they can trust any of your quotes. They\u2019ll have to check primary sources for any quotes you give, which will significantly raise the cost of reading your writing and reduce engagement with your ideas. But the main issue \u2013 putting words in someone\u2019s mouth without their consent \u2013 is gone if they consent. Similarly, it isn\u2019t deadnaming to use an old name of someone who consents to be called by either their old or new name.</p>\n<p>However, it\u2019s not your place to guess what words someone would consent to say. If they are a close friend, maybe you have a good understanding of what\u2019s OK with them, and I guess you could try to get away with it. I wouldn\u2019t recommend that and I wouldn\u2019t want to be friends with someone who thought they could speak for me and present it as a quote rather than as an informed guess about my beliefs or about what I would say. But if you want to quote your friend (or anyone else) saying something they haven\u2019t said, and you\u2019re pretty sure they\u2019d be happy to say it, there\u2019s a solution: ask them to say it and then quote them if they do choose to say it. On the other hand, if you\u2019re arguing with someone, you\u2019re in a poor position to judge what words they would consent to saying or what kind of wording edits would be meaningful to them. It\u2019s not reasonable to try to guess what wording edits a debate opponent would consent to and then go ahead with them unilaterally.</p>\n<p>Inaccurately paraphrasing debate opponents is a problem too, but it\u2019s much harder to avoid than misquoting is. Misquoting, like deadnaming, is something that you can almost entirely avoid if you want to.</p>\n<p>The changes you find small and unimportant can matter to other people with different perspectives on the issues. You may think that \u201cidea\u201d, \u201cconcept\u201d, \u201cthought\u201d and \u201ctheory\u201d are interchangeable words, but someone else may purposefully, non-randomly use each of those words in different contexts. It\u2019s important that people can control the nuances of their wordings when they want to (even if they can\u2019t give explicit arguments for why they use words that way). Even if an author doesn\u2019t (consciously) see any significant difference between his original wording and your misquote, the misquote is still less representative of his thinking (his subconscious or intuition chose to say it the other way, and that could be meaningful even if he doesn\u2019t realize it).</p>\n<p>Even if your misquote would be an accurate paraphrase, and won\u2019t do a bunch of harm by spreading severe misinformation, there\u2019s no need to put quote marks around it. If you\u2019re using an edited version of someone else\u2019s words, so leaving out the quote marks would be plagiarism, then use square brackets and ellipses. There\u2019s already a standard solution for how to edit quotes, when appropriate, without misquoting. There\u2019s no good reason to misquote.</p>\n<h2>Cost and Benefit</h2>\n<p>How costly is it to avoid misquotes or to avoid deadnaming? The cost is low but there are some reasons people misjudge it.</p>\n<p>Being precise has a high cost, at least initially. But misquoting, like misnaming, is a specific case where, with a low effort, people can get things right with high reliability and few accidents. Reducing genuine accidents to zero is unnecessary and isn\u2019t what the controversy is about.</p>\n<p>When a mistake is just an accident, correcting it shouldn\u2019t be a big deal. There is no shame is infrequent accidents. Attempts to correct misquotes sometimes turn into a much bigger deal, with each party writing multiple messages. It can even initiate drama. This is because people oppose the policy of not misquoting, rather than a cost inherent in a policy of not misquoting. It\u2019s the resistance to the policy, not the policy itself, which wastes time and energy and derails conversations.</p>\n<p>Most of the observed conversational cost, that goes to talking about misquotes, is due to people\u2019s pro-misquoting attitudes rather than due to any actual difficulty of avoiding misquotes. This misleads people about how large the cost is.</p>\n<p>Similarly, if you go to some right wing political forums, getting people to stop deadnaming would be very costly. They\u2019d fight you over it. But if they were happy to just do it, then the costs would be low. It\u2019s not very hard to very infrequently make updates to your memory about the names of a few people. Cost due to opposition to doing something correctly should be clearly differentiated from the cost of doing it correctly.</p>\n<p>To avoid misquotes, copy and paste. If you type in a quote from paper, double check it and/or disclaim it as potentially containing a typo. Most books are available electronically so typing quotes in from paper is usually unnecessary and more costly. Most cases of misquoting that I\u2019ve seen, or had a conflict over, involved a quote that could have been copy/pasted. Copy/pasting is easy not costly.</p>\n<p>Avoiding misquotes also involves never adding quotation marks around things which are not quotes but which readers would think were quotes. For example, don\u2019t write \u201cJohn said\u201d and then a paraphrase then also quote marks around it in order to make it seem more exact, precise, rigorous or official than it is. And don\u2019t put quote marks around a paraphrase because you believe you should use a quote, but you\u2019re too lazy to get the quote, and you want to hide that laziness by pretending you did quote.</p>\n<p>Accurate quoting can be more about avoiding bias than about effort or precision. You have to want to do it and then resist the temptation to violate the rules in ways that favor you. For some people, that\u2019s not even tempting. It\u2019s like how some people resist the temptation to steal while others don\u2019t find stealing tempting in the first place. You can get to the point that things aren\u2019t tempting and really don\u2019t take effort to not do. Norms can help with that. Due to better anti-stealing norms, many more people aren\u2019t tempted to steal than aren\u2019t tempted to misquote. Anyway, if someone gives in to temptation and steals, deadnames or misquotes, that is not an accident. It\u2019s a different thing. It\u2019s not permissible at EA to deadname because you gave in to temptation, and I suggest misquoting should work that way too.</p>\n<p>What\u2019s the upside of misquoting? Why are many people resistant to making a small effort to change? I think there are two main reasons. First, they confuse the misquoting issue with the general issue of being imprecise. They feel like someone asking them not to misquote is demanding that they be a more precise thinker and writer in general. Actually, people asking not to be misquoted, like people asking not to be deadnamed, don\u2019t want their personal domain violated. Second, people like misquoting because it lets lets them make biased changes to quotes. People don\u2019t like being controlled by rules that give them less choice of what to do and less opportunity to be flexible in their favor (a.k.a. biased). Many people have a general resistance to creating and following written policies. I\u2019ve <a href=\"https://forum.effectivealtruism.org/posts/7urvvbJgPyrJoGXq4/fallibilism-bias-and-the-rule-of-law\">written about</a> how that\u2019s related to not understanding or resisting the rule of law.</p>\n<p>Another cost of avoiding misquotes is that you should be careful when using software editing tools like spellcheck or Grammarly. They should have automatic quote detection features and warn you before making changes within quotes, but they don\u2019t. These tools encourage people to quickly make many small changes without reading the context, so people may change something without even knowing it\u2019s within a quote. People can also click buttons like \u201ccorrect all\u201d and end up editing quotes. Or they might decide to replace all instances of \u201ccolour\u201d with \u201ccolor\u201d in their book, do a mass find/replace, and accidentally change a quote. I wonder how many small misquotes in recent books are caused this way, but I don\u2019t think it\u2019s the cause of many misquotes on forums. Again, the occasional accident is OK; perfection is not necessary but people could avoid most errors at a low cost and stop picking fights in defense of misquotes or deadnaming.</p>\n<p>If non-accidental misquoting is prohibited at EA, just like deadnaming, then it will provide a primary benefit by defending people\u2019s control over their own speech. It will also provide a secondary benefit regarding truth, accuracy&nbsp;and precision. It\u2019s debatable how large that accuracy benefit is and how much cost it would be worth. However, in this case, the marginal cost of that benefit would be zero. If you change misquoting norms for another reason which is worth the cost by itself, then then the gain in accuracy is a free bonus.</p>\n<p>There are some gray areas regarding misquoting, where it\u2019s harder to judge whether it\u2019s an error. Those issues are more costly to police. However, most of the benefit is available just by policing misquotes which are clearly and easily avoidable, which is the large majority of misquotes. Doing that will have a good cost to benefit ratio.</p>\n<p>Another cost of misquoting is it can gaslight people, especially with small, subtle changes. It can cause them to doubt themselves or create false memories of their own speech to match the misquote. It takes work to double check what you actually said after reading someone quote you, which is a cost. Many people don\u2019t do that work, which leaves them vulnerable. There\u2019s a downside both do doing or not doing that work. That\u2019s a cost imposed by allowing misquotes to be common and legitimized.</p>\n<h3>Tables</h3>\n<p>Benefits and costs of <strong>anti-misquoting norms</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Benefits</th>\n<th>Costs</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Respect people\u2019s control over their speech</td>\n<td>Avoiding carelessness</td>\n</tr>\n<tr>\n<td>Accuracy</td>\n<td>Resisting temptation</td>\n</tr>\n<tr>\n<td>Prevent conflicts about misquotes</td>\n<td>Not getting to bias quotes in your favor</td>\n</tr>\n<tr>\n<td>No hidden, biased tweaks in quotes you read</td>\n<td>Learning to use copy/paste hotkeys</td>\n</tr>\n<tr>\n<td>Less time editing quotes</td>\n<td>Not getting full control over quoted text like you have over other text in your post</td>\n</tr>\n<tr>\n<td>Quotes and paraphrases differentiated</td>\n<td>Not getting to put quote marks around whatever you want to</td>\n</tr>\n<tr>\n<td>Filter out people who insist on misquoting</td>\n<td>Lose people who insist on misquoting</td>\n</tr>\n<tr>\n<td></td>\n<td>Effort to spread and enforce norm</td>\n</tr>\n</tbody>\n</table>\n<p>For comparison, here\u2019s a cost/benefit table for <strong>anti-deadnaming norms</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Benefits</th>\n<th>Costs</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Respect people's control over their name</td>\n<td>Avoiding carelessness</td>\n</tr>\n<tr>\n<td>Accuracy</td>\n<td>Resisting temptation</td>\n</tr>\n<tr>\n<td>Filter out persistent deadnamers</td>\n<td>Lose people who insist on deadnaming</td>\n</tr>\n<tr>\n<td></td>\n<td>Not getting to call people whatever you want</td>\n</tr>\n<tr>\n<td></td>\n<td>Effort to spread and enforce norm</td>\n</tr>\n</tbody>\n</table>\n<h2>Potential Objections</h2>\n<p>If I can\u2019t misquote, how can I tweak a quote wording to fit my sentence? Use square brackets.</p>\n<p>If I can\u2019t misquote, how can I supply context for a quote and keep it short? Use square brackets or explain the context before giving the quote.</p>\n<p>What if I want to type in a quote and make a typo? If you\u2019re a good enough typist that you don\u2019t mind typing extra words, I\u2019m sure you can also manage to use copy/paste hotkeys.</p>\n<p>What if I\u2019m quoting a paper book? Double check what you typed in and/or put a disclaimer that it\u2019s typed in by hand.</p>\n<p>What if an accident happens? As with deadnaming, rare, genuine accidents are OK. Accidents that happen because you don\u2019t really care about deadnaming or misquoting are not fine.</p>\n<p>Who cares? People who think about what words to say and not say, and put effort into those decisions. They don\u2019t want someone else to overrule those decisions. Whether you\u2019re one of those people or not, people who think about what to say are people you should want to have on your forum.</p>\n<p>Who else cares? People who want to form accurate beliefs about the world and have high standards don\u2019t want to read misquotes and potentially be fooled by them or have to look stuff up in primary sources frequently. It\u2019s much less work for people to not misquote in the first place than for readers (often multiple readers independently) to check sources.</p>\n<p>Is it really that big a deal? Quoting accurately isn\u2019t very hard and isn\u2019t that big a deal to do. If this issue doesn\u2019t matter much, just do it in the way that doesn\u2019t cause problems and doesn\u2019t draw attention to quoting. If people would stop misquoting then we could all stop talking about this.</p>\n<p>Can\u2019t you just ignore being misquoted? Maybe. You can also ignore being deadnamed, but you shouldn\u2019t have to. It\u2019s also hard enough to have discussions when people subtly reframe the issues, and indirectly reframe what you said (often by replying <em>as if</em> you said something, without claiming you said it), which is very common. Those actions are harder to deal with and counter when they involve misquotes \u2013 misquotes escalate a preexisting problem and make it worse. On the other hand, norms in favor of using (accurate) quotes more often would make it harder to be subtly biased and misleading about what discussion partners said.</p>\n<h2>Epistemic Status</h2>\n<p>I\u2019ve had strong opinions about misquoting for years and brought these issues up with many people. My experiences with using no-misquoting norms at my own forum have been positive. I still don\u2019t know of any reasonable counter-arguments that favor misquotes.</p>\n<h2>Conclusion</h2>\n<p>Repeated deadnaming is due to choice not accident. Even if a repeat offender isn\u2019t directly choosing to deadname on purpose, they\u2019re choosing to be careless about the issue on purpose, or they have a (probably political) bias. They could stop deadnaming if they tried harder. EA norms correctly prohibit deadnaming, except by genuine accident. People are expected to make a reasonable (small) effort to not deadname.</p>\n<p>Like deadnaming, misquoting violates someone else\u2019s consent and control over their personal domain. People see misquoting as being about the open debate over how precise people should be, but that is a secondary issue. They should have more empathy for people who want to control their own speech. I propose that EA\u2019s norms should be changed to treat misquoting like deadnaming. Misquoting is a frequent occurrence and the forum would be a better place if moderators put a stop to it, as they stop deadnaming.</p>\n<p>Norms that allow non-accidental misquoting alienate some people who might otherwise participate, just like allowing non-accidental deadnaming would alienate some potential participants. Try to visualize in your head what a forum would be like where the moderators refused to do anything about non-accidental deadnaming. Even if you don\u2019t personally have a deadname, it\u2019d still create a bad, disrespectful atmosphere. It\u2019s better to be respectful and inclusive, at a fairly small cost, instead of letting some forum users mistreat others. It\u2019s great for forums to enable free speech and have a ton of tolerance, but that shouldn\u2019t extend to people exercising control over something that someone else has the right to control, such as his name or speech. It\u2019s not much work to get people\u2019s names right nor to copy/paste exact quotes and then leave them alone (and to refrain from adding quotation marks around paraphrases). Please change EA\u2019s norms to be more respectful of people\u2019s control over their speech, as the norms already respect people\u2019s control over their name.</p>\n", "user": {"username": "Elliot Temple"}}, {"_id": "fEWyk4RrF4eKgSfXi", "title": "Towards More Modular Value/Impact Estimates", "postedAt": "2022-11-03T19:33:31.498Z", "htmlBody": "<p><strong>TLDR;</strong> This post explains why I think we should be more explicit about our values when making impact estimates/ value predictions and provides some example suggestions of how to do this.</p><p>There has been a string of recent posts discussing and predicting characteristics (EV, Variance, etc) about future value. (<a href=\"https://forum.effectivealtruism.org/posts/k9bXjXyjt9xeLqKji/how-binary-is-longterm-value\">How Binary is Longterm Value?</a>, <a href=\"https://forum.effectivealtruism.org/posts/WebLP36BYDbMAKoa5/the-future-might-not-be-so-great\">The Future Might Not Be So Great</a>, <a href=\"https://forum.effectivealtruism.org/posts/wqmY98m3yNs6TiKeL/parfit-singer-aliens\">Parfit + Singer + Aliens = ?</a>, <a href=\"https://forum.effectivealtruism.org/posts/zDJpYMtewowKXkHyG/alien-counterfactuals\">shameless plug </a>, etc. )</p><p>Moreover, estimating the \"impact\" of interventions is a central theme in this community. It is perhaps the core mission of Effective Altruism.&nbsp;</p><p>Most of the time I see posts discussing impact/value, I don't see a definition of value.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnzf9l1qwmh9\"><sup><a href=\"#fnnzf9l1qwmh9\">[1]</a></sup></span>&nbsp; What we define value to mean (sometimes called, ethics, morality, etc.) is the function that converts material outcomes (the is) into a number for which, bigger = better (the ought).</p><p>If someone makes a post engaging in value estimating and doesn't define value, it seems like there are two likely possibilities.</p><ul><li>Most people engaging with the post will use their own internal notion of value.</li><li>Most people will engage with the post using what they perceive to be the modal value in the community, so probably total utilitarianism.</li></ul><p>I believe these are both sub-optimal outcomes. &nbsp;I do not believe most people engaging with these posts are trying to actively grapple with meta-ethics, so in the first place, they might not care to talk through the fact that they have different internal notions of value. More importantly, the ability to identify and isolate cruxes is central to rationality. We should always aim to modularize our discussions as this clarifies disagreements in the moment and allows the conclusions of the conversation to be much more plug-and-pull in the future. On some questions of impact, it <i>could</i> be the case that the answer to the question is not a function of the value system we use. But I think this is incredibly unlikely<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefoqcitr2954b\"><sup><a href=\"#fnoqcitr2954b\">[2]</a></sup></span>&nbsp;and anyway we should explicitly come to that conclusion rather than assume it.&nbsp;</p><p>If the second outcome, at least most of us would be on the same page. Of course, not <i>everyone</i> would be on the same page. Also, it isn't like total utilitarianism is clearly defined. You still need to give utility a useable definition, <a href=\"https://forum.effectivealtruism.org/posts/hxtwzcsz8hQfGyZQM/an-introduction-to-the-moral-weight-project\">you need to create a weighting rule or map for sentient beings</a>, and you need to define if there is such a thing as negative lives (and if yes, where the line is), etc.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefovhmt73t4ba\"><sup><a href=\"#fnovhmt73t4ba\">[3]</a></sup></span>&nbsp;So you would still have a lesser version of the above point. Plus, we then have also created an environment with a de facto ethic, which doesn't seem like a good vibe to me.&nbsp;</p><p><strong>Suggestions</strong></p><p>Primary suggestion: Write your definition of value in your bio, and if you don't clarify in your comment/post, people should default to using this definition of value. I'm not sure there is an easily generalizable blueprint for all ethical systems, but here is an example of what a utilitarian version might look like (not my actual values). Note that this could probably be fleshed out more and/or better but I don't think it matters for the purpose of this post.</p><p><strong>BIO</strong></p><p>Ethical Framework: Total Utilitarianism</p><p>Definition of Utility: QALYS, but rescaled so that quality of life can dip negative</p><p>Weighting functions: Amount of neurons</p><p>Additional Clarifications: I believe this is implicit in my weighting function but I consider future and digital minds to be morally valuable. My definition of a neuron is (....). I would prefer to use my <a href=\"https://www.lesswrong.com/tag/coherent-extrapolated-volition\"><strong>Coherent Extrapolated Volition</strong></a><strong> </strong>over my current value system.&nbsp;</p><p>&nbsp;</p><p>Other suggestions I like less:</p><p>Suggestion: Define value in your question/comment post.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyhygkrpftgk\"><sup><a href=\"#fnyhygkrpftgk\">[4]</a></sup></span></p><p>Suggestion: Make a certain form of total utilitarianism the de jure meaning of value on the forum when people don't clearly define value or don't set a default value in their bio.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreff5yjb5g9dii\"><sup><a href=\"#fnf5yjb5g9dii\">[5]</a></sup></span></p><p>Suggestion: Don't do impact estimates in one go, do output/outcome estimates. Then extrapolate separately. I.E. ask questions like \"How many QALYs will there be in the future\" \"How many human rights will be violated\" etc.&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnzf9l1qwmh9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnzf9l1qwmh9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Sometimes I will see something like \"my ethics are suffering focused, so this leads me x instead of y\".&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnoqcitr2954b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefoqcitr2954b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If we think of morality as being an arbitrary map that takes the world as an input and spits out a (real) number, then it is an arbitrary map from &nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"F^n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.106em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.266em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>&nbsp;or&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"F^{\\infty}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.106em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.266em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">\u221e</span></span></span></span></span></span></span></span></span></span></span>&nbsp;to R, where F some set (Technically, the dimensions of the universe are not necessarily comprised of the same sets so this notation is wrong, plus I don't actually have any idea what I'm talking about). &nbsp;If this is the case, we can basically make the \"morality map\" do whatever we want. So when asking questions about how the value of the world will end up looking, we can almost certainly create two maps(moralities) that will spit out very different answers for the same world.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnovhmt73t4ba\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefovhmt73t4ba\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I understand how strict of a bar clarifying these things every post would be, and I don't think we need to be strict about it, but we should keep these things in mind and push towards a world where we are communicating this information.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyhygkrpftgk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyhygkrpftgk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This seems laborious</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnf5yjb5g9dii\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreff5yjb5g9dii\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We can of course make it explicit that we don't endorse this, and it is just a discussion norm. I would still understand if people feel this opens us up to reputational harms and thus is a bad idea.&nbsp;</p></div></li></ol>", "user": {"username": "Charles_Guthmann"}}, {"_id": "6xXqbebZjufoxiAtw", "title": "CEA uni groups team plans and priorities for next quarter", "postedAt": "2022-11-04T10:54:04.866Z", "htmlBody": "<p>This post covers the major plans and updates the CEA uni groups team intends to implement over the next quarter. Specifically, this post goes over our plans to:</p><ul><li>Re-focus UGAP to concentrate on starting new groups or restarting dormant uni groups;</li><li>Expand mentorship for EA group organizers at existing groups;</li><li>Update and streamline our system for screening and onboarding new groups.</li></ul><h3>UGAP Re-Focus</h3><p>As described in the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/d83HJFMnEvP6x6LaD/ugap-starter-program-retrospective\"><u>starter program review post</u></a>, we intend for <a href=\"https://www.notion.so/University-Group-Accelerator-Program-6df8c8fccf8b4ffbb6488d9dfa275282\">UGAP</a> to focus exclusively on supporting new and re-emerging groups next semester, rather than also including new organizers at existing groups.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefndi3tcmi4md\"><sup><a href=\"#fnndi3tcmi4md\">[1]</a></sup></span>&nbsp;We think that this will help make training and support clearer and more relevant for all participants.</p><p>We feel confident that UGAP offers helpful advice to orient new EA groups and reduce barriers to entry at scale. We have been pleased with the results of UGAP so far - there has been very strong demand and participants report high levels of satisfaction. More importantly, we\u2019ve seen a number of UGAP participants get more deeply involved in EA community building and direct work and we think UGAP has a good counterfactual impact case for several of these stories. Because of these reasons, we think there is a strong case for UGAP to continue to grow and improve in order to make it easy for new EA groups to start (and re-start when a previous iteration dies) using high-fidelity, strategic methods and best practices learned from other university groups.&nbsp;&nbsp;&nbsp;</p><p><strong>Applications for the next round of UGAP are now open for all new university groups. Apply&nbsp;</strong><a href=\"http://quilgo.com/t/KLN5oy1vB6b5wWOx\"><strong><u>here before November 20th!</u></strong></a></p><h3>Organizer Support Program (OSP)</h3><p>We are piloting an expanded mentorship program that is not limited to new groups. If your group is interested, please express interest&nbsp;<a href=\"http://quilgo.com/t/KLN5oy1vB6b5wWOx\"><u>here</u></a> before November 20th. One of your group leaders should apply as a representative of the group. Depending on capacity, mentorship offerings will encompass a range of frequencies and focuses that align with a group\u2019s needs.</p><p>While we are confident that UGAP provides useful support for new EA uni groups, we think the playbook for established EA groups is much more underdeveloped. This means that, at the current margin, providing more tailored mentorship and guidance to group organizers at established groups seems to be a better strategy. This intuition has been strengthened by the fact that organizers at more established groups have reported mentorship as the most useful component of UGAP for them. Over the next few months, we intend to roll out more mentorship programs and opportunities to group organizers (you can express interest in mentorship&nbsp;<a href=\"http://quilgo.com/t/KLN5oy1vB6b5wWOx\"><u>here</u></a>. This form will close on November 20th).</p><p>All university group organizers are eligible to apply for OSP. You can decide who attends these meetings, but we think it is best when only the main organizer(s) and those soon becoming the main organizer(s) attend. The mentorship program does not provide stipends.</p><p>We anticipate being constrained by mentor capacity and we cannot commit to offering mentorship to everyone who is interested in receiving it. Instead, we plan to prioritize offerings based on our analysis of who could benefit most from mentorship. Currently, for groups that we are able to support, the range of support we hope to eventually be able to provide will span from a one-off call to regular check-ins to in-person, multi-day visits from an experienced organizer.&nbsp;</p><p><strong>Relatedly, we are hoping to recruit and train more mentors to meet anticipated demand as much as possible. If you have experience running an EA uni group and are interested in mentoring new organizers, please fill out&nbsp;</strong><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdP7zYfJFOhBU2xVuRzE4d_2-xk-hWI5UT-BDmf1t2UTbQzbA/viewform?usp=sf_link\"><strong><u>this form</u></strong></a><strong> or email us at&nbsp;</strong><a href=\"mailto:ugap@centreforeffectivealtruism.org\"><strong><u>ugap@centreforeffectivealtruism.org</u></strong></a><strong> with questions.&nbsp;&nbsp;</strong></p><h3>Screening new organizers</h3><p>Note: The following is not confirmed but we wanted to give a heads up on what we are thinking about and leaning towards.&nbsp;</p><p>As the number of EA groups at universities grows and they continue to run more programs with more people, the importance of evaluating the quality and relative success of some strategies or programs over others is even more important. We now feel clearer that it is important to ensure that student organizers understand and are able to explain well what EA is and is currently doing.</p><p>To do this and to streamline the process for organizers seeking support, we are working to update our system for gaining eligibility to apply for&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/group-support-funding\"><u>basic groups expenses</u></a>. Before the end of Q1 2023, we intend to roll out a new process for evaluating new organizers and approving groups. We anticipate that we will ask an organizer or active advisor of every uni group to apply for CEA registration in order to be eligible for basic groups funding and support.</p><p>We are still working out the final details of the process to ensure a good user experience and a smooth evaluation process, but we anticipate that the broad strokes will end up resembling this:</p><ul><li>Organizers will submit an application designed to assess understanding of core EA ideas and principles and alignment with the basic goal of seeking to do good as well as possible.&nbsp;&nbsp;</li><li>If an organizer has previously received a stipend through UGAP, been approved as a registered CEA group organizer, or received an OP fellowship, they will be automatically eligible to apply for groups funding and mentorship.&nbsp;</li><li>When the approved group organizer passes off responsibility, a new organizer will be asked to re-apply to renew eligibility for these support resources.</li><li>In some cases, CEA may approve organizers for a 1-2 month trial period and those organizers will be asked to re-apply at the end of the trial.&nbsp;</li></ul><p>We hope that the process will clarify offerings for students and provide a mechanism for CEA to ensure that funding is directed as well as possible. If you have any questions or feedback as we work to finalize this process, please let us know.&nbsp;</p><h3>Conclusion</h3><p>We are excited for the work we have planned for next quarter and we believe that our current priorities are well calibrated to help improve the quantity, quality and potential of university EA groups across the world. Please feel free to share questions, comments, and feedback in the comments here or reach out to us directly at&nbsp;<a href=\"mailto:unigroups@centreforeffectivealtruism.org\"><u>unigroups@centreforeffectivealtruism.org</u></a>.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnndi3tcmi4md\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefndi3tcmi4md\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;If you aren\u2019t sure whether your group counts as \u201cnew\u201d or \u201crestarting,\u201d feel free to reach out to us at&nbsp;<a href=\"mailto:unigroups@centreforeffectivealtruism.org\"><u>unigroups@centreforeffectivealtruism.org</u></a>&nbsp;</p></div></li></ol>", "user": {"username": "Jesse Rothman"}}, {"_id": "BXeNWzEGv58MhJ9wt", "title": "CEA uni group team strategy for the next quarter ", "postedAt": "2022-11-04T10:55:06.492Z", "htmlBody": "<p>This post is part of a series to share more about the CEA uni groups team programs and strategy in the upcoming period.&nbsp;</p><p>In this post, we outline our current strategy for the next quarter, which focus on providing scalable support to promising university students to get started building high-quality, epistemically robust EA communities on their campuses. We also go over some of the activities that we would like to do later and things that we wish others would do now given our limited capacity.&nbsp;</p><h2>CEA Uni Groups team strategy&nbsp;</h2><h3>Mission</h3><p>Our mission is to support high-fidelity, ambitious, epistemically robust, constantly-learning EA communities at universities to attract, develop, or support impactful, talented EAs in order to ultimately build a better world for all sentient beings. We believe that supporting these groups is ultimately an effective way to identify, develop, encourage, retain, and direct talented and motivated people to work on the world\u2019s most pressing problems.</p><p>University groups have the potential to be especially significant EA spaces because:</p><ol><li>Students are in the process of deciding how to spend their careers.</li><li>Universities are a very large source of new EAs each year, which means that student groups are likely to have an outsized influence on the development of EA culture and movement priorities.&nbsp;</li><li>Universities are places where people are building communities and deep social networks. (We think it is, on net, likely that they will spend more social time deeply engaged with the EA community than professionals would.) These in-person social ties are important for people being more likely to take significant action.</li><li>(many) Universities have high concentrations of pre-screened talent.</li></ol><h3>What the CEA Uni Groups team does&nbsp;</h3><ul><li><strong>Identify, motivate, and support great group organizers through training, mentorship, and other opportunities;</strong><ul><li>How: We do this through programs like UGAP, mentoring, and retreats.</li><li>Why: We think group organizers are often the largest determinant of groups successfully channeling altruistic, truth-seeking individuals into high impact opportunities.&nbsp;</li></ul></li><li><strong>Help promote high-fidelity outreach and group strategy that optimizes for&nbsp;ambitious, epistemically robust, constantly-learning EA communities that attract, develop, and support impactful, talented EAs.</strong><ul><li>How: We screen against potentially negative group-organizers, manage the groups resource centre, and update the intro fellowship curriculum. We also provide funding for groups, provide some&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ko5fDSHFsJ35v6HMa/some-advice-the-cea-groups-team-gives-to-new-university\"><u>strategy advice</u></a>, and send a regular newsletter with opportunities.</li><li>Why: We think university groups will have a large effect on the culture of the EA movement and want to ensure they are epistemically rigorous. We also think that future influential people who may not adopt EA ideas might learn about EA through universities and want those interactions to be high-fidelity.</li></ul></li><li><strong>Provide opportunities to accelerate particularly promising organizers to pursue ambitious projects.</strong><ul><li>How: We do this via 1-1 calls, mentorship, retreats, grants, and internships</li><li>Why: Uni group organizers themselves often go into high-impact roles and make large contributions in EA (in and out of meta work). They have historically been a large source of entrepreneurial talent, which is needed in EA. We think promoting the creation of additional programs helps fill bottlenecks in the space.</li></ul></li><li><strong>Make it easy to start up university groups.&nbsp;</strong><ul><li>How: We do this through&nbsp;<a href=\"https://centreforeffectivealtruism.notion.site/centreforeffectivealtruism/University-Group-Accelerator-Program-6df8c8fccf8b4ffbb6488d9dfa275282\"><u>UGAP</u></a></li><li>Why: We think there is a lot of low-hanging fruit at unis that don\u2019t have EA groups and we want to make it easy for groups to be restarted if they are fading out. We have developed a product market fit for this with UGAP.</li></ul></li><li><strong>Reduce time costs in running groups.</strong><ul><li>How:&nbsp;<a href=\"https://resources.eagroups.org/home\"><u>Resource Centre</u></a></li><li>Why: We think group organizer time is valuable and want to reduce the amount of time they spend on repeated processes such as writing outreach emails.&nbsp;</li></ul></li></ul><h3>What are some things that we almost did and/or would love to do later?</h3><ul><li>More support for the most established groups<ul><li>While expanded mentorship opportunities will be available for advanced groups, we do not currently have many opportunities and resources for non-new groups. We would love to see more of this.</li></ul></li><li>Run retreats and events for non-UGAP participants (such as organizers from more advanced university groups)<ul><li>We think people\u2019s first retreat is often of high marginal value and can accelerate a group a lot. This is an area we would love to see developed more.</li></ul></li><li>Improving the resource centre<ul><li>We hope to expand capacity to be able to update and curate the best resources for group organizers.</li></ul></li></ul><h3>What are other things we would like to see?</h3><ul><li>Doing all of the above more thoroughly<ul><li>We have limited capacity and think there is a lot of room to improve our above efforts. In particular our mentorship program is very new and doesn\u2019t include trainings or resources specifically for new organizers at existing unis.</li></ul></li><li>Coordinate residencies for the start of the semester or<a href=\"https://forum.effectivealtruism.org/posts/fWTaBDBCQTg925gdg/idea-exchange-program-for-uni-groups\"><u> an organizer exchange program</u></a></li><li>Seed groups at important universities&nbsp;</li><li>Coordinate experimentation for groups</li><li>Facilitate regional networks of university groups</li><li>Set up regular sharing sessions for subgroups of the uni-group space</li><li>Create opportunities for promising group members to accelerate their engagement with EA quickly (such as impact generator workshops)</li><li>Coordinate VAs and consultants for professionalised university groups</li><li>Coordinate more internship opportunities for members of university groups</li></ul><h3>Feedback</h3><p>We are keen for feedback and input on our strategy and priorities. We intend to continue sharing updates on our work and hope that you\u2019ll share comments with us directly in the comments or via email at&nbsp;<a href=\"mailto:unigroups@centreforeffectivealtruism.org\"><u>unigroups@centreforeffectivealtruism.org</u></a>.&nbsp;</p><p><br>&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "Jesse Rothman"}}, {"_id": "2bfYxTt2FsGXnwDyt", "title": "A new place to discuss cognitive science, ethics and human alignment", "postedAt": "2022-11-04T14:34:13.625Z", "htmlBody": "<p>One could frame EA as the project of human alignment - of deconfusing ourselves about what we care about and figuring out how to actualize it. And there seems to be an interconnected bundle of problems at the core of this project:</p><ul><li>What should we fundamentally value?</li><li>How to distinguish rational intuitions from biases?</li><li>How to make an AGI care about these questions?</li><li>Which ideas should we spread to help humanity process these questions more complexly?</li><li>How certain can we be about all of this?</li></ul><p>These questions are particularly important, as we seem to live at the <a href=\"https://forum.effectivealtruism.org/topics/hinge-of-history/\">hinge of history</a> and at a time of a growth in consciousness research<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref83dx76xyi2n\"><sup><a href=\"#fn83dx76xyi2n\">[1]</a></sup></span>. They're interconnected but require a range of disciplines perhaps too wide for a single person to fully grasp - which suggests there could be a great added value in stronger cooperation.</p><p>So if you like impossible problems and want to work on them together, you're warmly welcome at the<strong> </strong><a href=\"https://www.facebook.com/groups/652418766529925/\"><i><strong>Mind &amp; Values Research Group</strong></i></a><i><strong>.</strong> </i>The group is about the intersections of EA, cognitive science, the nature of consciousness and intelligence, moral philosophy and the formulation &amp; propagation of ethical and rational principles.</p><p>How can these areas help nudge humanity in a positive directions?</p><p><strong>1. The philosophy of mind angle</strong></p><p>Deconfusing humanity about what we mean by values and intelligence could:</p><ul><li>Help solve the technical side of AI alignment<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflbqkns2j9t\"><sup><a href=\"#fnlbqkns2j9t\">[2]</a></sup></span>.</li><li>Advance the broad-longtermist mission introduced in <i>What We Owe the Future </i>- getting a clearer picture on which values should we lock in in these important times. Important topics here could be the nature of <a href=\"https://forum.effectivealtruism.org/topics/valence\">valence</a>, net-positiveness of experiences, <a href=\"https://pubmed.ncbi.nlm.nih.gov/32830051/\">animal</a> and <a href=\"https://qualiacomputing.com/2022/06/19/digital-computers-will-remain-unconscious-until-they-recruit-physical-fields-for-holistic-computing-using-well-defined-topological-boundaries/\">digital sentience</a>.</li><li>Help global prioritization by investigating the assumptions behind interventions for instance recommended by <a href=\"https://www.happierlivesinstitute.org/key-ideas\">Happier Lives</a>&nbsp;</li></ul><p><strong>2. The social change angle</strong></p><ul><li><a href=\"https://forum.effectivealtruism.org/topics/cognitive-enhancement\">Cognitive enhancement</a> research - how to support rationality &amp; moral circle expansions in society by formulating the most elegant case for rational ethics&nbsp;</li><li>Studying people's biases about values could help here<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmcqnanix96\"><sup><a href=\"#fnmcqnanix96\">[3]</a></sup></span>. This area can also be advanced with some inferences from experimental philosophy or even history &amp; sociology of ideas.</li></ul><h2>What could it lok like?</h2><ul><li><strong>Meetups: </strong>The group will vote on topics to discuss. For the following month or so, people will be welcome to collect materials from different angles. We'll discuss them in a virtual meetup and collect what was mentioned in a document people can get back to.</li><li><strong>Newsletter: </strong>If the group gets bigger and hard to follow, I'll create a newsletter to announce voting, meetups and to send out the notes. Meanwhile, I recommend turning on notifications for new posts.</li><li><strong>Networking: </strong>People looking for ideas or people to work with within an area are welcome to post even just a short introduction.</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn83dx76xyi2n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref83dx76xyi2n\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Which points against neglectedness but to brain research opening new possibilities and to existing (cognitive) resources to utilize. <a href=\"https://www.mdpi.com/2076-3425/10/1/41\">https://www.mdpi.com/2076-3425/10/1/41 </a><a href=\"https://www.proquest.com/docview/2703039855/fulltextPDF/3152D39660CF4000PQ/1?accountid=16531\">https://www.proquest.com/docview/2703039855/fulltextPDF/3152D39660CF4000PQ/1?accountid=16531</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlbqkns2j9t\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflbqkns2j9t\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See <a href=\"https://forum.effectivealtruism.org/posts/WdMnmmqqiP5zCtSfv/cognitive-science-psychology-as-a-neglected-approach-to-ai\">Sotala </a>or Superintelligence, pp. 406: <i>Should whole brain emulation research be promoted? </i>which indicates figuring out how human coherent extrapolation volition looks like could be of particular importance.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmcqnanix96\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmcqnanix96\">^</a></strong></sup></span><div class=\"footnote-content\"><p>An example here could be the research discussed in the 80k Hours podcast with <a href=\"https://80000hours.org/podcast/episodes/sharon-hewitt-rawlette-hedonistic-utilitarianism/\">Sharon H. Rawlette</a> and her <i>Feeling of Value</i>.</p></div></li></ol>", "user": {"username": "Daniel_Friedrich"}}, {"_id": "fhKJcrCmeTKEkn2kC", "title": "Which EA grants support service providers?", "postedAt": "2022-11-03T14:36:43.751Z", "htmlBody": "<p>Hello,&nbsp;</p><p>I founded a nonprofit org to uplift teens in-crisis.&nbsp;</p><blockquote><p>The BOOM works with youths on-probation, formerly incarcerated, in substance abuse treatment, and in the foster system. Our mission is to plant seeds for youth entrepreneurship and hardware engineering. Our goal is replicate our program in marginalized communities around the world.&nbsp;</p></blockquote><p>We're seeking grants to support our youth-development activities.&nbsp;</p><p>It seems that some (all?) EA grants are intended to promote Effective Altruism as a movement, rather than to directly support frontline service providers.&nbsp;</p><p>Is that true?</p><p>Thanks!</p>", "user": {"username": "The BOOM"}}, {"_id": "i3MnLKSzigLBz88ma", "title": "Recruiting Skilled Volunteers", "postedAt": "2022-11-03T14:36:43.762Z", "htmlBody": "<p>Hello,&nbsp;</p><p>I founded a nonprofit org to uplift teens in-crisis. We work with youths on-probation, formerly incarcerated, in substance abuse treatment, and in the foster system.&nbsp;</p><p><i><strong>The BOOM youth mentoring program exists to propel disadvantaged teens into futures of achievement, excellence, and prosperity. Our mission is to plant seeds for entrepreneurship and hardware engineering.&nbsp;</strong></i></p><p><i><strong>Winner&nbsp;of Protolabs&nbsp;</strong></i><a href=\"http://www.boo.ma/cool-idea-award\"><i><strong>Cool Idea</strong></i></a><i><strong> design award, The BOOM teaches teen apprentices&nbsp;who have limited economic resources&nbsp;</strong>to fabricate and market handmade electronic hardware.&nbsp;</i></p><p><i>The BOOM has been featured in&nbsp;</i><a href=\"https://boo.ma/eejournal\"><i>Electronic Engineering Journal</i></a><i>,&nbsp;</i><a href=\"https://boo.ma/makezine\"><i>Make Magazine</i></a><i>, and other journals. Partners include&nbsp;</i><a href=\"https://ewb-sfp.org/community-engineering-corps\"><i>Engineers Without Borders</i></a><i>.&nbsp;We completed an&nbsp;</i><a href=\"https://www.autodesk.com/technology-centers/residency\"><i>Autodesk</i></a><i>&nbsp;Residency.&nbsp;</i></p><p><i>Our goal is replicate our program in marginalized communities around the world.&nbsp;</i></p><p>We seek experienced electronics hardware engineers to donate a couple of hours per week. Is the EA community an appropriate path for us?</p>", "user": {"username": "The BOOM"}}, {"_id": "AuhkDHEuLNxqx9rgZ", "title": "A new database of nanotechnology strategy resources ", "postedAt": "2022-11-05T05:20:38.359Z", "htmlBody": "<p><i>Target audience: people who are considering trying out nanotechnology strategy research, or who want to learn more about nanotechnology (strategy)-related things.</i></p><p>Marie Davidsen Buhl and I have made a&nbsp;<a href=\"http://www.bensnodin.com/research_articles/nanotech_resources_database/\">database of resources relevant for nanotechnology strategy research</a>. It contains over 40 articles, sorted by relevance for people new to nanotechnology strategy research.</p><h3>Additional context:</h3><p>Why nanotechnology strategy research might be important from a longtermist EA perspective: Advanced nanotechnology might arrive in the next couple of decades (my wild guess: there\u2019s a 1-2% chance in the absence of transformative AI) and could have very positive or very negative implications for existential risk. There has been relatively little high-quality thinking on how to make the arrival of advanced nanotechnology go well, and I think there should be more work in this area (very tentatively, I suggest we want 2-3 people spending at least 50% of their time on this by 3 years from now).</p><p>See also my&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/oqBJk2Ae3RBegtFfn/my-thoughts-on-nanotechnology-strategy-research-as-an-ea\"><u>EA Forum post on this area</u></a> from earlier this year, and the <a href=\"https://80000hours.org/problem-profiles/atomically-precise-manufacturing/\"><u>80000 Hours cause profile</u></a>.</p><p>Marie and I created this resources database as part of our roles at&nbsp;<a href=\"http://rethinkpriorities.org/\">Rethink Priorities</a>.</p>", "user": {"username": "Ben_Snodin"}}, {"_id": "EafGgKEwzE4uQfKwv", "title": "Savings as donations", "postedAt": "2022-11-06T11:10:45.392Z", "htmlBody": "<h1>Saving vs. Donating</h1><p>I think that effective altruists need a new kind of financial instrument.<br><br>I feel inspired to:</p><ul><li><a href=\"https://www.mrmoneymustache.com/2012/01/13/the-shockingly-simple-math-behind-early-retirement/\">Save 64% of my income so that I can retire after 10ish years of working.</a> Under reasonable assumptions,<a href=\"https://www.mrmoneymustache.com/2012/05/29/how-much-do-i-need-for-retirement/\"> 25x your yearly expenditure means you can \"retire\" forever</a>. I wouldn't use this to lounge around on a beach forever, but I could take time off &nbsp;whenever I wanted and only work on things that were very important or very fun.&nbsp;</li><li>Donate large portions of my income to the most effective opportunities I can spot.</li></ul><p>Currently, I see these two goals as being in conflict. I want to save but I also want to donate.&nbsp;</p><p>I want both, and I could save now and donate later \u2013 but I don't fully trust my future-self to stick to the plan.&nbsp;</p><p>I think the right kind of financial instrument would align these parts of me such that I could satisfy my cravings for savings <i>and</i> donate much more than I otherwise would have. Win-win.</p><p>(epistemic status: I've thought about this for like 30 minutes and it seems reasonable to me).</p><h1>The basic mechanism</h1><p>The alternative investment option would:</p><ol><li>Keep all of my money invested and pay me 4% per year (with the option to reinvest this if I chose).</li><li>Donate all of the money at the end of my life to effective charities.</li></ol><p>Provided the mechanism for (2) was sufficiently robust, I could reasonably consider every dollar given to this fund as a donation to an effective charity. I would actually donate more this way, I think, because I would not feel the need to save for myself.</p><p>For the rest of the post I'm going to call it a <i>Personal Donation Fund, </i>or <i>PDF.</i></p><h1>Big Purchases and emergencies</h1><p>The proposal above isn\u2019t ideal if I want to buy a house, or if I need money in an emergency. Let's deal with each of these individually.&nbsp;</p><p>Suppose I need to pay emergency medical expenses. I could give the option to take money out of the <i>PDF</i> with a promise to pay it back later. (Each year the amount owed would be adjusted for inflation). I could spend this money that I've \"borrowed\" on the emergency expenses. I would pay back these loans later. If I failed to pay them back, then the loan amount should come out of my estate if I failed to pay them back.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefju5wuscnrf\"><sup><a href=\"#fnju5wuscnrf\">[1]</a></sup></span></p><p>For buying a house I would (at least) need a deposit. I could borrow the same way as above, but this might make it hard to secure a loan for the rest of the house. An alternative would be to co-buy the house. This way, when the house was sold the <i>PDF</i> and I would both receive a portion of the profits. From the perspective of the <i>PDF</i>, this would be just like making another investment. &nbsp;&nbsp;</p><h1>Open Questions and Opportunities</h1><ul><li>I would be personally willing to use a <i>PDF </i>if I could, would you?</li><li>Have I missed any salient features that seem really important?</li><li>I do not currently feel like this would be my comparative advantage to set up, maybe you know someone who would like to set it up?</li><li>It's possible that this would work best as a trust, in which case you might need other trustees. I'm not sure exactly what the best set up would look like.</li></ul><h1>Acknowledgements</h1><p>Jo Small helped shape my thinking on this. Tyrone Barugh and Ismam Huda gave me helpful comments on an earlier draft.&nbsp;</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnju5wuscnrf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefju5wuscnrf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It is possible that my estate would also not be able to pay the loan. In this case, I'm sortof okay with donating less, as long as I genuinely needed the money and made an effort to pay it back. If so, in some sense I really couldn't afford to donate so much \u2013 I just only found out later.&nbsp;<br><br>Another concern could be that I could borrow money from the <i>PDF </i>and gift it to others (for example my children). So long as my net worth was less than the amount owed, this would retract previous donations. In my own personal case, I still expect the <i>PDF </i>to increase my total donations (in expectation) \u2013 so this doesn't feel like an important concern for me personally.</p></div></li></ol>", "user": {"username": "rileyharris"}}, {"_id": "d336WFwLLqe8ufHn5", "title": "Open Letter Against Reckless Nuclear Escalation and Use", "postedAt": "2022-11-03T15:08:15.107Z", "htmlBody": "<p>The <a href=\"https://forum.effectivealtruism.org/topics/future-of-life-institute\">Future of Life Institute</a> (FLI) published an open letter against reckless nuclear escalation and use. It only took me about 1 min to read it and add my signature. I believe it was the most cost-effective action of my week, and I encourage EA Forum readers to sign it too!</p><p>I guess signing the letter leads to a reduction in existential risk of the order of magnitude of 10^-13 (=10^-(3 + 6 + 4)), assuming:</p><ul><li>The <a href=\"https://forum.effectivealtruism.org/topics/existential-risk\">existential risk</a> due to nuclear war from 2021 to 2120 is 0.1 %<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2v3lhmjzlyr\"><sup><a href=\"#fn2v3lhmjzlyr\">[1]</a></sup></span>, as guessed by Toby Ord in Table 6.1 of <a href=\"https://theprecipice.com/\">The Precipice</a>.</li><li>The existential risk reduction caused by FLI's open letter is 10^-6 of the above if 10 k people sign the letter.</li></ul><p>10^-13 might seem small, but it is of the same order of magnitude of the annual existential risk footprint of the mean human, which I estimated <a href=\"https://forum.effectivealtruism.org/posts/BxwSMFnuDgB4Fovaw/how-much-donations-are-needed-to-neutralise-the-annual-x\">here</a> to be 2*10^-13. The concept of personal footprint is flawed in many important ways, and my calculations are quite speculative. That being said, I still think they point to the high cost-effectiveness of signing <a href=\"https://futureoflife.org/open-letter/open-letter-against-reckless-nuclear-escalation-and-use/\">the letter</a>. For example, I would be very happy to cancel my carbon footprint for a cost of 1 min!</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2v3lhmjzlyr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2v3lhmjzlyr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I have also estimated the probabilility of extinction due to nuclear in the 21st century to be <a href=\"https://forum.effectivealtruism.org/posts/EixrWYgkox7noLacn/probability-of-extinction-for-various-types-of-catastrophes\">0.3 %</a>.</p></div></li></ol>", "user": {"username": "vascoamaralgrilo"}}, {"_id": "3sh8rsxYMQN6rrxu8", "title": "Should we be doing politics at all? Some (very rough) thoughts", "postedAt": "2022-11-03T18:18:41.790Z", "htmlBody": "<p><i>This bit of pondering was beyond the scope of the manuscript I was writing (a followup to </i><a href=\"https://forum.effectivealtruism.org/posts/yn2ozSswZuiKKBCtJ/the-rodent-birth-control-landscape\"><i>this post</i></a><i>, which is why the examples are all about anti-rodenticide interventions), but I still wanted to share it. Cut from rough draft, lightly edited it so it would make sense to Forum readers and to make the tone more conversational.</i></p><hr><p>It is often difficult to directly engage in political campaigns without incentives to lie or misrepresent. This is exacerbated by the differences in expected communication styles in politics vs the general public vs EA. There is a tradition of strong arguments in broader EA (+rationality) culture for EAs to entirely steer away from politics for both&nbsp;<a href=\"https://www.lesswrong.com/posts/9weLK2AJ9JEt2Tt8f/politics-is-the-mind-killer\"><u>epistemic</u></a> and&nbsp;<a href=\"https://www.overcomingbias.com/2019/03/tug-sideways.html\"><u>effectiveness</u></a> reasons. I find these arguments persuasive but wonder whether they have become an unquestioned dogma.&nbsp;</p><p>We don't hold any other class of interventions to the standards we hold political interventions. I find it hard to believe that effective charities working in developing countries never have to do ethically dubious things, like giving bribes, because refusing to do so would make it impossible to get anything done within the local culture. Yet EAs often consider it unacceptable for other EAs to engage in \"politician-speak\" or play political games to win a valuable election.</p><p>A major practical objection to political interventions is that they may swiftly age poorly or lock orgs/EA/society into particular positions rather than leaving them flexible to pursue the goals that motivated the original political push. Well-intended regulations today (full of compromises and ignorant of what may become important distinctions) could lead to difficulties in implementing better solutions tomorrow. This is a serious downside.</p><ul><li>When considering legal bans on rodenticides, I foresee these failure modes:<ul><li>Most likely failure mode: after great effort, resources, and political capital spent, second gen anticoagulant bans succeed in some areas, and pest management professionals just switch to first gen or non-anticoagulants that are nearly as bad for rodent and off-target animal welfare.</li><li>Worse alternative than rodenticides are developed in response, and the political strategy becomes an arms race.</li><li>Successful campaign to ban rodenticides, but closing loopholes, enforcing the ban, and enforcing <i>enforcement </i>of the ban by governments becomes never-ending followup job.</li></ul></li></ul><p>A moral qualm I have about EA playing the political game is that high leverage political campaigns may actually tend to subvert democracy, which might be bad in itself (depending on the situation, I think) and may lead to blowback against the political aim, EA, and/or a field like wild animal welfare. \u201cHigh leverage\u201d and \"democratic political process\" may be fundamentally at odds. EA is looking for interventions that will have the greatest effect <i>for our goals </i>with the fewest resource. If you consult the electorate about <i>their goals</i> and how they should be achieved, they rarely strongly agree on the thing we want to do. Voting is hardly an efficient market, but I still don't think political interventions are the place to go to find $1000 bills on the ground. If a lot of people already agree on a course of action, chances are it's not going to be a high leverage intervention because it's either 1) already being done, or 2) ineffective, empty, or symbolic.</p><p>EAs have leaned on legislative hacks like ballot measures for several of our greatest farmed animal welfare victories to date, most notably California\u2019s Proposition 12 and Massachusetts\u2019s Proposition 3. However, a major aspect of those amendments, the ban of animal products imported from other states with lower welfare standards, is now being&nbsp;<a href=\"https://thecounter.org/california-proposition-12-animal-welfare-supreme-court/\"><u>challenged by the pork industry in the US Supreme Court</u></a>. It is precisely because of the ease of special interests using that avenue (i.e. the high leverage for EA) that those amendments are now being seen as possibly illegitimate. (Massachusetts prop 3 passed with like 70% of the vote iirc, which makes me feel that it has a mandate at least in Massachusetts, regardless of whether MA has the right to impose limits on interstate commerce. So one guideline here may be \u201cEA political victories on contentious issues have to be landslides\u201d.)&nbsp;</p><p>Even if political accomplishments withstand judicial review, some EAs fear we may lose the public\u2019s good will if we are seen to be lobbying rather than faithfully attending to doing the most good. However, this point depends a lot on what one believes someone who is motivated to do the most good would do. I humbly submit that EAs engage in typical mind fallacy on this-- hands-off abstract theorizing is not everyone\u2019s idea of doing good. Many people may perceive EA or WAW as less sincere if we&nbsp;<i>don\u2019t&nbsp;</i>engage in contentious politics.&nbsp;</p><p>One positive of political campaigning is that this may be a more legible and straightforward demonstration of commitment to our goals for many non-EAs than many of the activities EAs consider highest impact. Campaigning to stop people using harmful rat poison is a lot more understandable and potentially persuasive to the uninitiated than, say, trying to remove fishery quotas to help the fish.&nbsp;</p><p>I think the reason that bribing local officials to establish new delivery routes for bednets would not be seen by most EAs as threatening to the epistemics or theory of change behind the intervention is that we have more distinct buckets for \"model of the world/theory of change/selecting intervention\" and \"executing intervention\" when we're thinking about public health &nbsp;than we do when thinking about politics. Granted, it's harder to separate the epistemics from the intervention when the intervention is selling yourself and your views than when it's distributing medicine, but I still think we could do a much better job mentally distinguishing political intervention from our models of the system. There's a suspicion of politics in much of the community (and particularly in the rationality community) that is out of proportion considering the danger that engaging in any intervention represents to someone who isn't distinguishing \"trying to achieve an instrumental goal\" with \"what's true and what my goals should be\".</p><p>The temptation to corruption in politics is a serious concern for individual EAs and EA in general, but there's a very compelling expected value argument for pursuing political remedies and political power. I propose that, to be able to access that value with less threat to the epistemics, models, and values of the movement, we have stronger distinctions between EA prioritization and the execution of actual interventions. This could be achieved through compartmentalizing tasks or job descriptions so that people who \"do prioritization\" are not the same as those that \"do politics\".&nbsp;</p>", "user": {"username": "Holly_Elmore"}}, {"_id": "nB778dXNsHqHthFC5", "title": "Metaforecast late 2022 update: GraphQL API, Charts, better infrastructure behind the scenes.", "postedAt": "2022-11-04T17:56:04.967Z", "htmlBody": "<p><strong>tl;dr</strong>:<a href=\"https://metaforecast.org/\">&nbsp;<u>Metaforecast</u></a> is a search engine and an associated repository for forecasting questions. Since our&nbsp;<a href=\"https://metaforecast.substack.com/p/metaforecast-update-better-search\"><u>last update</u></a>, we have added a GraphQL API, charts, and dashboards. We have also reworked our infrastructure to make it more stable.&nbsp;</p><h2><strong>New API</strong></h2><p>Our most significant new addition is our GraphQL API. It allows other people to build on top of our efforts. It can be accessed on&nbsp;<a href=\"https://metaforecast.org/api/graphql\"><u>metaforecast.org/api/graphql</u></a>, and looks similar to the EA Forum's&nbsp;<a href=\"https://forum.effectivealtruism.org/graphiql\"><u>own graphql api</u></a>.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1668005904/mirroredImages/nB778dXNsHqHthFC5/etiygy13q5t7pewiqb4z.png\"></p><p>To get the first 1000 questions, you could use a query like:&nbsp;</p><pre><code>{\n &nbsp;questions(first: 1000) {\n &nbsp;&nbsp;&nbsp;edges {\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;node {\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;id\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;title\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;url\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;description\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;options {\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;probability\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;qualityIndicators {\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;numForecasts\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stars\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;timestamp\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}\n &nbsp;&nbsp;&nbsp;}\n &nbsp;&nbsp;&nbsp;pageInfo {\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;endCursor\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;startCursor\n &nbsp;&nbsp;&nbsp;}\n &nbsp;}\n}</code></pre><p>You can find more examples, like code to download all questions, in our&nbsp;<a href=\"https://github.com/quantified-uncertainty/metaforecast/tree/master/scripts\"><u>/scripts</u></a> folder, to which we welcome contributions.</p><h2><strong>Charts and question pages.</strong></h2><p>Charts display a question's history. They look as follows:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1668005904/mirroredImages/nB778dXNsHqHthFC5/nk0t0ws8uars8mpkwtay.png\"></p><p>Charts can be accessed by clicking the expand button on the front page although they are fairly slow to load at the moment.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1668005904/mirroredImages/nB778dXNsHqHthFC5/plp73yeqkfil4dxod2h8.png\"></p><p>Clicking on the expand button brings the user to a question page, which contains a chart, the full question description, and a range of quality indicators:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1668005904/mirroredImages/nB778dXNsHqHthFC5/gvaja9ljtddjpoppkkh4.png\"></p><p>We are also providing an endpoint at&nbsp;<i>metaforecast.org/questions/embed/[id]</i> to allow other pages to embed our charts. For instance, to embed a question whose id is&nbsp;<i>betfair-1.178163916</i>, the endpoint would be<a href=\"https://metaforecast.org/questions/embed/betfair-1.178163916\">&nbsp;<u>here</u></a>. One would use it in the following code:&nbsp;</p><pre><code>&lt;iframe src=\"https://metaforecast.org/questions/embed/betfair-1.178163916\" height=\"200\" width=\"300\" title=\"Metaforecast question\"&gt;&lt;/iframe&gt;</code></pre><p>You can find the necessary question id by clicking a toggle under \"advanced options\" on the frontpage, or simply by noticing the id in our URL when expanding the question.</p><p>With time, we aim to improve these pages, make them more interactive, etc. We also think it would be a good idea to embed Metaforecast questions and dashboards into the EA Forum, and we are trying to collaborate with the <a href=\"https://github.com/ForumMagnum/ForumMagnum/pull/6015\">Manifold team</a>, who have&nbsp;<a href=\"https://github.com/ForumMagnum/ForumMagnum/pull/4907\"><u>done this before</u></a>, to make that happen.&nbsp;</p><h2><strong>Dashboards</strong></h2><p>Dashboards are collections of questions. For instance,&nbsp;<a href=\"https://metaforecast.org/dashboards/view/561472e0d2?numCols=2\"><u>here</u></a> is a dashboard on global markets and inflation, as embedded in&nbsp;<a href=\"https://globalguessing.com/russia-ukraine-forecasts/\"><u>Global Guessing</u></a>.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1668005904/mirroredImages/nB778dXNsHqHthFC5/uksijp04bsvgckvfopgf.png\"></p><p>Like questions, you can either&nbsp;<a href=\"http://metaforecast.org/dashboards/view/561472e0d2?numCols=2\"><u>view dashboards directly</u></a>, or&nbsp;<a href=\"http://metaforecast.org/dashboards/embed/561472e0d2?numCols=2\"><u>embed</u></a> them. You can also create them, at&nbsp;<a href=\"https://metaforecast.org/dashboards\"><u>https://metaforecast.org/dashboards</u></a>.</p><h2><strong>Better infrastructure</strong></h2><p>We have also revamped our infrastructure. We moved to from JavaScript to Typescript, from MongoDB to Postgres, and simplified our backend.&nbsp;</p><h2><strong>We are open to collaborations</strong></h2><p>We are very much open to collaborations. If you want to integrate Metaforecast into your project and need help do not hesitate to reach out, e.g., on our&nbsp;<a href=\"https://github.com/quantified-uncertainty/metaforecast/issues\"><u>Github</u></a>.&nbsp;</p><p>Metaforecast is also open source, and we welcome contributions. You can see some to-dos&nbsp;<a href=\"https://github.com/quantified-uncertainty/metaforecast#to-do\"><u>here</u></a>. Developing is going more slowly now because it's mostly driven by Nu\u00f1o working in his spare time, so contributions would be counterfactual.&nbsp;</p><h2><strong>Acknowledgements</strong></h2><figure class=\"image image_resized\" style=\"width:26.44%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668005905/mirroredImages/nB778dXNsHqHthFC5/rwrne2s5sucpflb1dmuu.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/046c828a4a3e65b158aab8691038a0ac5220c7937c8aabd1.png/w_97 97w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/046c828a4a3e65b158aab8691038a0ac5220c7937c8aabd1.png/w_177 177w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/046c828a4a3e65b158aab8691038a0ac5220c7937c8aabd1.png/w_257 257w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/046c828a4a3e65b158aab8691038a0ac5220c7937c8aabd1.png/w_337 337w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/046c828a4a3e65b158aab8691038a0ac5220c7937c8aabd1.png/w_417 417w\"></figure><p>Metaforecast is hosted by the<a href=\"https://quantifieduncertainty.org/\"><u> Quantified Uncertainty Research Institute</u></a>, and has received funding from<a href=\"https://astralcodexten.substack.com/p/acx-grants-results\">&nbsp;<u>Astral Codex Ten</u></a>. It has received significant contributions from&nbsp;<a href=\"https://berekuk.ru/\"><u>Vyacheslav Matyuhin</u></a>, who was responsible for the upgrade to Typescript and GraphQL. Thanks to Clay Graubard of&nbsp;<a href=\"https://globalguessing.com/\"><u>Global Guessing</u></a> for their comments and dashboards, to <a href=\"https://insightprediction.com/\">Insight Prediction</a> for help smoothing out their API, to Nathan Young for general comments, to others for their comments and suggestions.</p>", "user": {"username": "NunoSempere"}}]