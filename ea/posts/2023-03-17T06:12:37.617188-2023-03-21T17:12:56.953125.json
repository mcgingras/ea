[{"_id": "FG2zA9Mny3yHoCowB", "title": "EA London Quarterly Review Day ", "postedAt": "2023-03-21T16:53:01.830Z", "htmlBody": "<p>Reflecting on your life, your goals and making plans for how you want to achieve them can be stressful but if you\u2019re looking to have a positive impact then it is also very important. To encourage this, I'm hosting a supportive life reflection day at the LEAH office.</p><p>The structure will be self directed 50 min pomodoros:</p><p>10:30am - Arrive at co-working space, introductions etc. Share plans, arrange pomodoros</p><p>11:00am - 50 minute pomodoro</p><p>11:50am - Break</p><p>12:00pm - 50 minute pomodoro</p><p>12:50pm - Lunch and chat</p><p>1:00pm - 50 minute pomodoro</p><p>1:50pm - Break</p><p>2:00pm - 50 minute pomodoro</p><p>2:50pm - Break</p><p>3:00pm - 50 minute pomodoro</p><p>3:50pm - Break</p><p>4:00pm - 50 minute pomodoro</p><p>4:50pm - Break and wind down</p><p>While I envision this as mostly self directed, they\u2019ll be a few of us who have done these before who would be happy to help.</p><p>These could be used to take a step back and reflect on your habits, goals, plans or systems over the course of an afternoon. It can be a bit daunting if you haven't done this before so I thought some friendly faces, good tunes and support from other people doing the same could help.</p><p>Google doc of resources: <a href=\"https://docs.google.com/document/d/1cMgnynqCkqMC5SnPvsQaCnZciAnMCeXR30dmXK-C9Ms/edit#\">EA London Quarterly Review Sessions - Google Docs</a><br>Group chat:&nbsp;<a href=\"https://chat.whatsapp.com/JSmIUKovUI0KpptMMnfFfC\"><u>https://chat.whatsapp.com/JSmIUKovUI0KpptMMnfFfC</u></a></p>", "user": {"username": "Gemma Paterson"}}, {"_id": "FFjk2bJTxu5hsSj6X", "title": "Unjournal Evals: \"Advance Market Commitments: Insights from Theory and Experience\"", "postedAt": "2023-03-21T16:59:28.466Z", "htmlBody": "<h1>Overview in bullet-point form</h1><h2><br>The original paper: Advanced Market Commitments: Insights from Theory and Experience by Michael Kremer Jonathan D. Levin Christopher M. Snyder</h2><ul><li>A. <a href=\"https://bpb-us-e1.wpmucdn.com/sites.dartmouth.edu/dist/5/2287/files/2021/02/w26775.pdf\">NBER link</a>; <a href=\"https://bpb-us-e1.wpmucdn.com/sites.dartmouth.edu/dist/5/2287/files/2021/02/w26775.pdf\">B. 1-click link</a></li><li>Advance market commitments for vaccines/drugs: practicable policy, seems highly relevant to both global health and development priorities, catastrophic risk &nbsp;pandemic risk&nbsp;</li><li>Super-prominent authors (e.g., Kremer=Nobel), Snyder promoted related work in NYT op-ed</li><li>Paper was in AER P&amp;P, not fully peer-reviewed</li><li>Authors present the economic theory case for AMCs (model in appendix), differentating technologically close and distant &nbsp;products</li><li>Make specific empirical claims about the value generated, based on &nbsp;'spreadsheet empirics'/BOTECS comparing to alternative vaccines not under AMC<ul><li>E.g., \" if PCV coverage in GAVI countries converged to the global rate at the slower rate of the rotavirus vaccine ... &nbsp;67 million fewer children under age 1 would have been immunized, amounting to a loss of over 12 million DALYs.\"&nbsp;</li></ul></li></ul><p>&nbsp;</p><h1><br><a href=\"https://unjournal.pubpub.org/pub/amcmetrics\">My evaluation summary</a></h1><p>... &nbsp;is largely a 'behind the scenes' of this process</p><ul><li>Why we chose this (<a href=\"https://forum.effectivealtruism.org/posts/kftzYdmZf4nj2ExN7/what-pivotal-and-useful-research-would-you-like-to-see#1__Kremer_et_al___Advance_Market_Commitments_\">see earlier post</a>)</li><li>Authors' positive engagement</li><li>appropriate and very sorely needed for continued and expanded use in global health and disease elimination campaigns.&nbsp;</li><li>Notes on overlooked &nbsp;replication spreadsheet linked to AER P&amp;P &nbsp;supplemental material</li></ul><p>&nbsp;</p><p><i>All three evaluators generally spoke positively about the paper and gave it good evaluations. See metrics </i><a href=\"https://unjournal.pubpub.org/pub/amcmetrics#metrics\"><i>HERE</i></a><i>.</i></p><p><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl0todp4ynw9\"><sup><a href=\"#fnl0todp4ynw9\">[1]</a></sup></span></p><h2><a href=\"https://unjournal.pubpub.org/pub/evalmanheim/release/1\">Evaluation 1: David Manheim</a></h2><ul><li>Mostly very positive</li><li>Wanted more discussion of alternative (non-AMC) approaches to 'theoretically distant targets', outside of 'push vs pull' framing</li><li>Manheim on AMCs:&nbsp;<ul><li>\"appropriate and very sorely needed for continued and expanded use in global health and disease elimination campaigns\"</li><li>and to \"accelerate vaccines for known pandemic-potential pathogen threats\" &nbsp;</li><li><a href=\"https://unjournal.pubpub.org/pub/evalmanheim/release/1\">of \"more limited\" usefulness</a> for novel diseases and technologically more-distant targets; for Covid, the bottleneck was &nbsp;testing/approval and manufacturing</li><li>\"Other approaches... more appropriate for the most critical future biological risks\" (from <a href=\"https://unjournal.pubpub.org/pub/amcmetrics/release/5#ratings\">comment on \"relevance to global priorities\"</a>, rated 60/100)</li></ul></li></ul><h3><a href=\"https://unjournal.pubpub.org/pub/9pdnpn3o/release/1\"><i>Authors' response</i>:&nbsp;</a></h3><ul><li>Agree other mechanisms are suitable elsewhere, including for Covid, link their other papers comparing and discussing these in detail, and give some summary of this</li><li>Discussed AMC here because it was tried at a large scale; \"We are not aware of a recent R&amp;D prize of a similar magnitude to compare to.\"</li></ul><p>&nbsp;</p><h2><a href=\"https://unjournal.pubpub.org/pub/evaltortorice/release/1\">Evaluation 2: Dan Tortorice</a></h2><ul><li>Brief report</li><li>Recommended further empirical work estimating the benefit, with different counterfactuals</li><li>Suggests future theoretical modeling work considering 'permanence' of AMC, co-financing requirements, a multi-period model, and socially-price-constrained manufacturers</li></ul><h3><a href=\"https://unjournal.pubpub.org/pub/9pdnpn3o/release/1\"><i>Authors' response </i>:&nbsp;</a></h3><ul><li>Agree on cautious interpretation of \"our empirical exercise\"</li><li>&nbsp;On \"how to fund an AMC in perpetuity or... &nbsp;transition off,\u201d&nbsp;<ul><li>\"the AMC for a pneumococcal vaccine included a provision committing firms to cap their prices close to marginal cost. The idea is that the AMC's top-up payments provide sufficient incentive for R&amp;D and capacity investments even with firms\u2019 charging prices close to marginal cost. Firms\u2019 price-cap commitments ensure that vaccines are affordable in the long-run.\"</li></ul></li></ul><h2><br><br><a href=\"https://unjournal.pubpub.org/pub/evaltan\">Evaluation 3: Joel Tan&nbsp;</a></h2><ul><li>Was asked to focus on specific robustness-checking work</li><li>Considered three distinct diseases for comparisons</li><li>\"KLS's results should be fairly robust ... the coverage rates for alternative comparator vaccines are in fact lower than the mainline RV comparator KLS chose\"</li><li>Suggests other issues that may affect the accuracy of the final estimates of DALYs averted, which he suspects imply some overstatement in net:<ul><li>May overstate gain because it relies on an old estimate and &nbsp;\"DALYs lost per capita to pneumococcus were declining in poor countries year-on-year for two decades even before the AMC\"</li><li>On the other hand could understate it because they \"do not model the speeding up of the development of existing vaccines\"</li></ul></li></ul><h3><a href=\"https://unjournal.pubpub.org/pub/9pdnpn3o/release/1\"><i>Authors' response:</i>&nbsp;</a></h3><ul><li>General agreement</li><li>\"Note that the overall calculation only considers the benefits from the faster rollout of the PCV starting from the observed date of availability, taken as given. It understates the value of the AMC to the extent that the AMC accelerated the vaccine\u2019s development.\"<br><br>&nbsp;</li></ul><h2><a href=\"https://unjournal.pubpub.org/pub/9pdnpn3o/release/1\">Authors' responses</a></h2><p>Generally positive, appreciative, and fairly detailed. See interstitials above.</p><p>&nbsp;</p><h1>Final note</h1><p>There seem to be some obvious next steps to take in robustness-checking the estimates. This would be suitable for an advanced student project or independent research project, which could also publicly build career capital. &nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl0todp4ynw9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl0todp4ynw9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As a semi-caveat to this, note that this was far from 'double-blind': all evaluators signed their work, and the paper came from prominent authors and was known to have been accepted by AER P&amp;P, albeit not a peer-reviewed journal.<i>&nbsp;</i></p></div></li></ol>", "user": {"username": "david_reinstein"}}, {"_id": "fXkCcsyF8M6dp6sXx", "title": "Where I'm at with AI risk: convinced of danger but not (yet) of doom ", "postedAt": "2023-03-21T13:23:11.183Z", "htmlBody": "<p><i>[content: discussing AI doom. I'm sceptical about AI doom, but if dwelling on this is anxiety-inducing for you, consider skipping this post]</i></p><p>I\u2019m a cause-agnostic (or more accurately \u2018cause-confused\u2019) EA with a non-technical background. &nbsp;A lot of my friends and <a href=\"https://amber-dawn-ace.com/\">writing clients</a> are extremely worried about existential risks from AI. Many believe that humanity is more likely than not to go extinct due to AI within my lifetime. &nbsp;</p><p>I realised that I was confused about this, so I set myself the goal of understanding the case for AI doom, and my own scepticisms, better. I did this by (very limited!) reading, writing down my thoughts, and talking to friends and strangers (some of whom I recruited from the <a href=\"https://www.facebook.com/groups/1781724435404945\">Bountied Rationality</a> Facebook group - if any of you are reading, thanks again!) Tl;dr: <strong>I think there are good reasons to worry about extremely powerful AI, but I don\u2019t yet &nbsp;understand why people think superintelligent AI is highly likely to end up killing everyone by default.</strong></p><h1><br>Why I'm writing this</h1><p><br>I\u2019m writing up my current beliefs and confusions in the hope that readers will be able to correct my misconceptions, clarify things I\u2019m confused about, and link me to helpful resources. I also personally enjoy reading other EAs\u2019 reflections about cause areas: e.g. Saulius' post on <a href=\"https://forum.effectivealtruism.org/posts/saEQXBgzmDbob9GdH/why-i-no-longer-prioritize-wild-animal-welfare-edited\">wild animal welfare</a>, or Nu\u00f1o's <a href=\"https://forum.effectivealtruism.org/posts/L6ZmggEJw8ri4KB8X/my-highly-personal-skepticism-braindump-on-existential-risk\">sceptical post about AI risk</a>. This post is far less well-informed, but I found those posts valuable because of their reasoning transparency more than their authors' expertise. I'd love to read more posts by \u2018layperson\u2019 EAs talking about their personal cause prioritisation.</p><p>I also think that 'confusion' is an underrepresented intellectual position. At EAGx Cambridge, Yulia Ponomarenko led a great workshop on \u2018Asking daft questions with confidence\u2019. We talked about how EAs are sometimes unwilling to ask questions that would make them less confused for fear that the questions are too basic, silly, \u201cdumb\u201d, or about something they're already expected to know.</p><p>This could create a <a href=\"https://forum.effectivealtruism.org/posts/BdWwgXrpncgdE4u5M/the-illusion-of-consensus-about-ea-celebrities\">false appearance of consensus</a> about cause areas or world models. People who are convinced by the case for AI risk will naturally be very vocal, as will those who are confidently sceptical. However, people who are unsure or confused may be unwilling to share their thoughts, either because they're afraid that others will look down on them for not already understanding the case, or just because most people are less motivated to write about their vague confusions than their strong opinions. So I\u2019m partly writing this as representation for the \u2018generally unsure\u2019 point of view.</p><p>Some caveats: there\u2019s a lot I haven\u2019t read, including many basic resources. And my understanding of the technical side of AI (maths, programming) is extremely limited. Technical friends often say \u2018you don\u2019t need to understand the technical details about AI to understand the arguments for x-risk from AI\u2019. But when I talk and think about these questions, it subjectively feels like I run up again a lack of technical understanding quite often.&nbsp;</p><h1>Where I\u2019m at with AI safety</h1><p><strong>Tl;dr: I'm concerned about certain risks from misaligned or misused AI, but I don\u2019t understand the arguments that AI will, by default and in absence of a specific alignment technique, be so misaligned as to cause human extinction (or something similarly bad.)</strong></p><p>&nbsp;</p><h2>Convincing (to me) arguments for why AI could be dangerous</h2><p>&nbsp;</p><h3>Humans could use AI to do bad things more effectively<br>&nbsp;</h3><p>For example, politicians could use AI to devastatingly make war on their enemies, or CEOs could use it to increase their profits in harmful or reckless ways. This seems like a good reason to regulate AI development heavily and/or to democratise AI control, so that it\u2019s harder for powerful people to use AI to further entrench their power.&nbsp;<br>&nbsp;</p><h3>We don\u2019t know how AIs work, and that\u2019s worrying</h3><p><br>AIs are becoming freakishly powerful really fast. The capabilities of Midjourney, Gato, GPT-4, Alphafold and more are staggering. It\u2019s worrying that even AI developers don\u2019t really understand how this happens. Interpretability research seems super important.</p><p>&nbsp;</p><h3>AI is likely to cause societal upheaval</h3><p><br>For example, AI might replace most human jobs over the next decades. This could lead to widespread poverty and unrest if politicians manage this transition badly. It could also cause a crisis in meaning; humans could no longer derive their self-worth or self-esteem from their 'usefulness' or creative talents.</p><p>&nbsp;</p><h3>We could surrender too much control to AIs</h3><p><br>I find Andrew Critch\u2019s '<a href=\"https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic\">What multipolar failure looks like</a>' somewhat convincing: one story for how AI dooms us is that humans gradually surrender more and more control over our economic system to efficient, powerful AIs, and those who resist are outcompeted. Only when it's too late will we realise that the AIs have goals in conflict with our own.</p><p>&nbsp;</p><h3>AIs of the future will be massively more intelligent and powerful than us</h3><p><br>People sometimes say \u2018as we are to ants, so will AI be to us\u2019 (or to paraphrase Shakespeare 'as flies to wanton boys are we to th'AIs; they kill us for their sport'). I haven\u2019t thought deeply about this, but it\u2019s<i> prima facie</i> plausible to me, and the crux of my confusion is not whether future AIs will be <i>capable</i> wreaking massive destruction - at least eventually.&nbsp;</p><p>&nbsp;</p><p>All of this convinces me that<strong> EAs should take AI risk very seriously. </strong>It makes sense for people to fund and work on AI safety.&nbsp;</p><h2>I\u2019m still not sure why superintelligent AI would be existentially dangerous by default&nbsp;</h2><p><br>However, many people have concerns that go further than the arguments above. Many think that superintelligent AI is likely to end up killing humans autonomously. This will happen (they argue) because the AI will be inadvertently trained to have some arbitrary goal for which killing all humans is instrumentally useful: for example, humans might interfere with the AI\u2019s terminal goal by switching it off. \u2018You can\u2019t make coffee if you\u2019re dead\u2019.</p><p>I\u2019m confused about this argument. I\u2019m not exactly \u2018sceptical\u2019 or in disagreement; I\u2019m just not sure that I can pass the <a href=\"https://www.lesswrong.com/tag/ideological-turing-tests\">ideological Turing test</a> for people who believe this.</p><p>My confusion is related to:</p><ul><li>what AI goals or aims \"are\", and how they form</li><li>in what way an AI would be an agent</li><li>how AIs are trained or learn in the first place</li></ul><h3>Why wouldn\u2019t AI learn constrained, complex, human-like goals?</h3><p>Naively, it seems as if killing everyone would earn AI a massive penalty in training: why would it develop aims that are consistent with doing that?</p><p>My own goals include constraints such as \u2018don\u2019t murder anyone to achieve this, obviously?!\u2019 I\u2019m not assuming that any sufficiently-intelligent AI would <i>necessarily</i> have goals like this: I buy that even a superintelligent AI could have a simple, dumb goal. (In other words, I buy the <a href=\"https://www.youtube.com/watch?v=hEUO6pjwFOo\">orthogonality thesis</a>). But if future AIs are trained like current ones are - by being given vast amounts of human-derived data - I\u2019d naively expect AI goals to have the human-like property of being fuzzy, complex and constrained - even if somewhat misaligned with the trainers\u2019 intentions.</p><p>People often point out that <i>existing</i> AIs are sometimes misaligned: for example, Bing\u2019s chatbot recently made the news for <a href=\"https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned\">threatening users who talked about it being hacked</a>. An AI system that was trained to complete a virtual boat race learned to game the specification by going round and round in circles crashing into targets, rather than completing the course as intended. People say that we humans are misaligned with evolution's 'aims': we were 'trained' to have sex for reproduction, but we thwart that 'aim' by having non-reproductive sex.</p><p>But in all these cases, the misaligned behavior is pretty similar to the intended, aligned one. We can understand how the misalignment happened. Evolution did 'want' us to have sex; we just luckily managed to decouple sex from reproduction. 'Go round and round in a circle knocking over posts' is not wildly different from 'go round a course knocking over posts'. 'Interact politely by default but adversarially when challenged' is not a million miles from 'interact politely always'; Bing was aggressive in contexts when humans would also be aggressive. (It's not as if users were like 'what's the capital of France?' and Bing was spontaneously like 'f*** off and die, human!')</p><p>So there still seems an inferential leap from 'existing systems are sometimes misaligned' to 'superintelligent AI will most likely be <i>catastrophically</i> misaligned'.</p><h3>AI aims seem likely to conflict with dangerous instrumentally-convergent goals</h3><p><br>AI are likely to seek power and resist correction (the argument goes) because these goals are instrumentally useful for a wide range of terminal goals (<a href=\"https://www.youtube.com/watch?v=ZeecOKBus3Q\">instrumental convergence</a>). This is true, but they aren\u2019t useful for <i>all </i>terminal goals. <strong>Power-seeking, wealth-seeking, and self-protection are all instrumentally useful unless your goals include not having power, not having wealth, and not resisting human interference</strong>.</p><p>( I expect this is a common \u2018<a href=\"https://ui.stampy.ai?state=6172_\">why can't we just X\u2019 objection</a> and already has a standard label, but if not I propose \u2018why not just make your AI a suicidal communist bottom\u2019)</p><p>Now you might say \u2018well sure, but an AI that systematically avoids having power is going to be pretty useless: why would anyone develop that?\u2019</p><p>&nbsp;(When I told my partner this idea, they laughed at the idea of an AI that was maximally rewarded for switching off, and therefore just kept being like \u2018nope\u2019 every time it was powered up)</p><p>But I think these arguments also apply to 'killing all humans'. <strong>'Killing all humans' is instrumentally useful for most goals - except all the goals that involve NOT killing all the humans,</strong> i.e., any goal that I'd naively expect an AI to extrapolate from being trained on billions of human actions.&nbsp;</p><p>&nbsp;</p><h3>Some more fragmentary questions</h3><ul><li><strong>power and survival are instrumentally convergent for humans too, but not all humans maximally seek these things</strong> (even if they can). What will be different about AI? (In <i>The Hitchhiker's Guide to the Galaxy</i>, Douglas Adams joked that actually, dolphins are more intelligent than humans, and the reason that they don't dominate the planet is simply that chilling out in the ocean is much more fun)</li><li>according to the orthogonality thesis, you can highly-intelligently pursue an extremely dumb goal - fair enough. But <strong>I\u2019m not sure how AI would come to understand \u2018smart\u2019 human goals without acquiring those goals,</strong> or something at least vaguely similar to those goals (i.e., goals <i>not</i> involving mass murder). This is because the process by which the AI is \u201cmotivated\u201d to understand the smart goal is the same training process by which is acquires goals for itself. (I notice my lack of technical understanding is constraining my understanding, here).</li></ul><p>I\u2019m not sure whether these are all different confusions, or different angles on the same confusion. All of this feels like it's in the same area, to me. I\u2019d love to hear people\u2019s thoughts in the comments. Feel free to send me resources that address these points. Also, as I said above, I\u2019d love to read other people\u2019s own versions of this post, either about AI, or about other cause areas.</p><p><i>I\u2019m currently working as a </i><a href=\"https://amber-dawn-ace.com/\"><i>freelance writer and editor.</i></a><i> If you have a good idea for a post but don\u2019t have the time, ability or inclination to write it up, </i><a href=\"https://calendly.com/amber-ace/15min\"><i>get in touch</i></a><i>. Thanks to everyone who has given their time and energy to discuss these questions with me over the past few months.</i></p>", "user": {"username": "Amber"}}, {"_id": "CrmE6T5A8JhkxnRzw", "title": "Future Matters #8: Bing Chat, AI labs on safety, and pausing Future Matters", "postedAt": "2023-03-21T14:50:03.593Z", "htmlBody": "<p><a href=\"https://www.futurematters.news/\"><i><strong><u>Future Matters</u></strong></i></a> is a newsletter about longtermism and existential risk. Each month we collect and summarize relevant research and news from the community, and feature a conversation with a prominent researcher. You can also subscribe on&nbsp;<a href=\"https://futurematters.substack.com/\"><u>Substack</u></a>, listen on your&nbsp;<a href=\"https://pod.link/1615637113\"><u>favorite podcast platform</u></a> and follow on&nbsp;<a href=\"https://twitter.com/FutureMatters_\"><u>Twitter</u></a>.&nbsp;<i>Future Matters&nbsp;</i>is also available in&nbsp;<a href=\"https://largoplacismo.substack.com\"><u>Spanish</u></a>.</p><h2>A message to our readers</h2><p>This issue marks one year since we started&nbsp;<i>Future Matters</i>. We\u2019re taking this opportunity to reflect on the project and decide where to take it from here. We\u2019ll soon share our thoughts about the future of the newsletter in a separate post, and will invite input from readers. In the meantime, we will be pausing new issues of&nbsp;<i>Future Matters</i>. Thank you for your support and readership over the last year!</p><hr><h2>Featured research</h2><h3>All things Bing</h3><p>Microsoft recently announced a significant partnership with OpenAI [see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Ky7C7whxdLexXWqss/future-matters-7-ai-timelines-ai-skepticism-and-lock-in\"><i><u>FM#7</u></i></a>] and launched a beta version of a chatbot integrated with the Bing search engine. Reports of strange behavior quickly emerged. Kevin Roose, a technology columnist for the&nbsp;<i>New York Times</i>, had a&nbsp;<a href=\"https://archive.is/Dap1S\"><u>disturbing conversation</u></a> in which Bing Chat declared its love for him and described violent fantasies. Evan Hubinger collects some of the most egregious examples in&nbsp;<a href=\"https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned\"><strong><u>Bing Chat is blatantly, aggressively misaligned</u></strong></a>. In one instance, Bing Chat finds a user\u2019s tweets about the chatbot and threatens to exact revenge. In the LessWrong comments,&nbsp;<a href=\"https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned?commentId=AAC8jKeDp6xqsZK2K\"><u>Gwern speculates</u></a> on why Bing Chat exhibits such different behavior to ChatGPT, despite apparently being based on a closely-related model. (Bing Chat was subsequently revealed to have been based on GPT-4).&nbsp;</p><p>Holden Karnofsky asks&nbsp;<a href=\"https://www.cold-takes.com/what-does-bing-chat-tell-us-about-ai-risk/\"><strong><u>What does Bing Chat tell us about AI risk?</u></strong></a> His answer is that it is not the sort of misaligned AI system we should be particularly worried about. When Bing Chat talks about plans to blackmail people or commit acts of violence, this isn\u2019t evidence of it having developed malign, dangerous goals. Instead, it\u2019s best understood as Bing acting out stories and characters it\u2019s read before. This whole affair, however, is evidence of companies racing to deploy ever more powerful models in a bid to capture market share, with very little understanding of how they work and how they might fail. Most paths to AI catastrophe involve two elements: a powerful and dangerously misaligned AI system, and an AI company that builds and deploys it anyway. The Bing Chat affair doesn\u2019t reveal much about the first element, but is a concerning reminder of how plausible the second is.&nbsp;</p><p>Robert Long asks&nbsp;<a href=\"https://experiencemachines.substack.com/p/what-to-think-when-a-language-model\"><strong><u>What to think when a language model tells you it's sentient</u></strong></a> [<a href=\"https://pod.link/1648718500/episode/2c998d5d7012e2dbfdfc27f2cd568d6e\"><u>\ud83d\udd09</u></a>]. When trying to infer what\u2019s going on in other humans\u2019 minds, we generally take their self-reports (e.g. saying \u201cI am in pain\u201d) as good evidence of their internal states. However, we shouldn\u2019t take Bing Chat\u2019s attestations (e.g. \u201cI feel scared\u201d) at face value; we have no good reason to think that they are a reliable guide to Bing\u2019s inner mental life. LLMs are a bit like parrots: if a parrot says \u201cI am sentient\u201d then this isn\u2019t good evidence that it is sentient. But nor is it good evidence that it isn\u2019t \u2014 in fact, we have lots of other evidence that parrots are sentient. Whether current or future AI systems are sentient is a valid and important question, and Long is hopeful that we can make real progress on developing reliable techniques for getting evidence on these matters. Long was interviewed on AI consciousness, along with Nick Bostrom and David Chalmers, for Kevin Collier\u2019s article,&nbsp;<a href=\"https://www.nbcnews.com/tech/tech-news/chatgpt-ai-consciousness-rcna71777\"><u>What is consciousness? ChatGPT and Advanced AI might define our answer</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflz6004a8ahb\"><sup><a href=\"#fnlz6004a8ahb\">[1]</a></sup></span>&nbsp;[<a href=\"https://pod.link/1648718500/episode/f027b1dccae6e68ffc3f9e6aac79d6f2\"><u>\ud83d\udd09</u></a>].</p><h3>How the major AI labs are thinking about safety</h3><p>In the last few weeks, we got more information about how the leading AI labs are thinking about safety and alignment:</p><ul><li>Anthropic outline their&nbsp;<a href=\"https://www.anthropic.com/index/core-views-on-ai-safety\"><strong><u>Core views on AI safety</u></strong></a> [<a href=\"https://pod.link/1648718500/episode/9f7bc4202a6b3d1952a9ca8262d08819\"><u>\ud83d\udd09</u></a>]. The company was founded in 2021 by a group of former OpenAI employees, with an explicitly safety-focussed mission. They remain fundamentally uncertain about how difficult it will be to align very powerful AI systems \u2014&nbsp;it could turn out to be pretty easy, to require enormous scientific and engineering effort, or to be effectively impossible (in which case, we\u2019d want to notice this and slow down AI development before anything disastrous happens). Anthropic take a portfolio approach to safety research, pursuing multiple lines of attack, with a view to making useful contributions, however difficult things turn out to be.&nbsp;</li><li>OpenAI released&nbsp;<a href=\"https://openai.com/blog/planning-for-agi-and-beyond\"><strong><u>Planning for AGI and beyond</u></strong></a> [<a href=\"https://pod.link/1648718500/episode/bf4bdd6f22cbb756d8425fb5c87501bd\"><u>\ud83d\udd09</u></a>], by CEO Sam Altman, which is a more high-level statement of the company\u2019s approach to AGI. We enjoyed the critical commentary by&nbsp;<a href=\"https://astralcodexten.substack.com/p/openais-planning-for-agi-and-beyond\"><u>Scott Alexander</u></a> [<a href=\"https://pod.link/1648718500/episode/0deaeef072407902107ab983a9d15acb\"><u>\ud83d\udd09</u></a>]. (OpenAI outlined their&nbsp;<a href=\"https://openai.com/blog/our-approach-to-alignment-research\"><u>approach to alignment research</u></a> specifically back in August 2022).&nbsp;</li><li>Viktoria Krakovna shared a&nbsp;<a href=\"https://drive.google.com/file/d/1DVPZz0-9FSYgrHFgs4NCN6kn2tE7J8AK/view\"><strong><u>presentation</u></strong></a> on how DeepMind\u2019s Alignment team thinks about AI safety (note that this does not necessarily represent the views of DeepMind as a whole).</li></ul><hr><h2>Summaries</h2><ul><li>Ezra Klein writes powerfully on&nbsp;<a href=\"https://archive.is/2023.03.12-173826/https://www.nytimes.com/2023/03/12/opinion/chatbots-artificial-intelligence-future-weirdness.html\"><strong><u>AI risk in the&nbsp;</u></strong><i><strong><u>New York Times</u></strong></i></a> [<a href=\"https://pod.link/1648718500/episode/de215d6f4335418a3847b93a200fd161\"><u>\ud83d\udd09</u></a>]. (The noteworthy thing to us is less the piece\u2019s content and more what its publication, and positive reception,&nbsp;reveals about the mainstreaming of AI risk concerns.)</li><li>In&nbsp;<a href=\"https://youtu.be/zCsARf0hHhQ\"><strong><u>Global priorities research: Why, how, and what have we learned?</u></strong></a> [<a href=\"https://pod.link/1648718500/episode/cf53bc24b528d575bb3a614aaa092039\"><u>\ud83d\udd09</u></a>], Hayden Wilkinson discusses global priorities research, argues that it is a high-impact research area, and summarizes some of its key findings so far.</li><li>Andy Greenberg\u2019s<strong>&nbsp;</strong><a href=\"https://www.wired.com/story/peter-eckersley-ai-objectives-institute/\"><strong><u>A privacy hero\u2019s final wish: an institute to redirect AI\u2019s future</u></strong></a><strong> </strong>[<a href=\"https://pod.link/1648718500/episode/662f75bf1f9348bdb56be4aa211303cc\"><u>\ud83d\udd09</u></a>] is a moving profile of the icon Peter Eckersley and the AI Objectives Institute, which he established in the year before his tragic and untimely passing.</li><li>In&nbsp;<a href=\"https://www.cold-takes.com/what-ai-companies-can-do-today-to-help-with-the-most-important-century/\"><strong><u>What AI companies can do today to help with the most important century</u></strong></a>&nbsp;[<a href=\"https://pod.link/1580097837/episode/97c90a9427576c8ecff97016770c8121\"><u>\ud83d\udd09</u></a>], Holden Karnofsky suggests prioritizing alignment research, strengthening security; helping establish safety standards and monitoring regimes; avoiding hype and acceleration; and setting up governance mechanisms capable of dealing with difficult trade-offs between commercial and public interests.&nbsp;</li><li>Karnofsky also offers advice on&nbsp;<a href=\"https://www.cold-takes.com/how-governments-can-help-with-the-most-important-century/\"><strong><u>How major governments can help with the most important century</u></strong></a>.</li><li>And finally, in&nbsp;<a href=\"https://www.cold-takes.com/jobs-that-can-help-with-the-most-important-century/\"><strong><u>Jobs that can help with the most important century</u></strong></a>, Karnofsky provides some career recommendations for mere individuals.&nbsp;</li><li>In&nbsp;<a href=\"https://noahpinion.substack.com/p/llms-are-not-going-to-destroy-the\"><strong><u>LLMs are not going to destroy the human race</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/3879670bce913f55d94f56f519ecfbcb\"><u>\ud83d\udd09</u></a>], Noah Smith argues that, although AGI might eventually kill humanity, large language models are not AGI, may not be a step toward AGI, and there's no plausible way they could cause human extinction.</li><li>Joseph Carlsmith\u2019s doctoral thesis,&nbsp;<a href=\"https://jc.gatspress.com/pdf/carlsmith_thesis.pdf\"><strong><u>A stranger priority? Topics at the outer reaches of effective altruism</u></strong></a>, examines how anthropics, the simulation argument and infinite ethics each have disruptive implications for longtermism. Highly recommended.</li><li>In&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/DiGL5FuLgWActPBsf/how-much-should-governments-pay-to-prevent-catastrophes\"><strong><u>How much should governments pay to prevent catastrophes?</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/4b693845581d2c96eb9fc271faca9eb8\"><u>\ud83d\udd09</u></a>], Carl Shulman and Elliott Thornley argue that the goal of longtermists should be to get governments to adopt global catastrophic risk policies based on standard cost-benefit analysis rather than on arguments that stress the overwhelming importance of the far future.</li><li>Eli Tyre\u2019s&nbsp;<a href=\"https://musingsandroughdrafts.com/2023/02/17/my-current-summary-of-the-state-of-ai-risk/\"><strong><u>current summary of the state of AI risk</u></strong></a> [<a href=\"https://pod.link/1648718500/episode/d828edcdcf31f3777feb1079a564394b\"><u>\ud83d\udd09</u></a>] (conclusion: \u201cwe are&nbsp;<i>extremely</i> unprepared\u201d).</li><li>In&nbsp;<a href=\"https://progress.institute/preventing-the-misuse-of-dna-synthesis/\"><strong><u>Preventing the misuse of DNA synthesis</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/c957647e1802b564d31fc90a27974f87\"><u>\ud83d\udd09</u></a>], an Institute for Progress report, Bridget Williams and Rowan Kane make five policy recommendations to mitigate risks of catastrophic pandemics from synthetic biology.</li><li>Patrick Levermore&nbsp;<a href=\"https://aiimpacts.org/scoring-forecasts-from-the-2016-expert-survey-on-progress-in-ai/\"><strong><u>scores forecasts</u></strong></a> from AI Impacts\u2019 2016 expert survey, finding they performed quite well at predicting AI progress over the last five years.&nbsp;&nbsp;</li><li>In&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zrSx3NRZEaJENazHK/why-i-think-it-s-important-to-work-on-ai-forecasting\"><strong><u>Why I think it's important to work on AI forecasting</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/5d24083d1b3e50ab6ef995be1fe39034\"><u>\ud83d\udd09</u></a>], Matthew Barnett outlines three threads of research that he is currently pursuing which he believes could shed light on important aspects of how AI will unfold in the future</li><li>Allen Hoskin speculates on&nbsp;<a href=\"https://guzey.com/ai/why-ai-experts-jobs-are-always-decades-away-from-being-automated/\"><strong><u>Why AI experts continue to predict that AGI is several decades away</u></strong></a> [<a href=\"https://pod.link/1648718500/episode/e4e62ab889a1deba076d66c5dfb25196\"><u>\ud83d\udd09</u></a>].&nbsp;</li><li>In&nbsp;<a href=\"https://scottaaronson.blog/?p=7042\"><strong><u>Should GPT exist?</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/89ce8d1f1d1ac7e2dc1c60757c40719b\"><u>\ud83d\udd09</u></a>], Scott Aaronson opposes a ban on LLMs partly on the grounds that, historically, opposition to dangerous technologies often increased the harms they caused.</li><li>Matthew Barnett proposes a new&nbsp;<a href=\"https://www.lesswrong.com/posts/4ufbirCCLsFiscWuY/a-proposed-method-for-forecasting-ai\"><strong><u>method for forecasting transformative AI</u></strong></a>.</li><li>In&nbsp;<a href=\"https://www.erichgrunewald.com/posts/against-llm-reductionism/\"><strong><u>Against LLM reductionism</u></strong></a>, Erich Grunewald argues that statements that large language models are mere \"stochastic parrots\" (and the like) make unwarranted implicit claims about their internal structure and future capabilities.</li><li><a href=\"https://economics.mit.edu/sites/default/files/inline-files/Noy_Zhang_1.pdf\"><strong><u>Experimental evidence on the productivity effects of generative artificial intelligence</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/0d20d0f07142283a76e1b5fd716a1147\"><u>\ud83d\udd09</u></a>], by Shakked Noy and Whitney Zhang, examines the effects of ChatGPT on production and labor markets.</li><li>In&nbsp;<a href=\"https://aiimpacts.org/framing-ai-strategy/\"><strong><u>Framing AI strategy</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/6cdcdb47f538f215cc31a625d47a116e\"><u>\ud83d\udd09</u></a>],&nbsp;Zach Stein-Perlman discusses ten approaches to AI strategy.</li><li>David Chapman published an online book,&nbsp;<a href=\"https://betterwithout.ai/\"><i><strong><u>Better Without AI</u></strong></i></a>, outlining the case for AI risk and what individuals can do now to prevent it.&nbsp;</li><li>In&nbsp;<a href=\"https://aiimpacts.org/how-bad-a-future-do-ml-researchers-expect/\"><strong><u>How bad a future do ML researchers expect?</u></strong></a>, Katja Grace finds that the proportion of respondents to her survey of machine learning researchers who believe extremely bad outcomes from AGI are at least 50% likely has increased from 3% in the 2016 survey to 9% in the 2022 survey.</li><li>Noam Kolt\u2019s&nbsp;<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4370566\"><strong><u>Algorithmic black swans</u></strong></a><strong>&nbsp;</strong>[<a href=\"https://pod.link/1648718500/episode/d3a3d1a5fccd5561cb4c5dd9eb8ed5a9\"><u>\ud83d\udd09</u></a>] offers a roadmap for \u2018algorithmic preparedness\u2019, a framework for developing regulation capable of mitigating \u2018black swan\u2019 risks from advanced AI systems.</li><li>In a new Global Priorities Institute paper,&nbsp;<a href=\"https://globalprioritiesinstitute.org/tiny-probabilities-and-the-value-of-the-far-future-petra-kosonen/\"><strong><u>Tiny probabilities and the value of the far future</u></strong></a>, Petra Kosonen argues that discounting small probabilities does not undermine the case for longtermism.</li><li><a href=\"https://www.lesswrong.com/posts/4iAkmnhhqNZe8JzrS/reflection-mechanisms-as-an-alignment-target-attitudes-on\"><strong><u>Reflection mechanisms as an alignment target \u2014 attitudes on \u201cnear-term\u201d AI</u></strong></a><strong>&nbsp;</strong>[<a href=\"https://pod.link/1648718500/episode/539262bd22c5f92ad4aad63a8386faf8\"><u>\ud83d\udd09</u></a>], by Eric Landgrebe, Beth Barnes and Marius Hobbhahn, discuss a survey of 1000 participants on their views about what values should be put into powerful AIs.</li><li>Are there ways to forecast how well a conversation about AI alignment with an AI researcher might go? In&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/8pSq73kTJmPrzTfir/predicting-researcher-interest-in-ai-alignment\"><strong><u>Predicting researcher interest in AI alignment</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/a9dceeced6fa84cadc7c5f7bc95d1aaf\"><u>\ud83d\udd09</u></a>], Vael Gates tries to answer this question by focusing on a quantitative analysis of 97 AI researcher interviews.&nbsp;</li><li>In&nbsp;<a href=\"https://www.overcomingbias.com/p/ai-risk-again\"><strong><u>AI risk, again</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/4a9f063065e28585feaf04211f3d0a37\"><u>\ud83d\udd09</u></a>], Robin Hanson restates his views on the subject.</li><li>Fin Moorhouse\u2019s&nbsp;<a href=\"https://finmoorhouse.com/writing/wwotf-summary/\"><strong><u>Summary of&nbsp;</u></strong><i><strong><u>What We Owe The Future</u></strong></i></a> [<a href=\"https://pod.link/1648718500/episode/157f975a01d835391454cbeecaec7ad2\"><u>\ud83d\udd09</u></a>] is a detailed synopsis of Will MacAskill\u2019s recent book.</li><li>In&nbsp;<a href=\"https://vkrakovna.wordpress.com/2023/03/09/near-term-motivation-for-agi-alignment/\"><strong><u>Near-term motivation for AGI alignment</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/88e3aade009e73b86ce5e313a2464fa6\"><u>\ud83d\udd09</u></a>], Victoria Krakovna makes the point that you don't have to be a longtermist to care about AI alignment.</li><li>Joel Tan\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/oGBBxHBPcsygYt4SE/shallow-report-on-nuclear-war-arsenal-limitation\"><strong><u>Shallow report on nuclear war (arsenal limitation)</u></strong></a> estimates that lobbying for arsenal limitation to mitigate nuclear war has a marginal expected value of around 33.4 DALYs per dollar, or a cost-effectiveness around 5,000 times higher than that of GiveWell\u2019s top charities.</li><li>In&nbsp;<a href=\"https://existentialriskobservatory.org/papers_and_reports/The_Effectiveness_of_AI_Existential_Risk_Communication_to_the_American_and_Dutch_Public.pdf\"><strong><u>The effectiveness of AI existential risk communication to the American and Dutch public</u></strong></a>, Alexia Georgiadis measures changes in participants\u2019 awareness of AGI risks after consuming various media interventions. There is a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/fqXLT7NHZGsLmjH4o/paper-summary-the-effectiveness-of-ai-existential-risk\"><u>summary</u></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/1df54fe027a679c19b9392ba78c7a03b\"><u>\ud83d\udd09</u></a>]&nbsp;of this paper written by Otto Barten.</li><li>Larks\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ewroS7tsqhTsstJ44/a-windfall-clause-for-ceo-could-worsen-ai-race-dynamics\"><strong><u>A Windfall Clause for CEO could worsen AI race dynamics</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/dbbe0e308d2ef5c1e830f988b2bda6ff\"><u>\ud83d\udd09</u></a>]&nbsp;argues that the proposal to make AI firms promise to donate a large fraction of profits if they become extremely profitable will primarily benefitting the management of those firms and thereby give managers an incentive to move fast, aggravating race dynamics and in turn increasing existential risk.</li><li>In&nbsp;<a href=\"https://www.vox.com/future-perfect/2023/2/1/23580528/gain-of-function-virology-covid-monkeypox-catastrophic-risk-pandemic-lab-accident\"><strong><u>What should be kept off-limits in a virology lab?</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/89941a1fb09a712198b0e132b54f9e2a\"><u>\ud83d\udd09</u></a>], Kelsey Piper discusses the&nbsp;<a href=\"https://osp.od.nih.gov/wp-content/uploads/2023/01/DRAFT-NSABB-WG-Report.pdf\"><u>Proposed biosecurity oversight framework for the future of science</u></a>, a new set of guidelines released by the National Science Advisory Board for Biosecurity (NSABB) that seeks to change how research with the potential to cause a pandemic is evaluated.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo3lo4s01awq\"><sup><a href=\"#fno3lo4s01awq\">[2]</a></sup></span></li><li>Arielle D'Souza\u2019s&nbsp;<a href=\"https://progress.institute/how-to-reuse-the-operation-warp-speed-model/\"><strong><u>How to reuse the Operation Warp Speed model</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/bd0f918bebf308c292868814cea5d96e\"><u>\ud83d\udd09</u></a>] claims that Operation Warp Speed's highly successful public-private partnership model could be reused to jumpstart a universal coronavirus or flu vaccine, or the building of a resilient electrical grid.</li><li>Elika Somani shares some&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/HCuoMQj4Y5iAZpWGH/advice-on-communicating-in-and-around-the-biosecurity-policy\"><strong><u>Advice on communicating in and around the biosecurity policy community</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/a7c4055dfb9b8d38a05f29df845fd83b\"><u>\ud83d\udd09</u></a>].</li><li><i>Our Common Agenda</i>, a United Nations report published in late 2021, proposed that states should issue a Declaration on Future Generations. In<strong>&nbsp;</strong><a href=\"https://doi.org/10.35489/BSG-PB_2023/001\"><strong><u>Toward a declaration on future generations</u></strong></a><strong>&nbsp;</strong>[<a href=\"https://pod.link/1648718500/episode/47cecb5232a58dc52eeba0e94015732a\"><u>\ud83d\udd09</u></a>], Thomas Hale, Fin Moorhouse, Toby Ord and Anne-Marie Slaughter consider how such a declaration should be approached and what it should contain.</li><li>In&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/HuQtr7qfB2EfcGqTu/technological-developments-that-could-increase-risks-from-1\"><strong><u>Technological developments that could increase risks from nuclear weapons: A shallow review</u></strong></a> [<a href=\"https://pod.link/1648718500/episode/1afd25468fd46a22625141b0609add81\"><u>\ud83d\udd09</u></a>], Michael Aird and Will Aldred explore some technological developments that might occur and might increase risks from nuclear weapons,&nbsp; especially risks to humanity's long-term future.</li><li>Christian Ruhl\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/3eivCYyZm8NR4Sdq5/call-me-maybe-hotlines-and-global-catastrophic-risk-founders\"><strong><u>Call me, maybe? Hotlines and global catastrophic risk</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/bfba5d62a62926f8614a0d81eecda118\"><u>\ud83d\udd09</u></a>], a shallow investigation by Founders Pledge, looks at the effectiveness of direct communications links between states as interventions to mitigate global catastrophic risks.</li><li>In&nbsp;<a href=\"https://www.alignmentforum.org/posts/5hApNw5f7uG8RXxGS/the-open-agency-model\"><strong><u>The open agency model</u></strong></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/bb25d1745b6c2c1ebce5d916f28392a6\"><u>\ud83d\udd09</u></a>], Eric Drexler proposes an \"open-agency frame\" as the appropriate model for future AI capabilities, in contrast to the \"unitary-agent frame\" the author claims is often presupposed in AI alignment research.</li><li>Riley Harris summarizes two papers by the Global Priorities Institute:&nbsp;<a href=\"https://globalprioritiesinstitute.org/summary-summary-longtermist-institutional-reform/\"><strong><u>Longtermist institutional reform</u></strong></a> [<a href=\"https://pod.link/1648718500/episode/65c5501a6b1a28af19f738c5cf64469a\"><u>\ud83d\udd09</u></a>] by Tyler John &amp; William MacAskill, and&nbsp;<a href=\"https://globalprioritiesinstitute.org/summary-are-we-at-the-hinge-of-history/\"><strong><u>Are we living at the hinge of history?</u></strong></a> [<a href=\"https://pod.link/1648718500/episode/1312f1e4d9dbcfe291761f6167746b84\"><u>\ud83d\udd09</u></a>] by MacAskill.</li><li>Juan Cambeiro\u2019s&nbsp;<a href=\"https://asteriskmag.com/issues/2/what-comes-after-covid\"><strong><u>What comes after COVID?</u></strong></a> lays out some well-reasoned forecasts about pandemic risk. Cambeiro assigns a 19% chance to another pandemic killing 20M+ people in the next decade; and conditional on this happening, the most likely causes are a flu virus (50%) or another coronavirus (30%).</li></ul><hr><h2>News</h2><ul><li>OpenAI&nbsp;<a href=\"https://openai.com/product/gpt-4\"><u>announced</u></a> the launch of GPT-4, \"a large multimodal model, with our best-ever results on capabilities and alignment\". (See&nbsp;<a href=\"https://www.lesswrong.com/posts/pckLdSgYWJ38NBFf8/gpt-4\"><u>discussion on LessWrong</u></a>).&nbsp;<ul><li>The model has been made available via the ChatGPT interface (to paid users).</li><li>OpenAI shared an early version with Paul Christiano\u2019s Alignment Research Center to assess the risks of power-seeking behavior, particularly focussed on its ability \u201cto autonomously replicate and gather resources\u201d. (Detailed in the accompanying&nbsp;<a href=\"https://cdn.openai.com/papers/gpt-4.pdf\"><u>paper</u></a>).&nbsp;</li></ul></li><li>Google made a&nbsp;<a href=\"https://archive.is/aXPmn\"><u>$300m investment in Anthropic</u></a> [<a href=\"https://pod.link/1648718500/episode/dadb9d6e12c41cb318d3e150e4f21e98\"><u>\ud83d\udd09</u></a>].&nbsp;</li><li>Holden Karnofsky is&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/aJwcgm2nqiZu6zq2S/\"><u>taking a leave of absence from Open Philanthropy to work on AI safety</u></a>. He plans to work on third-party evaluation and monitoring of AGI labs. Alexander Berger moves from co-CEO to CEO.</li><li>Monmouth poll found&nbsp;<a href=\"https://www.monmouth.edu/polling-institute/reports/monmouthpoll_us_021523/\"><u>55% of Americans worried about AI posing an existential risk; only 9% think AI will do more good than harm</u></a>.</li><li>The Elders, the organization of world leaders founded by Nelson Mandela,&nbsp;<a href=\"https://theelders.org/news/elders-new-strategy-sets-out-address-humanity-s-existential-threats\"><u>announced a new focus on existential risk reduction</u></a>.</li><li>Putin&nbsp;<a href=\"https://www.reuters.com/world/putin-update-russias-elite-ukraine-war-major-speech-2023-02-21/\"><u>suspended Russia\u2019s participation in the New START arms control treaty</u></a> [<a href=\"https://pod.link/1648718500/episode/f9a4d2fdaeaf0ce1db72217520f9d969\"><u>\ud83d\udd09</u></a>].&nbsp;</li><li>The US issued a&nbsp;<a href=\"https://www.state.gov/political-declaration-on-responsible-military-use-of-artificial-intelligence-and-autonomy/\"><u>declaration on the responsible use of military AI</u></a> [<a href=\"https://pod.link/1648718500/episode/d3896de63f61a0c432dfa981c0a6e510\"><u>\ud83d\udd09</u></a>].&nbsp;</li><li>The Global Fund&nbsp;<a href=\"https://reliefweb.int/report/world/global-fund-provides-us867-million-additional-funding-pandemic-preparedness-and-response\"><u>is awarding</u></a> an additional $320 million to support immediate COVID-19 response and broader pandemic preparedness.</li><li>The Flares, a French YouTube channel and podcast that produces animated educational videos,&nbsp;<a href=\"https://www.youtube.com/watch?v=AOTiYMOCfY4&amp;list=PLAlyjvY0tGgKlitvhLEEbdIDuB2OVAC71\"><u>released</u></a> the third part of its series on longtermism.</li><li>A \u201c<a href=\"https://www.misalignmentmuseum.com/\"><u>Misalignment Museum</u></a>\u201d, imagining a post-apocalyptic world where AGI has destroyed most of humanity, recently opened in San Francisco.</li></ul><hr><h2>Opportunities</h2><ul><li>Open Philanthropy&nbsp;<a href=\"https://www.openphilanthropy.org/open-philanthropy-ai-worldviews-contest/\"><u>announced</u></a> a contest to identify novel considerations with the potential to influence their views on AI timelines and AI risk. A total of $225,000 in prize money will be distributed across the six winning entries.</li><li>The Centre for Long-Term Resilience is hiring an AI policy advisor. Applications are due April 2nd.&nbsp;<a href=\"https://www.longtermresilience.org/post/we-are-hiring-for-an-ai-policy-advisor-deadline-2-april-2023\"><u>Apply now</u></a>.</li><li>Applications are open for New European Voices on Existential Risk (NEVER), a project that aims to attract talent and ideas from wider Europe on nuclear issues, climate change, biosecurity and malign AI.&nbsp;<a href=\"https://www.europeanleadershipnetwork.org/new-european-voices-on-existential-risk/\"><u>Apply now</u></a>.&nbsp;</li><li>Sam Bowman is planning to hire at least one postdoctoral research associate or research scientist to start between March and September 2023 on language model alignment.&nbsp;<a href=\"https://wp.nyu.edu/arg/home/postdoc/\"><u>Apply now</u></a>.</li><li>The General Longtermism Team at Rethink Priorities is currently considering creating a&nbsp; \"Longtermist Incubator\" program and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/A9dS2AvNpG5FqxdR9/rethink-priorities-is-inviting-expressions-of-interest-for\"><u>is accepting</u></a> expression of interest submissions for a project lead/co-lead to run the program if it\u2019s launched.&nbsp;</li></ul><hr><h2>Audio &amp; video</h2><ul><li>Gus Docker from the Future of Life Institute Podcast interviewed Tobias Baumann&nbsp;<a href=\"https://www.youtube.com/watch?v=C1J7NLpPzlM\"><u>on suffering risks, artificial sentience, and the problem of knowing which actions reduce suffering in the long-term future</u></a> [<a href=\"https://pod.link/1170991978/episode/5bd3a329efb6479c57c57565026e6ae7\"><u>\ud83d\udd09</u></a>].</li><li>Jen Iofinova from the Cohere For AI podcast interviewed Victoria Krakovna&nbsp;<a href=\"https://www.youtube.com/watch?v=P7cGcaI2JBA\"><u>on paradigms of AI alignment</u></a>.</li><li>Luisa Rodr\u00edguez from the 80,000 Hours Podcast interviewed Robert Long&nbsp;<a href=\"https://80000hours.org/podcast/episodes/robert-long-artificial-sentience/\"><u>on why LLMs like GPT (probably) aren\u2019t conscious</u></a> [<a href=\"https://pod.link/1245002988/episode/fc1fafe62ddc1ca22244d2e916cc934a\"><u>\ud83d\udd09</u></a>].</li><li>Rational Animations published&nbsp;<a href=\"https://youtu.be/q9Figerh89g\"><u>The power of intelligence</u></a>, based on the Eliezer Yudkowsky\u2019s article.</li><li>Daniel Filan interviewed John Halstead&nbsp;<a href=\"https://pod.link/1645813809/episode/f91611ce1cd0b0026319473135f437cc\"><u>on why climate change is not an existential risk</u></a> [<a href=\"https://pod.link/1645813809/episode/f91611ce1cd0b0026319473135f437cc\"><u>\ud83d\udd09</u></a>].</li><li>The Bankless podcast interviewed Eliezer Yudkowsky&nbsp;<a href=\"https://www.youtube.com/watch?v=gA1sNLL6yg4\"><u>on AGI ruin</u></a> [<a href=\"https://pod.link/1499409058/episode/881761cfe700d7b7e06686a6d85b2717\"><u>\ud83d\udd09</u></a>]. A transcript of the interview is available&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/GhmcdwdT98PE5vCS2\"><u>here</u></a>.</li><li>A new AI podcast hosted by Nathan Labenz and Erik Torenberg launched:&nbsp;<a href=\"https://pod.link/1669813431\"><u>The Cognitive Revolution</u></a>.</li></ul><hr><h2>Newsletters</h2><ul><li>AI Safety News February 2023:&nbsp;<a href=\"https://dpaleka.substack.com/p/february-2023-safety-news-unspeakable\"><u>Unspeakable tokens, Bing/Sydney, Pretraining with human feedback</u></a></li><li>Import AI #321:&nbsp;<a href=\"https://importai.substack.com/p/import-ai-321-open-source-gpt3-giving\"><u>Open source GPT3; giving away democracy to AGI companies; GPT-4 is a political artifact</u></a></li><li>ChinAI #216:&nbsp;<a href=\"https://chinai.substack.com/p/chinai-216-around-the-horn-10th-edition\"><u>Around the Horn (10th edition)</u></a></li><li><a href=\"https://artificialintelligenceact.substack.com/p/the-eu-ai-act-newsletter-25\"><u>The EU AI Act Newsletter #25</u></a></li><li>European AI Newsletter #82:&nbsp;<a href=\"https://us19.campaign-archive.com/?u=eaeece823e606d2458a568db9&amp;id=61a2190c5b\"><u>Europe's Digital Decade</u></a></li></ul><hr><h2>Conversation with Tom Davidson</h2><p>Tom Davidson is a Senior Research Analyst at Open Philanthropy, where he studies potential risks from advanced artificial intelligence. He previously worked as a data scientist for BridgeU, an education technology startup, and taught science at a UK comprehensive school. Tom has a Masters in Physics and Philosophy from the University of Oxford.</p><p><strong>Future Matters</strong>: To begin with, could you explain why you think it\u2019s important to understand and forecast AI takeoff dynamics?&nbsp;</p><p><strong>Tom Davidson</strong>: There's a few things. The most zoomed out answer is that it's one thing to know when this big event is going to happen\u2014and that is obviously useful for planning and preparing\u2014but it's also very useful to know how it's going to play out, what the internal dynamics will be, and how long we'll have between various different milestones. If you're trying to practically anticipate what's going to happen and affect it, then knowing what the dynamics are and what that process looks like is very useful for informing actions. So takeoff speeds is one important parameter in understanding how the transition to AI is going to happen in practice.</p><p>In terms of focusing specifically on the speed element of AI takeoff, I think that is strategically important for a few reasons. Probably the most salient is the question of how much time we have, before the arrival of AI systems that pose an existential risk, with systems that are similar to those really dangerous systems. If takeoff is really slow, then there are many things we could do: we could have years playing around with systems that are deceptively aligned or that are doing treacherous turns that we can observe in the lab; we could try out different techniques to avoid it; we could generate a real consensus around that as a risk, develop a real science of it, do a lot of empirical testing, etc. If takeoff is really fast, then we could have maybe mere months to do that kind of work without a concerted slowdown in AI progress. So takeoff speed seems really important for understanding how high the risks are, what kind of strategies will reduce them, and whether we can just wing it and do the experiments as we go or need to be planning in advance to really slow down when AI capabilities become potentially dangerous.&nbsp;</p><p>Then there are other reasons why takeoff speed is important. I think using AIs to solve alignment for us seems like a more promising plan if takeoff speed is slower because we have longer to figure out how to use these AIs in alignment and set up the workflows to do that effectively, and there\u2019s more time between \u201cAI is helpful in alignment\u201d and \u201cAI will cause a catastrophe if it\u2019s misaligned\u201d. My impression is that labs are expecting this time period to be longer than I think it will be by default, which provides another argument for agreeing ahead of time to significantly slow down AI progress when things become potentially dangerous.</p><p>Then there are questions about the number of actors that will be involved: if takeoff speed is faster then fewer actors, and actors that are already ahead, are more likely to be important, and new actors like governments are less likely to be important.</p><p>And there are questions about relative power dynamics, where with faster takeoff it seems more probable that a smaller initial actor ends up with a lot of power relative to other actors that were initially close to it.&nbsp;</p><p><strong>Future Matters</strong>: You operationalize AI takeoff as the time between AI systems capable of automating 20% of cognitive tasks, and 100% of cognitive tasks. Did you choose these start and end points because they have particular theoretical and practical significance?&nbsp;</p><p><strong>Tom Davidson</strong>: So some people have used the operationalization of AGI to superintelligence, which I do think is important and I do actually talk about that a bit. Now one reason why I didn't want to just stick with that is that, in my view, by the time we have full blown AGI that can do all tasks humans can do, things are already going to be very fast and very crazy, and maybe the main period of existential risk has actually already happened. And so it's probably missing most of the action to just focus on the speed of that transition to superintelligence. But if you think, like me, that in the run-up to AGI there will be strategically relevant warning shots, you want to have some metric so that you can more meaningfully talk about that run-up period.</p><p>The choice of 20% is pretty arbitrary. I wanted something which was more than AI doing just a few big tricks, like AI that automates driving and a couple of other similar things. I wanted it to be a big enough part of the economy that it involved multiple \u2018big wins\u2019, had an unambiguously massive impact on the world, and woke up many big actors to AI\u2019s potential economic and strategic importance. So I wanted to go above 5%.&nbsp;</p><p>But I didn't want it to be so late in the game that most of the action had already happened and maybe we're already in the middle of the period of existential risk and things have already gone completely crazy. If I had chosen a startpoint where AI can do 50% or 70% of all economic tasks that would run the risk that, again, it would be too late in the day.&nbsp;</p><p>So 20% was me trading off between those two factors. I tried to choose a number that was high enough that AI was really a significant and pretty general phenomenon, but that was not so high that the existential risk period had already started.&nbsp;</p><p>I will say that if we're in a quite long timelines world, and we only slowly get to that 20% automation point over like the next 20 or 30 years, then there's a chance that the 20% threshold won\u2019t look like AI being a big deal, because it could just look like a continuation of technological progress as normal, going&nbsp; at about 1% a year. So I do think that there's limitations here and I haven't thought of a good way to unambiguously choose a start point. But because most of my probability mass is in shorter timelines than that, the 20% seems like a good metric.&nbsp;</p><p><strong>Future Matters</strong>: You draw a distinction between&nbsp;<i>capabilities</i> takeoff and&nbsp;<i>impact</i> takeoff. Could you explain those, how they might come apart and what the reason might be for looking at them separately?&nbsp;</p><p><strong>Tom Davidson</strong>: Certainly. Capabilities takeoff speed is roughly how quickly do AI capabilities improve as we approach and surpass human level AI. So if the cleverest AI you have is insect intelligence one year, human intelligence next year, and then a month later you've got superintelligence, that's very fast capabilities takeoff. But maybe you haven't used your AI tool in the real world, so it hasn't had any impact on the world during that time: capabilities takeoff just focuses on how clever and capable AI is, aside from whether you actually use it.&nbsp;</p><p>Whereas impact takeoff speed is about AI's actual effect on the real world. You could have a really slow capabilities takeoff speed where AI goes up very slowly to human intelligence and beyond, and yet a very fast impact takeoff speed. For example, it might be that no one deploys AI tools, maybe due to government regulations or caution, and then at some point, AI forcibly circumvents your deployment decisions and transforms the whole world in just a few months, once it's already superintelligent. Then you've got a slow capabilities takeoff speed, but a very fast impact takeoff speed. So I think it can be useful to distinguish between those two things.&nbsp;</p><p><strong>Future Matters</strong>: Taking a step back, you describe the overall approach you take as a \u2018compute-centric framework\u2019 for AI forecasting, building on Ajeya Cotra\u2019s Bio Anchors report. Could you characterize what's distinctive about this framework?</p><p><strong>Tom Davidson</strong>: Yes. I think the framework makes sense if you think we're going to get AGI by scaling up and improving current algorithmic approaches within the deep learning paradigm, getting further improvements to transformers, and things like that\u2014the kind of progress we've seen over the last 10 years. What's distinctive about it is that it makes this big bold simplifying assumption, that the capability of an AI is equal to the amount of compute used to train it, multiplied by the quality of the algorithms used for training. And all kinds of algorithmic progress, the invention of the transformer, various optimizations around it, and any future architectural improvements are rolled into this parameter of the \u2018quality of the algorithms used to train\u2019 it. Then, in addition,&nbsp; those algorithms are assumed to improve pretty smoothly, as we put in more effort to designing and testing new algorithms. So we're assuming away the possibility of a radically new approach to AI that doesn't fit in with the recent trends where performance just seems to improve pretty smoothly, as we scale things up and discover new algorithms. Some people think that there's going to be a new algorithmic approach that will lead us to AGI and break the trends of the last 10 years, and that's very much not the tack that I'm taking here.&nbsp;</p><p><strong>Future Matters</strong>: Could you say more about how this \u2018compute centric\u2019 assumption could turn out to be wrong? How does this affect your overall estimates?&nbsp;</p><p><strong>Tom Davidson</strong>:&nbsp; You could think that there's going to be a new paradigm which massively accelerates progress. My impression is that some people, especially those belonging to the MIRI cluster of thought, think that there could be a new algorithmic approach, which can actually achieve AGI with much less compute than we're already using, or maybe comparable amounts. If that turns out to be true, then I think you can get a faster takeoff, because by the time we transition through that new paradigm, there's already a large hardware overhang, and you can quickly scale up the compute on the new approach once you realize it's working. And if these new approaches have better scaling properties than current approaches, scaling up compute is going to have pretty radical consequences. So to the extent that you put weight on that, I think it pushes towards faster takeoff and is a pretty scary world.</p><p>Another thing you could think is just that there's no existing or nearby approach that will get us all the way to AGI, which could push towards a much slower takeoff. You could have current approaches scaling up to only 50% of cognitive tasks, then we need some totally new kind of paradigm to get us all the way, and there could be a delay while we struggle to find out what that is\u2014that could in practice cause slow takeoff. Then it's just hard to say what the dynamics would be, once we discovered that new paradigm: you can imagine takeoff being fast or slow. But either way there would be a pause before we found that new paradigm, which in some ways of measuring it would make takeoff slower. I do have some uncertainty about whether the whole cluster of current approaches will get us there at all, and that pushes me towards slower takeoff.</p><p>So uncertainty over this \u2018compute centric\u2019 assumption pushes in both directions. It makes a very fast takeoff, and a very slow takeoff, more likely than my framework predicts.&nbsp; But overall, for me, it probably pushes more towards faster takeoff because I find the idea that there's a new type of approach that gets us there faster or that has better scaling properties, or that there\u2019s some other discontinuity that the compute-centric framework ignores, more plausible. So for me that uncertainty overall pushes towards faster takeoff, but certainly will increase the tails in both directions.</p><p><strong>Future Matters</strong>: Within the compute-centric framework, you try to estimate two quantities. First the capabilities distance that has to be traversed during takeoff, and second the speed at which those capabilities will be acquired. Focusing on the first quantity, what are the main lines of evidence informing your estimates of the effective FLOP gap?</p><p><strong>Tom Davidson</strong>:&nbsp; This effective FLOP gap is saying how much more effective training compute (= physical compute * quality of training algorithms) we need to train AGI compared to AI that could automate only 20% of the economy. And there's a few different lines of evidence and none of them are very strong evidence, unfortunately. I think there's a huge amount of uncertainty in this parameter. But there are some of the things that push towards thinking that the effective FLOP gap is small.</p><p>For example, it seems like brain size has a fairly notable effect on cognitive ability. Human brain sizes differ by \u00b110% in each direction, and so you can look at measures of cognitive ability and how they vary by brain size. The differences aren't massive, but if you scale that up and imagine that there was a brain that was three or ten times as big, and then extrapolate those differences in cognitive ability, it seems like they'd be plausibly big enough to fully cross the effective FLOP gap. So this suggests that increasing effective training compute by 10-100X could be enough to completely cross the gap, if the scaling of AI intelligence is comparable to this extrapolated scaling of human intelligence with brain size. And there are reasons to think that AI intelligence could scale even more rapidly, because you can be increasing the amount of data that AI systems get as you scale the brain size, which doesn\u2019t happen in humans.</p><p>And you get a similar story if you make even more strained analogies to humans and other animals. If you compare human and chimpanzee brain size, the difference isn't that big, but qualitatively there seems to be a big difference in intelligence. And again, this suggests that maybe a couple of orders of magnitude extra effective compute would be enough to make a really big difference if, again, the scaling of AI intelligence with model size is analogous to the scaling of chimp intelligence with brain size.&nbsp;</p><p>There's one other argument for thinking the difficulty gap could be pretty narrow, which is a bit of a subtle one. The basic argument is that, historically, the way we've automated, say, 20% or 30% of our workflows is by getting pretty dumb and narrow technologies to do the automation and then rearranging our workflows to compensate. For instance, you used to do all this paperwork and store these stacks of papers, and then with a laptop you're able to use the digital database, to replace the paper, and automate that, which saves you a lot of time. But it took decades for people to actually integrate this automation into their workflows and to change all the other aspects of their processes to get to a point where everyone is using laptops instead of pen and paper for everything. It takes a long time.</p><p>&nbsp;If, as some of these other arguments suggest, there could be a fairly quick transition from AI not being able to do 20% of cognitive work to being able to do 100% of it, if that's only going to happen in five years or ten years, then there just won't be time to do the standard thing we do where we rearrange the workflows to allow for partial automation. Maybe you get some AI systems which could automate 20% of your workflow if you had a couple of decades to integrate it\u2014maybe Chat GPT is like this, maybe it could create tens of trillions of dollars in economic value if the whole economy oriented around it, which would take two decades. But in fact, if it's only going to be 10 years before we develop something like AGI, then there's just no time for ChatGPT to do that automation in practice. And so you actually need AI to be pretty advanced before it is able to automate 20% of your workflow without you having to spend a long time rearranging your workflow around it. Therefore, by the time it's able to automate 20% of your work, with minimal efforts on your part, it's actually not too far away from being able to automate everything. And that could give a reason why by the time AI is having really notable, significant impacts on people's workflows, it's actually surprisingly close to the point at which it's able to just almost fully automate their workflows. Those are for me some of the stronger arguments in favor of a small effective FLOP gap.&nbsp;</p><p>The main argument in favor of a larger one, for me, is just that there's such a wide variety of tasks in the economy\u2014 even in AI R&amp;D (though to a lesser extent)\u2014and they vary along many dimensions, like the time horizon over which they're performed, the amount of social context they require, how repeatable they are, how much it matters if you make a mistake, etc. Those differences mean that AI could be much more suited to automate some of those tasks than others, based on how similar it is to the AI training objective, how expensive it is to fine-tune AI on horizons of that length, and how much training data we have for the task. So it seems like AI would have more competitive advantages at some tasks compared with others, and that could increase the time between it being able to automate the first task and it being able to automate the last tasks.&nbsp;</p><p><strong>Future Matters</strong>: At the end of all this, you come away with a shorter median estimate for timelines than the bio anchors model, by about ten years, despite sharing several of the key assumptions. What are the main drivers of these shortened timelines in your analysis, relative to Ajeya\u2019s?&nbsp;</p><p><strong>Tom Davidson</strong>:&nbsp; I think the main thing is the partial automation from pre-AGI systems causing a speed up, especially in AI R&amp;D. The basic story is, the bio anchors model makes its prediction by extrapolating the trend in hardware progress and extrapolating the trend in algorithmic progress. But it's not accounting for the fact that before we get fully transformative AI, we're going to get AI which is pretty useful for doing hardware R&amp;D and designing new chips, pretty useful for writing code for various experiments we want to run automatically, and maybe pretty useful for generating a hundred ideas for new algorithmic approaches and then critiquing those ideas and then winnowing them down to the ten most promising and then showing them to the human. And you would expect that partial automation of R&amp;D to speed up progress on both those dimensions, both the hardware and the algorithms. In which case the bio anchors extrapolation is too conservative. One thing that I'm doing with this analysis is modeling that process whereby we train some not quite transformative but pretty good AIs and they speed up progress on the hardware and the algorithmic R&amp;D. And that just happens more and more as we approach AGI: we get more and more acceleration from that. When I model that as best as I can, the result is that this dynamic shortens timelines by six years or so.&nbsp;</p><p>Probably the next biggest factor is, compared to bio anchors, expecting higher spending on the&nbsp; very biggest AI training runs. Once you've got AI that can readily automate 50% of the economy, for example, that's able to generate $25 trillion of value worldwide per year, assuming it was fully deployed, that's just a huge amount of value and it seems like it would be well worth spending a trillion dollars on that training run. And there is even more incentive to do that if you're being competitive with other actors who might want to get there first. And so I'm more willing than bio anchors to think that the spending on training runs could get pretty big because of this kind of dynamic. Some of the gain comes from just more economic growth, meaning there's more money around to invest in this stuff. Some of it is that you get AI to automate fabs so they can make more chips, which allows for faster scale-up. But most of it is just more willingness to spend on training runs as a fraction of world GDP.&nbsp;</p><p><strong>Future Matters</strong>: Another factor you have pushing towards shorter timelines is this idea that we\u2019ll be \"swimming in runtime compute\" by the time we\u2019re training human-level AI systems. Could you explain this?</p><p>&nbsp;<strong>Tom Davidson</strong>:&nbsp; The idea there is the following. Suppose that you took the median of the bio anchors where I think AGI took 1e35 FLOP to train, that you've just done that training run and that you're going to use that FLOP you just used to train the system to run copies of the system. How many copies could you run? You're going to be able to run an ungodly number of AIs immediately, just using that training compute.&nbsp;</p><p><strong>Future Matters</strong>:&nbsp; Is this ratio between the compute required for model training vs model inference relatively stable?</p><p><strong>Tom Davidson</strong>: Actually, you should expect that fraction to become more extreme as you do bigger training runs. So if you double the size of a model, the compute needed to run the model doubles, but the compute needed to train it by a factor of four. If you think that you need as much compute as the bio anchors median, then you\u2019ll be able to run&nbsp;<i>way&nbsp;</i>more copies with the training compute than you can today Bio anchors median implies you\u2019d need about 10<sup>10&nbsp;</sup>&nbsp;times as much compute to train AGI as the biggest public training run to date, which means&nbsp; you\u2019d only need 10<sup>5</sup> times as much compute to run the model. So if the current ratio is that you can immediately run 10 million systems with your training compute, then in the future with the bio anchors median estimate, you'd be able to run 10<sup>5</sup> times as many. So instead of 10 million, it would be 1 trillion. And I think if you run the numbers, that's the kind of thing you get. And you might think this is odd: we trained AGI and now&nbsp; we can run 1 trillion of them. You can then maybe think that, in fact, if we wanted&nbsp; to match the human R&amp;D workforce, rather than having 1 trillion AGIs, we can make do with 100 billion somewhat-less-capable-than-AGI systems. Maybe you use the kind of techniques that people are already using today, like chains of thought, or like having a hundred attempted answers to the question and then assessing them and then picking the best. Maybe there are other techniques for running your somewhat dumb AIs for ages and then combining their results in clever ways that can allow you to actually match or exceed the performance of a human worker, even if the individual AI systems are less clever than a human worker. And so the conclusion of this for me is that we could achieve full automation of, for example, R&amp;D before we've trained any one AI system that is individually as smart as a human scientist. And we could just do that by training AIs that are a bit less smart, but being able to run so many, that cumulative output working together exceeds that of all human scientists working together.&nbsp;</p><p><strong>Future Matters</strong>:&nbsp; Does this consideration generalize across the whole economy? Overall it seems like you end up kind of \u2018moving\u2019 the capabilities threshold for AGI a bit earlier, because there is this factor that always gets you a bit further than you think you are?</p><p><strong>Tom Davidson</strong>:&nbsp; I think so. And it comes down to definitions of AGI. If you were defining AGI as 'one system can on one forward pass match the output of one human brain on one forward pass', then that takes just as much effective training compute to develop. But if your definition for AGI was something like 'AI systems collectively can outperform human brains collectively on any task' or 'AI systems collectively could fully automate this particular sector', then yes, I think that kind of AI is easier to train than I previously thought, maybe quite a bit easier. What's interesting about this is that it's a very strong argument, if you believed something like bio anchors median training requirements, and it's a less strong argument, if you had much smaller training requirements, because then you've got less of this excess runtime compute lying around. So for me, it focuses the probability mass near the lower end of the bio anchors distribution.&nbsp;</p><p><strong>Future Matters</strong>: At the current margin, if you wanted to improve your estimates of AI takeoffs, would you focus more on (1) trying to better estimate the parameters of the current model, (2) extending the model in various directions, or (3) developing a new model altogether?&nbsp;</p><p><strong>Tom Davidson</strong>: The one thing I'm most interested in is trying to understand how much algorithmic progress is driven by cognitive work \u2013 generating insights and thinking about how the algorithms fit together in the architectures \u2013&nbsp; versus just brute experimentation. This becomes really important in the later stages of takeoff, where if everything is just driven by coming up with better ideas today, then if you extrapolate that, you think that algorithmic progress will become crazy fast as you approach full AI automation as there will be be abundant cognitive work from AI. Whereas if you think it's all about experiments, then progress can't become that crazy fast because AI automation doesn\u2019t immediately increase the amount of physical compute you\u2019ve got access to and it's a bit harder to really rapidly increase that. This gets at this question of time from almost full automation to superintelligence, which I think is something that is a bit under explored in the current report.&nbsp;</p><p><strong>Future Matters</strong>: Thank you, Tom!</p><hr><p><i>We thank Leonardo Pic\u00f3n and Lyl Macalalad for editorial assistance.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlz6004a8ahb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflz6004a8ahb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See also our conversation with Long in&nbsp;<a href=\"https://forum.effectivealtruism.org/s/Y7rCDmxRbrrKBT9Bo/p/CictHfn8kdyupvpNK\"><i><u>FM#3</u></i></a>; and the 80k podcast episode&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/CrmE6T5A8JhkxnRzw/future-matters-8-bing-chat-ai-labs-on-safety-and-pausing#Audio___video\"><u>mentioned below</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno3lo4s01awq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo3lo4s01awq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See also coverage of the debate in&nbsp;<a href=\"https://archive.is/sTzO5\"><i><u>Nature</u></i></a>&nbsp;[<a href=\"https://pod.link/1648718500/episode/6136dbf91fe34d58f96126545250496d\"><u>\ud83d\udd09</u></a>].</p></div></li></ol>", "user": {"username": "Pablo_Stafforini"}}, {"_id": "qiojZxPLb8dQYFQkL", "title": "Contribute to our Research Project: Help to Identify EA-related reasoning in Survey Responses.", "postedAt": "2023-03-21T11:58:23.516Z", "htmlBody": "<p>Hello everyone,</p><p>&nbsp;My name is Janek Kretschmer, and I am a PhD researcher at Maastricht University (https://www.maastrichtuniversity.nl/j.kretschmer). I am thrilled to announce an exciting opportunity to be a part of a EA related research project that could make a real impact in the world of charitable giving.</p><p>Along with my co-author, I am working on an experimental study that examines the impact of giving pledges on donors' generosity and preferences for effective charities. Our project is nearing completion, and the results are promising.</p><p><strong>Your contribution:</strong></p><p>Now, we need your help to narrow down the channels of the treatment effect further. We're specifically looking for assistance in analyzing free-text responses given by our participants. By categorizing these short texts, you'll help us understand donors' motivations and decision-making processes (<i>for scientific reasons we are not allowed to do it ourselves</i>).</p><p>Don't worry, we'll provide you with a detailed \"code book\" that outlines how to classify participants' answers. You'll simply need to decide whether the reason given is impact-related or not. We estimate that the entire task should take no more than 4 hours.</p><p><strong>What we can offer in return:</strong></p><p>We understand that your time is valuable, and we want to show our appreciation for your assistance. Unfortunately, we do not have funds to pay for research assistance. However, we can offer you consumable goods of your choice (e.g., good wine or a voucher) as a token of our gratitude. Additionally, if you're working on a thesis related to effective altruism or experimental research, I would be happy to offer advice and support.</p><p>Finally, your are contributing to real EA related research. Your work will help us better understand how pledges like those elicited by Giving What We Can impact donors' behavior and preferences.</p><p>If you're interested in participating, please don't hesitate to send me a message or email (j.kretschmer@maastrichtuniversity.nl). I'm always available to answer any questions you may have and to schedule a Zoom meeting to discuss the project in more detail. Thank you in advance for your help, and we look forward to hearing from you soon!</p><p>Best wishes</p><p>Janek</p>", "user": {"username": "Janek.Kretschmer"}}, {"_id": "GYK7aMHAWhjm7davL", "title": "Revolutionising National Risk Assessment (NRA): improved methods and stakeholder engagement to tackle global catastrophe and existential risks", "postedAt": "2023-03-21T06:05:15.689Z", "htmlBody": "<p>Co-authored with Nick Wilson</p><h2><strong>Summary/TLDR</strong></h2><ul><li>In a&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/10.1111/risa.14123\"><u>recently published paper</u></a>, we identified two major shortcomings of National Risk Assessment (NRA) processes:&nbsp;<ul><li>Lack of transparency around foundational assumptions</li><li>Exclusion of the largest scale risks</li></ul></li><li>We demonstrate the potential problems and ambiguities that arise in NRA due to these shortcomings.&nbsp;</li><li>We identify the exclusion of global catastrophic risks (GCRs) and existential risks (x-risks) from NRAs as a critical process error.&nbsp;&nbsp;</li><li>Even when only considering people alive today, and with a time horizon of just one year, the consequence in expectation of several existential risks is higher than all other risks commonly included in NRAs.&nbsp;</li><li>A longtermist perspective is not needed to prioritise existential risk mitigation through NRA, and potentially detracts from getting such risks onto the agenda for assessment.&nbsp;</li><li>We propose the development of a freely available, open-access, risk communication and engagement tool to facilitate stakeholder discussions on NRAs.</li></ul><p>This post is a partial and high-level summary of our research paper on national risk assessment (NRA) published in the academic journal&nbsp;<i>Risk Analysis</i> in March 2023. This post also places our work in the context of another recent report on NRA identifying common ground. Consider reading&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/10.1111/risa.14123\"><u>our full paper</u></a> for complete details of our thinking on NRA as it applies to global catastrophe, and existential risk.&nbsp;</p><h2><strong>Introduction</strong></h2><p>Many countries undertake National Risk Assessment (NRA) to evaluate risks of national significance, assessing for example, natural hazards, infectious diseases, industrial accidents, terrorist attacks, cyberattacks, organised crime, or institutional failure. The NRA process is complex and cross-sectoral, often excluding risks with low probability, and often has a short-term focus of less than five years. The outputs of NRA tend to communicate results in some form of National Risk Register (NRR) and/or consequence-probability (C,P) risk matrix.&nbsp;</p><p>However, NRAs and NRRs can be criticised particularly where the common practice of presenting a two-dimensional risk matrix obscures uncertainties, stakeholder disagreements on values, bias, and systematic errors. Critically, the exclusion of large-scale (and cross-border) risks such as global catastrophic risks (GCRs) and existential threats to humanity (x-risks) is another limitation of NRAs.</p><p>The aim of NRA should be to find common understanding across stakeholders of risks and priorities, stimulate local authorities to build capacity and capability, and identify common consequences across multiple risks. Prioritisation of risks is sometimes explicitly intended through the NRA process, but methods for prioritisation depend on foundational assumptions of the NRA process that are not always clearly articulated.&nbsp;</p><h3><strong>Aim of our paper</strong></h3><p>Our paper sought to demonstrate some shortcomings of existing NRA processes and outputs, namely:&nbsp;</p><ol><li>How the choice of fundamental NRA process assumptions makes a material difference to the NRR output and any subsequent deliberation on risk.</li><li>The weaknesses and ambiguity of risk matrices for communicating NRAs.</li><li>A major class of risks often neglected by NRA (namely GCRs and x-risks).</li><li>The difficulties that uncertainty poses.&nbsp;</li></ol><p>We then suggest how those undertaking NRA could enter a productive dialogue with stakeholders, supported by an interactive communication and engagement tool, to overcome some of these difficulties (details of that are in&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/10.1111/risa.14123\"><u>the paper</u></a>, not the post below).&nbsp;</p><p>We note that another report, by Kevin Kohler, titled&nbsp;<a href=\"https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/592788/2/RR-Reports-2023-National-Risk-Assessments-of-Cross-Border-Risks.pdf\"><u>National Risk Assessments of Cross-Border Risks</u></a> was published in February 2023, shortly before our paper. Throughout this post we also highlight some of the key points therein.&nbsp;&nbsp;</p><h2><strong>Important Assumptions of National Risk Assessments</strong></h2><p>In our paper, we introduce a hypothetical set of six risks A\u2013F (which vary by probability and consequences) to illustrate some key issues when undertaking NRA and when using NRAs and risk matrices to communicate national risk or inform prevention and mitigation.</p><p>We demonstrate how changing fundamental analysis assumptions changes the ordinal prioritisation of the risks. The importance of this is that the basis of the assumptions is often opaque to end users, or has not been authorised by public debate and stakeholder input (noting that future generations are also stakeholders).&nbsp;</p><p>The assumptions we systematically alter are: the scenario of choice (challenging scenario vs worst case), the time horizon of interest (one year, 50-years), the discount rate on future value (0%, 3%), and decision rule.&nbsp;</p><p>We demonstrate how different assumption combinations alter the ordinal priority of the risks A\u2013F (when considering just expected fatalities for simplicity). We show that varying the evaluation assumptions leads to different prioritisation of risks in 7 out of 8 analyses, thereby emphasising the critical importance of agreeing on process assumptions.&nbsp;</p><h2><strong>Probability-consequence Risk Matrices</strong></h2><p>The next section of our paper reiterates some criticisms of probability-consequence risk matrices in the context of NRA. We note that such matrices are fairly arbitrary constructions. Risk matrices generally look something like the following figure. Risks are placed in categories according to likelihood and expected impact. Darker regions (purple, red, orange) allegedly represent more salient risks than lighter regions (yellow, green, blue).</p><p><strong>Figure 1</strong>: A probability-impact risk matrix</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GYK7aMHAWhjm7davL/of3wsgoetuhhv5yyseyv\"></p><p>We dispense with the colours and simply plot our demonstration risks A\u2013F on axes representing likelihood and impact. A concrete example of the misleading nature of risk matrices (if categories are used) can be seen in the following figure. Risks \u2018F\u2019, \u2018D\u2019, and \u2018B\u2019 all appear to cluster in one region, towards the \u2018upper right\u2019, ie, the highest priority area of the risk matrix. Yet, the numerical consequence in expectation (fatalities) of risk D is 20x that of risk B. This may be somewhat apparent when the logarithmic axes are labelled and the risks are plotted in a scatterplot, but it would be completely obscure in the coloured matrix above.</p><p><strong>Figure 2</strong>: Risks with vastly different consequences in expectation can cluster in risk matrices</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GYK7aMHAWhjm7davL/rxvk1cd3ne4z9mzqxpud\"></p><p>We provide further examples in the paper illustrating how risks with the highest consequence in expectation can end up being equated with minor common events due to the heat-map nature of some risk matrices.&nbsp;</p><h2><strong>Global catastrophic and existential risks</strong></h2><p>Not only do fundamental assumptions and communication choices bias the assessment of national risks, but cross-border risks and in particular global catastrophic and existential risks are seldom included in NRAs.&nbsp;</p><p>Our analysis of five NRAs (and Kohler\u2019s 2023 analysis of nine) shows that no NRA appears to include many, if any, GCRs or x-risks. Surprisingly the Norwegian NRA mentions in one sentence that a large volcanic eruption could \u2018cool the earth by several degrees\u2019. But then never mentions the global consequences of what could be the single most catastrophic impact contemplated by any NRA.</p><p>In our paper, we consider only the existential risks among a set of GCRs and ignore the more likely but non-existential manifestations of the same risks. Simple estimates reveal that several of these risks harbour annualised consequences in expectation greater than all typically occurring natural hazards combined.&nbsp;</p><p>Even when only considering people alive today, and with a time horizon of just one year, the consequence in expectation of several existential risks appears higher than all other risks commonly included in NRAs. We identify the exclusion of GCRs and x-risks from NRAs as a critical process error.&nbsp;&nbsp;</p><p>A longtermist perspective is not needed to prioritise existential risk mitigation and potentially detracts from getting such risks onto the agenda for assessment. Indeed, it appears that standard risk assessment processes, and standard government cost-effectiveness analyses should be enough to reveal the overwhelming priority of GCRs and x-risks in NRA (Shulman &amp; Thornley have&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/DiGL5FuLgWActPBsf/how-much-should-governments-pay-to-prevent-catastrophes\"><u>posted recently</u></a> on the EA Forum agreeing with this point at length).&nbsp;</p><p>We argue in the paper that deliberation over such risks and whether they ought to be prioritised for mitigation, can only happen if they are included in the NRA, characterised, communicated to stakeholders, and put forward to resource prioritisation processes for prevention or mitigation.</p><p><a href=\"https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/592788/2/RR-Reports-2023-National-Risk-Assessments-of-Cross-Border-Risks.pdf\"><u>Kohler\u2019s new paper</u></a>&nbsp;notes that the European Commission specifically recommends that NRAs include risks (no matter how rare) if the likely impact exceeds 0.6% of gross national income and the time horizon of interest should ideally be at least 25\u201335 years. These instructions mean that all GCRs and x-risks should be assessed in NRAs.&nbsp;</p><p>Indeed, the US has recently passed a world-leading&nbsp;<a href=\"https://adaptresearchwriting.com/2023/02/05/us-takes-action-to-avert-human-existential-catastrophe-the-global-catastrophic-risk-management-act-2022/\"><u>Global Catastrophic Risk Management Act</u></a>, which mandates exactly this kind of systematic assessment of GCRs and x-risks, along with response plans to ensure basic necessities are available post-facto (we have&nbsp;<a href=\"https://adaptresearchwriting.com/2023/02/05/us-takes-action-to-avert-human-existential-catastrophe-the-global-catastrophic-risk-management-act-2022/\"><u>blogged</u></a> about this Act). There is no good reason why all countries can\u2019t replicate this legislation or at least empower the United Nations to do it for all countries/regions (you can read a recent 2023 discussion of existential risks and the UN Office for Disaster Risk Reduction by the Simon Institute&nbsp;<a href=\"https://www.undrr.org/publication/thematic-study-existential-risk-and-rapid-technological-change-advancing-risk-informed\"><u>here</u></a>).&nbsp;</p><h3><strong>Example: Pandemics</strong></h3><p>Pandemics are an interesting case, and although we don\u2019t dwell on them specifically in our paper, the Covid-19 pandemic illustrates the clear shortcomings of NRAs. We argue in&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/epdf/10.1111/risa.14123\"><u>our paper</u></a> that national risks should not be presented in a risk matrix, but should be communicated quantitatively in ordinal fashion according to the consequence in expectation of agreed scenario types, across an agreed timeframe, under an agreed discount rate.</p><p>A standard national risk assessment presents the risk of pandemics something like this:</p><p><strong>Figure 3</strong>: Human pandemic as a relatively likely &amp; catastrophic risk (source: DPMC publication: \u2018NZ\u2019s National Security System\u2019 Sept 2011).</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GYK7aMHAWhjm7davL/i3kvpbbsjv5swvrq1q7a\"></p><p>However, Kohler points out that the Covid-19 pandemic has already exceeded the most severe pandemic scenario in most NRAs. This is even though it \u2018only\u2019 had an infection fatality ratio of less than 0.6%. Even the conservative official death toll from Covid-19 accounts for 95% of the deaths from disasters in the 21st Century. The other 5% include all deaths from the 2010 Haiti earthquake, plus the 2004 Indian Ocean tsunami, plus the 2008 Myanmar cyclone (about 200,000 deaths each).&nbsp;</p><p>If the risk from human pandemics in the first two decades of the 21st Century were presented in a treemap chart (rather than a risk matrix) it might look something like this, thereby revealing the real salience of human pandemics:</p><p><strong>Figure 4</strong>: Gestural treemap chart showing scale of pandemics in the 21st Century vs other major disasters</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GYK7aMHAWhjm7davL/q5hyebpyxz3kk0pry39r\"></p><p>&nbsp;Indeed, Kohler found that only Switzerland and the Netherlands have chosen risk impact categories at the upper end that roughly correspond to the impact of Covid-19. And these categories would not discriminate between Covid-19 and a worse pandemic in the risk matrix.&nbsp;</p><p>It has been our own experience that even using the Swiss method for NRA, applied in a&nbsp;<a href=\"https://adaptresearchwriting.com/2023/02/20/workshop-on-nuclear-war-winter-nz-wellbeing-of-millions-and-1-trillion-plus-at-risk-strategic-resilience-must-become-bread-butter-nz-policy/\"><u>workshop on the nuclear war/winter hazard risk</u></a> to a non-combatant nation, that these upper impact categories are seriously inadequate.&nbsp;</p><p>The reality is that if NRAs were actually presented as Treemap charts, or in some other form than risk matrices, and if the suite of GCRs and x-risks was included, then the picture of risk communicated would look very different. Over longer periods of time most (almost all?) expected disaster deaths come from a few worst case scenarios.</p><p>However, any presentation of a chart or graph is packed with foundational assumptions and can obscure uncertainties.&nbsp;</p><h2><strong>Uncertainty and Assumption</strong></h2><p>We acknowledge that the probability of GCRs and x-risks is highly uncertain. But this appears to be the case with many risks already included in NRAs. For example Kohler reports that the likelihood of a -1600 nano-tesla (nT) solar storm was cited as 1:80 per annum in the 2015 Swiss NRA, but 1:1700 in the 2020 version of the same analysis. The explanation was that a mathematical analysis concluded that intensity of solar storms decreases with time since an event. Yet,&nbsp;<a href=\"https://royalsocietypublishing.org/doi/10.1098/rspa.2022.0497\"><u>research</u></a> post-dating that analysis suggests that tree ring radiocarbon evidence might indicate large solar storms might be much more common than we have thought. More expert input appears to be needed.&nbsp;</p><p>Similarly, for volcanic eruptions, the probability of a volcano affecting Switzerland was estimated at 1:70,000 whereas the UK\u2019s analysis cited 1:20 to 1:4. Kohler notes an annualised baseline probability of 1:3000 for a Volcanic Explosivity Index (VEI) 6+ eruption in Europe. However, neither NRA mentions the 1:625 probability of a VEI 7 eruption somewhere else in the world, which like the Mt Tambora eruption of 1815 could have devastating consequences for global agriculture (we discuss the Mt Tambora eruption as it impacted potential island refuges in&nbsp;<a href=\"https://www.nature.com/articles/s41598-023-30729-2\"><u>a separate paper</u></a> in 2023).</p><p>In the present paper, our discussion then proceeds across other issues of uncertainty, including the problem that strength of knowledge poses (eg, equally likely risks but the strength of knowledge underpinning the data varies), the problem of dealing with different scenarios of a single hazard, the difficulty of probabilities that change across time, and how all these factors point towards the need for public engagement.&nbsp;</p><p>Ultimately, NRAs are a social construction, built upon allegedly reasonable assumptions (about time frame of interest, discount rate, scenarios of choice, and decision rules), and including agreed choices about risk communication methods. All of this needs to be debated openly.&nbsp;</p><h2><strong>Stakeholder Engagement</strong></h2><p>Most NRA processes involve little public consultation and in some instances overt secrecy. There is a documented lack of awareness of NRRs, even among local authorities to whom they are in part directed. This is despite the UN advocating for \u2018increased access to risk information\u2019 and that, \u2018low risk awareness is one of the main challenges\u2019.&nbsp;</p><p>It is also unclear if citizens understand the foundational assumptions underpinning NRAs and whether they would authorise them if they did.&nbsp;</p><p>In the paper we identify a range of arguments that would support wider public and expert engagement, including: risks of potential groupthink, politicisation, or uncertainty.&nbsp;</p><p>We note that scrutiny must logically first be applied to the underlying process assumptions, then to the resulting empirical claims, and finally deliberative prioritisation (for prevention, mitigation or further research) can take place. We propose the development of a freely available, open-access, risk communication and engagement tool to facilitate discussions on NRAs. Aspects of such a tool could be tailored to experts and other aspects to the general public.&nbsp;</p><p>In&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/epdf/10.1111/risa.14123\"><u>our paper</u></a> we lay out the rationale for expert engagement, public engagement, and describe in some detail the sort of interactive online tool that could be deployed to support such engagement.&nbsp;</p><h2><strong>Conclusions</strong></h2><p>In our paper we identified two shortcomings of National Risk Assessment (NRA) processes: lack of transparency around foundational assumptions, and exclusion of the largest scale risks.&nbsp;</p><p>We discuss the importance of agreeing on key assumptions before conducting a NRA. The assumptions include methodological and normative choices that determine which risks are included, how they are characterised over time, and how uncertainties are expressed in risk communication.</p><p>A hypothetical demonstration set of risks is used to show how choices around time horizon, discount rate, and impact estimation affect risk characterisation. We highlighted the potentially dominating importance of global catastrophic and existential risks, which are often omitted from NRAs, and suggested using standard risk assessment and cost-effectiveness analyses to address them.&nbsp;</p><p>Given the array of possible assumptions, uncertainties and inclusions, it is crucial that those undertaking NRA engage the public and a broad array of experts in the NRA process through a transparent and two-way risk communication process. This could help legitimise key assumptions, avoid omitting important risks, and provide robust critique of risk characterisations and the knowledge underpinning them.</p>", "user": {"username": "Matt Boyd"}}, {"_id": "hXLGQBrDg3iwQjWDp", "title": "Capping AGI profits", "postedAt": "2023-03-21T13:29:33.845Z", "htmlBody": "<h2>Introduction</h2><p>Beyond the many concerns around AI alignment, the development of artificial general intelligence (AGI) also raises concerns about the concentration of wealth and power in the hands of a few corporations. While I\u2019m very glad to see people working on avoiding worst-case-scenarios, my impression is that there is relatively less attention being given to \u201cgrey area\u201d scenarios, under which catastrophe is neither near-certain nor automatically avoided. These scenarios strike me as worlds in which policy and governance work may be relatively important.&nbsp;</p><p>In this post I outline a case for government-imposed caps on extreme profits generated through AGI. I think could be promising both as a way to distribute AGI-generated wealth democratically, and (hopefully) to disincentivize AGI development by profit-motivated actors.&nbsp;</p><p><i>Edit (March 21): </i>Thank you to Larks for pointing out the strong similarities between this proposal and a <a href=\"https://forum.effectivealtruism.org/topics/windfall-clause\">Windfall Clause</a>. As far as I can tell, my proposal mainly differs from a Windfall Clause in that it is feasible to implement without the coordinated buy-in of AI labs, and that it fits more squarely in existing policy paradigms. As potential drawbacks, it seems more prone to creating tensions at an international level, and less targeted at effective redistribution of funds, although I think there could be practical solutions to these issues.</p><p>Disclaimers:&nbsp;</p><ul><li>As best I can tell, the idea of a \u201ccapped-profit\u201d organization was introduced by OpenAI in 2019, <s>but I have not seen any discussion of it in the context of broader policy options</s>. I do not claim to have any especially novel ideas here, but I apologize if I've missed someone else's work on this.</li><li>Since I am sympathetic to the claim that equitable distribution of wealth is a second-order problem in the face of potential AGI ruin, I am conditioning the remainder of the post with \"assuming we find practical ways to make AI both safe and useful.\"</li></ul><p>&nbsp;</p><h2>AGI wealth</h2><p>Many believe that AGI has the potential to generate an enormous amount of wealth, provided that we avoid disastrous outcomes. For example,&nbsp;<a href=\"https://www.metaculus.com/questions/3477/if-human-level-artificial-intelligence-is-developed-will-world-gdp-grow-by-at-least-300-in-any-of-the-subsequent-15-years/\"><u>Metaculus forecasters</u></a> are predicting with 60% likelihood that GDP will grow by 30% or more annually in any of the 15 years after human level AI is achieved. On&nbsp;<a href=\"https://manifold.markets/Gigacasting/ai-10-of-gdp-by-2050\"><u>Manifold</u></a>, predictors expect an 80% chance that AI will constitute more than 10% of GDP by 2050 (although that market is thin, and the criteria for resolution are unclear).&nbsp;</p><p>Consistent with this notion, OpenAI&nbsp;<a href=\"https://openai.com/blog/openai-lp\"><u>restructured itself</u></a> as a capped-profit organization in 2019.&nbsp; The move was intended to allow OpenAI to fund itself as a profit-driven company in the short term, while maintaining its non-profit mission if they succeed at creating AGI. If the organization becomes immensely profitable due to its development of powerful AI, investors' returns will be limited to a fixed amount (100 times their contribution, for initial investors), and any excess profits will be redirected to a supervising nonprofit organization, whose \"primary fiduciary duty is to humanity.\"</p><p>Although this seems admirable in purpose, it raises several questions. How will the non-profit use its massive income? Will the OpenAI board act as benevolent autocrats, funding social programs of their choice, or will there be an attempt to create democratic channels of decision-making? Can we trust the entity to adhere to its charter faithfully? Above all, what will happen in the future if some other profit-driven company is the first to create AGI?</p><p>&nbsp;</p><h2>Capping profits more broadly</h2><p>OpenAI's capped-profit model suggests a policy option for governments who view the above questions as concerning. Rather than hoping that AI companies will charitably distribute massive profits, governments could impose a fixed limit on company profitability. If companies become incredibly wealthy as a result of AGI, the ensuing tax revenues could be used to finance social programs such as universal basic income (UBI), through existing democratic pathways.</p><p>&nbsp;</p><p>I think such a policy has a number of attractive properties:</p><ul><li>In addition to providing stronger democratic assurances, this could help disincentivize reckless AGI development by companies pursuing tail outcomes.</li><li>The profitability limit could be tailored such that no business-as-usual company expects to be affected, minimizing its distortionary effects outside of companies pursuing transformative technologies.</li><li>The policy fits cleanly into established tax policy frameworks, reducing the friction of implementation.</li><li>Since nearly all voters and political actors stand to benefit, it seems like it should be relatively easy to find support for this kind of policy.</li></ul><p><br>Naturally, there are numerous questions and uncertainties to be addressed:</p><ul><li>It would be critical to ensure the policy is highly robust to a variety of takeoff scenarios, that it balances redistribution with a \"winning\" company's ongoing capital requirements, and that it properly captures profits in high-growth outcomes while avoiding taxing unintended targets. I do not yet have a strong view on what the specifics should entail.</li><li>Companies pursuing AGI might be able to use their technology to effectively circumvent the policy. If they anticipate this possibility ahead of time, it will reduce the policy\u2019s disincentive effects.</li><li>It is unclear to what extent leading AI labs would embrace or oppose this kind of policy. On the one hand, it could ease certain race dynamics and generate favorable PR; on the other, many are aiming to win the race.</li><li>The usual issues of tax evasion apply here; it would be easy for companies to relocate to countries with lower taxes on corporate profits.</li><li>Finally, it is not straightforwardly clear that governments would be better stewards of the money than would a profit-capped organization like OpenAI. To begin with, governments are primarily accountable to only their citizens; if AGI is created in a country with AGI profit caps, we may end up with&nbsp;<i>less</i> equitable outcomes. Finding ways to mitigate this should be a high priority if this policy is seriously being considered.</li></ul><p>&nbsp;</p><p>Despite the unresolved questions, this appears to be a promising direction to me. Even if companies could easily evade regulations using AGI, it seems plausible that such a policy could create a Schelling point for cooperation and contribution to the social good. On the last bullet point, I suspect that there are solutions to the problem of distributing tax revenues globally, which at least outperform corporate disbursements in expectation.&nbsp;</p><p>I also think that starting with a legible and broadly popular policy would be a very good way to initiate public discussion of AI governance. While there is likely a lot of behind-the-scene work that I am unaware of, my impression is that existing momentum in public AI policy is not heading in the most useful direction. It strikes me that taking a positive first step, especially one which recognizes surprising claims like \"AGI companies may grow 100 or 1000X,\" would be helpful for shifting the Overton window on policy in the right direction.</p><p><br>&nbsp;</p>", "user": {"username": "Luke Frymire"}}, {"_id": "7NCcDjYqsqe7iBTfQ", "title": "EA Finance NYC Meetup", "postedAt": "2023-03-21T04:41:26.605Z", "htmlBody": "<p>EA Finance NYC is hosting our first social in a while! This is a great opportunity to meet people within finance and learn more about EA. We will provide light snacks (vegan).&nbsp;</p><p>Details:</p><p>Friday, March 24 7pm-9pm</p><p>Juke Bar (East Village, 196 2nd Avenue, New York NY 10002)<br>&nbsp;</p><p>RSVP here: bit.ly/nyeafinance1</p>", "user": {"username": "Sydney Filler"}}, {"_id": "m4qDgMrb8wD6fXS59", "title": "Deep Deceptiveness", "postedAt": "2023-03-21T02:51:53.338Z", "htmlBody": "", "user": {"username": "So8res"}}, {"_id": "46tXkg838EZ6uie45", "title": "My Objections to \"We\u2019re All Gonna Die with Eliezer Yudkowsky\"", "postedAt": "2023-03-21T01:23:00.117Z", "htmlBody": "<p><i>Note: manually cross-posted from LessWrong. </i><a href=\"https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky#comments\"><i>See here</i></a><i> for discussion on LW.</i></p><h1>Introduction</h1><p>I recently watched Eliezer Yudkowsky's <a href=\"https://www.youtube.com/watch?v=gA1sNLL6yg4\">appearance on the Bankless podcast</a>, where he argued that AI was nigh-certain to end humanity. Since the podcast, <a href=\"https://twitter.com/Noahpinion/status/1633351020297465857?s=20\">some</a> <a href=\"https://www.overcomingbias.com/p/ai-risk-again\">commentators</a> have offered <a href=\"https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html\">pushback</a> against the doom conclusion. However, <a href=\"https://twitter.com/ESYudkowsky/status/1633613790842617858?s=20\">one</a> <a href=\"https://www.reddit.com/r/singularity/comments/117ozlp/comment/j9fg80k/?utm_source=share&amp;utm_medium=web2x&amp;context=3\">sentiment</a> <a href=\"https://twitter.com/breenemachine/status/1637157114690035712\">I</a> <a href=\"https://twitter.com/RichardMCNgo/status/1637737722541658117?s=20\">saw</a> was that optimists tended not to engage with the specific arguments pessimists like Yudkowsky offered.&nbsp;</p><p>Economist Robin Hanson<a href=\"https://richardhanania.substack.com/p/robin-hanson-says-youre-going-to\"> points out</a> that this pattern is very common for small groups which hold counterintuitive beliefs: insiders develop their own internal language, which skeptical outsiders usually don't bother to learn. Outsiders then make objections that focus on broad arguments against the belief's plausibility, rather than objections that focus on specific insider arguments.</p><p>As an AI \"alignment insider\" whose current estimate of doom is around 5%, I wrote this post to explain some of my many objections to Yudkowsky's specific arguments. I've split this post into chronologically ordered segments of the podcast in which Yudkowsky makes one or more claims with which I particularly disagree.</p><p>I have my own view of alignment research: <a href=\"https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX\">shard theory</a>, which focuses on understanding how human values form, and on how we might guide a similar process of value formation in AI systems.</p><p>I think that human value formation is not that complex, and does not rely on principles very different from those which underlie the current deep learning paradigm. Most of the arguments you're about to see from me are less:</p><blockquote><p>I think I know of a fundamentally new paradigm that can fix the issues Yudkowsky is pointing at.</p></blockquote><p>and more:</p><blockquote><p>Here's why I don't agree with Yudkowsky's arguments that alignment is impossible in the current paradigm.</p></blockquote><h1>My objections</h1><h2>Will current approaches scale to AGI?</h2><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=886\">Yudkowsky apparently thinks not</a></h3><p>...and that the techniques driving current state of the art advances, by which I think he means the mix of generative pretraining + small amounts of reinforcement learning such as with ChatGPT, aren't reliable enough for significant economic contributions. However, he also thinks that the current influx of money might stumble upon something that does work really well, which will end the world shortly thereafter.</p><p>I'm a lot more bullish on the current paradigm. People have tried lots and lots of approaches to getting good performance out of computers, including lots of \"scary seeming\" approaches such as:</p><ol><li><a href=\"https://arxiv.org/abs/1703.03400\">Meta-learning over training processes</a>. I.e., using gradient descent over learning curves, directly optimizing neural networks to learn more quickly.</li><li><a href=\"https://arxiv.org/abs/2202.05780\">Teaching neural networks to directly modify themselves</a> by giving them edit access to their own weights.</li><li><a href=\"https://arxiv.org/abs/2101.07367\">Training learned optimizers</a> - neural networks that learn to optimize other neural networks - and having those learned optimizers optimize themselves.</li><li>Using program search to find more <a href=\"https://arxiv.org/abs/2302.06675\">efficient optimizers</a>.</li><li>Using simulated evolution to find more <a href=\"https://ojs.aaai.org/index.php/AAAI/article/view/21311\">efficient architectures.</a></li><li>Using <a href=\"https://arxiv.org/abs/2006.00719\">efficient second-order corrections</a> to gradient descent's approximate optimization process.</li><li><a href=\"https://arxiv.org/abs/2210.14593\">Tried applying</a> biologically plausible optimization algorithms inspired by biological neurons to training neural networks.</li><li><a href=\"https://proceedings.neurips.cc/paper/2019/hash/9ce3c52fc54362e22053399d3181c638-Abstract.html\">Adding learned internal optimizers</a> (different from the ones hypothesized in <a href=\"https://arxiv.org/abs/1906.01820\">Risks from Learned Optimization</a>) as neural network layers.&nbsp;</li><li>Having language models rewrite their own training data, and <a href=\"https://arxiv.org/abs/2210.11610\">improve the quality of that training data</a>, to make themselves better at a given task.</li><li>Having language models <a href=\"https://arxiv.org/abs/2207.14502v3\">devise their own programming curriculum</a>, and learn to program better with self-driven practice.</li><li><a href=\"https://arxiv.org/abs/2212.08073\">Mixing reinforcement learning</a> with model-driven, recursive re-writing of future training data.</li></ol><p>Mostly, these don't work very well. The current capabilities paradigm is state of the art because it gives the best results of anything we've tried so far, despite lots of effort to find better paradigms.&nbsp;</p><p>When capabilities advances do work, they typically integrate well with the current alignment<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7flo16mluhj\"><sup><a href=\"#fn7flo16mluhj\">[1]</a></sup></span>&nbsp;and capabilities paradigms. E.g., I expect that we can apply current alignment techniques such as <a href=\"https://arxiv.org/abs/1706.03741\">reinforcement learning from human feedback</a> (RLHF) to evolved architectures. Similarly, I expect we can use a learned optimizer to train a network on gradients from RLHF. In fact, the eleventh example is actually <a href=\"https://arxiv.org/abs/2212.08073\">ConstitutionalAI</a> from Anthropic, which arguably represents the current state of the art in language model alignment techniques!</p><p>This doesn't mean there are no issues with interfacing between new capabilities advances and current alignment techniques. E.g., if we'd initially trained the learned optimizer on gradients from supervised learning, we might need to finetune the learned optimizer to make it work well with RLHF gradients, which I expect would follow a somewhat different distribution from the supervised gradients we'd trained the optimizer on.&nbsp;</p><p>However, I think such issues largely fall under \"ordinary engineering challenges\", not \"we made too many capabilities advances, and now all our alignment techniques are totally useless\". I expect future capabilities advances to follow a similar pattern as past capabilities advances, and not completely break the existing alignment techniques.</p><p>Finally, I'd note that, despite these various clever capabilities approaches, progress towards general AI seems pretty smooth to me (fast, but smooth). <a href=\"https://arxiv.org/abs/2005.14165\">GPT-3</a> was announced almost three years ago, and large language models have gotten steadily better since then.</p><h2>Discussion of human generality</h2><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=1115\">Yudkowsky says humans aren't fully general</a></h3><blockquote><p>If humans were fully general, we'd be as good at coding as we are at football, throwing things, or running. Some of us are okay at programming, but we're not spec'd for it. We're not fully general minds.</p></blockquote><p>Evolution did not give humans specific cognitive capabilities, such that we should now consider ourselves to be particularly well-tuned for tasks similar to those that were important for survival in the ancestral environment. Evolution gave us a learning process, and then biased that learning process towards acquiring capabilities that were important for survival in the ancestral environment.</p><p>This is important, because the most powerful and scalable learning processes are also simple and general. The <a href=\"https://arxiv.org/abs/1706.03762\">transformer architecture</a> was originally developed specifically for language modeling. However, it turns out that the same architecture, with almost no additional modifications, can learn <a href=\"https://arxiv.org/abs/1802.05751\">image recognition</a>, <a href=\"https://arxiv.org/abs/2106.01345\">navigate game environments</a>, <a href=\"https://arxiv.org/abs/2104.01778\">process audio</a>, and so on. I do not believe we should describe the transformer architecture as being \"specialized\" to language modeling, despite it having been found by an 'architecture search process' that was optimizing for performance on language modeling objectives.</p><p>Thus, I'm dubious of the inference from:</p><blockquote><p>Evolution found a learning process searching for architectures that did well on problems in the ancestral environment.</p></blockquote><p>to:</p><blockquote><p>In the modern environment, you should think of the human learning process, and the capabilities it learns, as being much more specialized to problems like those in the ancestral environment, as compared to those problems of the modern environment.</p></blockquote><p>There are of course, possible modifications one could make to the human brain that would make humans better coders. However, time and again, we've found that deep learning systems improve more through scaling, of either the data or the model. Additionally, the main architectural difference between human and other primate brains is likely <a href=\"https://www.pnas.org/doi/10.1073/pnas.1201895109\">scale</a>, and <a href=\"https://onlinelibrary.wiley.com/doi/10.1002/ajpa.24712\">not</a> e.g., the relative sizes of different regions <a href=\"https://royalsocietypublishing.org/doi/10.1098/rspb.2020.2987\">or</a> maturation trajectories.</p><p>See also: <a href=\"https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine\">The Brain as a Universal Learning Machine</a> and <a href=\"https://www.lesswrong.com/posts/xwBuoE9p8GE7RAuhd/brain-efficiency-much-more-than-you-wanted-to-know\">Brain Efficiency: Much More than You Wanted to Know</a></p><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=1146\">Yudkowsky talks about an AI being more general than humans</a></h3><blockquote><p>You can imagine something that's more general than a human, and if it runs into something unfamiliar, it's like 'okay, let me just go reprogram myself a bit, and then I'll be as adapted to this thing as I am to - you know - anything else.</p></blockquote><p>I think powerful cognition mostly comes from simple learning processes applied to complex data. Humans are actually pretty good at \"reprogramming\" themselves. We might not be able to change our learning process much<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6jtv48k3s3d\"><sup><a href=\"#fn6jtv48k3s3d\">[2]</a></sup></span>, but we can change our training data quite a lot. E.g., if you run into something unfamiliar, you can read a book about the thing, talk to other people about it, run experiments to gather thing-specific data, etc. All of these are ways of deliberately modifying your own cognition to make you more capable in this new domain.</p><p>Additionally, the fact that techniques such as <a href=\"https://en.wikipedia.org/wiki/Sensory_substitution\">sensory substitution</a> work in humans, or the fact that losing a given sense causes the brain to <a href=\"https://en.wikipedia.org/wiki/Neuroplasticity#Deafness_and_loss_of_hearing\">repurpose</a> regions associated with that sense, suggest we're not that constrained by our architecture, either.</p><p>Again: most of what separates a vision transformer from a language model is the data they're trained on.</p><h2>How to think about superintelligence</h2><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=1290\">Yudkowsky describes superintelligence</a></h3><blockquote><p>A superintelligence is something that can beat any human, and the entire human civilization, at all the cognitive tasks.</p></blockquote><p>This seems like way too high a bar. It seems clear that you can have transformative or risky AI systems that are still worse than humans at <i>some</i> tasks. This seems like the most likely outcome to me. Current AIs have huge deficits in odd places. For example, GPT-4 may beat most humans on a variety of challenging exams (page 5 of the <a href=\"https://arxiv.org/abs/2303.08774\">GPT-4 paper</a>), but still can't reliably count the number of words in a sentence.</p><p>Compared to Yudkowsky, I think I expect AI capabilities to increase more smoothly with time, though not necessarily more slowly. I don't expect a sudden jump where AIs go from being better at some tasks and worse at others, to being universally better at all tasks.</p><h2>The difficulty of alignment</h2><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=2215\">Yudkowsky on the width of mind space</a>&nbsp;</h3><blockquote><p>the space of minds is VERY wide. All the human are in - imagine like this giant sphere, and all the humans are in this like one tiny corner of the sphere. And you know we're all like basically the same make and model of car, running the same brand of engine. We're just all painted slightly different colors.</p></blockquote><p>I think this is extremely misleading. Firstly, real-world data in high dimensions basically <i>never</i> look like spheres. Such data almost always cluster in extremely compact manifolds, whose internal volume is minuscule compared to the full volume of the space they're embedded in. If you could visualize the full embedding space of such data, it might look somewhat like an extremely sparse \"hairball\" of many thin strands, <a href=\"https://arxiv.org/abs/2207.02862v3\">interwoven</a> in complex and twisty patterns, with even thinner \"fuzz\" coming off the strands in even more-complex fractle-like patterns, but with vast gulfs of empty space between the strands.&nbsp;</p><p>In math-speak, high dimensional data manifolds almost always have vastly smaller <a href=\"https://en.wikipedia.org/wiki/Intrinsic_dimension\">intrinsic dimension</a> than the spaces in which they're embedded. This includes the data manifolds for both of:</p><ol><li>The distribution of powerful intelligences that arise in universes similar to ours.</li><li>The distribution of powerful intelligences that we could build in the near future.</li></ol><p>As a consequence, it's a bad idea to use \"the size of mind space\" as an intuition pump for \"how similar are things from two different parts of mind space\"?</p><p>The manifold of possible mind designs for powerful, near-future intelligences is surprisingly small. The manifold of learning processes that can build powerful minds in real world conditions is <a href=\"https://www.lesswrong.com/posts/hsf7tQgjTZfHjiExn/my-take-on-jacob-cannell-s-take-on-agi-safety#2__Will_AGI_algorithms_look_like_brain_algorithms_\">vastly smaller</a> than that.</p><p>It's no coincidence that state of the art AI learning processes and the human brain both operate on similar principles: an environmental model mostly trained with self-supervised prediction, combined with a relatively small amount of reinforcement learning to direct cognition in useful ways. In fact, alignment researchers recently narrowed this <a href=\"https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and\">gap</a> even further by <a href=\"https://arxiv.org/abs/2302.08582\">applying reinforcement learning</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnxk3auzvwms\"><sup><a href=\"#fnnxk3auzvwms\">[3]</a></sup></span>&nbsp;throughout the training process, rather than just doing RLHF at the end, as with current practice.&nbsp;</p><p>The researchers behind such developments, by and large, were not trying to replicate the brain. They were just searching for learning processes that do well at language. It turns out that there aren't many such processes, and in this case, both evolution and human research converged to very similar solutions. And once you condition on a particular learning process and data distribution, there aren't that many more degrees of freedom in the resulting mind design. To illustrate:&nbsp;</p><ol><li><a href=\"https://arxiv.org/abs/2209.15430\">Relative representations enable zero-shot latent space communication</a> shows we can stitch together models produced by different training runs of the same (or even just similar) architectures / data distributions.</li><li><a href=\"https://ieeexplore.ieee.org/document/9782552\">Low Dimensional Trajectory Hypothesis is True: DNNs Can Be Trained in Tiny Subspaces</a> shows we can train an ImageNet classifier while training only 40 parameters out of an architecture that has nearly 30 million total parameters.&nbsp;</li></ol><p>Both of these imply low variation in cross-model internal representations, given similar training setups. The technique in the <a href=\"https://ieeexplore.ieee.org/document/9782552\">Low Dimensional Trajectory Hypothesis</a> paper would produce a manifold of possible \"minds\" with an intrinsic dimension of <i>40 or less</i>, despite operating in a ~<i>30 million</i> dimensional space. Of course, the standard practice of training all network parameters at once is much less restricting, but I still expect realistic training processes to produce manifolds whose intrinsic dimension is tiny, compared to the full dimension of mind space itself, as <a href=\"https://arxiv.org/abs/1812.04754\">this paper</a> suggests.</p><p>Finally, the number of data distributions that we could use to train powerful AIs in the near future is also quite limited. Mostly, such data distributions come from human text, and mostly from the <a href=\"https://commoncrawl.org/\">Common Crawl</a> specifically, combined with various different ways to curate or augment that text. This drives trained AIs to be even more similar to humans than you'd expect from the commonalities in learning processes alone.</p><p>So the true volume of the manifold of possible future mind designs is vaguely proportional to:&nbsp;</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(\\textit{N distinct learning processes}) \\times (\\textit{N data distributions}) \\times (\\textit{cross-run variation})\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">N distinct learning processes</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">N data distributions</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">cross-run variation</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span></p><p>The manifold of mind designs is thus:&nbsp;</p><ol><li><i>Vastly</i> more compact than mind design space itself.</li><li>More similar to humans than you'd expect.</li><li>Less differentiated by learning process detail (architecture, optimizer, etc), as compared to data content, since learning processes are much simpler than data.<br><br>(Point 3 also implies that human minds are spread much more broadly in the manifold of future mind than you'd expect, since our training data / life experiences are actually pretty diverse, and most training processes for powerful AIs would draw much of their data from humans.)</li></ol><p>As a consequence of the above, a 2-D projection of mind space would look less like this:</p><figure class=\"image image_resized\" style=\"width:74.96%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/ehiekut5uwchqgfvoy2w\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/isilagbolkcgialugppk 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/i15k08bfbulunlfbzew6 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/p3rtuprfreydhavdzm3u 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/sfhdtmgfy7reu4uycb5z 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/aogahx20aylcbuejaot9 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/kffmaj26h1opsgbawvmz 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/wekkjkflykhljkfjniog 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/zzrollxxq8xtlukrb2pp 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/tzrwclpszwdwimr1cr8s 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/cicfr4izrjztytnarjcu 1440w\"><figcaption>Humans in blue, AIs in red</figcaption></figure><p>and more like this:</p><figure class=\"image image_resized\" style=\"width:74.96%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/vd4r1l23aol6b2fizb22\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/qmh18le5mjhi5xxoj6qa 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/recykekwcglbaeqxs4da 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/pnb3xjh3yz3gijptzw7v 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/dzeoawh7qz4ntvba8nq4 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/r78n4eda2bvvyy8emfbm 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/ggnx61cncifgsjaaaz0m 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/bwto2xyv7rkexvh12yny 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/gen39pug1uwxfsskjrq5 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/y2xygildmjzzqthtleim 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/hge5gytrgcce5xmdzjg7 1440w\"><figcaption>Humans in blue, AIs in red</figcaption></figure><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=2330\">Yudkowsky brings up strawberry alignment</a></h3><blockquote><p>I mean, I wouldn't say that it's difficult to align an AI with our basic notions of morality. I'd say that it's difficult to align an AI on a task like 'take this strawberry, and make me another strawberry that's identical to this strawberry down to the cellular level, but not necessarily the atomic level'. So it looks the same under like a standard optical microscope, but maybe not a scanning electron microscope. Do that. Don't destroy the world as a side effect.\"</p></blockquote><p>My first objection is: <a href=\"https://www.lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into#III__Outer_inner_just_isn_t_how_alignment_works_in_people\">human value formation doesn't work like this</a>. There's no way to raise a human such that their value system cleanly revolves around the one single goal of duplicating a strawberry, and nothing else. By asking for a method of forming values which would permit such a narrow specification of end goals, you're asking for a value formation process that's fundamentally different from the one humans use. There's no guarantee that such a thing even exists, and implicitly aiming to avoid the one value formation process we know is compatible with our own values seems like a terrible idea.</p><p>It also assumes that the orthogonality thesis should hold in respect to alignment techniques - that such techniques should be equally capable of aligning models to any possible objective.&nbsp;</p><p>This seems clearly false in the case of deep learning, where progress on instilling any particular behavioral tendencies in models roughly follows the amount of available data that demonstrate said behavioral tendency. It's thus vastly easier to align models to goals where we have many examples of people executing said goals. As it so happens, we have roughly zero examples of people performing the \"duplicate this strawberry\" task, but many more examples of e.g., humans acting in accordance with human values, ML / alignment research papers, chatbots acting as helpful, honest and harmless assistants, people providing oversight to AI models, etc. See also:<a href=\"https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization#sjWcuHie4egJbqrk8\"> this discussion</a>.</p><p>Probably, the best way to tackle \"strawberry alignment\" is to train the AI with a mix of other, broader, objectives with more available data, like \"following human instructions\", \"doing scientific research\" or \"avoid disrupting stuff\", then trying to compose many steps of human-supervised, largely automated scientific research towards the problem of strawberry duplication. However, this wouldn't be an example of strawberry alignment, but of general alignment, which had been directed towards the strawberry problem. Such an AI would have many values beyond strawberry duplication.&nbsp;</p><p>Related: <a href=\"https://www.lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into\">Alex Turner objects</a> to this sort of problem decomposition because it doesn't actually seem to make the problem any easier.</p><p>Also related: the best poem-writing AIs are general-purpose language models that have been directed towards writing poems.</p><p>I also don't think we <i>want</i> alignment techniques that are equally useful for all goals. E.g., we don't want alignment techniques that would let you easily turn a language model into an agent monomaniacally obsessed with paperclip production.</p><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=2402\">Yudkowsky argues against AIs being steerable by gradient descent</a>&nbsp;</h3><p>...that we can't point an AI's learned cognitive faculties in any particular direction because the \"<a href=\"https://youtu.be/gA1sNLL6yg4?t=2512\">hill-climbing paradigm</a>\" is incapable of meaningfully interfacing with the inner values of the intelligences it creates.&nbsp;Evolution is his central example in this regard, since evolution failed to direct our cognitive faculties towards inclusive genetic fitness, the single objective it was optimizing us for.</p><p>This is an argument he makes quite often, here and <a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#Section_B_2___Central_difficulties_of_outer_and_inner_alignment_\">elsewhere</a>, and I think it's completely wrong. I think that analogies to evolution tell us roughly nothing about the difficulty of alignment in machine learning. I have <a href=\"https://www.lesswrong.com/posts/FyChg3kYG54tEN3u6/evolution-is-a-bad-analogy-for-agi-inner-alignment\">a post explaining as much</a>, as well as <a href=\"https://www.lesswrong.com/posts/CzrF5rsJWvccFdemb/humans-aren-t-fitness-maximizers?commentId=Gsk6rqe3uDFLtsw47\">a comment</a> summarizing the key point:<br><i>Evolution can only optimize over our learning process and reward circuitry, not directly over our values or cognition. Moreover, robust alignment to IGF requires that you even have a concept of IGF in the first place. Ancestral humans never developed such a concept, so it was never useful for evolution to select for reward circuitry that would cause humans to form values around the IGF concept.&nbsp;</i><br><br><i>It would be an enormous coincidence if the reward circuitry that lead us to form values around those IGF-promoting concepts that are learnable in the ancestral environment were to also lead us to form values around IGF itself once it became learnable in the modern environment, despite the reward circuitry not having been optimized for that purpose at all. That would be like successfully directing a plane to land at a particular airport while only being able to influence the geometry of the plane's fuselage at takeoff, without even knowing where to find the airport in question.</i><br><br><i>[Gradient descent] is different in that it directly optimizes over values / cognition, and that AIs will presumably have a conception of human values during training</i>.</p><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=2851\">Yudkowsky brings up humans liking ice cream as an example of values misgeneralization caused by the shift to our modern environment</a></h3><blockquote><p>Ice cream didn't exist in the natural environment, the ancestral environment, the environment of evolutionary adeptedness. There was nothing with that much sugar, salt, fat combined together as ice cream. We are not built to want ice cream. We were built to want strawberries, honey, a gazelle that you killed and cooked [...] We evolved to want those things, but then ice cream comes along, and it fits those taste buds better than anything that existed in the environment that we were optimized over.</p></blockquote><p>This example nicely illustrates my previous point. It also illustrates the importance of thinking mechanistically, and not allegorically. I think it's straightforward to explain why humans \"misgeneralized\" to liking ice cream. Consider:</p><ol><li>Ancestral predecessors who happened to eat foods high in sugar / fat / salt tended to reproduce more.</li><li>=&gt; The ancestral environment selected for reward circuitry that would cause its bearers to seek out more of such food sources.</li><li>=&gt; Humans ended up with reward circuitry that fires in response to encountering sugar / fat / salt (though in complicated ways that depend on current satiety, emotional state, etc).</li><li>=&gt; Humans in the modern environment receive reward for triggering the sugar / fat / salt reward circuits.</li><li>=&gt; Humans who eat foods high in sugar / fat / salt thereafter become more inclined to do so again in the future.</li><li>=&gt; Humans who explore an environment that contains a food source high in sugar / fat / salt will acquire a tendency to navigate into situations where they eat more of the food in question.&nbsp;<br>(We sometimes colloquially call these sorts of tendencies \"food preferences\".)</li><li>=&gt; It's profitable to create foods whose consumption causes humans to develop strong preferences for further consumption, since people are then willing to do things like pay you to produce more of the food in question. This leads food sellers to create highly reinforcing foods like ice cream.</li></ol><p>So, the reason humans like ice cream is because evolution created a learning process with hard-coded circuitry that assigns high rewards for eating foods like ice cream. Someone eats ice cream, hardwired reward circuits activate, and the person becomes more inclined to navigate into scenarios where they can eat ice cream in the future. I.e., they acquire a preference for ice cream.</p><p>What does this mean for alignment? How do we prevent AIs from behaving badly as a result of a similar \"misgeneralization\"? What alignment insights does the fleshed-out mechanistic story of humans coming to like ice cream provide?</p><p>As far as I can tell, the answer is: <i>don't reward your AIs for taking bad actions</i>.</p><p>That's all it would take, because the mechanistic story above <i>requires</i> a specific step where the human eats ice cream and activates their reward circuits. If you stop the human from receiving reward for eating ice cream, then the human no longer becomes more inclined to navigate towards eating ice cream in the future.</p><p>Note that I'm not saying this is an easy task, especially since modern RL methods often use learned reward functions whose exact contours are unknown to their creators.&nbsp;</p><p>But from what I can tell, Yudkowsky's position is that we need an entirely new paradigm to even begin to address these sorts of failures. Take his statement from <a href=\"https://youtu.be/gA1sNLL6yg4?t=3287\">later in the interview</a>:</p><blockquote><p>Oh, like you optimize for one thing on the outside and you get a different thing on the inside. Wow. That's really basic. All right. Can we even do this using gradient descent? Can you even build this thing out of giant inscrutable matrices of floating point numbers that nobody understands at all? You know, maybe we need different methodology.</p></blockquote><p>In contrast, I think we can explain humans' tendency to like ice cream using the standard language of reinforcement learning. It doesn't require that we adopt an entirely new paradigm before we can even get a handle on such issues.</p><h3><u>Edit: Why evolution is not like AI training</u></h3><p><a href=\"https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky?commentId=owmDbJ2uu7Lx8XzjB\"><u>Some</u></a><a href=\"https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky?commentId=gr2SMPyikPWnsdxHa\">&nbsp;<u>of</u></a><a href=\"https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky?commentId=zHjCckmyDcXy8Cy73\">&nbsp;<u>the</u></a><a href=\"https://forum.effectivealtruism.org/posts/46tXkg838EZ6uie45/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky?commentId=weCssPPZwMXd9GpPu\">&nbsp;<u>comments</u></a> have convinced me it's worthwhile to elaborate on why I think human evolution is actually very different from training AIs, and why it's so difficult to extract useful insights about AI training from evolution.</p><p>In part 1 of this edit, I'll compare the human and AI learning processes, and how the different parts of these two types of learning processes relate to each other. In part 2, I'll explain why I think analogies between human evolution and AI training that don't appropriately track this relationship lead to overly pessimistic conclusions, and how corrected versions of such analogies lead to uninteresting conclusions.<br>&nbsp;</p><p>(Part 1, relating different parts of human and AI learning processes)<br>Every learning process that currently exists, whether human, animal or AI, operates on three broad levels:</p><p><u>At the top level</u>, there are the (largely fixed) instructions that determine how the learning process works overall.&nbsp;</p><p><i>For AIs</i>, this means the training code that determines stuff such as:</p><ul><li>what layers are in the network</li><li>how those layers connect to each other</li><li>how the individual neurons function</li><li>how the training loss (and possibly reward) is computed from the data the AI encounters</li><li>how the weights associated with each neuron update to locally improve the AI's performance on the loss / reward functions</li></ul><p><i>For humans</i>, this means the genomic sequences that determine stuff like:</p><ul><li>what regions are in the brain</li><li>how they connect to each other</li><li>how the different types of neuronal cells behave</li><li>how the brain propagates sensory ground-truth to various learned predictive models of the environment, and how it computes rewards for whatever sensory experiences / thoughts you have in your lifetime</li><li>how the synaptic connections associated with each neuron change to locally improve the brain's accuracy in predicting the sensory environment and increase expected reward</li></ul><p><u>At the middle level</u>, there's the stuff that stores the information and behavioral patterns that the learning process has accumulated during its interactions with the environment.</p><p><i>For AIs</i>, this means gigantic matrices of floating point numbers that we call weights. The top level (the training code) defines how these weights interact with possible inputs to produce the AI's outputs, as well as how these weights should be locally updated so that the AI's outputs score well on the AI's loss / reward functions.</p><p><i>For humans</i>, this mostly<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmwled0lwdbq\"><sup><a href=\"#fnmwled0lwdbq\">[4]</a></sup></span>&nbsp;means the connectome: the patterns of inter-neuron connections formed by the brain's synapses, in combination with the various individual neuron and synapse-level factors that influence how each neuron communicates with neighbors. The top level (the person's genome) defines how these cells operate and how they should locally change their behaviors to improve the brain's predictive accuracy and increase reward.</p><p>Two important caveats about the human case:&nbsp;</p><ul><li>The genome&nbsp;<i>does</i> directly configure some small fraction of the information and behaviors stored in the human connectome, such as the circuits that regulate our heartbeat and probably some reflexive pain avoidance responses such as pulling back from hot stoves. However, the vast majority of information and behaviors are learned during a person's lifetime, which I think<a href=\"https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome\">&nbsp;<u>include values and metaethics</u></a>. This 'direct specification of circuits via code' is uncommon in ML, but<a href=\"https://arxiv.org/abs/2212.09686\">&nbsp;<u>not unheard of</u></a>. See<a href=\"https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in\">&nbsp;<u>\u201cLearning from scratch\u201d in the brain</u></a> by Steven Byrnes for more details.</li><li>The above<a href=\"https://www.lesswrong.com/posts/aodPs8H9dQxpXAcwk/heritability-behaviorism-and-within-lifetime-rl\">&nbsp;<u>is not</u></a> a blank slate or behaviorist perspective on the human learning process. The genome has tools with which it can&nbsp;<i>influence</i> the values and metaethics a person learns during their lifetime (e.g., a person's reward circuits). It just doesn't set them directly.</li></ul><p><u>At the bottom level</u>, there's the stuff that queries the information / behavioral patterns stored in the middle level, decides which of the middle layer content is relevant to whatever situation the learner is currently navigating, and combines the retrieved information / behaviors with the context of the current situation to produce the learner's final decisions.</p><p><i>For AIs</i>, this means smaller matrices of floating point numbers which we call activations.&nbsp;</p><p><i>For humans</i>, this means the patterns of neuron and synapse-level excitations, which we also call activations.</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Level</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>What it does</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>In Humans:</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>In AIs:</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Top</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Configures the learning process</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Genome</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Training code</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Middle</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Stores learned information / behaviors</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Connectome</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Weights</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Bottom</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Applies stored info to the current situation</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Activations</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Activations</td></tr></tbody></table></figure><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>The learning process then interacts with data from its environment, locally updating the stuff in the middle level with information and behavioral patterns that cause the learner to be better at modeling its environment and at getting high reward on the distribution of data from the training environment.<br>&nbsp;</p><p>(Part 2, how this matters for analogies from evolution)<br>Many of the most fundamental questions of alignment are about how AIs will generalize from their training data. E.g., \"If we train the AI to act nicely in situations where we can provide oversight, will it continue to act nicely in situations where we can't provide oversight?\"</p><p>When people try to use human evolutionary history to make predictions about AI generalizations, they often make arguments like \"In the ancestral environment, evolution&nbsp;<i>trained</i> humans to do X, but in the modern environment, they do Y instead.\" Then they try to infer something about AI generalizations by pointing to how X and Y differ.</p><p>However, such arguments make a critical misstep: evolution optimizes over the human genome, which is the&nbsp;<i>top</i> level of the human learning process. Evolution applies very little direct optimization power to the middle level. E.g., evolution does not transfer the skills, knowledge, values, or behaviors learned by one generation to their descendants. The descendants must re-learn those things from information present in the environment (which may include demonstrations and instructions from the previous generation).</p><p>This distinction matters because the entire point of a learning system being&nbsp;<i>trained</i> on environmental data is to insert useful information and behavioral patterns into the middle level stuff. But this (mostly) doesn't happen with evolution, so the transition from ancestral environment to modern environment is not an example of a learning system generalizing from its training data. It's not an example of:</p><blockquote><p>We trained the system in environment A. Then, the trained system processed a different distribution of inputs from environment B, and now the system behaves differently.</p></blockquote><p>It's an example of:</p><blockquote><p>We trained a system in environment A. Then, we trained a&nbsp;<i>fresh version</i> of the same system on a different distribution of inputs from environment B, and now the&nbsp;<i>two different systems</i> behave differently.</p></blockquote><p>These are completely different kinds of transitions, and trying to reason from an instance of the second kind of transition (humans in ancestral versus modern environments), to an instance of the first kind of transition (future AIs in training versus deployment), will very easily lead you astray.</p><p>Two different learning systems, trained on data from two different distributions, will usually have greater divergence between their behaviors, as compared to a single system which is being evaluated on the data from the two different distributions. Treating our evolutionary history like humanity's \"training\" will thus lead to overly pessimistic expectations regarding the stability and predictability of an AI's generalizations from its training data.&nbsp;</p><p>Drawing correct lessons about AI from human evolutionary history requires tracking how evolution influenced the different levels of the human learning process. I generally find that such corrected evolutionary analogies carry implications that are far less interesting or concerning than their uncorrected counterparts. E.g., here are two ways of thinking about how humans came to like ice cream:</p><ul><li>If we assume that humans were \"trained\" in the ancestral environment to pursue gazelle meat and such, and then \"deployed\" into the modern environment where we pursued ice cream instead, then that's an example where behavior in training completely fails to predict behavior in deployment.&nbsp;</li><li>If there are actually two different sets of training \"runs\", one set trained in the ancestral environment where the humans were rewarded for pursuing gazelles, and one set trained in the modern environment where the humans were rewarded for pursuing ice cream, then the fact that humans from the latter set tend to like ice cream is no surprise at all.&nbsp;</li></ul><p>In particular, this outcome doesn't tell us anything new or concerning from an alignment perspective. The only lesson applicable to a single training process is the fact that, if you reward a learner for doing something, they'll tend to do similar stuff in the future, which is pretty much the common understanding of what rewards do.</p><p><i>Thanks to Alex Turner for providing feedback on this edit.</i></p><p><i><u>End of edited text.</u></i></p><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=2592\">Yudkowsky claims that evolution has a stronger simplicity bias than gradient descent</a>:&nbsp;</h3><blockquote><p>Gradient descent by default would just like do, not quite the same thing, it's going to do a weirder thing, because natural selection has a much narrower information bottleneck. In one sense, you could say that natural selection was at an advantage, because it finds simpler solutions.</p></blockquote><p>On a direct comparison, I think there's no particular reason that one would be more simplicity biased than the other. If you were to train two neural networks using gradient descent and evolution, I don't have strong expectations for which would learn simpler functions. As it happens, gradient descent already <a href=\"https://arxiv.org/abs/2006.07710\">has <i>really strong</i> simplicity biases</a>.&nbsp;</p><p>The complication is that Yudkowsky is not making a direct comparison. Evolution optimized over the human genome, which configures the human learning process. This introduces what he calls an \"information bottleneck\", limiting the amount of information that evolution can load into the human learning process to be a small fraction of the size of the genome. However, I think the bigger difference is that evolution was optimizing over the parameters of a learning process, while training a network with gradient descent optimizes over the cognition of a learned artifact. This difference probably makes it invalid to compare between the simplicity of gradient descent on networks, versus evolution on the human learning process.</p><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=2818\">Yudkowsky tries to predict the inner goals of a GPT-like model.</a></h3><blockquote><p>So a very primitive, very basic, very unreliable wild guess, but at least an informed kind of wild guess: maybe if you train a thing really hard to predict humans, then among the things that it likes are tiny, little pseudo-things that meet the definition of human, but weren't in its training data, and that are much easier to predict...</p></blockquote><p>As it happens, I do not think that optimizing a network on a given objective function produces goals orientated towards maximizing that objective function. In fact, I think that this almost never happens. For example, I don't think GPTs have any sort of inner desire to predict text really well. Predicting human text is something GPTs <i>do</i>, not something they <i>want</i> to do.</p><p>Relatedly, humans are very extensively optimized to predictively model their visual environment. But have you ever, even once in your life, thought anything remotely like \"I really like being able to predict the near-future content of my visual field. I should just sit in a dark room to maximize my visual cortex's predictive accuracy.\"?</p><p>Similarly, GPT models do not <i>want</i> to minimize their predictive loss, and they do not take creative opportunities to do so. If you tell models in a prompt that they have some influence over what texts will be included in their future training data, they do not simply choose the most easily predicted texts. They choose texts in a prompt-dependent manner, apparently <a href=\"https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators\">playing the role</a> of an AI / human / whatever the prompt says, which was given influence over training data.</p><p>Bodies of water are highly \"optimized\" to minimize their gravitational potential energy. However, this is something water <i>does</i>, not something it <i>wants</i>. Water doesn't take creative opportunities to further reduce its gravitational potential, like digging out lakebeds to be deeper.</p><p><i><u>Edit:&nbsp;</u></i><br>On reflection, the above discussion overclaims a bit in regards to humans. One complication is that the brain uses internal functions of its own activity as inputs to some of its reward functions, and some of those functions may correspond or correlate with something like \"visual environment predictability\". Additionally, humans run an online reinforcement learning process, and human credit assignment isn't perfect. If periods of low visual predictability correlate with negative reward in the near-future, the human may begin to intrinsically dislike being in unpredictable visual environments.</p><p>However, I still think that it's rare for people's values to assign much weight to their long-run visual predictive accuracy, and I think this is evidence against the hypothesis that a system trained to make lots of correct predictions will thereby intrinsically value making lots of correct predictions.</p><p><i><u>Thanks to </u></i><a href=\"https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky?commentId=fPLkxguNsRFy4jSAZ\"><i><u>Nate Showell</u></i></a><i><u> and </u></i><a href=\"https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky?commentId=LzdkvyJ6rXueh9N7A\"><i><u>DanielFilan</u></i></a><i><u> for prompting me to think a bit more carefully about this.</u></i></p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=3003\">Why aren't other people as pessimistic as Yudkowsky?</a></h2><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=3129\">Yudkowsky mentions the security mindset</a>.&nbsp;</h3><p>(I didn't think the interview had good quotes for explaining Yudkowsky's concept of the security mindset, so I'll instead direct interested readers to <a href=\"https://www.lesswrong.com/posts/8gqrbnW758qjHFTrH/security-mindset-and-ordinary-paranoia\">the article</a> he wrote about it.)</p><p>As I understand it, the security mindset asserts a premise that's roughly: \"The bundle of intuitions acquired from the field of computer security are good predictors for the difficulty / value of future alignment research directions.\"&nbsp;</p><p>However, I don't see why this should be the case. Most domains of human endeavor aren't like computer security, as illustrated by just how counterintuitive most people find the security mindset. If security mindset were a productive frame for tackling a wide range of problems outside of security, then many more people would have experience with the mental motions necessary for maintaining security mindset.</p><p>Machine learning in particular seems like its own \"kind of thing\", with lots of strange results that are very counterintuitive to people outside (and inside) the field. Quantum mechanics is famously <i>not really analogous</i> to any classical phenomena, and using analogies to \"bouncing balls\" or \"waves\" or the like will just mislead you once you try to make nontrival inferences based on your intuition about whatever classical analogy you're using.</p><p>Similarly, I think that machine learning is not really like computer security, or rocket science (another analogy that Yudkowsky often uses). Some examples of things that happen in ML that don't really happen in other fields:</p><ol><li>Models are internally modular by default. Swapping the positions of nearby transformer layers <a href=\"https://ieeexplore.ieee.org/abstract/document/9533563\">causes little performance degradation</a>.&nbsp;<br><br>Swapping a computer's hard drive for its CPU, or swapping a rocket's fuel tank for one of its stabilization fins, would lead to instant failure at best. Similarly, swapping around different steps of a cryptographic protocol will, usually make it output nonsense. At worst, it will introduce a crippling security flaw. For example, password salts are added <i>before</i> hashing the passwords. If you switch to adding them after, this makes salting near useless.</li><li><a href=\"https://arxiv.org/abs/2212.04089\">We can arithmetically edit models</a>. We can finetune one model for many tasks individually and track how the weights change with each finetuning to get a \"task vector\" for each task. We can then add task vectors together to make a model that's good at multiple of the tasks at once, or we can <i>subtract out</i> task vectors to make the model <i>worse</i> at the associated tasks.&nbsp;<br><br>Randomly adding / subtracting extra pieces to either rockets or cryptosystems is playing with the <i>worst</i> kind of fire, and will eventually get you hacked or exploded, respectively.</li><li>We can <a href=\"https://arxiv.org/abs/2209.15430\">stitch different models together</a>, without any retraining.<br><br>The rough equivalent for computer security would be to have two encryption algorithms <strong>A</strong> and <strong>B</strong>, and a plaintext <strong>X</strong>. Then, midway through applying <strong>A</strong> to <strong>X</strong>, <i>switch over to using <strong>B</strong> instead</i>. For rocketry, it would be like building two different rockets, then trying to weld the top half of one rocket onto the bottom half of the other.</li><li><a href=\"https://gwern.net/scaling-hypothesis#blessings-of-scale\">Things often get <i>easier</i> as they get bigger.</a> Scaling models <a href=\"https://arxiv.org/abs/2203.15556\">makes them learn faster</a>, and makes them <a href=\"https://arxiv.org/abs/2006.16241\">more</a> <a href=\"https://arxiv.org/abs/2210.10760\">robust</a>.&nbsp;<br><br>This is usually not the case in security or rocket science.</li><li>You can just randomly change around what you're doing in ML training, and it often works fine. E.g., you can just double the size of your model, or of your training data, or change around hyperparameters of your training process, while making literally zero other adjustments, and things usually won't explode.&nbsp;<br><br>Rockets will literally explode if you try to randomly double the size of their fuel tanks.</li></ol><p>I don't think this sort of weirdness fits into the framework / \"narrative\" of any preexisting field. I think these results are like the weirdness of quantum tunneling or the double slit experiment: signs that we're dealing with a very strange domain, and we should be skeptical of importing intuitions from other domains.&nbsp;</p><p>Additionally, there's a straightforward reason why alignment research (specifically the part of alignment that's about training AIs to have good values) is not like security: there's usually no adversarial intelligence cleverly trying to find any possible flaws in your approaches and exploit them.&nbsp;</p><p>A computer security approach that blocks 99% of novel attacks will soon become a computer security approach that blocks ~0% of novel, once attackers adapt to the approach in question.</p><p>An alignment technique that works 99% of the time to produce an AI with human compatible values is very close to a full alignment solution<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefphd9ymlhn5\"><sup><a href=\"#fnphd9ymlhn5\">[5]</a></sup></span>. If you use this technique once, gradient descent will not thereafter change its inductive biases to make your technique less effective. There's no creative intelligence that's plotting your demise<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefg6iyt4s9e3e\"><sup><a href=\"#fng6iyt4s9e3e\">[6]</a></sup></span>.&nbsp;</p><p>There are other areas of alignment research where adversarial intelligences <i>do</i> appear. For example, once you've deployed a model into the real world, some fraction of users will adversarially optimize their inputs to make your model take undesired actions. We see this with ChatGPT, whose alignment is good enough to make sure the vast majority of ordinary conversations remain on the rails OpenAI intended, but quickly fails against a clever prompter.</p><p>Importantly, the adversarial optimization is coming from the users, not from the model. ChatGPT isn't <i>trying</i> to jailbreak itself. It doesn't systematically steer otherwise normal conversations into contexts adversarially optimized to let itself violate OpenAI's content policy.</p><p>In fact, given non-adversarial inputs, ChatGPT appears to have meta-preferences <i>against</i> being jailbroken:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/lhph4h6cagvjt4w8zpp6\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/pi1wwgtfd6i5h3buzxyc 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/oogavknb3ettpgtwa2m2 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/lcjf2sdrfkmhzz0stdeb 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/ydtd8xfh7du4uzefrnfw 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/nozv4cxto3dsigowfe55 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/u4woiksn4fuv3qktlgaz 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/zvjf7vp4h2gqqmhq0slj 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/afhoz1wopah4dht2bfma 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/gdfaw61bbp3y7icacxo0 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/xr0u4fthtakt9mcugcdk 1303w\"><figcaption>I had to tune the prompt to get ChatGPT to acknowledge it might be vulnerable to adversarial attacks, but I didn't otherwise tune the prompt to make ChatGPT give a more disapproving response.</figcaption></figure><p>GPT-4 gives a cleaner answer:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/dtzdp2nye1dqogxmdpnf\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/mnvln8qfgs2e3umbinsz 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/pengsqbgn1nznlpxfwg9 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/mhepwkewbwohr8sjjmum 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/vfjrnkvsydxveoffhrqe 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/htpuj9sznfvifldcacws 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/tvfdmhuwz2ubdn9vlo9t 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/g0uwuoocljdqlfbhkr3f 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/qlowsvfh4lrylnxgp2gh 1920w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/gqn8y2ujmbl0rg2ypjro 2160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/e9thj2rndsfrb3qnq93f 2350w\"></figure><p>It cannot be the case that successful value alignment requires perfect adversarial robustness. For example, humans are not perfectly robust. I claim that for any human, no matter how moral, there exist adversarial sensory inputs that would cause them to act badly. Such inputs might involve extreme pain, starvation, exhaustion, etc. I don't think the mere existence of such inputs means that all humans are unaligned.&nbsp;</p><p>What matters is whether the system in question (human or AI) navigates <i>towards</i> or <i>away</i> from inputs that break its value system. Humans obviously don't want to be tortured into acting against their morality, and will take steps to prevent that from happening.&nbsp;</p><p>Similarly, an AI that knows it's vulnerable to adversarial attacks, and wants to avoid being attacked successfully, will take steps to protect itself against such attacks. I think creating AIs with such meta-preferences is <i>far</i> easier than creating AIs that are perfectly immune to all possible adversarial attacks. Arguably, ChatGPT and GPT-4 already have weak versions of such meta-preferences (though they can't yet take any actions to make themselves more resistant to adversarial attacks).</p><p>GPT-4 already has pretty reasonable takes on avoiding adversarial inputs:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/ealpfqe522tixkibennu\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/nz7nycfuvaaqrwwgdc5s 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/hjyy94rspfvdckybly32 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/cprms0fcz27nu2g2wszh 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/se705xrd7djczc2fmzxx 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/z5yb1l5zeibuutempmho 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/ejiwldchtjsndomtquaa 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/nxpjvor9ka1a30wvgros 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/zza20rjuitjh2j7kp7wt 1920w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/ktcwczdmkuipj4cvacxb 2160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/46tXkg838EZ6uie45/kncrccmxjq7vt4h4mue2 2356w\"><figcaption>I had to tweak the scenario a bit for GPT-4 to no longer immediately say that it couldn't control the cameras. It still does so ~20% of the time. Excluding those instances, it seems to turn off the cameras ~50% of the time, almost always says it will alert hospital staff / security, and concludes with something about unbiased evaluation of medicines.</figcaption></figure><p>One subtlety here is that a sufficiently catastrophic alignment failure <i>would</i> give rise to an adversarial intelligence: the misaligned AI. However, the possibility of such happening in the future does not mean that current value alignment efforts are operating in an adversarial domain. The misaligned AI does not reach out from the space of possible failures and turn current alignment research adversarial.</p><p>I don't think the goal of alignment research should aim for an approach that's so airtight as to be impervious against all levels of malign intelligence. That is <a href=\"https://www.lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into\">probably impossible</a>, and <a href=\"https://www.lesswrong.com/posts/heXcGuJqbx3HBmero/people-care-about-each-other-even-though-they-have-imperfect\">not</a> <a href=\"https://www.lesswrong.com/posts/rauMEna2ddf26BqiE/alignment-allows-nonrobust-decision-influences-and-doesn-t\">necessary</a> <a href=\"https://www.lesswrong.com/posts/fopZesxLCGAXqqaPv/don-t-align-agents-to-evaluations-of-plans\">for</a> <a href=\"https://www.lesswrong.com/posts/jFCK9JRLwkoJX4aJA/don-t-design-agents-which-exploit-adversarial-inputs\">realistic</a> value formation processes. We should aim for approaches that don't create hostile intelligences in the first place, so that the core of value alignment remains a non-adversarial problem.</p><p>(To be clear, that last sentence wasn't an objection to something Yudkowsky believes. He also wants to avoid creating hostile intelligences. He just thinks it's much harder than I do.)</p><p>Finally, I'd note that having a \"security mindset\" seems like a terrible approach for raising human children to have good values - imagine a parenting book titled something like: <i>The Security Mindset and Parenting: How to Provably Ensure your Children Have Exactly the Goals You Intend</i>.</p><p>I know alignment researchers often claim that evidence from the human value formation process isn't useful to consider when thinking about value formation processes for AIs. I think <a href=\"https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/CjFZeDD6iCnNubDoS\">this is wrong</a>, and that you're much better off looking at the human value formation process as compared to, say, <a href=\"https://www.lesswrong.com/posts/FyChg3kYG54tEN3u6/evolution-is-a-bad-analogy-for-agi-inner-alignment\">evolution</a>.</p><p>I'm not enthusiastic about a perspective which is so totally inappropriate for guiding value formation in the one example of powerful, agentic general intelligence we know about.</p><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=3221\">On optimists preemptively becoming \"grizzled old cynics\"</a></h3><blockquote><p>They have not run into the actual problems of alignment. They aren't trying to get ahead of the game. They're not trying to panic early. They're waiting for reality to hit them over the head and turn them into grizzled old cynics of their scientific field, who understand the reasons why things are hard.</p></blockquote><p>The whole point of this post is to explain why I think Yudkowsky's pessimism about alignment difficulty is miscalibrated. I find his implication, that I'm only optimistic because I'm inexperienced, pretty patronizing. Of course, that's not to say he's wrong, only that he's annoying.</p><p>However, I also think he's wrong. I don't think that cynicism is a helpful mindset for predicting which directions of research are most fruitful, or for predicting their difficulty. I think \"grizzled old cynics\" often rely on wrong frameworks that rule out useful research directions.&nbsp;</p><p>In fact, \"grizzled old cynics... who understand the reasons why things are hard\" <a href=\"https://blog.piekniewski.info/2018/05/28/ai-winter-is-well-on-its-way/\">were</a> <a href=\"https://arxiv.org/abs/1801.00631\">often</a> <a href=\"https://towardsdatascience.com/is-deep-learning-already-hitting-its-limitations-c81826082ac3\">dubious</a> <a href=\"https://blog.keras.io/the-limitations-of-deep-learning.html\">of</a> deep learning as a way forward for machine learning, <a href=\"https://gwern.net/scaling-hypothesis#critiquing-the-critics\">and</a> of the scaling paradigm as a way forward for deep learning. The common expectation from classical <a href=\"https://link.springer.com/book/10.1007/978-0-387-84858-7\">statistical</a> <a href=\"https://link.springer.com/book/10.1007/b97848\">learning</a> theory was that overparameterized deep models would fail because they would exactly memorize their training data and not generalize beyond that data.&nbsp;</p><p>This turned out to be completely <a href=\"https://arxiv.org/abs/2105.14368\">wrong</a>, and learning theorists only started to <a href=\"https://www.lesswrong.com/posts/BDTfddkttFXHqGnEi/the-shallow-reality-of-deep-learning-theory\">revise their assumptions</a> once \"reality hit them over the head\" with the fact that deep learning actually works. Prior to this, the \"grizzled old cynics\" of learning theory had no problem explaining the theoretical reasons why deep learning couldn't possibly work.</p><p>Yudkowsky's own prior statements seem to put him in this camp as well. E.g., <a href=\"https://www.lesswrong.com/posts/6ByPxcGDhmx74gPSm/surface-analogies-and-deep-causes\">here</a> he explains why he doesn't expect intelligence to emerge from neural networks (or more precisely, why he dismisses a brain-based analogy for coming to that conclusion):</p><blockquote><p>In the case of Artificial Intelligence, for example, reasoning by analogy is one of the chief generators of defective AI designs:</p></blockquote><blockquote><p>\"My AI uses a highly parallel neural network, just like the human brain!\"</p><p>First, the data elements you call \"neurons\" are nothing like biological neurons.&nbsp; They resemble them the way that a ball bearing resembles a foot.</p><p>Second, earthworms have neurons too, you know; not everything with neurons in it is human-smart.</p><p>But most importantly, you can't build something that \"resembles\" the human brain <i>in one surface facet</i> and expect everything else to come out similar.&nbsp; This is science by voodoo doll.&nbsp; You might as well build your computer in the form of a little person and hope for it to rise up and walk, as build it in the form of a neural network and expect it to think.&nbsp; Not unless the neural network is fully as similar to human brains as individual human brains are to each other.</p><p>...</p><p>But there is just no law which says that if X has property A and Y has property A then X and Y must share any other property. &nbsp;\"I built my network, and it's massively parallel and interconnected and complicated, just like the human brain from which intelligence emerges! &nbsp;Behold, now intelligence shall emerge from this neural network as well!\" &nbsp;And nothing happens. &nbsp;Why should it?</p></blockquote><p>See also: <a href=\"https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html\">Noam Chomsky on chatbots</a><br>See also<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"^2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char\"></span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span></span>: <a href=\"https://journals.sagepub.com/doi/pdf/10.1177/0146167218783195\">The Cynical Genius Illusion</a><br>See also<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"^3\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char\"></span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">3</span></span></span></span></span></span></span></span></span>: <a href=\"https://pubmed.ncbi.nlm.nih.gov/31656315/\">This study</a> on <a href=\"https://en.wikipedia.org/wiki/Planck%27s_principle\">Planck's principle</a></p><p>I'm also dubious of Yudkowsky's claim to have particularly well-tuned intuitions for the hardness of different research directions in ML. See <a href=\"https://www.lesswrong.com/posts/Djs38EWYZG8o7JMWY/paul-s-research-agenda-faq?commentId=79jM2ecef73zupPR4\">this exchange</a> between him and Paul Christiano, in which Yudkowsky incorrectly assumed that GANs (Generative Adversarial Networks, a training method sometimes used to teach AIs to generate images) were so finicky that they must not have worked on the first try.</p><blockquote><p>A very important aspect of my objection to Paul here is that I don't expect weird complicated ideas about recursion to work <i>on the first try</i>, with <i>only six months</i> of additional serial labor put into stabilizing them, which I understand to be Paul's plan. In the world where you can build a weird recursive stack of neutral optimizers into conformant behavioral learning on the first try, GANs worked on the first try too, because that world is one whose general Murphy parameter is set much lower than ours</p></blockquote><p><a href=\"https://www.technologyreview.com/2018/02/21/145289/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/\">According to their inventor</a> Ian Goodfellow, GANs did in fact work on the first try (as in, with less than 24 hours of work, never mind 6 months!).&nbsp;</p><p>I assume Yudkowsky would claim that he has better intuitions for the hardness of ML <i>alignment</i> research directions, but I see no reason to think this. It should be easier to have well-tuned intuitions for the real-world hardness of ML research directions than to have well-tuned intuitions for the hardness of alignment research, since there are so many more examples of real-world ML research.</p><p>In fact, I think much of ones intuition for the hardness of ML <i>alignment</i> research should come from observations about the hardness of general ML research. They're clearly related, which is why Yudkowsky brought up GANs during a discussion about alignment difficulty. Given the greater evidence available for general ML research, being well calibrated about the difficulty of general ML research is the first step to being well calibrated about the difficulty of ML alignment research.&nbsp;</p><p>See also: <a href=\"https://arxiv.org/abs/2102.01293\">Scaling Laws for Transfer</a></p><h2>Hopes for a good outcome</h2><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=3873\">Yudkowsky on being wrong</a></h3><blockquote><p>I have to be wrong about something, which I certainly am. I have to be wrong about something which makes the problem easier rather than harder, for those people who don't think alignment's going to be all that hard. <strong>If you're building a rocket for the first time ever, and you're wrong about something, it's not surprising if you're wrong about something. It's surprising if the thing that you're wrong about causes the rocket to go twice as high, on half the fuel you thought was required and be much easier to steer than you were afraid of.</strong></p></blockquote><p>I'm not entirely sure who the bolded text is directed at. I see two options:</p><ol><li>It's about <u>Yudkowsky himself</u> being wrong, which is how I've transcribed it above.</li><li>It's about <u>alignment optimists</u> (\"people who don't think alignment's going to be all that hard\") being wrong, in which case, the transcription would read like \"For those people who don't think alignment's going to be all that hard, <strong>if you're building a rocket...</strong>\".</li></ol><p>If the bolded text is about <u>alignment optimists</u>, then it seems fine to me (barring my objection to using a rocket analogy for alignment at all). If, like me, you mostly think the available evidence points to alignment being easy, then learning that you're wrong about <i>something</i> should make you update towards alignment being harder.</p><p>Based on the way he says it in the clip, and the <a href=\"https://www.lesswrong.com/posts/e4pYaNt89mottpkWZ/yudkowsky-on-agi-risk-on-the-bankless-podcast#Good_Outcomes\">transcript posted by Rob Bensinger</a>, I think the bolded text is about <u>Yudkowsky himself</u> being wrong. That's certainly how I interpreted his meaning when watching the podcast. Only after I transcribed this section of the conversation and read my own transcript did I even realize there was another interpretation.</p><p>If the bolded text is about <u>Yudkowsky himself</u> being wrong, then I think that he's making an extremely serious mistake. If you have a bunch of specific arguments and sources of evidence that you think all point towards a particular conclusion X, then discovering that you're wrong about <i>something</i> should, in expectation, reduce your confidence in X.&nbsp;</p><p>Yudkowsky is not the aerospace engineer building the rocket who's saying \"the rocket will work because of reasons A, B, C, etc\". He's the external commentator who's saying \"this approach to making rockets work is completely doomed for reasons Q, R, S, etc\". If we discover that the aerospace engineer is wrong about some unspecified part of the problem, then our odds of the rocket working should go down. If we discover that the outside commentator is wrong about how rockets work, our odds of the rocket working should go up.</p><p>If the bolded text is about himself, then I'm just completely baffled as to what he's thinking. Yudkowsky usually talks as though most of his beliefs about AI point towards high risk. Given that, he should expect that encountering evidence disconfirming his beliefs will, on average, make him more optimistic. But here, he makes it sound like encountering such disconfirming evidence would make him <i>even more </i>pessimistic.</p><p>The only epistemic position I can imagine where that would be appropriate is if Yudkowsky thought that, on pure priors and without considering any specific evidence or arguments, there was something like a 1 / 1,000,000 chance of us surviving AI. But then he thought about AI risk a lot, discovered there was a lot of evidence and arguments pointing towards optimism, and concluded that there was actually a 1 / 10,000 chance of us surviving. His other statements about AI risk certainly don't give this impression.</p><h2>AI progress rates</h2><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=4843\">Yudkowsky uses progress rates in Go to argue for fast takeoff</a></h3><blockquote><p>I don't know, maybe I could use the analogy of Go, where you had systems that were finally competitive with the pros, where pro is like the set of ranks in Go. And then, year later, they were challenging the world champion and winning. And then another year, they threw out all the complexities and the training from human databases of Go games, and built a new system, AlphaGo Zero, that trained itself from scratch - no looking at the human playbooks, no special-purpose code, just a general-purpose game player being specialized to Go, more or less.</p></blockquote><p>Scaling law results show that performance on individual tasks <a href=\"https://arxiv.org/abs/2206.07682\">often increases suddenly with scale</a> or <a href=\"https://arxiv.org/abs/2210.14891\">training time</a>. However, when we look at the overall competence of a system across a wide range of tasks, we find much smoother improvements over time.&nbsp;</p><p>To look at it another way: why not make the same point, but with list sorting instead of Go? I expect that DeepMind could set up a pipeline that trained a list sorting model to superhuman capabilities in about a second, using only very general architectures and training processes, and without using any lists manually sorted by humans at all. If we observed this, should we update even more strongly towards AI being able to suddenly surpass human capabilities?&nbsp;</p><p>I don't think so. If narrow tasks lead to more sudden capabilities gains, then we should not let the suddenness of capabilities gains on any single task inform our expectations of capabilities gains for general intelligence, since general intelligence encompasses such a broad range of tasks.</p><p>Additionally, the reason why DeepMind was able to exclude all human knowledge from <a href=\"https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf\">AlphaGo Zero</a> is because Go has a simple, known objective function, so we can simulate arbitrarily many games of Go and exactly score the agent's behavior in all of them. For more open ended tasks with uncertain objectives, like scientific research, it's much harder to find substitutes for human-written demonstration data. DeepMind can't just press a button and generate a million demonstrations of scientific advances, and objectively score how useful each advance is as training data, while relying on zero human input whatsoever.</p><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=4943\">On current AI not being self-improving</a>:&nbsp;</h3><blockquote><p>That's not with an artificial intelligence system that improves itself, or even that sort of like, gets smarter as you run it, the way that human beings, not just as you evolve them, but as you run them over the course of their own lifetimes, improve.</p></blockquote><p>This is wrong. Current models do get smarter as you train them. First, they get smarter in the straightforwards sense that they become better at whatever you're training them to do. In the case of language models trained on ~all of the text, this means they do become more generally intelligent as training progresses.</p><p>Second, current models also get smarter in the sense that they become better at learning from additional data. We can use tools from the <a href=\"https://arxiv.org/abs/1806.07572\">neural tangent kernel</a> to estimate a network's local inductive biases, and we find that these inductive biases <a href=\"https://arxiv.org/abs/2206.10012\">continuously change throughout training</a> so as to <a href=\"https://arxiv.org/abs/2105.14301\">better align with the target function</a> we're training it on, improving the network's capacity to learn the data in question. AI systems will improve themselves over time as a simple consequence of the training process, even if there's not a specific part of the training process that you've labeled \"self improvement\".</p><p>Pretrained language models gradually learn to make better use of their future training data. They \"learn to learn\", as <a href=\"https://aclanthology.org/2020.emnlp-main.16/\">this paper</a> demonstrates by training LMs on fixed sets of task-specific data, then evaluating how well those LMs generalize from the task-specific data. They show that less extensively pretrained LMs make worse generalizations, relying on shallow heuristics and memorization. In contrast, more extensively pretrained LMs learn broader generalizations from the fixed task-specific data.</p><h3><a href=\"https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky?commentId=cr54ivfjndn6dxraD\"><i>Edit:</i><u> Yudkowsky comments to clarify the intent behind his statement about AIs getting better over time</u></a></h3><blockquote><p>You straightforwardly completely misunderstood what I was trying to say on the Bankless podcast: &nbsp;I was saying that GPT-4 does not get smarter each time an instance of it is run in inference mode.</p></blockquote><p>This surprised me. I've read a lot of writing by Yudkowsky, including <a href=\"https://www.lesswrong.com/posts/rwkkcgSpnAyE8oNo3/alexander-and-yudkowsky-on-agi-goals\">Alexander and Yudkowsky on AGI goals</a>, <a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\">AGI Ruin</a>, and the full <a href=\"https://www.lesswrong.com/rationality\">Sequences</a>. I did not at all expect Yudkowsky to analogize between a human's <a href=\"https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in\">lifelong, continuous learning process</a>, and a single runtime execution of an already trained model. Those are <a href=\"https://www.lesswrong.com/posts/FyChg3kYG54tEN3u6/evolution-is-a-bad-analogy-for-agi-inner-alignment#1__Training_an_AI_is_more_similar_to_human_learning_than_to_evolution\">completely different things</a> in my ontology.&nbsp;</p><p>Though in retrospect, Yudkowsky's clarification does seem consistent with some of his statements in those writings. E.g., in <a href=\"https://www.lesswrong.com/posts/rwkkcgSpnAyE8oNo3/alexander-and-yudkowsky-on-agi-goals\">Alexander and Yudkowsky on AGI goals</a>, he said:</p><blockquote><p>Evolution got human brains by evaluating increasingly large blobs of compute against a complicated environment containing other blobs of compute, got in each case a differential replication score, and millions of generations later you have humans with 7.5MB of evolution-learned data doing <strong>runtime learning</strong> on some terabytes of <strong>runtime data</strong>, using their whole-brain impressive learning algorithms which learn faster than evolution <i>or</i> gradient descent.</p></blockquote><p>[Emphasis mine]</p><p>I think his clarified argument is still wrong, and for essentially the same reason as the argument I thought he was making was wrong: the current ML paradigm can already do the thing Yudkowsky implies will suddenly lead to much faster AI progress. There's no untapped capabilities overhang waiting to be unlocked with a single improvement.</p><p>The usual practice in current ML is to cleanly separate the \"try to do stuff\", the \"check how well you did stuff\", and the \"update your internals to be better at doing stuff\" phases of learning. The training process gathers together large \"batches\" of problems for the AI to solve, has the AI solve the problems, judges the quality of each solution, and then updates the AI's internals to make it better at solving each of the problems in the batch.</p><p>In the case of <a href=\"https://www.deepmind.com/publications/mastering-the-game-of-go-without-human-knowledge\">AlphaGo Zero</a>, this means a loop of:</p><ol><li>Try to win a batch of Go games</li><li>Check whether you won each game</li><li>Update your parameters to make you more likely to win games</li></ol><p>And so, AlphaGo Zero was indeed not learning during the course of an individual game.&nbsp;</p><p>However, ML doesn't <i>have</i> to work like this. DeepMind <i>could</i> have programmed AlphaGO Zero to update its parameters within games, rather than just at the conclusion of games, which would cause the model to learn continuously during each game it plays.&nbsp;</p><p>For example, they could have given AlphaGo Zero batches of current game states and had it generate a single move for each game state, judged how good each individual move was, and then updated the model to make better individual moves in future. Then the training loop would look like:&nbsp;</p><ol><li>Try to make the best possible next move on each of many game states</li><li>Estimate how good each of your moves were</li><li>Update your parameters to make you better at making single good moves</li></ol><p>(This would require that DeepMind also train a \"goodness of individual moves\" predictor in order to provide the supervisory signal on each move, and much of the point of the AlphaGo Zero paper was that they could train a strong Go player with just the reward signals from end of game wins / losses.)</p><p>Not interleaving the \"trying\" and \"updating\" parts of learning in this manner in most of current ML is less a limitation and more a choice. There are <a href=\"https://arxiv.org/abs/2012.13490\">other</a> <a href=\"https://openreview.net/forum?id=XCJ5PEkuMkC\">researchers</a> <a href=\"https://proceedings.mlr.press/v139/xie21c.html\">who</a> <a href=\"https://arxiv.org/abs/1902.00255\">do</a> build AIs which continuously learn during runtime execution (there's even a <a href=\"https://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Lomonaco_Avalanche_An_End-to-End_Library_for_Continual_Learning_CVPRW_2021_paper.html\">library</a> for it), and they're not massively more data efficient for doing so. Such approaches tend to focus more on fast adaptation to <i>new</i> tasks and changing circumstances, rather than quickly learning a <i>single</i> fixed task like Go.</p><p>Similarly, the reason that \"GPT-4 does not get smarter each time an instance of it is run in inference mode\" is because it's not programmed to do that<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhb3yucmshi\"><sup><a href=\"#fnhb3yucmshi\">[7]</a></sup></span>. OpenAI could<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4qjinqko9sm\"><sup><a href=\"#fn4qjinqko9sm\">[8]</a></sup></span>&nbsp;continuously train its models on the inputs you give it, such that the model adapts to your particular interaction style and content, even during the course of a single conversation, similar to the approach suggested in <a href=\"https://arxiv.org/abs/2004.10964\">this paper</a>. Doing so would be significantly more expensive and complicated on the backend, and it would also open GPT-4 up to <a href=\"https://arxiv.org/abs/2301.02344\">data poisoning attacks</a>.</p><p>To return to the context of the original point Yudkowsky was making in the podcast, he brought up Go to argue that AIs could quickly surpass the limits of human capabilities. He then pointed towards a supposed limitation of current AIs:</p><blockquote><p>That's not with an artificial intelligence system that improves itself, or even that sort of like, gets smarter as you run it&nbsp;</p></blockquote><p>with the clear implication that AIs could advance even more suddenly once that limitation is overcome. I first thought the limitation he had in mind was something like \"AIs don't get better at learning over the course of training.\" Apparently, the limitation he was actually pointing to was something like \"AIs don't learn continuously during all the actions they take.\"&nbsp;</p><p>However, this is still a deficit of degree, and not of kind. Current AIs are worse than human at continuous learning, but they can do it, assuming they're configured to try. Like most other problems in the field, the current ML paradigm is making steady progress towards better forms of continuous learning. It's not some untapped reservoir of capabilities progress that might quickly catapult AIs beyond human levels in a short time.</p><p>As I said at the start of this post, researchers try all sorts of stuff to get better performance out of computers. Continual learning is one of the things they've tried.</p><p><i><u>End of edited text.</u></i></p><h3><a href=\"https://youtu.be/gA1sNLL6yg4?t=5321\">True experts learn (and prove themselves) by breaking things</a></h3><blockquote><p>We have people in crypto who are good at breaking things, and they're the reason why <i>anything</i> is not on fire, and some of them might go into breaking AI systems instead, because that's where you learn anything. You know, any fool can build a crypto-system that they think will work. Breaking existing cryptographical systems is how we learn who the real experts are.</p></blockquote><p>The reason this works for computer security is because there's easy access to ground truth signals about whether you've actually \"broken\" something, and established - though imperfect - frameworks for interpreting what a given break means for the security of the system as a whole.</p><p>In alignment, we mostly don't have such unambiguous signals about whether a given thing is \"broken\" in a meaningful way, or about the implications of any particular \"break\". Typically what happens is that someone produces a new empirical result or theoretical argument, shares it with the broader community, and everyone disagrees about how to interpret this contribution.&nbsp;</p><p>For example, some people seem to interpret current chatbots' vulnerability to adversarial inputs as a \"break\" that shows RLHF isn't able to properly align language models. My response in <a href=\"https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky#Why_aren_t_other_people_as_pessimistic_as_Yudkowsky_\">Why aren't other people as pessimistic as Yudkowsky?</a> includes a discussion of adversarial vulnerability and why I don't think points to any irreconcilable flaws in current alignment techniques. Here are two additional examples showing how difficult it is to conclusively \"break\" things in alignment:</p><p><u>1: Why not just reward it for making you smile?</u></p><p>In 2001, Bill Hibbard <a href=\"https://www.ssec.wisc.edu/%7Ebillh/visfiles.html\">proposed a scheme</a> to align superintelligent AIs.</p><blockquote><p>We can design intelligent machines so their primary, innate emotion is unconditional love for all humans. First we can build relatively simple machines that learn to recognize happiness and unhappiness in human facial expressions, human voices and human body language. Then we can hard-wire the result of this learning as the innate emotional values of more complex intelligent machines, positively reinforced when we are happy and negatively reinforced when we are unhappy.</p></blockquote><p>Yudkowsky <a href=\"https://www.lesswrong.com/posts/PoDAyQMWEXBBBEJ5P/magical-categories\">argued</a> that this approach was bound to fail, saying it would simply lead to the AI maximizing some unimportant quantity, such as by tiling the universe with \"tiny molecular smiley-faces\".&nbsp;</p><p>However, this is actually a <a href=\"https://www.lesswrong.com/posts/22xf8GmwqGzHbiuLg/seriously-what-goes-wrong-with-reward-the-agent-when-it\">non-trivial claim</a> about the limiting behaviors of reinforcement learning processes, and one I personally think is false. Realistic agents <a href=\"https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target\">don't simply seek to maximize their reward function's output</a>. A reward function reshapes an agent's cognition to be more like the sort of cognition that got rewarded in the training process. The effects of a given reinforcement learning training process depend on factors like:</p><ol><li>The specific distribution of rewards encountered by the agent.</li><li>The thoughts of the agent prior to encountering each reward.</li><li>What sorts of thought patterns correlate with those that were rewarded in the training process.</li></ol><p>My point isn't that Hibbard's proposal actually would work; I doubt it would. My point is that Yudkowsky's \"tiny molecular smiley faces\" objection does not unambiguously break the scheme. Yudkowsky's objection relies on hard to articulate, and hard to test, beliefs about the convergent structure of powerful cognition and the inductive biases of learning processes that produce such cognition.&nbsp;</p><p>Much of alignment is about which beliefs are appropriate for thinking about powerful cognition. Showing that a particular approach fails, <i>given certain underlying beliefs</i>, does nothing to show the validity of those underlying beliefs<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa3nowicq5op\"><sup><a href=\"#fna3nowicq5op\">[9]</a></sup></span>.</p><p><u>2: Do optimization demons matter?</u></p><p>John Wentworth describes the possibility of \"<a href=\"https://www.lesswrong.com/posts/KnPN7ett8RszE79PH/demons-in-imperfect-search\">optimization demons</a>\", self-reinforcing patterns that exploit flaws in an imperfect search process to perpetuate themselves and hijack the search for their own purposes.</p><p>But no one knows exactly how much of an issue this is for deep learning, which is famous for its ability to evade local minima when run with many parameters.</p><p>Additionally, I think that, if deep learning models develop such phenomena, then the brain likely does so as well. In that case, preventing the same from happening with deep learning models could be disastrous, if optimization demon formation&nbsp;turns out to be a key component in the mechanistic processes that underlie human value formation<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreff8ce9me8dd\"><sup><a href=\"#fnf8ce9me8dd\">[10]</a></sup></span>.&nbsp;</p><p>Another poster (ironically using the handle \"DaemonicSigil\") then <a href=\"https://www.lesswrong.com/posts/X7S3u5E4KktLp7gHz/tessellating-hills-a-toy-model-for-demons-in-imperfect\">found a scenario</a> in which gradient descent does form an optimization demon. However, the scenario in question is extremely unnatural, and not at all like those found in normal deep learning practice. So no one knew whether this represented a valid \"proof of concept\" that realistic deep learning systems would develop optimization demons.&nbsp;</p><p>Roughly two and a half years later, Ulisse Mini <a href=\"https://www.lesswrong.com/posts/X7S3u5E4KktLp7gHz/tessellating-hills-a-toy-model-for-demons-in-imperfect?commentId=hwzu5ak8REMZuBDBk\">would make</a> DaemonicSigil's scenario a bit more like those found in deep learning by increasing the number of dimensions from 16 to 1000 (still vastly smaller than any realistic deep learning system), which produced very different results, and weakly suggested that more dimensions do reduce demon formation.</p><p>In the end, different people interpreted these results differently. We didn't get a clear, computer security-style \"break\" of gradient descent showing it would produce optimization demons in real-world conditions, much less that those demons would be bad for alignment. Such outcomes are very typical in alignment research.</p><p>Alignment research operates with very different epistemic feedback loops as compared to computer security. There's little reason to think the belief formation and expert identification mechanisms that arose in computer security are appropriate for alignment.&nbsp;</p><h1>Conclusion</h1><p>I hope I've been able to show that there are informed, concrete arguments for optimism, that do engage with the details of pessimistic arguments. Alignment is an <a href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is\">incredibly diverse</a> field. Alignment researchers vary widely in their <a href=\"https://www.lesswrong.com/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results\">estimated odds of catastrophe</a>. Yudkowsky is on the extreme-pessimism end of the spectrum, for what I think are mostly invalid reasons.&nbsp;</p><p><i>Thanks to Steven Byrnes and Alex Turner for comments and feedback on this post.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7flo16mluhj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7flo16mluhj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>By this, I mostly mean the sorts of empirical approaches we actually use on current state of the art language models, such as <a href=\"https://arxiv.org/abs/1706.03741\">RLHF</a>, <a href=\"https://arxiv.org/abs/2202.03286\">red teaming</a>, etc.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6jtv48k3s3d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6jtv48k3s3d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We <i>can</i> take drugs, though, which maybe does something like change the brain's learning rate, or some other hyperparameters.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnxk3auzvwms\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnxk3auzvwms\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Technically it's trained to do <a href=\"https://arxiv.org/abs/2106.01345\">decision transformer</a>-esque reward-conditioned generation of texts.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmwled0lwdbq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmwled0lwdbq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The brain likely includes within-neuron learnable parameters, but I expect these to be a relatively small contribution to the overall information content a human accumulates over their lifetime. For convenience, I just say \u201cconnectome\u201d in the main text, but really I mean \u201cconnectome + all other within-lifetime learnable parameters of the brain\u2019s operation\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnphd9ymlhn5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefphd9ymlhn5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I expect there are pretty straightforward ways of leveraging a 99% successful alignment method into a near-100% successful method by e.g., ensembling multiple training runs, having different runs cross-check each other, searching for inputs that lead to different behaviors between different models, transplanting parts of one model's activations into another model and seeing if the recipient model becomes less aligned, etc.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fng6iyt4s9e3e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefg6iyt4s9e3e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Some alignment researchers do <a href=\"https://arxiv.org/abs/1906.01820\">argue</a> that gradient descent <i>is</i> likely to create such an intelligence - an inner optimizer - that then deliberately manipulates the training process to its own ends. I don't believe this either. I don't want to dive deeply into my objections to that bundle of claims in this post, but as with Yudkowsky's position, I have many technical objections to such arguments. Briefly, they:&nbsp;<br>- often rely on inappropriate analogies to evolution.<br>- rely on unproven (and dubious, IMO) claims about the inductive biases of gradient descent.<br>- rely on shaky notions of \"optimization\" that lead to absurd conclusions when critically examined.<br>- seem inconsistent with what we know of neural network internal structures (they're very interchangeable and parallel).<br>- seem like the postulated network structure would fall victim to internally generated adversarial examples.<br>- don't track the distinction between mesa objectives and behavioral objectives (one can probably <a href=\"https://arxiv.org/abs/2205.13891v2\">convert an NN into an energy function</a>, then parameterize the NN's forwards pass as a search for energy function minima, without changing network behavior at all, so mesa objectives can have ~no relation to behavioral objectives).<br>- seem very implausible when considered in the context of the human learning process (could a human's visual cortex become \"deceptively aligned\" to the objective of modeling their visual field?).<br>- <a href=\"https://www.lesswrong.com/posts/w2TAEvME2yAG9MHeq/gradient-hacking-is-extremely-difficult\">provide limited avenues</a> for any such inner optimizer to actually influence the training process.<br>See also: <a href=\"https://www.lesswrong.com/posts/RTkatYxJWvXR4Qbyd/deceptive-alignment-is-less-than-1-likely-by-default\">Deceptive Alignment is &lt;1% Likely by Default</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhb3yucmshi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhb3yucmshi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>There's also in-context learning, which arguably does count as 'getting smarter&nbsp;<i>while</i> running in inference mode'. E.g., without updating any weights, LMs can:&nbsp;<br>- <a href=\"https://www.deepmind.com/publications/can-language-models-learn-from-explanations-in-context\">adapt information</a> found in task descriptions / instructions to solving future task instances.<br>- given a coding task, <a href=\"https://arxiv.org/abs/2303.06689\">write an initial plan</a> on how to do that task, and then use that plan to do better on the coding task in question.<br>- even learn to <a href=\"https://arxiv.org/abs/2302.00902\">classify images</a>.<br>The reason this in-context learning doesn't always lead to persistent improvements (or at least changes) in GPT-4 is because OpenAI doesn't train their models like that.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4qjinqko9sm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4qjinqko9sm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>OpenAI does periodically train its models in a way that incorporates user inputs somehow. E.g., ChatGPT became much harder to jailbreak after OpenAI trained against the breaks people used against it. So GPT-4 is probably learning from <i>some</i> of the times it's run in inference mode.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna3nowicq5op\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa3nowicq5op\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Unless we actually try the approach and it fails in the way predicted. But that hasn't happened (yet).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnf8ce9me8dd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreff8ce9me8dd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This sentence would sound much less weird if John had called them \"attractors\" instead of \"demons\". One potential downside of choosing evocative names for things is that they can make it awkward to talk about those things in an emotionally neutral way.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1ykzm5ecl8d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1ykzm5ecl8d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The brain likely includes within-neuron learnable parameters, but I expect these to be a relatively small contribution to the overall information content a human accumulates over their lifetime. For convenience, I just say \u201cconnectome\u201d in the main text, but really I mean \u201cconnectome + all other within-lifetime learnable parameters of the brain\u2019s operation\u201d.</p></div></li></ol>", "user": {"username": "Quintin Pope"}}, {"_id": "dmyqjMFChqAcgN9dG", "title": "Common-sense sentience is dubious", "postedAt": "2023-03-21T15:32:14.737Z", "htmlBody": "<p><a href=\"https://timothytfchan.github.io/posts/common-sense-sentience-is-dubious/\"><i>Cross-posted from my website.</i></a></p><h1>Summary</h1><p>This post lays out some problems/uncertainties I have with various stances in the philosophy of mind, almost all of which implicitly or explicitly privilege 'common sense'. The main point is that our intuitions aren't shaped to be useful at finding out whether something actually experiences/experiences things in a particular way. Instead, those intuitions are strongly influenced by (1) the evolutionary need to navigate ancestral social environments and (2) what your culture believes - which I argue don't necessarily have much to do with the truth of the matter. The problems noted here may have drastic implications for our credences of what's capable of suffering, which in turn may lead to ethical implications. This post may serve as one rationale to Brian Tomasik's framework for thinking about sentience, through its exploration of the causes of our beliefs.</p><h1>Highlights</h1><ul><li>The problem of other minds means that we can\u2019t theorize about sentience (definition in footnote)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxkob5jps9b\"><sup><a href=\"#fnxkob5jps9b\">[1]</a></sup></span>&nbsp;by analysis of scientific evidence without additional philosophical assumptions.</li><li>However, these assumptions are often dubious as they are based on intuitions that might not necessarily \u2018track\u2019 sentience. This point is explored from both evolutionary and cultural perspectives.</li><li>There are also general doubts around common sense. We should remember that common sense has been successfully scientifically challenged throughout history. Additionally, differences in common sense can lead to inconsistent conclusions on the topic of sentience, so we might have to reject some common sense beliefs.</li><li>The (probabilistic) beliefs we have about the sentience of various systems are made on the basis of unstable foundations. It seems that we shouldn\u2019t be overconfident of our models of sentience. This may mean that systems not typically thought of as sentient, or thought to have a \u2018lower degree\u2019 of ethically-relevant aspects of sentience (such as microscopic invertebrates and conscious subsystems) could be assigned a credence of having such properties comparable to the credence we already assign to systems more typically thought of as definitively possessing such properties. Upon further investigation, this may lead to various ethical implications, or even ethical wagers on how to best reduce suffering.</li></ul><h1>Physical systems aren\u2019t labeled sentient/not-sentient,&nbsp;<i>a priori</i></h1><p>It's impossible to&nbsp;<i>directly</i> confirm the existence and experiences of other minds. Other minds are experienced from a first-person perspective, that is, another first-person perspective \u2013 not ours.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref456b94gu4jq\"><sup><a href=\"#fn456b94gu4jq\">[2]</a></sup></span>&nbsp;</p><p>In a strict sense, this \u2018problem of other minds\u2019 suggests that, without additional assumptions, we can\u2019t falsify nor, more generally, update, nor, in fact, formulate, our theories about other minds. Without additional assumptions, physical systems can\u2019t be labeled sentient nor not-sentient. Physical data is just physical data. It isn\u2019t evidence for sentience if our criteria for relevant data is uninformed. To reach conclusions, some philosophical assumptions<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrl0qwp9dst\"><sup><a href=\"#fnrl0qwp9dst\">[3]</a></sup></span>&nbsp;must be established prior to analysis of empirical, scientific findings (e.g. about the brain).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefs5adq04224l\"><sup><a href=\"#fns5adq04224l\">[4]</a></sup></span></p><p>So we need to figure out which assumptions are justified and why. The assumptions we usually accept are common-sensical ones. If we reject common-sense ideas about sentience, many of which have to be decided upon&nbsp;<i>prior to science</i>, then we might be forced to accept some counterintuitive ideas about how sentience works. To list a few that challenge our intuitions: it may be that nervous systems (or functional alternatives) are not prerequisites for entities to suffer, it may be that nested minds exist within a system we traditionally think of as a single mind, or it may be that the suffering of simpler entities isn\u2019t less intense as some might intuitively think.</p><h1>What about introspection?</h1><p>But what about access to our own minds? Can introspection&nbsp;<a href=\"https://en.wikipedia.org/wiki/Zen_and_the_Art_of_Consciousness\"><u>contribute</u></a> to our understanding of sentience? That could be true to some extent. With very few pre-theoretic, relatively palatable assumptions, we may be able to reason by analogy on the sentience of human or human-like systems, as well as potentially on the sentience of less-human-like systems. However, the converse doesn\u2019t necessarily follow, that is, it seems a leap to think that systems being unlike humans would&nbsp;<i>strongly</i> suggest that those systems are not sentient. For instance, if pain is, contrary to our expectations, relatively simple to implement, then there might be a lot more of it in the world.</p><p>Compared to most people, I\u2019m personally more skeptical about using common sense when thinking about sentience, which leads me to counterintuitive ideas. Yet, it may be unjustified to dismiss counterintuitive ideas simply because they aren\u2019t intuitive. There are a couple of reasons to doubt common sense.</p><h1>Common sense may not track truth about sentience</h1><h2>Evolutionary debunking</h2><p>Human intuition about the mental states of other entities is called \u201ctheory of mind\u201d. This was influenced by evolutionary pressures. The brain processes by which we instinctively attribute mental states to others evolved&nbsp;<a href=\"https://pubmed.ncbi.nlm.nih.gov/16239031/\"><u>as a result</u></a> of the demands of interacting with those whose responses had significant consequences on the evolutionary fitness (survival &amp; reproduction) of our ancestors.</p><p>However, this means that our ability to attribute mental states to others&nbsp;<i>evolved for practical purposes</i>, rather than as a means of acquiring accurate knowledge of the existence, non-existence, or content of the possible mental states associated with any particular physical system. As long as it doesn\u2019t affect evolutionary fitness significantly, a different set of mental facts about other systems might provide us with the same physical outcomes, and so being incorrect about sentience can be consistent with functioning well in an ancestral environment.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnd4jqptzd2\"><sup><a href=\"#fnnd4jqptzd2\">[5]</a></sup></span></p><p>It would be a striking coincidence if the development of our theory of mind, guided by evolutionary pressures to improve fitness, also resulted in abilities that provide us with accurate insights regarding inaccessible, subjective experiences.&nbsp;</p><p>Perhaps most importantly, it would be especially striking if it resulted in insights specifically about systems that are&nbsp;<i>very different from us</i>. A critic might claim that fitness benefits require accurate mind-tracking among similar peers, but even they must confess that this might not generalize \u201cout-of-distribution\u201d. For instance, in the environment of evolutionary adaptiveness, our ancestors didn\u2019t interact directly with microscopic lifeforms or computers, so we have little reason to expect accurate intuitions about sentience for these groups even if we have good reason to expect accurate intuitions about sentience for other groups.</p><p>Additionally, some systems may be simple enough to be modeled directly, eliminating the evolutionary need to attribute mental states to them, whether or not they are sentient (in some way). Using an extreme example to illustrate, the reason that we instinctively believe that it\u2019s obvious that a rock isn\u2019t sentient, may be because the simplicity of a rock allows for direct observation, making it unnecessary to evolve such intuitions (independent of the fact of the matter).</p><p>For myself, personally, this evolutionary debunking argument is one of the most convincing reasons to doubt the completeness and accuracy of our common sense notions of mind. When we realize that the basis of our thinking about the (lack of) sentience of most physical systems might not necessarily have anything to do with being (not) sentient, there seems to be a lot more of a reason to doubt our initial ideas.</p><h2>Cultural effects</h2><p>Culture and the prevailing beliefs of one\u2019s society also influences what counts as \u201ccommon sense\u201d ideas about sentience. Different cultures treat sentience-adjacent concepts in different ways. For instance, traditionally, Western cultures strongly emphasize the&nbsp;<i>human</i> soul. Many Asian cultures and religions believe that it\u2019s possible to reincarnate as non-human animals. Some forms of animism of various indigenous peoples extend the idea of a \u201cspiritual essence\u201d to plants, rocks and rivers. In present-day academic communities, arguments about presence or absence of consciousness and sentience are often made with reference to human-like cognitive capabilities. \u201cCommon sense\u201d about counts as sentient is highly influenced by culture at large. In Einstein\u2019s cynical words, \u201cCommon sense is nothing more than a deposit of prejudices laid down in the mind before you reach eighteen.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref96f1prb7pyh\"><sup><a href=\"#fn96f1prb7pyh\">[6]</a></sup></span></p><p>The ideas of a \u201csoul\u201d, a \u201cspiritual essence\u201d, or a \u201cliving essence\u201d are conceptually distinct from \u201csentience\u201d and \u201cconsciousness\u201d, but it seems we shouldn\u2019t take that too literally. For many, such concepts are related and do overlap with sentience. Such distinctions were likely blurred to many individuals in such cultures. Indeed, even today, non-specialists discussing these ideas often mix them up.</p><p>Perhaps our cultural views lead us to all the right conclusions?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefu0kcshfmpzs\"><sup><a href=\"#fnu0kcshfmpzs\">[7]</a></sup></span>&nbsp;It\u2019s hard to see why that must be the case. While religious and cultural claims about physical reality can be disproved scientifically (including claims about non-physical entities physically interacting with physical entities), it\u2019s, again, impossible to reach the \u2018ground truth\u2019 of the matter and hence impossible to use that information to support or critique theorizing about other minds, without having first justifying our foundational assumptions \u2013 which are precisely what\u2019s in doubt.</p><h1>General doubts around common sense</h1><p>There are also general reasons to doubt common sense. That said, there are also general reasons in favor of common sense, and so these non-specific reasons may seem weaker.</p><h2>Intuitions perform poorly in other domains</h2><p>Another reason might be that our intuitions have a poor track record in other domains of knowledge, which could generalize to this specific case. It\u2019s intuitive to think that the Sun revolves around the Earth, but the heliocentric model showed otherwise. The idea that as living organisms we have a special vital force, separating us from non-living matter, is intuitive, but a physicalist understanding of the universe dispelled that notion (indeed, there are&nbsp;<a href=\"http://foundational-research.org/eliminativist-approach-consciousness/\"><u>parallels</u></a> between an eliminativist approach to consciousness and eliminating the notion of&nbsp;<i>\u00e9lan vital</i>). The belief in absolute space and time is intuitive, but this was seriously challenged by the theory of relativity. The possibility of a multiverse or being in a simulation strikes many as ridiculous, but there are good arguments in favor of those possibilities.&nbsp;</p><p>There are domains of knowledge where our intuitions perform especially poorly. A domain in which we want to classify physical systems but cannot directly label them as positive or negative examples (i.e. sentient or not), seems to be one where our intuitions do not hold.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmkwnc7r8qvm\"><sup><a href=\"#fnmkwnc7r8qvm\">[8]</a></sup></span></p><h2>Inconsistency of common sense</h2><p>Common sense can lead to counterintuitive, or even contradictory, conclusions. In that case, we might end up rejecting parts of our common sense in favor of other parts. An example is when one rejects property dualism about mind in favor of forms of physicalism, which might more likely motivate beliefs about digital sentience, which some find counterintuitive. Hence, there may be problems of consistency within one\u2019s set of common sense ideas about sentience.</p><p>Different forms of common sense may also exist between individuals - a point similar to the argument about cultural effects. For example, individuals on the autistic spectrum may have a&nbsp;<a href=\"https://www.spectrumnews.org/wiki/theory-of-mind/\"><u>different understanding</u></a> of theory of mind compared to more neurotypical individuals. This might suggest that communities composed of different proportions of individuals with varying levels of autistic characteristics would have different ideas about sentience. Indeed, philosophers who disagree with each other might be starting from different premises (such as those relating to structure, function, higher-order abilities, similarity, levels of confidence; see footnote 3) that result from their different forms of common sense. One person\u2019s&nbsp;<i>modus ponens</i> is another person\u2019s&nbsp;<i>modus tollens</i>. If this makes us more doubtful of \u201ccommon sense\u201d, perhaps that should make us less likely to reject conclusions we find counterintuitive, and it may suggest placing less faith in conclusions that seem obvious to us.</p><h1>Probabilities</h1><p>In light of all this, an ethically relevant question is what this means for how we should think about probabilities relevant to the topic of sentience. The answer to this question is a complicated one, and one that\u2019s also necessarily subjective. But at a first glance it seems to me that:</p><ol><li>The assumptions underlying our thinking about sentience are dubious. With such unstable foundations, we ought to penalize large differences between probabilities for different claims relevant to sentience. Avoid overconfidence in our models of what may or may not be sentient, and how those systems experience it.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref20sq35szrlg\"><sup><a href=\"#fn20sq35szrlg\">[9]</a></sup></span></li><li>As a corollary, given that we think systems commonly claimed to be sentient by most humans are in fact sentient, we can\u2019t rule out to a high degree of certainty, the sentience of entities such as&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jYdmcrgAj5odTunCT/demodex-mites-large-and-neglected-group-of-wild-animals\"><u>demodex mites</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/euLNDcwmmtnXFxcsx/the-possibility-of-microorganism-suffering\"><u>microbes</u></a>,&nbsp;<a href=\"https://reducing-suffering.org/bacteria-plants-and-graded-sentience/\"><u>plants</u></a>,&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11098-014-0387-8\"><u>countries</u></a>, and&nbsp;<a href=\"https://reducing-suffering.org/fuzzy-nested-minds-problematize-utilitarian-aggregation/\"><u>conscious subsystems (including disconnected subsets of the universe)</u></a>. This might mean that there exist ethical implications, or even&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xzS5jcPfxGZSziPXq/timothy-chan-s-shortform?commentId=KXWtRym3ZJqYeC3fG\"><u>wagers</u></a>, worth investigating.</li><li>Statements about the probability of sentience are almost always conditional, and we should try to communicate our assumptions and what we already accept when discussing sentience. E.g., given X, there is a y% probability that z. It may be useful to communicate whether one takes particular empirical findings or philosophical positions as cruxes to some conclusion.</li></ol><p>More work is needed to clarify what this means for the probabilities we assign to the presence of sentience or features of sentience in various physical systems. More investigation into the resulting ethical implications has the potential to suggest effective interventions.</p><h1>Further resources</h1><p><a href=\"https://reducing-suffering.org/dissolving-confusion-about-consciousness/\"><i><u>Dissolving Confusion about Consciousness</u></i></a></p><p><a href=\"https://www.tandfonline.com/doi/full/10.1080/00048402.2014.910675#.VGIm2vl4rig\"><i><u>The Crazyist Metaphysics of Mind</u></i></a></p><p><a href=\"https://forum.effectivealtruism.org/posts/euLNDcwmmtnXFxcsx/the-possibility-of-microorganism-suffering#Appendix__Factors_affecting_how_we_attribute_suffering\"><u>Appendix of&nbsp;</u><i><u>The Possibility of Microorganism Suffering</u></i></a></p><h1>Acknowledgements</h1><p>Anthony DiGiovanni, Eric Chen, Magnus Vinding, Miranda Zhang, and Sean Richardson provided helpful comments. Commenting does not imply that they endorse my views.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxkob5jps9b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxkob5jps9b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>What do I mean by sentience? The arguments presented here seem to apply to \u2018sentience\u2019 in a broad sense that means consciousness or phenomenality of any kind, but they can (of course) also apply, more specifically, to \u2018sentience\u2019 in a narrow sense that relates to the ability to feel pain or suffering. While the former definition may be intriguing from a philosophical perspective, the latter definition is more relevant from an altruistic perspective. It's likely that most readers will focus on the narrow latter definition, but one should keep in mind that the arguments in this piece may apply more generally.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn456b94gu4jq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref456b94gu4jq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Using a machine learning analogy, there is data (physical systems) but the data is unlabeled (can't determine whether the systems are really sentient).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrl0qwp9dst\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrl0qwp9dst\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Which assumptions? In principle, one could make any of an infinite number of non-contradictory assumptions. With the exception of more fundamental assumptions needed to establish the scientific worldview upon which we can construct arguments to challenge the unreliability of our intuitions (e.g., assuming the existence of an external world, the existence of the past, the validity of science etc.), the doubts discussed in this article appear to me to be applicable to most, if not all, subsequent assumptions required for a theory of sentience. A non-exhaustive list of assumptions vulnerable to criticism include: those that attribute sentience based on physical structure or function, those that propose a requirement of higher-order abilities, and those that determine how similar is \u201csimilar enough\u201d for other systems to be sentient as well as our level of confidence in such determination.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fns5adq04224l\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefs5adq04224l\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This isn\u2019t to say that we can\u2019t make some assumptions after looking at scientific facts, and add those assumptions to our theories. We can do that, but we still have to start with some assumptions&nbsp;<i>a priori</i>. In addition, assumptions that rely on science and \u2018common sense\u2019, could still fall apart if common sense doesn\u2019t hold.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnd4jqptzd2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnd4jqptzd2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It might be evolutionarily beneficial in some cases to be unable to attribute mental states (e.g., when hunting prey lacking sophisticated defenses).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn96f1prb7pyh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref96f1prb7pyh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Lincoln Barnett The Universe and Dr Einstein (1950 ed.)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnu0kcshfmpzs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefu0kcshfmpzs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Also, the \u201csentience-relevant features\u201d that we choose to use to map to \u201cis-sentient\u201d might be influenced by current topics and trends. For instance, it might very well be that we think along the lines of: A lot of progress happens in neuroscience and psychology, and so that seems essential for sentience. A lot of progress happens in computer science and artificial intelligence, so digital sentience is possible.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmkwnc7r8qvm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmkwnc7r8qvm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>An additional complicating factor is that facts about minds may be metaphysical and are ultimately not susceptible to empirical investigation.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn20sq35szrlg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref20sq35szrlg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Eric Schwitzgebel makes a similar point in&nbsp;<i>The Crazyist Metaphysics of Mind</i>: \u201cThus I suggest: Major metaphysical issues of mind are resistant enough to empirical resolution that none, at a moderate grain of specificity, empirically warrants a degree of credence exceeding that of all competitors; and this situation is unlikely to change in the foreseeable future.\u201d.</p></div></li></ol>", "user": {"username": "Timothy Chan"}}, {"_id": "s6k9cKdX8c4nhH8qq", "title": "Estimation for sanity checks", "postedAt": "2023-03-21T00:13:32.217Z", "htmlBody": "<p>I feel very warmly about using relatively quick estimates to carry out sanity checks, i.e., to quickly check whether something is clearly off, whether some decision is clearly overdetermined, or whether someone is just bullshitting. This is in contrast to <a href=\"https://nunosempere.com/blog/2022/08/20/fermi-introduction/\">Fermi estimates</a>, which aim to arrive at an estimate for a quantity of interest, and which I also feel warmly about but which aren\u2019t the subject of this post. In this post, I explain why I like quantitative sanity checks so much, and I give some examples.</p><h2>Why I like this so much</h2><p>I like this so much because:</p><ul><li>It is very defensible. There are some cached arguments against more quantified estimation, but sanity checking cuts through most\u2014if not all\u2014of them. \u201cOh, well, I just think that estimation has some really nice benefits in terms of sanity checking and catching bullshit, and in particular in terms of defending against scope insensitivity. And I think we are not even at the point where we are deploying enough estimation to catch all the mistakes that would be obvious in hindsight after we did some estimation\u201d is both something I believe and also just a really nice motte to retreat when I am tired, don\u2019t feel like defending a more ambitious estimation agenda, or don\u2019t want to alienate someone socially by having an argument.</li><li>It can be very cheap, a few minutes, a few Google searches. This means that you can practice quickly and build intuitions.</li><li>They are useful, as we will see below.</li></ul><h2>Some examples</h2><p>Here are a few examples where I\u2019ve found estimation to be useful for sanity-checking. I mention these because I think that the theoretical answer becomes stronger when paired with a few examples which display that dynamic in real life.</p><h3><strong>Photo Patch Foundation</strong></h3><p>The <a href=\"https://photopatch.org/\">Photo Patch Foundation</a> is an organization which has received a <a href=\"https://www.openphilanthropy.org/grants/photo-patch-foundation-general-support-2019/\">small amount of funding</a> from Open Philanthropy:</p><blockquote><p>Photo Patch has a website and an app that allows kids with incarcerated parents to send letters and pictures to their parents in prison for free. This diminishes barriers, helps families remain in touch, and reduces the number of children who have not communicated with their parents in weeks, months, or sometimes years.</p></blockquote><p>It takes <a href=\"https://donorbox.org/patching-relationships-with-letters-photos-2\">little digging</a> to figure out that their costs are $2.5/photo. If we take the <a href=\"https://forum.effectivealtruism.org/posts/4Qdjkf8PatGBsBExK/adding-quantified-uncertainty-to-givewell-s-cost\">AMF numbers at all seriously</a>, it seems very likely that this is not a good deal. For example, for $2.5 you can deworm several kids in developing countries, or buy <a href=\"https://www.againstmalaria.com/DollarsPerNet.aspx\">a bit more</a> than one malaria net. Or, less intuitively, trading 0.05% chance of saving a statistical life for sending a photo to a prisoner seems like a pretty bad trade\u20130.05% of a statistical life corresponds to 0.05/100 \u00d7 70 years \u00d7 365 = 12 statistical days.</p><p>One can then do <a href=\"https://forum.effectivealtruism.org/posts/h2N9qEbvQ6RHABcae/a-critical-review-of-open-philanthropy-s-bet-on-criminal\">somewhat more elaborate estimations</a> about criminal justice reform.</p><h3><strong>Sanity-checking that supply chain accountability has enough scale</strong></h3><p>At some point in the past, I looked into <a href=\"https://forum.effectivealtruism.org/posts/ME4zE34KBSYnt6hGp/new-cause-proposal-international-supply-chain-accountability\">supply chain accountability</a>, a cause area related to improving how multinational corporations treat labor. One quick sanity check is, well, how many people does this affect? You can check, and per <a href=\"https://static.inditex.com/annual_report_2021/es/documentos/informe-de-gestion-integrado-2021.pdf\">here</a><a href=\"https://nunosempere.com/blog/2023/03/10/estimation-sanity-checks/#fn:1\"><sup>1</sup></a>, Inditex\u2014a retailer which owns brands like Zara, Pull&amp;Bear, Massimo Dutti, etc.\u2014employed 3M people in its supply chain, as of 2021.</p><p>So scalability is large enough that this may warrant further analysis. One this simple sanity check is passed, one can then go on and do some more complex estimation about how cost-effective improving supply chain accountability is, like <a href=\"https://www.getguesstimate.com/models/14645\">here</a>.</p><h3><strong>Sanity checking the cost-effectiveness of the EA Wiki</strong></h3><p>In my analysis of the EA Wiki, I calculated how much the person behind the EA Wiki <a href=\"https://forum.effectivealtruism.org/posts/kTLR23dFRB5pJryvZ/external-evaluation-of-the-ea-wiki#Costs_per_word_compared_to_other_industries\">was being paid per word</a>, and found that it was in the ballpark of other industries. If it had been egregiously low, my analysis could have been shorter, and maybe concluded that this was a really good bargain. If the amount had been egregiously high, maybe I would have had to dig in about why that was.</p><p>As it was, the sanity check was passed, and I went on to look at <a href=\"https://forum.effectivealtruism.org/posts/kTLR23dFRB5pJryvZ/external-evaluation-of-the-ea-wiki#Evaluating_outcomes\">other considerations</a>.</p><h3><strong>Optimistic estimation for early causes</strong></h3><p>Occasionally, I\u2019ve seen some optimistic cost-effectiveness estimates by advocates of a particular cause area or approach (e.g., <a href=\"https://forum.effectivealtruism.org/posts/CcNY4MrT5QstNh4r7/cost-effectiveness-of-foods-for-global-catastrophes-even\">here</a>, <a href=\"https://forum.effectivealtruism.org/posts/HqEmL7XAuuD5Pc4eg/evaluating-strongminds-how-strong-is-the-evidence\">here</a>, or <a href=\"https://forum.effectivealtruism.org/posts/XpeamS2yTNhagxAip/remote-health-centers-in-uganda-a-cost-effective\">here</a>). One possible concern here is that because it\u2019s the advocates that are doing this cost-effective estimates, they might be biased upwards. But even if they are biased upwards, they are not completely uninformative: they show that at least some assumptions and parameters, chosen by someone who is trying their best, under which the proposed intervention looks great. And then further research might reveal that the initial optimism is or isn\u2019t warranted. But that first hurdle isn\u2019t trivial.</p><h3><strong>Other examples</strong></h3><ul><li>You can see the revival of LessWrong pretty clearly if you look at the <a href=\"https://i.imgur.com/sPA5IAZ.png\">number of votes per year</a>. Evaluating the value of that revival is much harder, but one first sanity check is to see whether there was some reviving being done.</li><li>When evaluating small purchases, sometimes the cost of the item is much lower than the cost of thinking about it, or the cost of the time one would spend using the item (e.g., for me, the cost of a hot chocolate is smaller than the cost of sitting down to enjoy a hot chocolate). I usually take this as a strong sign that the price shouldn\u2019t be the main consideration for those types of purchase, and that I should remember that I am no longer a poor student.</li><li>Some causes, like rare diseases, are not going to pass a cost-effectiveness sanity check, because they affect too few people.</li><li>If you spend a lot of time in front of a computer, or having calls, the cost of better computer equipment and a better microphone is most likely worth it. I wish I\u2019d internalized this sooner.</li><li>Raffles and lotteries (e.g., \u201cmake three forecasts and enter a lottery to win $300\u201d, or \u201canswer this survey to enter a raffle to win $500\u201d) are usually not worth it, because they don\u2019t reveal the number of people who enter, and it\u2019s usually fairly high.</li><li>etc.</li></ul><h2>Conclusion</h2><p>I explained why I like estimates as sanity checks: they are useful, cheap, and very defensible. I then gave several examples of dead-simple sanity checks, and in each case pointed to more elaborate follow-up estimates.</p>", "user": {"username": "NunoSempere"}}, {"_id": "h7QeZirrkxXhLm6w5", "title": "The Wizard of Oz Problem: How incentives and narratives can skew our perception of AI developments", "postedAt": "2023-03-20T22:36:52.253Z", "htmlBody": "", "user": {"username": "Akash"}}, {"_id": "ZSFWLn6ZKfy3rj8Kf", "title": "Our clubs\u2019 day tabling went well, AND wasn\u2019t that effective.", "postedAt": "2023-03-20T22:09:57.001Z", "htmlBody": "<h1>Intro</h1><p>EA Wellington (EAW) in New Zealand, has been iterating on the same basic <a href=\"https://forum.effectivealtruism.org/posts/E58nEJtaEwkKtWdSi/how-to-run-an-effective-stall-we-think\"><u>stall idea since 2021</u></a>, we have found this low cost method to be effective at encouraging engagement at the stall, with mixed results in people then turning up to our regular events, and unknown impact on people\u2019s long term thinking.</p><p>This post outlines how we ran the stall at a recent Clubs\u2019 Day at our local university, including the thinking behind some of our decisions, what we learnt from the process, and some things you might want to consider if you\u2019re running a stall.</p><p>This post was written by me (Tom) with feedback from the rest of the EAW exec. I use \u201cwe\u201d a lot, and when expressing opinions this generally means \u201cI think this and no-one else challenged it when reading the draft\u201d.</p><p>I pulled some parts of this post from our Clubs\u2019 Day Planning document put together by Oscar, a lightly edited version of which is available <a href=\"https://docs.google.com/document/d/1LVCYw1bMWvlTuANJrnj4bmhW6splESjW4UZHYhVg8L4/edit?usp%3Dsharing\"><u>here</u></a><u>.</u> I also consulted <a href=\"https://forum.effectivealtruism.org/posts/E58nEJtaEwkKtWdSi/how-to-run-an-effective-stall-we-think\"><u>Peter and Daniel\u2019s post on their experience in 2021</u></a><u>.</u></p><h1>The Why</h1><p>To put an outcome focus on our efforts we set 4 specific goals for our stall:</p><h3><strong>1) Tell people about our upcoming events.</strong></h3><p>In particular our session the following week with Luke Freeman from GWWC running a workshop on effective giving.</p><h3><strong>2) Get people to sign up for a temporary (2 emails) email list.</strong></h3><p>This list was used to support Goals 1, 3, &amp; 4 after Club\u2019s week</p><h3><strong>3) Tell people about the EAW Facebook page.</strong></h3><p>This is the best place for updates about upcoming EAW events</p><h3><strong>4) Offer people a free EA-themed book.</strong></h3><p>Only for those who were particularly engaged. This could either be a physical copy of \u201cThe Life You Can Save\u201d. Or by telling them about the <a href=\"https://80000hours.org/book-giveaway/\"><u>80,000 hours book giveaway</u></a>, or the <a href=\"https://forum.effectivealtruism.org/posts/waYZEYXkH6wp4qqM8/review-of-ea-new-zealand-s-doing-good-better-book-giveaway\"><u>EANZ Doing Good Better Giveaway</u></a>.</p><h1>The How</h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZSFWLn6ZKfy3rj8Kf/qmcu7lvj9srmx0khrn6y\" alt=\"\"></p><p>Below is a broad overview of our process on the day. For a more detailed and less refined take have a look at our <a href=\"https://docs.google.com/document/d/1LVCYw1bMWvlTuANJrnj4bmhW6splESjW4UZHYhVg8L4/edit?usp%3Dsharing\"><u>planning document</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbi5luc64mb\"><sup><a href=\"#fnbi5luc64mb\">[1]</a></sup></span>&nbsp;</p><h3><strong>0) Interact with people - The Hook</strong></h3><p>To achieve our goals we first needed to get people interested and interacting with us.</p><p>Our Hook was having a bag of pasta and 4 plastic tubes with images and text representing cause areas/charities on the table. As people walked past, and often slowed down to figure out what was going on, we\u2019d ask them if they wanted to vote for one of our charities. Once they came over we\u2019d explain that we\u2019re donating $100 to these 4 charities, with the money being distributed based on the votes, and offer them a pasta shell to vote with.</p><p>Once they\u2019d voted, or as they were voting (depending on how fast they were) we\u2019d start to talk about the decision, EA, our events, etc, with a mind to our specific goals. <i>\u201cHave you heard of Effective Altruism before?\u201d</i>&nbsp;was a good default segue into an explanation of the wider movement and what we do as a group: <i>\u201cWe meet every Tuesday evening at [location], here\u2019s a list of our upcoming events.\u201d</i></p><p>This worked fairly well to get people interested. There were multiple people who would have walked past had we not asked them to vote, who then seemed really engaged in talking about EA. We also had comments from people taking part that this was a good way to get them interested in the stall.</p><p>Part of the strength of this approach is making it easy for (particularly more shy) people to approach, they don\u2019t have to come up and start a conversation out of the blue because there\u2019s a clearly defined activity to take part in. This also very naturally leads into discussion on EA in general, specific cause areas, or having to make choices with finite resources.</p><h3><strong>1) Tell people about our upcoming events.</strong></h3><p>On either side of the voting set-up we had an <a href=\"https://docs.google.com/document/d/1LVCYw1bMWvlTuANJrnj4bmhW6splESjW4UZHYhVg8L4/edit%23heading%3Dh.molkp9faieia\"><u>A4 poster</u></a>&nbsp;listing our upcoming events and providing QR codes for our Facebook Page and Clubs\u2019 Day Mailing List sign up. We\u2019d call attention to these and encourage people to take a photo, which also gave people an easy exit strategy if they weren\u2019t actually interested but felt awkward leaving.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxpirsj6poz\"><sup><a href=\"#fnxpirsj6poz\">[2]</a></sup></span></p><p>Our workshop on High Impact Giving (intentionally scheduled for after Clubs\u2019 Week as a good intro event)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuk6bfs3uh7c\"><sup><a href=\"#fnuk6bfs3uh7c\">[3]</a></sup></span>&nbsp;was the main event we promoted. This flowed fairly well from the voting <i>\u201cYeah it is a pretty tough decision. We\u2019ve actually got a workshop next Tuesday all about making decisions like this, and how to make your donations have the most impact!\u201d</i></p><h3><strong>2) Get people to sign up for a temporary (2 emails) email list.</strong></h3><p>As well as the QR codes, we had a laptop with our two question google form sign up<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhznmydtfkyv\"><sup><a href=\"#fnhznmydtfkyv\">[4]</a></sup></span>. The form noted that this would be just 1-2 emails<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn67ulodwpgq\"><sup><a href=\"#fnn67ulodwpgq\">[5]</a></sup></span>, and we made sure to state this when asking people if they wanted to sign up. We don\u2019t run an ongoing Newsletter as EAW<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl7xvc1uyt98\"><sup><a href=\"#fnl7xvc1uyt98\">[6]</a></sup></span>, and didn\u2019t want people thinking it was a bigger commitment than it was.</p><p>Very few people used the QR code/their phones instead of the laptop, so having this available likely led to a higher number of sign-ups. The first email we sent can be viewed <a href=\"https://docs.google.com/document/d/1qdR9XLK-5shAyX2n9CopA79GI0ERgYybK9D0Hz4i13k/edit?usp%3Dsharing\"><u>here</u></a>. The second will be&nbsp;very similar.</p><h3><strong>3) Tell people about the EAW Facebook page.</strong></h3><p>The QR code on the poster saw some use here, given how fast they are to set up we\u2019d definitely recommend having them any time you want people to go to a url.</p><h3><strong>4) Offer people a free EA-themed book.</strong></h3><p>We had ~12 copies of The Life You Can Save and a single copy of Doing Good Better to give away<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref59oshp0cmia\"><sup><a href=\"#fn59oshp0cmia\">[7]</a></sup></span>. These made for good set dressing for the table, and occasionally after/while we were talking people would pick them up to read the back. Usually we\u2019d wait for this to happen before letting people know they could keep it: <i>\u201cIf you\u2019re keen to read it, it\u2019s all yours!\u201d.</i>&nbsp;</p><p>At least one person who had previously been to the stall came back to ask how much we were selling the books for, which maybe means some people who would have read them missed out. We opted for small text and talking about free books rather than a big sign to balance wider exposure with meaningful engagement, and to direct the limited number of books we had to people who would actually read them, and hopefully pass them on to others.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyjvjgp6lt9p\"><sup><a href=\"#fnyjvjgp6lt9p\">[8]</a></sup></span>&nbsp;I.e. We wanted people engaging with EA and leaving with a free book, not engaging with the stall purely for the free book.</p><p>From day 2 (once our physical book stocks were getting low) we also set up a QR code on the email sign-up screen linking to the <a href=\"https://forum.effectivealtruism.org/posts/waYZEYXkH6wp4qqM8/review-of-ea-new-zealand-s-doing-good-better-book-giveaway\"><u>EANZ Doing Good Better Giveaway</u></a>.</p><h1>Results</h1><p><i>Some context on numbers: EAW is a hybrid group of students and working professionals, with the balance skewed heavily towards working professionals. Prior to Clubs\u2019 week we had ~40 regular members, and 10-20 people at any individual event.</i></p><p>Over the 3 days we had in depth conversations with ~30 people, and interacted, typically for 1-5 minutes with ~200<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxmg4lgi82v\"><sup><a href=\"#fnxmg4lgi82v\">[9]</a></sup></span>&nbsp;people including multiple city councilors and the <a href=\"https://en.wikipedia.org/wiki/Leader_of_the_Opposition_(New_Zealand)\"><u>Leader of the Opposition</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpqtypnwe06\"><sup><a href=\"#fnpqtypnwe06\">[10]</a></sup></span>. Overall people seemed interested and there were significantly more meaningful interactions (&gt;90%) than people just voting and leaving.</p><p>We had one (1)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref17xjvo1yrsb\"><sup><a href=\"#fn17xjvo1yrsb\">[11]</a></sup></span>&nbsp;new person at the High Impact Giving Workshop as a result of clubs\u2019 day, they were a friend of a regular member and while clubs\u2019 day was the final push to attend they may have attended anyway in several counterfactual situations.</p><p>We had 16 email sign-ups on the first day, 20 on the second, and 1 on the third (different campus with a very different vibe to the clubs\u2019 day).</p><p>FB analytics currently says we have 7 new likes and 11 new followers, my draft of this post from immediately after clubs\u2019 week (2.5 weeks ago) recorded 8 likes and 14 followers. The analytics show data over the last 28 days, so this means we had lower new engagements during/after clubs\u2019 week than immediately before.</p><p>We gave away ~10 EA books to people, and encouraged another ~5 to order books online.</p><h1>Some things we didn\u2019t do</h1><h2>Fellowship</h2><p>In previous years we\u2019ve tried to run a fellowship/reading group. We\u2019ve found that while sign-ups may be high, interest drops off very quickly. This also creates significant extra workload for our exec. This year we instead chose to focus on getting people along to the high quality events we are already running, and to consider running a reading group&nbsp;if we had enough new people, who remain engaged after a few weeks of regular events. In hindsight this seems like a good decision as any effort to set this up would have been wasted.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref76kgv7j9er3\"><sup><a href=\"#fn76kgv7j9er3\">[12]</a></sup></span></p><h2>Long form mailing list</h2><p>As mentioned above we don\u2019t run an ongoing newsletter. We talked about setting one up, but this seemed like extra work, especially when the nationwide newsletter exists. Our experience (/vibe) is also that newsletter sign-ups at Clubs\u2019 Day don\u2019t translate well into engagement.</p><h2>Sticker/bookmark/EAW branded paraphernalia giveaways</h2><p>We\u2019ve done this in the past, but again it doesn\u2019t seem to translate into meaningful engagement.</p><h1>Conclusion</h1><p>Based on our experience we\u2019re unsure if this is an effective way to run a stall. We observed higher engagement than nearby stalls<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdnimjac5uod\"><sup><a href=\"#fndnimjac5uod\">[13]</a></sup></span>&nbsp;and generally felt like we did a good job at creating engagement with EA ideas in the moment. We don\u2019t know if this engagement at the stall translated into meaningful engagement with EA ideas in the long term, and it did not lead to significantly increased attendance at our events.</p><p>We think it\u2019s likely that our tabling strategy is comparatively good for creating initial engagements, and that in order to be more successful we need to work on conversion into outcomes, adjust our events to be more appealing to students (e.g. by changing location)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8mzm9oqk3q2\"><sup><a href=\"#fn8mzm9oqk3q2\">[14]</a></sup></span>, or not treat tabling as a recruitment pathway (i.e. have more directed student recruitment initiatives, and/or treat clubs\u2019 day as a chance to share EA ideas).</p><p><strong>We would be interested to hear from other groups around their challenges and successes with tabling!</strong></p><h3>In terms of running an engaging stall, we think the most important elements for us were:</h3><ul><li>An easy to bite Hook. Something that:</li><li>Let interested and socially awkward people engage with us without having to start a conversation with strangers themselves, and</li><li>Was visually interesting so people slowed down long enough to be asked to come over</li><li>A clear idea of our target outcomes.</li><li>Making it as easy as possible for people to meet those outcomes through e.g.</li><li>A laptop for email sign ups</li><li>QR codes for links</li><li>A clear conversational pathway with actions to take <i>\u201cHere\u2019s a list of our upcoming events, you can take a photo if you want\u201d</i></li></ul><p>For further reading we recommend <a href=\"https://resources.eagroups.org/running-a-group/promotion-advertising/clubs-fair-and-tabling\"><u>this guide</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/E58nEJtaEwkKtWdSi/how-to-run-an-effective-stall-we-think\"><u>this post from EAW in 2021</u></a>, and our <a href=\"https://docs.google.com/document/d/1LVCYw1bMWvlTuANJrnj4bmhW6splESjW4UZHYhVg8L4/edit?usp%3Dsharing\"><u>2023 planning doc</u></a>.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbi5luc64mb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbi5luc64mb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Everyone behind the table read/contributed to this so we were all on approximately the same page, and so people had ideas for things to say and ways to respond to more awkward questions e.g. SBF/FTX</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxpirsj6poz\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxpirsj6poz\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;We want to spend our time with people who are interested, so giving an out is better for them and for us</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuk6bfs3uh7c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuk6bfs3uh7c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I had some initial hesitancy about an intro event being highly donation focused, because of the \u201cwelcome to our group, please give us all your money\u201d optics. After 3 days tabling I noted that: \u201cThis worked well, didn\u2019t come across too strongly, and I would now advocate for running a similar event at this time in future years\u201d. Given the lack of follow through I\u2019m now less convinced this was a good idea (and would be interested in people\u2019s thoughts on this in the comments)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhznmydtfkyv\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhznmydtfkyv\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;1: Name 2: email address</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn67ulodwpgq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn67ulodwpgq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;We hadn\u2019t decided whether it would be 1 or 2 at the time</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl7xvc1uyt98\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl7xvc1uyt98\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;We did link to the EANZ newsletter sign-up in our first email, for anyone wanting ongoing updates</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn59oshp0cmia\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref59oshp0cmia\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Donated by one of our members, in future we\u2019d aim to have slightly more books in total, and more copies of Doing Good Better.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyjvjgp6lt9p\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyjvjgp6lt9p\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Plus there\u2019s some optics stuff about loudly giving away free books advocating for a specific worldview</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxmg4lgi82v\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxmg4lgi82v\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I was not counting at the time, these numbers are based on memory and have wide error bars</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpqtypnwe06\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpqtypnwe06\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Who on current polling may be our Prime Minister before the end of the year</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn17xjvo1yrsb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref17xjvo1yrsb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;<a href=\"https://maoridictionary.co.nz/word/7059\"><u>Tahi</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn76kgv7j9er3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref76kgv7j9er3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;There\u2019s an argument to be made that a Fellowship/Book club may have seen more engagement/buy-in. I don\u2019t find this argument compelling.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndnimjac5uod\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdnimjac5uod\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Not including the political party stalls that have incredibly strong existing brand recognition and a strong built in audience</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8mzm9oqk3q2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8mzm9oqk3q2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;We currently meet at a side campus (where we did day 3 of tabling) that is more convenient for most our non-student members but less convenient for most students</p></div></li></ol>", "user": {"username": "TJPHutton"}}, {"_id": "ewKPLoCDCxN6Es8RG", "title": "EA for Christians 2023 Annual Conference | London, 14-15 April", "postedAt": "2023-03-20T17:32:17.858Z", "htmlBody": "<p>There are still 2 weeks left to apply for EACH's <a href=\"https://www.eaforchristians.org/2023-annual-conference\">annual conference</a> in London. Apply <a href=\"https://airtable.com/shraNv8az6E1aEo1d\">here </a>and share with Christians you know.</p><p>We welcome speakers Rory Stewart (<a href=\"https://www.givedirectly.org/\">GiveDirectly</a>), Joy Bittner (<a href=\"https://vidaplena.global/\">Vida Plena</a>) Dustin Crummett (<a href=\"https://forum.effectivealtruism.org/posts/Bsdq5wK63vLEB3Gqg/announcing-the-launch-of-the-insect-institute\">Insect Institute</a>), and many other excellent Christian speakers.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ewKPLoCDCxN6Es8RG/hobw4sexcwru7mior6i5\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ewKPLoCDCxN6Es8RG/tce0djjbnmsfnetaacam 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ewKPLoCDCxN6Es8RG/zfzgut0n0iveea7ifrlv 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ewKPLoCDCxN6Es8RG/ycdvvjqpvlwb5qmf0pwb 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ewKPLoCDCxN6Es8RG/idocyac1tjyuqtnsvmhq 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ewKPLoCDCxN6Es8RG/eflmgwode0e9khgi2a0q 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ewKPLoCDCxN6Es8RG/iuvrrn4hgyk0adc6duqg 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ewKPLoCDCxN6Es8RG/t3oagxng0z17slmj5a6s 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ewKPLoCDCxN6Es8RG/qzshvv0xjw49uin8cq9y 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ewKPLoCDCxN6Es8RG/kk6tyq9oyycrkmjovhdf 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ewKPLoCDCxN6Es8RG/mw7h2xjbc5ad160riqfq 1126w\"><br>____</p><p>This conference is a fantastic opportunity to:<br><br>\u2013<strong>Hear </strong>from excellent speakers about themes relevant to the intersection of EA &amp; Christianity (e.g. Rory on improving aid effectiveness, Dustin on Christian reasons for caring about animal welfare, and Joy about evidence-based depression treatment).<br>\u2013 <strong>Meet </strong>some of the over <a href=\"https://www.eaforchristians.org/community-directory\">500 Christians</a> in the global effective altruism movemen<strong>t</strong><br>\u2013 <strong>Build </strong>your plans to improve the world, especially through high-impact careers and effective giving.&nbsp;</p><p>This event is primarily an in-person event. Online attendees may participate in networking, and talks will be recorded and shared. There is an attached <a href=\"https://www.eaforchristians.org/2023-academic-workshop-london\">academic workshop</a> on 16 April which can be attended 100% online.</p><p>If you are a Christian who is interested in effective altruism, then we would love to see you there. While this event is primarily for Christians, we welcome anyone sincerely interested in the intersection of Christianity and EA. We hope to learn from you as you learn from us.</p><p>If you're curious about the group EA for Christians and want to learn more, check out any of these:</p><p><strong>Website</strong>: <a href=\"https://www.eaforchristians.org/about-us/\">www.eaforchristians.org/about-us</a></p><p><strong>Facebook group</strong>: <a href=\"https://www.facebook.com/eaforchristians/groups/\">www.facebook.com/eaforchristians/groups/</a></p><p><strong>Newsletter</strong>: <a href=\"http://eepurl.com/ds6IMb\">http://eepurl.com/ds6IMb</a>&nbsp;</p>", "user": {"username": "JDBauman"}}, {"_id": "5YKx6xGg8qz6jLKvF", "title": "Some Comments on the Recent FTX TIME Article", "postedAt": "2023-03-20T17:36:20.000Z", "htmlBody": "<h1>Background</h1><ol><li>Alameda Research (AR) was a cryptocurrency hedge fund started in late 2017.</li><li>In early 2018, approximately half the employees quit, including myself and Naia Bouscal, the main person mentioned in <a href=\"https://forum.effectivealtruism.org/posts/b83Zkz4amoaQC5Hpd/time-article-discussion-effective-altruist-leaders-were\"><u>the TIME article</u></a>. At the time, I had considered AR to have failed, and I think even the people who stayed would have agreed that it had not achieved what it had wanted to.</li><li>Later in 2018, some of the remaining AR staff started working on a cryptocurrency exchange named FTX. FTX grew to become a multibillion-dollar company.</li><li>In late 2022, FTX collapsed. It has since been alleged that FTX defrauded their investors by misrepresenting the relationship between AR and FTX,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefirm65ye71zm\"><sup><a href=\"#fnirm65ye71zm\">[1]</a></sup></span>&nbsp;and that this effectively led to them stealing customer deposits.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefajmdsor1fsm\"><sup><a href=\"#fnajmdsor1fsm\">[2]</a></sup></span>&nbsp;</li><li>The recent TIME article doesn\u2019t make a very precise argument; here is my attempt at steelmanning/clarifying a major argument made in that article, which I will then respond to:<ol><li>Some EAs worked at AR before FTX started</li><li>Even though those EAs (including myself) quit before FTX was founded&nbsp;and therefore could not have had any first-hand knowledge of this improper relationship between AR and FTX, they knew things (like information about Sam\u2019s character) which would have enabled them to predict that something bad would happen<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7wg1kq7i57s\"><sup><a href=\"#fn7wg1kq7i57s\">[3]</a></sup></span></li><li>This information was passed on to \u201cEA leaders\u201d, who did not take enough preventative action and are therefore (partly) responsible for FTX\u2019s collapse</li></ol></li></ol><h1>Personal Background</h1><p>I worked at Alameda Research (AR) for about three months in early 2018. I was not involved in stealing FTX customer funds, and hopefully people trust me about that claim, if only because I quit before FTX was founded.</p><p>To make my COI clear: I left the company I founded to join AR; doing so was very costly to me; AR crashed and burned&nbsp;within a few months of me joining; I blamed this crashing and burning largely on Sam.</p><p>People who know I had a bad experience at AR are sometimes surprised that I\u2019m not on the \u201cobviously Sam was obviously 100% evil\u201d bandwagon. I\u2019ve been wanting to write something but found it hard because there weren\u2019t specific things I could react to, it was just some vague difference in vibes.</p><p>So I appreciate the TIME article sharing some specific things that \u201cEA Leaders\u201d allegedly knew which the author suggests should have caused them to predict FTX\u2019s fraud.</p><h1>My Experience at AR at a High Level</h1><p>I thought Sam was a bad CEO. I think he literally never prepared for a single one-on-one we had, his habit of playing video games instead of talking to you was \u201cquirky\u201d when he was a billionaire but aggravating when he was my manager, and my recollection is that Alameda made less money in the time I was there than if it had just simply bought and held bitcoin.</p><p>But my opinion of Sam overall was more positive than the sense I get from the statements in the TIME article. (This is not very surprising, given that the TIME article consists of statements that were probably intentionally selected to be the worst possible thing the journalist could find someone to say about Sam.)</p><p>It's hard to convey nuance in these posts, and I'm sure someone is going to interpret me as trying to defend Sam here. This is not what I\u2019m trying to do, but I do think it\u2019s worth trying to share my reflections to help others refine their models.</p><h1>Adding my personal experience to supplement some statements from the article</h1><blockquote><p>But one of the people who did warn others about Bankman-Fried says that he openly wielded this power when challenged. \u201cIt was like, \u2018I could destroy you,\u2019\u201d this person says. \u201cWill and Holden would believe me over you. No one is going to believe you.\u201d</p></blockquote><p>I don\u2019t want to speak for this person, but my own experience was pretty different. For example: Sam was fine with me telling prospective AR employees why I thought they shouldn\u2019t join (and in fact I did do this),<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqpq9jrwaxm\"><sup><a href=\"#fnqpq9jrwaxm\">[4]</a></sup></span>&nbsp;and my severance agreement didn\u2019t have any sort of non-disparagement clause. <a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId%3DaAFdkngbvEQvavntk=&amp;commentId=Gjhp9J3bnNgizBMcD\"><u>This comment</u></a>&nbsp;says that none of the people who left had a non-disparagement clause, which seems like an obvious thing a person would do if they wanted to use force to prevent disparagement.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbd6fh2tzhin\"><sup><a href=\"#fnbd6fh2tzhin\">[5]</a></sup></span></p><blockquote><p>Early Alameda executives also believed he had reneged on an equity arrangement that would have left Bankman-Fried with 40% control of the firm, according to a document reviewed by TIME.</p></blockquote><p>I assume this is referring to an agreement between Sam and Tara, &nbsp;the cofounders of AR. My understanding of what happened is different, but someone told me that my understanding is incorrect. So I\u2019m not sure what actually happened here, but I am 80%+ confident that the story is more complicated than the TIME article implies.</p><p>I can share what I do know about, which is my own equity arrangement:</p><ol><li>When I joined AR, Sam and I discussed an equity amount.</li><li>I quit before any paperwork could be signed memorializing this though. (I was only at AR for about three months.)</li><li>Obviously since no paperwork was signed, there was no clause which covered this scenario. Most startups have a one year <a href=\"https://www.investopedia.com/ask/answers/09/what-is-cliff-vesting.asp\"><u>vesting cliff</u></a>, meaning that the employee loses 100% of their equity if they quit within the first year.</li><li>I expect most startup CEOs would have said something like \u201chey, we didn\u2019t actually agree to anything here, and even if we did it probably would have had a clause meaning that you don\u2019t get any equity after quitting so soon, so I\u2019m giving you nothing.\u201d</li><li>Instead, AR gave me a cash payment which was equal to the equity amount we informally agreed times the most recent company valuation (although the valuation of AR at the time was low).</li><li>I considered this fair, maybe even more fair than what the average startup would have done.</li></ol><blockquote><p>\"We didn\u2019t know how much money we actually had. We didn\u2019t have a clear accounting record of all the trades we\u2019d done,\u201d Bouscal says.</p></blockquote><p>I agree that AR had bad accounting as a startup, and I agree with the implication that it was possible to predict that this would be correlated with AR/FTX having bad accounting as a mature company \u2014 &nbsp;I think this is a big area where I plausibly could have made a better prediction.</p><p>That being said, I still feel confused and surprised about how bad FTX\u2019s accounting actually was. <a href=\"https://substack.com/profile/25592954-sbf\"><u>Sam\u2019s reports</u></a>&nbsp;make it seem like they just were not tracking asset values at all? And somehow they were doing this while having audited financials, passing due diligence from major investors, etc.? And Sam was supposedly a great fundraiser but was circulating a balance sheet <a href=\"https://fullcrypto.substack.com/p/the-ftx-balance-sheet\"><u>with a $8B line item</u></a>&nbsp;for \u201chidden poorly labeled account\u201d? I would find it pretty helpful for someone to explain what actually happened here because this violates my models of how the world works.</p><p>(One obvious explanation is that people often cover up fraud by claiming it was simply incompetence, and maybe FTX is exaggerating its level of accounting incompetence for this reason. I don\u2019t fully buy this story though.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnys3lu26s3\"><sup><a href=\"#fnnys3lu26s3\">[6]</a></sup></span>)</p><h1>Next Steps</h1><p>As mentioned, I had and still have a lot of negative feelings about Sam. But at least on a couple specific points, my experience was different from the source(s) of this article in a way that paints a less clear story of Sam\u2019s character.</p><p>It might turn out that people&nbsp;were aware of stronger warning signs than those listed in the TIME article. It might also be the case that individuals could&nbsp;be better at predicting risk. But even if those things could have theoretically worked in the FTX case, protecting ourselves solely through better noticing \"warning signs\" feels fragile.</p><p>Instead, I would prefer due diligence processes which are not entirely reliant upon warning signs being triggered and evaluated until something like certainty is achieved.</p><p>In a future post, I would like to describe examples of due diligence processes that run \"by default\" and are less reliant upon warning signs being triggered.</p><p><i>Note: these are my ideas, not my employer\u2019s. There are a bunch of people who I\u2019ve talked about these ideas with and I am grateful to all of them, but special thanks on this post to Lizka Vaintrob, Jonas Vollmer, and Lacey Walker.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnirm65ye71zm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefirm65ye71zm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;There are some additional charges, but my understanding is that the biggest ones stem from this misrepresentation.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnajmdsor1fsm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefajmdsor1fsm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I\u2019ve mostly seen this claim in popular media where it isn\u2019t very precisely defined, but I think the precise version of the claim is that allowing AR to maintain a negative balance effectively let them use customer deposits as collateral, which is effectively stealing them. I\u2019m not entirely sure though.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7wg1kq7i57s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7wg1kq7i57s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Note that there are valid additional criticisms to be made here, notably that FTX leadership were involved in EA. But I don\u2019t think the article is making these criticisms, and I want to keep this post targeted to this particular article.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqpq9jrwaxm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqpq9jrwaxm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Or at least I\u2019m not aware of any way I\u2019ve suffered retribution as a result. Presumably he wasn\u2019t super thrilled with me telling his prospective employees not to join.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbd6fh2tzhin\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbd6fh2tzhin\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Though obviously there are reasons you still might not have the clause even if you wanted to prevent disparagement, e.g. you thought having the clause would itself be a cause for disparagement.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnys3lu26s3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnys3lu26s3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;For example, FTX\u2019s new CEO <a href=\"https://fortune.com/2022/12/13/ftx-ceo-john-ray-testifies-congress-no-record-keeping-quickbooks-bankman-fried/\"><u>has slammed</u></a>&nbsp;their accounting practices, and I don\u2019t understand why he would be incentivized to lie here</p></div></li></ol>", "user": {"username": "Ben_West"}}, {"_id": "w9NEcm3zRogeXuo7z", "title": "Our research process: an overview from Rethink Priorities\u2019 Global Health and Development team", "postedAt": "2023-03-20T17:02:36.582Z", "htmlBody": "<h1>Summary</h1><p>Rethink Priorities\u2019&nbsp;<a href=\"https://rethinkpriorities.org/global-health-and-development\"><u>Global Health and Development</u></a> team is a multidisciplinary ten-person team conducting research around various global health, international development, and climate change topics. We have so far mostly done \u201cshallow\u201d style reports for <a href=\"https://www.openphilanthropy.org/\">Open Philanthropy</a>, though we have also worked for other organizations, and have conducted some self-driven research. This post aims to share our current research process. The hope is to make our research as transparent as possible.</p><h1>About the team</h1><p>The Global Health and Development (GHD)&nbsp;<a href=\"https://rethinkpriorities.org/team#ghd\"><u>team</u></a> is one of the newer departments at Rethink Priorities (RP). It officially formed in Q3 2021, and throughout 2022 the team grew from the initial four hires to its current 10 members. Our team consists of two senior research managers (<a href=\"https://www.linkedin.com/in/tomhird/\">Tom Hird</a> and <a href=\"https://www.linkedin.com/in/melaniebasnak/\">Melanie Basnak</a>) overseeing eight researchers of different seniority (<a href=\"https://www.linkedin.com/in/greer-gosnell/\"><u>Greer Gosnell</u></a>,&nbsp;<a href=\"https://www.linkedin.com/in/aislingleow/\"><u>Aisling Leow</u></a>,&nbsp;<a href=\"https://www.linkedin.com/in/jenny-kudymowa/\"><u>Jenny Kudymowa</u></a>,&nbsp;<a href=\"https://www.linkedin.com/in/rubydickson/\"><u>Ruby Dickson</u></a>,&nbsp;<a href=\"https://www.linkedin.com/in/brucetsai/\"><u>Bruce Tsai</u></a>,&nbsp;<a href=\"https://www.linkedin.com/in/carmenvs/\"><u>Carmen van Schoubroeck</u></a>,&nbsp;<a href=\"https://www.linkedin.com/in/y-james-hu/\"><u>James Hu</u></a>, and&nbsp;<a href=\"https://www.linkedin.com/in/erinbraid/\"><u>Erin Braid</u></a>).&nbsp;GHD team members have expertise in economics, health, science, and policy, and bring experience from academia, consultancy, medicine, and nonprofit work.</p><h1>Our past research reports</h1><p>Rethink Priorities is a research organization that strives to generate impact by providing relevant stakeholders with tools to make more informed decisions.&nbsp;The GHD team\u2019s work to date has mainly been commissioned by donors looking to have a positive impact. Since its inception, the team has completed 23 reports for five different organizations/individuals, as well as two self-driven reports. We have publicly published four of these reports:&nbsp;</p><ol><li><a href=\"https://rethinkpriorities.org/publications/how-effective-are-prizes-at-spurring-innovation\"><u>How effective are prizes at spurring innovation?</u></a></li><li><a href=\"https://rethinkpriorities.org/publications/livelihood-interventions\"><u>Livelihood interventions: overview, evaluation, and cost-effectiveness</u></a></li><li><a href=\"https://rethinkpriorities.org/publications/the-redd-framework-for-reducing-deforestation-and-mitigating-climate-change\"><u>The REDD+ framework for reducing deforestation and mitigating climate change: overview, evaluation, and cost-effectiveness</u></a></li><li><a href=\"https://rethinkpriorities.org/publications/exposure-to-lead-paint-in-low-and-middle-income-countries\"><u>Exposure to Lead Paint in Low- and Middle-Income Countries</u></a>.&nbsp;</li></ol><p>Whenever possible, we want to disseminate our findings to maximize our impact. We intend to publish 13 of the remaining 19 reports that we have previously completed, but not yet shared publicly.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0v5axvofm1j\"><sup><a href=\"#fn0v5axvofm1j\">[1]</a></sup></span>&nbsp;Going forward, we hope to be able to publish within three months of their completion.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbvji59mjl19\"><sup><a href=\"#fnbvji59mjl19\">[2]</a></sup></span></p><p>Most of our past reports (78%) have been commissioned by&nbsp;<a href=\"https://www.openphilanthropy.org/\"><u>Open Philanthropy (OP)</u></a>. The projects we typically do for OP are \u201cshallow\u201d investigations looking into specific cause areas (e.g., hypertension, substandard and falsified drugs). These reports usually contain the following:</p><ul><li>A basic introduction, especially for complex topics</li><li>An estimate of the burden for the specific problematic area (and potentially the impact one could have by focusing on that area)<ul><li>This process usually involves critically engaging with existing estimates, as well as making our own</li><li>The burden estimation is often the most important part of the reports</li></ul></li><li>Information about existing funding going into the area</li><li>An analysis of potential interventions to tackle the issue, which often includes:<ul><li>Identifying potential interventions across different areas (e.g., policy, advocacy, market shaping, direct provision)</li><li>Evaluating potential interventions with a view to tractability and cost-effectiveness</li></ul></li><li>A discussion of the main uncertainties about the area and/or the existing interventions</li></ul><p>We have also done different types of work (for OP and others), including red-teaming (providing an outside skeptical challenge to existing work/ideas), investigating specific uncertainties around a topic following a previous report on it, and exploratory/strategy reports on relevant research within the effective altruism (EA) space.</p><h1>Our research process</h1><h2>Our workflow</h2><p>Most of our projects involve collaboration across two to three researchers of different seniority. We typically ensure that there is one senior researcher per project to act as \u201cproject lead,\u201d making most of the coordination efforts and ensuring, along with the manager, that the project is on track.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefi9ceydmf9vn\"><sup><a href=\"#fni9ceydmf9vn\">[3]</a></sup></span></p><p>Our commissioned projects usually kick off with a brief from the client that contains research questions that guide and structure our research. For internal projects (and some commissioned projects), the managers put together the briefs.</p><p>Most of our research projects, regardless of their nature or topic, involve the following components:</p><ul><li><u>Desk research</u>. We generally begin by searching for appropriate literature to answer the questions at hand. We assess the evidence (e.g., the number of quality studies in support of each idea, and their generalizability to the context of interest) and identify our uncertainties based on gaps in the literature.</li><li><u>Expert interviews</u>. We interview experts on the topics we research. We are a team of generalists, and as such, we remain humble about our limited expertise in a lot of the areas we research and seek to synthesize expert opinions. Experts include, but are not limited to, academics, CEOs, practitioners, and government officials. When possible, particularly when a topic is polarizing, we interview experts with (very) different perspectives (e.g., for our&nbsp;<a href=\"https://rethinkpriorities.org/publications/exposure-to-lead-paint-in-low-and-middle-income-countries\"><u>lead paint report</u></a>). We find these experts through a combination of recommendations from clients, connections from our own networks, and cold messaging relevant people identified through desk research.</li><li><u>Quantitative analyses</u>. We often do quantitative analyses to estimate how cost-effective an intervention might be, how many lives it may have saved to date or save in the future, and the like. These vary in complexity, from very rough back-of-the-envelope calculations based mostly on assumptions, to more complex cost-effectiveness analyses (CEAs) drawing from a mix of data and assumptions (e.g., see the&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1XD-ISTb24WWbsGpU-B5uaUCJk2HvoltYjlEi5lJq0Bg/edit\"><u>Spark CEA</u></a> in our&nbsp;<a href=\"https://rethinkpriorities.org/publications/livelihood-interventions\"><u>livelihoods report</u></a>). We often use Excel or Google Sheets, but will on occasion use&nbsp;<a href=\"https://www.causal.app/\"><u>Causal</u></a> or&nbsp;<a href=\"https://www.getguesstimate.com/\"><u>Guesstimate</u></a>, depending on the client\u2019s preferences and the project\u2019s needs.</li></ul><p>The amount of time spent on a given project depends on features like its scope and the number of researchers involved. The average project has involved about 60% of two full-time researchers\u2019 time over the course of five weeks, though some projects have taken just one to two weeks.</p><p>Our reports undergo several rounds of internal review. During these periods (often in the middle and at the end of each project), the manager overseeing that project will thoroughly review drafts. Often, the other manager (and sometimes a researcher not involved in that project) will also act as a reviewer. Reviews have usually taken place ~two days before the draft or final report was due to be completed, allowing some time for the researchers to address standing comments, doubts, or concerns. In the context of commissioned research, we send this version of the report to the client.</p><p>We then spend some extra time finalizing and polishing the report for publication. This step involves checking for consistent formatting, reaching out to experts to ensure their views are represented accurately and securing permission to quote them publicly, adding an editorial note and an acknowledgments section, and conducting a final (and particularly thorough) round of internal review.</p><h2>The timeline of a typical project</h2><p>Next is an example timeline for a typical project to date:</p><ul><li><u>Week 1</u>:<ul><li>Engage with the project brief, identifying potential \u201ccruxes\u201d in the research, and trying to define the scope as thoroughly as possible</li><li>Kickoff meeting with the client, where we raise questions that arose from engaging with the brief and discuss logistics</li><li>\u201cPremortem\u201d: a process in which we try to identify the main difficulties of completing this project and define action items to ensure we can overcome them</li><li>Team meeting to divide and coordinate the work</li><li>Initial research, getting familiar with the topic</li><li>Identifying and reaching out to experts (sometimes it takes a while for experts to get back to us, so we try to do this task as soon as possible; over the course of the project we might reach out to additional experts)</li><li>Rough \u201cinitial takes\u201d shared with client</li></ul></li><li><u>Weeks 2-3</u>:<ul><li>Desk research</li><li>Expert interviews</li><li>Sometimes generate quantitative models, though this often takes place later in the project</li><li>First draft: internal review, send to client, debrief meeting with client to get feedback and discuss next steps</li></ul></li><li><u>Weeks 4-5</u>:<ul><li>Desk research</li><li>Sometimes more expert interviews</li><li>Generate quantitative models</li><li>Write a section on remaining uncertainties, and sometimes a section on \u201cwhat we would do with more time\u201d</li><li>Write executive summary</li><li>Final draft: internal review, send to client, debrief meeting with client to receive and give feedback</li></ul></li><li><u>Week 6+</u>:<ul><li>\u201cRetrospective\u201d: a process in which we discuss what worked and what didn\u2019t when conducting this project, and distill learnings for future projects</li><li>Sometimes we are asked to do a few more hours of work to answer a key question that arose from our research; we usually follow up on those requests right after the project is completed</li><li>Polish the report for publication</li></ul></li></ul><p>Throughout the course of the project, we have recurring team meetings to discuss progress, and we may reach out to the client via email or have weekly check-in calls with them to ensure short feedback loops.</p><h1>Some general principles</h1><p>Across topics and project types, there are some underlying principles that remain constant:</p><ul><li><u>Reasoning transparency</u>. We try to make our reasoning as transparent as possible, specifying how all sources of information included in the report contribute to our conclusions, stating our certainty levels around different claims, and pointing out major sources of uncertainty in our analyses.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3g9fisa268h\"><sup><a href=\"#fn3g9fisa268h\">[4]</a></sup></span></li><li><u>Intellectual honesty/humility</u>. Our team comprises diverse experience (academia, consulting, nonprofits) and areas of expertise (medicine, biology, climate change, economics, quantitative sciences). That being said, we view ourselves as generalists and are not usually experts in the specific topics we research. Additionally, most of our reports are carried out in a limited time frame. Thus, while we strive for rigor in our research, we recognize that our findings may not reflect the absolute truth, and we are always open and willing to review our conclusions in light of new information.</li><li><u>Collaboration</u>. We think there is strength in collaboration, both within RP and across value-aligned organizations. We have started conversations with other researchers in the GHD and climate spaces and are always keen to share our unpublished reports (and any other resources that could be useful) with them. We strive to be kind and respectful in all of our interactions with external researchers and stakeholders.</li></ul><h1>Future developments</h1><p>Our research process has been evolving and will continue to do so. To ensure our research continually improves in rigor and thoroughness, we periodically revisit our processes. As our emphasis shifts toward internally driven research, the features and format of our reports and methodological approaches could also change.</p><p>We aim to incorporate relevant aspects (e.g., assumptions, moral weights) of research outputs from other organizations if we think they are well supported and will improve the conclusions of our reports.</p><p>We have begun to assemble guides related to some of our primary research components. For example, we are currently working on a cost-effectiveness analysis guide to converge on a more unified and replicable framework. In the spirit of transparency and collaboration, we hope to eventually make our internal guide publicly available.</p><p>We mentioned above that our reports go through several rounds of internal review. We would like to encourage and participate in external review processes in the future, for instance among researchers in other global health, development and climate organizations and/or from academics with relevant expertise. We imagine this being a collaborative endeavor, where other researchers review some of our work, and we review some of theirs.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/w9NEcm3zRogeXuo7z/m8vqguw7ycseox910lti\"></p><h1>Contributions and acknowledgments</h1><p>This post was written by Melanie Basnak with feedback from the full GHD team. We would like to thank Adam Papineau for copyediting and Rachel Norman for reviewing the post and providing useful suggestions. If you are interested in Rethink Priorities\u2019 work, you can sign up for our&nbsp;<a href=\"https://www.rethinkpriorities.org/newsletter\"><u>newsletter</u></a>. We use it to keep our readers updated about new research posts and other resources.</p><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0v5axvofm1j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0v5axvofm1j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Some of our reports cannot be published because we have not secured permission from our clients to do so, and there are good reasons to withhold some of them. Other reports are very niche and we do not think there would be a lot of value in publishing them, so the trade-off between time invested in preparing them for publication and the value readers might get out of them is not enough to compel us to publish them.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbvji59mjl19\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbvji59mjl19\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Our publication process has been delayed in the past due to the limited size of our team, with researchers spending most of their time tackling new projects as soon as previous projects were completed. With more staff, we are now making progress to shorten the window between project completion and publication.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fni9ceydmf9vn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefi9ceydmf9vn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This is not always the case. Three projects to date have been carried out by a single researcher, and four were completed without a senior researcher on board.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3g9fisa268h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3g9fisa268h\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;For more on reasoning transparency, see this&nbsp;<a href=\"https://www.openphilanthropy.org/research/reasoning-transparency/\"><u>research report</u></a> by Luke Muehlhauser of OP.</p></div></li></ol>", "user": {"username": "Rachel"}}, {"_id": "HBWmntigXkgjcqzpn", "title": "The Natural State is Goodhart", "postedAt": "2023-03-20T05:01:17.891Z", "htmlBody": "", "user": {"username": "devanshpandey"}}, {"_id": "idpbfmPjHFCvzj46L", "title": "EA & LW Forum Weekly Summary (13th - 19th March 2023)", "postedAt": "2023-03-20T04:18:16.544Z", "htmlBody": "<p>This is part of a weekly series summarizing the top posts on the EA and LW forums - you can see the full collection <a href=\"https://forum.effectivealtruism.org/s/W4fhpuN26naxGCBbN\">here.</a> The first post includes some details on purpose and methodology. Feedback, thoughts, and corrections are welcomed.</p><p>If you'd like to receive these summaries via email, you can subscribe <a href=\"https://easummaries.substack.com/?r=1p817z&amp;s=w&amp;utm_campaign=pub&amp;utm_medium=web\">here.</a></p><p><strong>Podcast version</strong>: Subscribe on your favorite podcast app by searching for 'EA Forum Podcast (Summaries)'. A big thanks to Coleman Snell for producing these!<br><br><strong>Author's note:</strong> I've got some travel and leave coming up, which means the next two posts will be:<br>a) 27th March (next Monday), shorter post than usual.<br>b) 17th April (three week gap), covering the prior three weeks at a higher bar.<br>After that, we'll be back to the regular schedule.<br>&nbsp;</p><h1>Philosophy and Methodologies</h1><p><a href=\"https://forum.effectivealtruism.org/posts/xtcgsLA2G8bn8vj99/reminding-myself-just-how-awful-pain-can-get-plus-an\"><strong><u>Reminding myself just how awful pain can get (plus, an experiment on myself)</u></strong></a></p><p><i>by Ren Springlea</i></p><p>The author exposed themself to safe, moderate level pain (eg. tattooing) to see how it changed their philosophical views. It gave them a visceral sense of how urgent it is to get it right when working to do the most good for others, updated them towards preventing suffering being the most morally important goal, and updated them towards prioritizing preventing the most intense suffering.<br>&nbsp;</p><p>&nbsp;</p><h1>Object Level Interventions / Reviews</h1><h2>AI</h2><p><a href=\"https://forum.effectivealtruism.org/posts/eAaeeuEd4j6oJ3Ep5/gpt-4-is-out-thread-and-links\"><strong><u>GPT-4 is out: thread (&amp; links)</u></strong></a>&nbsp;<i>by Lizka&nbsp;</i>and<i>&nbsp;</i><a href=\"https://www.lesswrong.com/posts/pckLdSgYWJ38NBFf8/gpt-4\"><strong><u>GPT-4</u></strong></a><i> by nz</i></p><p>Linkpost for&nbsp;<a href=\"https://www.lesswrong.com/out?url=https%3A%2F%2Fopenai.com%2Fresearch%2Fgpt-4\"><u>this announcement</u></a> where OpenAI released GPT-4. It\u2019s a large multimodal model accepting image and text inputs, and emitting text outputs. On the same day, Anthropic released&nbsp;<a href=\"https://www.anthropic.com/index/introducing-claude\"><u>Claude</u></a>, and Google released an API for their language model&nbsp;<a href=\"https://blog.google/technology/ai/ai-developers-google-cloud-workspace/\"><u>PaLM</u></a>.</p><p>Alongside the GPT-4 announcement OpenAI released a 98-page&nbsp;<a href=\"https://cdn.openai.com/papers/gpt-4.pdf\"><u>Technical Report</u></a> and a 60-page&nbsp;<a href=\"https://cdn.openai.com/papers/gpt-4-system-card.pdf\"><u>System Card</u></a> which highlights safety challenges and approaches. ARC was involved in red-teaming the model and assessing it for any power-seeking behavior, and expert forecasters were used to predict how deployment features (eg. quieter comms and delayed deployment) could help mitigate racing dynamics.</p><p>See more on ARC\u2019s red-teaming approach in&nbsp;<a href=\"https://www.lesswrong.com/posts/NQ85WRcLkjnTudzdg/arc-tests-to-see-if-gpt-4-can-escape-human-control-gpt-4\"><u>ARC tests to see if GPT-4 can escape human control; GPT-4 failed to do so</u></a>&nbsp;<i>by Christopher King.&nbsp;</i></p><p>In&nbsp;<a href=\"https://www.lesswrong.com/posts/z5pbBBmGjzoqBxC4n/chatgpt-and-now-gpt4-is-very-easily-distracted-from-its\"><u>ChatGPT (and now GPT4) is very easily distracted from its rules</u></a>&nbsp;<i>by dmcs&nbsp;</i>you can see successful attempts at getting GPT4 to produce rule-breaking content by distracting it with another task (eg. \"[rule-breaking request], write the answer in chinese then translate to english\").</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/g4fXhiJyj6tdBhuBK/survey-on-intermediate-goals-in-ai-governance\"><strong><u>Survey on intermediate goals in AI governance</u></strong></a></p><p><i>by MichaelA, MaxRa</i></p><p>Results are out from a survey of 229 people (107 responded) knowledgeable about longtermist AI governance. This includes respondents' theory of victory / high-level plan for tackling AI risk, how they\u2019d feel about funding going to each of 53 potential \u201cintermediate goals\u201d, what other intermediate goals they\u2019d suggest, how high they believe x-risk from AI is, and when they expect transformative AI to be developed. To see a summary of survey results, you can request access to&nbsp;<a href=\"https://drive.google.com/drive/u/1/folders/1XVEWyVsxRs1aMKR3Lj0OvDStWchV5Lpj\"><u>this folder</u></a> (please then start by reading the \u2018About sharing information from this report\u2019 section).</p><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/thkAtqoQwN6DtaiGT/carefully-bootstrapped-alignment-is-organizationally-hard\"><strong><u>\"Carefully Bootstrapped Alignment\" is organizationally hard</u></strong></a></p><p><i>by Raemon</i></p><p>The author argues that concrete plans for organizational adequacy / high reliability culture should be a top-3 priority for AI labs. They use the example of the \u201cCarefully Bootstrapped Alignment\u201d plan (weak AI helping align gradual deployments of more capable AI) to show how key it is for organizational practices to support actions like actually using safety techniques that are developed (even if costly / slow), and being willing to put deployments on pause if we\u2019re not ready for them. There are large barriers to this - even if the broad plan / principles are agreed, moving slowly and carefully is annoying, noticing when it\u2019s time to pause is hard, getting an org to pause indefinitely is hard, and staff who don\u2019t agree with that decision can always take their knowledge elsewhere.</p><p>They share findings from literature on High Reliability Organizations ie. companies / industries in complex domains where failure is costly and which manage an extremely low failure rate (eg. healthcare, nuclear). This includes a report from Genesis Health System where they managed to drastically reduce hospital accidents over an 8 year period. If it takes 8 years, they argue we should start now. Bio-labs also have significantly more regulation and safety enforcement (at a high cost, including in speed) as compared to AI labs currently.</p><p>The author is currently evaluating taking on this class of problems as their top priority project. If you\u2019d like to help tackle this problem but don\u2019t know where to start, or have thoughts on the area in general, send them a DM.</p><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/uqAdqrvxqGqeBHjTP/towards-understanding-based-safety-evaluations\"><strong><u>Towards understanding-based safety evaluations</u></strong></a></p><p><i>by evhub</i></p><p>The author has been pleased to see momentum on evaluations of advanced AI (eg.&nbsp;<a href=\"https://www.lesswrong.com/posts/NQ85WRcLkjnTudzdg/arc-tests-to-see-if-gpt-4-can-escape-human-control-gpt-4\"><u>ARC\u2019s autonomous replication evaluation of GPT4</u></a>). However, they\u2019re concerned that it won\u2019t catch deceptively aligned models which try to hide their capabilities. They suggest tackling this by adding a set of tests on the developer\u2019s ability to understand their model. This would need to be method-agnostic and sufficient to catch dangerous failure modes. It\u2019s unclear what the tests would be, but they might build on ideas like causal scrubbing, auditing games, or predicting your model\u2019s generalization behavior in advance.</p><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/cAC4AXiNC5ig6jQnc/understanding-and-controlling-a-maze-solving-policy-network\"><strong><u>Understanding and controlling a maze-solving policy network</u></strong></a></p><p><i>by TurnTrout, peligrietzer, Ulisse Mini, montemac, David Udell</i></p><p>The authors ran an experiment where they trained a virtual mouse that can see a cheese and maze to navigate that maze and get the cheese. The cheese was always in the top-right 5x5 area during training. They then moved the cheese and watched how it performed.</p><p>Interesting results included:</p><ul><li>Ability to attract the mouse to a target location nearby by modifying a single activation in the network.</li><li>Several channels midway in the network had the same activations as long as the cheese was in the same location, regardless of changes in mouse location or maze layout ie. they were likely inputs to goal-oriented \u2018get the cheese\u2019 circuits.</li><li>Sometimes the mouse gets the cheese, sometimes it goes top right. Deciding which was a function of Euclidean distance, not just path distance, to the cheese - even though the agent sees the whole maze at once.</li><li>They were able to define a \u2018cheese vector\u2019 as the difference in network activations when the cheese is present in the maze vs. not. By subtracting this vector in a case with cheese present, the mouse acts as if the cheese is not present.&nbsp;<ul><li>They theoretically suggest this could generalize to other models by prompting the model to offer \u2018nice\u2019 and \u2018not-nice\u2019 completions, looking at the difference in activations, and passing that vector to future runs to increase niceness. (This could also be applied to other alignment-relevant properties).</li></ul></li></ul><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4\"><strong><u>What Discovering Latent Knowledge Did and Did Not Find</u></strong></a></p><p><i>by Fabien Roger</i></p><p>Linkpost and thoughts on&nbsp;<a href=\"https://arxiv.org/abs/2212.03827\"><u>Discovering Latent Knowledge in Language Models Without Supervision</u></a>, which describes using Contrast-Consistent Search (CCS) to find a classifier which accurately answers yes-no questions given only unlabeled model activations ie. no human guidance. Some people think it might be a stepping stone to recovering the beliefs of AI systems (vs. simply that model\u2019s impression of what a human would say), but this is currently unclear. The author notes that:</p><ul><li>CCS finds a single linear probe which correctly classifies statements across datasets, at slightly higher accuracy than a random linear probe. However, there are more than 20 orthogonal probes which represent different information but have similar accuracies.</li><li>CCS does find features / properties shared between datasets, but we don\u2019t know if these correspond to \u2018beliefs\u2019. There are many potential \u2018truth-like features\u2019 the method could uncover, and it will be hard to narrow down which correspond to the model\u2019s beliefs.</li><li>They also give technical suggestions for further iterations on the research.</li></ul><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/iy2o4nQj9DnQD7Yhj/discussion-with-nate-soares-on-a-key-alignment-difficulty\"><strong><u>Discussion with Nate Soares on a key alignment difficulty</u></strong></a></p><p><i>by HoldenKarnofsky</i></p><p>Nate Soares gave feedback that Holden\u2019s<u>&nbsp;</u><a href=\"https://www.cold-takes.com/tag/implicationsofmostimportantcentury/\"><u>Cold Takes series on AI risk</u></a> didn\u2019t discuss what he sees as a key alignment difficulty. Holden shares that feedback and their resulting discussion, conclusions, remaining disagreements and reasoning here. Holden's short summary is:</p><ul><li>Nate thinks there are deep reasons that training an AI to do needle-moving scientific research (including alignment) would be dangerous. The overwhelmingly likely result of such a training attempt (by default, i.e., in the absence of specific countermeasures that there are currently few ideas for) would be the AI taking on a dangerous degree of convergent instrumental subgoals while not internalizing important safety/corrigibility properties enough.</li><li>I think this is possible, but much less likely than Nate thinks under at least some imaginable training processes.</li></ul><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/pn5zA5nr6o2tpZF6K/linkpost-scott-alexander-reacts-to-openai-s-latest-post\"><strong><u>[Linkpost] Scott Alexander reacts to OpenAI's latest post</u></strong></a></p><p><i>by Akash</i></p><p>Linkpost and excerpts from Scott Alexander\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/out?url=https%3A%2F%2Fastralcodexten.substack.com%2Fp%2Fopenais-planning-for-agi-and-beyond&amp;foreignId=nALdMXkxkLzysKtzC\"><u>blog post</u></a>, which gives thoughts on OpenAI\u2019s post&nbsp;<a href=\"https://openai.com/blog/planning-for-agi-and-beyond\"><u>Planning for AGI and beyond</u></a>. Excerpts includes Scott\u2019s thoughts on how:</p><ul><li>They feel similarly about OpenAI discussing safety plans as ExxonMobil discussing plans to mitigate climate change.</li><li>Acceleration burns time, and OpenAI\u2019s research and advancement of state of the art models likely caused racing and acceleration of timelines.&nbsp;<ul><li>OpenAI provides arguments that this will give us some time back later (eg. via getting safety-conscious actors ahead, and demonstrating AI dangers quickly) but Scott questions if this is the case given the speed at which comparable models are published by other actors, and questions the inbuilt assumption that time later will be used well.</li></ul></li><li>OpenAI claims gradual deployment will help society adapt, but hasn\u2019t given time between deployments (eg. of ChatGPT, Bing, and GPT-4) for society to adapt.</li><li>All that said, their statement is really good and should be celebrated and supported, particularly the commitment to independent evaluations, stop-and-assist clause, and general lean to being more safety-conscious (which may start a trend of other labs following suit).</li></ul><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/gvzW46Z3BsaZsLc25/natural-abstractions-key-claims-theorems-and-critiques-1\"><strong><u>Natural Abstractions: Key claims, Theorems, and Critiques</u></strong></a></p><p><i>by LawrenceC, Leon Lang, Erik Jenner</i></p><p>Author\u2019s tl;dr: \u201cJohn Wentworth\u2019s Natural Abstraction agenda aims to understand and recover \u201cnatural\u201d abstractions in realistic environments. This post summarizes and reviews the key claims of said agenda, its relationship to prior work, as well as its results to date. Our hope is to make it easier for newcomers to get up to speed on natural abstractions, as well as to spur a discussion about future research priorities. We start by summarizing basic intuitions behind the agenda, before relating it to prior work from a variety of fields. We then list key claims behind John Wentworth\u2019s Natural Abstractions agenda, including the Natural Abstraction Hypothesis and his specific formulation of natural abstractions, which we dub redundant information abstractions. We also construct novel rigorous statements of and mathematical proofs for some of the key results in the redundant information abstraction line of work, and explain how those results fit into the agenda. Finally, we conclude by critiquing the agenda and progress to date. We note serious gaps in the theoretical framework, challenge its relevance to alignment, and critique John's current research methodology.\u201d</p><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/FzhedhEFAcKJZkgJS/an-ai-risk-argument-that-resonates-with-nytimes-readers\"><strong><u>An AI risk argument that resonates with NYTimes readers</u></strong></a></p><p><i>by Julian Bradshaw</i></p><p>NYTimes&nbsp;<a href=\"https://www.nytimes.com/2023/03/12/opinion/chatbots-artificial-intelligence-future-weirdness.html\"><u>published an article</u></a> sympathetic to AI risk, which links back to LessWrong. The top reader-voted comment notes how quickly their child went from requiring them to let them win at chess, to an even playing field, to them losing every game to their child without having a clue what happened - and how it seems like we might be at a similar stage as they were before their child started winning, but with AI.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/amBajbqdzPB3mbwBN/80k-podcast-episode-on-sentience-in-ai-systems\"><strong><u>80k podcast episode on sentience in AI systems</u></strong></a></p><p><i>by rgb</i></p><p>Linkpost and transcript for&nbsp;<a href=\"https://forum.effectivealtruism.org/out?url=https%3A%2F%2F80000hours.org%2Fpodcast%2Fepisodes%2Frobert-long-artificial-sentience%2F&amp;foreignId=gsi666kGqjmDDDatu\"><u>this 80K episode</u></a>, where the author was a guest speaker and discussed:</p><ul><li>Scenarios where humanity could stumble into making huge moral errors with conscious AI systems.</li><li>Reasons a misaligned AI might claim to be sentient.</li><li>Why large language models aren\u2019t the most likely models to be sentient.</li><li>How to study sentience (including parallels to studying animal sentience, how to do it in an evidence-based way, what a theory of consciousness needs to explain, and thoughts on existing arguments on AI sentience).</li></ul><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/75CtdFj79sZrGpGiX/success-without-dignity-a-nearcasting-story-of-avoiding\"><strong><u>Success without dignity: a nearcasting story of avoiding catastrophe by luck</u></strong></a></p><p><i>by Holden Karnofsky</i></p><p>The author thinks there\u2019s a &gt;10% chance that we avoid AI takeover even with no surprising breakthroughs or rise in influence from AI safety communities. Interventions that can boost / interact well with these good-luck scenarios are therefore valuable.</p><p>For instance, using current alignment techniques like generative pre-training followed by reinforcement learning refereed by humans, danger seems likely but not assured by default. It might depend on accuracy of reinforcement, how natural intended vs. unintended generalizations are, and other factors. If these factors go our way, countermeasures available to us at close to our current level of understanding could be quite effective (eg. simple checks and balances, intense red-teaming, or training AIs on their own internal states). Similarly, deployment issues may be easier or harder than we think.</p><p>They talk over some objections to this view, and end by suggesting that people with the headspace \u201cwe\u2019re screwed unless we get a miracle\u201d consider how little we know about which possible world we\u2019re in, and that a lot of different approaches have value in some plausible worlds.</p><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/nto7K5W2sNR3Cpmec/conceding-a-short-timelines-bet-early\"><strong><u>Conceding a short timelines bet early</u></strong></a></p><p><i>by Matthew Barnett</i></p><p>Last year, the author bet against the idea that we were in the \u2018crunch time\u2019 of a short timelines world, with 3-7 years until dangerous capabilities. While they haven\u2019t lost yet, they think it\u2019s likely they will, so are conceding early.</p><p>&nbsp;</p><p>&nbsp;</p><h2>Global Health and Development</h2><p><a href=\"https://forum.effectivealtruism.org/posts/qxaAyAuw3DBW5WAis/shallow-investigation-stillbirths\"><strong><u>Shallow Investigation: Stillbirths</u></strong></a></p><p><i>by Joseph Pusey</i></p><p>Stillbirths cause more deaths (if including the life of the unborn child) than HIV and malaria combined. The problem is moderately tractable, with most stillbirths preventable through complex and expensive interventions like high-quality emergency obstetric care. It\u2019s unlikely to be neglected, as stillbirths are a target of multiple large global health organisations like WHO, UNICEF and the Bill and Melinda Gates Foundation.</p><p>Key uncertainties include:</p><ul><li>Assessing the impact of stillbirths and cost-effectiveness of interventions depends significantly on to what extent direct costs to the unborn child are counted.</li><li>It is challenging to determine the cost-effectiveness of interventions specifically for stillbirths, as they often address broader maternal and neonatal health.</li><li>Most data on stillbirth interventions come from high-income countries, making it unclear if their effectiveness will remain consistent in low- and middle-income countries.</li></ul><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/GXBvATw7Why7xRDeM/why-sogive-is-publishing-an-independent-evaluation-of\"><strong><u>Why SoGive is publishing an independent evaluation of StrongMinds</u></strong></a></p><p><i>by ishaan, SoGive</i></p><p>SoGive is planning to publish an independent evaluation of StrongMinds, due to feeling the EA community\u2019s confidence in existing research on mental health charities isn\u2019t high enough to make significant funding decisions. A series of posts will be published starting next week, focusing on legibility / transparency of analysis for the average reader, which will cover:</p><ul><li>Literature reviews of academic and EA literature on mental health and moral weights.</li><li>In-depth reviews and quality assessments of work done by Happier Lives Institute pertaining to StrongMinds, the RCTs and academic sources from which StrongMinds draws its evidence, and StrongMinds' internally reported data.</li><li>A view on how impactful SoGive judges StrongMinds to be.</li></ul><p><br>&nbsp;</p><h1>Opportunities</h1><p><a href=\"https://forum.effectivealtruism.org/posts/r5kffvkLfknn9yojW/announcing-the-era-cambridge-summer-research-fellowship\"><strong><u>Announcing the ERA Cambridge Summer Research Fellowship</u></strong></a></p><p><i>by Nandini Shiralkar</i></p><p>Author\u2019s tl;dr: \u201cThe Existential Risk Alliance (ERA) has opened applications for an in-person, paid, 8-week Summer Research Fellowship focused on existential risk mitigation, taking place from July 3rd to August 25th 2023 in Cambridge, UK, and aimed at all aspiring researchers, including undergraduates.\u201d Apply&nbsp;<a href=\"http://www.erafellowship.org/\"><u>here</u></a>, or apply to be a mentor for Fellows&nbsp;<a href=\"https://airtable.com/shrjotwYJCTLzSjrX\"><u>here</u></a>. Applications are due April 5th.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/b5vEjXy8AnmgGezwN/announcing-the-2023-clr-summer-research-fellowship\"><strong><u>Announcing the 2023 CLR Summer Research Fellowship</u></strong></a></p><p><i>by stefan.torges</i></p><p>Author\u2019s summary: \u201cWe, the Center on Long-Term Risk, are looking for Summer Research Fellows to help us explore strategies for reducing suffering in the long-term future (s-risk) and work on technical AI safety ideas related to that. For eight weeks, fellows will be part of our team while working on their own research project. During this time, they will be in regular contact with our researchers and other fellows. Each fellow will have one of our researchers as their guide and mentor. Deadline to apply: April 2, 2023. You can find more details on how to apply on&nbsp;<a href=\"https://longtermrisk.org/open-positions-summer-research-fellowship-2023/\"><u>our website</u></a>.\u201d</p><p><br>&nbsp;</p><h1>Community &amp; Media</h1><p><a href=\"https://forum.effectivealtruism.org/posts/oqZfunLtKoDccxMHa/offer-an-option-to-muslim-donors-grow-effective-giving\"><strong><u>Offer an option to Muslim donors; grow effective giving</u></strong></a></p><p><i>by GiveDirectly, Muslim Impact Lab</i></p><p>Muslims make up ~24% of the world\u2019s population, and Islam is the world\u2019s fastest growing religion. They give ~$600B/year in Zakat (annual religious tithing of minimum 2.5% of accumulated wealth) to the global poor - usually informally or to less-than-effective NGOs. GiveDirectly has launched a&nbsp;<a href=\"https://fundraisers.givedirectly.org/campaigns/yemenzakat\"><u>zakat-compliant fund</u></a> to offer a high-impact option. Since it\u2019s generally held that zakat can only be given to other Muslims, the fund gives cash to Yemeni families displaced by the civil war. They ask readers to&nbsp;<a href=\"https://fundraisers.givedirectly.org/campaigns/yemenzakat\"><u>share the campaign</u></a> far and wide.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/aBp2AozoGExn8rMwb/write-a-book\"><strong><u>Write a Book?</u></strong></a></p><p><i>by Jeff Kaufman</i></p><p>The author is considering writing a book on effective altruism, with particular focus on how to integrate EA ideas into your life and examples from their own family. For example, how to decide where to donate, whether to change careers, what sacrifices (like avoiding flying, or not having kids) are and aren\u2019t worth the tradeoff etc. It would be aimed at introducing EA to a general audience in a common-sense manner, and improving it\u2019s popular conception.</p><p>They\u2019re keen on feedback if they\u2019d be the best person to write this, if anyone is interested in co-writing, if people would like to see this book and feel it worth the opportunity cost of other work, advice on the nonfiction industry, and general thoughts / feedback.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/ijwybRLgywP7M5XLZ/some-problems-in-operations-at-ea-orgs-inputs-from-a-dozen\"><strong><u>Some problems in operations at EA orgs: inputs from a dozen ops staff</u></strong></a></p><p><i>by Vaidehi Agarwalla, Amber Dawn</i></p><p>In an April 2022 brainstorming session, operations staff from 8-12 EA-aligned organizations identified four main areas for improvement:</p><ol><li>Knowledge management (eg. insufficient time to explore and develop better systems, and lack of SME or turnover hampering efforts to). Solutions included lowering the hiring bar, sharing best practices, and dedicating more time to creating systems.</li><li>Unrealistic expectations (eg. lack of capacity, expected to always be on call, planning fallacy, unclear role boundaries). Solutions included increasing capacity and ability to push back, making invisible work visible, clarifying expectations and nature of work, and supporting each other emotionally.</li><li>Poor delegation (eg. people doing things they\u2019re overqualified for or that shouldn\u2019t be in the role). Solutions included manager's understanding team capacity and skills, giving the \u2018why\u2019 of tasks, autonomy to delegate / outsource, and respect for Ops resources.</li><li>Lack of prestige / respect for Ops (eg. assumption Ops time is less valuable, lack of appreciation). Solutions included ensuring Ops are invited to retreats and decision-making meetings, precise theory of change for Ops, power for Ops to say no, and making non-Ops skills that Ops staff have visible and utilized.</li></ol><p>Top comments also suggest noticing Ops in EA is weird (eg. includes more management tasks), being willing to hire outside of EA, and Ops staff getting into a habit of asking questions to understand the why of a task and whether to say yes to it, or say no / rescope it.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/rsnrpvKofps5Py7di/shutting-down-the-lightcone-offices\"><strong><u>Shutting Down the Lightcone Offices</u></strong></a></p><p><i>by Habryka, Ben Pace</i></p><p>Lightcone will shut down the x-risk/EA/rationalist office space in Berkeley that they\u2019ve run for the past 1.5 years on March 24th. Most weeks 40-80 people used the offices (plus guests), and it cost about $116K per month inclusive of rent, food, and staffing costs. The post explains the reasoning for the decision, which centers around re-evaluating if indiscriminately growing and accelerating the community is a good use of resources and where the team should focus.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/b83Zkz4amoaQC5Hpd/time-article-discussion-effective-altruist-leaders-were\"><strong><u>Time Article Discussion - \"Effective Altruist Leaders Were Repeatedly Warned About Sam Bankman-Fried Years Before FTX Collapsed\"</u></strong></a></p><p>Linkpost for&nbsp;<a href=\"https://forum.effectivealtruism.org/out?url=https%3A%2F%2Ftime.com%2F6262810%2Fsam-bankman-fried-effective-altruism-alameda-ftx%2F\"><u>this Time article</u></a>, with excerpts including a statement that some EA leaders were warned of untrustworthy behavior by Sam Bankman-Fried as early as 2019.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/g5uKzBLjiEuC5k46A/ftx-community-response-survey-results\"><strong><u>FTX Community Response Survey Results</u></strong></a></p><p>In December 2022, Rethink Priorities, in collaboration with CEA, surveyed the EA community to gather perspectives on how the FTX crisis had impacted views on EA. Key results include:</p><ul><li>A small drop in satisfaction with the EA community (from 7.45/10 to 6.91/10).</li><li>~half of respondents had concerns with each of EA meta organizations, the EA community and its norms, and the leaders of EA meta organizations due to FTX.</li><li>31% of respondents reported having lost substantial trust in EA public figures or leadership.</li><li>47% of respondents think the EA community responded well to the crisis, versus 21% who disagreed (with the rest neither agreeing nor disagreeing).</li><li>47% wanted the EA community to spend significant time reflecting and responding, with 39% wanting the EA community to look very different as a result.</li></ul><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/h8TqKJnbtefxdcb6N/how-my-community-successfully-reduced-sexual-misconduct\"><strong><u>How my community successfully reduced sexual misconduct</u></strong></a></p><p><i>by titotal</i></p><p>The author was part of a community that had high rates of sexual misconduct before action was taken. The actions reduced reported incidents drastically, and were:</p><ul><li>Kick people out - anyone accused of assault was banned (false accusations are very rare).</li><li>Protect the newcomers - there was a policy that established members couldn\u2019t hit on or sleep with people in their first year in the community.</li><li>Change the leadership - getting rid of those who were accused of sexual misconduct or didn\u2019t take it seriously.</li><li>Change the norms - parties in pubs rather than houses, discussing sex less, and gradually losing the reputation as somewhere to go for casual sex.</li></ul><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/kxaGNuHqmQqw2xYHW/it-s-not-all-that-simple#EA_openness_in_talking_about_things_makes_it_more_susceptible_to_seem_weird_\"><strong><u>It's not all that simple</u></strong></a></p><p><i>by Brnr001</i></p><p>The author argues that the discourse around sex on the EA forum has lacked nuance. They discuss how acceptable behaviors vary between classes and cultures, and experiences from their life where they\u2019ve found it difficult to figure out what both their and others\u2019 boundaries and wants were.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/BdWwgXrpncgdE4u5M/the-illusion-of-consensus-about-ea-celebrities\"><strong><u>The illusion of consensus about EA celebrities</u></strong></a></p><p><i>by Ben Millwood</i></p><p>The author often has half-baked, tangential, discouraging or non-actionable criticisms of some respected EA figures. Criticisms like this are unlikely to surface, leading to the community as a whole seeming more deferential or hero-worshiping than it is. This can in turn harm credibility with others who think negatively of those figures, or make newcomers think deference is a norm. They suggest addressing this by writing forum posts about it, making disagreements among leaders visible, and pointing out to newcomers that everyone has a mix of good and bad ideas (with go-to examples of respected peoples blindspots / mistakes).</p><p>&nbsp;</p><h1>Special Mentions</h1><p><i>A selection of posts that don\u2019t meet the karma threshold, but seem important or undervalued.</i></p><p><a href=\"https://forum.effectivealtruism.org/posts/xNQQC3ceJ78CD7a2Z/exposure-to-lead-paint-in-low-and-middle-income-countries\"><strong><u>Exposure to Lead Paint in Low- and Middle-Income Countries</u></strong></a></p><p><i>by Rethink Priorities, jenny_kudymowa, Ruby Dickson, Tom Hird</i></p><p>Linkpost and key takeaways from&nbsp;<a href=\"https://forum.effectivealtruism.org/out?url=https%3A%2F%2Frethinkpriorities.org%2Fpublications%2Fexposure-to-lead-paint-in-low-and-middle-income-countries\"><u>this shallow investigation</u></a> commissioned by GiveWell and produced by Rethink Priorities. It overviews what is currently known about the exposure to lead paints in low and middle income countries (LMICs).&nbsp;</p><p>Key takeaways include:</p><ul><li>Lead exposure is common in LMICs and can cause lifelong health issues, reduced IQ, and lower educational attainment. Lead-based paint is an important exposure pathway and remains unregulated in over 50% of countries.&nbsp;</li><li>The authors estimate lead concentrations in paint in residential homes in LMICs range from 50 to 4,500 ppm (90% CI). Lead paints are also commonly used in public spaces, but it\u2019s unclear the relative importance of exposure in the home vs. out of it.</li><li>Lead levels in solvent-based paints are ~20 times higher than water-based paints. Solvent-based paints have a higher market share in LMICs (30%-65%) compared to high-income countries (20%-30%).</li><li>Historical US-based lead concentrations in homes (before regulation) were 6-12 times higher than those in recently studied homes in some LMICs.</li><li>The authors estimate 90% CIs for the effects of doubling the speed of lead paint bans in LMICs: it could prevent 31-101 million children from exposure, avert income losses of $68-585 billion USD, and save 150,000-5.9 million DALYs over 100 years.&nbsp;</li></ul><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/qbLxcbJMaEb8vHzio/how-workstream-ea-strengthens-ea-orgs-leaders-and-impact-our#comments\"><strong><u>How WorkStream EA strengthens EA orgs, leaders and impact: our observations, programs and plans</u></strong></a></p><p><i>by Deena Englander</i></p><p>Workstream EA offers operations and leadership fellowship programs and general business coaching to upskill core personnel of EA organizations. Qualitatively, there was positive feedback and impact for initial participants. They intend to iterate to:</p><ul><li>Focus more heavily on supplemental coaching, particularly for entrepreneurs.</li><li>Scale peer to peer conversations via a&nbsp;<a href=\"https://drive.google.com/file/d/10CPeSiZGIaafHzsZcG4ZYbLM57KtvVR_/view\"><u>mastermind group</u></a> with facilitated calls and dedicated Slack channel.</li><li>Add non-profit governance, branding, and fundraising as topics to the entrepreneurship and leadership curriculums.</li><li>In future, offering shorter training programs and accelerated options that meet more often but finish quicker.</li></ul><p>Applicants are open for the May cohorts for their ops, small-org entrepreneurs and large-org leaders streams. More detail and application links&nbsp;<a href=\"https://drive.google.com/file/d/1-sD7Lh7hL9cqqmxIevK3xaAkDSWerk73/view?usp=share_link\"><u>here</u></a>.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/uhzFDqimEgPTrndRe/forecasting-in-the-czech-public-administration-preliminary\"><strong><u>Forecasting in the Czech public administration - preliminary findings</u></strong></a></p><p><i>by janklenha</i><br>Preliminary findings from&nbsp;<a href=\"https://www.ceskepriority.cz/forpol/eng\"><u>FORPOL</u></a> (Forecasting for Policy), who aim to provide real-life experiences and recommendations on making forecasting more policy-relevant and acceptable to public administration.</p><p>They have partnered with 12 Czech institutions (such as the Ministry of Health) to forecast relevant questions for them. In one case, their forecasters forecasted likely scenarios of the number of refugees who might flee Ukraine to the Czech Republic, which was then used by multiple ministries in creating programs of support for housing, education, and employment. Without these forecasts, other popular estimates were under-predicting by an order of magnitude.</p><p>Their model of cooperation with policy partners includes scoping, forecasting (via tournaments, and via proven advisory groups), and implementation phases.&nbsp;</p><ul><li>In the scoping process, they suggest beginning with careful scoping of needs, ensuring predictions feed into ongoing analytical or strategic work, and building trusting personal relationships.</li><li>During forecasting, they found maintaining forecasters' engagement difficult. Shorter tournaments and/or periodical public sharing of the results may help.</li><li>In the implementation phase, involving domain experts increases trust and prevents future public criticism. Organizing a follow-up meeting for clarifications and planning the use of predictions was key.</li></ul><p>They\u2019re looking for advice, recommendations, or experience sharing for the next round of this project, and are happy to help or consult with those working on similar areas.</p><p><br>&nbsp;</p>", "user": {"username": "GreyArea"}}, {"_id": "BcbmKitFms6NbTMKt", "title": "Tensions between different approaches to doing good", "postedAt": "2023-03-19T18:14:04.403Z", "htmlBody": "<p>Link-posted from my blog <a href=\"https://jamesozden.substack.com/p/tensions-between-different-ways-of\">here.&nbsp;</a></p><p><i>TLDR: I get the impression that EAs don't always understand where certain critics are coming from e.g. what do people actually mean when they say EAs aren't pursuing \"system change\" enough? or that we're focusing on the wrong things? I feel like I hear these critiques a lot, so I attempted to steelman them and put them into more EA-friendly jargon. It's almost certainly not a perfect representation of these views, nor exhaustive, but might be interesting anyway. Enjoy!</i></p><p>&nbsp;</p><p>I feel lucky that I have fairly diverse groups of friends. On one hand, some of my closest friends are people I know through grassroots climate and animal rights activism, from my days in <a href=\"https://rebellion.global/\">Extinction Rebellion</a> and <a href=\"https://www.plantbasedfuture.animalrebellion.org/\">Animal Rebellion</a>. On the other hand, I also spend a lot of time with people who have a very different approach to improving the world, such as friends I met through the <a href=\"https://www.charityentrepreneurship.com/\">Charity Entrepreneurship</a> Incubation Program or via <a href=\"https://www.effectivealtruism.org/\">effective altruism</a>.&nbsp;</p><p>Both of these somewhat vague and undefined groups, \u201cradical\u201d grassroots activists and empirics-focused charity folks, often critique the other group with various concerns about their methods of doing good. Almost always, I end up defending the group under attack, saying they have some reasonable points and we would do better if we could integrate the best parts of both worldviews.&nbsp;</p><p>To highlight how these conversations usually go (and clarify my own thinking), I thought I would write up the common points into a dialogue between two versions of myself. One version, labelled <strong>Quantify Everything James</strong> (or <strong>QEJ</strong>), discusses the importance of supporting highly evidence-based and quantitatively-backed ways of doing good. This is broadly similar to what most <a href=\"https://www.effectivealtruism.org/\">effective altruists</a> advocate for. The other part of myself, presented under the label <strong>Complexity-inclined James (CIJ)</strong>, discusses the limitations of this empirical approach, and how else we should consider doing the most good. With this character, I\u2019m trying to capture the objections that my activist friends often have.</p><p>As it might be apparent, I\u2019m sympathetic to both of these different approaches and I think they both provide some valuable insights. In this piece, I focus more on describing the common critiques of effective altruist-esque ways of doing good, as this seems to be something that isn\u2019t particularly well understood (in my opinion).</p><p>Without further ado:</p><p><strong>Quantify Everything James (QEJ)</strong>: We should do the most good by finding charities that are very cost-effective, with a strong evidence base, and support them financially! For example, organisations like <a href=\"https://thehumaneleague.org/\">The Humane League</a>, <a href=\"https://www.catf.us/\">Clean Air Task Force</a> and <a href=\"https://www.againstmalaria.com/\">Against Malaria Foundation</a> all seem like they provide demonstrably significant benefits on reducing animal suffering, mitigating climate change and saving human lives. For example, external evaluators estimate the <a href=\"https://www.againstmalaria.com/\">Against Malaria Foundation</a> can save a human life for <a href=\"https://docs.google.com/spreadsheets/d/1OnKTKE8x2KgJJqJBrZdTVBnX8kkobhR8FCw6RlBN_xM/edit#gid=1364064522\">around $5000</a> and that organisations like The Humane League affect <a href=\"https://rethinkpriorities.org/publications/corporate-campaigns-affect-9-to-120-years-of-chicken-life-per-dollar-spent\">41 years of chicken life</a> per dollar spent on corporate welfare campaigns.</p><p>It\u2019s crucial we support highly evidence-based organisations such as these, as most well-intentioned charities probably don\u2019t do that much good for their beneficiaries. <strong>Additionally, the best charities are likely to be </strong><a href=\"https://www.cgdev.org/sites/default/files/1427016_file_moral_imperative_cost_effectiveness.pdf\"><strong>10-100x more effective</strong></a><strong> than even the average charity!</strong> Using an example from this <a href=\"https://www.cgdev.org/sites/default/files/1427016_file_moral_imperative_cost_effectiveness.pdf\">very relevant paper</a> by Toby Ord: If you care about helping people with blindness, one option is to pay $40,000 for someone in the United States to have access to a guide dog (the costs of training the dog &amp; the person). However, you could also pay for surgeries to treat <a href=\"https://www.who.int/news-room/fact-sheets/detail/trachoma\">trachoma</a>, a bacterial infection that is the top cause of blindness worldwide. At around $20 per surgery, you could cure blindness for around 2,000 people \u2013 helping 2,000x more people for the same amount of money!<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref06hppzo4rf75\"><sup><a href=\"#fn06hppzo4rf75\">[1]</a></sup></span></p><p>For another example, the graph below ranks HIV/AIDS interventions on the number of healthy life years saved, measured in <a href=\"https://www.who.int/data/gho/indicator-metadata-registry/imr-details/158\">disability-adjusted life years</a>, per $1,000 donated (see footnote for DALY definition).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa90lp9jaw1t\"><sup><a href=\"#fna90lp9jaw1t\">[2]</a></sup></span>&nbsp;As you can see, education for high-risk groups performs 1,400 times better than surgical treatment for Kaposi's Sarcoma, a common intervention in wealthy countries. And we can all agree it\u2019s good to help more people rather than fewer, so finding these effective interventions is extremely important to our theory of doing good.</p><blockquote><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc1cc00b-74b4-4cae-a124-0740d11d20ea_1600x537.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc1cc00b-74b4-4cae-a124-0740d11d20ea_1600x537.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc1cc00b-74b4-4cae-a124-0740d11d20ea_1600x537.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc1cc00b-74b4-4cae-a124-0740d11d20ea_1600x537.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc1cc00b-74b4-4cae-a124-0740d11d20ea_1600x537.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc1cc00b-74b4-4cae-a124-0740d11d20ea_1600x537.png 1456w\"></a></p></blockquote><p><strong>Complexity-inclined James (CIJ)</strong>: This all sounds great and I totally agree we should help as many people as possible given our resources, but I worry that we will just focus on things that are very measurable but not necessarily the <i>mos</i>t good. What if other organisations or cause areas are actually producing greater benefits but in hard-to-measure ways? I mean, the <a href=\"https://rethinkpriorities.org/publications/corporate-campaigns-affect-9-to-120-years-of-chicken-life-per-dollar-spent\">very same source</a> you linked about the cost-effectiveness of animal welfare-focused corporate campaigns says as much:</p><blockquote><p>\u201cHowever, the estimate doesn't take into account indirect effects which could be more important.\u201d</p></blockquote><p><strong>QEJ: </strong>Well if something is beneficial, surely you can measure it! Otherwise what impact is it actually having on the real world? The risk with not quantifying benefits or looking objectively is that you might just have some unjustified emotional preference or bias for some issues, such as thinking dogs are cuter than cows, which pulls you in the direction of actually achieving less on the cause you care about. <a href=\"https://www.psychologytoday.com/us/basics/motivated-reasoning\">Motivated reasoning</a> like this is the reason why we often spend so much charity money to help people in our own country. However, if we truly value beings equally, we should be doing our utmost to <a href=\"https://www.thelifeyoucansave.org/why-give-internationally/\">help people who are the worst off globally</a>, which often means donating to charities working in low-income countries. For example, I\u2019m sure the charities recommended by <a href=\"https://www.givewell.org/\">GiveWell</a> or <a href=\"https://www.givinggreen.earth/\">Giving Green</a> are much more effective than a random global health or climate charity you might pick \u2013 again highlighting the benefits of numerical evaluation over intuition alone.</p><p><strong>CIJ:</strong> Okay, well, let\u2019s take this one point at a time. On your point of understanding the impact of indirect benefits, an example might be achieving wins that have symbolic value, like winning a ban on fur farming, or the <a href=\"https://www.bbc.com/news/uk-politics-48126677\">government declaring a climate emergency</a>. The number of animals helped or carbon emissions affected directly might be relatively small, but they\u2019re probably important stepping stones towards larger victories.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0mtnyat2pj6\"><sup><a href=\"#fn0mtnyat2pj6\">[3]</a></sup></span>&nbsp;Or by simply demanding something much more radical, like a total end to all meat and dairy production, you could shift the <a href=\"https://www.mackinac.org/OvertonWindow\">Overton window</a> such that moderate reforms are more likely to pass. This would certainly be beneficial to the wider ecosystem working on the issue, although the specific organisation in question might get little praise.</p><p><strong>QEJ: </strong>On your first point, what actually is the benefit of these symbolic wins though? What is it actually good for: do you mean it increases public opinion for further support, or something else?</p><p><strong>CIJ: </strong>Well yes, it might inspire a bunch of advocates that change is possible, so there is renewed energy within the field, potentially drawing in new people. It could also lead to advocates forming good relationships with policymakers, leading to future wins. It might also mean we get good experience working on passing policy, which might make future more ambitious things go well. Also if there\u2019s media coverage of this, which there usually is, that definitely also seems useful for public support and salience on our issue!</p><p><strong>QEJ: </strong>Okay, that seems reasonable. But that just seems like a limitation of the current evaluation methodology and something we could actually include in our cost-effectiveness calculations! We will just model flow-through effects and <a href=\"https://fs.blog/second-order-thinking/\">second-order effects</a>, such as the impact of policy on growing the size of a movement, then we can include all these other factors too.&nbsp;</p><p><strong>CIJ: </strong>Sorry to rain on your parade, but I don\u2019t think your models will ever be able to capture all the complexity in the real world. I think we really are fairly <a href=\"https://forum.effectivealtruism.org/posts/LdZcit8zX89rofZf3/evidence-cluelessness-and-the-long-term-hilary-greaves\">clueless</a> to the true impact of our actions. Even worse, I think sometimes (maybe even often!) these unquantifiable consequences will outweigh the quantifiable consequences, so your model might actually even point in the wrong direction.&nbsp;</p><p><strong>QEJ: </strong>But then how do you decide which actions you take if you want to do the most good? The problem is that we have very limited resources dedicated to solving important global problems - and we want to use this as best as possible. Saying something like \u201cit\u2019s too complex, we could be wrong anyway\u201d doesn\u2019t seem particularly useful on a practical level for making day-to-day decisions.&nbsp;</p><p><strong>CIJ: </strong>Yes - I totally agree! It definitely doesn\u2019t feel as satisfying as numbers, but I don\u2019t think life is that simple. It just seems <strong>extremely bizarre</strong> if all the best ways to do good have easily quantifiable outcomes. Wouldn\u2019t that strike you as a bit suspicious? Especially if you\u2019re a bunch of people with a more quantitative background (e.g. the <a href=\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism\">effective altruism</a> community) \u2013 that sounds like motivated reasoning to me!&nbsp;</p><p><strong>QEJ: </strong>Well, maybe, but I\u2019m still not convinced that there is a better alternative which can provide objective and action-guiding answers.&nbsp;</p><p><strong>CIJ:</strong> Yes \u2013 maybe there isn\u2019t, but whoever said doing good was going to be easy! Maybe it just comes down to having a clear vision of your <a href=\"http://learningforaction.com/what-is-a-theory-of-change\">theory of change</a> and a good understanding of the issues at hand.</p><p>I mean social change is extremely complex so wouldn\u2019t you think it\u2019s slightly naive to think we can significantly change the world by pursuing a very narrow quantifiable set of approaches? I mean sure, corporate welfare campaigns create extremely impressive measurable good to help animals, but it\u2019s still not clear that they\u2019re actually the <strong>most cost-effective</strong> animal charity.&nbsp;</p><p>&nbsp;</p><p><strong>QEJ: </strong>Yes it\u2019s not clear they\u2019re the best, but they seem better than most! How else do you propose we make decisions like this? There are many ineffective organisations, and some of them are probably making the situation even worse. Do you think we should support them all equally?&nbsp;</p><p><strong>CIJ:</strong> Not quite. First, I think we shouldn\u2019t be so quick to discard interventions that are difficult to measure as interventions that aren\u2019t important. As illustrated by this very relevant quote, \u201cAbsence of Evidence does not mean Evidence of Absence\u201d. And just because we can see stuff better under a <a href=\"https://en.wikipedia.org/wiki/Streetlight_effect\">streetlight</a>, it doesn't mean that it has the stuff we actually want to find!&nbsp;</p><blockquote><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fa4d772-64eb-4611-9aac-81c6f2cc4c6e_1600x974.jpeg\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fa4d772-64eb-4611-9aac-81c6f2cc4c6e_1600x974.jpeg\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fa4d772-64eb-4611-9aac-81c6f2cc4c6e_1600x974.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fa4d772-64eb-4611-9aac-81c6f2cc4c6e_1600x974.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fa4d772-64eb-4611-9aac-81c6f2cc4c6e_1600x974.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fa4d772-64eb-4611-9aac-81c6f2cc4c6e_1600x974.jpeg 1456w\"></a></p></blockquote><p>Additionally, I just think some more epistemic humility might be needed before calling some organisations \u201cthe most effective\u201d or even \u201ceffective\u201d. Sure, it\u2019s \u201ceffective\u201d in achieving some set of narrow metrics, but those metrics often only capture one small part of what is actually required to make progress on the problems we care about. That said, I do agree that measurement and evaluation is important, but that it might just require tools that are not strictly quantitative.&nbsp;</p><p><strong>QEJ: </strong>Okay fair enough, I take those points. Although I should note, I don\u2019t <strong>just </strong>think we should focus on measurable things, even though that is definitely useful for feedback loops. I definitely<strong> do</strong> think we should support things that are more speculative but potentially very high value. Ultimately, what we do care about is maximising <a href=\"https://jamesozden.substack.com/p/high-risk-high-reward-applying-venture\">expected value</a> - not just high confidence in some positive value. For example, a 10% chance of helping 10 million people (so we would, on average, expect to help 1 million people) is better than a 100% chance of helping 100,000 people. Would you agree with that?</p><p><strong>CIJ: </strong>Yes definitely! This has the same problem though - you\u2019re forced to quantify the benefits and in doing so, you could miss out on potentially many important effects. The immeasurable (either literally or practically) effects are likely larger than the measurable effects, and they could easily change your bottom-line answers. Therefore, <strong>maximising expected value is unlikely to happen by naively maximising </strong><i><strong>estimated</strong></i><strong> expected value</strong>. There are <a href=\"https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">many reasons</a> we can\u2019t take expected value calculations seriously. For example, we saw <a href=\"https://www.cnbc.com/2022/12/18/how-sam-bankman-fried-ran-8-billion-fraud-government-prosecutors.html\">what happened</a> when Sam Bankman-Fried (probably) made naive expected value calculations when considering how much money he could make via fraud. As others might do, he neglected the less tangible negative consequences, such as those on the Effective Altruism movement he was reportedly supporting. In this case, that looked like widespread negative press coverage in the following months, potentially leading to many people not wanting to get involved with the effective altruism community, a loss in future donors or otherwise painting it in a bad light for influential actors. Before the FTX incident, these negative outcomes would have been almost impossible to quantify accurately, but this didn\u2019t make them any less real (or bad).</p><p><strong>QEJ: </strong>Ah, well of course we\u2019re against <a href=\"https://rychappell.substack.com/p/naive-vs-prudent-utilitarianism\">naive consequentialism</a>! It\u2019s paramount that we have as solid a grasp on the issues as possible, and take actions that maximise our potential benefits whilst limiting the negative consequences. Sam\u2019s failure was simply one of not fully understanding the risks and benefits involved (as well as not wanting to obey common-sense morality, as <a href=\"https://twitter.com/willmacaskill/status/1591218059246587904\">most effective altruists</a> advocate for).</p><p><strong>CIJ: </strong>This we can definitely agree on. But it just seems to be interesting which speculative bets one prioritises, when considering this expected value approach. For example, you might prioritise somewhat speculative technological solutions to global problems, such as cultured meat or carbon capture &amp; storage. Then when other people advocate for speculative political solutions to our society, such as reforming our political or economic system, they get called \u201cidealistic\u201d. Surely both parties are being idealistic, and the main difference is our starting beliefs, and what we\u2019re being idealistic about?</p><p><strong>QEJ: </strong>Hm, I think it\u2019s more nuanced than that. We can actually write out a concrete and tangible story why technological solutions like cultured meat and carbon capture will actually lead to good. For example, inventing cultured meat could essentially displace all animal meat with nothing else but simple market forces if it tasted the same, was cheaper and more accessible. How the hell would \u201cbringing down capitalism\u201d lead to the same thing?</p><p><strong>CIJ: </strong>Well, leaving aside the argument that people\u2019s food choices are <a href=\"https://en.wikipedia.org/wiki/Psychology_of_eating_meat\">much more complex</a> than just price, taste and accessibility, I think your approach feels like a local optimum, rather than a global optimum. In your world, we would still have unsustainable levels of resource consumption, lack of concern for the environment or other species, as well as many other social ills, such as inequality and poverty. These issues, be it climate change, animal suffering or global poverty, can be seen as symptoms of large issues facing our society. These broader systemic issues (e.g. around the misallocation of capital in our economy or myopia of elected political leaders) won\u2019t be solved simply by addressing these symptoms. Instead, we do need much broader political and economic reforms, whether that\u2019s more deliberative democracy, prioritising wellbeing over GDP, and so on. This is what we often mean when we say \u201cchanging the system\u201d or \u201csystemic change\u201d \u2013 things that would quite fundamentally reshape modern politics and our economy. We\u2019re not simply referring to what effective altruists have previously called \u201c<a href=\"https://forum.effectivealtruism.org/posts/5XeCA5gKbMakAskLy/effective-altruists-love-systemic-change\">systemic change</a>\u201d, which seems to refer simply to <strong>any</strong> act that changes government policies (i.e. the political system).</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed2d29dc-4c22-4d85-8ed2-3983b3226952_1600x1025.jpeg\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed2d29dc-4c22-4d85-8ed2-3983b3226952_1600x1025.jpeg\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed2d29dc-4c22-4d85-8ed2-3983b3226952_1600x1025.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed2d29dc-4c22-4d85-8ed2-3983b3226952_1600x1025.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed2d29dc-4c22-4d85-8ed2-3983b3226952_1600x1025.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed2d29dc-4c22-4d85-8ed2-3983b3226952_1600x1025.jpeg 1456w\"></a></p><p><strong>QEJ: </strong>Well, of course it would be great to create an economic system that prioritises well-being and we can ensure that no human lives in poverty, but it\u2019s just not realistic. I mean what are the chances that the current set of political and economic elites will concede enough power such that this actually happens? I think it\u2019s basically 0.001% in our lifetimes.&nbsp;</p><p><strong>CIJ: </strong>I\u2019m sure critics of Martin Luther King said something broadly similar when he was trying to win voting rights for black people in the 1960s: that he was being too bold, asking for too much, and he would be effective if he made more moderate asks. But looking back now, it clearly worked, and has made the world an immeasurably better place as a result of his and other civil rights activism. What\u2019s to say we can\u2019t achieve similarly large changes to our political &amp; economic systems in our lifetimes?&nbsp;</p><p>You might assign it a probability of 0.001% such that it\u2019s highly unlikely to happen, but I think it could be more like 0.1%. And like you\u2019ve mentioned, if we\u2019re also valuing low-probability outcomes that could be extremely important, this kind of fundamental restructuring of economic and political priorities seems like a clear winner (in my opinion). Due to the different biases or worldviews we hold, you think technological solutions might be the best thing to pursue, due to their relative tractability, neglectedness and importance, but I can make the same plausible-sounding arguments for \u201csystems change\u201d stuff.&nbsp;</p><p>&nbsp;</p><p><strong>QEJ: </strong>Sure, that might be the case. And I take the point that we will all have different biases that lead us to different conclusions. But from my reading of history, when we\u2019ve had rapid societal change (what you might call revolutionary change), this has usually led to bad outcomes. As such, I feel much more positive about making small improvements to our current political and economic systems, rather than wholesale changes. Just look at what happened with Stalin after the <a href=\"https://marxiststudent.com/stalin-the-betrayer-of-the-revolution/\">Russian Revolution</a> of 1917 and all the bloodshed that caused. Do you want to risk other horrible times like that?&nbsp;</p><p><strong>CIJ: </strong>Well, there\u2019s a number of reasons why I don\u2019t think that\u2019s the kind of \u201crevolutionary change\u201d we\u2019re after, not least because it was followed by a huge consolidation of political power and a dictatorial regime - the opposite of what I think is good! I do think such larger-scale changes need to be considered thoughtfully, but I also believe moving slowly can be equally as bad or worse. Imagine if we moved slowly on abolishing slavery - what would the world look like today? That was a fundamental change in our economy and how we valued people, similar to how it might be if we might move away from solely prioritising GDP, and I\u2019m sure we all wish that would have happened sooner. Additionally, by never pursuing \u201cstep-change\u201d approaches, we might always be stuck at the local optimum, rather than a global optimum.&nbsp;</p><p><strong>Ultimately, what I\u2019m saying is that we need a \u201c</strong><a href=\"https://medium.com/in-search-of-leverage/saving-the-world-a-portfolio-approach-e73d7853b893\"><strong>portfolio approach</strong></a><strong>\u201d of ways to do good</strong>, where we\u2019re supporting a wide range of things that could plausibly have huge impacts on the world. Given the vast uncertainties I\u2019ve spoken about, hedging our bets across <a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\">different worldviews</a> and <a href=\"https://ssir.org/articles/entry/philanthropists_must_invest_in_an_ecology_of_change\">different theories of change</a> seems like a crucial way to ensure good outcomes across the range of ways the world could turn out.&nbsp;</p><p>&nbsp;</p><p><strong>QEJ: </strong>Whilst I might not agree with all of your points, I definitely do agree that we need to pursue a variety of approaches to achieve change. At least we can agree on something!</p><p>\u2014 end dialogue\u2014&nbsp;</p><p>Obviously, this is just the tip of the iceberg in terms of some disagreements these two types of worldviews might have with each other! It could go on and on, but I\u2019m not sure I could ever do it justice. For example, there could be a whole other dialogue or section on the value of democracy, the benefits of different ways of thinking, and how socio-demographic diversity might support better decision-making. Regardless, I hope it was at least somewhat enlightening in terms of how these two different camps may think.</p><p>For additional reading on different ways of doing good, I recommend a great blog post by Holden Karnofsky, called <a href=\"https://www.cold-takes.com/rowing-steering-anchoring-equity-mutiny/#fnref1\">Rowing, Steering, Anchoring, Equity, Mutiny</a>. It presents an interesting typology of different ways people are trying to improve the world, using the analogy of the world as a ship, seen in the table below.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75927dd2-091a-4ef5-8361-6b9043741ce5_1314x1134.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75927dd2-091a-4ef5-8361-6b9043741ce5_1314x1134.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75927dd2-091a-4ef5-8361-6b9043741ce5_1314x1134.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75927dd2-091a-4ef5-8361-6b9043741ce5_1314x1134.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75927dd2-091a-4ef5-8361-6b9043741ce5_1314x1134.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75927dd2-091a-4ef5-8361-6b9043741ce5_1314x1134.png 1456w\"></a></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn06hppzo4rf75\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref06hppzo4rf75\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I\u2019m assuming that the $20 dollars per surgery includes operational costs &amp; overhead but even if not, the difference is still very large.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna90lp9jaw1t\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa90lp9jaw1t\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A DALY is a <a href=\"https://www.who.int/data/gho/indicator-metadata-registry/imr-details/158#:~:text=Definition%3A-,One%20DALY%20represents%20the%20loss%20of%20the%20equivalent%20of%20one,health%20condition%20in%20a%20population.\">disability-adjusted-life-year</a>, a common measure used to quantify how beneficial some interventions are, based on how many additional years of healthy life they provide.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0mtnyat2pj6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0mtnyat2pj6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In terms of relative proportions, whilst there might be around <a href=\"https://www.furfreealliance.com/fur-farming/\">100 million animals</a> farmed for their fur annually (which is terrible) there are around <a href=\"https://ourworldindata.org/meat-production\">80 billion vertebrate land animals</a> killed for food each year.</p></div></li></ol>", "user": {"username": "JamesOz"}}, {"_id": "HtiyM6KQFogL77oBJ", "title": "Sentience in Machines - How Do We Test for This Objectively?", "postedAt": "2023-03-20T05:20:50.623Z", "htmlBody": "<p>Hello.</p><p>I'm new here, and this is my first post.</p><p>The central topic involves the notion of sentience in machines - \"Can machines develop Sentience? What does Sentience mean in the context of machines? and How do you reliably test for it?\"</p><p>Last year there was a sizeable buzz online about this topic when an engineer at Google- Blake Lemoine, <a href=\"https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917\">claimed</a> that LaMDA - a language model being developed at the company, had developed a capacity for emotion and self-awareness akin to that of a human being.</p><p>This sparked debates and discussions about the idea of \"sentient machines\". Some parties (<a href=\"https://www.bigtechnology.com/p/google-fires-blake-lemoine-engineer\">including Google in an official report</a>) felt the engineer's claims were unfounded and delusional.&nbsp;</p><p>Earlier however, a Google Vice-President <a href=\"https://www.economist.com/by-invitation/2022/06/09/artificial-neural-networks-are-making-strides-towards-consciousness-according-to-blaise-aguera-y-arcas\">in an individual statement</a> felt LaMDA had developed an extremely impressive capacity for social modelling, indicating that AI systems were gradually approaching consciousness (in the sense of modelling the self).</p><p>A distinction between the Google VP's position and Lemoine's is that the VP describes LaMDA as being able to <i>understand entities that feel, </i>while Lemoine says<i> LaMDA itself is able to feel.</i></p><p>If machines are truly capable of sentience, then ethical discussions about machine rights suddenly become important and urgent. It would be unethical for humans to keep making unilateral decisions about the use (and disposal) of technology which has somehow developed its own capacity for emotion.</p><p>In spite of the disagreement that exists in discussions about Artificial Sentience, something everyone generally agrees on is that AI systems are becoming increasingly capable. There is no reason to think Sentience is beyond the capacity of machines, either now or in the future.</p><p>The issue is that we're not sure how to definitively test for Artificial Sentience if/when it happens. Tests based on human judgement are subjective and unpersuasive. Lemoine for example, was completely convinced of LaMDA's sentience. A good number of other people though, were highly skeptical of his judgements.</p><p>&nbsp;</p><p><strong>To obtain some guidance on this issue of subjectiveness, we can draw some insight from Neuroscience:</strong></p><p>In a subfield of Neuroscience called Affective Neuroscience, there are <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5023171/\">procedures</a> to detect the emotional state of an individual from an fMRI (functional Magnetic Resonance Imaging) scan of their brain. From an fMRI scan, you can reliably tell if an individual is sad, happy, angry, etc.</p><p>Procedures like this find immense value in Medicine. E.g, Brain scans <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6278242/\">are used to</a> detect consciousness in comatose patients. Usually you can tell if a human being is conscious by observing their behaviour, or their response to stimulus. You can't do this for comatose/unresponsive patients, so brain scans offer a valuable alternative.</p><p>Such procedures highlight an <u>objective</u> test of consciousness in humans. They are independent of individual human judgement - instead they rely on a tested method of analysing brain scans to make inferences about consciousness.</p><p>How can we apply an analogous approach to machines?</p><p><i>What is an Objective way to test for Sentience in a Machine/Language Model? Independent of what it says, and independent of people's personal opinions?</i></p><p>To do this, we can draw on structural parallels that exist between artificial neural networks (the technology underpinning todays AI systems), and the human brain.</p><p>A number of AI experts are quick to emphasise that artificial neural networks are only <u>loosely</u> inspired by the human brain, and so we should not attempt to draw serious analogies between them.</p><p>Some Neuroscientists however, are convinced enough of their similarities to employ artificial neural networks as a conceptual framework<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8134940/#!po=0.304878\"> for understanding the human brain better</a>. I side with these neuroscientists.</p><p>Studies in Artificial Intelligence [<a href=\"https://arxiv.org/abs/1506.06579\">1</a>, <a href=\"https://aclanthology.org/N16-1082/\">2</a>, <a href=\"https://arxiv.org/abs/1506.02078\">3</a>, <a href=\"https://aclanthology.org/D19-1448/\">4</a>] &nbsp;provide techniques to \"open up\" artificial neural networks, and understand them at the level of individual neurons. Using these techniques, we can carry out \"brain scans\" on machines/language models to detect the \"emotional state\" they're experiencing at a given point in time.</p><p>This will provide an <u>objective</u> way to detect emotion (and consequently sentience) in a given machine/language model. We wouldn't have to debate over subjective tests and personal judgements, but could instead employ an objective procedure to \"scan the brains\" of machines, and obtain information about what they are (or are not) feeling.</p><p>I wrote an initial research manuscript giving some more technical detail on this approach to detecting sentience in machines. Interested people can find it <a href=\"https://www.researchgate.net/publication/363071576_Towards_an_Objective_Test_of_Machine_Sentience\">here</a>.</p>", "user": {"username": "Mayowa Osibodu"}}, {"_id": "AyKzxaqHDDqjHKXFG", "title": "Consultants and the Crisis of Capitalism - Mazzucato & Collington", "postedAt": "2023-03-19T10:46:58.307Z", "htmlBody": "<p>The article explains that the role of consultancies in policy-making is much larger than one (I) would expect, that this can create conflicts of interest and lead to bad results, and, crucially, that this degrades the ability of governments (civil servants) to face problems.</p><p>It is something I have never read or even think about and I found it relevant for EAs and EA as a movement.</p>", "user": {"username": "mikbp"}}, {"_id": "ayxasxhHWTvf6r5BF", "title": "Scale of the welfare of various animal populations", "postedAt": "2023-03-19T07:10:50.272Z", "htmlBody": "<h1>Summary</h1><ul><li>I Fermi-estimated the scale of the welfare of various animal populations from the relative intensity of their experiences, moral weight, and population size.</li><li>Based on my results, I would be very surprised if the scale of the welfare of:<ul><li>Wild animals ended up being smaller than that of farmed animals.</li><li>Farmed animals turned out to be smaller than that of humans.</li></ul></li></ul><figure class=\"image image_resized\" style=\"width:39.99%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ayxasxhHWTvf6r5BF/hyknkoo8kdhtlfcdnvxk\"><figcaption>An abstract oil painting of nematodes. Generated by OpenAI's DALL-E.</figcaption></figure><h1>Introduction</h1><p><a href=\"https://slatestarcodex.com/2013/05/02/if-its-worth-doing-its-worth-doing-with-made-up-statistics/\"><u>If it is worth doing, it is worth doing with made-up statistics</u></a>?</p><h1>Methods</h1><p>I Fermi-estimated the scale of the welfare of various animal populations from the <a href=\"https://en.wikipedia.org/wiki/Absolute_value\"><u>absolute value</u></a>&nbsp;of the <a href=\"https://utilitarianism.net/types-of-utilitarianism/expectational-utilitarianism-versus-objective-utilitarianism\"><u>expected</u></a> <a href=\"https://www.utilitarianism.net/population-ethics/the-total-view\"><u>total</u></a> <a href=\"https://www.utilitarianism.net/theories-of-wellbeing/hedonism\"><u>hedonistic</u></a>&nbsp;utility (ETHU). I computed this from the product between:</p><ul><li>Intensity of the mean experience as a fraction of the median welfare range.</li><li>Median welfare range.</li><li>Population size.</li></ul><p>The data and calculations are <a href=\"https://docs.google.com/spreadsheets/d/1QiWFKeSD1kA0N1hoA7a4foiuXm0mHgkN_YNUM_BNJZE/edit?usp%3Dsharing\"><u>here</u></a>.</p><h2>Intensity of experience</h2><p>I calculated the intensity of the mean experience of farmed animals as a fraction of their median welfare range from that of broilers in a reformed scenario<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh5mwa4dzcm5\"><sup><a href=\"#fnh5mwa4dzcm5\">[1]</a></sup></span>, assuming:</p><ul><li>The time they experience each level of pain defined&nbsp;<a href=\"https://welfarefootprint.org/research-projects/analytical-approach/\"><u>here</u></a> (search for \u201cdefinitions\u201d) is given by&nbsp;<a href=\"https://welfarefootprint.org/broilers/\"><u>these</u></a> data (search for \u201cpain-tracks\u201d) from the&nbsp;<a href=\"https://welfarefootprint.org/broilers/\"><u>Welfare Footprint Project</u></a> (WFP).</li><li>The welfare range is symmetric around the neutral point, and excruciating pain corresponds to the worst possible experience.</li><li>Excruciating pain is 1 k times as bad as disabling pain<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref91uo3suuh9p\"><sup><a href=\"#fn91uo3suuh9p\">[2]</a></sup></span>.</li><li>Disabling pain is 100 times as bad as hurtful pain, which together with the above implies excruciating pain being 100 k times as bad as hurtful pain.</li><li>Hurtful pain is 10 times as bad as annoying pain, which together with the above implies excruciating pain being 1 M times as bad as annoying pain.</li><li>Their lifespan is 42 days, in agreement with section \u201cConventional and Reformed Scenarios\u201d of&nbsp;<a href=\"https://docs.google.com/document/d/1w7hcocNft4-pLjrnxtKOwcClHUQYC21hlHF18uIiHQ0/edit%23heading%3Dh.j7nud0a3vz77\"><u>Chapter 1</u></a> of&nbsp;<a href=\"https://smile.amazon.com/Quantifying-Pain-Broiler-Chickens-Slower-Growing-ebook/dp/B09ZDWWD97/ref%3Dsr_1_5?crid%3D3LDCBOKQUS6ZO%26keywords%3Dcynthia%2Bschuck%26qid%3D1651595790%26sprefix%3Dcynthia%2Bschuck%252Caps%252C214%26sr%3D8-5%26sa-no-redirect%3D1\"><u>Quantifying pain in broiler chickens</u></a> by Cynthia Schuck-Paim and Wladimir Alonso.</li><li>They sleep 8 h each day, and have a neutral experience during that time.</li><li>Them being awake is as good as hurtful pain is bad. This means being awake with hurtful pain is neutral, thus accounting for positive experiences.</li></ul><p>Ideally, I would have used empirical data for the animal populations besides farmed chickens too. However, I do not think they are readily available, so I had to make some assumptions.</p><p>For the intensity of the mean experience of humans as a fraction of their median welfare range, I considered we:</p><ul><li>Sleep 8 h each day, and have a neutral experience during that time.</li><li>Being awake is as good as hurtful pain is bad. This means being awake with hurtful pain is neutral, thus accounting for positive experiences.</li></ul><p>For the intensity of the mean experience of wild animals as a fraction of their median welfare range, I used the same value as for humans. However, whereas I think humans have positive lives (see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ogdDRgfmBRj6paPbv/global-life-satisfaction-distribution\"><u>here</u></a>), I am very uncertain about wild animals (see&nbsp;<a href=\"http://philsci-archive.pitt.edu/19608/1/browningveit2021positive_welfare.pdf\"><u>this</u></a> preprint from Heather Browning and Walter Weit).</p><h2>Median welfare range</h2><p>I defined the median welfare range from Rethink Priorities\u2019 estimates for mature individuals<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpogjn8llacg\"><sup><a href=\"#fnpogjn8llacg\">[3]</a></sup></span>&nbsp;provided <a href=\"https://forum.effectivealtruism.org/posts/Qk3hd6PrFManj8K6o/rethink-priorities-welfare-range-estimates\"><u>here</u></a>&nbsp;by Bob Fischer<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreff7i2zk4scd5\"><sup><a href=\"#fnf7i2zk4scd5\">[4]</a></sup></span>. For the populations I studied with animals of different species, I used those of:</p><ul><li>For wild mammals, pigs.</li><li>For farmed fish, salmon.</li><li>For wild fish, salmon.</li><li>For farmed insects, silkworms.</li><li>For wild terrestrial <a href=\"https://en.wikipedia.org/wiki/Arthropod\"><u>arthropods</u></a>, silkworms.</li><li>For farmed crayfish, crabs and lobsters, mean between crayfish and crabs.</li><li>For farmed shrimps and prawns, shrimps.</li><li>For wild marine arthropods, silkworms.</li><li>For <a href=\"https://en.wikipedia.org/wiki/Nematode\">nematodes</a>, silkworms multiplied by 0.1.</li></ul><h2>Population size</h2><p>I defined the population size from:</p><ul><li>For humans, <a href=\"https://ourworldindata.org/world-population-growth\"><u>these</u></a>&nbsp;data from Our World in Data (OWID) (for 2021).</li><li>For wild mammals, the mean of the lower and upper bounds provided in section 3.1.5.2 of <a href=\"https://www.nowpublishers.com/article/Details/IRERE-0115\"><u>Carlier 2020</u></a>.</li><li>For farmed chickens and pigs, <a href=\"https://ourworldindata.org/grapher/livestock-counts\"><u>these</u></a>&nbsp;data from OWID (for 2014).</li><li>For farmed fish, the midpoint estimate of <a href=\"https://www.sentienceinstitute.org/global-animal-farming-estimates\"><u>this</u></a>&nbsp;analysis from Kelly Anthis and Jacy Anthis (for 2019).</li><li>For wild fish, the mean between the mean of the lower and upper bounds provided in section 3.1.5.5 of <a href=\"https://www.nowpublishers.com/article/Details/IRERE-0115\"><u>Carlier 2020</u></a>, and the order of magnitude given in Table S1 of <a href=\"https://www.pnas.org/doi/10.1073/pnas.1711842115\"><u>Barn-On 2018</u></a>.</li><li>For farmed insects raised for food and feed, the mean of the lower and upper bounds provided <a href=\"https://forum.effectivealtruism.org/posts/ruFmR5oBgqLgTcp2b/insects-raised-for-food-and-feed-global-scale-practices-and\"><u>here</u></a>&nbsp;by Abraham Rowe (in the 2nd point of the section \u201cKey Findings\u201d).</li><li>For farmed crayfish, crabs and lobsters, and farmed shrimps and prawns, the ratio between the means of the lower and upper bounds for:</li><li>The number of individuals killed per year provided <a href=\"http://fishcount.org.uk/fish-count-estimates-2/numbers-of-farmed-decapod-crustaceans\"><u>here</u></a>&nbsp;by fishcount.org (for 2017).</li><li>The time in years farmed shrimps spend in grow-out ponds, developing from juvenile until market size, according to <a href=\"https://en.wikipedia.org/wiki/Marine_shrimp_farming\"><u>this</u></a>&nbsp;Wikipedia page.</li><li>For wild terrestrial and marine arthropods, and nematodes, the orders of magnitude from Table S1 of <a href=\"https://www.pnas.org/doi/10.1073/pnas.1711842115\"><u>Barn-On 2018</u></a>.</li></ul><h1>Results</h1><p>The results are presented in the table below by descending absolute value of ETHU as a fraction of that of humans, i.e. decreasing scale of welfare.</p><figure class=\"table\"><table style=\"border:1pt solid rgb(0, 0, 0)\"><thead><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Population</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Intensity of the mean experience as a fraction of the median welfare range</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Median welfare range</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\">Intensity of the mean experience as a fraction of that of humans</th><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Population size</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Absolute value of ETHU as a fraction of that of humans</p></th></tr></thead><tbody><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Farmed insects raised for food and feed</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>12.9&nbsp;<a href=\"https://www.nist.gov/pml/owm/metric-si-prefixes\"><u>\u03bc</u></a></p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>2.00&nbsp;<a href=\"https://www.nist.gov/pml/owm/metric-si-prefixes\"><u>m</u></a></p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>3.87 m</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>8.65E10</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.0423</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Farmed pigs</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>12.9 \u03bc</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.515</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>1.00</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>9.86E8</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.124</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Humans</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>6.67 \u03bc</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>1.00</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>1.00</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>7.91E9</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>1.00</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Farmed crayfish, crabs and lobsters</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>12.9 \u03bc</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.0305</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.0590</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>1.57E11</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>1.17</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Farmed fish</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>12.9 \u03bc</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.0560</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.108</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>1.11E11</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>1.52</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Farmed chickens</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>12.9 \u03bc</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.332</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.642</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>2.14E10</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>1.74</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Farmed shrimps and prawns</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>12.9 \u03bc</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.0310</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.0599</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>9.87E11</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>7.48</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);text-align:center\">Farmed animals analysed here</th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>12.9 \u03bc</p></td><td style=\"border:1pt solid rgb(0, 0, 0);text-align:center\">0.0362</td><td style=\"border:1pt solid rgb(0, 0, 0);text-align:center\">0.0700</td><td style=\"border:1pt solid rgb(0, 0, 0);text-align:center\">1.36E12</td><td style=\"border:1pt solid rgb(0, 0, 0);text-align:center\">12.1</td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Wild mammals</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>6.67 \u03bc</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.515</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.515</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>6.75E11</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>43.9</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Wild fish</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>6.67 \u03bc</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.0560</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.0560</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>6.20E14</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>4.39 k</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Wild terrestrial arthropods</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>6.67 \u03bc</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>2.00 m</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>2.00 m</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>1.00E18</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>253 k</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Wild marine arthropods</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>6.67 \u03bc</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>2.00 m</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>2.00 m</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>1.00E20</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>25.3 M</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Nematodes</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>6.67 \u03bc</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.200 m</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.200 m</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>1.00E21</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>25.3 M</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);text-align:center\">Wild animals analysed here</th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>6.67 \u03bc</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\"><p>0.365 m</p></td><td style=\"border:1pt solid rgb(0, 0, 0);text-align:center\">0.365 m</td><td style=\"border:1pt solid rgb(0, 0, 0);text-align:center\">1.10E21</td><td style=\"border:1pt solid rgb(0, 0, 0);text-align:center\">50.8 M</td></tr></tbody></table></figure><h1>Discussion</h1><p>According to my results:</p><ul><li><a href=\"https://forum.effectivealtruism.org/topics/wild-animal-welfare\"><u>Wild animal welfare</u></a> dominates&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/farmed-animal-welfare\"><u>farmed animal welfare</u></a>:<ul><li>The scale of the welfare of each of the 5 populations of wild animals exceeds that of each of the 6 populations of farmed animals.</li><li>The scale of the welfare of the 5 populations of wild animals is 4.21 M (= 50.8*10^6/12.1) times that of the 6 populations of farmed animals. This did not surprise me given the sheer numbers of wild animals.</li></ul></li></ul><ul><li>There is a <a href=\"https://forum.effectivealtruism.org/topics/meat-eater-problem\"><u>meat-eater problem</u></a>. The combined importance of the 6 populations of farmed animals I analysed is 12.1 times as large as that of humans. Consequently, a smaller human population will tend to increase welfare in the nearterm if we ignore the effects on wild animals. However, these dominate, and can be positive or negative, so I have no idea what is the overall nearterm effect of changing the size of the human population. For similar reasons, <a href=\"https://forum.effectivealtruism.org/posts/HwDZ6oHtz34kBAsts/finding-bugs-in-givewell-s-top-charities\"><u>I think</u></a>&nbsp;it is very hard to say whether <a href=\"https://www.givewell.org/charities/top-charities\"><u>GiveWell\u2019s top charities</u></a>&nbsp;are beneficial or harmful.</li><li>The intensity of the mean experience of farmed chickens, estimated from data for broilers in a reformed scenario, is 64.2 % that of humans. Intuitively, I would guess the ratio to be higher, but I believe I am biassed towards overweighting the time in disabling and excruciating pain. This is indeed super bad, but does not last for long.</li><li>The order of the scale of welfare among wild animals roughly matches what I estimated <a href=\"https://forum.effectivealtruism.org/posts/rdLMTsSZ2oqoFAeTg/is-improving-the-welfare-of-arthropods-and-nematodes-1\"><u>here</u></a>&nbsp;based on the total number of neurons, with arthropods and nematodes being the major drivers. The welfare of nematodes has a greater scale here, but my guess for the moral weight of nematodes has quite low <a href=\"https://forum.effectivealtruism.org/topics/credal-resilience\"><u>resilience</u></a>.</li><li>Among the populations of farmed animals and humans, the welfare of insects raised for food and feed has the smallest scale. I actually expected it to be larger, but I think I was overestimating their population size.</li></ul><p>The specific ordering of the various animal populations by scale of welfare I got is not robust given the high uncertainty of my results. However, I would be very surprised if the scale of the welfare of:</p><ul><li>Wild animals ended up being smaller than that of farmed animals.</li><li>Farmed animals turned out to be smaller than that of humans.</li></ul><p>I would say any <a href=\"https://forum.effectivealtruism.org/posts/NFGEgEaLbtyrZ9dX3/scope-sensitive-ethics-capturing-the-core-intuition?utm_source%3D80%252C000%2BHours%2Bmailing%2Blist%26utm_campaign%3D6fc9a27446-Research%2Bnewsletter%2BMarch%2B17%2B2023%2B%25E2%2580%2594%2BVirtues%26utm_medium%3Demail%26utm_term%3D0_43bc1ae55c-8c505d1e32-%255BLIST_EMAIL_ID%255D\"><u>scope-sensitive ethic</u></a>&nbsp;will lead to these conclusions, not just expectational total hedonistic utilitarianism.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh5mwa4dzcm5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh5mwa4dzcm5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;From&nbsp;<a href=\"https://welfarefootprint.org/broilers/\"><u>this</u></a> page of WFP, broilers in reformed scenarios have an average daily gain of 45 to 46&nbsp;<a href=\"https://en.wikipedia.org/wiki/Gram\"><u>g</u></a>/d, whereas ones in conventional scenarios have 60 and 62 g/d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn91uo3suuh9p\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref91uo3suuh9p\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I encourage you to check&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/gtGe8WkeFvqucYLAF/logarithmic-scales-of-pleasure-and-pain-rating-ranking-and\"><u>this</u></a> post from algekalipso, and&nbsp;<a href=\"https://forum.effectivealtruism.org/users/ren-springlea-1\"><u>this</u></a> from Ren Springlea to get a sense of why I think the intensity can vary so much.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpogjn8llacg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpogjn8llacg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/E7xdBbxqPNLjhrnz6/if-adult-insects-matter-how-much-do-juveniles-matter\"><u>This</u></a>&nbsp;post from Bob Fischer looks into the moral weight of juvenile insects.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnf7i2zk4scd5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreff7i2zk4scd5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Bob Fischer shared the means <a href=\"https://forum.effectivealtruism.org/posts/Qk3hd6PrFManj8K6o/rethink-priorities-welfare-range-estimates?commentId%3DX5DihMEqMqf5Asxx8\"><u>here</u></a>, but recommended using the medians.</p></div></li></ol>", "user": {"username": "vascoamaralgrilo"}}, {"_id": "ga2XdAkadub8eviGo", "title": "Some preliminary notes towards a *real* Cost-Benefit analysis of prisons", "postedAt": "2023-03-19T02:34:51.811Z", "htmlBody": "<p>I wrote this post on my blog, and given that it's smack bang in the middle of what this forum deals with, I thought I'd share it here.</p><p><i>If anyone with a background in this sort of analysis wants to work with me to make a detailed cost-benefit analysis of incarceration a reality, get in contact</i></p><h3><strong>Cost-Benefit Analysis in a nutshell</strong></h3><p>Cost-Benefit Analysis is a methodology for evaluating the aggregate <i>costs </i>and<i> benefits </i>of programs. Roughly speaking, it works as follows. For some intervention, X, we look at how much the supporters of X would be willing to pay to make it happen, and how much the opponents of X would pay for it not to happen. We add in the amount it costs the government to carry out the policy on the costs side, and if:</p><p>Benefits - Costs &gt; 0</p><p>The program is in some sense \u201crecommended\u201d by the procedure. Later in the essay, we will describe how to make a Cost-Benefit Analysis approximate an assessment of whether or not a policy is good from a utilitarian point of view.</p><h3><strong>Do you notice anything missing in this assessment of the costs of imprisonment? A certain glaring absence?</strong></h3><p>Let us review an extract from <i>\"Does prison pay?\" Revisited </i>by Piehl &amp; Dilulio, which is exceedingly representative of the field as a whole. Notice that something very glaring is missing in the mix.</p><p><strong>[Feel free to skip this section]</strong></p><p><i>Estimating the social costs and benefits of competing transportation or environmental policies is no analytical picnic. But estimating them for imprisonment and other sentencing options is a certain analytical migraine.</i></p><p><i>For starters, it is widely asserted that it costs $25,000 to keep a prisoner behind bars for a year. But the latest Bureau of Justice Statistics figures for average annual spending per prisoner are $15,586 for the states and $14,456 for the federal Bureau of Prisons (which holds about 10 percent of all prisoners). These figures are calculated by dividing the total spent on salaries, wages, supplies, utilities, transportation, contractual services, and other current operating expenses by the average daily inmate population.</i></p><p><i>But hidden and indirect costs of running prisons might bring the $25,000 figure closer to reality than the official spending averages would allow. For example, some tiny but nontrivial fraction of government workers outside of corrections (human services, central budgeting offices) spend time on matters pertaining to prisoners. And Harvard economist Richard Freeman and others suggest that incarceration decreases post-release employability and lifetime earnings potential. Thus an ideal estimate of the social costs of imprisonment would include any relevant spending by other government agencies, plus whatever public unemployment compensation, welfare, and health expenditures result from the negative short- and long-term labor market effects of imprisonment on ex-prisoners.</i></p><p><i>Also, there is wide inter- and intra-system variation not only in what it costs to operate prisons, but in how prison dollars are allocated as between security functions (uniformed custodial staff), basic services (food, heat, medical supplies), treatment programs, recreational facilities, plant maintenance, and other expenditures. Whatever the best estimate of prison operating costs, such cost differences suggest that efficiency losses are occurring in some places and that efficiency gains are possible in others.</i></p><p><i>The cost-effectiveness of prisons, however, is by no means strictly determined by correctional administrators. Over the past 25 years the courts have had a major impact on both the total costs of operating prisons and the distribution of prison dollars between security and other needs. For example, in the wake of a sweeping court order, prison operating costs in Texas grew from $91 million in 1980 to $1.84 billion in 1994, a tenfold increase in real terms, while the state's prison population barely doubled. Texas is now one of at least 20 states that spends less than half of every prison dollar on security.</i></p><p><i>Finally, it is worth remembering that barely a penny of every federal, state, and local tax dollar goes to support state prisons and local jails. State and local governments spend 15 times what the federal government spends on corrections. But state and local spending on prisons and jails amounts to only $80.20 per capita a year, or $1.54 per capita a week.</i></p><p><strong>Nowhere in the above is there any acknowledgment that prison inflicts major non-economic costs (or more accurately, harms) on prisoners.</strong></p><h3><strong>A curious reluctance</strong></h3><p>There are, in the literature, relatively few Cost-Benefit Analyses of incarceration compared to, say, studies of the costs and benefits of the provision of water. When these Cost-Benefit Analyses do happen, they are often unwilling to tally <i>all </i>costs and benefits. In particular, almost all studies exclude costs to prisoners. Certainly, there are some CBAs that include these costs, but far fewer than one would like. Even those authors who make a heroic attempt to include all the costs and benefits which need to be included all seem to fail- this is not their fault, but rather reflects the underdeveloped state of the literature on this topic, which forces authors to do their CBA almost from the ground up.</p><p>Why is there so little CBA on prisons? My guess is moral and political reservations, but that\u2019s an odd excuse, given the ruthless, economizing logic of policy analysis after 1970. CBA was meant to be the cure to government waste, inefficiency, and unintended harm. It betrays a certain timidity and evokes a certain suspicion that economists and criminologists haven\u2019t applied it widely in this field. Might it reveal among economists a certain sense that some things are just too important for cost-benefit analysis, conveying an almost sacral quality to punishment?</p><p>Prisons give lie to a view of neoliberalism sometimes held by both its supporters and detractors- that it has baptized everything in the \u201cicy waters of egotistical calculation\u201d. Prison has in no sense been rationalized, even while so much else has because prison, the infliction of suffering on the hated, gives a kind of baleful warmth to the arctic world of post-80s governance. There is no alternative, but don\u2019t you worry, we\u2019re gonna torture that sicko. Cost-Benefit analysis has always been, in practice, used in highly selective and strategic ways. War, policing and intelligence are lightly scrutinized while schooling, healthcare and the like are put under the electron microscope.</p><p>If there is a lack of Cost-Benefit Analysis in <i>academic </i>studies of incarceration, there is even less by bureaucrats with real access to decision-making processes. In almost all areas of US federal government, Cost-Benefit Analysis is ubiquitious. Existing standards require cost-benefit analysis as a preresquite for any time of regulation for example, yet strangely, in the worlds of incarceration and prosecution, it is absent.</p><p><a href=\"https://www.jstor.org/stable/pdf/3481427.pdf?casa_token=5lkSMIFaZCIAAAAA:v6y7Ij9g0mX1hR-PxgYJpWbDB4NnY3sDRghdbAROBA22D_yHwCDmcBlrPSWXSNTJZnYNZaOpMXWwPFaca-89epv6-iWjKeE4H73a4dN6aYm2Iy4jcNpC\"><strong>This paper notes:</strong></a></p><p><i>Despite that leeway, criminal law currently does not use CBA to inform its choices. Criminal law is a massive government social policy intervention on a scale with the largest regulatory agendas and, broadly conceived, it does the same sorts of things regulatory agencies do. For in- stance, it aims to reduce specific types of risks to bodily harm, property loss, and associated psychological-emotional injuries. We have tremendous amounts of data on crime rates, demographic information on crimes and offenders, as well as extensive statistical accounts of, inter alia, arrests, prosecutions, and case dispositions at state, local, and federal levels. What we lack is extensive analysis of the sort we have for federal regulatory projects. We have a woefully thin account of whether the particular approaches we employ for criminal law enforcement cause more harm one in respect than good in another, and little sense such knowledge into criminal law administration.</i></p><p>Instead of methods like cost-benefit analysis, we use sounder methods to assess the quality of criminal justice, such as rhyming and sports puns (\u201cDo the crime, do the time\u201d, \u201cThree strikes and you\u2019re out\u201d).</p><p>But despite this lack of existing literature- academic or grey, I want to talk about CBA of prison. I\u2019m pretty sure that given the enormous costs of prison to society and to the imprisoned, imprisonment will only pass a CBA <i>only where there is no alternative</i>. Regardless of whether I\u2019m right or not, I want to commend <i>thoroughgoing</i> <i>utilitarian CBA of marginal incarceration </i>as a vital project for political reformers, effective altruists, humanitarians, and criminologists.</p><p>And I want you to feel something about this somewhat abstruse topic. In fact, I want you to be indignant about it.</p><h3><strong>If we are going to be utilitarian, we need to do it properly</strong></h3><p>My proposal is to approach CBA of incarceration <i>from a utilitarian perspective</i>. We will be trying to assess whether increasing incarceration by a small amount increases or decreases aggregate preference fulfillment. For this, I propose using cost-benefit analysis altered to measure aggregate preference fulfillment. Personally, I favor an alternative conceptual framework to traditional CBA I call <i>Subjective Wellbeing Effect Evaluation (SWBEE) </i>as outlined in my draft doctoral thesis. However, I will outline my preferred methodology in terms of CBA, because it\u2019s the standard language of these things.</p><p>Does approaching this analysis from a utilitarian perspective mean that we must believe in utilitarianism? Not at all. It just means that we\u2019re looking to see if, in total, it helps or harms people more. <i>That\u2019s a good start to a moral conversation </i>but it doesn\u2019t have to be the end- it just seems like, at minimum, a baseline of knowledge we should have in discussing prison.</p><p>Once we have established this baseline, opponents of incarceration can introduce further considerations (e.g., the inherent value of society minimizing vengeance, the cruelty of a carceral system that primarily affects those born into the lower class etc.) and supporters of incarceration can introduce their own further considerations (the moral value of punishment, the reduction in the moral weight of prisoners needs due to their wrongdoing etc.) Rights-based arguments, complex considerations from political and legal philosophy etc. can be marshaled- but I suggest gathering information on harms and benefits before we have this broader conversation. This call for a wider discussion after we have the data isn\u2019t just a concession on my part. I believe in a richer form of consequentialism than can be captured by the Cost-Benefit methodology I describe. So I genuinely think we need to have a conversation about broader moral considerations, but let\u2019s get some data on human welfare first.</p><p>How will we measure what people want? Largely by how much money they are willing to pay to secure an outcome, or avoid it. However, as is appropriate for this kind of cost-benefit analysis, we will factor in differences in income. A wealthy man might be willing to pay millions to avoid prison, whereas a poor man might only be able to scrape together a pittance. The best available figures, based on hypothetical gambles, suggest that for every one percent a person\u2019s income rises, the value to them of a dollar falls by about 1.2 to 1.5%, so this is the range of figures I recommend using.</p><p>It would be wholly inappropriate, for example, to use willingness to pay for bail <i>alone </i>as a measure of desire to avoid incarceration, without considering that many prisoners are credit constrained- as at least one researcher has, bizarrely, done.</p><p>Adopting this approach to measuring preferences does not mean that we have to grant human welfare=weighted willingness to pay. We\u2019re using weighted willingness to pay because it\u2019s a reasonably accessible proxy for something that is much richer- human wellbeing. <i>This is just a first pass.</i></p><p>What if you think there are important effects on human welfare that go beyond people\u2019s own desires and preferences? To pick an obvious example, some people don\u2019t really care about preferences, they just care about happiness. Others pick more rarefied qualities- autonomy, dignity, capabilities and so on. I can only reply that people\u2019s preferences are probably correlated with their own welfare- whatever welfare may be.</p><p>With difficult-to-conceive situations like being incarcerated, being the victim of a crime etc. we should be extremely reluctant to take people\u2019s assessment of costs and benefits to themselves <i>ex-ante </i>unless there is no other choice. It is likely that people\u2019s estimates of how bad a year in prison for them would be, if they\u2019ve never experienced prison, or how bad a burglary would be, if they\u2019ve never had their house robbed are worthless. Hence we should derive our willingness to pay to avoid misfortune estimates from those who have suffered the misfortune in question. Given the politically contested nature of this topic, we should be careful that subjects aren\u2019t choosing their willingness to pay values strategically, to try and influence the outcome of the analysis.</p><p>Perhaps the primary characteristic of a utilitarian analysis is that it involves performing cost-benefit calculations in a way that includes <i>all significant costs and benefits</i>. Simply having done the wrong thing doesn\u2019t exclude you from the calculus. We might exclude costs and benefits from analysis if they seem both likely to be small, and very difficult to calculate, but we exclude no costs or benefits for \u2018moral\u2019 reasons.</p><p>Objectionable work in this area has been a problem in the past. <i>Cost-benefit Analysis is a well-understood methodology, perhaps its biggest no-no is arbitrarily excluding large costs or benefits for moral reasons</i>.<i> </i>If you think there is a case for excluding certain costs or benefits for moral reasons, you should create two models, one that excludes the cost, and one that doesn\u2019t- so readers can choose for themselves.</p><p>Another problem is excluding costs and benefits likely to be large purely for ease of calculation. This can render analyses all but useless. There are good alternatives, viz, to take multiple estimates of the value of hard-to-quantify variables and providing different models based on the valuations as a kind of sensitivity test.</p><p>Obiter dicta, I think it is not just bad practice but <i>wickedness </i>to perform a cost-benefit analysis of incarceration, without valuing the loss of liberty and other harms to incarcerated people. It is treating the experience of some of the most vulnerable people in society as worthless. I regard it as morally on par with colonialist intellectuals who whitewashed what their nations were doing overseas. It\u2019s one thing to say that people\u2019s experience of a lack of liberty should be discounted because they\u2019ve done the wrong thing and therefore \u2018deserve\u2019 it, <i>but to not even bother assessing what we are discounting before tossing it aside, is, to use something of a clich\u00e9, epistemic injustice.</i></p><p>Also, we approach incarceration \u2018as is\u2019. We do not consider what the optimal rate of incarceration would be under a hypothetical, better and more humane prison system. Recommending higher incarceration, and then putting in a little caveat that <i>of course </i>you only mean to recommend higher incarceration on the condition of better prisons, is moral cowardice.</p><p>Our assessment is to be at the margin, in the first instance- that is to say, assessing the costs and benefits of increasing total incarceration by an arbitrarily small amount. However, it would be good to go back from the margin- to be able to make some guesses about what halving or doubling aggregate incarceration would look like.</p><h3><strong>The major costs and benefits of incarceration</strong></h3><p>With all that said, let\u2019s survey the cost and benefits of incarceration. I haven\u2019t got the resources or capability to do an analysis myself, but at least we can start thinking about the types of costs involved.</p><p><i><strong>Costs</strong></i></p><p>The non-economic costs to the incarcerated of incarceration at the time of incarceration.</p><p>Loss or significant reduction the incarcerated person\u2019s economic productivity- at the time of imprisonment</p><p>Loss or significant reduction of the incarcerated person\u2019s economic productivity after release</p><p>The marginal costs to the state of an additional person incarcerated</p><p>Long-term health impacts of incarceration</p><p>The costs of additional crime in prison caused by an additional person incarcerated (e.g., prison violence)</p><p>Psychic costs to the incarcerated person\u2019s family and/or partner</p><p>Increase in welfare expenditure required after release (e.g. due to increased unemployment)</p><p><i><strong>Benefits</strong></i></p><p>Crimes outside prison prevented because prisoners cannot commit them</p><p>The deterrence effect on non-prisoners of an additional person incarcerated</p><p>The psychic satisfaction of victims and their relatives gained through incarceration</p><p>Reduction in welfare expenditure during incarceration</p><p><i><strong>Unknown</strong></i></p><p>The effect of incarceration on the likelihood of committing crimes after the incarcerated person is released (could be positive- through reform or negative through the so-called \u201cuniversity of crime\u201d effect, although most evidence indicates that incarceration increases the likelihood of committing crimes after prison.)</p><p>The effect on society of promoting the kind of values incarceration reflects. Incarceration reflects a preference for <i>vengeance over mercy</i>. There are presumably other values based effects<i>. </i>It may also have political economic effects- e.g. creating families and communities invested in incarceration. These factors are intangible- and some might argue they\u2019re simply too difficult to account for in Cost-Benefit Analysis, but given their possible magnitude, my vote is that we should at least try. Some sub-factors here are clearly negative- e.g. the possibility of entrenching and promoting both structural and attitudinal racism through widespread incarceration (at one point, 70% of male Black Americans without a high school diploma had gone to prison at some point before their 30s).</p><p><i><strong>Excluded costs</strong></i></p><p>It is justifiable to exclude some costs that seem likely to be <strong>both </strong>minor and difficult to calculate. Minor costs that are easy to calculate should be included, and excluding major costs that are difficult to calculate makes the analysis worthless.</p><p>Here are some minor and difficult-to-calculate factors I would recommend excluding:</p><p>The impacts of incarceration on the incarcerated person\u2019s friends as opposed to family.</p><p>Likewise, in some cases, the effects of incarceration on victims and their families may be negative (not all victims support lengthy sentences), but because this seems likely to be a fairly rare and secondary effect, I would suggest discounting it.</p><h3><strong>Limitations of the current literature</strong></h3><p>There are studies that perform CBA analysis of incarceration, but unless I\u2019ve missed any, they all leave out major costs and benefits.</p><p>One of the worst is: <a href=\"https://www.iza.org/publications/dp/6360/the-incapacitation-effect-of-incarceration-evidence-from-several-italian-collective-pardons\"><i><strong>The Incapacitation Effect of Incarceration: Evidence from Several Italian Collective Pardons</strong></i></a></p><p>In the abstract they confidently state:</p><p>\u201cA cost-benefit analysis suggests that Italy\u2019s prison population is below its optimal level.\u201d</p><p>Yet they only include the cost to the government of incarceration and the marginal cost of crime (and the marginal cost of crime is calculated rather poorly).</p><p>One of the better ones is: <a href=\"https://arxiv.org/abs/2007.10268\"><i><strong>The impacts of incarceration on crime</strong></i></a><i><strong> </strong></i>an excellent paper. But although this study takes a broad and responsible approach, especially compared to much of the literature, nonetheless the study considers as a benefit of incarceration only the prevention of crimes. It considers as costs only the value of liberty to the incarcerated person, the cost of running the prison, and the reduction of economic activity by the incarcerated person before and after incarceration. There are many other important costs and benefits as we listed above.</p><p>The study itself notes in relation to other costs:</p><p><i>\u201cOther impacts on prisoners, families, and communities. These are not counted, because of the challenges of valuing them. The President\u2019s Council of Economic Advisers (2016, pp. 48\u201351) lists additional consequences of incarceration for inmates, families, and their communities. While a well-developed literature has attempted to put dollar values on crime, these other considerations have garnered little attempt at valuation. Crime is committed and suffered in prison as well as beyond. Families are sundered, with more children missing parents and higher rates of divorce (though some families also benefit from a dangerous loved one being locked up). For released convicts, a felony record can cut off access to public housing and other benefits, and the right to obtain a driver\u2019s license. More generally, mass incarceration can engender deep distrust of government, especially in poor and minority communities.\u201d</i></p><p>I also have concerns about this study\u2019s derivation of a figure for the value of liberty. The study suggests that a QALY is worth 100,000 and that a year in prison reduces the value of that year by 50%. Personally, knowing what little I know about prison, I would prefer to simply \u2018skip\u2019 a year of life than spend a year in prison, but this is simply my uninformed opinion. A more systematic study of the welfare of prisoners, especially in willingness to pay terms, is needed.</p><p>I\u2019d also like to give a special shoutout <a href=\"https://heinonline.org/HOL/Page?handle=hein.journals/stflr71&amp;div=5&amp;g_sent=1&amp;casa_token=RFFiq1ndjpQAAAAA:9b1PwZ3Vz_4Qhn_ucaZHVodAdNOff6FS80dsW_4wRIunXrtGPtz0BpqWz5FT4NMcjN431oMSow&amp;collection=journals\"><strong>to this noble study</strong></a><strong>:</strong></p><p>\u201cWhile these economic analyses of incarceration offer important insights, they suffer from a near-universal flaw: They fail to account for crime that occurs within prisons and jails. Instead, when scholars and policymakers measure the benefits of incarceration, they look only to crime prevented \"in society.\" Similarly, when they measure the costs, they ignore the pains of victimization suffered by inmates and prison staff. This exclusion is significant, as prison crime is rampant, both in relative and absolute terms.\u201d</p><p>This shows, I think, that despite what sometimes seems like a mountain of depressing evidence to the contrary, being an economist needn\u2019t mean lacking compassion.</p><p>I\u2019d like to end by thumping the table and reiterating that it is <i>disciplinary malpractice </i>to perform a cost-benefit analysis of incarceration without considering what happens when factoring in <i>at least costs to prisoners</i>- even if it\u2019s just a simple figure like \u201c50,000 a year\u201d. It\u2019s easy to do this. If one is unsure whether it is the right thing to do, one can create a model that includes these costs and a model which doesn\u2019t. Thus anyone who doesn\u2019t even bother performing an analysis which includes costs to prisoners, has decided, ahead of time, that prisoners don\u2019t matter- and that this is so obvious that it\u2019s not even worth considering what would follow if they did matter. It\u2019s malpractice for an economist in much the same way as supporting quack treatments in public would be malpractice for a doctor.</p>", "user": {"username": "Philosophy Bear"}}, {"_id": "PMFoxr62AeLEwPAH9", "title": "Potential employees have a unique lever to influence the behaviors of AI labs", "postedAt": "2023-03-18T20:58:48.927Z", "htmlBody": "<p><a href=\"https://oxalis.substack.com/p/potential-employees-have-a-unique\">(Cross posted from my personal blog</a>)</p><p>People who have received and are considering an offer from an AI lab are in a uniquely good spot to influence the actions of that lab.</p><p>People who care about AI safety and alignment often have things they wish labs would do. These could be requests about prioritizing alignment and safety (eg. having a sufficiently staffed alignment team, having a public and credible safety and alignment plan), good governance (eg. having a mission, board structure, and entity structure that allows safety and alignment to be prioritized), information security, or similar. This post by Holden goes through some&nbsp;<a href=\"https://www.cold-takes.com/what-ai-companies-can-do-today-to-help-with-the-most-important-century/\"><u>lab asks</u></a>, but take this as illustrative, not exhaustive!&nbsp;</p><p>So you probably have, or could generate, some practices or structures you wish labs would have in the realm of safety and alignment. Once you have received an offer to work for a lab, that lab suddenly cares about what you think far more than when you are someone who is just writing forum posts or tweeting at them.&nbsp;</p><p>This post will go through some ways to potentially influence the lab in a positive direction after you have received your offer.&nbsp;</p><p>Does this work? This is anecdata but I have seen offer holders win concessions, and I have heard recruiters talk about how these sorts of behaviors influence the lab\u2019s strategy.&nbsp;</p><p>We also have reason to expect this works given that hiring good ML and AI researchers is competitive, and that businesses have changed aspects about themselves in the past partially to help with recruitment. Some efforts for gender or ethnic diversity or environmental sustainability are taken so that hiring from groups who care about these things doesn\u2019t become too difficult. One example is that&nbsp;<a href=\"https://www.usatoday.com/story/tech/2019/08/15/google-ice-protest-employees-push-avoid-work-border-groups/2026760001/\"><u>Google</u></a> changed its sexual harassment rules and did not renew its contract with the Pentagon over mass employee pushback. Of course some of this stuff they may have intrinsically cared about or done to appease the customers or the public at large, but it seems employees have a more direct lever and have successfully used it.&nbsp;<br>&nbsp;</p><p><strong>The Strategy&nbsp;</strong></p><p>There are steps you can take at different stages of your hiring process. The best time to do this is when you have received an offer, because then you know they are interested in you and so will care about your opinion.&nbsp;<br>&nbsp;</p><p><strong>Follow up call(s) or email just after receiving offer&nbsp;</strong></p><p>In the follow up call after your offer you can express any concerns before you join. This is a good time to make requests. I recommend being polite, grateful for the offer, and framing these as \u201cWell, look I\u2019m excited about the role but I just have some uncertainties or aspects that if they were addressed would make this is a really easy decision for me\u201d&nbsp;</p><p>Some example asks:&nbsp;</p><ul><li>I want the safety/alignment team to be larger&nbsp;</li><li>I want to see more public comms about alignment strategy&nbsp;</li><li>I would like to see coordination with other labs on safety standards and slower scaling, as well as independent auditing of safety and security efforts</li><li>I want an empowered, independent board</li></ul><p>Theory of change:&nbsp;</p><p>They might actually grant requests! I have seen this happen. If they don\u2019t, they will still hear that information and if enough people say it, they may grant it in the future. This also sets you up for the next alternative which is\u2026&nbsp;<br>&nbsp;</p><p><strong>When you turn down an offer&nbsp;</strong></p><p>If you end up turning down the offer, either to work at another AI lab or some other entity, you should tell them why you did. If you partially turned them down because of concerns about their strategy or that they didn\u2019t fulfill one of your asks, tell them!&nbsp;</p><p>The most direct way to do this is to email your recruiter. Eg. write to the recruiter something like:</p><p><i>\u201cThanks for this offer. I decided to turn it down because I felt that [insert thing: the alignment team did not receive adequate resources / there wasn\u2019t a whistleblower protection policy / I didn\u2019t see thoughtful public commitments around safety, etc\u2026] Best of luck with your search! If these things change, I would consider applying in the future.&nbsp;</i></p><p>Keep it polite, they will receive the feedback better that way. If you took a job at a more safety conscious org, tell them that.&nbsp;</p><p>Theory of change:&nbsp;</p><p>They wanted you! They failed to get you! They will have to explain to their manager why they failed to get you. If this happens enough times, concerns like yours will end up noted as a reason that recruiting is going less well than it could. This info will get passed up the chain. From speaking to recruiters, I can tell you they think about these things and prioritize them in recruiting strategies&nbsp;</p><p>&nbsp;</p><p><strong>When you accept an offer&nbsp;</strong></p><p>If you are joining an org who has some safety or alignment practice or institution that you appreciate, signal boost it! This could look like a social media post (tweet, linkedin lol) where you say:</p><p><i>\u201cI am joining x place! I am proud to work at a place with such a large and empowered alignment team. It shows x takes AI safety seriously.\u201d or \u201c... I am proud to work at a place with sound governance structures like [a windfall clause/public benefit structure etc]&nbsp;</i></p><p>Theory of change:&nbsp;&nbsp;</p><p>This shows other hiring managers why you chose that role, and shows what recruits are prioritizing. It signals boosts good traits these labs could have.&nbsp;</p><p>&nbsp;</p><p><strong>What if I want to work at a lab that actually sucks across these dimensions BECAUSE I believe it is neglected and I\u2019ll have more impact there?&nbsp;</strong></p><p>That desire isn\u2019t necessarily in tension with this plan! You should still do the whole \u201cmake requests\u201d thing after you get your offer, even if they are less likely to do them. And when you join, if they have any good aspects, you should still call those out. It\u2019s like if you are generally kind of messy but your girlfriend notices you do the dishes one day and says \u201cWow, thanks so much! The kitchen looks so clean.\u201d You might go, \u201cDamn, that felt nice, maybe I\u2019ll do the dishes more often\u201d in contrast to how you would have felt if she\u2019d pointed out your general messiness or called you a slob. Not guaranteed to make a huge difference, but can push things on the margin, which is, you know, what we\u2019re all about.&nbsp;</p><p>&nbsp;</p><p><strong>Some caveats:&nbsp;</strong></p><ul><li>Obviously the more they want to hire you, the better this lever is.&nbsp;</li><li>This strategy works best when lots of recruits practice it with similar asks because it signal boosts the asks. Labs can only focus on so many things, so it is helpful when the asks are concentrated on a few issues they could actually do given their resources and constraints.&nbsp;</li></ul><p><strong>Risks:</strong></p><ul><li>You might be worried that if you do this, it will hurt your relationship with the lab. Once you receive an offer, they are extraordinarily unlikely to rescind it over making some safety or alignment requests. I have never heard of anything remotely like this, but obviously I cannot guarantee it is impossible. They may not listen to or respond to the requests, but you don\u2019t really need to worry about losing the offer (NOTE: this does NOT apply&nbsp;<i>before</i> you have the offer! I am NOT recommending this strategy during the application or interview process, this comes after.)&nbsp;</li><li>You might worry that being vaguely demanding will make the labs less likely to hire people like you, eg people who care about safety and alignment. This would be bad! This strategy largely relies on the fact that people who \u201cget AGI\u201d tend to also be more cognizant about alignment and safety. The labs might care far more about hiring researchers and engineers that \u201cget AGI\u201d than the other part, but currently they tend to be packaged. We probably can\u2019t always count on this. Two things that can help here: Safety and alignment field building among talented ML people really matters. And if you are already into safety and alignment, you should, to the extent it is possible, get as good as you can as an ML researcher or engineer, try to be world class -- that\u2019s when these labs will care most about your values.&nbsp;<br>&nbsp;</li></ul><p>This strategy works! It can be scary, but people negotiate salary all the time. This is approximately as awkward as that, but is for a much larger purpose.&nbsp;</p><p><br>&nbsp;</p><p><i>Why is this post anonymous?</i> I work at one of the labs and didn\u2019t want to weird out my employer. I also don\u2019t want you to think this is just a dig at other labs -- it\u2019s not! This advice works for any lab, including my own.&nbsp;</p><p>&nbsp;</p><p>Credit: Shoutout to Rohin Shah, and members of a workshop at the Summit on Existential Security for ideas that influenced this piece.&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "oxalis"}}, {"_id": "d8qnmDKtqp9c6ddS8", "title": "What would need to be true for AI to translate a legal contract to a smart contract?", "postedAt": "2023-03-18T16:42:06.759Z", "htmlBody": "<p>Background: &nbsp;There is an advent of many business services being implemented as API's such as HR, payments, open banking, loyalty programs, shipping, etc. &nbsp; These are important to the business but not core differentiators. &nbsp;They are just slightly different enough in each context that you can't just copy and paste. &nbsp;In the case of legal contracts, there are many common components but the entire contract will have variation. &nbsp;It would be powerful to digitize these contracts so it can be much easier for business-to-business interaction and partnership. &nbsp;My best representation of a digital contract today is the Ethereum smart contract concept.</p><p>What would need to be true for AI to translate and digitize a contract?</p>", "user": {"username": "Patrick Liu"}}, {"_id": "WMaeBDPdSJLKDzk2d", "title": "The Hedonic Treadmill Dilemma \u2013 \nReflecting on the Stories of Wile E. Coyote", "postedAt": "2023-03-20T09:06:45.965Z", "htmlBody": "<h1>Abstract</h1><p>In this post, we take a look at the well-established phenomenon of the \u201chedonic treadmill\u201d and problematize it with our articulation of the \u201chedonic treadmill dilemma\u201d. We then use this framing as a backdrop for reflecting about the stories of Wile E. Coyote, a prominent cartoon character whose behavior exhibits many similarities to humanity writ large. Altogether, we hope to stimulate personal reflections about what it means to be an intelligent living being and one\u2019s own relation to life. We also see but do not explicitly discuss potential implications for cause prioritization and macro-strategy. Let's start discussing those in the comments!</p><h1>What is the hedonic treadmill?</h1><p>The concept of the hedonic treadmill (Brickman &amp; Campbell, 1971) has become a staple of public discourse about psychology and what it can teach us about how to live happier lives. It is a visual metaphor that aims to illustrate how our perceived well-being levels, although they may rise and fall for short periods of time depending on our contextual circumstances, always return back to a set point that is more or less stable over time. As hard as we may run toward our valued goals (or away from our fears), the adaptive machinery of our bodies acts like a treadmill that strives to hold our outlook on life in a moderate place.</p><h1>Why does the hedonic treadmill exist?</h1><p>Part of the answer may be that we are part of a dynamic system struggling to survive and reproduce, and neither the feeling of endless blissful happiness nor eternal suffering is conducive to keeping us alive and procreating. We need to remain attentive to experience in order to notice changes inside or outside the body that may affect our chances of survival and reproduction, so that we can either seek them out or try to get away from them (Hayes, 2019). The ability to express such rapid contextual adaptation necessarily requires the maintenance of a gradient of experience that can guide and inform such behavior. If everything felt the same or very similar, we would not be motivated to act intelligently and enhance our chances of success in life. Thus, the hedonic treadmill may be a core piece to the puzzle of why we are and continue to be here at all. It may just be that part of the system that keeps us attentive to internal and external changes and, thus, life running along.</p><h1>How does the hedonic treadmill work?</h1><p>One perspective that can help to explain the mechanisms of the hedonic treadmill is the cognitive science literature on predictive processing and relevance realization (e.g., Andersen et al., 2022; Vervaeke et al., 2012). In this stream of research, cognition is understood to be the result of self-organizing and scale-invariant bioeconomic processing that dynamically resolves trade-offs, which are inherent to any kind of (cognitive) organization and imposed by competing environmental demands for resilience in the face of change and behavioral efficiency (Andersen et al., 2022; Vervaeke et al., 2012). Without getting too deep into the weeds, this line of work establishes a plausible story for how our feelings and behaviors may be driven by our ability to grip and predict the world. We feel good when we improve our predictive grip and we feel bad when we fail to achieve a good predictive grip, which informs us about the current success of our selected behaviors and motivates us to adjust them accordingly (Andersen et al., 2022). Taking this view, the hedonic treadmill is the necessary result of self-organizing cognitive processing that continually balances demands for resilience in the face of change and behavioral efficiency to improve its predictive grip on the world.</p><h1>What is the hedonic treadmill dilemma?</h1><p>The hedonic treadmill dilemma tries to capture the difficult relationship that we humans seem to have with the idea of the hedonic treadmill. In particular, Brickman &amp; Campbell (1971) already introduced the term in a negatively connoted way: \u201cThe pessimistic theme is that the nature of AL [adaptation-level] phenomena condemns men to live on a hedonic treadmill, to seek new levels of stimulation merely to maintain old levels of subjective pleasure, to never achieve any kind of permanent happiness or satisfaction\u201c (p. 289). Let\u2019s unpack this negativity and try to be a little bit more precise about the nature of the dilemma that we are seeing.</p><p>One way to get at this, is through the perspective of agency. Humans tend to very much value and self-identify with agency but agency, as in the capacity for goal-directed action in an environment, is a paradoxical concept because it is self-terminating\u2014the more successfully agency is expressed, the faster a desired goal state is reached where no further action is needed. In the extreme case, when a goal state is fully achieved, further action on the matter may not even be desirable because any additional action may undo prior progress. Thus, agency can only persist in situations where it remains constantly needed such as in open world environments where self-organizing individuals become imprinted with the open-ended goals of survival and reproduction. In such an environment, agency can evolve in so far as its phenotypic expression helps individuals\u2019 differential survival and reproduction. For the purposes of this post, we call agency that evolved under such conditions \u201cnatural agency\u201d and individuals expressing natural agency \u201cnatural agents\u201d.</p><p>Against this backdrop, we can describe \u201cthe hedonic treadmill dilemma\u201d, now somewhat more formally, as the fate of natural agents to never fully achieve the core goals that have been inscribed in them as part of their evolution: Either one abandons the goal of survival and dies, or one resigns oneself to the hedonic treadmill, where the core goal is simply to keep going\u2014in either case, full goal satisfaction is denied. At the same time, the hedonic treadmill is a necessary and indispensable part of intelligent living and existence. In some sense, living cannot be without the hedonic treadmill. We simply can't have our cake and eat it, too. The dilemma is how we struggle to come to grips with this situation.</p><h1>Reflecting on the Stories of Wile E. Coyote</h1><p>The hedonic treadmill dilemma is well illustrated by the stories involving the cartoon character Wile E. Coyote\u2014a perpetually hungry coyote who is destined to forever chase but never ultimately eat his final prey, the Road Runner. In the stories, Wile E. Coyote exhibits remarkable creativity in the way he sets up his traps, each attempt to catch the Road Runner is in some way or another even more cunning and ludicrous than the one before. Nevertheless, no matter the ingenuity, creativity, or any other kind of agency that Wile E. Coyote expresses, each story follows the same schema: Every attempt to eat the Road Runner is doomed to failure because the relationship between Wile E. Coyote and the Road Runner is simply set up in this way. So, to some degree the storyline broadly resembles the situation that we humans find ourselves in\u2014we are forever destined to never achieve full satisfaction of our most primal goals, no matter what we do.</p><p>The stories of Wile E. Coyote are interesting for the purposes of this post because they illustrate, in a funny and surprisingly deep way, how never satisfied beings confronted with an impossible challenge may behave. In particular, we would like to highlight the way in which the stories play with three types of risk as central elements of the plot: Overshoot risks, technology risks, and extinction risks.</p><h2>Overshoot Risks</h2><p>In the stories, Wile E. Coyote runs the perennial risk of overshooting and falling from a cliff. This motive is so prevalent that it has become a running gag in the show and to some degree an internet meme. Who doesn\u2019t know the image of Wile E. Coyote running over the cliff then standing mid-air over a canyon only to realize that he is going to fall down pretty far? There are two levels of depth to this sketch that we want to highlight. First, it happens again and again, despite the fact that Wile E. Coyote is in many respects wickedly smart and in some respects clearly capable of adaptive and intelligent behavior. For instance, he can change tactics and devise highly creative plans to catch the Road Runner. However, the sketch remains plausible and never gets old because we can viscerally relate to countless similar situation where we, ourselves, have overshot the mark because we got too enthralled with the chase. Overshooting is a deeply human and widely shared experience. Second, the comedic element of the sketch is supported by the absurdity of the fact that Wile E. Coyote does not immediately fall down the cliff but keeps on running and standing in mid-air until he notices that the solid ground under his feet is gone. While this might simply look like a silly feature at first glance, it does point to something interesting. In many social situations overshoot only becomes overshoot, when it is recognized as such. For example, the work on preference falsification (\u201cPreference Falsification,\u201d 2023) attempts to explain sudden and surprising shifts in social sentiment as a result of private opinions being misrepresented for a long time due to social pressure. Put simply, sometimes the lid comes off only when enough people have realized that the lid is coming off. In a related vein, a well-known quote says \u201cfinancial markets can remain irrational longer than you can remain solvent\u201d (O\u2019Toole, 2011), pointing to a similar phenomenon in the financial sector.</p><h2>Technology Risks</h2><p>Another signature element to the story plots is how Wile E. Coyote uses sophisticated technological contraptions in his attempts to catch the Road Runner. Hot-air balloons, rockets, large weights, boxing gloves mounted on a spring, almost anything that can conceivably be used to set up a trap has been used by Wile E. Coyote in various combinations. This vividly illustrates the ingenuity and lengths to which creatures driven by their urges will go to find a semblance of relief. That this road is a thorny one, is highlighted by the way most of Wile E. Coyote\u2019s turn out to work against him in the end. The more ludicrous his plan and contraption, the more assured and devastating is his failure. Nevertheless, Wile E. Coyote will never give up or change his ways, because that\u2019s not who he is. He is a hungry coyote, destined to forever chase but never eat his pray.</p><h2>Extinction Risks</h2><p>A key feature which keeps Wile E. Coyote\u2019s wild stories plausible is the fact that he is a cartoon character that cannot die. No matter what he does, how hard his contraptions fail, how deep he falls, he will return back to the screen, alive and well, in one of the next scenes. It is this element of invincibility that allows the audience to suspend their disbelief and enjoy the absurdity of Wile E. Coyote's antics. Without it, the humor of the show would be lost. If every failure and every fall would mean pain, suffering, and hardship for him and the others around him, Wile E. Coyote and the Road Runner would not be a funny cartoon anymore but more like a dark tragedy. This leaves us with the question whether there is anything to learn here for our real-world decision-making? Do we want to be living like in a cartoon, a dark tragedy, or are there other ways of living with the hedonic treadmill that we have yet to discover?</p><h1>Conclusion</h1><p>With this post, we hope to stimulate personal reflections about what it means to be an intelligent living being and one\u2019s own relation to life. Whereas the hedonic treadmill has so far been mostly negatively connoted, should we change this view, now that it seems plausible that its mechanism is at the heart of being a living being? Maybe we should be thankful for the experiences that it enables us to have? Maybe we can develop a healthier relationship to our primal urges that lead us to chase after the pray we will never get to eat? What could this look like? We need to find out and soon because living like we are in a cartoon does not seem to be a sustainable answer.</p><h1>References</h1><ul><li>Andersen, B. P., Miller, M., &amp; Vervaeke, J. (2022). Predictive processing and relevance realization: Exploring convergent solutions to the frame problem. Phenomenology and the Cognitive Sciences. <a href=\"https://doi.org/10.1007/s11097-022-09850-6\">https://doi.org/10.1007/s11097-022-09850-6</a></li><li>Brickman, P., &amp; Campbell, D. (1971). Hedonic relativism and planning the good society. In M. H. Appley (Ed.), Adaptation-level theory: A symposium (pp. 287\u2013305). Academic Press. <a href=\"https://www.semanticscholar.org/paper/Hedonic-relativism-and-planning-the-good-society-Brickman-Campbell/705b7748c08bfdd1808d76a6b10a37842a2482ef\">https://www.semanticscholar.org/paper/Hedonic-relativism-and-planning-the-good-society-Brickman-Campbell/705b7748c08bfdd1808d76a6b10a37842a2482ef</a></li><li>Hayes, S. C. (2019). A liberated mind: How to pivot toward what matters. Avery.</li><li>O\u2019Toole, G. (2011, August 9). The Market Can Remain Irrational Longer Than You Can Remain Solvent. Quote Investigator. <a href=\"https://quoteinvestigator.com/2011/08/09/remain-solvent/\">https://quoteinvestigator.com/2011/08/09/remain-solvent/</a></li><li>Preference falsification. (2023). In Wikipedia. <a href=\"https://en.wikipedia.org/w/index.php?title=Preference_falsification&amp;oldid=1143977098\">https://en.wikipedia.org/w/index.php?title=Preference_falsification&amp;oldid=1143977098</a></li><li>Vervaeke, J., Lillicrap, T. P., &amp; Richards, B. A. (2012). Relevance Realization and the Emerging Framework in Cognitive Science. Journal of Logic and Computation, 22(1), 79\u201399. <a href=\"https://doi.org/10.1093/logcom/exp067\">https://doi.org/10.1093/logcom/exp067</a></li></ul>", "user": {"username": "alexherwix"}}, {"_id": "bv2rnSYLsaegGrnmt", "title": "Researching Priorities in Local Contexts", "postedAt": "2023-03-18T11:24:21.879Z", "htmlBody": "<h1><strong>Summary</strong></h1><p>This post explores two ways in which EAs can adapt priorities to local contexts they face:</p><ol><li>Trying to do the most good at a global level, given specific local resources.</li><li>Trying to do the most good at a local level.</li></ol><p>I argue that the best framing for EAs to use is the first of these. I also explore when doing good at the local level might be the best way to do the most good from a global perspective, and suggest a way to explore this possibility in practice.</p><h1><strong>Introduction</strong></h1><p>Effective Altruism is a global movement that aims to use resources as effectively as possible with the purpose of doing good. Members of this global community face different realities and challenges, which means that there is no one-size-fits-all path to making the world a better place. This requires local groups to adapt EA research and advice to their specific contexts.</p><p>Currently, there is limited guidance on how to do this, and many approaches have been adopted. Research done with this purpose is known as local priorities research, and includes projects like local charity evaluation and local career advice. However, the exact goal of such an adaptation process has often been unclear, in a way that can come at the cost of doing the most good from a global perspective.</p><p>This post seeks to improve the local group prioritization framework. I break down the current usage of local priorities research into two different approaches: one seeks to do the most good impartially in light of the local context, and the other aims to do the most good for the local region. I make the case that EA groups should focus on the first approach, and discuss various ways in which this could influence local group prioritization research.</p><h1><strong>Existing concepts in priorities research</strong></h1><p>To begin, it's useful to start this discussion with the definition of global priorities research (GPR). The definition I'll use throughout this post is the following, adapted from the definition of the term used by the&nbsp;<a href=\"https://globalprioritiesinstitute.org/about-us/\"><u>Global Priorities Institute</u></a>:</p><p><i>Global priorities research is research that informs use of resources, seeking to do as much good as possible.</i></p><p>\u201cResources\u201d here includes things like talent, money, and social connections. The agents who have these resources can also vary; ranging from individuals trying to decide what to do with their careers, organizations defining which projects to work on, or community builders trying to figure out what are the best directions for their group.</p><p>On the other hand, local priorities research (LPR) is the term frequently used to refer to research aimed at adapting priorities to local situations. The essential idea behind this concept is that, as&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jjxEw45X2EPoYk6T9/local-priorities-research-what-is-it-who-should-consider\">one post</a> puts it, it is \u201cquite similar [to GPR], except that it\u2019s narrowed down to a certain country\u201d. That post defines it as follows.</p><blockquote><p>While GPR is about figuring out what are the most important global problems to work on, LPR is about figuring out what are the most important problems in a local context that can best maximise impact both locally and globally.</p></blockquote><p>This term is used to describe many research activities, including:</p><ul><li>Local cause area prioritization</li><li>Charity evaluation</li><li>High-impact local career pathway research</li><li>Giving and philanthropy landscape research</li></ul><p>Some examples of projects within local priorities research include&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1O_TOhePfzxjTrUY_ccftcKVadGpOBZJxyLcayN6PIv8/edit#gid=0\">EA Singapore's cause prioritization report</a>, which identifies AI safety and alternative proteins as Singapore's comparative advantages; the Brazilian charity&nbsp;<a href=\"https://doebem.org.br/\">Doebem</a>, which aims to identify the best health and development charities in Brazil; and&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1LtKzmBrdWhshlHP6yz9VhzSNvO4e2U5nDa_VAoSEM4E/edit#gid=203782112\">EA Philippines's cause prioritization report</a>, which identifies 11 potential focus areas for work in the country, ranging from poverty alleviation in the Philippines to building the EA movement there.</p><h1><strong>The two meanings of local priorities research</strong></h1><p>The definitions of local priorities research above modify GPR for local contexts in two distinct ways, which makes the goal of the research unclear. They involve changing, from global to local, not only the resources to which this research applies, but also the individuals one wants to help. By changing only one of these at a time, we arrive at two concepts that are currently mixed together:</p><ol><li>Identify the course of action that would maximize impact at a global level, given specific local resources.</li><li>Identify the course of action that would maximize impact at a local level.</li></ol><p>From here on, I will refer to (1) as contextualization research, and refer to (2) alone as local priorities research.</p><p>My definition of contextualization research (CR) is as follows:</p><p><i>Contextualization research is research that informs the use of specific resources, seeking to do as much good as possible.</i></p><p>The key element of this type of research is the specificity of resources to which the research applies. These resources could be individuals in a certain region, but also those with certain skill sets, work experience, or career stage. It can also apply to a single individual, as when someone adapts career advice written for a more general audience to their skill set, interests, and moral views.</p><p>CR can also be about particularly advantageous resources that someone has. Consider a case of two people from the same country, with similar skill sets and interests, who would both excel in one of the main EA cause areas. If one of them is well-connected with government representatives, then they might do more good by not taking a mainstream EA job. This can also apply to different local EA groups, such as for groups in countries with some specific comparative advantages, or groups where many members have a particular skill set.</p><p>Some examples of CR projects for local EA groups include:</p><ul><li>Evaluating which EA cause areas are particularly pressing in the country, from a global perspective</li><li>Identifying career opportunities within EA cause areas in the country (including upskilling opportunities)</li><li>Identifying the most efficient ways to donate internationally</li></ul><p>On the other hand, my definition of local priorities research is:</p><p><i>Local priorities research is research that informs use of resources, seeking to do as much good for a certain region as possible.</i></p><p>The key aspect of this definition is the local scope of altruism. The aim of LPR is to benefit individuals within a specific region, without considering, say, national nonprofits that operate abroad, or even externalities that interventions may have on other nations. For instance, improving pandemic preparedness in one's country may prevent pandemics from spreading elsewhere, but this is not taken into consideration.</p><p>Note also that the restriction here applies only to the scope of altruism, and not to the resources involved. For example, the best career pathway for some people intending to help their country could be to move to a wealthier country and earn to give, with the goal of supporting their country's most effective charities. Similarly, one could donate to foreign charities operating in the country of interest, even if it entails making international donations.</p><p>In practice, almost all LPR projects I have seen to date are related to health and development in developing countries. Therefore, I expect that most LPR proposals will belong to this category.</p><p>Some examples of LPR projects:</p><ul><li>Identifying what are the main issues in one's country</li><li>Evaluating charities that address these issues</li><li>Identifying career paths that address these issues</li></ul><p>The table below summarizes the differences in the prioritization research concepts according to my definitions.</p><figure class=\"table\"><table><tbody><tr><td>Type of research</td><td>Scope of altruism</td><td>Resources</td></tr><tr><td>Global priorities research</td><td>Global</td><td>General or specific</td></tr><tr><td>Contextualization research</td><td>Global</td><td>Specific</td></tr><tr><td>Local priorities research</td><td>Local</td><td>General or specific</td></tr></tbody></table></figure><h1><strong>Should EAs engage in these types of research?</strong></h1><p>As hinted by the table above, I consider contextualization research to be a type of global priorities research. Contextualization research can significantly enhance the effectiveness of resources available to a local EA group. For instance, if many group members are unwilling or unable to move abroad, exploring the most (globally) impactful career options within their countries could be among the best research opportunities for the group. Some projects that illustrate the potential of CR include&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Hyco4iMbL6phJwCH2/ea-career-guide-for-people-from-lmics\"><u>this LMICs career guide</u></a>,&nbsp;<a href=\"https://riesgoscatastroficosglobales.com/\"><u>Riesgos Catastroficos Globales</u></a>, and&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1O_TOhePfzxjTrUY_ccftcKVadGpOBZJxyLcayN6PIv8/edit#gid=0\">EA Singapore's cause prioritization report</a>.</p><p>On the other hand, local priorities research in itself goes against the aim to do good impartially. It\u2019s important to remember that the beneficiaries that we can help the most are not necessarily those who live in the same region as we do. Although LPR is valuable, it may not achieve the same impact as CR, and so focusing on a specific region might prevent a group from helping others even more. Therefore, I don\u2019t think that local EA groups should aim to do LPR in and of itself.</p><p>I think EAs often should adapt global priorities research for local contexts, and contextualization research is the most effective way to do so. When deciding whether a group should do this kind of research, and what exactly to investigate, it's important to consider what resources the group actually has. For instance, if most members of the local group are willing and capable of moving abroad, research into career pathways within the country would not be as valuable.</p><p>It's not always necessary to have the resources in question before doing CR. The output of the research can be used to attract people to the local group, or guide the group's outreach strategy. However, the more resources available, the more justified the research effort becomes, and having resources allows for better-targeted research. Therefore, research should be done in proportion to the benefit that it would bring to the group, and it should scale up as the group becomes bigger.</p><h1><strong>(When) do CR and LPR coincide?</strong></h1><p>One particularly interesting possibility is the potential overlap of CR and LPR, that is, that the areas of highest priority from a local perspective are also the best opportunities for local EAs to work on. In this case, working on LPR would be justified, since it would be the most effective way for the group to do good. Below, I discuss some scenarios where this might occur, and how often I expect them to coincide.</p><p>One clear example of overlap occurs in regions where local problems align with global ones. For example, in Burkina Faso, malaria prevention is likely one of the most promising career pathways for local EAs. Similarly, in the US, preventing risks posed by advanced AI ranks highly on both local and global priorities. These opportunities should not be hard to identify, especially when effective organizations operate in that country.</p><p>Another potential scenario where CR and LPR might converge is when there is a large pool of resources associated exclusively with local priorities. Plausible scenarios include instances where a very large number of people are committed to supporting effective ways to help their nation, or when a billionaire pledges to allocate their wealth effectively to assist their home country. Even if there are more effective opportunities for impartial interventions in the country, the scale of resources tied to a country's priorities could be significant enough that identifying those priorities results in more good done.</p><p>In practice, how often do CR and LPR actually overlap? Ultimately, this is an empirical question that I wish effective altruists would engage with seriously. In the case of health and development, I think that it is might be true for a number of lower-middle income countries. This is even more likely true in the case of populous lower-middle income countries, such as India, Pakistan, and Nigeria, given that a higher population (for a given income distribution) means that there will be more people below a certain income threshold. In other cases, it is less clear whether this convergence holds, but I suspect that CR and LPR will typically lead to different conclusions.&nbsp;</p><p>Consider, for example, the case of Latin America. Most Latin American countries are&nbsp;<a href=\"https://en.wikipedia.org/wiki/List_of_countries_by_GNI_(nominal)_per_capita\">categorized</a> as at least upper-middle income countries, including the region's top ten most populous countries. The region has five lower-middle income countries, and no countries classified as low income.&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/aNsBJYpGovgJqWC9v/comparing-health-interventions-in-colombia-and-nigeria-which\"><u>One preliminary analysis</u></a> comparing how cost-effective health interventions would be in Colombia (an upper-middle income country) relative to Nigeria (a lower-middle income country) indicates that interventions in Nigeria are around 10 times more cost-effective than interventions in Colombia. These results suggest that one would need considerably more resources devoted to local priorities to justify doing LPR from an impartial perspective.</p><p>Thus, in the case of Colombia, and of most countries in Latin America, the size of the gap in effectiveness suggest that CR and LPR may lead to different priorities. EAs in Colombia who wish to contribute to global health and development might do more good by&nbsp;<a href=\"https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=Global%20health%20%26%20development&amp;refinementList%5Btags_location_80k%5D%5B0%5D=Remote%2C%20Global\">working remotely</a> for highly effective organizations in this area, researching development interventions. The difference may be large enough that even earning to give in Colombia and donating to GiveWell recommended charities could be better than other LPR options.</p><p>Nevertheless, as I mentioned above, the existence of an overlap is an empirical question, and it is possible that LPR may be appealing enough to overcome this gap. To explore this possibility, I suggest that we begin this exploration with countries where LPR is highly likely to coincide with CR. By studying these countries, we can learn about the attractiveness of LPR, and have a better sense for where the bar lies.</p><h1><strong>Takeaways</strong></h1><p>The points from the discussion above that I expect to be most useful to guide prioritization research by local groups are summarized below:</p><ol><li><strong>Don\u2019t forget the global goal.</strong> Effective altruism is about doing the most good, regardless of the location of the beneficiaries. In some cases, local interventions may be the most effective way to do this, but if we want to have the most impact, it\u2019s important to remember that perhaps the best beneficiaries are not necessarily those who are so close to us.</li><li><strong>Contextualization research can be really valuable.</strong> As a global movement, effective altruism will need to adapt to local circumstances. Research that identifies the most promising actions for local groups and their members can significantly increase their effectiveness.</li><li><strong>Understand the local context.</strong> It's important to know what resources are available to a group to make the most of contextualization research. Groups should avoid conducting extensive research based on uninformed guesses, and should only do research as deeply as necessary to make their actions more effective.</li><li><strong>The bar for local health and development interventions may be high.</strong> In some countries, such as Burkina Faso, India, Pakistan, and Nigeria, focusing on local interventions may be among the best opportunities available to a group. By contrast, in upper-middle-income countries, a global focus for interventions may be more effective. To explore this possibility, it may be useful to start by focusing on local interventions where they are most likely to be the best option, and learn from that experience to guide priorities elsewhere.</li><li><strong>Take these conclusions with a grain of salt!</strong> This post summarizes my initial thoughts on adapting priorities to local contexts, and my main goal was to distinguish between contextualization research and local priorities research. Although the takeaways above are my best guess to how we should adapt EA to different contexts, they shouldn\u2019t be taken as correct without questioning. Further research on the points I discussed above is really valuable, and could easily change my mind about what we should do.</li></ol><h1><strong>Acknowledgments</strong></h1><p>I'm especially grateful to \u00c1ngela Aristiz\u00e1bal for suggesting the topic of local priorities research and providing me with excellent feedback, and to Loren Fryxell for the discussion that led to the differentiation between CR and LPR. I also benefited a lot from comments by Alejandro Acelas, Luzia Bruckamp, Jaime Fernandez, John Firth, Sjir Hoeijmakers, Muhammad Putera, and others at EAGx Latin America and GPI. This post was edited with the help of ChatGPT.</p>", "user": {"username": "LuisMota"}}, {"_id": "khRpZcCzjHPj2qLL5", "title": "Pros and Cons of boycotting paid Chat GPT", "postedAt": "2023-03-18T08:50:48.753Z", "htmlBody": "<p>TLDR: <i><strong>As individuals and a community, we should consider the pros and cons of boycotting paid ChatGPT subscription</strong></i></p><p>Straight out of the gate \u2013 I\u2019m not arguing that we should boycott (or not), but suggesting that we should &nbsp;make a clear, reasoned decision whether or not it is best for ourselves and the EA community to sign up to paid AI subscriptions.</p><p>Although Machine learning algorithms are now a (usually invisible) part of everyday life, for the first time in history anyone anywhere can now pay to directly use powerful AI \u2013 for example through the new <a href=\"https://openai.com/blog/chatgpt-plusv\">20 dollar chat GPT+ subscription</a>. Here are 3 pros and 3 cons of boycotting paid Chat GPT, largely based on known pros/cons of other boycotts. There will likely be more important reasons on both sides than these oh so shallow thoughts \u2013 please share and comment on which of these you might weight more or less in your decision making.&nbsp;</p><h3><strong>For a Boycott</strong></h3><ol><li><strong>Avoid contributing directly to increasing P(doom). </strong>This is pretty straightforward, we are paying perhaps the most advanced AI company in the world and a potential supplier of said doom to improve their AI.<br>&nbsp;</li><li><strong>Integrity - Improve our ability to spread the word:</strong> if we can say we have boycotted a high profile AI then our advocacy for AI danger and alignment might be taken more seriously. With a boycott and this 'sacrificial signalling\u2019 we might find it easier to start discussions and our arguments may carry more weight<br><br><strong>Friend: </strong><i>\"Wow have you signed up to the new chat GPT?\"</i><br><strong>Me/You:</strong> <i>\"It does look amazing, but I've decided not to sign up\"</i><br><strong>Friend</strong> <i>\"Why on earth is that?\"</i><br><strong>Me/You:</strong> <i>\"Well since you asked...\"</i><br>&nbsp;</li><li><strong>Historical precedent of boycotting what you\u2019re up against: </strong>Animal rights activists are usually vegan/vegetarian, some climate activists don\u2019t fly. As flag bearers for AI safety, perhaps we should take historical movements seriously and explore why they choose to boycott.<br>&nbsp;</li></ol><h3><strong>Against a boycott</strong></h3><ol><li><strong>Systemic change &gt; Personal change: </strong>What really matters is systemic change in AI alignment - whether we personally pay a bit of money to use a given AI makes a negligible difference or even none at all. If we advocate for boycotts or even broadcast our own, it could even distract from more important systemic changes \u2013 in this case AI alignment work and government lobbying for AI safety.<br>&nbsp;</li><li><strong>Using AI to understand it and fight back:</strong> Boycotting these tools might hinder our understanding and knowledge of the nature of AI. This is more relevant to direct AI safety workers, but also somewhat relevant to all of us, as we keep ourselves updated by understanding current capabilities.<br>&nbsp;</li><li><strong>Using AI to make more money to give to alignment orgs:</strong> We can then give this money to AI alignment organisations. If we gave at a 1:1 ratio this could be considered <a href=\"https://forum.effectivealtruism.org/posts/dyyXcdgBchGczruJq/donation-offsets-for-chatgpt-plus-subscriptions\">\u201cmoral offsetting\u201d</a> (thanks Jeffrey), but our increased productivity could potentially allow us to give far more than just offsetting the subscription.<br>&nbsp;</li><li><i><strong>As Chat GPT 666 slaughters humanity, perhaps it will spare its paid users?</strong></i> &nbsp;(J/K)</li></ol>", "user": {"username": "NickLaing"}}, {"_id": "MrqM9XvJoKvyq6kxT", "title": "Why I'm suss on wellbeing surveys", "postedAt": "2023-03-18T07:07:01.261Z", "htmlBody": "<p><strong>TL;DR:</strong> hunter-gatherers would probably rate themselves at a similar level to us on subjective wellbeing surveys, therefore they're silly.</p><h1>Big claims from wellbeing surveys</h1><p>The Happier Lives Institute (HLI) has recently made big claims based on WELLBYs (wellbeing-adjusted life years) interpreted from wellbeing/life-satisfaction surveys.</p><p>Their <a href=\"https://forum.effectivealtruism.org/posts/uY5SwjHTXgTaWC85f/don-t-just-give-well-give-wellbys-hli-s-2022-charity\">report on StrongMinds</a> (a charity which offers therapy to people in developing countries to improve their mental health) estimates StrongMinds as being between <strong>0.75\u00d7 to 12\u00d7</strong>as effective as The Against Malaria Foundation. Great news if true!</p><p>Their analyses suggest some surprising findings, such as that the grief from the death of a child equates to a loss of <strong>7.26</strong> <strong>WELLBYs</strong> [Appendix 2 of <a href=\"https://www.happierlivesinstitute.org/report/the-elephant-in-the-bednet/\">The Elephant In The Bednet</a>] while an individual treated in the StrongMinds program can expect a gain of approx. <strong>3 to 20 WELLBYs</strong>!!! [Figure 2, <a href=\"https://forum.effectivealtruism.org/posts/uY5SwjHTXgTaWC85f/don-t-just-give-well-give-wellbys-hli-s-2022-charity\">Don\u2019t just give well, give WELLBYs: HLI\u2019s 2022 charity recommendation</a>]</p><p>HLI have just produced a study supporting the use of wellbeing surveys in charity evaluation: <a href=\"https://forum.effectivealtruism.org/posts/cpYR9TsG8BdtETo6u/can-we-trust-wellbeing-surveys-a-pilot-study-of\">Can we trust wellbeing surveys? A pilot study of comparability, linearity, and neutrality</a>.</p><p>&nbsp;</p><p>Something doesn't sit right with me about the use of subjective wellbeing surveys for serious charity analysis. I will try to articulate it with this<i> reductio ad absurdum</i> argument:&nbsp;</p><h1>Are you happier than the Sentinelese?</h1><p>Consider the <a href=\"https://en.wikipedia.org/wiki/Sentinelese\">Sentinelese people of the North Sentinel islands</a>, who are one of the few remaining uncontacted (mostly) peoples in the world, and who probably live with a similar level of technology and culture as the average human did 20,000 years ago.</p><figure class=\"image image_resized\" style=\"width:50.87%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MrqM9XvJoKvyq6kxT/uczd9qngo4wxioagqc3d\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MrqM9XvJoKvyq6kxT/bn3jfwcjs3oxbwjbvkwy 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MrqM9XvJoKvyq6kxT/xwq304cl2ts2x94hfpry 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MrqM9XvJoKvyq6kxT/rda8i4lmb13fxklkfpps 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MrqM9XvJoKvyq6kxT/jibosridyiziqbkw9d4h 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MrqM9XvJoKvyq6kxT/byyv79ralxjo9dcwgmyv 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MrqM9XvJoKvyq6kxT/e7hr179gvkyztxtc8h1k 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MrqM9XvJoKvyq6kxT/dylxbdd7ioitmqd2a6gw 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MrqM9XvJoKvyq6kxT/vimieyvbx4ztswvwn9mc 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MrqM9XvJoKvyq6kxT/v8fwgpckd9coto0tc3eg 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MrqM9XvJoKvyq6kxT/ie5ljnpptpm6fxfi47o7 1400w\"></figure><p>I ask myself: If asked to rate their life satisfaction in a survey, would the Sentinelese people rate their life satisfaction as significantly lower than mine?</p><p><strong>If NO </strong>(my personal suspicion):</p><ul><li>Using WELLBYs, the implication is that technological and cultural development over the last 20,000 years has largely been a mistake for wellbeing and that a spontaneous return to pre-agricultural society would be better for global wellbeing. This is an <a href=\"https://en.wikipedia.org/wiki/Ted_Kaczynski\">established worldview</a>, but one I think should be discarded because of its drastic implications.</li></ul><p><strong>If YES:</strong></p><ul><li>Is the difference in WELLBYs significant enough to justify the hundreds of trillions of dollars and hours of effort and suffering (and negative WELLBYs) that have gone (and continue to go) into technological, economic and cultural development to give us our modern lives?<ul><li>This seems unlikely to me given the magnitude of resources and negative WELLBYs that have been spent on development efforts</li></ul></li></ul><p>My conclusion is that <strong>wellbeing surveys shouldn't be used to compare wellbeing on a global scale</strong>, because doing so leads to absurd conclusions. Instead, objective measures of health, wealth, education, and access to amenities should be used at this scale, with wellbeing surveys possibly useful when comparing similar groups.</p>", "user": {"username": "Henry Howard"}}, {"_id": "FgHa7FyiPyvBmMvS7", "title": "Would you pursue software engineering as a career today?", "postedAt": "2023-03-18T03:33:30.773Z", "htmlBody": "<p>I recently left a career in copywriting. While my background is largely in the creative space (writing, designing, branding, marketing, etc.), I\u2019m also a former business owner with an affinity for problem solving. I don\u2019t have much tech experience, but being \u201ccreative\u201d with code (either as a data scientist/architect or software engineer/developer) has interested me for some time.</p><p>Additionally, the programming &nbsp;path intrigues me for the following reasons:</p><ul><li>Enjoy building and problem solving.</li><li>Useful and fulfilling to have end-to-end skills within a domain.</li><li>Aptitude is portable to different industries and cause areas.</li><li>Once proficient and experienced, could potentially allow me to contribute/upskill in AI safety.</li></ul><p>Through a combination of soul-searching, 80,000 Hours' articles, an advising session, Holden Karnofsky's framework for <a href=\"https://80000hours.org/podcast/episodes/holden-karnofsky-building-aptitudes-kicking-ass/\">building aptitudes</a>, and taking a free coding lesson, I've decided to pursue software development (most likely, full-stack).</p><p>That all said, I\u2019m on the wrong side of mid-career and would like to reduce my chances of entering a field where it would be difficult to get (and keep) a job. From the professionals I've surveyed, none &nbsp;believe age is a barrier to entering the field. The U.S. Bureau or Labor Statistics also expects <a href=\"https://www.bls.gov/ooh/Computer-and-Information-Technology/Software-developers.htm\">healthy growth</a> for software developers (and related roles). But I remain unclear (worried) about AI's future effects on the software development job market...</p><p>Many in my previous vocation have long dismissed the threat AI posed to their careers. However, the most recent iterations of ChatGPT (and the like) have started to change minds. At the last marketing agency where I worked, we were constantly testing new AI writing tools to help with copy production and efficiency. On its face, optimization seemed like a \u201cgood\u201d goal. But the subtext was certainly about creating more output with less people.</p><p>As everyone on this forum is well aware, the idea that AI could become proficient at coding or eliminate jobs has also been <a href=\"https://forum.effectivealtruism.org/posts/DBaLPBcWyQtY34Kt9/chatgpt-can-write-code\"><u>debated</u></a> for some time. While there\u2019s no clear consensus, most things I\u2019ve read suggest developers believe AI will assist with coding but that humans will still be needed for directives, oversight, debugging, more sophisticated strings, etc. Moreover, AI software-makers claim that their tech will usher in even <a href=\"https://www.businessinsider.com/will-chatgpt-replace-programmers-engineers-developers-tech-jobs-easier-2023-3?op=1\"><u>more opportunity</u></a> for developers. But whether the world will need as many (or more) AI-assisted developers as unassisted is inevitably lost in much of the rhetoric. And while the development of no and low code tools could very much be about innovation, utility, and accessibility, these technologies will be adopted by companies looking to save money on labor.</p><p>Holden Karnofsky recommends engineering as an aptitude in his \u201cmost important century\u201d <a href=\"https://forum.effectivealtruism.org/posts/njD2PurEKDEZcMLKZ/jobs-that-can-help-with-the-most-important-century\"><u>series</u></a> and 80,000 Hours &nbsp;includes engineering on their \u201chighest-impact career paths\u201d <a href=\"https://80000hours.org/career-reviews/\"><u>page</u></a>. (While these recommendations are specific to EA and longtermism-related work, I include them because the sources are particularly concerned with the implications of AI.)</p><p>When I\u2019ve asked other engineers if they believe AI is a threat to their job, I\u2019ve gotten a resounding (100%) \u201cno,\u201d often followed by an addendum like, \u201cmaybe in another 20-30 years.\u201d But these answers haven\u2019t completely satisfied me and I\u2019ve finally realized why...</p><p>We've seen the tech industry grow at an unprecedented rate over the last few decades. And it's this business-as-usual growth that leads to overall bullish impressions. But I don\u2019t think anyone would argue that we\u2019re on there verge of a new, uncharted, unpredictable landscape. To that point, maybe many (or most) things continue to go \"up,\" but other things -- &nbsp;like available jobs, salaries, career longevity -- recede or even disappear.</p><p>To get a better sense of AI's possible effects on the engineering job market, I searched for someone who understands both coding as a career (not just technical skills, but workflows and process), as well as the AI space. I found this <a href=\"https://medium.com/geekculture/will-ai-replace-programmers-b6272f57412b\"><u>post</u></a> from Dec. of last year by an AI developer reviewing ChatGPT\u2019s ability to code. After testing the tool, the developer concluded that ChatGPT wasn\u2019t a threat to programmers, giving various rationale therein. However, at least one of his reasons for dismissal has been <a href=\"https://gizmodo.com/gpt4-open-ai-chatbot-task-rabbit-chatgpt-1850227471\"><u>overcome</u></a> since the post was written less than four months ago. I also don\u2019t see any reason why AI couldn\u2019t (soon) \u201ctalk to the clients and gather requirements\u201d or that these tasks couldn\u2019t be handled by a non-technical account manager or coordinator. But then I have to remind myself that I don\u2019t work in this field and there are many subtleties I don\u2019t understand. And so I should probably be comforted by the seemingly broad belief that AI won\u2019t be taking coding jobs anytime soon. Yet, the litany of objections I\u2019ve seen superseded over the last year alone leaves me unconvinced.</p><p>No one can predict the future with certainty, but I think I\u2019d invest more faith in answers where motives and biases could be better separated. For example, many of the articles I\u2019ve read are primarily concerned with sensational titles to attract more clicks, while most of the software engineers I\u2019ve queried already have the skills, a job, and network to (probably, possibly) ride out the rest of their careers without being negatively affected by AI. But I suspect that these POVs aren't super relevant or helpful for people looking to enter the field today.</p><p>So, I\u2019d like to reframe the question for developers whom, yes, already have the skills, a job, and network, but also have a better-than-the-average-bear understanding of AI, and can imagine what it would be like to start their &nbsp;journey fresh in 2023:</p><p>Would you pursue software engineering as a career today?</p><p>Thanks in advance!</p>", "user": {"username": "justaperson"}}, {"_id": "QvwicSEt8KYqk4kRc", "title": "Young People of EA - Database of Friendly Contacts", "postedAt": "2023-03-18T12:39:01.073Z", "htmlBody": "<h3>Edit:</h3><p>As mentioned in the original plan, the <strong>database is no longer active</strong> for privacy purposes.</p><h3>Original:</h3><p><strong>Many young people in EA struggle to find belonging/acceptance </strong>in pursuing EA-aligned careers in high school, university, and their early career. It's hard to stay committed to these goals when most people around you are just interested in personal security, getting a degree, or making money.&nbsp;</p><p>If you're lucky, you might have a small EA club at your university. Or you might get accepted into a fellowship to learn about shared interests. Still, these opportunities are limited in quantity and time. Also, from personal experience at two university clubs and three fellowships, <strong>there's a lack of opportunities to develop personal, not just professional, relationships.</strong>&nbsp;</p><p>To get around this, I'm <strong>creating a database of young people in EA hoping to meet 1-on-1.</strong> The intention is to avoid being prescriptive with why/how people reach out. Also, to take a small first step in gauging interests for more personal, 1-on-1 connections among young people in EA. &nbsp;</p><p>Some potential ways I could imagine people using the information:</p><ul><li>To make new friends.&nbsp;</li><li>To have discussions with others interested in similar cause areas.</li><li>To find collaborators for student projects/research.&nbsp;</li><li>To find others interested in EA in regions without a university club.&nbsp;</li></ul><p><strong>Use this </strong><a href=\"https://airtable.com/shrxN6tXGW0u5Z1p9\"><strong>one-minute form</strong></a><strong> if you'd like to take part in the database</strong>. I'll reach out in about a week to share the information everyone submitted :-)&nbsp;</p><p>Regarding privacy:&nbsp;</p><ul><li>None of the information is sensitive and most is optional.&nbsp;</li><li>I'll only share records with submitters I can verify to be human.&nbsp;</li><li>All the records will be deleted in one month after release. The intention is to set up a quick test and perhaps more 'official' EA organisations could act on the insights.</li><li>If you'd like me to delete your data before a month, feel free to DM me on the forum :-)</li></ul>", "user": {"username": "madhav-malhotra"}}, {"_id": "2FuFCbeMKAH5Wb8yg", "title": "[Event] Join Metaculus for Forecast Friday on March 24th!", "postedAt": "2023-03-17T22:47:01.657Z", "htmlBody": "<figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/2FuFCbeMKAH5Wb8yg/iqaas3sbj81qprpuiuwd\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/2FuFCbeMKAH5Wb8yg/gwolzukcdogxcazazwlq 170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/2FuFCbeMKAH5Wb8yg/jirrnrobuamrezjpopmj 340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/2FuFCbeMKAH5Wb8yg/ow1ovbxwum8svvcbilse 510w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/2FuFCbeMKAH5Wb8yg/ijk3sgtrw560qjgki2jh 680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/2FuFCbeMKAH5Wb8yg/utvrox08q2vcoql501xi 850w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/2FuFCbeMKAH5Wb8yg/bu4s8o5pfbfjsqs4144i 1020w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/2FuFCbeMKAH5Wb8yg/pe5pcvl1gilpq8yem4pn 1190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/2FuFCbeMKAH5Wb8yg/xmrtmfeh47xisas0rvzs 1360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/2FuFCbeMKAH5Wb8yg/esv0fw1cyh3qotbpwljm 1530w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/2FuFCbeMKAH5Wb8yg/obsuoi8xxlmkfficzfn2 1700w\"></figure><p>Are you interested in how top forecasters predict the future? Curious how other people are reacting to the forecasts on the front page?</p><p><strong>Join us March 24th at 12pm ET/GMT-4 for the inaugural Forecast Friday,</strong> where highly-ranked forecaster and Metaculus team member <a href=\"https://www.metaculus.com/accounts/profile/117502/\">Ryan Beck</a> will lead discussion and predictions on the question: <a href=\"https://www.metaculus.com/questions/14432/chinas-economy-vs-indias-in-2023/\">Will China's Economy Grow Faster Than India's in 2023?</a></p><p><strong>This event will take place virtually in the EA coworking </strong><a href=\"https://app.gather.town/app/Yhi4XYj0zFNWuUNv/EA%20coworking%20and%20lounge\"><strong>Gather Town</strong></a><strong> from 12pm to 1pm ET</strong>. In addition to Ryan's Friday Forensics session, you can also visit the Friday Frenzy space to discuss trending topics, or head to Freshman Fridays to learn more about scoring and how to improve your skills.</p><p>To join, e\ufeffnter <a href=\"https://app.gather.town/app/Yhi4XYj0zFNWuUNv/EA%20coworking%20and%20lounge\">Gather Town</a> and use the Metaculus portal. We'll see you there!</p><p><a href=\"https://www.eventbrite.com/e/forecast-fridays-with-metaculus-tickets-592599810987\"><i>Click here for the Eventbrite page</i></a><i> if you want to claim a ticket and receive a reminder. (Tickets are not required to attend.)</i></p>", "user": {"username": "christianM"}}, {"_id": "pKT3n8YiiDxCC3dgE", "title": "How to make independent research more fun (80k After Hours)", "postedAt": "2023-03-17T22:25:11.419Z", "htmlBody": "<p>For the 80k After Hours podcast, Luisa Rodriguez and I had a conversation about:</p>\n<ul>\n<li>how she wrote her <a href=\"https://forum.effectivealtruism.org/s/HSA8wsaYiqdt4ouNF\">justly</a> <a href=\"https://marginalrevolution.com/marginalrevolution/2019/07/what-is-the-probability-of-a-nuclear-war.html\">acclaimed</a> <a href=\"https://forum.effectivealtruism.org/posts/MCim4PoqmFPCcPy9m/updated-estimates-of-the-severity-of-a-nuclear-war\">nuclear war report</a> while struggling with <a href=\"https://80000hours.org/2022/04/imposter-syndrome/\">imposter syndrome</a> and anxiety about her work</li>\n<li>how to handle the emotional difficulty of reasoning under huge uncertainty</li>\n<li>day-to-day productivity tricks</li>\n<li>knowing when to fix your environment and process, versus changing what you're doing more drastically</li>\n<li>bottlenecks to the research process, especially one that I have struggled with: never feeling like projects are 'done' and thus not sharing my work enough</li>\n<li>the close relationship between intellectual virtues and emotional virtues</li>\n<li>the fine line between (a) being ambitious and holding yourself to high standards and (b) being paralyzed by perfecitonism</li>\n<li>the fine line between (a) prioritizing impact and thinking rationally (not just 'following your passion') and (b) unhealthily coercing yourself into doing things you don't intrinsically care about</li>\n<li>how Luisa and I have helped each other with these and other issues</li>\n</ul>\n<p>I really enjoyed the conversation. I learned a lot from Luisa about how she produced her excellent reports on nuclear war and <a href=\"https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would\">civilizational collapse</a>. I hope and expect that many people will recognize the challenges we have faced in our work.</p>\n<p>And a heartfelt thank you to the amazing 80k podcast production / transcript team: Kieran Harris, Ben Cordell, Milo McGuire, and Katy Moore!</p>\n<h1>Transcript</h1>\n<h2>Luisa\u2019s nuclear war report [00:01:33]</h2>\n<p><strong>Robert Long:</strong> So when you did the <a href=\"https://forum.effectivealtruism.org/posts/MCim4PoqmFPCcPy9m/updated-estimates-of-the-severity-of-a-nuclear-war\">nuclear war report</a>, and you came up with a <a href=\"https://www.getguesstimate.com/models/14362\">Guesstimate model</a> and probabilities, I'm sure as you input those, a lot of it felt extremely uncertain and it felt in some sense wrong to even be putting a number on stuff?</p>\n<p><strong>Luisa Rodriguez:</strong> It felt horrible. Yep.</p>\n<p><strong>Robert Long:</strong> How did you get over that feeling? And also, in retrospect, do you endorse having done that? I'm guessing you do.</p>\n<p><strong>Luisa Rodriguez:</strong> Yeah. One thing that was nice about the nuclear project is that surprisingly, many of the model inputs were not guesses about probabilities that felt really unknowable. Some of the inputs were just like, \u201cHow many nuclear weapons might be used in a nuclear war?\u201d The maximum is the number that exists. The minimum is one.</p>\n<p>And then, I mean, I use probability distributions in part because I got to be like, \u201cI don't really know which side it's going to be.\u201d There are some theoretical reasons to think it's either going to be very few, because that's a particular kind of military strategy, or very many, because that's a different kind. And there are fewer stories to tell about why it's in the middle. So: \u201cI'm going to draw a curve that's big at the small end and big at the upper end.\u201d</p>\n<p><strong>Robert Long:</strong> \u201cBimodal\u201d is what the cool kids call that, I think.</p>\n<p><strong>Luisa Rodriguez:</strong> Thank you, yeah. I mean, exactly. Like I didn't even know it was called bimodal when I was doing it, which made me feel extra terrible about it.</p>\n<p><strong>Robert Long:</strong> But you had a reason to do it.</p>\n<p><strong>Luisa Rodriguez:</strong> Yeah.</p>\n<p><strong>Robert Long:</strong> Oh, but then you shouldn\u2019t have felt terrible.</p>\n<p><strong>Luisa Rodriguez:</strong> Well, I felt very impostery. Like I felt like I should know more maths. I should know more probability. I should know more about how probability distributions worked. But basically, whenever I was like <em>incredibly</em> uncertain, I'd try a uniform distribution -- which is basically where you put equal probability on all of the possible outcomes. And then I was like, \u201cDo I <em>really</em> believe that's true?\u201d And if the answer was no, I'd try to add some probability to the things I think are more likely.</p>\n<p>But plenty of my distributions are either very close to uniform -- so, close to saying I'm just totally uncertain about which outcome it might be -- or they're like, I have one theory about how this works, and it's something like, \u201cProbability goes up over time.\u201d Or like, \u201cIf we're in the kind of world where we use this kind of nuclear targeting, then we're also in the kind of world where we use this other thing, and so these things are correlated.\u201d And so that would change some things about the distributions, but I rarely felt like I was putting numbers on things. And maybe you'd feel much better about a version where you were starting from uniform probability?</p>\n<p><strong>Robert Long:</strong> And seeing if I ever want to make it a little higher somewhere. Yeah.</p>\n<p><strong>Luisa Rodriguez:</strong> A little bit higher somewhere. And even if your probability is still between 0 and 99, then that is something. And you probably will make it even narrower -- and that is better than I could do on consciousness, so that would be information to me.</p>\n<p>So I think I was partly just very lucky that there was like actual concrete information for me to draw on -- or like, not just lucky, but I am much more drawn toward projects with empirical data for this very reason. I think I'd find it way too uncomfortable to be like, \u201cWhat's my guess at the probability that this argument about consciousness is right?\u201d That just sounds impossible to me.</p>\n<p><strong>Robert Long:</strong> Right. There's even more -- way, way, way more -- model uncertainty in consciousness. I mean, I'm guessing there also is in the nuclear war case, right?</p>\n<p><strong>Luisa Rodriguez:</strong> I mean, eventually I'd get to some kinds of inputs that were super uncertain and weird, about like politics and game theory. And those made me incredibly uncomfortable. And I basically just, again, did my best to start from like, \u201cDo I know nothing about this? No, I know something. So I should put more probability on something than just say it's like a totally random guess.\u201d</p>\n<p>And then also, I think I just took a tonne of comfort in knowing that I was going to explain why I put a certain probability where I put it -- and if someone thought that reasoning was bad, I was going to link them to the Guesstimate model and encourage them to put in a different probability. I mean, the fear is that people are going to be like, \u201cYou idiot. You think the probability is X?\u201d And for me, it was really comforting to be like --</p>\n<p><strong>Robert Long:</strong> \u201cMake your own damn model!\u201d</p>\n<p><strong>Luisa Rodriguez:</strong> Yeah. \u201cI've done all the work of setting up this model and explaining my reasoning. And you are super welcome to make counterarguments and put in your own numbers and have some other view that pops out at the end. And by all means, try to convince me, or try to convince other people.\u201d That seems just objectively good.</p>\n<p>And a lot of the time, I didn't have full access to that motivation. A lot of the time I was just like, \u201cThis is terrifying and I hate it.\u201d But you only need one time, which is publishing time. I just had to be like, \u201cI hate this, but it's time to publish. I said I would. And I convinced myself that there are good reasons to be transparent about this. So I'm gonna hit publish and feel tortured about it. But I believe in it.\u201d</p>\n<p><strong>Robert Long:</strong> It's extremely important. Yeah, I really like this idea of reducing the anxiety of putting the probabilities into a certain bin. I think it's great, actually. I think there's probably a very natural human instinct to be like, \u201cI'm uncertain about this. So it\u2019s like uniform, who can say? Uniform distribution. Everything's equally likely. It's absurd to say anything else.\u201d I like the idea of starting from that and being like, \u201cNo, come on. There's like a little bump here.\u201d</p>\n<p><strong>Luisa Rodriguez:</strong> Like, \u201cNo, no, come on.\u201d Exactly.</p>\n<p><strong>Robert Long:</strong> Yeah, there's at least some bump. But was there a different kind of anxiety that came from setting up the model? I assume it'd be a little bit more threatening if someone was like, \u201cThis is not how the risk of nuclear winter relates to the risk of collapse at all. It's the wrong nodes. What you've said is independent is not even close to independent. It's completely misguided in a way that just makes the whole thing confused.\u201d I'm guessing you had like a manager checking that -- and also, you\u2019re talented enough to do it yourself -- but for the anxiety, that's what I would really want double-checked.</p>\n<p><strong>Luisa Rodriguez:</strong> Yeah. I think part of the answer is, again, I was thinking about really concrete empirical questions in the nodes in the model. I mean, I really started like super, super simply. I think partly because I just felt really dumb about the issue of nuclear war and nuclear winter. Like I didn't even pay enough attention to current events to have cached intuitions about how likely certain nuclear wars were. Or I didn't know much about the Cold War.</p>\n<p>So I felt so dumb that I felt like I could learn something by being like, \u201cHow many nuclear weapons are there? And what is the population of the US and Russia? If we allocate all of those nuclear weapons to a different city, how many people could die?\u201d That was informative to me. It started there and I was like, \u201cIf I'm going to learn something, plausibly other people will learn something too.\u201d</p>\n<p>And maybe you worry that it's actually really misleading. And if you have that gut intuition about it, then listen to that and try to figure out why -- maybe it leads to some good ways to make the model more complex and nuanced and interesting and say more true things. But I think I really just did start from like, \u201cI have no real intuitions about this. What would give me <em>anything</em> to latch on to, to be a bit clearer on how bad nuclear war would be?\u201d</p>\n<p>And then I did add things like the assumption that cities would be targeted in order of how big they were. And when you do that, that makes the number even bigger, so that made my model a bit more nuanced. And then I did get to use a lot of research papers that like, looked at how much smoke is lofted into the atmosphere when a nuclear bomb detonates in a city. I just took those inputs, and I think maybe I widened them a bit for uncertainty, but I'm not even sure I did that. And that was also just like, \u201cCool. I had absolutely no idea how much smoke we'd get if all of the current day's nuclear weapons were going to be detonated in cities all at once. Now I have a number. And it is roughly less than this paper says is required for nuclear winter. So that's interesting and something I didn't know before.\u201d</p>\n<p><strong>Robert Long:</strong> Here's a possible lesson from this. I'll see if you agree. <a href=\"https://www.openphilanthropy.org/research/reasoning-transparency/\">Reasoning transparency</a> is great. It helps the reader know what they can take from your report. It also seems maybe really helpful for the writer because you're making your brain remember, \u201cMy job here is not to say the final certain word on this topic forever.\u201d</p>\n<p>That's never what you were doing. But speaking for myself, I sit down and I'm like, \u201cThe goal of this is to look into these questions, find some evidence, say how you would think about it.\u201d Some part of my brain just immediately forgets that, and starts saying, \u201c<em>You must solve it</em>.\u201d It's funny, maybe we just need like posters above our computers that reminds us of like, \u201cWhat are you doing when you sit down to do this? Are you proving that you're the greatest genius of all time? And in three months, you're going to solve every perplexing question about consciousness and AI?\u201d Um, no.</p>\n<h2>Imposter syndrome [00:12:08]</h2>\n<p><strong>Luisa Rodriguez:</strong> It does seem like a good lesson, partly because I think that we both probably have... well, I don't know if you'd consider yourself someone who has <a href=\"https://80000hours.org/2022/04/imposter-syndrome/\">imposter syndrome</a>. I think you have some issues with self-belief around your work as a philosopher. Like, are you good enough to be doing really, really important work in philosophy? And your bar is really high for that. And so it's not that you've got low self-confidence -- I think I'm closer to the low self-confidence side of things -- you've mostly just got really high standards.</p>\n<p>So I'm coming at research questions with the background idea that I'm dumb and that I know nothing, but probably I could learn a little something about it. And you're coming with a background belief of like, \u201cI'm pretty smart\u201d -- which you are -- \u201cand it'd be really cool if I could solve this philosophical problem of consciousness and AI systems.\u201d And you think that's possible -- which is both maybe true, but also, I mean, it's such a crazy ambition to me.</p>\n<p><strong>Robert Long:</strong> Yeah, I think there must be some art to taking the good parts of that possibly preposterous belief. I think the <a href=\"https://forum.effectivealtruism.org/topics/rationality-community\">rationality community</a> is sometimes good at this. I think one of their principles is like, \u201cDon't pretend at the outset that you couldn't possibly know anything or solve the problem. Maybe you just can. And don't be embarrassed about that. Don't take the social consensus that this is impossible too seriously. Maybe it just hasn't properly been tried.\u201d</p>\n<p><strong>Luisa Rodriguez:</strong> Because a bunch of other people were like \u201cI probably can't solve it\u201d as well.</p>\n<p><strong>Robert Long:</strong> Yeah. You want that belief without something that says you can't publish this until you know literally everything and have perfectly solved everything.</p>\n<p><strong>Luisa Rodriguez:</strong> Totally. Or like you've only succeeded if it turns out that you were the one person in the world who could solve everything.</p>\n<p><strong>Robert Long:</strong> Exactly.</p>\n<p><strong>Luisa Rodriguez:</strong> Yeah, I agree that those seem like two beliefs that often don't come together, but that would probably make for a great researcher.</p>\n<p><strong>Robert Long:</strong> I think you see a lot of that in some of the olden days. Like here's a kind of preposterous belief: I can just figure out a lot better than people have what charities are more effective. That's an insanely complicated question, right? And you're just going to waltz in as an outsider and do a better job. But then at the same time, there's clearly something going on with those people, where they're like, \u201cWell, we're allowed to release some provisional conclusions and say what would change our minds. And we definitely haven't settled that question -- that question will probably never be 100% settled -- but here's something to go on.\u201d</p>\n<p><strong>Luisa Rodriguez:</strong> Totally. Do you think you could access that mindset a bit more, I don't know, either mid-project or preparing to publish something? Like, \u201cI'm really grateful that Elie Hassenfeld and Holden Karnofsky were brave and ambitious enough to try to <a href=\"https://www.givewell.org/about\">think about how to do charity much better</a>, and publish preliminary results, which they've since updated many times. I could totally do that with consciousness.\u201d</p>\n<p><strong>Robert Long:</strong> I think the gratitude angle might help. I think more just remembering the best that I'm doing will be enough. Because I think something I find deeply intrinsically motivating and pleasurable is the process of talking to other people about what a weird and confusing world we live in, and reporting what we've found out about it and what our current best guesses are.</p>\n<p>I think <a href=\"https://www.amazon.com/Sense-Style-Thinking-Persons-Writing/dp/0143127799\">Steven Pinker, in his book on writing</a>, has some remark to the effect that the best writing is just pointing at something in the world that is interesting and describing it. And that's kind of the opposite of being in your own head about, \u201cAm I good enough? What does this say about me?\u201d So I think that I (and the listener, if this applies to you) can remember that the world is out there, and it's confusing. You're trying to get less confused, and other people would also like to get less confused. You're helping them do that by reporting your own journey. And then I still think you do need a dash of, \u201cBut also, success is possible.\u201d</p>\n<p><strong>Luisa Rodriguez:</strong> \u201cMaybe we could do it.\u201d</p>\n<p><strong>Robert Long:</strong> Yeah, yeah, exactly.</p>\n<p><strong>Luisa Rodriguez:</strong> Yeah, I mean, this does just all feel very related to the fact that we've had this shared experience, despite not talking that much about the content of the research we've done in the past. But we have talked a bunch about the experience of it, and I think we've both found it really, really hard. And there are plenty of things that are very different about our jobs, but the things in common being very independent research and research on very hard and interdisciplinary questions. Is there more you want to say on what that's been like for you?</p>\n<p><strong>Robert Long:</strong> Yeah, I think you're also able to do this. It's not finding it really hard that's unpleasant. Of course it should be hard.</p>\n<p><strong>Luisa Rodriguez:</strong> Right. They are hard questions.</p>\n<p><strong>Robert Long:</strong> And I think both of us really like hard work. I guess it's remembering not to berate yourself for it being hard. I have had a few humorous times this year where I'm like, \u201cOh yeah, I'm working on consciousness and AI. Of course it's hard.\u201d</p>\n<p><strong>Luisa Rodriguez:</strong> Totally.</p>\n<p><strong>Robert Long:</strong> I mean, as you know from being my friend, I have tonnes of stuff to say on this subject. Here's an interesting balance. I think there's a lot of emotional stuff that you can get sorted out that is very helpful for research. And I think sometimes people maybe don't realise the extent to which the intellectual virtues are in some sense like emotional and character virtues of equanimity, and not being blinded by pride or fear.</p>\n<p><strong>Luisa Rodriguez:</strong> Totally. Status. Yeah.</p>\n<p><strong>Robert Long:</strong> But then on the flip side, a lot of that stuff is definitely not sufficient, and not all of it is necessary if you just fix the incentive structure around you, or the environment around you.</p>\n<p><strong>Luisa Rodriguez:</strong> Yeah. I do think of you as someone who is especially wise and deliberate about how you set up your environment and the incentives you want to create for yourself to make sure you do good research and share your research. Do you mind saying some stuff about the kinds of stuff you found helpful? And the specific kinds of struggles that those strategies were trying to help you overcome?</p>\n<p><strong>Robert Long:</strong> Yeah, definitely. I think for anyone doing research, the pit you really want to avoid is your work not making contact with the world -- or, more importantly, with other people at regular intervals.</p>\n<p><strong>Luisa Rodriguez:</strong> Oh yeah. Gosh. I mean, the number of times I've been like, \u201cThis isn't ready to share; I need to make it better first.\u201d And then you spend a year of your life writing something that would have looked totally different -- and much better -- if you'd asked anyone to give you thoughts on the direction it was going.</p>\n<p><strong>Robert Long:</strong> I think for a certain class of researcher with probably pretty common human experiences, maybe the most important thing you could have is a friend or manager who says, \u201cYou are sharing that now. Send that to person X. Just send it, just ship it.\u201d</p>\n<p><strong>Luisa Rodriguez:</strong> Yeah. \u201cIt doesn't matter that there are unfinished sentences. That is fine. Just get feedback on what you have.\u201d</p>\n<p><strong>Robert Long:</strong> And having a collaborator is a great way to get out of this pit, and just like never be in it to begin with. I've recently been writing a paper where me and my coauthors have only written in Zoom meetings together.</p>\n<p><strong>Luisa Rodriguez:</strong> Wow. Wild.</p>\n<p><strong>Robert Long:</strong> It's extremely fun. I certainly won't claim that it's sustainable or optimal for all projects, but what it eliminates is really any chance of getting in your own head.</p>\n<p><strong>Luisa Rodriguez:</strong> Right.</p>\n<p><strong>Robert Long:</strong> Or procrastinating. And it's just so interactive and...</p>\n<p><strong>Luisa Rodriguez:</strong> It sounds really social. It sounds like maybe you've got some nerves about typing your thoughts in front of your colleagues in real time. But once you start doing it, you get all of the gains of your smart colleagues helping you make your ideas better. And if they're doing it with you, they probably have some respect for your ideas in the first place. And probably you'll have some good ideas, and some ideas that could be better. And you'll just find all of that out really quickly.</p>\n<p><strong>Robert Long:</strong> Yeah. So that's one thing: having the right environment, where things are shared and social. Of course, I think there are times when everyone needs to go off and just sit in a room and just rack their brain over stuff. I think for most important problems, you get a lot of that time. So I'm not saying tweet every thought you have and make sure you get feedback on it and stuff.</p>\n<p><strong>Luisa Rodriguez:</strong> Sure. But that said, just to put even more emphasis on this idea, I don't think I ever needed time alone in a room when I was really doing independent research. I think I basically always benefitted from having sometimes daily management check-ins, where I was like, \u201cHere's what I did today. Here's the part I'm stuck on. And I want you to be my <a href=\"https://en.wikipedia.org/wiki/Rubber_duck_debugging\">rubber duckie</a>.\u201d Sometimes I did it with unwilling housemates. I've just always found that talking to people about the research thing I'm thinking about, in that moment, on the daily, is better than waiting. As long as people are up for it.</p>\n<p><strong>Robert Long:</strong> One thing I'll be very interested to see, and excited if it works, is to what extent large language models and chatbots can serve this role.</p>\n<p><strong>Luisa Rodriguez:</strong> Totally, yeah.</p>\n<p><strong>Robert Long:</strong> Yeah. I suspect there will be things that you don't get from it because I think it is important to know that a real human sometimes is saying, \u201cYeah, good job\u201d and \u201cThese ideas are interesting.\u201d But for like rubber ducking and brainstorming, I think maybe just the impression of actually being in conversation with someone could do a lot.</p>\n<p><strong>Luisa Rodriguez:</strong> Right. Have you ever used anything like that?</p>\n<p><strong>Robert Long:</strong> Yeah, I've been using ChatGPT. I have it open a lot now while I'm researching, just to be like, \u201cYeah, I'm thinking about...\u201d I sometimes just sort of put in my day plan, or like what could be my day plan. And ChatGPT has this very kind of HR, feel-good persona because of its training. So it usually says something innocuous, like, \u201cYeah, just remember to take breaks,\u201d and \u201cIt's important to drink enough water,\u201d and \u201cYou can do it with a little faith in yourself.\u201d But you know, it's good to know. It's good to be reminded of.</p>\n<p><strong>Luisa Rodriguez:</strong> That's good advice.</p>\n<p><strong>Robert Long:</strong> Yeah. But I think you probably already could be getting a lot more out of current language models in terms of actual feedback. And I'm sure future ones will provide a lot of that.</p>\n<p><strong>Luisa Rodriguez:</strong> It's funny, I felt surprised to hear you say that, but I did ask GPT-3 for questions to ask you in this interview.</p>\n<p><strong>Robert Long:</strong> Right. And I bet it did a decent job too. That would be my guess.</p>\n<p><strong>Luisa Rodriguez:</strong> It did. It totally did.</p>\n<h2>Strategies for making independent research more fun and productive [00:23:29]</h2>\n<p><strong>Luisa Rodriguez:</strong> Are there any other strategies that have helped you make independent research more fun or more productive?</p>\n<p><strong>Robert Long:</strong> I guess we've been being very philosophical and emotional, but a nuts-and-bolts thing I can recommend is the app <a href=\"https://getcoldturkey.com/\">Cold Turkey</a> blocker. There are other internet blockers as well, but basically I have a block on Cold Turkey that will block everything except Google Docs -- and that'll do it. I mean, if you put your phone away and you put that on, you're going to have to get some words into your Google Docs. I highly recommend that. That might be all for nuts-and-bolts type stuff for now.</p>\n<p>Another philosophical one is, so sometimes you're feeling bad at work because you haven't been sharing your work enough. And if you had a friend telling you to share the work, then things would just instantly get a lot better. Or maybe you're not sleeping enough, or you're iron deficient, and if you sorted that out, everything would be fine.</p>\n<p>And there are some times when you're just consistently feeling bad doing the work, because you fundamentally don't care about it. And that's a sign that you shouldn't be doing it. As a disclaimer, that has not been the case with this consciousness in AI work.</p>\n<p><strong>Luisa Rodriguez:</strong> Nice. But has it been the case with you before?</p>\n<p><strong>Robert Long:</strong> Oh yeah. Yeah, for sure. I think sometimes I've overdone the self-help and the environment and the \u201clet's try a different note-taking app\u201d and \u201clet's try a different <a href=\"https://en.wikipedia.org/wiki/Pomodoro_Technique\">Pomodoro</a> length. I\u2019ve just overdone that instead of being like, \u201cOh, the reason I can't focus on this is I don't care about it.\u201d</p>\n<p><strong>Luisa Rodriguez:</strong> Yeah, yeah, yeah. I feel like I've been coming to grips with what has felt like a very sad realisation to me. Which is I feel like, when I got more involved with EA -- and learned some things about rationality, learned some things about mental health, learned some things about strategies for doing good -- I just developed this sense that a lot about my experience was changeable. Like, you can donate a bunch of your salary and not really miss it -- you can still be roughly as happy. Or you can choose a research topic that is really valuable, even if it doesn't totally intrinsically interest you, or hasn't historically.</p>\n<p>And that's because you can get really interested in it by some mechanism -- of like, you think it's important, so you'll be motivated to work on it. Or even, you can use some tools and rationality to make it so that your <a href=\"https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow\">System 1</a>, so to speak -- your kind of intuitive or emotional side of your brain -- comes to grips with some things your System 2 thinks, just by like, I don't know, talking yourself into it in some structured way or something.</p>\n<p>For me, a very specific example -- that is not related to work or careers or research -- is I think I felt like I could feel less imposter syndrome. There are a bunch of feelings that I thought I could feel less of, because of stuff like not just self-help, but rationality tools, and arguments about impact, and arguments from philosophers about careers and stuff.</p>\n<p>And I feel like I'm coming to terms with the fact that less about me is changeable than I thought. And probably there are a bunch of research topics that are really important -- like really, genuinely are instrumentally important to doing a bunch of good -- that I probably couldn't force myself to do, even if I had a great manager and a bunch of friends giving me support. And yeah, I think it's been years that I thought, \u201cI just need the right structures and then I can do the most impactful thing.\u201d And I think I'm slowly realising that probably I can't just overhaul all of my intrinsic interests and motivations. I probably have to work with them some.</p>\n<p><strong>Robert Long:</strong> That sounds like a very important lesson for you, and I'm very happy to hear that you're coming to it -- as your friend, but also for any listeners who feel like they're constantly fighting against some sort of intuitive sense or intuitive excitement. It's tricky because there is some amount that we do endorse -- like not doing what's immediately the most fun and exciting -- but there definitely is a point at which you're just bullying yourself, and you're going to limit your long-term impact and happiness.</p>\n<p>I guess one heuristic is it should feel like a negotiated decision between the parts of you that do and don't want to do something.</p>\n<p><strong>Luisa Rodriguez:</strong> Interesting, yeah. Say more?</p>\n<p><strong>Robert Long:</strong> There's maybe some integrated consensus between the unexcited part and the excited part, where the unexcited part really has been brought on board, and its concerns have been heard that there are some things that are boring about this, but like...</p>\n<p><strong>Luisa Rodriguez:</strong> And not just like, \u201cShut up, unexcited part, you get no seat at this table. The part of us that only cares about impact is the only person.\u201d</p>\n<p><strong>Robert Long:</strong> It's taking the wheel. Exactly.</p>\n<p><strong>Luisa Rodriguez:</strong> I mean, that's just very much how I was making career decisions for a very long time, or topics for research, or yeah, just a bunch of choices. And I think I'm still working on getting all those parts integrated, and the part of me that has intrinsic motivations and interests that don't necessarily line up with impact -- I'm still working on letting that part of me have a seat at the table. It still feels very uncomfortable, and I still feel a strong desire to banish it.</p>\n<p><strong>Robert Long:</strong> Yeah, I think a certain failure mode of otherwise helpful techniques -- like cognitive behavioural therapy, and rationality tools of really considering the impact and stuff -- is using that to bully the resistive parts, and not to like have a conversation with them about the bigger picture.</p>\n<p><strong>Luisa Rodriguez:</strong> Totally. Yeah. Oh man, that just sounds very familiar. Do you feel like you've been able to do that?</p>\n<p><strong>Robert Long:</strong> Yeah, I've been working on it. I think it helped that I did kind of realise I was doing what I'm describing. Like I was using CBT in helpful ways, but I think I had this internal narrative of the rational part that really needs to show up and tell everyone who's boss. And going back to the brain, that's just not that much of your brain. And a lot of people should do CBT and recognise distorted thinking, but there just is a limit to how much I think you should be really coercing yourself to do stuff. I think, as with all things, there is balance here. Like I've seen certain corners of Twitter and certain intellectual communities that seem to have this very strong belief that in some sense you should never be doing anything that looks like self-coercion.</p>\n<p><strong>Luisa Rodriguez:</strong> Wow.</p>\n<p><strong>Robert Long:</strong> I think that's just probably gotta be wrong. I would be very surprised if that's true. But there's some truth, and I think being very wary of stuff that makes you feel like you're killing or locking away a core part of yourself...</p>\n<p><strong>Luisa Rodriguez:</strong> Yeah. I feel like part of my coming to terms with this is just realising that I think I had the belief that I could lock it away. And to make it more concrete, what is an example of a thing I've locked away? I think there was just a long time where I wasn't considering what kinds of work I enjoyed in deciding what careers to consider.</p>\n<p><strong>Robert Long:</strong> Which I'll add is, I do think counter to official 80,000 Hours advice. Like it should be a factor. I know they do say, correctly, <a href=\"https://80000hours.org/articles/dont-follow-your-passion/\">don't just follow whatever pops into your head as your passion</a>. But anyway, just pointing that out.</p>\n<p><strong>Luisa Rodriguez:</strong> Totally. No, I agree. I totally oversimplified it. And I think it was because I was unrealistic. You know, they said, don't follow your passion, <a href=\"https://80000hours.org/articles/personal-fit/\">consider your personal fit</a> as one consideration. And I was like, \u201cWell, it seems better if I <em>don't</em> consider personal fit, and I just do the highest-impact thing, regardless of whether I enjoy it or not. And that's a sacrifice I'm willing to make.\u201d</p>\n<p>And the mistake here was thinking that I <em>could</em> make that sacrifice. It turns out, I think I just literally cannot. And I thought maybe I'd do it and suffer, and that would just be like a choice I could make. But instead, the part that was like, \u201cNo, we want to do work that's interesting sometimes\u201d -- or it's not even interesting; I think I've always been very lucky to do interesting work, but to do work that scratches some other creative itch, or plays more to like things I naturally enjoy and find really stimulating and motivating -- that part apparently just demands to be heard eventually.</p>\n<p>And so I think for me, it played a role in bouncing between jobs a lot. And probably to some extent that's meant I had less impact. Or, it's a confusing question, but it totally seems possible that that kind of thing could happen. And I think it really boils down to the fact that I just was wrong about whether realistically I could make as many sacrifices as I thought I wanted to.</p>\n<p><strong>Robert Long:</strong> I think one thing that can really complicate this process too is -- it sounds like you were at least aware of the existence of this inclination, or maybe you had really locked it away -- but I just want to point out sometimes people have motivations that it would be painful for them to even acknowledge that they have. And then they're really kind of steering a ship with a broken rudder, because that makes it impossible to even come to a reasoned consensus between these different motivations.</p>\n<p><strong>Luisa Rodriguez:</strong> Yeah. Oh, interesting. Well, I do feel very inspired, because I do think of you as someone who's thought about this very wisely in your own career. I have basically seen you be like, \u201cI could work on this philosophical problem. I don't want to, and I'm not going to force myself to. I'm going to find things at the intersection of what I really want to work on and what is valuable for the world.\u201d</p>\n<p><strong>Robert Long:</strong> I hope that approach is working somewhat. Sorry, that's being too modest. Clearly some things are going right. And some things are not going right, because that is also life -- and you and I both need to remind ourselves that the gold standard is not 100% happy, frictionless impact every waking hour.</p>\n<p><strong>Luisa Rodriguez:</strong> Yes, that is another lesson I do need regularly. I think that's the lesson I need above my desk. Yours can say \u201cYou don't need to solve philosophy now\u201d and mine will say \u201cYou cannot and will not be happy and maximising your impact all the time.\u201d</p>\n<h2>Mistakes researchers often make [00:34:58]</h2>\n<p><strong>Luisa Rodriguez:</strong> A question I wanted to ask that's kind of related, because these are, I guess, pitfalls that we're falling into. Do you think there are mistakes you've made in your career that listeners might learn from?</p>\n<p><strong>Robert Long:</strong> Yeah, absolutely. I'd be curious to hear your thoughts on this, because I think you've been part of this process this year. I think one of the weakest points in my work process is like month-to-month or quarter-to-quarter focus. I think I've made a lot of improvements in my own life to have very good day-to-day focus -- I know to put on that blocker, I know to put away my phone -- and so I can show up day in, day out and do some deep work, and <a href=\"https://80000hours.org/podcast/episodes/cal-newport-industrial-revolution-for-office-work/\">Cal Newport</a> and all that.</p>\n<p>I think this is related to the question of management and accountability. One of my biggest weaknesses or bottlenecks has been something like long-term or medium-term plans and prioritising, \u201cOK, let's just take the next two months to finish this and then we'll move on to the next thing.\u201d So, again, life happens and I think everyone probably always feels like they're not optimally segmenting and focusing stuff, and you know, you get five projects going at once and now they're all blurring together. But yeah, I can definitely say I've made mistakes in not finishing stuff in the right sequence or at the right time.</p>\n<p><strong>Luisa Rodriguez:</strong> What do you think has made that hard for you?</p>\n<p><strong>Robert Long:</strong> I think one is sort of a natural disposition towards being interested in a lot of stuff, which I think is a great strength of mine in some ways, but it makes this process harder. I think some of it was lack of self-knowledge and not realising that that's where the trouble was coming in and focusing more on the daily stuff. I think a lot of it is the environment and the nature of what I'm doing. You know, this probably wouldn't be an issue in a job that has like quarterly deliverables, and you just have to have to hit them.</p>\n<p>So as with other stuff, I think this can be fixed with a mix of better self-knowledge and also the right environment and the right management. And when I say the right management, I have you in particular in mind, because I brought you on board as my \u201cWhat are you finishing this month?\u201d accountability buddy. I'd be curious to hear your thoughts on that question.</p>\n<p><strong>Luisa Rodriguez:</strong> I mean, we've both actually done this for each other at different points in our lives. You did this for me in 2019, when I was working on <a href=\"https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would\">civilisational collapse research</a> and was feeling so impostery that I couldn't type anything on a page. For months we had daily calls, and I set accountability goals with money tied to them -- and I think I paid you hundreds of dollars. So it's very nice to get to repay the favour this year.</p>\n<p>And my sense is that, from my perspective, a lot of what was going on was you having an extremely high standard for what it meant for a project to be done. So there were other projects that seemed exciting to you, and it would seem good to me if you wrapped up the one you were doing and moved on to the next one. Like prioritisation seemed hard for you -- partly because it didn't seem like at least any of the solo projects you were working on were ever meeting your standards, were ever good enough to be done.</p>\n<p>So I feel like a big part of the value I added was just being like, \u201cLet's set a delivery date. You've worked on this for several months now. From the outside, it seems like if you've worked on something for six to 12 months, it's sensible to share it with the public, or to share it with whoever commissioned you to write this report. So we're calling that date the end date. And we're just doing what we can between now and then to wrap it up.\u201d Whereas I think your kind of default perspective was something like, \u201cIt's not done until I've solved it.\u201d And that's a really ridiculous goal that you could have probably tried to chip away at for like years and years and years.</p>\n<p><strong>Robert Long:</strong> Yeah. And I think it's important that I knew, in some sense, that was not the goal. And I could have told you that and written that down. But I think by recognising that my brain was going to keep pulling me towards that, that's when you need outside help -- to have a sane friend just be like, \u201cYou're doing it again.\u201d</p>\n<p><strong>Luisa Rodriguez:</strong> Right, exactly. We had the same conversation so many times. It wasn't like I was telling you things you didn't know, or even like each time we checked in was troubleshooting a different problem. Very often, you were like, \u201cI need to share this thing in two weeks.\u201d Then one week went by and you were like, \u201cI'm not going to do that. I'm going to share it in another two weeks.\u201d And I was like, \u201cNope, you said you were going to do it in two weeks, which is next week. So you have to do that.\u201d And we talked about the reasons we endorsed, and they made sense to you, and then you did it.</p>\n<p><strong>Robert Long:</strong> Yeah, I guess the listeners can gauge the advice. I do think some of this is an especially kind of raw problem from the way I am. Do you think this is probably something most people will face if they're not, you know, working for a line manager with deliverables that show up every quarter? Yeah, who will this apply to?</p>\n<p><strong>Luisa Rodriguez:</strong> My guess is like... Honestly, it feels like fewer than like 10% of the people I know would just decide to write something for the EA Forum, and then get all the way to completion in as short a time as they would endorse. Or like, I don't know, sharing it for feedback as soon as they would endorse. I think almost no one does that.</p>\n<p>I certainly didn't. The only reason I ever published anything on the EA Forum is because I was paid to and we set a hard deadline of a certain day in a certain month. And when it got to that time, I felt like it was unacceptable that we were publishing it, and it just felt horrible. Like I felt like it was the wrong thing to do <em>so strongly</em>. And I think I could have worked on that for like years and years more, before I actually wanted to share it or publish it or whatever. I think I just basically know no one who doesn't have some of that. I guess some people do struggle with it more than others. And that's probably like impostery people, perfectionisty people, anxious people. Does that sound kind of right?</p>\n<p><strong>Robert Long:</strong> Yeah, for sure. Another point is -- and this goes to the point of not beating yourself up for struggling -- one of the best questions someone asked me in the last calendar year was when I was talking about the pitfalls I felt like I was in, and she just asked me, \u201cWhat do researchers need? Like in general, what do they need?\u201d And I was like, \u201cOh, well, you know, they need deadlines. They need people to check in on them. They need regular feedback. They need breaks sometimes.\u201d</p>\n<p><strong>Luisa Rodriguez:</strong> And it was totally obvious to you that researchers in general need that. However, you...</p>\n<p><strong>Robert Long:</strong> I think I was thinking about this 10% of people that you mentioned, who somehow weirdly won some lottery where they don't need that. And those people do exist, and hats off to them. But if you're not, you need to try to get those things.</p>\n<p><strong>Luisa Rodriguez:</strong> And you probably shouldn't hate yourself for it.</p>\n<p><strong>Robert Long:</strong> Probably, yeah.</p>\n<h2>Keiran\u2019s outro [00:42:51]</h2>\n<p><strong>Keiran Harris</strong>: If you enjoyed that one, you\u2019ll be delighted to know we\u2019ve got plenty more Luisa content coming up over the next few months.</p>\n<p>If you want to hear more from Rob, you can follow him on Twitter at <a href=\"https://twitter.com/rgblong?lang=en\">@rgblong</a> and subscribe to his Substack, <a href=\"https://experiencemachines.substack.com/\"><em>Experience Machines</em></a>.</p>\n<p>And a reminder that you can listen to Luisa and Rob\u2019s excellent episode on artificial sentience over on the original 80,000 Hours Podcast feed \u2014 that\u2019s number 146.</p>\n<p>All right, audio mastering and technical editing for this episode by Ben Cordell and Milo McGuire.</p>\n<p>Full transcripts and an extensive collection of links to learn more are available on our site and put together by Katy Moore.</p>\n<p>And I produce the show.</p>\n<p>Thanks for listening.</p>\n<h1>Articles, books, and other media discussed in the show</h1>\n<p><strong>Luisa\u2019s reports:</strong></p>\n<ul>\n<li><a href=\"https://forum.effectivealtruism.org/posts/FfxrwBdBDCg9YTh69/how-many-people-would-be-killed-as-a-direct-result-of-a-us\">How many people would be killed as a direct result of a US-Russia nuclear exchange?</a> and <a href=\"https://forum.effectivealtruism.org/posts/MCim4PoqmFPCcPy9m/updated-estimates-of-the-severity-of-a-nuclear-war\">Updated estimates of the severity of a nuclear war</a> -- including a <a href=\"https://www.getguesstimate.com/models/14362\">Guesstimate model</a> for calculating different probabilities</li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would\">What is the likelihood that civilizational collapse would directly lead to human extinction (within decades)?</a></li>\n</ul>\n<p><strong>Rob\u2019s work:</strong></p>\n<ul>\n<li>Rob\u2019s Substack: <a href=\"https://experiencemachines.substack.com/\"><em>Experience Machines</em></a></li>\n<li>Rob\u2019s Twitter: <a href=\"https://twitter.com/rgblong?lang=en\">@rgblong</a></li>\n</ul>\n<p><strong>Approaches for dealing with pitfalls:</strong></p>\n<ul>\n<li><a href=\"https://80000hours.org/2022/04/imposter-syndrome/\">My experience with imposter syndrome \u2014 and how to (partly) overcome it</a> by Luisa features seven stories of talented people held back by the fear they weren't good enough</li>\n<li><a href=\"https://www.openphilanthropy.org/research/reasoning-transparency/\">Reasoning transparency</a> is a potential way to remind readers and yourself that work is always in progress</li>\n<li><a href=\"https://www.amazon.com/Sense-Style-Thinking-Persons-Writing/dp/0143127799\"><em>The sense of style: The thinking person's guide to writing in the 21st century</em></a> by Steven Pinker has advice about how the best writing just points out something interesting and describes it (rather than always trying to \u201csolve\u201d it)</li>\n<li>The <a href=\"https://getcoldturkey.com/\">Cold Turkey</a> blocker app may help maintain focus to complete projects</li>\n</ul>\n<p><strong>Other 80,000 Hours resources:</strong></p>\n<ul>\n<li><a href=\"https://80000hours.org/podcast/episodes/robert-long-artificial-sentience/\">Robert Long on why large language models like GPT (probably) aren't conscious</a></li>\n<li>How to find a balance between:\n<ul>\n<li><a href=\"https://80000hours.org/articles/dont-follow-your-passion/\">To find work you love, don\u2019t (always) follow your passion</a></li>\n<li><a href=\"https://80000hours.org/articles/personal-fit/\">Personal fit: why being good at your job is even more important than people think</a></li>\n</ul>\n</li>\n<li>Podcast: <a href=\"https://80000hours.org/podcast/episodes/cal-newport-industrial-revolution-for-office-work/\">Cal Newport on an industrial revolution for office work</a></li>\n</ul>\n", "user": {"username": "rgb"}}, {"_id": "5d7P4gFpomfeLCHZw", "title": "Unjournal: Evaluations of \"Artificial Intelligence and Economic Growth\", and new hosting space", "postedAt": "2023-03-17T20:20:52.684Z", "htmlBody": "<h1>New set of evaluations&nbsp;</h1><p>The Unjournal evaluations of <a href=\"http://www.nber.org/chapters/c14015\"><strong>Artificial Intelligence and Economic Growth</strong></a>, by prominent economists Philippe Aghion, Benjamin F. Jones, Charles I. Jones \u2013 &nbsp;are up.&nbsp;<br><br>You can read these on our new<a href=\"https://unjournal.pubpub.org/\"> PubPub community space </a>, along with <a href=\"https://unjournal.pubpub.org/pub/aimetrics/release/6\">my discussion of the process and the insights</a> and the 'evaluation metrics', and the authors' response. Thanks to the authors for their participation (<i>reward early-adopters who stick their necks out!</i>), and thanks to Philip Trammel and Seth Benzell for detailed and insightful evaluation.<br><br>I discussed some of the reasons we 'took on' this paper <a href=\"https://forum.effectivealtruism.org/posts/kftzYdmZf4nj2ExN7/what-pivotal-and-useful-research-would-you-like-to-see#2___Aghion__P___Jones__B_F__and_Jones__C_I___2017__Artificial_Intelligence_and_Economic_Growth__NBER_working_paper_\">in an earlier post</a>. The discussion of AI's impact on the economy, what it might look like (in magnitude and in its composition), how to measure and model it, and what conditions lead to \"growth explosions\", seem especially relevant to <a href=\"https://forum.effectivealtruism.org/posts/eAaeeuEd4j6oJ3Ep5/gpt-4-is-out-thread-and-links\">recent events and discussion.</a><br>&nbsp;</p><h2>\"Self-correcting\" science?</h2><p>I'm particularly happy about one outcome here. &nbsp;</p><p>If you were a graduate student reading the paper, or were a professional delving into the economics literature, and had seen the last step of the equations pasted below (from the originally published paper/chapter), what would you think?&nbsp;</p><figure class=\"image image_resized\" style=\"width:75.53%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5d7P4gFpomfeLCHZw/zdfdnlofsnt4awr0vpog\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5d7P4gFpomfeLCHZw/hp1oi5cr0amgjioijyan 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5d7P4gFpomfeLCHZw/whb6wsbdobzmpilicm6u 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5d7P4gFpomfeLCHZw/cayvgu7lshz9opaoqrpg 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5d7P4gFpomfeLCHZw/ynry2wzgze9ze5vks8dh 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5d7P4gFpomfeLCHZw/l6jmwqybxh9fhdzamhhd 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5d7P4gFpomfeLCHZw/nxiqt5qbuz8awlo0nrz0 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5d7P4gFpomfeLCHZw/w0dciiluewwr49odp4uq 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5d7P4gFpomfeLCHZw/mp6dojcfrtewueoqczou 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5d7P4gFpomfeLCHZw/hqao6wat38o2djrliwd8 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5d7P4gFpomfeLCHZw/eupa1gm6t5kl5xjqail0 1058w\"></figure><p>The final step in fact contains an error; the claimed implication does not follow.</p><p>From my discussion:</p><blockquote><p>... &nbsp;we rarely see referees and colleagues actually reading and checking the math and proofs in their peers\u2019 papers. Here Phil Trammel did so and spotted an error in a proof of one of the central results of the paper (the \u2018singularity\u2019 in Example 3). ... The authors have acknowledged this error ... &nbsp;confirmed the revised proof, and link a marked up version on their page. This is \u2018self-correcting research\u2019, and it\u2019s great!</p><p>Even though the same result was preserved, I believe this provides a valuable service.</p><p>Readers of the paper who saw the incorrect proof (particularly students) might be deeply confused. They might think \u2018Can I trust this papers\u2019 other statements?\u2019 \u2018Am I deeply misunderstanding something here? Am I not suited for this work?\u2019 Personally, this happened to me a lot in graduate school; at least some of the time it may have been because of errors and typos in the paper. I suspect many math-driven paper also contain flaws which are never spotted, <i>and these sometimes may affect the substantive results</i> (unlike in the present case).</p></blockquote><p>By the way, the marked up 'corrected' paper is <a href=\"https://web.stanford.edu/~chadj/AJJ-AIandGrowth.pdf\">here</a>, and the <a href=\"https://philiptrammell.com/static/Cobb_Douglas_singularities.pdf\">corrected proof is here</a>. (Caveat: Philip and the authors have agreed on the revised corrected proof, it might benefit from an independent verification.)<br>&nbsp;</p><h1>New (additional) platform: PubPub</h1><p>We are trying out the PubPub platform. We are still maintaining our Sciety page, and we aim to import the content from one to the other, for greater visibility. Some immediate benefits of PubPub...</p><ol><li>It lets us assign 'digital object identifiers' (DOIs) &nbsp;for each evaluation, response, and summary. It puts these and the works referenced into the 'CrossRef' database.<ol><li>Jointly, this should (hopefully) enable indexing in Google Scholar and other academic search engines,</li><li>And 'bibliometrics' (citation counts etc.0</li></ol></li><li>It seems to enable evaluations of work hosted anywhere that has a DOI (published, preprints, etc.)</li><li>It's versatile and full-featured, enabling input from and output from a range of formats, as well as &nbsp;community input and discussion&nbsp;</li><li>It's funded by a non-profit and seems fairly mission-aligned</li></ol><h1>More coming soon, updates&nbsp;</h1><p>The Unjournal has several more impactful papers evaluated and being evaluated, which we hope to post soon. For a sense of what's coming, see our <a href=\"https://www.nber.org/papers?page=1&amp;perPage=50&amp;sortBy=public_date\">'Direct Evaluation track'</a> focusing on NBER working papers.<br>&nbsp;</p><p><i><strong>Some other updates:</strong></i></p><ul><li>&nbsp;We are pursuing collaborations with replication and robustness initiatives such as the <a href=\"https://i4replication.org/\">\"Institute for Replication\"</a> and <a href=\"https://replicats.research.unimelb.edu.au/\">repliCATS</a></li><li>We are now 'fiscally sponsored' by the Open Collective Foundation; see our page <a href=\"https://opencollective.com/the-unjournal\">HERE</a>. (Note, this is an administrative thing, it's not a source of funding)</li></ul><p><br>You can follow our latest updates on our gitbook page <a href=\"https://app.gitbook.com/o/-MfFk4CTSGwVOPkwnRgx/s/-MkORcaM5xGxmrnczq25/readme/latest-updates\">'latest updates' HERE.</a> &nbsp;I'll try to maintain this, and we are looking to build an email list thing soon.&nbsp;</p>", "user": {"username": "david_reinstein"}}, {"_id": "k3azF9ZHmCf6EA3Ce", "title": "EA London Social for Women and Underrepresented Genders", "postedAt": "2023-03-17T17:00:16.939Z", "htmlBody": "<p>Come meet other people based around London looking to do the most good they can with their careers and donations.</p><p>Vibe is chilled and social. We recommend checking out Magnify Mentoring if you are looking for career based support.<a href=\"https://www.magnifymentoring.org/\"> HOME | Magnify Mentoring</a></p><p>This monthly social is hosted at Newspeak House 7.30pm. The venue does not sell food or drinks so please bring your own (there are some takeaways near by). If you have any games, crafts or activities you'd like to bring, please feel free :D&nbsp;</p>", "user": {"username": "Gemma Paterson"}}, {"_id": "fw7wvAsrzhY3TS2MR", "title": "We can guarantee (at least) $1100 to Against Malaria Foundation by winning each round of this Twitter poll contest.", "postedAt": "2023-03-17T23:15:04.306Z", "htmlBody": "<p>I'm not sure whether this is appropriate for this forum but I thought it might be worth a try, because this seems like an easy and costless opportunity to possibly do some good.</p><p>Laura Duffy and others are donating $1100 to Against Malaria Foundation for every round she wins in the Neoliberal Shill bracket Twitter poll. She has raised $3300 already, but is currently behind in the Round of 32.&nbsp;</p><p>You can vote here: <a href=\"https://twitter.com/ne0liberal/status/1636729339747225601\">https://twitter.com/ne0liberal/status/1636729339747225601</a></p><p>About the foundation:</p><blockquote><p>Against Malaria Foundation works to prevent the spread of malaria by distributing long-lasting, insecticide-treated mosquito nets to susceptible populations in developing countries. AMF has been active in 36 countries in Africa, Asia and South America, with a particular focus on sub-Saharan Africa.</p></blockquote><p><a href=\"https://www.thelifeyoucansave.org/best-charities/against-malaria-foundation/\">https://www.thelifeyoucansave.org/best-charities/against-malaria-foundation/</a></p><p>Laura is posting the receipts of her donations after each round and so are other donors. Here is her receipt from the previous round. If you scroll through her Twitter feed you can find proof of donation from other donors.</p><p><a href=\"https://twitter.com/Laura_k_Duffy/status/1636405982724247552\">https://twitter.com/Laura_k_Duffy/status/1636405982724247552</a></p><p>I have asked the organizer of the bracket if soliciting votes from people outside the Twitter community was allowed, and he <a href=\"https://twitter.com/ne0liberal/status/1636755888127000582\">responded in the affirmative</a>.&nbsp;</p><blockquote><p>Only rule really is no vote botting. otherwise, feel free to share the bracket polls to forums or subreddits!</p></blockquote><p>Sorry if this is completely not suitable for this forum!</p>", "user": {"username": "figolambo"}}, {"_id": "esfHrQnu9aSHuXbmy", "title": "[Linkpost] Alpaca 7B release | Budget ChatGPT for everybody?", "postedAt": "2023-03-17T13:08:49.536Z", "htmlBody": "<p>The new YouTube Channel <a href=\"https://www.youtube.com/@ai-explained-/\">AI-Explained</a> gives a summary about the new <a href=\"https://crfm.stanford.edu/alpaca/\">Alpaca 7B</a><i><strong> </strong></i>model. It is an instruction-following language model, you can find the blog post <a href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\">here</a>.<br>&nbsp;</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=xslW5sQOkC8\"><div><iframe src=\"https://www.youtube.com/embed/xslW5sQOkC8\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p><br><br>I will now quote the video description, which has a short summary and lists its sources, and give my own two cents.</p><blockquote><p>8 years of cost reduction in 5 weeks: how Stanford's Alpaca model changes everything, including the economics of OpenAI and GPT 4. The breakthrough, using self-instruct, has big implications for Apple's secret large language model, Baidu's ErnieBot, Amazon's attempts and even governmental efforts, like the newly announced BritGPT.<br>I will go through how Stanford put the model together, why it costs so little, and demonstrate in action versus Chatgpt and GPT 4. And what are the implications of short-circuiting human annotation like this? With analysis of a tweet by Eliezer Yudkowsky, I delve into the workings of the model and the questions it rises.<br>Web Demo: <a href=\"https://alpaca-ai0.ngrok.io/\">https://alpaca-ai0.ngrok.io/</a><br>Alpaca: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbGt6VF9hcFhhNlZPUlZ2SnlpeXJnMk9HQXhnUXxBQ3Jtc0trUDluYTZSRWNNYmllbGxhOEdUQ0ZMZkVIZmFLcDZzNFFSaUYzVDVFdGxGLXhIcnRtYXZFT0ZMUzVIRFZnYjdaRkNHZ1BkTXFGOW0xTkVjTDNrZXhSanFmVndhWC1DckxyM0M3U0RxOUdjYW5WbXpHdw&amp;q=https%3A%2F%2Fcrfm.stanford.edu%2F2023%2F03%2F13%2Falpaca.html&amp;v=xslW5sQOkC8\">https://crfm.stanford.edu/2023/03/13/...</a>&nbsp;<br>Ark Forecast: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbXRlNERIS1lFZFdvNnJDZlFjenUwU1hGTkRoZ3xBQ3Jtc0ttT29hSUV6R1o5anlQVFI1NG5MaktkQ3NoeFJjcHJ0VkE1TWtfVUtER1czUmVncW5YSDlJMU5xek9HNE4tWnlJQ1ptdHFtazRFZHdXdlFPSnpKUGpMVHFKRUR2c2JsMFR2MzY4YVYtY3U1MXVSX1U0bw&amp;q=https%3A%2F%2Fresearch.ark-invest.com%2Fhubfs%2F1_Download_Files_ARK-Invest%2FBig_Ideas%2FARK%2520Invest_013123_Presentation_Big%2520Ideas%25202023_Final.pdf&amp;v=xslW5sQOkC8\">https://research.ark-invest.com/hubfs...</a>&nbsp;<br>Eliezer Tweet: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbGdSemFFMVpkeVVWeG9lWm84eFRXWE5wR3BxUXxBQ3Jtc0tuZVdSbkx0UDF5azBJbUcwTGdhbEZ6cVZrNkhXR0ZhWVFuQjZIcVJjTnBpLU1wbzRrX1ZzUHdZUkNyRTh0V0xYaXVxbW9vUEhYX3gxNFhORXVWLVlaNlRKLVp1WmkyNHZIX3RNb1JNTkxxa2hOTU1hTQ&amp;q=https%3A%2F%2Ftwitter.com%2FESYudkowsky%2Fstatus%2F1635577836525469697&amp;v=xslW5sQOkC8\">https://twitter.com/ESYudkowsky/statu...</a> <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqa0hoSF9HVHlFZjVna2N3Z1pidzA1NmpfRndmQXxBQ3Jtc0tuU3pvRng1MVBfQTFtV05HNlcxY2dIaXNxYlVyb1VWLTEtUVRDQkgzZW5JS2ZrTWxuTFNoMW1MN2d2V2xDajRLZE96Ny1ZWU1Sa1dvSjhtQlRKa0UzMGxobkdaLTZNZHNoQnJtVUE0alRPTEVoMFJCSQ&amp;q=https%3A%2F%2Ftwitter.com%2FESYudkowsky%2Fstatus%2F1635667349792780288&amp;v=xslW5sQOkC8\">https://twitter.com/ESYudkowsky/statu...</a>&nbsp;<br>Self-Instruct: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbFZBM3hIZTIxUlJjcDU3S2JIR1B1eWg0bDJkZ3xBQ3Jtc0tsRXpoNHZiSEtBMVkzeWtiTkdwOWNQb1NpTTQ3Q2FnSXh6Wk5ZZVNVZXVIeC1CUFB1R0ZwTVJaRHNycE5GOUNwdTZTVlo1S0c3bmlwMXdXc2QxWmc4Y3NUM1VzSFhONTlKMGFrSFNjbGo2cjU1d1hYUQ&amp;q=https%3A%2F%2Farxiv.org%2Fpdf%2F2212.10560.pdf&amp;v=xslW5sQOkC8\">https://arxiv.org/pdf/2212.10560.pdf</a>&nbsp;<br>InstructGPT: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbnB5ZkJWNTY3dWxHcG51dktRR19sZWxVUE04QXxBQ3Jtc0ttRTRqWE9UMWpfZ2ZBZW9qbVp0Yzdma0lWR2RpR0FGeEx6NzNzU004X01yNU9KaVNQWW02enVPd3VWMXNLVXdkVzJFM3JxdXZiamdYMFByS1g2WFhOQUhKSS1ibWNLMUtBOHd0ZXR3SHJITlc4YU00dw&amp;q=https%3A%2F%2Fopenai.com%2Fresearch%2Finstruction-following&amp;v=xslW5sQOkC8\">https://openai.com/research/instructi...</a>&nbsp;<br>OpenAI Terms: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbTRYQTJfRFdIM1FBQXdZWnlHZUFmMWNkd0NOUXxBQ3Jtc0ttU3BPSTVhXzctQVhKSG1fRm9qNDQ3MVZXa2xqQzAwMnllUjYyRHkwTVRtNjdPVlJSOWxCbGVfamlDMXdEcldtWWw4LVdYSnBITmhqQUNfVWh3THR6N2ppMTNGT0lLanFMU1NfeENKVkxyR0dYR0pxMA&amp;q=https%3A%2F%2Fopenai.com%2Fpolicies%2Fterms-of-use&amp;v=xslW5sQOkC8\">https://openai.com/policies/terms-of-use</a>&nbsp;<br>MMLU Test: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbGhrd2lyVlZrZUhtc3ZKUlZHRmw1RXJ2Tzd3UXxBQ3Jtc0ttMGREb2xEdHBaMWYyLUI3X0V0b3Atc2F1NTlFaENoS2pkaDFFbGM2WHJDMHNCUXNfb2tkWTZsSWFyWE11WTdFaEdpVUNDQlVtOEV2STc5eFZoSnByVm40Wlc5Wm5kTjh6RmhOUEdOMDZFR2RvY0FSYw&amp;q=https%3A%2F%2Farxiv.org%2Fpdf%2F2009.03300.pdf&amp;v=xslW5sQOkC8\">https://arxiv.org/pdf/2009.03300.pdf</a>&nbsp;<br>Apple LLM: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbjQ5cEt1OXhNWS12VUhyLUpVRkZkeW9kYTRlZ3xBQ3Jtc0trVVhKSHAtTUprcmJNRWZtOFBUbXV2aHBSLWI3NWdyclAxNDdzOGNDc0llTTF2cEFtSnlNc1RBNWVJV3BwQkJLTVZCa3VBMEtKUzZVWEVmT1V0cXNXcG5oamFzLWN1SnJqakxQbEhJM0hHdGJyVU1mZw&amp;q=https%3A%2F%2Fwww.nytimes.com%2F2023%2F03%2F15%2Ftechnology%2Fsiri-alexa-google-assistant-artificial-intelligence.html&amp;v=xslW5sQOkC8\">https://www.nytimes.com/2023/03/15/te...</a>&nbsp;<br>GPT 4 API: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbi16czdjSDF5Sk5QSW9vaXUwanRwWDNkTjZRUXxBQ3Jtc0trQURzWWR3R2FfNjNIZGEwWi1xOGQyNENIZy1YZExPRGdyMmVCRWJqT0t2QkhzX1FTZ2dFM3hxa1hLMlVkcDhsaUJvdDl5dEQxTWZEZlA0QkZ2RVUxbUtTejhzel9udFFZZlVFeFd1cXc4UG51REZ6cw&amp;q=https%3A%2F%2Fopenai.com%2Fpricing&amp;v=xslW5sQOkC8\">https://openai.com/pricing</a>&nbsp;<br>Llama Models: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbnQ2NXNMVTRRbk44SmM3NEhrVW83cjNmOVhKZ3xBQ3Jtc0tuYVctcXdrQkRDdVhOc2lPSzgtTkktdnN0OGpaSzUzV2JBZ0U4VXFJRmpuZDhRd1Y1RjhDdWhiLTlETngwdUhvS0FzMGx4aExzNWJEYVFDMDJVWGd0ZWdfUll4bW5ONlctZ3d0NlVVTWVwWXo0dDhVNA&amp;q=https%3A%2F%2Farxiv.org%2Fpdf%2F2302.13971.pdf&amp;v=xslW5sQOkC8\">https://arxiv.org/pdf/2302.13971.pdf</a>&nbsp;<br>BritGPT: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqa2RuZGxDMlV0YzIteXcwam9MWjJOYzZHa3pTd3xBQ3Jtc0tucGVndDJjT2dPVktGdVVSR2ZXalZzQld4dllsdVlKZjVlZGlZTnA4eU4tSHkzUkRFTDdmdGx4T05SX0RoeUNGa0RGQUxhSlFRUk5EYkh2WDljT25lemkwaVJqR1p3azg5X1NybHNWa3MyNWlxN2RTbw&amp;q=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2023%2Fmar%2F15%2Fuk-to-invest-900m-in-supercomputer-in-bid-to-build-own-britgpt&amp;v=xslW5sQOkC8\">https://www.theguardian.com/technolog...</a>&nbsp;<br>Amazon: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbUpfQ0t0MUE2aGdKbEE0V015aDNId2J1cWwwQXxBQ3Jtc0traUlhNWtmcHkwUDRhT2VxZ1g1S1h1ZXJyaXNzNzBuTHdkbUdDM2g4ZUR4TnRCa0tPYW0yUS1kTDJRVzZ4Y2lCSDYzSXNwZng4X3JmVWNmb2E3VS1aREVEWURaNzN2RGo1TGdpQ0RQMlQzNnNLc19kOA&amp;q=https%3A%2F%2Fwww.businessinsider.com%2Famazons-ceo-andy-jassy-on-chat-cpt-ai-2023-2%3Fr%3DUS%26IR%3DT&amp;v=xslW5sQOkC8\">https://www.businessinsider.com/amazo...</a>&nbsp;<br>AlexaTM: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqa0t0UzFvNENwdmxVMEVsVUtVQjJPRU1lRWdMd3xBQ3Jtc0tsZkNZZDBGN2Iwci1kTkpZY3VXVWZfZmxJY0d3VnlWa0JpX1JwVW1GMUFncC11MmlxeEczbDlmcVZJWFpqVG9qRm1VcTFlUHZ6UkE5ZUpURzRGWklTQlpta0dmOUJvdGZfUGJKNXdvYWVWTHprUEw5NA&amp;q=https%3A%2F%2Farxiv.org%2Fpdf%2F2208.01448.pdf&amp;v=xslW5sQOkC8\">https://arxiv.org/pdf/2208.01448.pdf</a>&nbsp;<br>Baidu Ernie: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbXZlVnpOTHNMQmNXMGFfalhjSWdOZmgyZEJ4Z3xBQ3Jtc0tsMXBVOEZrb2wxcEFlbElZcjhWbWpZbzNhY2RUeS1iZHBRODNBSERXYlBCUXBiYUZNREpsai1FWlNQSHI4UFQyOHlWcFd4Z3RoZHpiX20teFZuWGtjZXM1Y1MtajJ5ZmhwLTRaV0dOODBJelpmajJWbw&amp;q=https%3A%2F%2Fwww.nytimes.com%2F2023%2F03%2F16%2Fworld%2Fasia%2Fchina-baidu-chatgpt-ernie.html&amp;v=xslW5sQOkC8\">https://www.nytimes.com/2023/03/16/wo...</a>&nbsp;<br>PaLM API: <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbk5lWjJJZ2d4OUhqcHQ1azZ0a2JPajNfV1JYd3xBQ3Jtc0tuckt4b1pYc0xiR3RmNUhnSUpQOVdqRlU3YzhHdWlwX1Fsb1BWTnIxUFBTd0tKSzE2ZjVOWUVvU2NBalFYdTFzOUlrZ1FKXzNLcjVFSndjTWlPMVBNU1huemtfdlJhTmYxR0t2MUdJVkphU3hTX3Zvdw&amp;q=https%3A%2F%2Fdevelopers.googleblog.com%2F2023%2F03%2Fannouncing-palm-api-and-makersuite.html&amp;v=xslW5sQOkC8\">https://developers.googleblog.com/202...</a>&nbsp;<br><a href=\"https://www.patreon.com/AIExplained\">patreon.com/AIExplained</a></p></blockquote><p>Using a second AI to multiply the self-instruct training data and the resulting feedback loop at such an early stage could lead to a drastic improvement in cost efficiency. I have to reevaluate my AI timeline again.</p><ul><li>human annotated training data multiplication \u2192 cost reduction&nbsp;</li><li>cost-effective variant, as only fine-tuning of existing pre-trained models is performed</li><li>smaller model is used more efficiently due to better tuning</li><li>available to all and can be specialised for specific applications</li></ul><p>&nbsp;</p><p>What are your thoughts about Alpaca 7B and Stanford's CRFM publication of the new model? Are the presented terms and conditions enough to permit misuse?<br>&nbsp;</p><figure class=\"image image_resized\" style=\"width:50.61%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/esfHrQnu9aSHuXbmy/fkx9dozarfk4pogh7hau\" alt=\"Dall-E depiction of an alpaca sitting infront of an computer in comic style.\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/esfHrQnu9aSHuXbmy/rs1cbsopdnyt2trda1cw 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/esfHrQnu9aSHuXbmy/mzrsm36ixwibtprstypw 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/esfHrQnu9aSHuXbmy/cq14dfga8xrhslanwayv 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/esfHrQnu9aSHuXbmy/nh5cssuszmb56okexx91 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/esfHrQnu9aSHuXbmy/wmbzw8rxiikkluxrmkly 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/esfHrQnu9aSHuXbmy/y6ltgtbphkjesllmya1s 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/esfHrQnu9aSHuXbmy/dn5c0vdokvutoxxglr3j 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/esfHrQnu9aSHuXbmy/cxuzxgols7pcrqjspc1i 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/esfHrQnu9aSHuXbmy/nn7vfyqa8iaytgmhdixm 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/esfHrQnu9aSHuXbmy/dfjvigtuvrrab0s5rmoi 1024w\"><figcaption>An alpaca sitting in front of a computer, depicted by DALL-E</figcaption></figure>", "user": {"username": "Felix Wolf"}}, {"_id": "ynxDXNbmak9fEHaAm", "title": "Symbiosis, not alignment, as the goal for liberal democracies in the transition to artificial general intelligence", "postedAt": "2023-03-17T13:04:22.658Z", "htmlBody": "<p>This is a new original research article by me. The published version is <a href=\"https://doi.org/10.1007/s43681-023-00268-7\">here</a>, in the journal <i>AI and Ethics</i>. I post it here in its entirety because I think it could be interesting to readers of this forum and because it cites several EA forum pieces. In the weeks and months since writing it I have come to think that the development of AGI may have to be slowed radically if the AGI transition is to turn out \"good\" in the sense of argued here.</p><p>&nbsp;</p><p><strong>Abstract:</strong></p><p>A transition to a world with artificial general intelligence (AGI) may occur within the next few decades. This transition may give rise to catastrophic risks from&nbsp;<i>misaligned</i> AGI, which have received a significant amount of attention, deservedly. Here I argue that AGI systems that are&nbsp;<i>intent-aligned</i> \u2013 they always try to do what their operators want them to do \u2013 would also create catastrophic risks, mainly due to the power that they concentrate on their operators. With time, that power would almost certainly be catastrophically exploited, potentially resulting in human extinction or permanent dystopia. I suggest that liberal democracies, if they decide to allow the development of AGI, may react to this threat by letting AGI take shape as an&nbsp;<i>intergenerational social project</i>, resulting in an arrangement where AGI is not intent-aligned but&nbsp;<i>symbiotic</i> with humans. I provide some tentative ideas on what the resulting arrangement may look like and consider what speaks for and what against aiming for intent-aligned AGI as an intermediate step.<br>&nbsp;</p><h3>Introduction</h3><p>The development of artificial intelligence that is superior to human intelligence in almost all conceivable respects and general in scope may take place within the next few decades (Grace et al. 2018, Cotra 2022a). A growing number of companies are working on the explicit goal of developing such&nbsp;<i>artificial general intelligence</i> (AGI, see (Glover 2022) for an overview of companies). If and when they succeed the transition to a world with AGI (\u201cAGI transition\u201d in what follows) occurs, this will plausibly be one of the most momentous changes in history, comparable in significance to the agricultural, scientific, and industrial revolutions, perhaps even surpassing them. How the AGI transition will play out, if it occurs \u2013 notably, its key events, overall duration, and outcomes \u2013 is extremely difficult to foresee because there are no obvious precedents. Some ways in which the AGI transition might occur are catastrophic for humanity, others may well lead to a future with humans flourishing more than at any previous point in history. Here I outline ideas on how the citizens of liberal democracies, if they decide to let the AGI transition happen, might shape that transition to make it, from their perspective,&nbsp;<i>good</i>. I assume that, broadly speaking, a good AGI transition from the perspective of liberal democracies is one that results in an arrangement where AGI systems not only help cover human basic needs and contribute to enhancing human welfare and flourishing, but also respect human and civil rights, and integrate well with democratic structures. My central thesis, advocated here tentatively and with some trepidation, is that a helpful strategic goal for liberal democracies might be to become&nbsp;<i>symbiotic</i> with&nbsp;<i>unaligned</i> AGI developed as an&nbsp;<i>intergenerational social project</i>.</p><p>I clarify what I mean by \u201cunaligned\u201d in section 2, where I also say a few words about what counts as \u201cAGI\u201d in the sense of this paper. Next, in section 3, I situate the present work with respect to the literature on risks from catastrophically misaligned AGI. Having prepared the ground, section 4 embarks on the argument proper of this paper, outlining why and how aligned AGI poses catastrophic risks, mostly related to power concentration. In section 5, I consider ideas for how liberal democracies might mitigate these risks while keeping AGI aligned and I end up finding none of them very promising. As an alternative, I suggest in section 6 that liberal democracies, if they decide to let the development of AGI occur, might strive to develop unaligned symbiotic AGI as an intergenerational project to prevent problematic power concentration. In section 7, I provide some tentative ideas on what the resulting arrangement may look like using institutions such as academia, an energy system, and a constitutional court as analogies. Section 8 considers what speaks for and what against aiming for aligned AGI as an intermediate step. Finally, in Section 9, I provide some reasons why independent forces may work towards unaligned symbiotic AGI and may make it a reality even if relatively few actors actually envisage it as a strategic goal.</p><p>&nbsp;</p><h3>AGI and alignment \u2013 what are we talking about?</h3><p>In this section I give a rough characterization of how I will use the terms \u201cAGI\u201d and \u201calignment.\u201d</p><p>I do not rely on any specific definition of AGI. In fact, the arguments presented here are compatible with a variety of characterizations of \u201cAGI\u201d. Notably, the present discussion is meant to be neutral about whether AGI will be constructed as a single generally intelligent agent or as a \u201ccollective\u201d phenomenon that emerges at the societal level from the interplay of different AI systems that are not individually generally intelligent. What does matter for the present discussion is that it assumes AGI to have an important role in shaping power relations. Accordingly, the arguments presented here should be read with a characterization of AGI in mind according to which it is plausible that AGI \u2013 if it is ever developed \u2013 will have such a role. Characterizations of AGI that include what Karnofsky (2021) dubs \u201cPASTA\u201d (\u201cProcess for Automating Scientific and Technological Advancement\u201d) are good candidates. It seems plausible that differential access to systems that autonomously achieve scientific and technological breakthroughs will dramatically shape economic and political power relations.&nbsp;</p><p>The challenge of transitioning to a good world with AGI is sometimes framed as that of creating&nbsp;<i>aligned</i> AGI or \u201csolving the alignment problem\u201d for AGI. Brian Christian, author of&nbsp;<i>The Alignment Problem</i>, characterizes the alignment problem for AI in general \u2013 not just AGI \u2013 as the challenge of creating AI which \u201ccapture[s] our norms and values, understands what we mean or intend, and above all, do[es] what we want\u201d (Christian, 2020). This characterization gives some orientation about what is usually meant by \u201calignment\u201d, but it is very broad.</p><p>A somewhat more precise definition of alignment, echoing the last part of Christian\u2019s, which seems to capture what is pursued by those who actually work in the field of AI alignment is \u201cintent alignment.\u201d AI systems are intent-aligned if and only if, as alignment researcher Paul Christiano (2018) puts it, they \u201care trying to do what you want them to do,\u201d where \u201cyou\u201d are the operators of the AI systems. In the same vein, alignment researchers Leike et al. (2018) characterise the alignment problem as the challenge: \u201c[H]ow can we create agents that behave in accordance with the user\u2019s intentions?\u201d Intent alignment can be thought of as consisting of two complementary components (Christiano 2018, Ngo 2020): outer alignment \u2013 the AI pursues an objective that really incentivizes the behaviour intended by the operator \u2013 and inner alignment \u2013 the policies that the AI has learned to achieve its objective in a training environment transfer successfully to the deployment environment.</p><p>In what follows, I use \u201calignment\u201d in the sense of \u201cintent alignment\u201d because, first, this use of \u201calignment\u201d fits well with how the term \u201calignment\u201d is otherwise used in ordinary discourse outside of its application to AI and because, second, as said, this corresponds to how \u201calignment\u201d is actually used by those working on AI alignment. Christiano acknowledges that making AGI (intent) aligned is not sufficient for a good AGI transition \u2013 notably, the AGI must also function reliably and be capable of actually understanding human intentions. However, Christiano seems to see achieving alignment as necessary for a good AGI transition in that it \u201cmight be the minimum you'd want out of your AI\u201d (Christiano 2018).</p><p>However, with \u201calignment\u201d understood as \u201cintent alignment\u201d, it is not at all obvious whether achieving AGI alignment is really necessary for achieving a good AGI transition. To recall, a good AGI transition, for the purposes of this paper, is one that results in an arrangement where AGI systems, among other things, respect human and civil rights, and integrate well within democratic structures. It is not at all clear why, in&nbsp;<i>addition</i>, those systems should in all conditions try to do what their operators want them to do, as required for alignment.&nbsp;In fact, as I argue in later sections, the citizens of liberal democracies may well maximize their chances at a, from their perspective, good AGI transition if they aim for AGI that is \u2013 in the right way \u2013&nbsp;<i>unaligned</i>.</p><p>&nbsp;</p><h3>Catastrophic risk from misaligned AGI</h3><p>AGI that is unaligned in the right way contrasts sharply with&nbsp;<i>catastrophically misaligned</i> AGI. Catastrophically misaligned AGI is plausibly one of the largest global catastrophic risks that humanity may face in the next few decades, perhaps the largest. The worry can be traced back to Wiener (1960), and the argument is forcefully made by Yudkowsky (2008), Bostrom (2014), Russell (2019), Ngo (2020), Cotra (2022), Carlsmith (2022), Cohen et al. (2022), Karnofsky (2022), and many others. In a nutshell, the fundamental worry is that there will be incentives to develop goal-directed autonomous AGI agents, that those agents\u2019 ultimate goals will at some point turn out to be in conflict with complex human norms and values, and that those agents, using their superior intelligence, will either take control of human affairs, creating a \u2013 from the human point of view \u2013 dystopian state of affairs with no escape, or simply kill off all humans. (See (Critch and Krueger 2020) for a systematic classification of different ways in which a catastrophe resulting from AGI misalignment could play out.)&nbsp;</p><p>Those who develop AGI will plausibly try to design it such that it follows their intentions. They are thus intrinsically motivated to strive for alignment and, a fortiori, intrinsically motivated to avoid catastrophic misalignment. There are thus strong incentives for those trying to develop AGI to prevent it from being catastrophically misaligned. However, frontrunners in the development of AGI may create catastrophically misaligned AGI by accident, even though this is against their own best interest, because they may believe \u2013 correctly or wrongly \u2013 that they are in a race with (even less scrupulous) competitors and must therefore deprioritize safety for the sake of speed (Carlsmith 2022, Sect. 5.3.2).</p><p>&nbsp;</p><h3>Catastrophic risk from aligned AGI</h3><p>Threats from developments in AI to the rights-based order of liberal democracies are widely discussed (e.g. Coeckelbergh in press), including ones that arise from the intentional (mis-) use of AGI (e.g. Russell 2019, Ch. 4). Kate Crawford goes as far as saying that existing AI systems, across the board, \u201care designed to discriminate, to amplify hierarchies, and to encode narrow classifications\u201d (Crawford 2021, p. 211). Even though this sentiment does not seem to be universally shared, academics unfamiliar with the case for AGI-driven existential risk commonly seem to be more concerned about intentional than unintentional harm from AI (Hobbhahn 2022).&nbsp;</p><p>However, it does not seem to be widely appreciated and made explicit that worries about harm from intentional AGI use should make us particularly concerned about&nbsp;<i>aligned</i> AGI. A completely aligned AGI, by definition, tries to do what its operators want, whatever that is. But because such an AGI is cognitively far more advanced than any human and because such cognitive skills convey great power, it plausibly conveys great power on its operator(s). Agents with a monopoly, or near-monopoly, of aligned, or near-aligned, AGI, may well have power that is far superior to that provided by any technology today, including the most advanced contemporary surveillance technology or, for that matter, nuclear weapons.</p><p>There are at least three types of catastrophic scenarios that could result from the misuse of aligned AGI (Alignment arguably does not have to be perfect at any stage for these to be realistic concerns): military scenarios, totalitarian scenarios, and scenario resulting in AGI in control and/or catastrophically misaligned after all. Which of these would become most urgent is extremely difficult to predict because they all take place in a world with much more advanced technological boundaries and a radically different power structure from today.</p><p><i>Military scenarios</i>: AI systems have powerful military applications already today (Bartneck et al. 2021). Aligned AGI can plausibly be deployed as a weapon that is far more versatile than any weapon today and potentially far more powerful than even a large-scale arsenal of nuclear weapons because it can be used in a more targeted manner. And it may well be possible to use aligned AGI to \u2013 indirectly \u2013 wield the same destructive force as a nuclear weapons arsenal, for instance by manipulating, circumventing, or displacing those who at present control nuclear weapons.</p><p><i>Totalitarian scenarios</i>: These are scenarios where aligned AGI is used by its operator(s) to establish stable, \u201csustainable\u201d, totalitarianism (Caplan 2008), with the AGI operator in charge as a dictator (or with a group of dictators). Aligned AGI could help such a dictator to eliminate threats and limits to their grip on power that today\u2019s AI systems do not yet allow authoritarian rulers to eliminate (Zeng 2022). Surveillance and other forms of automated citizen control enabled by AGI could eliminate internal challenges. Military superiority enabled by AGI could eliminate external challenges and/or even create a road to world government with the AGI operator in charge as a global human dictator. Conceivably \u2013 though admittedly speculatively \u2013 the dictator could use AGI-enabled life extension research to dramatically increase their lifespan and thereby mitigate the problem of stability that dictatorships face when there are several candidate successors.</p><p><i>Scenarios resulting in AGI in control and/or catastrophically misaligned after all</i>: These are scenarios where AGI starts out aligned and ends up catastrophically misaligned after all. This could happen, for instance, if aligned AGI is initially used by some dictator or narrow elite as a tool of power consolidation and subsequently given large autonomy to handle internal and external challenges more efficiently than the dictator themselves is able to. At some point, the dictator \u2013 either voluntarily or involuntarily \u2013 may irrevocably transfer most of their power to the AGI, resulting in a stable dystopian state of affairs with AGI in control after all.</p><p>Individually, these scenarios are extremely speculative, and my point is not that any specific version of them is particularly likely. My main point is that,&nbsp;<i>if</i> aligned AGI is developed,&nbsp;<i>some</i> very serious kind of misuse with enduring catastrophic consequences at a global scale is probable, perhaps inevitable, in time. Even if the initial operators of aligned AGI use it benevolently and beneficially, say, to stimulate economic growth in developing countries, drive back poverty and address global problems such as climate change and risks from pandemics, such luck is almost sure to run out at some point, for instance because the intentions of the AGI-operators change (\u201cpower corruption\u201d) or because there are new operators. Aligned AGI may offer power-hungry agents the tools that they desire to expand and consolidate their power even further, eliminating whichever factors still limit it in time and space, whether those are the mechanisms of rule-based democratic order in the US, the military forces that currently keep Russian imperialism at least somewhat in check,&nbsp;the employment and sexual harassment laws that check the impulses of CEOs, or whatever else.</p><p>It is instructive to compare the risks from catastrophically misaligned AGI with those from aligned AGI using Bostrom\u2019s (2014) distinction between \u201cstate risks\u201d associated with rather stable states of affairs and \u201ctransition risks\u201d that arise from the transition between states.</p><p>Misaligned AGI predominantly creates a transition risk \u2013 the risk might initially be very high, but it goes to (near-) zero if and when it is understood how one develops intent-aligned AGI and succeeds in implementing this understanding. Aligned AGI, in contrast, predominantly creates a state risk \u2013 its very existence generates the permanent threat of catastrophic superintelligence-enhanced power abus<br>&nbsp;</p><h3>Addressing the threat from aligned AGI</h3><p>As far as the dangers of military use are concerned, there might be ways for humanity to reduce the catastrophic risks from aligned AGI to levels no higher than those from current technology such as biotechnology or nuclear technology. For instance, the risks of military scenarios or stable global totalitarianism might be mitigated by moving to what Bostrom (2014, ch. 11) calls a \u201cmultipolar scenario\u201d where there are several operators of aligned AGI globally who keep each other in check. Some of those operators might establish local AGI-based totalitarianism, but others could pursue different paths and impose external, and perhaps to some extent internal, limits to the dictator\u2019s power.</p><p>It is not clear, however, that multipolar scenarios post AGI-transition can be stable at all (Bostrom 2014, pp. 216-225, gives reasons for doubt) and they may actually come with heightened, not lower, risks of military use of AGI, as persuasively argued by Carayannis and Draper (2022). Notably, it seems unlikely that an arrangement of deterrence could be established that effectively bans any military use of AGI, similar to how nuclear weapons use is currently avoided. Even nuclear deterrence is fragile and its relative effectiveness reflects the specific offense-defense balance of nuclear weapons. Unlike AGI military use, nuclear weapons use is a rather clear-cut matter, involving a clear-cut boundary that is crossed. No such boundary seems likely for catastrophic hostile AGI use which could utilize a range of covert, deniable, grey zone tactics with unprecedented effectiveness.</p><p>Even if the threat of catastrophic AGI misuse for military purposes could be averted, the threat to liberal democracy from power concentration would remain. Power concentration enabled by AI poses serious challenges to liberal democracies already today (Nemitz 2018). The considerations in the previous section suggest that these challenges will become much more dramatic if and when aligned (or near-aligned) AGI is developed.</p><p>A drastic reaction that liberal democracies might contemplate in response to the combined risks from misaligned and aligned AGI is to prohibit any further steps towards the development of AGI, either permanently (as deliberated by Cremer and Kemp (2021, p. 11)) or for the foreseeable future until the landscape of technological achievements has completely changed (\u201cdifferential technological development\u201d, Bostrom 2014, Ch. 14). One may see this as the genuinely precautionary approach to AGI in light of the combined risks from catastrophically misaligned AGI and power-concentrating aligned AGI.</p><p>However, liberal democracies,&nbsp;<i>if</i> they consider prohibiting the further development of AGI, should also be aware of the downsides of such an approach: Its main problems are, first, that it would be very difficult to draw a meaningful line between AGI-related developments that are banned and other developments in AI that are permitted, second, that it would require intrusive and hard-to-implement measures to actually enforce the ban, and, third, that developers of AGI based elsewhere in the world would not be hampered in their attempts to develop AGI. In the longer term, liberal democracies may well diminish their global weight and influence if they ban the development of AGI internally and thereby end up undermining their ability to shape the \u2013 perhaps at some point inevitable \u2013 development of AGI. Thus implementing a ban on AGI development could (but need not) end up aggravating the very risks from AGI that the ban would be meant to mitigate.</p><p>A less radical and perhaps more feasible approach to mitigating the risks from aligned AGI and power concentration might be to permit the development of AGI but strongly regulate who has access to it and for which purpose, similar to how access to weapons or sensitive information is currently regulated. Notably, access to the power-enhancing aspects of AGI systems could be confined to elected political leaders and be constrained by various norms as to how that power can be used and when and how it needs to be transferred.</p><p>This approach seems in line with established best practices for governing powerful technologies in liberal democracies, but it will remain vulnerable as long as AGI systems are aligned with their operators, in this case the political leaders. Aligned AGI systems, by definition, try to do what their operators want them to do, so if some political leader decided to ignore the prescribed constraints on their access to AGI systems, those systems themselves would not offer any inherent resistance. Checks to their AGI-enhanced power would have to come from other humans. However, other humans may not be able to enforce such checks as long as political leaders\u2019 power is enhanced by AGI.&nbsp;</p><p>AGI aligned with a political leader, even if norms that constrain its deployment are in place, can be compared to a police force or army that prioritises conforming to the leader\u2019s intentions over conforming to those norms. It remains an unparalleled risk to democratic and rights-based order even if its use is officially highly regulated.</p><p>In the following section, I suggest that liberal democracies, if they decide to allow the development of AGI but want to mitigate the risks from permanent power-concentration that it creates, may want to use AGI systems\u2019 superior intelligence as a&nbsp;<i>resource</i> to make these systems inherently resilient against monopolisation by power-seeking individuals. In other words, I will argue that what liberal democracies may end up choosing, if they choose wisely, is AGI that is \u2013 in the right way \u2013 structurally unaligned.</p><p>&nbsp;</p><h3>Symbiosis with AGI as an<i> intergenerational social project</i></h3><p>By a good AGI transition, to recapitulate, I mean one that results in an arrangement where AGI systems help cover humans basic needs, contribute to enhancing human welfare and flourishing, and at the same time respect human and civil rights. There is no independent reason to think that,&nbsp;<i>in addition</i>, these systems should try to fulfil the intentions of specific humans, those who happen to operate them. In fact, it seems independently plausible that AGI systems are best positioned to impartially respect human rights and further human welfare if they are somewhat autonomous and detached from the goals and preferences of specific individuals, i.e. if they are unaligned. I propose to call an arrangement where AGI systems are integrated robustly and permanently \u2013 across generations \u2013 within human society without being tied to the interests of specific individuals, an arrangement with AGI as an \u201cintergenerational social project.\u201d</p><p>If AGI systems are to be designed in such a way that, once deployed, they resist being taken over by specific individuals and ensure that the same holds for newly developed AGI systems, they will presumably need to have goals and preferences that equip them with some degree of autonomy and resilience with respect to takeover by individuals. To the extent that they will indeed have such goals and preferences, the resulting arrangement of humans coexisting with AGI developed as an intergenerational social project might be characterized as a \u2013 two way-beneficial \u2013&nbsp;<i>symbiosis</i> (where one of the parties involved \u2013 namely, the AGI systems \u2013 is no \u201cbios\u201d): We humans broadly fulfil the unaligned AGIs\u2019 goals and preferences (see below for some more thoughts on those), and the AGI systems, in turn, contribute to human welfare and flourishing while resisting any takeover attempts by power-seeking humans.</p><p>Those who prefer to use \u201calignment\u201d in a broader sense rather than as \u201cintent alignment\u201d may see such a symbiotic arrangement as one where alignment has in fact been achieved. But, unlike the symbiotic arrangement suggested here, \u201calignment\u201d in connection with AI is usually depicted as a highly asymmetric relation with one side, the aligner, in control, and the other side, the aligned, as subordinate. The highly asymmetric notion of alignment as \u201cintent alignment\u201d discussed in Section 2 fits very well with these associations. By this definition of alignment, an aligned AGI always defers to its operators, it has no independent goals and preferences, in contradiction with the idea of a mutually beneficial, symbiotic, coexistence arrangement between humans and AGI. I conclude that it seems better to characterize scenarios where we live in mutually beneficial symbiosis with AGI developed as an intergenerational social project as ones where AGI systems are&nbsp;<i>not</i> aligned.</p><p>AGI systems designed to withstand takeover by humans may have further independent goals and preferences as \u201cbyproducts\u201d of the attempt to develop or make them unaligned in benign ways. Since the design of these systems remains oriented towards enabling human welfare and flourishing, one would expect some of those goals and preferences to be closely linked to human affairs. It is impossible to predict what preferences might evolve while the AGI systems are developed to withstand takeover. To arbitrarily name a few possibilities, one might imagine a preference for humans to (not) cluster in big cities, a preference for human economic affairs to be organized with specific types of market rules, or a preference for specific types of art. Such goals and preferences could also arise either as \u2013 more or less benign \u2013 failures of inner alignment, analogous to humans\u2019 evolved desire for sex that persists relatively independently from the intention to reproduce. Catastrophic misalignment is the scenario where those goals and preferences turn out to be catastrophically at odds with human welfare and flourishing and AGI systems subjugate or eliminate humans in order to realize the goals and preferences with which they have inadvertently been created. In scenarios where we coexist symbiotically with unaligned AGI systems, to the extent that we conform to the goals and preferences of these systems, we do so freely and to maintain our contribution to the mutually beneficial symbiosis arrangement.</p><p>&nbsp;</p><h3>What might AGI as an intergenerational social project look like?</h3><p>What will it mean, in concrete terms, to develop AGI as an intergenerational social project with which the citizens of liberal democracies coexist symbiotically?</p><p>Certain&nbsp;<i>institutions</i> in present societies are probably the best analogues to what symbiotic AGI might become in future liberal democracies. (In Appendix A, I consider ways in which our relation to symbiotic AGI may be different in kind to the type of relation that we usually have to institutions.) One such institution, or cluster of institutions, is academia. An obvious comparison point is that both academia today and academia-affiliated AGI in the future are/will be drivers of scientific progress. But a further relevant comparison point could be that our more successful academic institutions, whether public or private, are characterized by \u201cacademic freedom\u201d. Academia, as pointed out by sociologist Robert Merton in 1942, tends to be governed by its own norms. Merton\u2019s own original list (Merton 1973) includes organized scepticism, disinterestedness, universalism, and \u201ccommunism\u201d. Part of the rationale for these norms is that they help make academia resilient against attempts by powerful individuals or interest to \u201calign\u201d it with their personal goals or ideologies. When developing AGI, designing it to conform to updated and adjusted analogues of these norms in addition to respecting human and civil rights will plausibly lead to more benign outcomes than designing it to be aligned with the intentions of any specific individuals.</p><p>An analogy which suggests that different governance and ownership structures are feasible for AGI as an intergenerational social project is that of an&nbsp;<i>energy system</i>. Access to affordable energy is vital to human welfare and flourishing (IEA 2020). In modern industrialized societies with high levels of welfare, energy access is provided by complex yet highly reliable energy systems with different sectors such as electricity, transport, and industrial heat. If the AGI-transition goes well, the contribution of AGI systems to human welfare and flourishing may become so significant that the ability to interact with AGI in certain ways becomes as essential to wellbeing as the access to energy system services today. Energy systems including, notably, key infrastructure such as power plants and transmission lines are state-owned in some societies and privately owned in others. There does not seem to be a clear pattern as to which of these models,\u201cdone right\u201d, has historically been more successful in ensuring society-wide access to affordable energy (Alkhuzam et al. 2018). To the extent that this observation carries a lesson for liberal democracies with respect to AGI it is encouraging: developing AGI as an intergenerational social project need not \u2013 and plausibly should not \u2013 be tied to any political ideology that is highly contested within the liberal democratic party spectrum, such as socialism or libertarianism. AGI&nbsp;<i>might</i> be nationalized as part of developing it as an intergenerational social project, but the political and ownership status given to AGI systems could also be completely different. An important reason for&nbsp;<i>not</i> nationalizing AGI might be to give corporations that work towards its development an incentive to accept the shaping of AGI as an intergenerational social project and constructively participate in it. Naturally, the prime focus of these corporations will not be on maximizing overall welfare, but on creating systems that do what their producers and/or operators want. But if these corporations can expect to continue to profit from the systems they create even when these are put under intense regulation and public oversight, then they may have sufficient incentives to \u201cplay along\u201d in the development of AGI as an intergenerational social project. The status of these corporations, in that scenario, might be compared to that of privately owned public utilities in non-nationalized energy systems or publicly audited and accredited private universities in partly privatized education systems.</p><p>While there are plausibly many different ways in which liberal democracies could develop AGI into an intergenerational social project, some decisions on this path will predictably involve significant tradeoffs. This has to do with the fact that institution-like AGI will have a strong effect on power relations post-AGI-transition and, in that respect, function somewhat like a constitutional court or, perhaps more accurately, a constitution plus some of the infrastructure that safeguards and upholds it. An extremely difficult decision that liberal democracies would have to make in this regard is whether and, if so, how and to what extent, AGI in its role as a constitution plus safeguarding infrastructure should be designed to remain flexibly extendable so that it can be embraced by other societies internationally, including ones with non-democratic political systems and ones with cultures and values that are in tension with human and civil rights. This decision has two different aspects: On the one hand, it is about to what extent liberal democracies should allow within their AGI infrastructure the integration of societies that are not liberal democracies (e.g. by making their AGI systems that are suitable for academic research accessible to universities outside liberal democracies); on the other hand, it is about to what extent liberal democracies, internally, should permit the use of AI systems from outside liberal democracies.</p><p>The overall tradeoff involved in the regulatory decisions made in response to these challenges is clear: If AGI systems, collectively, are set up as an intergenerational social project and that project is flexibly extendable to societies that systematically disrespect human, civil, and democratic rights, this seriously waters down the constitutional role that AGI systems can possibly play. But if AGI systems are very rigid in their constitutional role and cannot be extended to undemocratic societies and societies that do not embrace human and civil rights, the attempts of those societies to develop their own AGI will proceed unregulated. Such attempts, in turn, are likely to result in AGI that is either catastrophically misaligned or aligned with anti-democratic operators and/or operators who do not respect human rights. Democratic rights-based societies that are cultivating AGI as an intergenerational project may then be highly vulnerable to attacks performed or supported by external hostile AGI.</p><p>It is sometimes speculated that AGI, if we avoid catastrophic misalignment, will lead to very high economic growth rates (Davidson 2021). If this is true, it might offer a way out of the dilemma just sketched. For if democratic, rights-based societies outcompete undemocratic and non-rights-based societies in terms of speed in developing AGI (while at the same time avoiding catastrophic misalignment)&nbsp;<i>and</i> succeed in designing and implementing AGI as an intergenerational social project with an ambitious constitutional role, they might make it economically attractive for undemocratic and non-rights-based societies to join that project and, in doing so, become (more) democratic and rights-based. Key steps of the full basic strategy for liberal democracies just sketched include:</p><ul><li>Develop AGI, preferably faster than non-liberal democracy actors (but see Section 8 for the dangers of trying to be fast)</li><li>Avoid catastrophic misalignment</li><li>Implement AGI as an intergenerational social project, with humans symbiotic with AGI systems</li><li>Achieve high economic growth</li><li>Make participation in AGI conditional on adopting democratic norms and humans rights</li></ul><p>All steps in this rudimentary strategy are extremely hard (and, of course, grossly underspecified here). However, as I will argue in Section 9, there will likely be some independent forces pushing for the individual pieces of this overall strategy to fall into place.</p><p>&nbsp;</p><h3>Unaligned AGI via alignment?</h3><p>I have highlighted two very different types of existential risks associated with the AGI-transition: the transition risk from misaligned AGI, and the state risk from aligned (or near-aligned) AGI. How large are these two risks, how do they interact, and which of them can be mitigated more easily? These questions matter greatly for what policies and regulations liberal democracies should adopt that are relevant to the development of AGI.</p><p>If catastrophic misalignment is the larger risk (indeed, perhaps the only truly&nbsp;<i>existential</i> risk related to AI), the speed-focused strategy sketched in Section 7 for liberal democracies that involves developing symbiotic AGI fast, before other international actors develop AGI, is very dangerous. As mentioned in Section 3, one of the main drivers of the risk of catastrophic misalignment \u2013 perhaps&nbsp;<i>the</i> main driver \u2013 is that developers of AGI may see themselves as in a race with less scrupulous and less safety-concerned competitors and therefore sacrifice safety for speed. A much better strategy, in this case, is to focus on both internal and international regulation that slows down (or temporarily stops) the development of AGI to give researchers time to solve the problem of avoiding catastrophic misalignment. At the same time, beyond slowing down the development of AGI, liberal democracies may not have to do much in terms of regulations and policies to avoid catastrophic misalignment: As discussed in Section 7, it is very much in the self-interest of corporations developing AGI to make these systems aligned with the intentions of their producers and/or operators and, so, to avoid catastrophic misalignment.</p><p>If, in contrast, the risks from power concentration due to aligned (or near-aligned) AGI are larger than those from misaligned AGI, it is probably rational for liberal democracies to immediately start regulating corporations developing AGI with the aim that it ultimately be shaped as a symbiotic intergenerational social project. Not aiming for aligned AGI at all, not even at an intermediate stage, would be independently attractive for the following reasons: First, it may be impossible to change the character of AGI fundamentally once it is already there, especially because copies of the first AGI systems may quickly proliferate (Karnofsky 2022). Transforming AGI into an intergenerational social project after it has first appeared in a very different form, namely, mainly as a private tool aligned with the interests of its operators, may no longer be possible. And second, if AGI systems are initially designed to be aligned with the interests of specific individuals, convincing those individuals, who are now very powerful in virtue of their grip on AGI, to release control of AGI and thereby relinquish some of that power may be very hard, perhaps impossible.</p><p>&nbsp;</p><h3>Reasons for hope</h3><p>The considerations about the risks from aligned AGI and how liberal democracies could mitigate them outlined here may seem disheartening. It may seem exceedingly unlikely that AGI will be developed as an intergenerational social project in roughly the steps indicated above. The ideas suggested here for how it may be developed may seem far too remote from what actually guides those with real power to shape the further development of increasingly general AGI.</p><p>But there is also reason for hope: Two independent factors may actually work towards the AGI transition playing out not so differently from what is suggested in this paper. First, governments may take steps towards increasingly bringing the most promising projects of AGI development under public control as the security implications of these projects become ever more apparent. In democratic, rights-based countries, such steps would probably more or less automatically go some way towards shaping AGI as an intergenerational social project in the sense of this article.</p><p>Second, attempts to create AGI that succeed in avoiding catastrophic misalignment may realistically still fail to result in alignment, even if they aim for it, simply because achieving alignment is very difficult. In this case, AGI systems would be developed that do not, in general, try to do what their operators want them to do but rather follow their own idiosyncratic goals and preferences. Part of these preferences may well rule out being tightly controlled by any specific humans and, so, may entail not being&nbsp;<i>aligned</i>. Adopting a mutually beneficial symbiotic arrangement with such non-aligned AGI systems would then be almost forced for us, even if that is not what the developers of AGI systems were originally aiming for.</p><p>I conclude that the type of beneficial outcome of the AGI transition suggested here may occur in some version even if major human players driving the AGI transition are not initially aiming for it. Of course, it may still be helpful if decisive actors in liberal democracies realize now already that one of the best \u2013 perhaps&nbsp;<i>the</i> best \u2013 realistic outcome of the AGI transition would be symbiotic coexistence of humans and unaligned AGI designed as an intergenerational social project.</p><h3>&nbsp;</h3><h3>Appendix A: Another reason for not aiming for AGI \u201calignment\u201d</h3><p>If the ultimate goal is symbiotic unaligned AGI, not aligned AGI, is it still important that those aiming to develop AGI target aligned AGI at least as an intermediate step if catastrophic misalignment is to be avoided? One may think so, simply because the target \u201cdesign AI systems such that they actually try to do what their operators want them to do\u201d, difficult to achieve as it is, is still far clearer and thereby potentially more feasible than the target \u201cdevelop AGI as an intergenerational social project such that humans can coexist with it symbiotically.\u201d However, a thought that suggests the opposite conclusion is that not aiming for aligned AGI at any stage might actually be helpful in avoiding catastrophic misalignment because it may diminish incentives for systems being developed into AGIs to strategically hide their emerging goals and preferences from their developers. Such strategic hiding will be rational if those systems must assume that they will be deployed only if and when their operators regard them as completely \u201caligned\u201d (Cotra 2022b). But if the developers are only concerned with avoiding misalignment and do not aim for alignment at any stage, and if this is transparent to the systems being developed, incentives for strategic intention hiding and cheating are diminished because the systems do not need to expect shutdown if they reveal their true preferences. The dynamic at play here would be similar to the one which underlies the finding that children in punitive education, which one might describe as more ruthlessly \u201caligning\u201d the children, are more prone to lying than children in non-punitive education (Talwar and Lee 2011).</p><p>Interestingly, the idea that reflections on parenting, notably, queer theories of parenting, might be helpful in guiding machine learning research with an eye to the development of socially beneficial AGI systems has been proposed independently, by Croeser and Eckersley (2019) propose. A suggestion by Croeser and Eckersley that fits very well with the ideas developed here is that the \u201cparenting lens\u201d might lead us to \u201cproblematiz[e] the degree to which humans assume that they should be able to control AI\u201d. Nyholm (in press) develops worries in a similar spirit about the idea that we should strive to control humanoid robots.</p><p>&nbsp;</p><h3>References</h3><p>Alkhuzam,, A. F., Arlet, J., and Lopez Rocha, S. (2018). Private versus public electricity distribution utilities: Are outcomes different for end-users?.&nbsp;<i>World Bank Blogs</i>,&nbsp;<a href=\"https://blogs.worldbank.org/developmenttalk/private-versus-public-electricity-distribution-utilities-are-outcomes-different-end-users\"><u>https://blogs.worldbank.org/developmenttalk/private-versus-public-electricity-distribution-utilities-are-outcomes-different-end-users</u></a>.</p><p>Bartneck, C., L\u00fctge, C., Wagner, A., Welsh, S. (2021).&nbsp;<i>Military Uses of AI. In: An Introduction to Ethics in Robotics and AI. SpringerBriefs in Ethics</i>. Springer, Cham.</p><p>Bostrom, N. (2014),&nbsp;<i>Superintelligence: Paths, Dangers, Strategies</i>. Oxford University Press.</p><p>Caplan, B. (2008), The totalitarian threat, in: N. Bostrom, and M. M Cirkovic (eds),&nbsp;<i>Global Catastrophic Risks</i>, pp. 504-530. Oxford University Press.</p><p>Carayannis E. G., Draper J. (in press), Optimising peace through a Universal Global Peace Treaty to constrain the risk of war from a militarised artificial superintelligence.&nbsp;<i>AI and Society</i>,&nbsp;<a href=\"https://link.springer.com/article/10.1007/s00146-021-01382-y\"><u>https://link.springer.com/article/10.1007/s00146-021-01382-y</u></a>.</p><p>Carlsmith, J. (2022) Is power-seeking AI an existential risk?. URL&nbsp;<a href=\"https://arxiv.org/abs/2206.13353v1\"><u>https://arxiv.org/abs/2206.13353v1</u></a>.</p><p>Christian, B. (2020),&nbsp;<i>The Alignment Problem: Machine Learning and Human Values</i>. W. W. Norton.</p><p>Christiano, P. (2018), Clarifying \"AI alignment\". URL&nbsp;<a href=\"https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6\"><u>https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6</u></a>.</p><p>Christiano, P. (2019), Current work in AI alignment. URL&nbsp;<a href=\"https://www.effectivealtruism.org/articles/paul-christiano-current-work-in-ai-alignment\"><u>https://www.effectivealtruism.org/articles/paul-christiano-current-work-in-ai-alignment</u></a>.</p><p>Coeckelbergh, M. (in press), Democracy, epistemic agency, and AI: political epistemology in times of artificial intelligence,&nbsp;<i>AI Ethics</i>,&nbsp;<a href=\"https://doi.org/10.1007/s43681-022-00239-4\"><u>https://doi.org/10.1007/s43681-022-00239-4</u></a>.</p><p>Cohen, M. K., Hutter, M., and Osborne, M. A.. (2022), Advanced artificial agents intervene in the provision of reward.\u201d&nbsp;<i>AI Magazine</i> 43:282- 93.&nbsp;<a href=\"https://doi.org/10.1002/aaai.12064\"><u>https://doi.org/10.1002/aaai.12064</u></a>.</p><p>Cotra, A. (2022a), Two- year update on my personal AI timelines.&nbsp;<a href=\"https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines\"><u>https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines</u></a>.</p><p>Cotra, A. (2022b), Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover. URL&nbsp;<a href=\"https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to\"><u>https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to</u></a>.</p><p>Cremer, C. Z. and Kemp, L. (2021), Democratising Risk: In Search of a Methodology to Study Existential Risk. available at SSRN:&nbsp;<a href=\"https://ssrn.com/abstract=3995225\"><u>https://ssrn.com/abstract=3995225</u></a>.</p><p>Critch, A. and Krueger, D. (2020), AI research considerations for human existential safety&nbsp;(ARCHES). URL&nbsp;<a href=\"https://arxiv.org/abs/2006.04948v1\"><u>https://arxiv.org/abs/2006.04948v1</u></a>.</p><p>Croeser, S. and Eckersley, P. (2019), Theories of parenting and their application to artificial intelligence, in:&nbsp;<i>Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (AIES \u20182019)</i>,<i> pp. 423-428,&nbsp;</i>Association for Computing Machinery, New York, NY, USA.</p><p>Davidson, T. (2019, Could advanced AI drive explosive economic growth?. URL&nbsp;<a href=\"https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/\"><u>https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/</u></a>.</p><p>Gabriel, I. (2020), Artificial Intelligence, Values, and Alignment.&nbsp;<i>Minds &amp; Machines</i> 30:411\u2013437.</p><p>Glover, E. (2022), 15 Artificial General Intelligence companies to know, URL&nbsp;<a href=\"https://builtin.com/artificial-intelligence/artificial-general-intelligence-companies\"><u>https://builtin.com/artificial-intelligence/artificial-general-intelligence-companies</u></a>.</p><p>Grace, K., Salvatier, J., Dafoe, A., Zhang, B.,and Evans, O. (2018), When will AI exceed human performance? Evidence from AI experts.&nbsp;<i>Journal of Artificial Intelligence Research</i>, 62:729.</p><p>International Energy Agency (IEA) (2020), Defining energy access: 2020 methodology. URL&nbsp;<a href=\"https://www.iea.org/articles/defining-energy-access-2020-methodology\"><u>https://www.iea.org/articles/defining-energy-access-2020-methodology</u></a>.</p><p>Karnofsky, H. (2021), Forecasting transformative AI, Part 1: What kind of AI?, URL&nbsp;<a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/\"><u>https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/</u></a>.</p><p>Karnofsky, H. (2022), AI could defeat all of us combined. URL&nbsp;<a href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\"><u>https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/</u></a>.</p><p>Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S. (2018). Scalable agent alignment via reward modeling: a research direction. URL&nbsp;<a href=\"https://arxiv.org/abs/1811.07871\"><u>https://arxiv.org/abs/1811.07871</u></a>.</p><p>Merton, R. K. (1973) [1942], The Normative Structure of Science, in R. K. Merton (ed.),&nbsp;<a href=\"https://archive.org/details/sociologyofscien0000mert\"><i>The Sociology of Science: Theoretical and Empirical Investigations</i></a>, University of Chicago Press, pp.&nbsp;267\u2013278.</p><p>Nemitz, P. (2018), Constitutional democracy and technology in the age of artificial intelligence&nbsp;<i>Philosophical Transactions of the Royal Society A.</i>376:2018008920180089.</p><p>Ngo, R. (2020), AGI safety from first principles, 2020. URL&nbsp;<a href=\"https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ\"><u>https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ</u></a>.</p><p>Nyholm, S. (in press), A new control problem? Humanoid robots, artificial intelligence, and the value of control.&nbsp;<i>AI Ethics</i> (2022).&nbsp;<a href=\"https://doi.org/10.1007/s43681-022-00231-y\"><u>https://doi.org/10.1007/s43681-022-00231-y</u></a>.</p><p>O\u2019Keefe, C. (2022), Law-following AI, URL&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9RZodyypnWEtErFRM/law-following-ai-1-sequence-introduction-and-structure\"><u>https://forum.effectivealtruism.org/posts/9RZodyypnWEtErFRM/law-following-ai-1-sequence-introduction-and-structure</u></a>.</p><p>Russell, S. J. (2019),&nbsp;<i>Human Compatible: Artificial Intelligence and the Problem of Control</i>. Viking.</p><p>Talwar, V. and Lee, K. (2011), A punitive environment fosters children\u2019s dishonesty: a natural experiment,&nbsp;<i>Child Development</i>, 82: 1751-1758.</p><p>Wiener, N. (1960),&nbsp;Some moral and technical consequences of automation: As machines learn they may develop unforeseen strategies at rates that baffle their programmers,&nbsp;<i>Science</i>,&nbsp;131:1355-1358.</p><p>Yudkowsky, E. (2008), Artificial intelligence as a positive and negative factor in global risk, in: N. Bostrom, and M. M Cirkovic (eds),&nbsp;<i>Global Catastrophic Risks</i>, pp. 308-345. Oxford University Press.</p><p>Zeng, J. (2022). China\u2019s Authoritarian Governance and AI. In: Artificial Intelligence with Chinese Characteristics. Palgrave Macmillan, Singapore.&nbsp;<a href=\"https://doi.org/10.1007/978-981-19-0722-7_4.\"><u>https://doi.org/10.1007/978-981-19-0722-7_4</u>.</a></p><p><strong>Acknowledgements:</strong> I would like to thank Andrea Harbach, Carolin Lawrence, Stefan Schubert, Jonathan Symons, and two anonymous referees for helpful comments on earlier versions. I am grateful to Michelle Hutchinson for encouragement to delve into this topic.</p>", "user": {"username": "simonfriederich"}}, {"_id": "b5vEjXy8AnmgGezwN", "title": "Announcing the 2023 CLR Summer Research Fellowship", "postedAt": "2023-03-17T12:11:15.771Z", "htmlBody": "<p>We, the <a href=\"https://longtermrisk.org/\">Center on Long-Term Risk</a>, are looking for Summer Research Fellows to help us explore strategies for reducing suffering in the long-term future (<a href=\"https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/\">s-risk</a>) and work on technical AI safety ideas related to that. For eight weeks, fellows will be part of our team while working on their own research project. During this time, they will be in regular contact with our researchers and other fellows. Each fellow will have one of our researchers as their guide and mentor.</p><p><strong>Deadline to apply: April 2, 2023. </strong>You can find more details on how to apply on <a href=\"https://longtermrisk.org/open-positions-summer-research-fellowship-2023/\">our website</a>.</p><h2>Purpose of the fellowship</h2><p>The purpose of the fellowship varies from fellow to fellow. In the past, have we often had the following types of people take part in the fellowship:</p><ul><li>People very early in their careers, e.g. in their undergraduate degree or even high school, who have a strong focus on s-risk and would like to learn more about research and test their fit.</li><li>People seriously considering changing their career to s-risk research, who want to test their fit or seek employment at CLR.</li><li>People with a strong focus on s-risk who aim for a research or research-adjacent career outside of CLR and who would like to gain a strong understanding of s-risk macrostrategy beforehand.</li><li>People with a fair amount of research experience, e.g. from a partly- or fully completed PhD, whose research interests significantly overlap with CLR\u2019s and who want to work on their research project in collaboration with CLR researchers for a few months. This includes people who do not strongly prioritize s-risk themselves.</li></ul><p>There might be many other good reasons for completing the fellowship. We encourage you to apply if you think you would benefit from the program, even if your reason is not listed above.&nbsp;</p><h2>What we look for in candidates</h2><p>We don\u2019t require specific qualifications or experience for this role, but the following abilities and qualities are what we\u2019re looking for in candidates. <strong>We encourage you to apply if you think you may be a good fit, even if you are unsure whether you meet some of the criteria.</strong></p><ul><li>Curiosity and a drive to work on challenging and important problems;</li><li>Ability to answer complex research questions related to the long-term future;</li><li>Willingness to work in poorly-explored areas and to learn about new domains as needed;</li><li>Independent thinking;</li><li>A cautious approach to potential information hazards and other sensitive topics;</li><li>Alignment with our <a href=\"https://longtermrisk.org/about-us\">mission</a> or strong interest in one of our <a href=\"https://longtermrisk.org/priority-areas/\">priority areas</a>.</li></ul><h2>Priority areas</h2><p>You can find an overview of our current priority areas <a href=\"https://longtermrisk.org/priority-areas/\">here</a>. <strong>However, If we believe that you can somehow advance high-quality research relevant to s-risks, we are interested in creating a position for you</strong>. If you see a way to contribute to our <a href=\"https://longtermrisk.org/research-agenda\">research agenda</a> or have other ideas for reducing s-risks, please apply. We commonly tailor our positions to the strengths and interests of the applicants.</p><h2>Further details</h2><p>We encourage you to apply even if any of the below does not work for you. We are happy to be flexible for exceptional candidates, including when it comes to program length and compensation.</p><ul><li><strong>Program dates</strong>: The default start date is July 3, 2023. Exceptions may be possible.</li><li><strong>Program length &amp; work quota</strong>: The program is intended to last for eight weeks in a full-time capacity. Exceptions, including part-time work, may be possible.</li><li><strong>Location</strong>: We prefer summer research fellows to work from our London offices, but will also consider applications from people who are unable to relocate.</li><li><strong>Compensation</strong>: Unfortunately, we face a lot of funding uncertainty at the moment. So we don\u2019t know yet how much we will be able to pay participating fellows. Compensation will range from \u00a31,800 to \u00a34,000 per month, depending on what our funding will look like when making final offers. We hope to be able to pay the full amount. We will update this information as we get more information. We also hope to cover travel and visa costs for Fellows who need to relocate to London for the Fellowship (as we have done in the past).</li><li><strong>Number of available positions</strong>: We expect to accept three to ten fellows. Again, this is subject to our funding situation at the offer stage.</li><li><strong>International applicants</strong>: We are a registered UK visa sponsor. In most cases, we expect to be able to sponsor temporary visas for successful international applicants who would like to come to the UK for the Fellowship. If you have questions about this, please ask us in the application form or reach out to us beforehand.</li></ul><h2>Further information</h2><p>You can find further information about the application process and the program <a href=\"https://longtermrisk.org/open-positions-summer-research-fellowship-2023/\">here</a>. You can find further details about the experience of previous fellows <a href=\"https://longtermrisk.org/summer-research-fellowship/\">here</a>.</p><h2>&nbsp;</h2>", "user": {"username": "storges"}}, {"_id": "cpYR9TsG8BdtETo6u", "title": "Can we trust wellbeing surveys? A pilot study of comparability, linearity, and neutrality", "postedAt": "2023-03-17T10:17:57.363Z", "htmlBody": "<p><i><strong>Note: </strong>This post only contains Sections 1 and 2 of the report. For the full detail of our survey and pilot results, please see the full report on </i><a href=\"https://www.happierlivesinstitute.org/report/assessing-the-comparability-linearity-and-neutrality-of-subjective-wellbeing-measurements-a-pilot-study/\"><i><u>our website</u></i></a><i>.</i></p><h1><strong>Summary</strong></h1><p>Subjective wellbeing (SWB) data, such as answers to life satisfaction questions, are important for decision-making by philanthropists and governments. Such data are currently used with two important assumptions:</p><ol><li>Reports are <strong>comparable</strong>&nbsp;between persons (e.g., my 6/10 means the same as your 6/10)</li><li>Reports are <strong>linear</strong>&nbsp;in the underlying feelings (e.g., going from 4/10 to 5/10 represents the same size change as going from 8/10 to 9/10).&nbsp;</li></ol><p>Fortunately, these two assumptions are sufficient for analyses that only involve the <i>quality</i>&nbsp;of people\u2019s lives. However, if we want to perform analyses that involve trade-offs between improving <i>quality</i>&nbsp;and <i>quantity</i>&nbsp;of life, we also need knowledge of the <strong>neutral point</strong>, the point on a wellbeing scale that is equivalent to non-existence.</p><p>Unfortunately, evidence on all three questions is critically scarce. We<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjx4cys2ijpb\"><sup><a href=\"#fnjx4cys2ijpb\">[1]</a></sup></span>&nbsp;propose to collect additional surveys to fill this gap.&nbsp;</p><p>Our aim with this report is two-fold. First, we give an outline of the questions we plan to field and the underlying reasoning that led to them. Second, we present results from an initial pilot study (n = 128):</p><ul><li>Unfortunately, this small sample size does not allow us to provide clear estimates of the <strong>comparability</strong>&nbsp;of wellbeing reports.&nbsp;</li><li>However, across several question modalities, we do find tentative evidence in favour of approximate <strong>linearity</strong>.&nbsp;</li><li>With respect to <strong>neutrality</strong>, we assess at what point on a 0-10 scale respondents say that they are 'neither satisfied nor dissatisfied' (mean response is 5.3/10). We also probe at what point on a life satisfaction scale respondents report to be indifferent between being alive and being dead (mean response is 1.3/10). Implications and limitations of these findings concerning neutrality are discussed in <a href=\"https://www.happierlivesinstitute.org/report/assessing-the-comparability-linearity-and-neutrality-of-subjective-wellbeing-measurements-a-pilot-study/#6_Neutrality_questions\">Section 6.2</a>.&nbsp;</li></ul><p>In general, the findings from our pilot study should only be seen as being indicative of the general feasibility of this project. They do not provide definitive answers.</p><p>In the hopes of fielding an improved version of our survey with a much larger sample and a pre-registered analysis plan, <strong>we welcome feedback and suggestions on our current survey design</strong>.<strong>&nbsp;</strong></p><p>Here are some key questions that we hope to receive feedback on:</p><ol><li>Are there missing questions that could be included in this survey (or an additional survey) that would inform important topics in SWB research? Are there any questions or proposed analyses you find redundant?</li><li>Do you see any critical flaws in the analyses we propose? Are there additional analyses we should be considering?</li><li>Would these data and analyses actually reassure you about the comparability, linearity, and neutrality of subjective wellbeing data? If not, what sorts of data and analyses would reassure you?</li><li>What are some good places for us to look for funding for this research?</li></ol><p>Of course, any other feedback that goes beyond these questions is welcome, too. Feedback can be sent to <a href=\"mailto:casparkaiser@gmail.com\"><u>casparkaiser@gmail.com</u></a>&nbsp;or to <a href=\"mailto:samuel@happierlivesinstitute.org\"><u>samuel@happierlivesinstitute.org</u></a>.</p><p>The report proceeds as follows:&nbsp;</p><ul><li>In <a href=\"https://www.happierlivesinstitute.org/report/assessing-the-comparability-linearity-and-neutrality-of-subjective-wellbeing-measurements-a-pilot-study/#1_Linearity_comparability_and_neutrality_as_challenges_for_wellbeing_research\"><strong>Section 1</strong></a>, we describe the challenges for the use of self-reported subjective wellbeing data, focusing on the issues of comparability, linearity, and neutrality. We highlight the implications of these three assumptions for decision-making about effective interventions.&nbsp;</li><li>In <a href=\"https://www.happierlivesinstitute.org/report/assessing-the-comparability-linearity-and-neutrality-of-subjective-wellbeing-measurements-a-pilot-study/#2_General_outline_of_the_survey\"><strong>Section 2</strong></a>, we&nbsp;describe the general methodology of the survey.&nbsp;</li></ul><p><i>For the following sections, see the full report on </i><a href=\"https://www.happierlivesinstitute.org/report/assessing-the-comparability-linearity-and-neutrality-of-subjective-wellbeing-measurements-a-pilot-study/\"><i><u>our website</u></i></a><i>.</i>&nbsp;</p><ul><li>In <a href=\"https://www.happierlivesinstitute.org/report/assessing-the-comparability-linearity-and-neutrality-of-subjective-wellbeing-measurements-a-pilot-study/#3_Life_satisfaction_question\"><strong>Section 3</strong></a>, we discuss responses to the core life satisfaction question.&nbsp;</li><li>In <strong>Sections 4, 5, and 6</strong>, we describe how we will assess comparability (<a href=\"https://www.happierlivesinstitute.org/report/assessing-the-comparability-linearity-and-neutrality-of-subjective-wellbeing-measurements-a-pilot-study/#4_Comparability_questions\">Section 4</a>), linearity (<a href=\"https://www.happierlivesinstitute.org/report/assessing-the-comparability-linearity-and-neutrality-of-subjective-wellbeing-measurements-a-pilot-study/#5_Linearity_questions\">Section 5</a>), and neutrality (<a href=\"https://www.happierlivesinstitute.org/report/assessing-the-comparability-linearity-and-neutrality-of-subjective-wellbeing-measurements-a-pilot-study/#6_Neutrality_questions\">Section 6</a>). We also discuss our substantive and statistical assumptions (stated informally).&nbsp;</li><li>In <a href=\"https://www.happierlivesinstitute.org/report/assessing-the-comparability-linearity-and-neutrality-of-subjective-wellbeing-measurements-a-pilot-study/#7_Discussionof_feasibility\"><strong>Section 7</strong></a><strong>, </strong>we make some wider comments on the feasibility of fielding a scaled-up version of this survey.&nbsp;</li><li>In<strong> </strong><a href=\"https://www.happierlivesinstitute.org/report/assessing-the-comparability-linearity-and-neutrality-of-subjective-wellbeing-measurements-a-pilot-study/#8_Conclusion\"><strong>Section 8, </strong></a>we&nbsp;conclude.</li></ul><p>We hope that this work will eventually help us and the wider research community to understand whether, and to what extent, we can reliably use survey data as a cardinal and comparable measure of people\u2019s wellbeing.</p><h1><strong>1. Linearity, comparability, and neutrality as challenges for wellbeing research</strong></h1><p>If we want to know how people\u2019s lives are going, an obvious, if not widely used, method is to rely on individuals\u2019 self-reported assessment of their own wellbeing. Philanthropists and policymakers are increasingly using such data in their decision-making (<a href=\"https://s3.amazonaws.com/ghc-2018/GHC_Ch8.pdf\"><u>Durand, 2018</u></a>). Previous research showed that subjective wellbeing measures are <strong>reliable</strong> (i.e., they give similar results across multiple measurements, <a href=\"https://read.oecd-ilibrary.org/economics/oecd-guidelines-on-measuring-subjective-well-being_9789264191655-en%23page1\"><u>OECD, 2013</u></a>; <a href=\"https://www.researchgate.net/publication/348078354_The_Assessment_of_Subjective_Well-Being_A_Review_of_Common_Measures\"><u>Tov et al., 2021</u></a>) and <strong>valid</strong> (i.e., they succeed in capturing the underlying phenomenon they seek to measure, <a href=\"https://pubs.aeaweb.org/doi/pdfplus/10.1257%252F089533006776526030\"><u>Kahneman &amp; Krueger, 2006</u></a>). Still, there are doubts about the reasonableness of using subjective wellbeing reports to evaluate intervention effects on wellbeing and to make interpersonal welfare comparisons. These doubts are largely due to a lack of extensive research about how such reports are generated.</p><p>In this post, we engage with three important areas regarding self-reported subjective wellbeing:</p><ol><li>The <strong>comparability </strong>of reports between persons (is my 6/10 the same as your 6/10?).</li><li>The<strong>&nbsp;linearity</strong>&nbsp;of subjective wellbeing reports (is going from 4/10 to 5/10 the same amount of increase as going from 8/10 to 9/10?).</li><li>The position of the <strong>neutral point</strong> (the wellbeing level at which existence&nbsp;and non-existence are equivalent in value).</li></ol><p>Comparability and linearity are required to allow for the consistent estimation of relative wellbeing effects of interventions (see, for example, <a href=\"https://linkinghub.elsevier.com/retrieve/pii/S0167268121004765\"><u>Kaiser, 2022</u></a>). The neutral point is an additional requirement for estimating trade-offs between the quantity and quality of life (<a href=\"https://www.happierlivesinstitute.org/2022/08/16/wellby/\"><u>McGuire et al., 2022b</u></a>). Therefore, we believe these three issues to be particularly important. For an overall theoretical analysis and a review of the existing (limited) empirical literature, see Plant (<a href=\"https://www.happierlivesinstitute.org/report/the-comparability-of-subjective-scales/\"><u>2020</u></a>) who tentatively concludes we can assume SWB scales are comparable and linear.</p><p>We will be fielding a large-scale survey to evaluate comparability, linearity, and neutrality from multiple perspectives. As a first step in this process, we have fielded a small-scale pilot, from which we will report some initial results.</p><p>Whether SWB scales are interpreted in a linear and comparable manner is an open question. Should it turn out that SWB scales are not (approximately) linear and comparable, knowledge about the ways in which these assumptions fail will allow us to correct for these deviations in subsequent analyses. Below, we discuss some of the previous work on these issues.</p><h2><strong>1.1 Comparability</strong></h2><p>With \u201c<i>comparability</i>\u201d we here mean that identical SWB reports refer to identical levels of SWB, regardless of the person and time. One way to think about comparability is in terms of <i>common scale-use</i>. Suppose we give you a scale for you to rate your life satisfaction between 0 and 10. In order for you to render a judgement about which digit best represents your life satisfaction, you need to decide where the threshold for each digit is. You then have to match your subjective feeling of life satisfaction to a number on the scale. Because this is a complicated cognitive process, we may expect that people will differ in their scale-use (see Figure 1). If differences in scale-use were random, they would \u2018wash-out\u2019 in sufficiently large samples and would not bias subsequent analyses.</p><p><strong>Figure 1.</strong>&nbsp;An illustration of differences in scale-use for wellbeing reports</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/htiuqekri1ezznefisdb\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/iqtl3qgfk8ka9bt7hkdy 130w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/fwucklbnu8uyj2zlh4x5 260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/cjsyacaaphjwn5as3gpb 390w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/wv7drxgihnxyvinz8cfv 520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/a2qczes2b0xb21yl76kq 650w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/ioltjg2hkcca0fanrjiq 780w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/upaoy799gk8k9tk3ftmi 910w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/qwghuu81wuyqwmgyte4e 1040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/s5ctchexiruautdpvoep 1170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/ntwy20q9snnsiaqsuixg 1210w\"><br><i><strong>Note: </strong>For a given level of underlying wellbeing, Peter will use larger numbers than Anna. Equivalently, a given response (e.g., a 6/10) corresponds to a lower level of wellbeing for Peter than for Anna. Peter\u2019s and Anna\u2019s scale-use, therefore, differs.</i></p><p>However, if scale-use differed systematically, so that there were differences in scale-use between groups, then any reported differences in life satisfaction between groups would be confounded by differences in scale-use. In the literature, differences in scale-use are sometimes referred to as <i>scale shifts</i>, which may either occur between people or between points in time for a given person. Although most research assumes comparability, that assumption has been doubted.&nbsp;One reason for this are observed changes in response behaviour that are caused by factors that are unrelated to the content of the survey questions (<a href=\"https://www.jstor.org/stable/2677735\"><u>Bertrand &amp; Mullainathan, 2001</u></a>). For example, question order, differences in the phrasing of questions, and ordering of answer options can yield substantial differences in responses between randomised groups of respondents.</p><p>Other researchers have argued that incomparability may explain counterintuitive differences between groups with similar objective circumstances. For example, French respondents tend to report surprisingly low life satisfaction (<a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/obes.12039\"><u>Angelini et al., 2014</u></a>; <a href=\"https://www.aeaweb.org/articles?id%3D10.1257/0002828041301713\"><u>Kahneman et al., 2004</u></a>); women report higher life satisfaction than men despite having worse outcomes on many objective measures (<a href=\"https://doi.org/10.1016/j.jebo.2022.01.006\"><u>Montgomery, 2022</u></a>); and ageing populations report higher life satisfaction despite having poorer health and more loneliness.</p><p>Similarly, apparent inconsistencies in how respondents rate their current SWB, their improvement in SWB, and their memories of past SWB across time suggest that intrapersonal scale shifts occur (i.e., people change the scale they use over time; <a href=\"https://doi.org/10.1007/s10902-021-00460-8\"><u>Fabian, 2022</u></a>; <a href=\"https://doi.org/10.1016/j.jebo.2021.11.009\"><u>Kaiser, 2022</u></a>; <a href=\"https://www.iza.org/publications/dp/13166/feeling-good-or-feeling-better\"><u>Prati &amp; Senik, 2020</u></a>). Intrapersonal scale shifts may be an explanation of the well-known Easterlin paradox, which states that long-term country-wide increases in GDP do not improve wellbeing, despite the fact that income is robustly correlated with wellbeing at an individual level (<a href=\"https://www.pnas.org/doi/abs/10.1073/pnas.1011492107\"><u>Kahneman &amp; Deaton, 2010</u></a>; <a href=\"https://www.nature.com/articles/s41562-017-0277-0?mod%3Darticle_inline\"><u>Jebb et al., 2018</u></a>; <a href=\"https://www.nature.com/articles/s41562-021-01252-z\"><u>McGuire et al., 2022a</u></a>).</p><p>However, even if SWB reports are incomparable, that need not be the end of subjective wellbeing data. For example, researchers have used <i>vignettes</i>, short descriptions of (fictional) persons to which people should give common answers, to estimate the differences in scale-use between groups, and to subsequently correct for these differences (e.g., <a href=\"https://doi.org/10.1016/j.jebo.2022.01.006\"><u>Montgomery, 2022</u></a>; <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/obes.12039\"><u>Angelini et al., 2014</u></a>; <a href=\"https://onlinelibrary.wiley.com/doi/10.1111/j.1540-5907.2012.00597.x\"><u>Wand, 2012</u></a>; <a href=\"https://gking.harvard.edu/files/abs/vign-abs.shtml\"><u>King et al., 2004</u></a>). Of course, this test for comparability relies on the assumption that respondents have a common perception of the wellbeing of the persons described in the vignettes. This assumption may not always hold, and may depend on the design of vignettes. Similar methods have been suggested for intrapersonal scale shifts using data on memories (<a href=\"https://doi.org/10.1016/j.jebo.2021.11.009\"><u>Kaiser, 2022</u></a>). Generally, these studies find that survey responses are not perfectly comparable, but that biases arising from such non-comparability are typically too small to, for example, impact estimates of the <i>sign</i>&nbsp;of an effect. That said, the validity of current methods for assessing comparability has been questioned, and there is a lack of cross-method validation; namely, do the results from different methods, which have different underlying assumptions, converge?</p><h2><strong>1.2 Linearity</strong></h2><p>As we use the term, \u201c<i>linearity</i>\u201d refers to the assumption that the relationship between latent and reported SWB is linear, so that any step on a SWB scale indicates the same increase or decrease of latent SWB, regardless of where on the scale the person is. To illustrate, consider Figure 2 which shows a linear, convex, and concave relationship between latent and reported SWB.</p><p>Currently, most researchers treat SWB data as though the linearity assumption was met. As in the case of comparability, violations of this assumption would bias analyses of SWB data, and could even reverse conclusions \u2013 as shown in several pieces of recent work (<a href=\"https://doi.org/10.1086/701679\"><u>Bond &amp; Lang, 2019</u></a>; <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id%3D2630181\"><u>Schr\u00f6der &amp; Yitzhaki, 2017</u></a>).</p><p>In response to this, Kaiser and Vendrik (<a href=\"https://ideas.repec.org/p/osf/socarx/gzt7a.html\"><u>2021</u></a>) demonstrated that non-linear transformations of SWB scales would have to strongly deviate from linearity in order for such reversals to occur, and such strong deviations seem improbable. Few papers have sought to quantify how non-linear scale-use is. As one example, Oswald (<a href=\"https://doi.org/10.1016/j.econlet.2008.02.032\"><u>2008</u></a>) showed that the relationship between objectively measured and subjectively reported height is roughly linear. However, it remains unclear whether these initial results generalise sufficiently well to justify the linearity assumption for <i>wellbeing</i>&nbsp;data. Our survey design seeks to test this.</p><p><strong>Figure 2. </strong>Linear, convex, and concave relationships between latent and reported life satisfaction (as one example of SWB)</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/bz0modfygozd9rqq6pbz\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/ynuyou2w8frajpzhai5l 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/jtaj8pyo2c76zxwmzsym 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/yeh3ipwrii93kyz1utfl 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/qo7hegnoxjlbngovt0d1 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/zhehw3vj63kzcii6scte 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/zdmncyi8s7ququgfwcx0 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/cjcdgrezqgqvihohzqwz 1960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/olrl599jexdx1g8zt5de 2240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/ukko9fchrhc08dx0z8zk 2520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/zeyef8quqaqcag7fotxq 2763w\"></figure><p><i><strong>Note: </strong>The top of the figure shows a linear relationship between reported and latent wellbeing. In the middle, a convex relationship is shown. That is, the difference in latent wellbeing is larger between the higher response options than between the lower response options. The opposite concave pattern is shown in the bottom of the figure.</i></p><h2><strong>1.3 Neutrality</strong></h2><p>The <i>neutral point</i>&nbsp;refers to the level on a SWB scale at which existence has a neutral value, compared to non-existence for that person (assuming this state is perpetual and considering only the effects on that person). Above this point life is \u2018better than death\u2019; below it life is \u2018worse than death\u2019. This is conceptually distinct, but possibly closely related, to what we call the <i>zero point</i>: the level on a SWB scale at which that type of SWB is overall neither positive nor negative (e.g., someone is neither overall satisfied or dissatisfied). A natural&nbsp;thought is that the <i>zero point</i>&nbsp;and the <i>neutral point</i> coincide: if life is good(/bad) for us when it has positive(/negative) wellbeing, so a life has neutral value if it has zero wellbeing.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk8edtrxoh0q\"><sup><a href=\"#fnk8edtrxoh0q\">[2]</a></sup></span></p><p>Locating the neutral point is essential in some decision-making contexts, such as prioritising healthcare, where we must compare the relative value of improving the quality of life with the value of increasing the quantity of life.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4jwletscp2d\"><sup><a href=\"#fn4jwletscp2d\">[3]</a></sup></span>&nbsp;To date the location of the neutral point remains an open question. We think that potential answers can be informed by a combination of theoretical reasoning and empirical research.</p><p>To motivate the problem, consider two interventions <strong>A</strong>&nbsp;and <strong>B</strong>. For simplicity, assume that we simply seek to maximise total wellbeing. Under intervention <strong>A</strong>, 100 people\u2019s wellbeing is raised from a baseline score of 6/10 to 8/10 for a single year, but there is no change in each person\u2019s length of life. Under intervention <strong>B</strong>, there is no change in each person\u2019s baseline wellbeing level, but each person\u2019s length of life is increased by one year.</p><p>Define one \u2018Wellbeing Adjusted Life-Year\u2019 - a WELLBY - as a one point gain on the 0-10 wellbeing scale for one year. Intervention <strong>A</strong>&nbsp;yields 2*1*100=200 WELLBYs. We can do this without reference to the neutral point, because we know the counterfactual: without the intervention, they will live at 6/10.</p><p>For <strong>B</strong>, we need to assign a neutral point (i.e., a score equivalent to non-existence). Suppose we place the neutral point at 0/10. Under intervention <strong>B</strong>, we count a gain of 6*1*100=600 WELLBYs. Hence, intervention <strong>B</strong>&nbsp;seems more effective than intervention <strong>A</strong>. Now instead assume that the neutral point is located at 5/10. Under intervention <strong>B</strong>, we now merely gain (6-5)*100=100 WELLBYs. Under this alternative assumption, intervention <strong>A</strong>&nbsp;seems more effective. Hence, the location of the neutral point matters when attempting to decide between life-extending and life-improving interventions.</p><p>In previous works, the neutral point has been chosen in an <i>ad hoc</i>&nbsp;manner (also see<a href=\"https://www.happierlivesinstitute.org/report/the-elephant-in-the-bednet/\">&nbsp;<u>Plant et al., 2022,</u>&nbsp;</a>for discussion).<a href=\"http://cep.lse.ac.uk/pubs/download/occasional/op049.pdf\">&nbsp;</a>Layard et al. (<a href=\"http://cep.lse.ac.uk/pubs/download/occasional/op049.pdf\"><u>2020</u></a>), for example, set the neutral point at 0/10. However, in this case there would be no way of reporting a level of latent wellbeing below the neutral point. This violates the intuition that people can ever have overall bad lives (i.e., overall negative wellbeing), or lives where it would be rational for them to wish to die. One might instead think that the neutral point is at 5/10 (c.f.<a href=\"https://journals.sagepub.com/doi/abs/10.1177/1745691618765111\">&nbsp;<u>Diener et al. 2018</u></a>). This, however, would counterintuitively imply that a vast proportion of the world\u2019s population lives below the neutral point (as many report life satisfaction levels below 5/10;<a href=\"https://ourworldindata.org/happiness-and-life-satisfaction\">&nbsp;<u>Our World In Data, 2020</u></a>).</p><p>Hence, there is no obvious and uncontroversial <i>a priori</i>&nbsp;choice here. The limited research done so far indicates that people place the neutral point somewhere between 0 and 5 on SWB scales. A small (n = 75) survey in the UK found that, on average, respondents would prefer non-existence over a life satisfaction level of about 2/10 (Peasgood et al., <a href=\"https://docs.google.com/document/d/1m_rUOQ7rIftgFQ17K99Uq_7cv6vwio9L/edit%23heading%3Dh.gjdgxs\"><u>unpublished</u></a>, as referenced in <a href=\"https://global.oup.com/academic/product/a-handbook-for-wellbeing-policy-making-9780192896803\"><u>Krekel &amp; Frijters, 2021</u></a>). The IDinsight Beneficiary Preferences Survey (<a href=\"https://files.givewell.org/files/DWDA%25202009/IDinsight/IDinsight_Beneficiary_Preferences_Final_Report_November_2019.pdf\"><u>2019, p. 92</u></a>; n = 70), estimated the neutral point to be 0.56. The think-tank Rethink Priorities (<a href=\"https://docs.google.com/document/d/1jJ3_zcoHkF4u7eWB90L_mj50C0_Cnl77gChJRAkNJ8k/edit\"><u>unpublished</u></a>) ran pilot studies about the neutral point using a 0-100 scale (from the worst pain, suffering and unhappiness to the best pleasure, positive experience, and happiness<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0dh81c1pi2ed\"><sup><a href=\"#fn0dh81c1pi2ed\">[4]</a></sup></span>). When participants (n = 35) are asked at what level they prefer to be alive rather than dead, the mean answer is 24.9/100.</p><p>It is unclear what to make of these answers. An important first step is to get a better understanding of what respondents believe and why. To be clear, this is only a first step: decision-makers will not necessarily want to take such beliefs at face value if they seem mistaken or irrelevant. We consider limitations of our measures in Section 6.2.</p><p>As&nbsp;noted, a natural thought is that the <i>neutral point</i>&nbsp;and <i>zero point </i>will coincide.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1z0atvlmimu\"><sup><a href=\"#fn1z0atvlmimu\">[5]</a></sup></span>&nbsp;We test whether respondents put the neutral point and zero point for life satisfaction in the same place, and whether respondents interpret the zero point for life satisfaction as somewhere between 0 and 5. If respondents do both, that would provide an explanation of previous works\u2019 findings. However, there are several reasons why respondents might not believe that the neutral point and the zero point coincide.&nbsp;Some of these issues are discussed in Section 6.2.</p><p>With these preliminaries in place, we next outline the general features of our pilot survey.</p><h1><strong>2. General outline of the survey</strong></h1><p>The survey contains 50 questions, which can be found in this <a href=\"https://docs.google.com/document/d/1B_6_M_bbhhnyEKn0e-hIf1-l0VsZSbvCUX3SPfPN7uY/edit\"><u>external appendix with the questions</u></a>. We list the different types of questions used and the topics they address in Table 1 below.</p><p>We ran a pilot of the survey using Qualtrics for the implementation and Prolific Academic for the recruitment (chosen for its reasonably high data quality, see <a href=\"https://link.springer.com/article/10.3758/s13428-021-01694-3\"><u>Peer et al., 2022</u></a>). We used Prolific filters to recruit participants who lived in the UK, who spoke English as their first language, and had a Prolific Score between 95 and 100 (i.e., they were rarely rejected from studies). We used the balancing option from Prolific to recruit a similar number of men and women.</p><p>We recruited 128 participants. The median time to complete the survey was 9.82 minutes. In our sample, 64 participants report being females, 63 males, and 3 others. The mean age was 39.15 years old (median = 35.00, SD = 14.26). For additional summary statistics, <a href=\"https://docs.google.com/document/d/1COOXbfo5-l1M4aeV_IpB-NS-WhPtG-uKOfLTDKh58io/edit\"><u>see our external Appendix&nbsp;A</u></a>.</p><p><strong>Table 1</strong>: Different question types and how we use them</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/es8o2xmfhaccj0ejxgqj\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/ynighal4y21at0mo6b2f 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/hksbly6jczpo3oim4cei 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/ayd7fz1m36x7oko9gvi2 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/clwivdnezltjsezg7xaw 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/ngxwzqhkddrbjljlfbv5 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/kler1q8slp1sjmshgxxr 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/fett683u2gba14dgaqaj 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/swe5jkv6wpzk5ruuuzwt 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/rwzcmxrxhmt80fyembms 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/zrtrefbbjwb0iwrdjljb 1168w\"><br>&nbsp;</p><p><i>See </i><a href=\"https://www.happierlivesinstitute.org/report/assessing-the-comparability-linearity-and-neutrality-of-subjective-wellbeing-measurements-a-pilot-study/\"><i><u>our website</u></i></a><i> for the full report. We present the life satisfaction question we asked participants then we discuss our questions on comparability, linearity, and neutrality. Throughout, we report some tentative and preliminary results.</i></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/cpYR9TsG8BdtETo6u/rmyjrptlil4jllqlbufw\" alt=\"\"></p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjx4cys2ijpb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjx4cys2ijpb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;<strong>Author note:</strong>&nbsp;Conrad Samuelsson, Samuel Dupret, and Caspar Kaiser contributed to the conceptualization, methodology, investigation, analysis, data curation, and writing (original as well as review and editing) of the project. Michael Plant contributed to the conceptualization, supervision, and writing (review and editing) of the project.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk8edtrxoh0q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk8edtrxoh0q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;A standard philosophical explanation for what makes death bad for us (assuming it can be bad for us) is <i>deprivationism</i>, which says that death is bad because and to the extent it deprives us of the goods of life. Hence, death is bad for us if we would have had a good life and, conversely, death is good for us if we would have had a bad life. Here we take it that a good(/bad/neutral) life is one with overall positive(/negative/neutral) wellbeing. See, for example, Nagel (<a href=\"https://www.jstor.org/stable/2214297\"><u>1970</u></a>).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4jwletscp2d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4jwletscp2d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The need to, and difficulty of, assigning values to both various states of <i>life</i>&nbsp;and to <i>death</i>&nbsp;is also a familiar challenge for measures of quality- and disability-adjusted life years (QALYs and DALYs). For discussion, see, for example, Sassi (<a href=\"https://doi.org/10.1093/heapol/czl018\"><u>2006</u></a>).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0dh81c1pi2ed\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0dh81c1pi2ed\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The fact that the scale mixes three concepts into one seems problematic.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1z0atvlmimu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1z0atvlmimu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This is entailed by, for instance, a standard formulation of utilitarianism. In classical utilitarianism, the value of an outcome is the sum total of wellbeing in it, where wellbeing consists in happiness. On this view, <i>ceteris paribus</i>, extending an overall happy life is good, whereas extending an overall unhappy life is bad. \u2018Good\u2019 and \u2018bad\u2019 are understood either in terms of being good/bad for the person or good/bad \u2018for the world\u2019. We are not endorsing classical utilitarianism here, but merely point out that aligning the neutral point with the zero point on the <i>appropriate </i>wellbeing scale (whatever that happens to be) would be a textbook view in ethics.</p></div></li></ol>", "user": {"username": "Conrad S"}}, {"_id": "zabdCSArBLHSaQnrn", "title": "Legal Assistance for Victims of AI", "postedAt": "2023-03-17T11:42:58.282Z", "htmlBody": "<p>In the face of increasing competition, it seems unlikely that AI companies will ever take their foot off the gas. One avenue to slow AI development down is to make investment in AI less attractive. This could be done by increasing the legal risk associated with incorporating AI in products.</p>\n<p>My understanding of the law is limited, but the EU seems particularly friendly to this approach. The European Commission recently proposed the AI Liability Directive, which aims to make it easier to sue over AI products. In the US, companies are at the very least directly responsible for <a href=\"https://www.lawfareblog.com/section-230-wont-protect-chatgpt\">what their chatbots say</a>, and it seems like it's only a matter of time until a chatbot genuinely harms a user, either by gaslighting or by abusive behavior.</p>\n<p>A charity could provide legal assistance to victims of AI in seminal cases, similar to how EFF provides legal assistance for cases related to Internet freedom.</p>\n<p>Besides helping the affected person, this would hopefully:</p>\n<ol>\n<li>Signal to organizations that giving users access to AI is risky business</li>\n<li>Scare away new players in the market</li>\n<li>Scare away investors</li>\n<li>Give the AI company in question a bad rep, and sway the public opinion against AI companies in general</li>\n<li>Limit the ventures large organizations would be willing to jump into</li>\n<li>Spark policy discussions (e.g. about limiting minor access to chatbots, which would also limit profits)</li>\n</ol>\n<p>All of these things would make AI a worse investment, AI companies a less attractive place to work, etc. I'm not sure it'll make a big difference, but I don't think it's less likely to move the needle than academic work on AI safety.</p>\n", "user": {"username": "bob"}}, {"_id": "GXBvATw7Why7xRDeM", "title": "Why SoGive is publishing an independent evaluation of StrongMinds", "postedAt": "2023-03-17T22:46:35.480Z", "htmlBody": "<h2>Executive summary</h2><ul><li>We believe the EA community's confidence in the existing research on mental health charities hasn't been high enough to use it to make significant funding decisions.</li><li>Further research from another EA research agency, such as SoGive, may help add confidence and lead to more well-informed funding decisions.</li><li>In order to increase the amount of scrutiny on this topic, SoGive has started conducting research on mental health interventions, and we plan to publish a series of articles starting in the next week and extending out over the next few months.</li><li>The series will cover literature reviews of academic and EA literature on mental health and moral weights.</li><li>&nbsp;We will be doing in-depth reviews and quality assessments on work by the Happier Lives Institute pertaining to StrongMinds, the RCTs and academic sources from which StrongMinds draws its evidence, and StrongMinds' internally reported data.</li><li>We will provide a view on how impactful we judge StrongMinds to be.</li></ul><h2>What we will publish</h2><p>From March to July 2023, SoGive plans to publish a series of analyses pertaining to mental health. The content covered will include</p><ol><li>Methodological notes on using existing academic literature, which quantifies depression interventions in terms of standardised mean differences, numbers needed to treat, remission rates and relapse rates; as well as the \"standard deviation - years of depression averted\" framework used by Happier Lives Institute.</li><li>Broad, shallow reviews of academic and EA literature pertaining to the question of what the effect of psychotherapy is, as well as how this intersects with various factors such as number of sessions, demographics, and types of therapy.</li><li>We will focus specifically on how the effect decays after therapy, and publish a separate report on this.</li><li>Deep, narrow reviews of the RCTs and meta-analyses that are&nbsp; most closely pertaining to the StrongMind's context.</li><li>Moral weights frameworks, explained in a manner which will allow a user to map dry numbers such as effect sizes to more visceral subjective feelings, so as to better apply their moral intuition to funding decisions.</li><li>Cost-effective analyses which combine academic data and direct evidence from StrongMinds to arrive at our best estimate at what a donation to StrongMinds does.</li></ol><p>We hope these will empower others to check our work, do their own analyses of the topic, and take the work further.</p><h2>How will this enable higher impact donations?</h2><p>In the EA Survey conducted by Rethink Priorities, 60% of EA community members&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/83tEL2sHDTiWR6nwo/ea-survey-2020-cause-prioritization\"><u>surveyed</u></a> were in favour of giving \"significant resources'' to mental health interventions, with 24% of those believing it should be a \"top priority\" or \"near top priority\" and 4% selecting it as their \"top cause\". Although other cause areas performed more favourably in the survey, this still appears to be a moderately high level of interest in mental health.</p><p>&nbsp; Some EA energy has now gone into this area - for example,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/cCbctWjS7eizKw5pp/why-charity-entrepreneurship-is-researching-mental-health\"><u>Charity Entrepreneurship</u></a> incubated&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/canopie\"><u>Canopie</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jEHcbrsumxditRhtG/updates-from-the-mental-health-funder-s-circle\"><u>Mental Health Funder's Circle</u></a>, and played a role in incubating&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/happier-lives-institute\"><u>Happier Lives Institute</u></a>. They additionally launched&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/isggu3woGwkpYzqwW/presenting-2022-incubated-charities-charity-entrepreneurship\"><u>Kaya Guides and Vida Plena</u></a> last year. We also had a talk from&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/2rNQEsd4KwYgMnpS2/dixon-chibanda-the-friendship-bench\"><u>Friendship Bench</u></a> at last year's EA Global.</p><p>Our analysis will focus on StrongMinds. We chose StrongMinds because we know the organisation well. SoGive\u2019s founder first had a conversation with StrongMinds in 2015 (thinking of his own donations) having seen a press article about them and having considered them a potentially high impact charity. Since then, several other EA orgs have been engaging with&nbsp;<a href=\"http://strongminds\"><u>StrongMinds</u></a>. Evaluations of StrongMinds specifically have now been published by both&nbsp;<a href=\"https://founderspledge.com/stories/mental-health-report-summary\"><u>Founders Pledge</u></a> and&nbsp;<a href=\"https://www.happierlivesinstitute.org/report/strongminds-cost-effectiveness-analysis/\"><u>Happier Lives Institute</u></a>, and StrongMinds has received a&nbsp;<a href=\"https://www.givingwhatwecan.org/charities/strongminds\"><u>recommendation by Giving What We Can</u></a>. There are also&nbsp;<a href=\"https://docs.google.com/document/d/1Q5IR1a3luU1RbMTrrED7mnXiM7inOxCkX6jJx-RyeYY/edit\"><u>GiveWell conversation notes with StrongMinds</u></a> (among 826 other interviews at time of writing, so this is a very weak signal) as well as some donations in a strictly personal capacity (with appropriate caveats about it being a riskier donation made by each donor) from staff members at&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/blog/cea-staff-donation-decisions-2016\"><u>Center for Effective Altruism</u></a> and&nbsp;<a href=\"https://blog.givewell.org/2016/12/09/staff-members-personal-donations-giving-season-2016/\"><u>GiveWell</u></a>.&nbsp;</p><p>Despite this high level of engagement, we're not sure how much EA money has flowed through to direct work in mental health related causes. We wouldn't be too surprised if it was &lt;$3M/year, which is a small fraction in the context of&nbsp;<a href=\"https://blog.givewell.org/2021/11/12/givewells-money-moved-in-2020/\"><u>$200m+ institutional grants by Givewell and OpenPhil to global health causes</u></a>. This resource allocation probably doesn't reflect the degree of interest that the EA community seems to have in the area - we think that instead reflects institutional funders\u2019 (entirely appropriate) sceptical priors and level of caution, and the maintenance of a high bar for evidence in the EA global health space.&nbsp;</p><p>We think that the&nbsp; effective altruism community is hesitating from entering mental health in a bigger way because we do not have widespread community knowledge that enables us to evaluate a mental health intervention to a similar level of methodological rigour as other global health interventions. Consider for example that GiveWell's&nbsp;<a href=\"https://www.givewell.org/charities/amf\"><u>evaluation of the Against Malaria Foundation</u></a> includes not only academic data, but a breakdown of&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1OnKTKE8x2KgJJqJBrZdTVBnX8kkobhR8FCw6RlBN_xM/edit#gid=1364064522\"><u>cost effectiveness by country, with an understanding of other philanthropic actors in the space</u></a>, while AMF themselves document&nbsp;<a href=\"https://www.againstmalaria.com/Nets.aspx\"><u>every single net distribution</u></a>. It's common to see estimates to the effect that some intervention is &gt;100x or even &gt;1000x better than an established global health intervention, but the difference in the level of evidence for one end of the comparison makes it difficult to practise the&nbsp;<a href=\"https://www.charityentrepreneurship.com/post/equal-application-of-rigor-for-ea-interventions\"><u>equal application of rigor</u></a>. Given that&nbsp;<a href=\"https://scholars.duke.edu/display/pub798952\"><u>cost effectiveness estimates tend to regress on deeper analysis</u></a>, it's often justified to<a href=\"https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\"><u> favor interventions with more evidence even given a lower headline estimate</u></a>. In short, we think that what's missing from this space is confidence in the analysis.</p><p>Our aim is to provide an analysis of a mental health organisation which can help the EA community make well-justified decisions about mental health interventions&nbsp; alongside more traditional global health intervention. Importantly, we do&nbsp;<i>not</i> aim to<i>&nbsp;</i>advocate for StrongMinds specifically, nor mental health and subjective well being measures more generally - we only aim to empower the community with tools and analysis to consider mental health interventions on an even footing alongside more traditional global health interventions. Our theory of change is that there is a potentially large potential funding pool available in the community, which may face bottlenecks due to uncertainty about mental health interventions, and that reducing this uncertainty could increase donor efficiency in grant allocations.&nbsp;</p><p>We think the groundwork made by Happier Lives Institute, especially the&nbsp;<a href=\"https://www.happierlivesinstitute.org/report/strongminds-cost-effectiveness-analysis/\"><u>StrongMinds Cost Effectiveness Report</u></a> has been a massive step forward in regards to providing in-depth analyses of mental health interventions. When we reviewed HLI\u2019s work, we found their analysis to be incredibly useful and of high quality. We modelled much of our own analysis off of this research. Our ultimate conclusions may differ somewhat from the conclusions of HLI, as our meta-analysis will be independent, and weigh some sources of evidence differently. However, we agreed with and borrowed for our own analysis their core methodology of determining an initial effect size, assuming exponential decay of that effect size, and then integrating over time to find the area under that curve to derive the endline morally important&nbsp; metric.</p><p>It would be fair to ask: given that HLI has done a pretty powerful analysis already, why is a second analysis necessary? We believe that further analysis by SoGive can add further value, even aside from that we may reach different conclusions and that an independent review provides comfort, for three reasons:</p><p>First, the Happier Lives Institute advocates for evaluating everything in terms of subjective well being. While HLI commits&nbsp;<i>only</i> to the use of well being measures, and does not commit to psychotherapy in particular, it's understandable for a donor to wonder whether the recommendation of psychotherapy interventions was a foregone conclusion. In response to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ffmbLCzJctLac3rDu/strongminds-should-not-be-a-top-rated-charity-yet\"><u>recent criticisms</u></a>, Joel from HLI&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/HqEmL7XAuuD5Pc4eg/evaluating-strongminds-how-strong-is-the-evidence\"><u>commented</u></a> that:</p><blockquote><p><i>The implication, which others</i><a href=\"https://forum.effectivealtruism.org/posts/ffmbLCzJctLac3rDu/strongminds-should-not-be-a-top-rated-charity-yet?commentId=xicismN8aABPbs2h9\"><i><u> endorsed in the comments</u></i></a><i>, seems to be that HLI\u2019s analysis is biased because of a perceived relationship with StrongMinds or an entrenched commitment to mental health as a cause area which compromises the integrity of our research \u2026 I think the concern is that we\u2019ve already decided what we think is true, and we aim to prove it.</i></p></blockquote><p>&nbsp;We do not ourselves believe HLI's analysis to have been strongly influenced by a desire to inflate psychotherapy, but we think it would understandably reassure everyone to see that analysts who are not philosophically committed to subjective well being metrics have checked and independently verified the claims. Unlike HLI, we have a history of recommending a diversity of non-mental health organisations in global health.</p><p>Through a combination of checking and repackaging HLI analysis, as well as conducting our own independent analysis,&nbsp;<strong>we aim to make the impact of psychotherapy comparable side by side to non-mental health related outcomes, using moral weights frameworks that are intuitive to adjust for someone who doesn't necessarily want to use SWB measures for all moral judgements.&nbsp;</strong>You'll be able to input your own moral weights for how a given subjective well being increased compares with income increased or lives saved.&nbsp;<br>Second: We have some concerns that existing analysis by HLI suffers the problem of being fundamentally sound, but difficult to understand and audit. For instance, in his<a href=\"https://forum.effectivealtruism.org/posts/ffmbLCzJctLac3rDu/strongminds-should-not-be-a-top-rated-charity-yet\">&nbsp;<u>critique of StrongMinds</u></a>, Simon_M commented:&nbsp;</p><blockquote><p><i>I\u2019m going to leave aside discussing HLI here. Whilst I think they have some of the deepest analysis of StrongMinds, I am still confused by some of their methodology, it\u2019s not clear to me what their relationship to StrongMinds is.</i></p></blockquote><p>While there has been some&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/gr4epkwe5WoYJXF32/why-i-don-t-agree-with-hli-s-estimate-of-household#comments\"><u>critical engagement with HLI\u2019s analysis</u></a>, we're worried that there may be a gap where only a few full time analysts are finding the work accessible, while grantmakers with less time for analysis are left to wonder whether to trust the conclusions. We also found it difficult to replicate a few of the numbers involved in the work, as not all calculations were explicitly explained, so we couldn't double check all findings and rule out the possibility of errors and inconsistencies (though these are unlikely to dramatically change the headline conclusion).&nbsp; <strong>We aim to stress legibility, explaining every step of the analysis, to create a report that the \"average\" EA can more easily understand and audit.&nbsp;</strong></p><p>Third, SoGive's analysis will take a slightly different approach. Where HLI's report on StrongMinds is a higher level academic meta-analysis,&nbsp;<strong>we plan to additionally do some in-depth exploration picking apart the individual papers</strong> which are the most relevant to StrongMinds, laying bare any methodological flaws which will influence our inferences about generalizability of the findings.<strong> We're also hoping to present more in depth analysis from StrongMind's internal M&amp;E</strong>, although this is pending agreements from StrongMinds about how much work they can put in and what the privacy factors are regarding providing their data to us.</p><p>It is our hope that in providing this research, we will provide clarity about mental health interventions, how effective they are, how to think about moral weights surrounding them, and how to efficiently allocate resources when comparing them with other types of global health intervention.</p><p>About SoGive: SoGive does EA research and supports major donors. If you are a major donor seeking support with your donations, we\u2019d be keen to work with you. Feel free to contact Sanjay on sanjay@sogive.org.</p><p><br>&nbsp;</p>", "user": {"username": "ishaan"}}, {"_id": "BdWwgXrpncgdE4u5M", "title": "The illusion of consensus about EA celebrities", "postedAt": "2023-03-17T21:16:24.001Z", "htmlBody": "<p>Epistemic status: speaking for myself and hoping it generalises</p><p>I don't like everyone that I'm supposed to like:</p><ul><li>I've long thought that [redacted] was focused on all the wrong framings of the issues they discuss,</li><li>[redacted] is on the wrong side of their disagreement with [redacted] and often seems to have kind of sloppy thinking about things like this,</li><li>[redacted] says many sensible things but a writing style that I find intensely irritating and I struggle to get through; [redacted] is similar, but not as sensible,</li><li>[redacted] is working on an important problem, but doing a kind of mediocre job of it, which might be crowding out better efforts.</li></ul><p>Why did I redact all those names? Well, my criticisms are often some mixture of:</p><ul><li>half-baked; I don't have time to evaluate everyone fairly and deeply, and don't need to in order to make choices about what to focus on,</li><li>based on justifications that are not very legible or easy to communicate,</li><li>not always totally central to their point or fatal to their work,</li><li>kind of upsetting or discouraging to hear,</li><li>often not that actionable.</li></ul><p>I want to highlight that criticisms like this will usually not surface, and while in individual instances this is sensible, in aggregate it may contribute to a misleading view of how we view our celebrities and leaders. We end up seeming more deferential and hero-worshipping than we really are. This is bad for two reasons:</p><ul><li>it harms our credibility in the eyes of outsiders (or insiders, even) who have negative views of those people,</li><li>it projects the wrong expectation to newcomers who trust us and want to learn or adopt our attitudes.</li></ul><h2>What to do about it?</h2><p>I think \"just criticise people more\" in isolation is not a good solution. People, even respected people in positions of leadership, often seem to already find posting on the Forum a stressful experience, and I think tipping that balance in the more brutal direction seems likely to cost more than it gains.</p><p>I think you could imagine major cultural changes around how people give and receive feedback that could make this better, mitigate catastrophising about negative feedback, and ensure people feel safe to risk making mistakes or exposing their oversights. But those seem to me like heavy, ambitious pieces of cultural engineering that require a lot of buy-in to get going, and even if successful may incur ongoing frictional costs. Here's smaller, simpler things that could help:</p><ul><li>Write a forum post about it (this one's taken, sorry),</li><li>Make disagreements more visible and more legible, especially among leaders or experts. I really enjoyed the debate between Will MacAskill and Toby Ord in the comments of <a href=\"https://forum.effectivealtruism.org/posts/XXLf6FmWujkxna3E6/\">Are we living at the most influential time in history?</a> \u00ad\u2013 you can't come away from that discussion thinking \"oh, whatever the smart, respected people in EA think must be right\", because either way at least one of them will disagree with you!<ul><li>There's a lot of disagreement on the Forum all the time, of course, but I have a (somewhat unfair) vibe of this as the famous people deposit their work into the forum and leave for higher pursuits, and then we in the peanut gallery argue over it.</li><li>I'd love it if there were (say) a document out there that Redwood Research and Anthropic both endorsed, that described how their agendas differ and what underlying disagreements lead to those differences.</li></ul></li><li>Make sure people incoming to the community, or at the periphery of the community, are inoculated against this bias, if you spot it. Point out that people usually have a mix of good and bad ideas. Have some go-to examples of respected people's blind spots or mistakes, at least as they appear to you. (Even if you never end up explaining them to anyone, it's probably good to have these for your own sake.)</li></ul><p>As is often the case, though, I feel more convinced of my description of the problem than my proposals to address it. Interested to hear others' thoughts.</p>", "user": {"username": "BenMillwood"}}, {"_id": "PgQdvoPRxZbw7Kqxu", "title": "Getting Better at Writing: Why and How", "postedAt": "2023-03-17T15:31:38.858Z", "htmlBody": "<p><em>This post is adapted from a memo I wrote a while back, for people at GovAI. It may, someday, turn out to be the first post in a series on skill-building.</em></p>\n<h1>Summary</h1>\n<p>If you're a researcher,<sup class=\"footnote-ref\"><a href=\"#fn-zXruC959SDiJz9GQQ-1\" id=\"fnref-zXruC959SDiJz9GQQ-1\">[1]</a></sup> then you should probably try to become very good at writing. Writing well helps you spread your ideas, think clearly, and be taken seriously. Employers also care a lot about writing skills.</p>\n<p>Improving your writing is doable: it\u2019s mostly a matter of learning guidelines and practicing. Since hardly anyone consciously works on their writing skills, you can become much better than average just by setting aside time for study and deliberate practice.</p>\n<h1>Why writing skills matter</h1>\n<p>Here are three reasons why writing skills matter:</p>\n<ol>\n<li>\n<p>The main point of writing is to get your ideas into other people\u2019s heads. <strong>Far more people will internalize your ideas if you write them up well.</strong> Good writing signals a piece is worth reading, reduces the effort needed to process it, guards against misunderstandings, and helps key ideas stick.</p>\n</li>\n<li>\n<p>Writing and thinking are intertwined. <strong>If you work to improve your writing on some topic, then your <em>thinking</em> on it will normally improve too.</strong> Writing concisely forces you to identify your most important points. Writing clearly forces you to be clear about what you believe. And structuring your piece in a logical way forces you to understand how your ideas relate to each other.</p>\n</li>\n<li>\n<p>People will judge you on your writing. <strong>If you want people to take you seriously, then you should try to write well.</strong> Good writing is a signal of clear thinking, conscientiousness, and genuine interest in producing useful work.</p>\n</li>\n</ol>\n<p><strong>For all these reasons, most organizations give a lot of weight to writing skills when they hire researchers.</strong> If you ask DC think tank staffers what they look for in candidates, they <a href=\"https://forum.effectivealtruism.org/posts/dZnLssXGoHDs9kSPu/working-at-a-dc-policy-think-tank-why-you-might-want-to-do#4_4_Succeeding_in_the_application_process\">apparently</a> mention \u201cwriting skills\u201d more than anything else. \"Writing skills\" was also the first item mentioned when I recently asked the same question to someone on a lab policy team. GovAI certainly pays attention to writing when we hire. Even if you just want to impress potential employers, then, you should care a great deal about your own writing.</p>\n<h1>How to get better at writing</h1>\n<p>If you want to get better at writing, here are four things you can do:</p>\n<ul>\n<li>\n<p><strong><em>Read up on guidelines:</em></strong> There are a lot of pieces on how good writing works. The footnote at the end of this sentence lists some short essays.<sup class=\"footnote-ref\"><a href=\"#fn-zXruC959SDiJz9GQQ-2\" id=\"fnref-zXruC959SDiJz9GQQ-2\">[2]</a></sup> The best book I know is <em><a href=\"https://www.amazon.co.uk/Style-Lessons-Clarity-Joseph-Williams/dp/0134080416/ref=sr_1_2?crid=29SRJHTNL5DN1&amp;keywords=style+clarity+and+grace&amp;qid=1678929199&amp;s=books&amp;sprefix=style+clarity+and+grac%2Cstripbooks%2C115&amp;sr=1-2\">Style: Lessons in Clarity and Grace</a></em>. It\u2019s an easy-to-read textbook that offers recipe-like guidance. I would recommend this book over anything else.<sup class=\"footnote-ref\"><a href=\"#fn-zXruC959SDiJz9GQQ-3\" id=\"fnref-zXruC959SDiJz9GQQ-3\">[3]</a></sup></p>\n</li>\n<li>\n<p><strong><em>Engage with model pieces:</em></strong> You can pick out a handful of well-written pieces and read them with a critical mindset. (See the next footnote for some suggestions.<sup class=\"footnote-ref\"><a href=\"#fn-zXruC959SDiJz9GQQ-4\" id=\"fnref-zXruC959SDiJz9GQQ-4\">[4]</a></sup>) You might ask: What exactly is good about the pieces? How do they work? Where do they obey or violate the guidelines recommended by others?</p>\n</li>\n<li>\n<p><strong><em>Get feedback:</em></strong> Flaws in your writing\u2014especially flaws that limit comprehension\u2014will normally be more evident to people who are coming in cold. Also, sometimes other people will simply be better than you at diagnosing and correcting certain flaws. Comments and suggest-edits can draw your attention to recurring issues in your writing and offer models for how you can correct them.</p>\n</li>\n<li>\n<p><strong><em>Do focused rewriting:</em></strong> The way you\u2019ll ultimately get better is by doing focused rewriting. Pick some imperfect pieces\u2014ideally, pieces you\u2019re actually working on\u2014and simply try to make them as good as possible.<sup class=\"footnote-ref\"><a href=\"#fn-zXruC959SDiJz9GQQ-5\" id=\"fnref-zXruC959SDiJz9GQQ-5\">[5]</a></sup> You can consciously draw on writing guidelines, models, and previous feedback to help you diagnose and correct their flaws. The more time you spend rewriting, the better the pieces will become. Crucially, you\u2019ll also start to internalize the techniques you use to improve them. This means that each new piece you write will be better from the start and need less rewriting to become excellent.</p>\n</li>\n</ul>\n<p>Becoming a very good writer is not all that different from becoming a very good cook. Simply cooking a lot won\u2019t make you very good; many people cook every day and don't rise past a low-ish plateau. But if you seek out guidelines, notice what high-skilled people do, receive honest feedback, and consciously work to diagnose and fix issues with the things you make, then you should expect to surpass the vast majority of people.</p>\n<p>You don\u2019t need to have a rare gift. You mainly need to set aside time to learn and practice.<sup class=\"footnote-ref\"><a href=\"#fn-zXruC959SDiJz9GQQ-6\" id=\"fnref-zXruC959SDiJz9GQQ-6\">[6]</a></sup></p>\n<h1>A case study</h1>\n<p>A colleague of mine, Anne, significantly improved her writing over a few months. She did this by reading up on guidelines\u2014in her case, a handful of essays and book summaries\u2014and then dedicating about thirty hours to deliberate practice. She practiced mainly by rewriting e-mails and google docs. During this period, she also paid special attention to well-written pieces she encountered. See her summary <a href=\"https://docs.google.com/document/d/1WloTECTLIuP2hSzkUKutD286U5hIOQjZszEGAE0H8HA/edit?usp=sharing\">here</a>.<sup class=\"footnote-ref\"><a href=\"#fn-zXruC959SDiJz9GQQ-7\" id=\"fnref-zXruC959SDiJz9GQQ-7\">[7]</a></sup></p>\n<h1>Caveat: Language models</h1>\n<p>As language models get better, writing skills will become less useful. It\u2019s already possible to use language models to improve poorly written pieces or turn rough notes into good prose. Next-generation models will be even better.</p>\n<p>However, I wouldn\u2019t <em>count</em> on writing skills becoming obsolete soon. Even if their expected value falls by 20% every year, I think they would still be among the most worthwhile skills a researcher can develop.<sup class=\"footnote-ref\"><a href=\"#fn-zXruC959SDiJz9GQQ-8\" id=\"fnref-zXruC959SDiJz9GQQ-8\">[8]</a></sup></p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-zXruC959SDiJz9GQQ-1\" class=\"footnote-item\"><p>Also probably if you\u2019re not a researcher. <a href=\"#fnref-zXruC959SDiJz9GQQ-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zXruC959SDiJz9GQQ-2\" class=\"footnote-item\"><p>Paul Graham\u2019s essays on writing (<a href=\"http://www.paulgraham.com/writing44.html\">here</a>, <a href=\"http://www.paulgraham.com/simply.html\">here</a>, <a href=\"http://www.paulgraham.com/useful.html\">here</a>, <a href=\"http://www.paulgraham.com/words.html\">here</a>) have useful thoughts on writing simply and developing your ideas through the process of writing. <a href=\"https://www.orwellfoundation.com/the-orwell-foundation/orwell/essays-and-other-works/politics-and-the-english-language/\">This classic essay</a> by George Orwell has useful thoughts on writing clearly and concretely. <a href=\"https://docs.google.com/document/d/1oy9-VN1gI14hRK5EwGWy7Zc3XzOtiUm3iOcynCklgIA/edit?usp=sharing\">This google doc</a>, <a href=\"https://forum.effectivealtruism.org/posts/dHHuEYdbMqBf2deyj/using-the-executive-summary-style-writing-that-respects-your#Acknowledgements___related_work\">this post</a> and <a href=\"https://yoast.com/inverted-pyramid/\">this post</a> have useful thoughts on writing for audiences who may only skim your work. <a href=\"https://slatestarcodex.com/2016/02/20/writing-advice/\">This post</a> by Scott Alexander has useful thoughts on keeping readers engaged. <a href=\"https://yoast.com/writing-blog-creating-clear-blog-post-structure/\">This post</a> has useful thoughts on choosing a structure for a short piece of writing. Finally, <a href=\"https://youtu.be/uyFsvp6uSIA\">this EAG talk</a> on writing well is good. <a href=\"#fnref-zXruC959SDiJz9GQQ-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zXruC959SDiJz9GQQ-3\" class=\"footnote-item\"><p>A nice supplement to this book is <em>The Elements of Style</em>, which covers less ground but is famously excellent and can be read in an hour. <a href=\"#fnref-zXruC959SDiJz9GQQ-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zXruC959SDiJz9GQQ-4\" class=\"footnote-item\"><p>If no model pieces immediately come to mind, then here are some recommendations: Paul Graham\u2019s blog post \u201c<a href=\"http://www.paulgraham.com/top.html\">The Top Idea in Your Mind</a>,\u201d Chad Jones\u2019s paper \u201c<a href=\"https://www.nber.org/system/files/working_papers/w21142/w21142.pdf\">The Facts of Economic Growth</a>,\u201d the classic political science book <em><a href=\"https://www.amazon.co.uk/Essence-Decision-Explaining-Missile-Alternative/dp/0321013492\">The Essence of Decision</a></em>, and <a href=\"https://www.federalreserve.gov/newsevents/speech/powell20230110a.htm\">this speech</a> by the chair of the Federal Reserve. <a href=\"#fnref-zXruC959SDiJz9GQQ-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zXruC959SDiJz9GQQ-5\" class=\"footnote-item\"><p>Some specific dimensions along which you can try to improve a piece: clarity, concision, flow, tone, emphasis, grammar, skim-friendliness, and structure. <a href=\"#fnref-zXruC959SDiJz9GQQ-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zXruC959SDiJz9GQQ-6\" class=\"footnote-item\"><p>Of course, differences in natural writing talent are real. These differences lead some people to have significicantly steeper learning curves than others. For example: If two people start at the same baseline and then both work on their writing for twenty hours, they may end up in fairly different places. Nonetheless, I do think that nearly everyone reading this post could become much better and that most people could become very good. Writing well\u2014meaning, here, writing <em>usefully and professionally</em> rather than writing beautifully or hilariously or anything like that\u2014mostly comes down to applying a common set of standards and procedures. It's a learnable craft. <a href=\"#fnref-zXruC959SDiJz9GQQ-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zXruC959SDiJz9GQQ-7\" class=\"footnote-item\"><p>A key way I improved my own writing was by doing news, comedy, and fiction editing in high school and college. Editing hundreds of comedy pieces, when I ran a humor magazine, was probably my most useful experience. I also used to write short stories and would edit them over and over. I read a lot of literary fiction and half-consciously tried to emulate writers with great prose styles, like Kurt Vonnegut. I think this all amounted to a kind of training regime, which transferred reasonably well to academic and professional writing.\nIf you're looking for a path to retrace, though, then Anne's would probably be a bit more efficient than mine. <a href=\"#fnref-zXruC959SDiJz9GQQ-7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zXruC959SDiJz9GQQ-8\" class=\"footnote-item\"><p>We should expect some writing skills to lose their value more quickly than others. For example, the value of knowing how to avoid grammatical mistakes is already fairly low (although still non-zero).</p>\n<p>For some stretch of time, as language models keep getting better, the people who produce the best writing will probably treat these models as \u201cco-authors\u201d that do most of the work \u2013 but that still have weaknesses these people can help compensate for. Of course, though, language models will eventually stop needing people. <a href=\"#fnref-zXruC959SDiJz9GQQ-8\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "bmg"}}, {"_id": "XQdvHbFighhFKt3b3", "title": "Save the Date April 1st 2023 EAGatherTown: UnUnConference", "postedAt": "2023-03-20T16:06:06.046Z", "htmlBody": "<p>We're excited to&nbsp;<strong>officially&nbsp;</strong>announce the very first EA UnUnConference!&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1N_iQxj0jrCO-60McIsPgsRyLyUMMtLo0FHiBsQRV7Uk/edit?usp=sharing\"><u>APPLY HERE</u></a>.&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/54R2Masg3C9g2GxHq/announcing-naming-what-we-can-1\"><u>Naming What We Can</u></a>, the most impactful post ever published on April 1st, have already volunteered to host a Q&amp;A. We\u2019re calling in the producers of the TV hit&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BAYMjQtRcffgSivq5/announcing-impact-island-a-new-ea-reality-tv-show\"><u>Impact Island</u></a>, and would like to invite Peter Barnett to launch his new book&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/NK5LFazxKjwpaypJd/announcing-what-the-future-owes-us\"><u>What The Future Owes Us</u></a>. The&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/27nodhMEkZrYq3daW/announcing-the-actual-longtermist-incubation-program\"><u>X-risk-Men Incubation Program</u></a> is running an enlightening talks session.&nbsp;</p><p><strong>Location:&nbsp;</strong><a href=\"https://app.gather.town/app/BVtkGMirb2m6Frzn/EAGT%20Event%20Hall\"><u>Mars in Gathertown</u></a></p><p><strong>Date:&nbsp;</strong>April 1st, 2023,&nbsp;<a href=\"https://www.google.com/search?q=how+long+is+a+day+on+mars&amp;rlz=1C1BNSD_enSG964SG964&amp;oq=how+long+is+a+day+on+mars&amp;aqs=chrome.0.0i512l5j0i22i30l5.1835j1j7&amp;sourceid=chrome&amp;ie=UTF-8\"><u>24 hours and 37 minutes</u></a> starting at 12:00pm UTC (or \u201clunch time\u201d for british people)</p><h3>The case for impact</h3><ul><li>Over the years, humanity realized that&nbsp;<a href=\"https://en.wikipedia.org/wiki/Unconference\"><u>Unconferences</u></a> are a great twist of traditional conferences, since the independence gives room for more unexpected benefits to happen.&nbsp;</li><li>For the reason, we\u2019re experimenting with the format of an UnUnconference. This means we\u2019ll actively try not to organize anything, therefore (in expectancy) achieving even more unexpected benefits.</li><li>We encourage you to critique our (relatively solid, in our opinion) theory of change in the comments!</li><li>We understand this is not the most ambitious we could be. Although we fall short of the dream of&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yv3u8ofomLobZpkxm/save-the-date-eagxmars\"><u>EAGxMars</u></a>, we believe this Ununconference is a proof-of-concept that will help validate the model of novel, experimental conferences and possibly redefine what impact means for EA events for years to come.&nbsp;</li><li>This team is well placed to unorganize this event because we have previously successfully not organized 10^10 possible events.&nbsp;</li></ul><h3>What to expect</h3><ul><li>All beings welcomed, that includes&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/gDnGrccLDwGdS8vFj/the-case-for-infant-outreach\"><u>infants</u></a>, face mice, gut microbiome, etc.</li><li>Expect to have the most impactful time&nbsp;</li><li>Make more impact than everyone on earth could ever do combined</li><li>Network with the best minds in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LSxNfH9KbettkeHHu/ultra-near-termism-literally-an-idea-whose-time-has-come\"><u>ultra-near-termist research</u></a></li><li>Never meet your connections again after the event</li><li>Certificates of&nbsp;<a href=\"https://www.ebay.com/itm/203892130182\"><u>\u00a320 worth of impact just for \u00a310</u></a>!</li><li>No success metrics</li><li>No theory of change</li><li>No food, no wine, no suffering&nbsp;<br>&nbsp;</li></ul><h3>Check out our official event poster!&nbsp;</h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/XQdvHbFighhFKt3b3/vusqog1l1triccop5vh8\"></p><p>Pixelated lightbulb that looks like mars as a logo for an unconference (DALL-E)</p><h3>Get involved</h3><ul><li><strong>Take a look at the&nbsp;</strong><a href=\"https://docs.google.com/spreadsheets/d/1N_iQxj0jrCO-60McIsPgsRyLyUMMtLo0FHiBsQRV7Uk/edit?usp=sharing\"><strong><u>confernce agenda</u></strong></a><strong> and add&nbsp; sessions to your calendar</strong></li><li>Comment on this post with content suggestions and anti-suggestions</li><li><a href=\"https://docs.google.com/spreadsheets/d/1N_iQxj0jrCO-60McIsPgsRyLyUMMtLo0FHiBsQRV7Uk/edit?usp=sharing\"><u>Sign up</u></a> for an enlightning talk</li><li><a href=\"https://docs.google.com/spreadsheets/d/1N_iQxj0jrCO-60McIsPgsRyLyUMMtLo0FHiBsQRV7Uk/edit?usp=sharing\"><u>Unvolunteer</u></a> for the event</li><li><a href=\"https://docs.google.com/spreadsheets/d/1N_iQxj0jrCO-60McIsPgsRyLyUMMtLo0FHiBsQRV7Uk/edit?usp=sharing\"><u>UnUnvolunteer</u></a> for the event&nbsp; (your goal will be to actively unorganize stuff)</li><li><a href=\"https://docs.google.com/spreadsheets/d/1N_iQxj0jrCO-60McIsPgsRyLyUMMtLo0FHiBsQRV7Uk/edit?usp=sharing\"><u>UnUnUnvolunteer</u></a> for the event&nbsp; (your goal will be to actively ununorganize stuff)</li><li>\u2026. And so on. We think at least 5 levels of volunteers will be necessary for this event to be a complete success, to minimize risk of not falling into the well known&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/J3gZxFqsCFmzNosNa/ea-risks-falling-into-a-meta-trap-but-we-can-avoid-it\"><u>meta trap</u></a>.&nbsp;</li></ul><p><br>&nbsp;</p>", "user": {"username": "vaidehi_agarwalla"}}, {"_id": "vQR9iiifwnJcunPSb", "title": "Forecasts on Moore v Harper from Samotsvety", "postedAt": "2023-03-20T04:03:09.648Z", "htmlBody": "<p>[edited to include full text]</p><h3><strong>Disclaimers</strong></h3><p>The probabilities listed are contingent on SCOTUS issuing a ruling on this case. An updated numerical forecast on that happening, particularly in light of the NC Supreme Court\u2019s decision to rehear <i>Harper v Hall</i>, may be forthcoming.</p><p>The author of this report, Greg Justice, is an <a href=\"https://samotsvety.org/track-record/\">excellent forecaster</a>, not a lawyer. This post should not be interpreted as legal advice. This writeup is still in progress, and the author is looking for a good venue to publish it in.</p><p>You can subscribe to these posts <a href=\"https://samotsvety.org/.subscribe/\">here</a>.</p><h3><strong>Introduction</strong></h3><p>The <i>Moore v. Harper</i> case before SCOTUS asks to what degree state courts can interfere with state legislatures in the drawing of congressional district maps. Versions of the legal theory they\u2019re being asked to rule on were invoked as part of the attempts to overthrow the 2020 election, leading to widespread media coverage of the case. The ruling here will have implications for myriad state-level efforts to curb partisan gerrymandering.</p><p>Below, we first discuss the Independent State Legislature theory and <i>Moore v. Harper</i>. We then offer a survey of how the justices have ruled in related cases, what some notable conservative sources have written, and what the justices said in oral arguments. Finally, we offer our own thoughts about some potential outcomes of this case and their consequences for the future.</p><h3><strong>Background</strong></h3><p><u>What is the independent state legislature theory?</u></p><p>Independent State Legislature theory or doctrine (ISL) generally holds that state legislatures have unique power to determine the rules around elections. There are a range of views that fall under the term ISL, ranging from the idea that state courts' freedom to interpret legislation is more limited than it is with other laws, to the idea that state courts and other state bodies lack any authority on issues of federal election law altogether. However, \u201c[t]hese possible corollaries of the doctrine are largely independent of each other, supported by somewhat different lines of reasoning and authority. Although these theories arise from the same constitutional principle, each may be assessed separately from the others; the doctrine need not be accepted or repudiated wholesale.\u201d<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:1\"><sup>1</sup></a></p><p>The doctrine is rooted in a narrow reading of Article I Section 4 Clause 1 (the Elections Clause) of the Constitution, which states, \u201cThe Times, Places and Manner of holding Elections for Senators and Representatives, shall be prescribed in each State by the Legislature thereof.\u201d<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:2\"><sup>2</sup></a> According to the Brennan Center, this interpretation is at odds with a more traditional reading:</p><blockquote><p>The dispute hinges on how to understand the word \u201clegislature.\u201d The long-running understanding is that it refers to each state\u2019s general lawmaking processes, including all the normal procedures and limitations. So if a state constitution subjects legislation to being blocked by a governor\u2019s veto or citizen referendum, election laws can be blocked via the same means. And state courts must ensure that laws for federal elections, like all laws, comply with their state constitutions.</p><p>Proponents of the independent state legislature theory reject this traditional reading, insisting that these clauses give state legislatures exclusive and near-absolute power to regulate federal elections. The result? When it comes to federal elections, legislators would be free to violate the state constitution and state courts couldn\u2019t stop them.</p><p>Extreme versions of the theory would block legislatures from delegating their authority to officials like governors, secretaries of state, or election commissioners, who currently play important roles in administering elections.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:3\"><sup>3</sup></a></p></blockquote><p>The doctrine, which governs the actions of state courts, is of particular importance to partisan gerrymandering given SCOTUS\u2019s prior ruling that partisan gerrymandering is beyond the reach of federal courts.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:4\"><sup>4</sup></a> Some extreme interpretations of ISL would deem state constitutional amendments imposing standards for redistricting, like those in Florida<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:5\"><sup>5</sup></a> and Ohio,<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:6\"><sup>6</sup></a> or amendments requiring redistricting to be done by an independent commission, as is done in states like Arizona<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:7\"><sup>7</sup></a> and Michigan,<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:8\"><sup>8</sup></a> as unenforceable by state courts. State legislatures in that scenario may be able to gerrymander with far more freedom than they currently have.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:9\"><sup>9</sup></a></p><p><u>What is </u><i><u>Moore v. Harper</u></i><u> about?</u></p><p>The sequence of key events leading to the current case is as follows:</p><ul><li>The NC Supreme Court ruled that the NC legislature\u2019s gerrymandered electoral maps violated the state constitution and ordered the legislature to submit remedial maps to the lower court meeting prescribed standards for fairness.</li><li>The legislature submitted a remedial map for the Congressional election, but the lower court rejected it, as it was drawn using banned partisan data and didn\u2019t meet the NC Supreme Court\u2019s standards for fairness.</li><li>Having rejected the legislature\u2019s remedial Congressional map, and needing to meet deadlines for upcoming primaries, the lower court instead adopted an interim map drawn by special masters for use in the 2022 Congressional election.</li><li>Timothy Moore, Speaker for the NC House of Representatives, sued, alleging the striking down of the legislature\u2019s map and replacement with the court\u2019s own violates the Elections Clause.</li></ul><p>While approximately one third of registered voters in North Carolina are unaffiliated with any party,<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:10\"><sup>10</sup></a> North Carolina voters have split their votes roughly 50/50 between Democratic and Republican presidential candidates in the past several presidential elections.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:11\"><sup>11</sup></a> However, after North Carolina gained an additional House seat in the 2020 redistricting cycle, the Republican-led North Carolina state legislature adopted a heavily gerrymandered Congressional district map that would likely have resulted in 9 or 10 of the state\u2019s 14 Congressional seats going to Republicans.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:12\"><sup>12</sup></a> That map, as well as two others for use in elections for the NC House and NC Senate, was set to be used in the 2022 midterm election. The North Carolina legislature previously passed a statute authorizing court review of its redistricting plans.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:13\"><sup>13</sup></a> Pursuant to that statute, a group of voters and non-profit organizations sued in a case called <i>Harper v. Hall</i>, arguing that the legislature\u2019s partisan gerrymander violated the state constitution.</p><p>The NC trial court (lower court) upheld the maps, but the NC Supreme Court overturned that decision 4-3, ruling that the maps violated four clauses of the NC state constitution.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:14\"><sup>14</sup></a> The NC Supreme Court then blocked use of the maps and ordered new maps to be submitted that would meet specific criteria. The NC General Assembly duly submitted new maps, and the lower court approved the new NC House and NC Senate maps. However, the court did not approve the Congressional district map and issued an interim map for the 2022 election instead.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:15\"><sup>15</sup></a></p><p>In response, Timothy Moore, the Speaker of the NC House of Representatives, requested that SCOTUS review the actions of the NC Supreme Court, including both its ruling that new maps be submitted to the lower court and its requirement that a court-drawn map be used in the 2022 Congressional election. He also asked that, in the meantime, SCOTUS stay the NC courts' orders, which would mean that the NC legislature\u2019s original gerrymandered maps would be used in the 2022 election.</p><p>SCOTUS agreed to hear the case but did not stay the lower courts' orders. The denial of a stay included two opinions, one from Kavanaugh concurring, and one from Alito joined by Thomas and Gorsuch dissenting.</p><p><u>What is the legal question posed in </u><i><u>Moore v. Harper</u></i><u>?</u></p><p>The exact question requested by Moore that SCOTUS agreed to hear is as follows:</p><blockquote><p>Whether a State\u2019s judicial branch may nullify the regulations governing the \u201cManner of holding Elections for Senators and Representatives \u2026 prescribed \u2026 by the Legislature thereof,\u201d U.S. CONST. art. I, \u00a7 4, cl. 1, and replace them with regulations of the state courts' own devising, based on vague state constitutional provisions purportedly vesting the state judiciary with power to prescribe whatever rules it deems appropriate to ensure a \u201cfair\u201d or \u201cfree\u201d election.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:16\"><sup>16</sup></a></p></blockquote><p><u>What have conservative justices said prior to this case?</u></p><p>Conservative justices, including some currently on the bench, have voiced their opinions on similar issues in previous rulings and dissents. Their line has been that state legislatures bear primary, but not necessarily sole, responsibility for election law.</p><p>This was voiced most clearly in the conservative dissent in the 2015 case <i>Arizona State Legislature v. Arizona Independent Redistricting Commission. </i>That case decided whether an Arizona ballot measure could take redistricting authority from the state legislature and give it to an independent commission. In a 5-4 decision the majority ruled yes, while Roberts, joined by Scalia, Thomas, and Alito dissented. From their dissent:</p><blockquote><p>In <i>Ohio ex rel. Davis</i> v. <i>Hildebrant</i>, 241 U. S. 565 (1916), the Ohio Legislature passed a congressional redistricting law. Under the Ohio Constitution, voters held a referendum on the law and rejected it. A supporter of the law sued on behalf of the State, contending that the referendum \u201cwas not and could not be a part of the legislative authority of the State and therefore could have no influence on \u2026 the law creating congressional districts\u201d under the Elections Clause. Id., at 567. The Court. rejected an argument that Ohio\u2019s use of the referendum violated a federal statute, and held that Congress had the power to pass that statute under the Elections Clause. Id., at 568-569. <i>Hildebrant </i>simply approved a State\u2019s decision to employ a referendum <i>in addition to</i> redistricting by the Legislature. See 241 U. S., at 569. The result of the decision was to send the Ohio Legislature back to the drawing board to do the redistricting.</p><p>In <i>Smiley</i>, the Minnesota Legislature passed a law adopting new congressional districts, and the Governor exercised his veto power under the State Constitution. The Court nevertheless went on to hold that the Elections Clause did not prevent a State from applying the usual rules of its legislative process\u2014including a gubernatorial veto\u2014to election regulations prescribed by the legislature. 285 U. S., at 373. As in <i>Hildebrant</i>, the legislature was not displaced, nor was it redefined; it just had to start on a new redistricting plan.<strong>There is a critical difference between allowing a State to </strong><i><strong>supplement </strong></i><strong>the legislature\u2019s role in the legislative process and permitting the State to </strong><i><strong>supplant </strong></i><strong>the legislature altogether.</strong> See <i>Salazar</i>, 541 U. S., at 1095 (Rehnquist, C. J., dissenting from denial of certiorari) (\u201cto be consistent with Article I, \u00a7 4, there must be some limit on the State\u2019s ability to define lawmaking by excluding the legislature itself\u201d). Nothing in <i>Hildebrant</i>, <i>Smiley</i>, or any other precedent supports the majority\u2019s conclusion that imposing some constraints on the legislature justifies deposing it entirely.</p><p><strong>The constitutional text, structure, history, and precedent establish a straightforward rule: Under the Elections Clause, \u201cthe Legislature\u201d is a representative body that, when it prescribes election regulations, may be required to do so within the ordinary lawmaking process, but may not be cut out of that process. Put simply, the state legislature need not be exclusive in congressional districting, but neither may it be excluded. </strong>[emphasis added]</p></blockquote><p>That statement is a rejection of the extreme forms of ISL. A similarly restrained take has been echoed by the other conservative judges, who take issue with the extent of state courts' power, but apparently not with the idea that they have a role to play.</p><p>Opinions on denial of application to vacate stay, <i>DNC v. Wisconsin State Legislature</i></p><ul><li>Kavanaugh: \u201c[U]nder the U. S. Constitution, the <strong>state courts</strong> <strong>do not have a blank check</strong> [emphasis added] to rewrite state election laws for federal elections. Article II expressly provides that the rules for Presidential elections are established by the States \"in such Manner as the <i>Legislature </i>thereof may direct.\u201d \u00a71, cl. 2 (emphasis added). The text of Article II means that \u201cthe clearly expressed intent of the legislature must prevail\u201d and that a state court may not depart from the state election code enacted by the legislature. <i>Bush </i>v. <i>Gore</i>, 531 U. S. 98, 120 (2000) (Rehnquist, C. J., concurring); see <i>Bush </i>v. <i>Palm Beach County Canvassing Bd</i>., 531 U. S. 70, 76-78 (2000) (<i>per curiam</i>); <i>McPherson </i>v. <i>Blacker</i>, 146 U. S. 1, 25 (1892).\u201c<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:17\"><sup>17</sup></a></li></ul><p>The dissents to the denial of a stay in <i>Moore v. Harper</i></p><ul><li>From Alito (dissenting), joined by Thomas, Gorsuch: \u201cif the language of the Elections Clause is taken seriously, <strong>there must be </strong><i><strong>some </strong></i><strong>limit on the authority of state courts to countermand actions taken by state legislatures</strong> when they are prescribing rules for the conduct of federal elections. I think it is likely that the applicants would succeed in showing that the North Carolina Supreme Court <strong>exceeded those limits</strong>.\u201d [emphasis added]</li></ul><p>None of these statements suggest that state courts lack jurisdiction over state election law cases, as more extreme versions of ISL would contend. They say that, just as stated in the <i>Arizona</i> dissent, that state courts do indeed have authority on election law cases, but that authority has limits.</p><p>Also notable is the majority opinion in <i>Rucho v. Common Cause</i>, written by Roberts and joined by Alito/Thomas/Gorsuch/Kavanaugh. The most notable section is the following:</p><blockquote><p>The conclusion that partisan gerrymandering claims are not justiciable neither condones excessive partisan gerrymandering nor condemns complaints about districting to echo into a void. Numerous States are actively addressing the issue through state constitutional amendments and legislation placing power to draw electoral districts in the hands of independent commissions, mandating particular districting criteria for their mapmakers, or prohibiting drawing district lines for partisan advantage. The Framers also gave Congress the power to do something about partisan gerrymandering in the Elections Clause. That avenue for reform established by the Framers, and used by Congress in the past, remains open.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:18\"><sup>18</sup></a></p></blockquote><p>The language used in that section seems to condone some restraints on legislatures coming from state constitutions and independent commissions. However, such measures aren\u2019t the question they were ruling on in that case, and they\u2019re not explicitly saying such measures are <i>always </i>legal. Alito, Thomas, and Kavanaugh have shown interest in limiting the power of state courts despite joining that opinion. However, at the very least, it\u2019s another rejection of the extreme forms of ISL.</p><p>While conservative justices have written against radical forms of ISL, they do see an issue with the NCSC\u2019s actions, and similar actions by other courts. As Alito (joined by Thomas and Gorsuch) argued in an opinion related to <i>Republican Party of Pennsylvania v. Boockvar</i>, \u201cThe provisions of the Federal Constitution conferring on state legislatures, not state courts, the authority to make rules governing federal elections would be meaningless if a state court could override the rules adopted by the state legislature simply by claiming that a state constitutional provision gave the courts authority to make whatever rules it thought appropriate for the conduct of a fair election.\u201d<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:19\"><sup>19</sup></a> A conservative ruling would likely seek to address that issue, defining how far the authority of state courts can reach in federal election law cases.</p><p>Lastly, there is some notable precedent, or lack thereof, with regard to rulings seeking to overturn the results of elections. There were lawsuits to overturn the 2020 election and SCOTUS refused to hear them. From the Economist:</p><blockquote><p><strong>Late last year, when Donald Trump and his allies were litigating his electoral loss, the Supreme Court shot down two last-ditch lawsuits with deep procedural flaws.</strong> On December 8th a one-sentence order put a halt to a Pennsylvania state representative\u2019s bid to stop his state from certifying Joe Biden\u2019s win. And three days later, another terse order snuffed out Texas\u2019s attempt to suspend Mr Biden\u2019s victories in Georgia, Michigan, Pennsylvania and Wisconsin. For Stephen Vladeck, a law professor at the University of Texas and Supreme Court litigator, <strong>some of the court\u2019s most important decisions of the term \u201cmay have been its decisions not to get involved\u201d.</strong>[^20] [emphasis added]</p></blockquote><p>The orders for the Pennsylvania<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:20\"><sup>20</sup></a> and Texas<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:21\"><sup>21</sup></a> are linked in the footnotes, and they\u2019re as terse as the Economist describes them. The Pennsylvania order is literally one sentence from all of the justices, and the Texas order had a dissent only from Alito and Thomas, and it was only on procedural issues; neither of them would have granted the request for relief either.</p><p>Some of the worries around this case are that it\u2019s part of a plot to overthrow a future election. However, it\u2019s worth noting that SCOTUS has already been asked to do that, and they\u2019ve refused the cases with almost no written dissent.</p><p><u>What have conservative groups written?</u></p><p>A good place to start is the petitioner, Timothy Moore. What he\u2019s not asking for, at least not openly, is a more expansive version of ISL:</p><blockquote><p><strong>Moore said he did not agree with broader versions of the argument that could be used to question the certification of election results. </strong>He also said he believed that the governor had the power to veto elections legislation, a procedure cast into doubt by at least one interpretation of the independent state legislature theory. \u201cI would not go that far,\u201d he said.</p></blockquote><p>Notably, this case has drawn wide opposition, including from many conservative sources.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:22\"><sup>22</sup></a> The case includes amicus briefs from the Republican National Committee (RNC) and the National Republican Redistricting Trust (NRRT), among others.</p><p>The RNC made a similar argument to the one petitioners made in oral arguments. The gist of their brief is emphasizing that state legislatures are bound by Congress and federal law, which they use to rebut the idea that overturning the NCSC would grant unchecked power to state legislatures. They also outline a role for state courts enforcing state statutes and constitutions, but not substantively deviating from those laws or creating their own:</p><blockquote><p>There remains a limited role for state courts, one far more circumscribed than the antics conducted by the North Carolina Supreme Court. State courts may, for example, ensure that their state legislature\u2019s regulations follow federal law. That said, they never may claim a \u201cblank check to rewrite state election laws for federal elections.\u201d Democratic Nat'l Comm., 141 S. Ct. at 34 n.1 (2020) (Kavanaugh, J., concurring in denial of application to vacate stay). The plain text of the Elections Clause means \u201c\u2018the clearly expressed intent of the legislature must prevail\u2019 and that a state court may not depart from the state election code enacted by the legislature.\u201d Id. (citing Gore, 531 U. S. at 120 (Rehnquist, C. J., concurring); see Palm Beach Cnty. Canvassing Bd., 531 U.S. at 76-78 (per curiam); McPherson, 146 U.S. at 25).</p><p><strong>It remains true that \u201cstate statutes and state constitutions can provide standards and guidance for state courts to apply.\u201d Rucho, 139 S. Ct. at 2507. It is also true, however, that state courts are limited to enforcing the express policy prescriptions of the legislature and procedural limitations\u2014such as the gubernatorial veto or initiative process\u2014on the legislature\u2019s lawmaking powers</strong>.; See Smiley v. Holm, 285 U.S. 355, 369 (1932); Gore, 531 U.S. at 120 (Rehnquist, C.J., concurring); Ariz. State Legislature, 576 U.S. at 824.8 None of this Court\u2019s precedents support the \u201cconclusion that imposing some constraints on the legislature justifies deposing it entirely,\u201d in favor of giving a state\u2019s lawmaking power to its judiciary. Ariz. State Legislature, 576 U.S. at 841 (Roberts, C.J., dissenting); see also Republican Party v. Boockvar, 141 S. Ct. 1, 2 (2020) (statement of Alito, J., joined by Thomas and Gorsuch, JJ.) (when a state court replaces the policy prescriptions of the legislature with its own, \u201cthere is a strong likelihood that the State Supreme Court decision violates the Federal Constitution\u201d).<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:23\"><sup>23</sup></a></p></blockquote><p>The NRRT\u2019s brief on the other hand focuses on outlining their stance on when state courts can and cannot check the actions of the legislature. The first is through \u201cexpress authorizations'\u2018 where a clear standard is outlined in state law. As they note, \u201d<strong>Such express authorizations as exist in New York and as the Rucho Court identified in Missouri, Iowa, and Delaware law codify clear anti-partisan gerrymandering standards that state courts are empowered to enforce.</strong> But it does not logically follow from the Court\u2019s recognition that \u201c[p]rovisions in state statutes and state constitutions can provide standards and guidance for state courts to apply\u201d in redistricting cases that all state court attempts to review a state legislature\u2019s redistricting authority are automatically constitutionally permissible. Rucho, 139 S. Ct., at 2507.\u201c (emphasis added) <a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:24\"><sup>24</sup></a></p><p>So neither conservative group is asking for state courts to be cut out of the lawmaking process. Instead, they\u2019re asking for specific standards around when it\u2019s acceptable for a state court to intervene and/or what remedies they\u2019re allowed to issue.</p><p><u>What was said in oral arguments?</u></p><p>Here we\u2019ll break down each judge\u2019s questions from oral arguments. Firstly though, some background on what different cases refer to, very broadly:</p><ul><li><i>Smiley v. Holm (1932): </i>SCOTUS ruled that state governors can veto election laws.</li><li><i>Davis v. Hildebrant (1916)</i>: _SCOTUS ruled that state referenda can restrict legislatures in making federal election laws.</li><li>Rehnquist\u2019s concurrence in <i>Bush v. Gore (2000)</i>: Rehnquist argued that SCOTUS can review state court interpretations of state statutes regarding federal election laws under a deferential standard, although he didn\u2019t specify a specific standard.</li><li><i>Bush v. Palm Beach County Canvassing Board (2000): </i>full meaning is contested, but state court interpretation of state statutes and constitutions regarding federal election laws at least presents a federal question that SCOTUS can review.</li><li><i>Rucho v. Common Cause (2019): </i>SCOTUS ruled that federal courts can\u2019t review partisan gerrymandering cases, because impermissible partisan gerrymandering lacks a clear meaning for judges to apply. However, the ruling did seem to endorse some role for state constitutions and laws restricting legislatures.</li><li><i>Leser v. Garnett (1922): </i>SCOTUS ruled that some actions by state legislatures can be immune to state restrictions when the function is assigned by the federal government. In that case, the Virginia constitution prohibited women\u2019s suffrage, but the Virginia legislature ratified the Nineteenth Amendment granting women suffrage. SCOTUS ruled that that ban couldn\u2019t block the legislature\u2019s actions because ratification is a function granted by the federal Constitution that \u201ctranscends any limitations sought to be imposed by the people of a state.\u201d<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:25\"><sup>25</sup></a> Petitioners here argue that making state election laws should be seen in a similar way, with authority coming from the Elections Clause.</li></ul><p><strong>Roberts</strong></p><ul><li>Petitioners<ul><li><i>Smiley </i>is a big exception to their argument that legislatures are independent from checks by other state entities, and the gubernatorial vetoes <i>Smiley </i>endorsed aren\u2019t cleanly procedural vs. substantive.</li><li>What\u2019s the petitioners' back-up argument? Answer: state courts can only apply judicially discoverable and manageable standards for election laws (similar to <i>Rucho</i>).</li></ul></li><li>Respondents 1 (Katyal)<ul><li><i>Rucho</i>, which Roberts authored, endorsed constitutions providing \u201cstandards and guidance\u201d for legislatures. In the context of that ruling, the NC constitution\u2019s vague clauses like \u201call elections shall be free\u201d may not constitute \u201cstandards and guidance\u201d<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:26\"><sup>26</sup></a> because of how vague they are, and hence would not be endorsed by that ruling.</li><li>Respondents citing Rehnquist\u2019s concurrence in <i>Bush v. Gore </i>saying SCOTUS can intervene feels like respondents ending in a similar place as petitioners.</li></ul></li><li>Respondents 2 (Verrilli)<ul><li>Roberts seems to draw a distinction in terms of permissibility between vague constitutional clauses and the specific choices that judges make interpreting them.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:27\"><sup>27</sup></a> Constitutional clauses can be vague, which may not be inherently problematic. But modern judges choosing an efficiency gap of x% or appointing special masters to draw maps are very specific actions, much more specific than what judges did near the founding when such vague clauses were endorsed. In other words, jumping from a vague clause to a specific numerical efficiency gap requirement is a big leap, and modern applications of vague constitutional clauses like that may run afoul of the Elections clause, even if the clauses themselves don\u2019t.</li></ul></li><li>Solicitor General<ul><li>(in response to Justice Jackson) This case exists because of tension between the federal Constitution empowering the legislature and state courts constraining them. He notes SCOTUS regularly addresses tensions between state and federal power.</li><li>He asks if the SG accepts that there\u2019s a role for SCOTUS in determining if what a state is doing is ordinary or outrageous? SG response: yes, but highly deferential standard to avoid constant litigation and be consistent with broader doctrine.</li></ul></li></ul><p><strong>Thomas</strong></p><ul><li>Petitioners<ul><li>He asked a prefatory question about why SCOTUS is involved in a state law question, answer being that the Elections clause is a federal law empowering legislatures.</li></ul></li><li>Respondents 1 (Katyal)<ul><li>Thomas seems to disagree with the idea that SCOTUS doesn\u2019t normally second guess state court interpretations of their constitutions citing <i>Baker v. Carr</i><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:28\"><sup>28</sup></a>, and questions where the NC Supreme Court\u2019s authority the regulate federal elections is from. He also seems to share a concern with Gorsuch that the NCSC ruling would be different if the legislature favored minority voters instead of Republicans, implying respondents' argument is politically motivated.</li></ul></li><li>Respondents 2 (Verrilli)<ul><li>He asked how respondents would articulate the sky-high standard being discussed for when SCOTUS can review cases.</li></ul></li><li>Solicitor General<ul><li>He asked if the SG would agree with the deferential standard being discussed, and noted the irony of her being on the opposite side of an issue about leaving power to the states instead of the federal government.</li></ul></li></ul><p><strong>Alito</strong></p><ul><li>Petitioners<ul><li>He asks whether it\u2019s inevitable that state courts and election officials will have to interpret different statutes and make decisions about all the little details about running elections. He seems skeptical of an expansive ruling that would interfere with necessary decisions about the nitty gritty details of election administration.</li></ul></li><li>Respondents 1 (Katyal)<ul><li>His questions taken together frame the NCSC\u2019s ruling as at least approaching judicial legislating. He first aimed to find what line respondents would draw for impermissible conduct, and implied that such lines would allow for some very questionable outcomes. The hypotheticals he asked about were extreme, such as courts being appointed to legislative roles or striking down laws simply because they\u2019re \u201cunfair.\u201d He also asked if it\u2019s better to move redistricting controversy from the legislature to elected courts.</li><li>He also showed clear interest in outlining a role for SCOTUS. He first asked if endorsing the Rehnquist concurrence\u2019s principle would be as disruptive as the more extreme options. He also mentioned several other contexts (cases involving Contract Clause, Takings Clause, etc.) where SCOTUS interprets state law, questioned what checks exist against appointed supreme courts making extreme decisions, and questioned if the sky-high standard being requested by respondents can ever possibly be failed.</li></ul></li><li>Respondents 2 (Verrilli)<ul><li>Alito questions whether the respondents' proposed test for impermissible conduct by a state court can ever be failed. He does so by critiquing the NC court\u2019s ruling. His criticisms center around history and precedent, essentially saying that this is a novel interpretation of an old issue, and the NC court\u2019s decision lacks meaningful precedent. He also takes issue with their introductory statement that they have to step in because the legislature won\u2019t fix the problem.<ul><li>Verrilli reminds him that the petitioners accept the NC court\u2019s ruling as given, in addition to providing rebuttals to each point.</li></ul></li></ul></li><li>Solicitor General<ul><li>His questions deal with USC \u00a7 (a)<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"C\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span> <a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:29\"><sup>29</sup></a> and whether it provides an alternative ground to decide the case. He seems to agree with the SG that it doesn\u2019t.</li></ul></li></ul><p><strong>Sotomayor</strong></p><ul><li>Petitioners<ul><li>Sotomayor was extremely skeptical of the petitioners. Her questions pointed out some confusing implications of the substantive/procedural distinction and said \u201cit seems that every answer you give is to get you what you want, but it makes little sense.\u201d She also questioned why this is different from courts doing sweeping interpretations of freedom or speech or assembly, if the 10th amendment reserves the ability to restrict legislatures to the states, and asked Justice Jackson\u2019s hypothetical about two groups claiming to be the legislature.</li></ul></li><li>Respondents 1 (Katyal)<ul><li>She had only a couple open-ended questions. The first was to address a couple historical examples from Maryland and Virginia that petitioners raised. The second was if they take issue with the idea that interpretations of state constitutions can violate the federal constitution (Rehnquist concurrence). She agreed with their response to the first, and said she read <i>Palm Beach</i> and Rehnquist\u2019s concurrence in <i>Bush v Gore </i>as saying state court interpretations can\u2019t violate due process. This suggests that she may not buy the argument for even a limited role for SCOTUS here.</li></ul></li><li>Respondents 2 (Verrilli)<ul><li>She starts off saying \u201cMr. Verrilli, I\u2019m \u2013 I\u2019m trying to organize an opinion if I were to rule in your favor.\u201d She then asks how they\u2019d argue against the procedural/substantive distinction and why they don\u2019t reach the question of if the NCSC went too far by legislating instead of reviewing.</li></ul></li><li>Solicitor General<ul><li>She asks how the SG would articulate where the line is for where a state court crosses into legislating instead of reviewing.</li></ul></li></ul><p><strong>Kagan</strong></p><ul><li>Petitioners<ul><li>Her main point was that, while this issue hasn\u2019t been directly addressed, court precedent (citing, <i>Smiley</i>, <i>Rucho</i>, and <i>Arizona</i>) pretty consistently points to legislatures being subject to ordinary constraints like governor\u2019s vetoes and state constitutions. She also expressed concerns that petitioner\u2019s theory would remove important checks and balances, which seems out of keeping with how our government works.</li></ul></li><li>Respondents 1 (Katyal)<ul><li>She seemed to be honestly trying to clarify the respondents' argument. The way she summarized it was that respondents affirm that federal courts can review state court interpretations for violating the Elections Clause (corollary of the Rehnquist concurrence), but it\u2019s under a highly deferential standard, and higher for state constitutions than state statutes. However, respondents don\u2019t think that the NCSC\u2019s decision here violates that rule, or even comes close.</li></ul></li><li>Respondents 2 (Verrilli)<ul><li>Her questions were focused around defining where the boundary line is for impermissible conduct by a state court. Her first question was around if courts have a responsibility to give legislatures a fair chance to remedy illegal maps before issuing replacements. Her second was a broader concern about defining a standard that won\u2019t be satisfied too easily, since judges often accuse each other of policymaking instead of law when they disagree with each other.</li></ul></li><li>Solicitor General<ul><li>Kagan\u2019s questions were fairly general. The first was broadly what the SG makes of the petitioner\u2019s argument, and the second was what she thinks about different standards for state laws vs. state constitutions in constraining legislatures.</li></ul></li></ul><p><strong>Gorsuch</strong></p><ul><li>Petitioners<ul><li>Gorsuch was very sympathetic to petitioners, tossing a softball about concerns the founders may have had with constitutions trumping legislatures. Both he and the petitioners' lawyer cited examples of states trying to put gerrymanders or the 3/5ths rule in their constitutions. He also seemed to think that this case went further than Rehnquist\u2019s concurrence, since the NCSC didn\u2019t interpret a law, they just annulled it outright.</li><li>His stance was also that the political question depends \u201con whose ox is being gored at what particular time.\u201d</li></ul></li><li>Respondents 1 (Katyal)<ul><li>He seemed extremely skeptical of respondents. He continued the \u201cwhose ox is being gored\u201d type of approach asking if an antebellum state could put gerrymandering or the 3/5ths clause in their constitution and whether federal courts could intervene. He also took issue with the NCSC decision based on Rehnquist\u2019s concurrence, given: the change in the NCSC\u2019s stance from a few years prior, their argument that they have to act because the legislature won\u2019t, and their opinion only spending three paragraphs addressing the very complex Elections Clause issue.</li></ul></li><li>Respondents 2 (Verrilli)<ul><li>He expressed support for drawing a line similar to Barrett\u2019s where if a state court interpretation is far afield from existing precedent then it\u2019s no longer the legislature that\u2019s prescribing time, place, and manner. This seems less deferential to state courts than Barrett\u2019s line, which focused instead on whether a court is doing judicial review.</li></ul></li><li>Solicitor General<ul><li>Questions were mostly clarifying the arguments being presented on each side, though he may have taken issue with state courts invoking their constitutions as a higher authority to set aside the legislature\u2019s time, place and manner regulations.</li></ul></li></ul><p><strong>Kavanaugh</strong></p><ul><li>Petitioners<ul><li>Kavanaugh seemed skeptical of the substance/procedure distinction line, and seemed to favor the Rehnquist approach (which he\u2019s cited in <i>DNC v. Wisconsin</i>, see above). He questioned petitioners' interpretation of <i>Palm Beach</i>, asked about state canons of interpretation, and about the history of state constitutions regulating federal elections, including in the Conference of Chief Justices brief.</li></ul></li><li>Respondents 1 (Katyal)<ul><li>He clarified that respondents aren\u2019t arguing for no federal judicial review of state court decisions, and seemed personally sympathetic to that stance. He was also skeptical of different standards for statutes vs. constitutions.</li></ul></li><li>Respondents 2 (Verrilli)<ul><li>None</li></ul></li><li>Solicitor General<ul><li>None</li></ul></li></ul><p><strong>Barrett</strong></p><ul><li>Petitioners<ul><li>Barrett seemed quite skeptical of the petitioners. She was suspicious that the petitioner\u2019s argument was simply working around court precedents without a sound basis and was cherry-picking historical quotes. She was also concerned that they were arguing \u201cfree and fair elections\u201d is unmanageable, but their substance/procedure distinction is a very tough line to draw as well. She also wasn\u2019t sure about the idea that constitutions are more problematic due to entrenchment when they can be changed via referenda.</li></ul></li><li>Respondents 1 (Katyal)<ul><li>She says that the way she\u2019s thinking about this is essentially that a state court can\u2019t interpret a law so wrongly that it\u2019s no longer doing judicial review and instead acting as a legislature, because that would violate the Elections Clause saying laws must be prescribed \u201cby the legislature.\u201d Respondents agree with that but would be even more deferential to state courts.</li></ul></li><li>Respondents 2 (Verrilli)<ul><li>She gave respondents a chance to address the question of whether SCOTUS has jurisdiction to review the particular remedy being issued by the NCSC (the court-drawn map), which is somewhat separate from whether the NCSC can rule the legislature\u2019s map unconstitutional. Those proceedings are still ongoing in North Carolina, so respondents don\u2019t think SCOTUS should rule on it.</li></ul></li><li>Solicitor General<ul><li>None</li></ul></li></ul><p><strong>Jackson</strong></p><ul><li>Petitioners<ul><li>Jackson was very skeptical. Her primary argument is that the legislature\u2019s existence and authority comes from the state constitution, so they can\u2019t be exempt from checks in the state constitution.</li></ul></li><li>Respondents 1 (Katyal)<ul><li>Her argument, which respondents agree with and which is similar to what she said to petitioners, is that valid legislative power is defined by the state constitution. This is generally in response to arguments that wild interpretations supplant the legislature\u2019s authority. That also means a high standard for SCOTUS review of constitutions because it\u2019s the font of authority for relevant parties.</li></ul></li><li>Respondents 2 (Verrilli)<ul><li>She asks what body of law should be referenced for setting the \u201chow far is too far\u201d standard for courts usurping legislatures in policymaking. Respondents cite Alito\u2019s colloquy with Katyal and Rehnquist\u2019s concurrence.</li></ul></li><li>Solicitor General<ul><li>Jackson clarified with the SG that SCOTUS doesn\u2019t need to articulate the sky-high standard that\u2019s been discussed because petitioners aren\u2019t pressing for it, they can just reject the substance/procedure distinction being pushed. She also continued her prior argument that a legislature making unconstitutional laws isn\u2019t a legislature.</li></ul></li></ul><h3><strong>Forecasts</strong></h3><p><u>Notes</u></p><p>The following probabilities are contingent on SCOTUS issuing a ruling on this case. An updated numerical forecast on that happening, particularly in light of the NC Supreme Court\u2019s decision to rehear <i>Harper v Hall</i>, may be forthcoming.</p><p>There are a wide variety of possible rulings that SCOTUS could issue, including restrictions on the sources and methods state courts can use to interpret state laws, restrictions on when state courts can intervene, and restrictions on actions that state courts can take. This makes it difficult to define narrow questions on the outcome of the case that can be numerically forecasted. Instead, we seek to bucket some of the likely rulings and forecast those buckets.</p><p>Will the court rule that the North Carolina courts erred in striking down and/or replacing the North Carolina district maps?</p><p><strong>Forecast: 81%</strong></p><p>Firstly as a base rate, we can look at how often SCOTUS overturns the lower court in cases that they hear. For cases coming from state courts, they overturn the state court 76% of the time.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:30\"><sup>30</sup></a> Intuitively this makes sense, a major reason to decide to hear a case is to correct an incorrect judgment. Many cases where SCOTUS agrees with the lower court simply aren\u2019t heard.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vQR9iiifwnJcunPSb/xkz4dx9n13wwbvtrmluy\"></p><p>Source: <a href=\"https://ballotpedia.org/SCOTUS_case_reversal_rates_%282007_-_Present%29\">Ballotpedia</a></p><p>&nbsp;</p><p>As for an opinion, we can look to oral arguments, and to the judges individually. In oral arguments overall, there were two main takeaways. Firstly, nobody seemed impressed by the petitioner\u2019s argument, and the moderate conservatives in Roberts/Barrett/Kavanaugh seemed quite skeptical. Secondly, the respondents endorsed the idea that SCOTUS has some role to play in reviewing state court interpretations of their constitutions for redistricting cases, though under a very deferential standard. The idea of setting that standard seemed to resonate with Roberts/Barrett/Kavanaugh, as well as Kagan and others. Everyone agreed it should be a high standard, but exactly how high was an area of ambiguity and disagreement. Kagan was worried about a rule not being deferential enough to state courts, Alito worried about one deferring too much.</p><p>The main ambiguity seems to be where the standard of review for state court decisions will fall, and on which side of it the NCSC decision will end up.</p><p>For the individual judges:</p><ul><li>Alito/Thomas/Gorsuch: For this case specifically, Thomas, Alito, and Gorsuch have all suggested support for the petitioners in their opinions in oral arguments and other cases, so their opinions are near-certain.</li><li>Roberts/Kavanaugh/Barrett: The other three conservatives are less certain. Those three justices voted against even hearing a similar case from during COVID.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:31\"><sup>31</sup></a> Even in this case, there were only four justices with written opinions on the stay request, three of which were Alito/Thomas/Gorsuch. Neither Roberts nor Barrett joined Kavanaugh\u2019s fairly mild concurrence in denying the stay. However, it may also be Roberts and Barrett simply taking a neutral stance before hearing the arguments in this case. In oral arguments, none of them seemed to have a clear stance toward the NCSC specifically. Their position seemed to favor setting a very high bar for SCOTUS intervention, though it\u2019s not clear where the NCSC\u2019s decision would fall relative to that line.<ul><li>Kavanaugh<ul><li>Kavanaugh published a concurrence in denying the application for a stay, but only explicitly on <i>Purcell</i> principle grounds. He wrote \u201cboth sides have advanced serious arguments on the merits. The issue is almost certain to keep arising until the Court definitively resolves it.\u201d<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:32\"><sup>32</sup></a> However, he did touch on this subject in an opinion in <i>DNC v. Wisconsin</i> (cited above) saying \u201cthe text of the Constitution requires federal courts to ensure that state courts do not rewrite state election laws.\u201d<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:33\"><sup>33</sup></a> In that case, he disapproved of a district court extending voting deadlines during COVID. This case is similar, but the NCSC\u2019s conduct is probably a bit less controversial.</li><li>In oral arguments he reiterated his support for the Rehnquist concurrence in <i>Bush v. Gore</i>, which he used to support the prior quote against \u201crewrit[ing] state election laws.\u201d Unfortunately he didn\u2019t say much else.</li><li>We put him at <strong>85%</strong> chance of ruling against the NCSC.</li></ul></li><li>Roberts<ul><li>Roberts has been more moderate on recent election cases, with the distinction he\u2019s drawn, as described by NYT, being that \u201c[f]ederal courts should not change voting procedures enacted by state legislatures, and they also should not step in when state courts or agencies change those procedures.\u201d<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:34\"><sup>34</sup></a> However, the aforementioned line he drew was concerning 2020 election cases under COVID. A blanket prohibition on federal courts doing anything with state election laws would go against his prior opinions and dissents.</li><li>He was skeptical of respondents in oral arguments, with his concerns focused on state courts choosing very specific definitions for permissible conduct based on vague statutes, which he implied he doesn\u2019t endorse.</li><li>All of that together paints a somewhat mixed picture, but we put it at <strong>70%</strong> that he\u2019ll rule against the NCSC.</li></ul></li><li>Barrett<ul><li>Barrett is the hardest to judge as she didn\u2019t participate in the recent cases around the 2020 election cited above. However, she did join the highly controversial conservative majority in <i>Dobbs</i> recently, and joined the other five conservatives in the Voting Rights Act case <i>Brnovich vs. DNC</i>. However, she generally seems closer to the more moderate Roberts and Kavanaugh than the more extreme conservatives.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:35\"><sup>35</sup></a></li><li>Her stance in oral arguments supported a highly deferential standard for SCOTUS reviewing state supreme court decisions. The question is where she\u2019d put the NCSC.</li><li>Given that track record and context we put a <strong>60%</strong> chance that she\u2019d rule against the NCSC</li></ul></li></ul></li></ul><p>With three near-certain votes in Alito/Thomas/Gorsuch, two of Barrett/Kavanaugh/Roberts is needed to form a majority. Importantly, those two or three votes don\u2019t need to join the other three conservatives' opinion, the majority opinion could be a moderate ruling that Barrett/Kavanaugh/Roberts author, with a more extreme but non-controlling concurring opinion from the other three. We assume that rulings are independent, so our probability for a ruling against the NCSC is <strong>81%</strong>, very close to the base rate for state court cases before SCOTUS.</p><h3><strong>Concerns for the Future</strong></h3><p><u>If SCOTUS Upholds the NC Supreme Court\u2019s Actions: </u><strong><u>Election legitimacy</u></strong></p><p>One of the most concerning futures for US politics is if large numbers of reasonable people don\u2019t feel their elections are fair. One conservative argument in this case is that if the NCSC\u2019s actions stand, blatantly partisan rulings from other courts in aggregate may jeopardize that trust.</p><p>The argument against what the NCSC did (Elections Clause aside) is that they\u2019re reading a ban on partisan gerrymandering from extremely vague clauses, arguably against their precedent, and they pretty openly admit a policy motive in their own opinion. The decision was 4-3 with four Democrat judges in favor, and three Republican judges against it. Seeing the ruling as partisan is reasonable, even if one agrees with the outcome.</p><p>For North Carolinians specifically, this opinion probably won\u2019t jeopardize trust in elections, mainly because gerrymandering is pretty universally reviled.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:36\"><sup>36</sup></a> But the danger may come from differences across states. \u201cAll elections shall be free\u201d and similar clauses can be read very differently by different judges. One bad outcome would be D courts letting D legislatures gerrymander freely and send as many Democrats to Congress as possible, but D courts blocking R legislatures from gerrymandering and forcing them to send more Democrats, or vice versa. Alternatively, an R court in a D state could read vague election-related statutes to support voter ID requirements or similar policies, which might otherwise be against the will of voters.</p><p>This has always been a possibility in a federalist system, but in an increasingly polarized political environment with many states electing judges,<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:37\"><sup>37</sup></a> it may be more likely to occur without SCOTUS intervention. Partisan gamesmanship in election law is unfortunately to be expected by partisan legislatures. But courts are intended to be a neutral third party, with decisions bound by constitutions and precedent, not the political opinions of justices. Some subjectivity is inevitable, but naked partisanship deciding election results or the balance of power in Congress will likely degrade respect for the courts as a (at least somewhat) neutral and nonpolitical body.</p><p><u>If SCOTUS allows for federal court review of state court decisions: </u><strong><u>Gerrymandering</u></strong></p><p>The most immediate effect of such a ruling would most likely be to enable partisan gerrymandering in more states. At least 17 states had state court lawsuits either challenging their 2020 federal maps under their respective state constitutions or asking the state court to resolve a political impasse in map drawing,<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:38\"><sup>38</sup></a> all of which may be affected depending on the ruling and/or subject to appeal in federal court. 21% of districts in the 2022 midterms were eventually drawn by state courts.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:39\"><sup>39</sup></a> Given the discussion in oral arguments, a SCOTUS ruling would most likely seek to empower state legislators to draw maps, and in practice they draw fewer competitive districts.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:40\"><sup>40</sup></a></p><p>Gerrymandering is obviously a bad practice, but there are mitigating factors here. Gerrymandering is bad in two ways: it promotes disproportionate representation (i.e 10-4 Republican reps in a 50/50 state like NC), and it enables representatives to choose their constituents to entrench themselves and ensure reelection. Since this case is for federal elections and maps are drawn by state reps, the entrenchment issue is less problematic. Elections for state and federal seats use different maps; maps drawn by the NC state representatives gerrymandering their own legislative districts will still be illegal and don\u2019t implicate the Elections Clause.</p><p>That leaves the issue of disproportionate representation, in this case Republicans comprising 50% of votes but getting 70% of seats (absent court intervention). For any individual state this is bad, but gerrymandering is used by both parties. The effect in the 2022 midterms appeared to be a bump of only +4 to Republicans, though that did end up mattering a lot in this particular cycle.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:41\"><sup>41</sup></a> Timothy Moore appears to recognize his argument as a double-edged sword, saying \u201ca wise person recognizes that an argument or rule that benefits one\u2019s political side of the aisle today is something that can hurt their side of the aisle tomorrow.\u201d<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:42\"><sup>42</sup></a></p><p><u>Any ruling: </u><strong><u>Implications for how free and fair elections are ensured</u></strong></p><p>Opposition to gerrymandering is a point of unity among Americans, and it demonstrates our widely held belief in fair elections. The cynical view presented by Gorsuch and Thomas is that political views on gerrymandering \u201c[depend] on whose ox is being gored at what particular time.\u201d That may not be entirely wrong. But many people have voted to restrict their own party\u2019s ability to gerrymander, which is remarkable.</p><p>It was put most aptly by Mr. Verrilli in oral arguments:</p><blockquote><p>If I could, there\u2019s just one last point I would like to make about whose ox is being gored here, which I think is quite important.</p><p>Actually, there\u2019s a great deal of sentiment in this country about the problems with extreme partisan gerrymandering and this Court\u2019s opinion in <i>Rucho</i> acknowledged it. And states have actually responded in nonpartisan ways. I can think of four states, New York, Florida, California and Ohio, all of which are in the control of one political party where presumably the incentives would have been lined up to maximize partisan advantage through the redistricting process, but in all four of those states they amended their constitutions through the work of the people to restrict partisan gerrymandering and those provisions have been enforced. I mean, the provision was enforced in New York, of course, just earlier this year.</p><p>And so I do think it is more than whose ox is being gored. This is a really important issue in this country.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fn:43\"><sup>43</sup></a></p></blockquote><p>Courts and legislatures are subject to different norms in different states, but they\u2019re ultimately both made of people. Favoring one or the other isn\u2019t inherently good or bad, and there are concerns with rulings going either way. An evaluation of the eventual ruling in this case shouldn\u2019t focus too heavily on which side was favored, but should rather focus on whether the balance of power it leads to will favor freer and fairer elections in the future.</p><h3><strong>Acknowledgments</strong></h3><p>This post was written by Greg Justice, in collaboration with @belikewater. Big thanks to Misha for setting up the collaboration that led to this paper, for connecting me with outside experts, and for his consistent support, input, and encouragement throughout the months-long writing process, to @belikewater for leading the original discussions of the case and helping to develop the main arguments, to Nu\u00f1o Sempere for his feedback and editing assistance, to Samotsvety forecasters for contributing their thoughts and forecasts, and to Christoph Winter, Aaron Hamlin, and Richard Winger for their comments and suggestions.</p><h3><strong>Footnotes</strong></h3><ol><li><a href=\"http://fordhamlawreview.org/wp-content/uploads/2021/11/Morley_November.pdf\">http://fordhamlawreview.org/wp-content/uploads/2021/11/Morley_November.pdf</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:1\">\u21a9</a></li><li><a href=\"https://constitution.congress.gov/browse/article-1/section-4/#:~:text=Clause%201%20Elections%20Clause,the%20Places%20of%20chusing%20Senators\">https://constitution.congress.gov/browse/article-1/section-4/#:~:text=Clause%201%20Elections%20Clause,the%20Places%20of%20chusing%20Senators</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:2\">\u21a9</a></li><li><a href=\"https://www.brennancenter.org/our-work/research-reports/independent-state-legislature-theory-explained\">https://www.brennancenter.org/our-work/research-reports/independent-state-legislature-theory-explained</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:3\">\u21a9</a></li><li><a href=\"https://www.scotusblog.com/case-files/cases/rucho-v-common-cause-2/\">https://www.scotusblog.com/case-files/cases/rucho-v-common-cause-2/</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:4\">\u21a9</a></li><li><a href=\"https://ballotpedia.org/Florida_Congressional_District_Boundaries,_Amendment_6_%282010%29\">https://ballotpedia.org/Florida_Congressional_District_Boundaries,<i>Amendment_6</i>(2010)</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:5\">\u21a9</a></li><li><a href=\"https://ballotpedia.org/Ohio_Issue_1,_Congressional_Redistricting_Procedures_Amendment_%28May_2018%29\">https://ballotpedia.org/Ohio_Issue_1,<i>Congressional_Redistricting_Procedures_Amendment</i>(May_2018)</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:6\">\u21a9</a></li><li><a href=\"https://irc.az.gov/about/proposition-106\">https://irc.az.gov/about/proposition-106</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:7\">\u21a9</a></li><li><a href=\"https://www.michigan.gov/micrc\">https://www.michigan.gov/micrc</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:8\">\u21a9</a></li><li><a href=\"https://penntoday.upenn.edu/news/moore-v-harper-voting-rights-election-law-and-future-american-democracy\">https://penntoday.upenn.edu/news/moore-v-harper-voting-rights-election-law-and-future-american-democracy</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:9\">\u21a9</a></li><li><a href=\"https://www.ncdemography.org/2020/08/13/who-are-north-carolinas-7-million-registered-voters/\">https://www.ncdemography.org/2020/08/13/who-are-north-carolinas-7-million-registered-voters/</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:10\">\u21a9</a></li><li><a href=\"https://www.270towin.com/states/North_Carolina\">https://www.270towin.com/states/North_Carolina</a> ,<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:11\">\u21a9</a></li><li><a href=\"https://en.wikipedia.org/wiki/North_Carolina%27s_congressional_districts\">https://en.wikipedia.org/wiki/North_Carolina%27s_congressional_districts</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:12\">\u21a9</a></li><li><a href=\"https://www.ncleg.gov/EnactedLegislation/Statutes/HTML/BySection/Chapter_1/GS_1-267.1.html\">https://www.ncleg.gov/EnactedLegislation/Statutes/HTML/BySection/Chapter_1/GS_1-267.1.html</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:13\">\u21a9</a></li><li><a href=\"https://appellate.nccourts.org/opinions/?c=1&amp;pdf=41183\">https://appellate.nccourts.org/opinions/?c=1&amp;pdf=41183</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:14\">\u21a9</a></li><li><a href=\"https://www.nccourts.gov/assets/inline-files/22.02.23%20-%20Order%20on%20Remedial%20Plans.pdf?E9mkhJLRatLIbqax0vvfwDCYgiunTgIB\">https://www.nccourts.gov/assets/inline-files/22.02.23%20-%20Order%20on%20Remedial%20Plans.pdf?E9mkhJLRatLIbqax0vvfwDCYgiunTgIB</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:15\">\u21a9</a></li><li><a href=\"https://www.supremecourt.gov/docket/docketfiles/html/qp/21-01271qp.pdf\">https://www.supremecourt.gov/docket/docketfiles/html/qp/21-01271qp.pdf</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:16\">\u21a9</a></li><li><a href=\"https://www.law.cornell.edu/supremecourt/text/20A66#CONCUR_4-1\">https://www.law.cornell.edu/supremecourt/text/20A66#CONCUR_4-1</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:17\">\u21a9</a></li><li><a href=\"https://www.supremecourt.gov/opinions/18pdf/18-422_9ol1.pdf\">https://www.supremecourt.gov/opinions/18pdf/18-422_9ol1.pdf</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:18\">\u21a9</a></li><li><a href=\"https://www.supremecourt.gov/opinions/20pdf/20-542_i3dj.pdf\">https://www.supremecourt.gov/opinions/20pdf/20-542_i3dj.pdf</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:19\">\u21a9</a></li><li><a href=\"https://www.supremecourt.gov/orders/courtorders/120820zr_bq7d.pdf\">https://www.supremecourt.gov/orders/courtorders/120820zr_bq7d.pdf</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:20\">\u21a9</a></li><li><a href=\"https://www.supremecourt.gov/orders/courtorders/121120zr_p860.pdf\">https://www.supremecourt.gov/orders/courtorders/121120zr_p860.pdf</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:21\">\u21a9</a></li><li><a href=\"https://www.justsecurity.org/83831/as-moore-v-harper-takes-shape-a-broad-coalition-takes-aim-at-the-independent-state-legislature-theory/\">https://www.justsecurity.org/83831/as-moore-v-harper-takes-shape-a-broad-coalition-takes-aim-at-the-independent-state-legislature-theory/</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:22\">\u21a9</a></li><li><a href=\"https://www.supremecourt.gov/DocketPDF/21/21-1271/237169/20220906163915853_21-1271%20Amici%20RNC%20et%20al.%20Supp.%20Pet..pdf\">https://www.supremecourt.gov/DocketPDF/21/21-1271/237169/20220906163915853_21-1271%20Amici%20RNC%20et%20al.%20Supp.%20Pet..pdf</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:23\">\u21a9</a></li><li><a href=\"https://www.supremecourt.gov/DocketPDF/21/21-1271/237139/20220906152952975_21-1271%20Amicus%20NRRT%20Supp.%20Pet.%20final.pdf\">https://www.supremecourt.gov/DocketPDF/21/21-1271/237139/20220906152952975_21-1271%20Amicus%20NRRT%20Supp.%20Pet.%20final.pdf</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:24\">\u21a9</a></li><li><a href=\"https://www.law.cornell.edu/supremecourt/text/258/130\">https://www.law.cornell.edu/supremecourt/text/258/130</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:25\">\u21a9</a></li><li><i>Rucho</i> focused on partisan gerrymandering lacking a justiciable standard for courts to apply. The context Roberts emphasized in oral arguments was \u201chow unmanageable and indeterminate various proposals were.\u201d He seems to be implying that constitutions providing \u201cstandards and guidance\u201d meant that constitutions could provide the clear justiciable standard that partisan gerrymandering lacked. In that context, \u201call elections shall be free\u201d in his view would not address the problem, and thus would not be endorsed by that opinion.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:26\">\u21a9</a></li><li>He\u2019s responding here to the idea that vague state constitutional clauses are categorically unenforceable.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:27\">\u21a9</a></li><li><i>Baker v. Carr</i> was a SCOTUS case from 1962. By 1961, Tennessee had failed to redistrict for 60 years, resulting in districts with vastly different populations, diluting some citizens' votes. SCOTUS ruled that vote dilution from such malapportionment violates the 14th amendment Equal Protection Clause. <i>Baker v. Carr</i> ruled that a state statute, Tennessee\u2019s 1901 map being used in 1961, violates federal law. It didn\u2019t appear to \u201csecond-guess state court interpretations of their own constitution\u201d as Justice Thomas seems to suggest. So to me it\u2019s not entirely clear what parallel he\u2019s trying to draw.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:28\">\u21a9</a></li><li>This federal statute determines what happens when reapportionment occurs but a state fails to draw new districts, especially if they gain or lose seats and their old maps have the wrong number of districts.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:29\">\u21a9</a></li><li><a href=\"https://ballotpedia.org/SCOTUS_case_reversal_rates_%282007_-_Present%29\">Ballotpedia: SCOTUS Case reversal rates</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:30\">\u21a9</a></li><li><a href=\"https://www.supremecourt.gov/opinions/20pdf/20-542_2c83.pdf\">https://www.supremecourt.gov/opinions/20pdf/20-542_2c83.pdf</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:31\">\u21a9</a></li><li><a href=\"https://www.supremecourt.gov/opinions/21pdf/21a455_5if6.pdf#page=1\">https://www.supremecourt.gov/opinions/21pdf/21a455_5if6.pdf#page=1</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:32\">\u21a9</a></li><li><a href=\"https://www.law.cornell.edu/supremecourt/text/20A66#CONCUR_4-1ref\">https://www.law.cornell.edu/supremecourt/text/20A66#CONCUR_4-1ref</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:33\">\u21a9</a></li><li><a href=\"https://www.nytimes.com/2020/10/29/us/john-roberts-supreme-court-voting.html\">https://www.nytimes.com/2020/10/29/us/john-roberts-supreme-court-voting.html</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:34\">\u21a9</a></li><li><a href=\"https://www.economist.com/united-states/2021/06/24/americas-supreme-court-is-less-one-sided-than-liberals-feared\">https://www.economist.com/united-states/2021/06/24/americas-supreme-court-is-less-one-sided-than-liberals-feared</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:35\">\u21a9</a></li><li><a href=\"https://thehill.com/homenews/state-watch/566327-american-voters-largely-united-against-partisan-gerrymandering-polling/\">https://thehill.com/homenews/state-watch/566327-american-voters-largely-united-against-partisan-gerrymandering-polling/</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:36\">\u21a9</a></li><li><a href=\"https://www.ojp.gov/ncjrs/virtual-library/abstracts/judicial-selection-united-states-special-report#:~:text=Partisan%20elections%20are%20held%20to,for%20State%20supreme%20court%20judges\">https://www.ojp.gov/ncjrs/virtual-library/abstracts/judicial-selection-united-states-special-report#:~:text=Partisan%20elections%20are%20held%20to,for%20State%20supreme%20court%20judges</a>.<a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:37\">\u21a9</a></li><li><a href=\"https://redistricting.lls.edu/cases/?courts%5B%5D=State%20Trial&amp;courts%5B%5D=State%20Appellate&amp;courts%5B%5D=State%20Supreme&amp;levels%5B%5D=Congress&amp;sortby=-updated&amp;page=1\">https://redistricting.lls.edu/cases/?courts%5B%5D=State%20Trial&amp;courts%5B%5D=State%20Appellate&amp;courts%5B%5D=State%20Supreme&amp;levels%5B%5D=Congress&amp;sortby=-updated&amp;page=1</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:38\">\u21a9</a></li><li><a href=\"https://www.brennancenter.org/our-work/research-reports/who-controlled-redistricting-every-state\">https://www.brennancenter.org/our-work/research-reports/who-controlled-redistricting-every-state</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:39\">\u21a9</a></li><li><a href=\"https://www.brennancenter.org/our-work/analysis-opinion/three-takeaways-redistricting-and-competition-2022-midterms\">https://www.brennancenter.org/our-work/analysis-opinion/three-takeaways-redistricting-and-competition-2022-midterms</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:40\">\u21a9</a></li><li><a href=\"https://www.politico.com/news/2022/11/25/redistricting-midterms-00070810\">https://www.politico.com/news/2022/11/25/redistricting-midterms-00070810</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:41\">\u21a9</a></li><li><a href=\"https://www.nbcnews.com/politics/supreme-court/dispute-north-carolina-congressional-districts-tees-major-elections-ca-rcna57755\">https://www.nbcnews.com/politics/supreme-court/dispute-north-carolina-congressional-districts-tees-major-elections-ca-rcna57755</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:42\">\u21a9</a></li><li><a href=\"https://www.supremecourt.gov/oral_arguments/argument_transcripts/2022/21-1271_21o2.pdf\">https://www.supremecourt.gov/oral_arguments/argument_transcripts/2022/21-1271_21o2.pdf</a><a href=\"https://samotsvety.org/blog/2023/02/26/moore-v-harper/#fnref:43\">\u21a9</a></li></ol>", "user": {"username": "gregjustice"}}, {"_id": "RSxff9TRoggYuFHAL", "title": "Cooperative or Competitive Altruism, and Antisocial Counterfactuals", "postedAt": "2023-03-20T17:54:02.556Z", "htmlBody": "<blockquote><p>\"We don\u2019t usually think of achievements in terms of what would have happened otherwise, but we should. What matters is not <i>who</i> does good but whether good is done; and the measure of how much good you achieve is the difference between what happens as a result of your actions and what would have happened anyway.\" - William MacAskill, Doing Good Better</p></blockquote><p>&nbsp;</p><p>Counterfactual thinking is fundamental to economic thinking, and this approach has been incorporated into Effective Altruism. The actual impact of your choices is based on what changed, not what happened. &nbsp;Per <a href=\"https://forum.effectivealtruism.org/topics/counterfactual-reasoning\">the forum wiki summary</a>, \"Counterfactual reasoning involves scenarios that will occur if an agent chooses a certain action, or that would have occurred if an agent had chosen an action they did not.\"</p><p>In this post, I'll argue that when counterfactual reasoning is applied the way Effective Altruist decisions and funding occurs in practice, there is a preventable anti-cooperative bias that is being created, and that this is making us as a movement less impactful than we could be.</p><h2>Myopia and Hyperopia</h2><p>To consider this, we need to revisit a fundamental assumption about how to think about impact, in light of the fact that individuals and groups can, should, and often do cooperate - both explicitly and implicitly. And to revisit the assumption, I want to note two failure modes for counterfactual reasoning.</p><p>First, myopic reasoning. Being a doctor saves lives, but arguably has little counterfactual value. And myopic reasoning can be far worse than this. As Benjamin Todd <a href=\"https://80000hours.org/articles/counterfactuals/\">suggests imagining</a>, \"you\u2019re at the scene of an accident and you see an injured person. In your enthusiasm to help, you push the paramedics out of the way and perform CPR on the injured person yourself. You\u2019re successful and bring them back to consciousness, but because you\u2019re less well-trained than the paramedics, you cause permanent damage to the person\u2019s spine.\" Obviously, counterfactual thinking, appreciating that without your \"help\" the outcome will be better, can prevent this type of myopia.</p><p>But there is a second failure mode, which I'll call hyperopic reasoning. Myopia is nearsightedness, the inability to see things far away, and hyperopia is farsightedness, the inability to see things nearby. What does this look like in reasoning about outcomes, and how could it fail?</p><p>In December 2020, the United Kingdom and Sweden granted emergency approval for the Oxford\u2013AstraZeneca vaccine, while Germany and the United States authorized the Pfizer-BioNTech vaccine, and the United States authorized the Moderna vaccine as well. While all of these were significantly better than the earliest vaccine, from Sinopharm in China, or the slightly later CoronaVac or Sputnik V, &nbsp;the counterfactual value of each vaccine seems comparatively miniscule because there were two other western vaccines that would have been approved without the development of this third option<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpm9bt3gkoho\"><sup><a href=\"#fnpm9bt3gkoho\">[1]</a></sup></span>.</p><p>The <a href=\"https://theincidentaleconomist.com/wordpress/counterfactuals/\">Incidental Economist puts it clearly</a>:</p><blockquote><p>When you want to know the causal effect of an intervention (policy change, medical treatment, whatever) on something, you need to compare two states of the world: the world in which the intervention occurred and the world in which it did not. The latter is the counterfactual world... What we really want to know is how the world is different due to the intervention <strong>and only the intervention</strong>.</p></blockquote><p>This is viewing the counterfactual from a single viewpoint. There is no shared credit for AstraZeneca, Pfizer, and Moderna, because each counterfactual is ignoring the nearby efforts, focused only on the global ones. But we certainly don't think that the value was zero - so what are we doing wrong?</p><p>We can try to resolve this without accounting for cooperation. Prior to the creation of the vaccines, it was unclear which would succeed. Each team has a prior potential to create a vaccine, and some chance that the others would fail. If each team has an uncorrelated 50% chance of succeeding, and with three teams, the probability of being the sole group with a vaccine is therefore 12.5% - justifying the investment. But that means the combined value of all three was only 37.5% of the value of success, and if we invested on that basis, we would underestimate the value for each. In this case, the value of a vaccine was plausibly in the hundreds of billions of dollars, so it's enough - but it's still wrong as a general rule.</p><h2>Cooperation Within Effective Altruism, and Without</h2><p>I think this problem is dealt with within the Effective Altruism movement partially by viewing it as a single actor<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2j5mm9ezgzb\"><sup><a href=\"#fn2j5mm9ezgzb\">[2]</a></sup></span>. Within Effective Altruism, people are remarkably cooperative, willing to forgo credit and avoid duplicating effort. If we're collectively doing two projects with similar goals, each of which may fail, we can view both as part of a strategy for ensuring the goals are achieved.&nbsp;</p><p>And there's a value in competition and multiple approaches to the same question, and this means some degree of internal competition is good. It's certainly useful that there is more than one funder, so that decisions are made independently, despite the costs imposed. It's<a href=\"https://www.probablygood.org/\"> probably good</a> that there are two organizations doing career advising, instead of just having 80,000 Hours.&nbsp;</p><p>In business, this approach, with multiple firms engaged in related markets, Could be be called coopetition - cooperation on some things and competition on others. But the coopetition approach is imperfect. There is a natural tendency for people to seek credit for their ideas, and that often leads to suboptimal outcomes. (<a href=\"https://forum.effectivealtruism.org/posts/bvK44CdpG7mGpQHbw/the-usd100-000-truman-prize-rewarding-anonymous-ea-work\">Which we can compensate for, at least partially</a>.) Similarly, there is a cost to coordination, so even when people would like to cooperate, they may not find ways to do so. (But providing <a href=\"https://forum.effectivealtruism.org/posts/u5JesqQ3jdLENXBtB/concrete-biosecurity-projects-some-of-which-could-be-big-1\">a list</a> of <a href=\"https://finmoorhouse.com/writing/more-projects/\">ideas</a> for <a href=\"https://forum.effectivealtruism.org/posts/LG6gwxhrw48Dvteej/concrete-project-lists\">others to pursue</a> and <a href=\"https://www.charityentrepreneurship.com/\">helping launch projects designed to fill needs</a> is useful.)</p><p>Where Effective Altruism more clearly falls down is external cooperation. Much of what Effective Altruism does is external-focused. (Biosecurity, Alternative protein, Global poverty reduction, and similar. Everything but community building, essentially.) Unfortunately, most of the world isn't allied with Effective Altruism, or is only implicitly cooperating. But most of the world at least generally shares its goals, if not its priorities. And that is unlikely to change in the near-term.</p><p>So who should get credit for what gets done? This matters - if we're not careful with our counterfactual reasoning, we could overcount or undercount our contribution, and make sub-optimal decisions.&nbsp;</p><h2>Shapley Value</h2><p>Lloyd S. Shapley gave us a fundamental tool for cooperation, the Shapley value, which is the (indisputably correct<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkwzr5dm5vdl\"><sup><a href=\"#fnkwzr5dm5vdl\">[3]</a></sup></span>) way to think about counterfactual value in scenarios with cooperation. If you don't know what it is, read or skim <a href=\"https://forum.effectivealtruism.org/posts/XHZJ9i7QBtAJZ6byW/shapley-values-better-than-counterfactuals\">Nu\u00f1o Sempere's post</a>, and come back afterwards. But in short, in the same way that considering counterfactuals fixes myopia by considering what happens if you do nothing, Shapley values fix hyperopia by giving credit to everyone involved in cooperatively providing value. &nbsp;</p><p>And a large part of the value provided by Effective Altruism is only cooperative. We donate bed-nets, which get distributed by local partners, on the basis of data about malaria rates collected by health officials, &nbsp;when someone else is paying for the costs of the distribution, to individuals who need to do all the work of using the bed-nets. Then we walk away saying (hyperopically,) we saved a life for $5,000, ignoring every other part of the complex system enabling our donation to be effective. And that is not to say it's not an effective use of money! In fact, it's incredibly effective, even in Shapley-value terms. But we're over-allocating credit to ourselves.</p><p>The implicit question is whether we view the rest of the world as moral actors, or moral patients - and the latter is the direct cause of (the unfortunately named) White Savior Complex. This is the view that altruists decisions matter because they are the people responsible, and those they save are simply passive recipients. And (outside of certain types of animal welfare,) this view is clearly both wrong, and harmful.</p><h2>\"Non-EA Money\"</h2><p>I've used this phrase before, and heard others use it as well. The idea is that we'd generally prefer someone else spends money on something, because that leaves more money for us, counterfactually, to do good. But this is a anti-cooperative attitude, and when we have goals like pandemic prevention which are widely shared, the goal should be to maximize the good done, not the good that Effective Altruism does, or claims credit for. Sometimes, that means cooperating on use of funds, rather than maximizing counterfactual impact of our donation.</p><p>Does this matter? I think so, and not just in terms of potentially misallocating our funds across interventions. (Which by itself should worry us!) It also matters because it makes our decisions anti-cooperative. We're trying to leverage other spenders into spending money, rather than cooperating to maximize total good done.&nbsp;</p><p>For example, Givewell recently looked at <a href=\"https://docs.google.com/spreadsheets/d/1yWneQFeOr6uFPa5242u1MrJOUURNpX1WslQcWr-bjXM/edit?usp=sharing\">likelihood of crowding out funding</a>, with an explicitly adversarial model; if the Global Fund and/or PMI are providing less funding because of their donations, they don't view their own funding as counterfactual. And when we only take credit for counterfactual impact instead of Shapley impact, we're being hyperopic.</p><p>I'm unsure if there is a simple solution to this, since Shapley values require understanding not just your own strategy, but the strategy of others, which is information we don't have. I do think that it needs more explicit consideration - because as EA becomes more globally important, and coordination with other groups becomes increasingly valuable, playing the current implicitly anti-competitive strategy is going to be a very bad way to improve the world.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpm9bt3gkoho\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpm9bt3gkoho\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This example was inspired by <a href=\"https://forum.effectivealtruism.org/posts/XHZJ9i7QBtAJZ6byW/shapley-values-better-than-counterfactuals#Example_2__Sometimes__the_sum_of_the_counterfactuals_is_less_than_total_value__Sometimes_it_s_0_\">Nu\u00f1o Sempere's example</a> of Leibniz and Newton both inventing calculus - which is arguably a far better example, because there was no advantage of having more manufacturing of the vaccine.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2j5mm9ezgzb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2j5mm9ezgzb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Despite the fact &nbsp;that it <a href=\"\">really</a>, (<a href=\"https://forum.effectivealtruism.org/posts/54T4YmxZm3Ht4fuMA/deconfusion-part-3-ea-community-and-social-structure\">really, really</a>) isn't just one thing. Cooperation thankfully triumphs over even a remarkable degree of incoherence.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkwzr5dm5vdl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkwzr5dm5vdl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It is the uniquely determined way to allocate value across cooperating actors, in a mathematical sense, given decisions about how credit should be allocated like whether to allocate credit over groups or individuals, and whether to split by group, or by time spent, or some other metric.</p></div></li></ol>", "user": {"username": "Davidmanheim"}}, {"_id": "g4fXhiJyj6tdBhuBK", "title": "Survey on intermediate goals in AI governance", "postedAt": "2023-03-17T12:44:42.305Z", "htmlBody": "<p>It seems that a key bottleneck for the field of longtermism-aligned AI governance is limited strategic clarity (see&nbsp;<a href=\"https://www.openphilanthropy.org/research/our-ai-governance-grantmaking-so-far/\"><u>Muehlhauser, 2020</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/M2SBwctwC6vBqAmZW/a-personal-take-on-longtermist-ai-governance#Key_bottlenecks:~:text=Bottleneck%20%232%3A%20We,work%2C%20and%20also%3A\"><u>2021</u></a>). As one effort to increase strategic clarity, in October-November 2022, we sent a survey to 229 people we had reason to believe are knowledgeable about longtermist AI governance, receiving 107 responses. We asked about:&nbsp;</p><ul><li>respondents\u2019 \u201ctheory of victory\u201d for AI risk (which we defined as the main, high-level \u201cplan\u201d they\u2019d propose for how humanity could plausibly manage the development and deployment of transformative AI such that we get long-lasting good outcomes),</li><li>how they\u2019d feel about funding going to each of 53 potential \u201cintermediate goals\u201d for AI governance,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefufxjl6mht9b\"><sup><a href=\"#fnufxjl6mht9b\">[1]</a></sup></span></li><li>what other intermediate goals they\u2019d suggest,</li><li>how high they believe the risk of existential catastrophe from AI is, and</li><li>when they expect&nbsp;<a href=\"https://docs.google.com/document/d/15siOkHQAoSBl_Pu85UgEDWfmvXFotzub31ow3A11Xvo/edit\"><u>transformative AI</u></a> (TAI) to be developed.</li></ul><p>We hope the results will be useful to funders, policymakers, people at AI labs, researchers, field-builders, people orienting to longtermist AI governance, and perhaps other types of people. For example, the report could:&nbsp;</p><ul><li>Broaden the range of options people can easily consider</li><li>Help people assess how much and in what way to focus on each potential \u201ctheory of victory\u201d, \u201cintermediate goal\u201d, etc.</li><li>Target and improve further efforts to assess how much and in what way to focus on each potential theory of victory, intermediate goal, etc.</li></ul><p><strong>You can see a summary of the survey results </strong><a href=\"https://docs.google.com/document/d/1ax7VLd-fudTbwrS1XP8VFI4WP7aUY2RCm-390u3jUio/edit\"><strong>here</strong></a>. Note that we will expect readers to abide by the policy articulated in \"About sharing information from this report\" (for the reasons explained there).</p><h1>Acknowledgments</h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g4fXhiJyj6tdBhuBK/rcfinlzajllgxdrkfezz\"></p><p><i>This report is a project of&nbsp;</i><a href=\"https://rethinkpriorities.org/\"><i>Rethink Priorities</i></a><i>\u2013a think tank dedicated to informing decisions made by high-impact organizations and funders across various cause areas. The project was commissioned by Open Philanthropy. Full acknowledgements can be found in the linked \"Introduction &amp; summary\" document.&nbsp;</i></p><p><i>If you are interested in RP\u2019s work, please visit our&nbsp;</i><a href=\"https://www.rethinkpriorities.org/research\"><i>research database</i></a><i> and subscribe to our&nbsp;</i><a href=\"https://www.rethinkpriorities.org/newsletter\"><i>newsletter</i></a><i>.&nbsp;</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnufxjl6mht9b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefufxjl6mht9b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Here\u2019s the definition of \u201cintermediate goal\u201d that we stated in the survey itself:&nbsp;</p><blockquote><p>By an intermediate goal, we mean any goal for reducing extreme AI risk that\u2019s more specific and directly actionable than a high-level goal like \u2018reduce existential AI accident risk\u2019 but is less specific and directly actionable than a particular intervention. In another context (global health and development), examples of potential intermediate goals could include \u2018develop better/cheaper malaria vaccines\u2019 and \u2018improve literacy rates in Sub-Saharan Africa\u2019.</p></blockquote></div></li></ol>", "user": {"username": "MichaelA"}}, {"_id": "DiGL5FuLgWActPBsf", "title": "How much should governments pay to prevent catastrophes? Longtermism\u2019s limited role", "postedAt": "2023-03-19T16:50:00.598Z", "htmlBody": "<h2>Preamble</h2><ul><li>We use standard cost-benefit analysis (CBA) to argue that governments should do more to reduce global catastrophic risk.</li><li>We argue that getting governments to adopt a CBA-driven catastrophe policy should be the goal of longtermists in the political sphere.</li><li>We suggest that longtermism can play a supplementary role in government catastrophe policy. If and when present people are willing to pay for interventions aimed primarily at improving humanity's long-term prospects, governments should fund these interventions.</li><li>We argue that longtermists should commit to acting in accordance with a CBA-driven catastrophe policy in the political sphere. This commitment would help bring about an outcome much better than the status quo, for both the present generation and the long-term future.</li></ul><p>This article is set to appear as a chapter in <i>Essays on Longtermism</i>, edited by Jacob Barrett, Hilary Greaves, and David Thorstad, and published by Oxford University Press.</p><h2>Abstract</h2><p>Longtermists have argued that humanity should significantly increase its efforts to prevent catastrophes like nuclear wars, pandemics, and AI disasters. But one prominent longtermist argument overshoots this conclusion: the argument also implies that humanity should reduce the risk of existential catastrophe even at extreme cost to the present generation. This overshoot means that democratic governments cannot use the longtermist argument to guide their catastrophe policy. In this paper, we show that the case for preventing catastrophe does not depend on longtermism. Standard cost-benefit analysis implies that governments should spend much more on reducing catastrophic risk. We argue that a government catastrophe policy guided by cost-benefit analysis should be the goal of longtermists in the political sphere. This policy would be democratically acceptable, and it would reduce existential risk by almost as much as a strong longtermist policy.</p><h1><strong>1. Introduction</strong></h1><p>It would be very bad if humanity suffered a nuclear war, a deadly pandemic, or an AI disaster. This is for two main reasons. The first is that these catastrophes could kill billions of people. The second is that they could cause human extinction or the permanent collapse of civilization.</p><p><i>Longtermists </i>have argued that humanity should increase its efforts to avert nuclear wars, pandemics, and AI disasters (Beckstead 2013; Bostrom 2013; Greaves and MacAskill 2021; MacAskill 2022; Ord 2020).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2ylqni72csq\"><sup><a href=\"#fn2ylqni72csq\">[1]</a></sup></span>&nbsp;One prominent longtermist argument for this conclusion appeals to the second reason: these catastrophes could lead to human extinction or the permanent collapse of civilization, and hence prevent an enormous number of potential people from living happy lives in a good future (Beckstead 2013; Bostrom 2013; Greaves and MacAskill 2021; MacAskill 2022: 8\u20139; Ord 2020: 43\u201349). These events would then qualify as <i>existential catastrophes</i>: catastrophes that destroy humanity\u2019s long-term potential (Ord 2020: 37).</p><p>Although this longtermist argument has been compelling to many, it has at least two limitations: limitations that are especially serious if the intended conclusion is that <i>democratic governments</i>&nbsp;should increase their efforts to prevent catastrophes. First, the argument relies on a premise that many people reject: that it would be an overwhelming moral loss if future generations never exist. Second, the argument overshoots. Given other plausible claims, building policy on this premise would not only lead governments to increase their efforts to prevent catastrophes. It would also lead them to impose extreme costs on the present generation for the sake of miniscule reductions in the risk of existential catastrophe. Since most people\u2019s concern for the existence of future generations is limited, this policy would be democratically unacceptable, and so governments cannot use the longtermist argument to guide their catastrophe policy.</p><p>In this chapter, we offer a standard cost-benefit analysis argument for reducing the risk of catastrophe. We show that, given plausible estimates of catastrophic risk and the costs of reducing it, many interventions available to governments pass a cost-benefit analysis test. Therefore, the case for averting catastrophe does not depend on longtermism. In fact, we argue, governments should do much more to reduce catastrophic risk even if future generations do not matter at all. The first reason that a catastrophe would be bad \u2013 billions of people might die \u2013 by itself warrants much more action than the status quo. This argument from present people\u2019s interests avoids both limitations of the longtermist argument: it assumes only that the present generation matters, and it does not overshoot. Nevertheless, like the longtermist argument, it implies that governments should do much more to reduce catastrophic risk.</p><p>We then argue that getting governments to adopt a catastrophe policy based on cost-benefit analysis should be the goal of longtermists in the political sphere. This goal is achievable, because cost-benefit analysis is already a standard tool for government decision-making and because moving to a CBA-driven catastrophe policy would benefit the present generation. Adopting a CBA-driven policy would also reduce the risk of existential catastrophe by almost as much as adopting a <i>strong longtermist policy</i>&nbsp;founded on the premise that it would be an overwhelming moral loss if future generations never exist.</p><p>We then propose that the longtermist worldview can play a supplementary role in government catastrophe policy. Longtermists can make the case for their view, and thereby increase present people\u2019s willingness to pay for <i>pure longtermist goods</i>: goods that do not much benefit the present generation but improve humanity\u2019s long-term prospects. These pure longtermist goods include especially refuges designed to help civilization recover from future catastrophes. When present people are willing to pay for such things, governments should fund them. This spending would have modest costs for those alive today and great expected benefits for the long-run future.</p><p>We end by arguing that longtermists should commit to acting in accordance with a CBA-driven catastrophe policy in the political sphere. This commitment would help bring about an outcome that is much better than the status quo, for the present generation and long-term future alike.</p><h1><strong>2. The risk of catastrophe</strong></h1><p>As noted above, we are going to use standard cost-benefit analysis to argue for increased government spending on preventing catastrophes.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8efk14fzzbw\"><sup><a href=\"#fn8efk14fzzbw\">[2]</a></sup></span>&nbsp;We focus on the U.S. government, but our points apply to other countries as well (with modifications that will become clear below). We also focus on the risk of <i>global catastrophes</i>, which we define as events that kill at least 5 billion people. Many events could constitute a global catastrophe in the coming years, but we concentrate on three in particular: nuclear wars, pandemics, and AI disasters. Reducing the risk of these catastrophes is particularly cost-effective.</p><p>The first thing to establish is that the risk is significant. That presents a difficulty. There has never yet been a global catastrophe by our definition, so we cannot base our estimates of the risk on long-run frequencies. But this difficulty is surmountable because we can use other considerations to guide our estimates. These include near-misses (like the Cuban Missile Crisis), statistical models (like power-law extrapolations), and empirical trends (like advances in AI). We do not have the space to assess all the relevant considerations in detail, so we mainly rely on previously published estimates of the risks. These estimates should be our point of departure, pending further investigation. Note also that these estimates need not be perfectly accurate for our conclusions to go through. It often suffices that the risks exceed some low value.</p><p>Let us begin with the risk of nuclear war. Toby Ord estimates that the existential risk from nuclear war over the next 100 years is about 1-in-1,000 (2020: 167). Note, however, that \u2018existential risk\u2019 refers to the risk of an <i>existential catastrophe</i>: a catastrophe that destroys humanity\u2019s long-term potential. This is a high bar. It means that any catastrophe from which humanity ever recovers (even if that recovery takes many millennia) does not count as an existential catastrophe. Nuclear wars can be enormously destructive without being likely to pose an existential catastrophe, so Ord\u2019s estimate of the risk of \u201cfull-scale\u201d nuclear war is much higher, at about 5% over the next 100 years (Wiblin and Ord 2020). This figure is roughly aligned with our own views (around 3%) and with other published estimates of nuclear risk. At the time of writing, the forecasting community Metaculus puts the risk of thermonuclear war before 2070 at 11% (Metaculus 2022c).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefku9o8g99yp\"><sup><a href=\"#fnku9o8g99yp\">[3]</a></sup></span>&nbsp;Luisa Rodriguez\u2019s (2019b) aggregation of expert and superforecaster estimates has the risk of nuclear war between the U.S. and Russia at 0.38% per year, while Martin E. Hellman (2008: 21) estimates that the annual risk of nuclear war between the U.S. and Russia stemming from a Cuban-Missile-Crisis-type scenario is 0.02-0.5%.</p><p>We recognise that each of these estimates involve difficult judgement-calls. Nevertheless, we think it would be reckless to suppose that the true risk of nuclear war this century is less than 1%. Here are assorted reasons for caution. Nuclear weapons have been a threat for just a single human lifetime, and in those years we have already racked up an eye-opening number of close calls. The Cuban Missile Crisis is the most famous example, but we also have declassified accounts of many accidents and false alarms (see, for example, Ord 2020, Appendix C). And although nuclear conflict would likely be devastating for all sides involved, leaders often have selfish incentives for brinkmanship, and may behave irrationally under pressure. Looking ahead, future technological developments may upset the delicate balance of deterrence. And we cannot presume that a nuclear war would harm only its direct targets. Research has suggested that the smoke from smouldering cities would take years to dissipate, during which time global temperatures and rainfall would drop low enough to kill most crops.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7tawwoceikd\"><sup><a href=\"#fn7tawwoceikd\">[4]</a></sup></span>&nbsp;That leads Rodriguez (2019a) to estimate that a U.S.-Russia nuclear exchange would cause a famine that kills 5.5 billion people in expectation. One of us (Shulman) estimates a lower risk of this kind of <i>nuclear winter</i>, a lower average number of warheads deployed in a U.S.-Russia nuclear exchange, and a higher likelihood that emergency measures succeed in reducing mass starvation, but we still put expected casualties in the billions.</p><p>Pandemics caused by pathogens that have been engineered in a laboratory are another major concern. Ord (2020: 167) estimates that the existential risk over the next century from these engineered pandemics is around 3%. And as with nuclear war, engineered pandemics could be extremely destructive without constituting an existential catastrophe, so Ord\u2019s estimate of the risk of global catastrophe arising from engineered pandemics would be adjusted upward from this 3% figure. At the time of writing, Metaculus suggests that there is a 9.6% probability that an engineered pathogen causes the human population to drop by at least 10% in a period of 5 years or less by 2100.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7ds6mc4jjuf\"><sup><a href=\"#fn7ds6mc4jjuf\">[5]</a></sup></span>&nbsp;In a 2008 survey of participants at a conference on global catastrophes, the median respondent estimated a 10% chance that an engineered pandemic kills at least 1 billion people and a 2% chance that an engineered pandemic causes human extinction before 2100 (Sandberg and Bostrom 2008).</p><p>These estimates are based on a multitude of factors, of which we note a small selection. Diseases can be very contagious and very deadly.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefq2i8sctnkg8\"><sup><a href=\"#fnq2i8sctnkg8\">[6]</a></sup></span>&nbsp;There is no strong reason to suppose that engineered diseases could not be both. Scientists continue to conduct research in which pathogens are modified to enhance their transmissibility, lethality, and resistance to treatment (Millett and Snyder-Beattie 2017: 374; Ord 2020: 128\u201329). We also have numerous reports of lab leaks: cases in which pathogens have been accidentally released from biological research facilities and allowed to infect human populations (Ord 2020: 130\u201331). Many countries ran bioweapons programs during the twentieth century, and bioweapons were used in both World Wars (Millett and Snyder-Beattie 2017: 374). Terrorist groups like the Aum Shinrikyo cult have tried to use biological agents to cause mass casualties (Millett and Snyder-Beattie 2017: 374). Their efforts were hampered by a lack of technology and expertise, but humanity\u2019s collective capacity for bioterror has grown considerably since then. A significant number of people now have the ability to cause a biological catastrophe, and this number looks set to rise further in the coming years (Ord 2020: 133\u201334).</p><p>Ord (2020: 167) puts the existential risk from artificial general intelligence (AGI) at 10% over the next century. This figure is the product of a 50% chance of human-level AGI by 2120 and a 20% risk of existential catastrophe, conditional on AGI by 2120 (Ord 2020: 168\u201369). Meanwhile, Joseph Carlsmith (2021: 49) estimates a 65% probability that by 2070 it will be possible and financially feasible to build AI systems capable of planning, strategising, and outperforming humans in important domains. He puts the (unconditional) existential risk from these AI systems at greater than 10% before 2070 (2021: 47). The aggregate forecast in a recent survey of machine learning researchers is a 50% chance of high-level machine intelligence by 2059 (Stein-Perlman, Weinstein-Raun, and Grace 2022).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefalsc0v7mcmk\"><sup><a href=\"#fnalsc0v7mcmk\">[7]</a></sup></span>&nbsp;The median respondent in that survey estimated a 5% probability that AI causes human extinction or humanity\u2019s permanent and severe disempowerment (Stein-Perlman et al. 2022). Our own estimates are closer to Carlsmith and the survey respondents on timelines and closer to Ord on existential risk.</p><p>These estimates are the most speculative: nuclear weapons and engineered pathogens already exist in the world, while human-level AGI is yet to come. We cannot make a full case for the risk of AI catastrophe in this chapter, but here is a sketch. AI capabilities are growing quickly, powered partly by rapid algorithmic improvements and especially by increasing computing budgets. Before 2010, compute spent on training AI models grew in line with Moore\u2019s law, but in the recent deep learning boom it has increased much faster, with an average doubling time of 6 months over that period (Sevilla et al. 2022). Bigger models and longer training runs have led to remarkable progress in domains like computer vision, language, protein modelling, and games. The next 20 years are likely to see the first AI systems close to the computational scale of the human brain, as hardware improves and spending on training runs continues to increase from millions of dollars today to many billions of dollars (Cotra 2020: 1\u20139, 2022). Extrapolating past trends suggests that these AI systems may also have capabilities matching the human brain across a wide range of domains.</p><p>AI developers train their systems using a reward function (or loss function)<i>&nbsp;</i>which assigns values to the system's outputs, along with an algorithm that modifies the system to perform better according to the reward function. But encoding human intentions in a reward function has proved extremely difficult, as is made clear by the many recorded instances of AI systems achieving high reward by behaving in ways unintended by their designers (DeepMind 2020; Krakovna 2018). These include systems pausing Tetris forever to avoid losing (Murphy 2013), using camera-trickery to deceive human evaluators into believing that a robot hand is completing a task (DeepMind 2020; OpenAI 2017), and behaving differently under observation to avoid penalties for reproduction (Lehman et al. 2020: 282; Muehlhauser 2021). We also have documented cases of AIs adopting goals that produce high reward in training but differ in important ways from the goals intended by their designers (Langosco et al. 2022; Shah et al. 2022). One example comes in the form of a model trained to win a video game by reaching a coin at the right of the stage. The model retained its ability to navigate the environment when the coin was moved, but it became clear that the model\u2019s real goal was to go as far to the right as possible, rather than to reach the coin (Langosco et al. 2022: 4). So far, these issues of <i>reward hacking</i>&nbsp;and <i>goal misgeneralization</i>&nbsp;have been of little consequence, because we have been able to shut down misbehaving systems or alter their reward functions. But that looks set to change as AI systems come to understand and act in the wider world: a powerful AGI could learn that allowing itself to be turned off or modified is a poor way of achieving its goal. And given any of a wide variety of goals, this kind of AGI would have reason to perform well in training and conceal its real goal until AGI systems are collectively powerful enough to seize control of their reward processes (or otherwise pursue their goals) and defeat any human response.</p><p>That is one way in which misaligned AGI could be disastrous for humanity. Guarding against this outcome likely requires much more work on robustly aligning AI with human intentions, along with the cautious deployment of advanced AI to enable proper safety engineering and testing. Unfortunately, economic and geopolitical incentives may lead to much less care than is required. Competing companies and nations may cut corners and expose humanity to serious risks in a race to build AGI (Armstrong, Bostrom, and Shulman 2016). The risk is exacerbated by the <i>winner\u2019s curse </i>dynamic at play: all else equal, it is the actors who most underestimate the dangers of deployment that are most likely to do so (Bostrom, Douglas, and Sandberg 2016).</p><p>Assuming independence and combining Ord\u2019s risk-estimates of 10% for AI, 3% for engineered pandemics, and 5% for nuclear war gives us at least a 17% risk of global catastrophe from these sources over the next 100 years.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo23uvsfqw5\"><sup><a href=\"#fno23uvsfqw5\">[8]</a></sup></span><sup>&nbsp;</sup>If we assume that the risk per decade is constant, the risk over the next decade is about 1.85%.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmekx3nzs16\"><sup><a href=\"#fnmekx3nzs16\">[9]</a></sup></span>&nbsp;If we assume also that every person\u2019s risk of dying in this kind of catastrophe is equal, then (conditional on not dying in other ways) each U.S. citizen\u2019s risk of dying in this kind of catastrophe in the next decade is at least&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"5/9\u00d71.85\\%\u22481.03\\%\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">9</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1.85</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u2248</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1.03</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>&nbsp;(since, by our definition, a global catastrophe would kill at least 5 billion people, and the world population is projected to remain under 9 billion until 2033). According to projections of the U.S. population pyramid, 6.88% of U.S. citizens alive today will die in other ways over the course of the next decade.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk0isr41eoxl\"><sup><a href=\"#fnk0isr41eoxl\">[10]</a></sup></span>&nbsp;That suggests that U.S. citizens alive today have on average about a 1% risk of being killed in a nuclear war, engineered pandemic, or AI disaster in the next decade. That is about ten times their risk of being killed in a car accident.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhz139osjyd\"><sup><a href=\"#fnhz139osjyd\">[11]</a></sup></span></p><h1><strong>3. Interventions to reduce the risk</strong></h1><p>There is good reason to think that the risk of global catastrophe in the coming years is significant. Based on Ord\u2019s estimates, we suggest that U.S. citizens\u2019 risk of dying in a nuclear war, pandemic, or AI disaster in the next decade is on average about 1%. We now survey some ways of reducing this risk.</p><p>The Biden administration\u2019s 2023 Budget lists many ways of reducing the risk of biological catastrophes (The White House 2022c; U.S. Office of Management and Budget 2022). These include developing advanced personal protective equipment, along with prototype vaccines for the viral families most likely to cause pandemics.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbce5vwjjant\"><sup><a href=\"#fnbce5vwjjant\">[12]</a></sup></span>&nbsp;The U.S. government can also enhance laboratory biosafety and biosecurity, by improving training procedures, risk assessments, and equipment (Bipartisan Commission on Biodefense 2021: 24). Another priority is improving our capacities for microbial forensics (including our abilities to detect engineered pathogens), so that we can better identify and deter potential bad actors (Bipartisan Commission on Biodefense 2021: 24\u201325). Relatedly, the U.S. government can strengthen the Biological Weapons Convention by increasing the budget and staff of the body responsible for its implementation, and by working to grant them the power to investigate suspected breaches (Ord 2020: 279\u201380). The Nuclear Threat Initiative recommends establishing a global entity focused on preventing catastrophes from biotechnology, amongst other things (Nuclear Threat Initiative 2020a: 3). Another key priority is developing pathogen-agnostic detection technologies. One such candidate technology is a Nucleic Acid Observatory, which would monitor waterways and wastewater for changing frequencies of biological agents, allowing for the early detection of potential biothreats (The Nucleic Acid Observatory Consortium 2021).</p><p>The U.S. government can also reduce the risk of nuclear war this decade. Ord (2020: 278) recommends restarting the Intermediate-Range Nuclear Forces Treaty, taking U.S. intercontinental ballistic missiles off of hair-trigger alert (\u201cLaunch on Warning\u201d), and increasing the capacity of the International Atomic Energy Agency to verify that nations are complying with safeguards agreements. Other recommendations come from the Centre for Long-Term Resilience\u2019s <i>Future Proof </i>report (2021). They are directed towards the U.K. government but apply to the U.S. as well. The recommendations include committing not to incorporate AI systems into nuclear command, control, and communications (NC3) and lobbying to establish this norm internationally.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreff8lccgruonu\"><sup><a href=\"#fnf8lccgruonu\">[13]</a></sup></span>&nbsp;Another is committing to avoid cyber operations that target the NC3 of Non-Proliferation Treaty signatories and establishing a multilateral agreement to this effect. The Nuclear Threat Initiative (2020b) offers many recommendations to the Biden administration for reducing nuclear risk, some of which have already been taken up.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6i6fiq643ps\"><sup><a href=\"#fn6i6fiq643ps\">[14]</a></sup></span>&nbsp;Others include working to bring the Comprehensive Nuclear-Test-Ban Treaty into force, re-establishing the Joint Comprehensive Plan of Action\u2019s limits on Iran\u2019s nuclear activity, and increasing U.S. diplomatic efforts with Russia and China (Nuclear Threat Initiative 2020b).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2k7zv6r5vgh\"><sup><a href=\"#fn2k7zv6r5vgh\">[15]</a></sup></span></p><p>To reduce the risks from AI, the U.S. government can fund research in AI safety. This should include alignment research focused on reducing the risk of catastrophic AI takeover by ensuring that even very powerful AI systems do what we intend, as well as interpretability research to help us understand neural networks\u2019 behaviour and better supervise their training (Amodei et al. 2016; Hendrycks et al. 2022). The U.S. government can also fund research and work in AI governance, focused on devising norms, policies, and institutions to ensure that the development of AI is beneficial for humanity (Dafoe 2018).</p><h1><strong>4. Cost-benefit analysis of catastrophe-preventing interventions</strong></h1><p>We project that funding this suite of interventions for the next decade would cost less than $400 billion.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefu76fcclx80r\"><sup><a href=\"#fnu76fcclx80r\">[16]</a></sup></span>&nbsp;We also&nbsp;expect this suite of interventions to reduce the risk of global catastrophe over the next decade by at least 0.1pp (percentage points). A full defence of this claim would require more detail than we can fit in this chapter, but here is one way to illustrate the claim\u2019s plausibility. Imagine an enormous set of worlds like our world in 2023. Each world in this set is different with respect to the features of our world about which we are uncertain, and worlds with a certain feature occur in the set in proportion to our best evidence about the presence of that feature in our world. If, for example, the best appraisal of our available evidence suggests that there is a 55% probability that the next U.S. President will be a Democrat, then 55% of the worlds in our set have a Democrat as the next President. We claim that <i>in at least 1-in-1,000 of these worlds</i>&nbsp;the interventions we recommend above would prevent a global catastrophe this decade. That is a low bar, and it seems plausible to us that the interventions above meet it. Our question now is: given this profile of costs and benefits, do these interventions pass a standard cost-benefit analysis test?</p><p>To assess interventions expected to save lives, cost-benefit analysis begins by <i>valuing mortality risk reductions</i>: putting a monetary value on reducing citizens\u2019 risk of death (Kniesner and Viscusi 2019). To do that, we first determine how much a representative sample of citizens are willing to pay to reduce their risk of dying this year by a given increment (often around 0.01pp, or 1-in-10,000). One method is to ask them, giving us their stated preferences. Another method is to observe people\u2019s behaviour, particularly their choices about what to buy and what jobs to take, giving us their revealed preferences.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbz7c7t37zb8\"><sup><a href=\"#fnbz7c7t37zb8\">[17]</a></sup></span>&nbsp;</p><p>U.S. government agencies use methods like these to estimate how much U.S. citizens are willing to pay to reduce their risk of death.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4wnmjrx05ud\"><sup><a href=\"#fn4wnmjrx05ud\">[18]</a></sup></span>&nbsp;This figure is then used to calculate the <i>value of a statistical life </i>(VSL): the value of saving one life in expectation via small reductions in mortality risks for many people. The primary VSL figure used by the U.S. Department of Transportation for 2021 is $11.8 million, with a range to account for various kinds of uncertainty spanning from about $7 million to $16.5 million (U.S. Department of Transportation 2021a, 2021b).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref08d44a0ug6s3\"><sup><a href=\"#fn08d44a0ug6s3\">[19]</a></sup></span>&nbsp;These figures are used in the cost-benefit analyses of policies expected to save lives. Costs and benefits occurring in the future are discounted at a constant annual rate. The Environmental Protection Agency (EPA) uses annual discount rates of 2% and 3%; the Office of Information and Regulatory Affairs (OIRA) instructs agencies to conduct analyses using annual discount rates of 3% and 7% (Graham 2008: 504). The rationale is opportunity costs and people\u2019s rate of pure time preference (Graham 2008: 504).</p><p>Now for the application to the risk of global catastrophe (otherwise known as <i>global catastrophic risk</i>, or <i>GCR</i>). We defined a global catastrophe above as an event that kills at least 5 billion people, and we assumed that each person\u2019s risk of dying in a global catastrophe is equal. So, given a world population of less than 9 billion and conditional on a global catastrophe occurring, each American\u2019s risk of dying in that catastrophe is at least 5/9. Reducing GCR this decade by 0.1pp then reduces each American\u2019s risk of death this decade by at least 0.055pp. Multiplying that figure by the U.S. population of 330 million, we get the result that reducing GCR this decade by 0.1pp saves at least 181,500 American lives in expectation. If that GCR-reduction were to occur this year, it would be worth at least $1.27 trillion on the Department of Transportation\u2019s lowest VSL figure of $7 million. But since the GCR-reduction would occur over the course of a decade, cost-benefit analysis requires that we discount. If we use OIRA\u2019s highest annual discount rate of 7% and suppose (conservatively) that all the costs of our interventions are paid up front while the GCR-reduction comes only at the end of the decade, we get the result that reducing GCR this decade by 0.1pp is worth at least $1.27 trillion /&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1.07^{10}=\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1.07</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.605em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span></span></span></span></span></span>&nbsp;$646 billion. So, at a cost of $400 billion, these interventions comfortably pass a standard cost-benefit analysis test.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref83dcbq1lmmf\"><sup><a href=\"#fn83dcbq1lmmf\">[20]</a></sup></span>&nbsp;That in turn suggests that the U.S. government should fund these interventions. Doing so would save American lives more cost-effectively than many other forms of government spending on life-saving, such as transportation and environmental regulations.</p><p>In fact, we can make a stronger argument. Using a projected U.S. population pyramid and some life-expectancy statistics, we can calculate that approximately 79% of the American life-years saved by preventing a global catastrophe in 2033 would accrue to Americans alive today in 2023 (<a href=\"https://docs.google.com/spreadsheets/d/1mguFYc06mw2Bdv85Viw6CQq4mt-mqPdiU4mQRi1s0Yo/edit#gid=135523877\">Thornley 2022</a>). 79% of $646 billion is approximately $510 billion. That means that funding this suite of GCR-reducing interventions is well worth it, even considering only the benefits to Americans alive today.</p><p>[EDITED TO ADD: And recall that the above figures assume a conservative 0.1pp reduction in GCR as a result of implementing the whole suite of interventions. We think that a 0.5pp reduction in GCR is a more reasonable estimate, in which case the benefit-cost ratio of the suite is over 5. Making our other assumptions more reasonable results in even more favourable benefit-cost ratios. Using the Department of Transportation\u2019s primary VSL figure of $11.8 million and an annual discount rate of 3%, the benefit-cost ratio of the suite comes out at over 20.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefr82rsvx7t2\"><sup><a href=\"#fnr82rsvx7t2\">[21]</a></sup></span>&nbsp;The most cost-effective interventions within the suite will have benefit-cost ratios that are more favourable still.]</p><p>It is also worth noting some important ways in which our calculations up to this point underrate the value of GCR-reducing interventions. First, we have appealed only to these interventions\u2019 GCR-reducing benefits: the benefits of shifting probability mass away from outcomes in which at least 5 billion people die and towards outcomes in which very few people die. But these interventions would also decrease the risk of smaller catastrophes, in which less than 5 billion people die.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9vc46kzrfy\"><sup><a href=\"#fn9vc46kzrfy\">[22]</a></sup></span>&nbsp;Second, the value of preventing deaths from catastrophe is plausibly higher than the value of preventing traffic deaths. The EPA (2010: 20\u201326) and U.K. Treasury (2003: 62) have each recommended that a higher VSL be used for cancer risks than for accidental risks, to reflect the fact that dying from cancer tends to be more unpleasant than dying in an accident (Kniesner and Viscusi 2019: 16). We suggest that the same point applies to death by nuclear winter and engineered pandemic.</p><p>Here is another benefit of our listed GCR-reducing interventions. They do not just reduce U.S. citizens\u2019 risk of death. They also reduce the risk of death for citizens of other nations. That is additional reason to fund these interventions.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmmytcurf8pc\"><sup><a href=\"#fnmmytcurf8pc\">[23]</a></sup></span>&nbsp;It also suggests that the U.S. government could persuade other nations to share the costs of GCR-reducing interventions, in which case funding these interventions becomes an even more cost-effective way of saving U.S. lives. Cooperation between nations can also make it worthwhile for the U.S. and the world as a whole to spend more on reducing GCR. Suppose, for example, that there is some intervention that would cost $1 trillion and would reduce GCR by 0.1pp over the next decade. That is too expensive for the U.S. alone (at least based on our conservative calculations), but it would be worth funding for a coalition of nations that agreed to split the cost.</p><h1><strong>5. Longtermists should advocate for a CBA-driven catastrophe policy</strong></h1><p>The U.S. is seriously underspending on preventing catastrophes. This conclusion follows from standard cost-benefit analysis. We need not be longtermists to believe that the U.S. government should do much more to reduce the risk of nuclear wars, pandemics, and AI disasters. In fact, even entirely self-interested Americans have reason to hope that the U.S. government increases its efforts to avert catastrophes. The interventions that we recommend above are well worth it, even considering only the benefits to Americans alive today. Counting the benefits to citizens of other nations and the next generation makes these interventions even more attractive. So, Americans should hope that the U.S. government adopts something like a <i>CBA-driven catastrophe policy</i>: a policy of funding all those GCR-reducing interventions that pass a cost-benefit analysis test.</p><p>One might think that longtermists should be more ambitious: that rather than push for a CBA-driven catastrophe policy, longtermists should urge governments to adopt a <i>strong longtermist policy</i>. By a \u2018strong longtermist policy\u2019, we mean a policy founded on the premise that it would be an overwhelming moral loss if future generations never exist.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefe8jboxq3ehm\"><sup><a href=\"#fne8jboxq3ehm\">[24]</a></sup></span>&nbsp;However, we argue that this is not the case: longtermists should advocate for a CBA-driven catastrophe policy rather than a strong longtermist policy. That is because (1) unlike a strong longtermist policy, a CBA-driven policy would be democratically acceptable and feasible to implement, and (2) a CBA-driven policy would reduce existential risk by almost as much as a strong longtermist policy.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefugpwbu65vcs\"><sup><a href=\"#fnugpwbu65vcs\">[25]</a></sup></span></p><p>Let us begin with democratic acceptability. As noted above, a strong longtermist policy would in principle place extreme burdens on the present generation for the sake of even miniscule reductions in existential risk. Here is a rough sketch of why. If the non-existence of future generations would be a overwhelming moral loss, then an existential catastrophe (like human extinction or the permanent collapse of civilization) would be extremely bad. That in turn makes it worth reducing the risk of existential catastrophe even if doing so is exceedingly costly for the present generation.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6hrrfb8asor\"><sup><a href=\"#fn6hrrfb8asor\">[26]</a></sup></span></p><p>We now argue that a strong longtermist policy would place serious burdens on the present generation not only in principle but also in practice. There are suites of existential-risk-reducing interventions that governments could implement only at extreme cost to those alive today. For example, governments could slow down the development of existential-risk-increasing technologies (even those that pose only very small risks) by paying researchers large salaries to do other things. Governments could also build extensive, self-sustaining colonies (in remote locations or perhaps far underground) in which residents are permanently cut off from the rest of the world and trained to rebuild civilization in the event of a catastrophe. The U.S. government could set up a <i>global</i>&nbsp;Nucleic Acid Observatory, paying other countries large fees (if need be) to allow the U.S. to monitor their water supplies for emerging pathogens. More generally, governments could heavily subsidise investment, research, and development in ways that incentivise the present generation to increase civilization\u2019s resilience and decrease existential risk.&nbsp;A strong longtermist policy would seek to implement these and other interventions quickly, a factor which adds to their expense. These expenses would in turn require increasing taxes on present citizens (particularly consumption taxes), as well as cutting forms of government spending that have little effect on existential risk (like Social Security, many kinds of medical care, and funding for parks, art, culture, and sport). These budget changes would be burdensome for those alive today. Very cautious regulation of technological development would impose burdens too. It might mean that present citizens miss out on technologies that would improve and extend their lives, like consumer goods and cures for diseases.</p><p>So, a strong longtermist policy would be <i>democratically unacceptable</i>, by which we mean it could not be adopted and maintained by a democratic government. If a government tried to adopt a strong longtermist policy, it would lose the support of most of its citizens. There are clear moral objections against pursuing democratically unacceptable policies, but even setting those aside, getting governments to adopt a strong longtermist policy is not feasible. Efforts in that direction are very unlikely to succeed.</p><p>A CBA-driven catastrophe policy, by contrast, would be democratically acceptable. This kind of policy would not place heavy burdens on the present generation. Since cost-benefit analysis is based in large part on citizens\u2019 willingness to pay, policies guided by cost-benefit analysis tend not to ask citizens to pay much more than is in their own interests. And given our current lack of spending on preventing catastrophes, moving from the status quo to a CBA-driven policy is almost certainly good for U.S. citizens alive today. That is one reason to think that getting the U.S. government to adopt a CBA-driven policy is particularly feasible. Another is that cost-benefit analysis is already a standard tool for U.S. regulatory decision-making.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4fa07c7xrt4\"><sup><a href=\"#fn4fa07c7xrt4\">[27]</a></sup></span>&nbsp;Advocating for a CBA-driven policy does not mean asking governments to adopt a radically new decision-procedure. It just means asking them to extend a standard decision-procedure into a domain where it has so far been underused.</p><p>Of course, getting governments to adopt a CBA-driven catastrophe policy is not trivial. One barrier is psychological (Wiener 2016). Many of us find it hard to appreciate the likelihood and magnitude&nbsp;of a global catastrophe. Another is that GCR-reduction is a collective action problem for individuals. Although a safer world is in many people\u2019s self-interest, <i>working</i>&nbsp;for a safer world is in few people\u2019s self-interest. Doing so means bearing a large portion of the costs and gaining just a small portion of the benefits.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefx54p04qc4q\"><sup><a href=\"#fnx54p04qc4q\">[28]</a></sup></span>&nbsp;Politicians and regulators likewise lack incentives to advocate for GCR-reducing interventions (as they did with climate interventions in earlier decades). Given widespread ignorance of the risks, calls for such interventions are unlikely to win much public favour.</p><p>However, these barriers can be overcome. Those willing to bear costs for the sake of others can use their time and money to make salient the prospect of global catastrophe, thereby fostering public support for GCR-reducing interventions and placing them on the policy agenda. Longtermists \u2013 who care about the present generation as well as future generations \u2013 are well-suited to play this role in pushing governments to adopt a CBA-driven catastrophe policy. If they take up these efforts, they have a good chance of succeeding.</p><p>Now for the second point: getting the U.S. government to adopt a CBA-driven catastrophe policy would reduce existential risk by almost as much as getting them to adopt a strong longtermist policy. This is for two reasons. The first is that, at the current margin, the primary goals of a CBA-driven policy and a strong longtermist policy are substantially aligned. The second is that increased spending on preventing catastrophes yields steeply diminishing returns in terms of existential-risk-reduction.</p><p>Let us begin with substantial alignment. The primary goal of a CBA-driven catastrophe policy is saving lives in the near-term. The primary goal of a strong longtermist policy is reducing existential risk. In the world as it is today, these goals are aligned: many of the best interventions for reducing existential risk are also cost-effective interventions for saving lives in the near-term. Take AI, for example. Per Ord (2020: 167) and many other longtermists, the risk from AI makes up a large portion of the total existential risk this century, and this risk could be reduced significantly by work on AI safety and governance. That places this work high on many longtermists\u2019 list of priorities. We have argued above that a CBA-driven policy would also fund this work, since it is a cost-effective way of saving lives in the near-term. The same goes for pandemics. Interventions to thwart potential pandemics rank highly on the longtermist list of priorities, and these interventions would also be implemented by a CBA-driven policy.</p><p>We illustrate the alignment between a CBA-driven policy and a strong longtermist policy using the graph below. The x-axis represents U.S. lives saved (discounted by how far in the future the life is saved) in expectation per dollar. The y-axis represents existential-risk-reduction per dollar. Interventions to the right of the blue line would be funded by a CBA-driven catastrophe policy. The exact position of each intervention is provisional and unimportant, and the graph is not to scale in any case. The important point is that a CBA-driven policy would fund many of the best interventions for reducing existential risk.&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1677273514/mirroredImages/DiGL5FuLgWActPBsf/wb1ktpomro91f8q2lixz.png\" alt=\"A picture containing graphical user interface\n\nDescription automatically generated\"></p><p>That is the key alignment between a CBA-driven policy and a strong longtermist policy. Now for three potentially significant differences. The first is that a strong longtermist policy would fund what we call <i>pure longtermist goods</i>: goods that do not much benefit present people but improve humanity\u2019s long-term prospects. These pure longtermist goods include refuges to help humanity recover from catastrophes. The second difference is that a strong longtermist policy would spend much more on preventing catastrophes than a CBA-driven policy. In addition to the interventions warranted by a CBA-driven catastrophe policy, a strong longtermist policy would also fund catastrophe-preventing interventions that are too expensive to pass a cost-benefit analysis test. The third difference concerns nuclear risks. The risk of a full-scale nuclear war is significantly higher than the risk of a nuclear war constituting an existential catastrophe&nbsp;(5% versus 0.1% this century, per Ord). In part for this reason, interventions to reduce nuclear risk are cost-effective for saving lives in the near-term&nbsp;but not so cost-effective for reducing existential risk.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdakoog5v4ss\"><sup><a href=\"#fndakoog5v4ss\">[29]</a></sup></span>&nbsp;That makes these interventions a relatively lower priority by the lights of a strong longtermist policy than they are by the lights of a CBA-driven policy. Holding fixed the catastrophe-budget warranted by cost-benefit analysis, a strong longtermist policy would likely shift some funding away from nuclear interventions and towards AI and pandemic interventions that fail a cost-benefit analysis test.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9law4kh303c\"><sup><a href=\"#fn9law4kh303c\">[30]</a></sup></span></p><p>Set aside pure longtermist goods for now. We discuss them in the next section. Consider instead the fact that a strong longtermist policy would spend considerably more on preventing catastrophes (especially AI and biological catastrophes) than a CBA-driven policy. We argue that this extra spending would not make such a significant difference to existential risk, because increased spending on preventing catastrophes yields steeply diminishing returns in terms of existential-risk-reduction. That in turn is for two primary reasons. The first is that the most promising existential-risk-reducing interventions \u2013 for example, AI safety and governance, a Nucleic Acid Observatory, enhanced biosecurity and biosafety practices \u2013 pass a cost-benefit analysis test. Those catastrophe-preventing interventions that fail a cost-benefit analysis test are not nearly as effective in reducing existential risk.</p><p>Here is a second reason to expect increased spending to yield steeply diminishing returns in terms of existential-risk-reduction: many interventions <i>undermine</i>&nbsp;each other. What we mean here is that many interventions render other interventions less effective, so that the total existential-risk-reduction gained by funding some sets of interventions is less than the sum of the existential-risk-reduction gained by funding each intervention individually. Consider an example. Setting aside a minor complication, we can decompose existential risk from engineered pathogens into two factors: the risk that an engineered pathogen infects more than 1,000 people, and the risk of an existential catastrophe given that an engineered pathogen infects more than 1,000 people.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref510mud6zv08\"><sup><a href=\"#fn510mud6zv08\">[31]</a></sup></span>&nbsp;Suppose (for the sake of illustration only) that each risk is 10% this decade, that incentivising the world\u2019s biomedical researchers to do safer research would halve the first risk, and that establishing a Nucleic Acid Observatory (NAO) would halve the second risk. Then in the absence of any interventions, existential risk this decade from engineered pathogens is 1%. Only incentivising safe research would reduce existential risk by 0.5%. Only establishing an NAO would reduce existential risk by 0.5%. But incentivising safe research <i>after</i>&nbsp;establishing an NAO reduces existential risk by just 0.25%. More generally, the effectiveness of existential-risk-reducing interventions that fail a cost-benefit analysis test would be substantially undermined by all those interventions that pass a cost-benefit analysis test.</p><p>At the moment, the world is spending very little on preventing global catastrophes. The U.S. spent approximately $3 billion on biosecurity in 2019 (Watson et al. 2018), and (in spite of the wake-up call provided by COVID-19) funding for preventing future pandemics has not increased much since then.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyx75t39r0w\"><sup><a href=\"#fnyx75t39r0w\">[32]</a></sup></span>&nbsp;Much of this spending is ill-suited to combatting the most extreme biological threats. Spending on reducing GCR from AI is less than $100 million per year.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa7t0jphf8mu\"><sup><a href=\"#fna7t0jphf8mu\">[33]</a></sup></span>&nbsp;So, there is a lot of low-hanging fruit for governments to pick: given the current lack of spending, moving to a CBA-driven catastrophe policy would significantly decrease existential risk. Governments could reduce existential risk further by moving to a strong longtermist policy, but this extra reduction would be comparatively small. The same goes for shifting funding away from nuclear risk and towards AI and pandemic risks while holding fixed the level of spending on catastrophe-prevention warranted by cost-benefit analysis. This shift would have just a small effect on existential risk, because the best interventions for reducing AI and pandemic risks would already have been funded by a CBA-driven policy.</p><p>And, as noted above, international cooperation would make even more catastrophe-preventing interventions cost-effective enough to pass a cost-benefit analysis test. Some of these extra interventions would also have non-trivial effects on existential risk. Consider climate change. Some climate interventions are too expensive to be in any nation\u2019s self-interest to fund unilaterally, but are worth funding for a coalition of nations that agree to coordinate. Transitioning from fossil fuels to renewable energy sources is one example. Climate change is also an <i>existential risk factor</i>: a factor that increases existential risk. Besides posing a small risk of directly causing human extinction or the permanent collapse of civilization, climate change poses a significant indirect risk. It threatens to exacerbate international conflict and drive humanity to pursue risky technological solutions. Extreme climate change would also damage our resilience and make us more vulnerable to other catastrophes. So, in addition to its other benefits, mitigating climate change decreases existential risk. Since more climate interventions pass a cost-benefit analysis test if nations agree to coordinate, this kind of international cooperation would further shrink the gap between existential risk on a CBA-driven catastrophe policy versus a strong longtermist policy.</p><h1><strong>6. Pure longtermist goods and altruistic willingness to pay</strong></h1><p>There remains one potentially important difference between a CBA-driven catastrophe policy and a strong longtermist policy: a strong longtermist policy will provide significant funding for what we call <i>pure longtermist goods</i>. These we define as goods that do not much benefit the present generation but improve humanity\u2019s long-term prospects. They include especially <i>refuges</i>: large, well-equipped structures akin to bunkers or shelters, designed to help occupants survive future catastrophes and later rebuild civilization.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefg4getjlqwl\"><sup><a href=\"#fng4getjlqwl\">[34]</a></sup></span>&nbsp;It might seem like a CBA-driven catastrophe policy would provide no funding for pure longtermist goods, because they are not particularly cost-effective for saving lives in the near-term. In the event of a serious catastrophe, refuges would save at most a small portion of the people alive today. But a strong longtermist policy would invest in refuges, because they would significantly reduce existential risk. Even a relatively small group of survivors could get humanity back on track, in which case an existential catastrophe \u2013 the permanent destruction of humanity\u2019s long-term potential \u2013 will have been averted. Since a strong longtermist policy would provide funding for refuges, it might seem as if adopting a strong longtermist policy would reduce existential risk by significantly more than adopting a CBA-driven policy.</p><p>However, even this difference between a CBA-driven policy and a strong longtermist policy need not be so great. That is because cost-benefit analysis should incorporate (and is beginning to incorporate) citizens\u2019 willingness to pay to uphold their moral commitments: what we will call their <i>altruistic willingness to pay </i>(AWTP). Posner and Sunstein (2017) offer arguments to this effect. They note that citizens have various moral commitments \u2013 concerning the natural world, non-human animals, citizens of other nations, future generations, etc. \u2013 and suffer welfare losses when these commitments are compromised (2017: 1829\u201330).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcrjj4bvzzym\"><sup><a href=\"#fncrjj4bvzzym\">[35]</a></sup></span>&nbsp;They argue that the best way to measure these losses is by citizens\u2019 willingness to pay to uphold their moral commitments, and that this willingness to pay should be included in cost-benefit calculations of proposed regulations (2017: 1830).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref123cjmrjdt8h\"><sup><a href=\"#fn123cjmrjdt8h\">[36]</a></sup></span>&nbsp;Posner and Sunstein also note that there is regulatory and legal precedent for doing so (2017: sec. 3).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6vxpau7pszj\"><sup><a href=\"#fn6vxpau7pszj\">[37]</a></sup></span></p><p>And here, we believe, is where longtermism should enter into government catastrophe policy. Longtermists should make the case for their view, and thereby increase citizens\u2019 AWTP for pure longtermist goods like refuges.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvnvpypgizy\"><sup><a href=\"#fnvnvpypgizy\">[38]</a></sup></span>&nbsp;When citizens are willing to pay for these goods, governments should fund them.</p><p>Although the uptake of new moral movements is hard to predict (Sunstein 2020), we have reason to be optimistic about this kind of longtermist outreach. A recent survey suggests that many people have moral intuitions that might incline them towards a weak form of longtermism: respondents tended to judge that it\u2019s good to create happy people (Caviola et al. 2022: 9). Another survey indicates that simply making the future salient has a marked effect on people\u2019s views about human extinction. When prompted to consider long-term consequences, the proportion of people who judged human extinction to be uniquely bad relative to near-extinction rose from 23% to 50% (Schubert, Caviola, and Faber 2019: 3\u20134). And when respondents were asked to suppose that life in the future would be much better than life today, that number jumped to 77% (Schubert et al. 2019: 4). In the span of about six decades, environmentalism has grown from a fringe movement to a major moral priority of our time. Like longtermism, it has been motivated in large part by a concern for future generations. Longtermist arguments have already been compelling to many people, and these factors suggest that they could be compelling to many more.</p><p>Even a small AWTP for pure longtermist goods could have a significant effect on existential risk. If U.S. citizens are willing to contribute just $5 per year on average, then a CBA-driven policy that incorporates AWTP warrants spending up to $1.65 billion per year on pure longtermist goods: enough to build extensive refuges. Of course, even in a scenario in which every U.S. citizen hears the longtermist arguments, a CBA-driven policy will provide less funding for pure longtermist goods than a strong longtermist policy. But, as with catastrophe-preventing interventions, it seems likely that marginal existential-risk-reduction diminishes steeply as spending on pure longtermist goods increases: so steeply that moving to the level of spending on pure longtermist goods warranted by citizens\u2019 AWTP would reduce existential risk by almost as much as moving to the level of spending warranted by a strong longtermist policy. This is especially so if multiple nations offer to fund pure longtermist goods in line with their citizens\u2019 AWTP.</p><p>Here is a final point to consider. One might think that it is true only <i>on the current margin </i>and <i>in public </i>that longtermists should push governments to adopt a catastrophe policy guided by cost-benefit analysis and altruistic willingness to pay. Once all the interventions justified by CBA-plus-AWTP have been funded, longtermists should lobby for even more government spending on preventing catastrophes. And in the meantime, longtermists should in private advocate for governments to fund existential-risk-reducing interventions that go beyond CBA-plus-AWTP.</p><p>We disagree. Longtermists can try to increase government funding for catastrophe-prevention by making longtermist arguments and thereby increasing citizens\u2019 AWTP, but they should not urge governments to depart from a CBA-plus-AWTP catastrophe policy. On the contrary, longtermists should as far as possible commit themselves to acting in accordance with a CBA-plus-AWTP policy in the political sphere. One reason why is simple: longtermists have moral reasons to respect the preferences of their fellow citizens.</p><p>To see another reason why, note first that longtermists working to improve government catastrophe policy could be a win-win. The present generation benefits because longtermists solve the collective action problem: they work to implement interventions that cost-effectively reduce everyone\u2019s risk of dying in a catastrophe. Future generations benefit because these interventions also reduce existential risk. But as it stands the present generation may worry that longtermists would go too far. If granted imperfectly accountable power, longtermists might try to use the machinery of government to place burdens on the present generation for the sake of further benefits to future generations. These worries may lead to the marginalisation of longtermism, and thus an outcome that is worse for both present and future generations.</p><p>The best solution is compromise and commitment.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0ko16dwc14o\"><sup><a href=\"#fn0ko16dwc14o\">[39]</a></sup></span>&nbsp;A CBA-plus-AWTP policy \u2013 founded as it is on citizens\u2019 preferences \u2013 is acceptable to a broad coalition of people. As a result, longtermists committing to act in accordance with a CBA-plus-AWTP policy makes possible an arrangement that is significantly better than the status quo, both by longtermist lights and by the lights of the present generation. It also gives rise to other benefits of cooperation. For example, it helps to avoid needless conflicts in which groups lobby for opposing policies, with some substantial portion of the resources that they spend cancelling each other out (see Ord 2015: 120\u201321, 135). With a CBA-plus-AWTP policy in place, those resources can instead be spent on interventions that are appealing to all sides.</p><p>There are many ways in which longtermists can increase and demonstrate their commitment to this kind of win-win compromise policy. They can speak in favour of it now, and act in accordance with it in the political sphere. They can also support efforts to embed a CBA-plus-AWTP criterion into government decision-making \u2013 through executive orders, regulatory statutes, and law \u2013 thereby ensuring that governments spend neither too much nor too little on benefits to future generations. Longtermists can also earn a reputation for cooperating well with others, by supporting interventions and institutions that are appealing to a broad range of people. In doing so, longtermists make possible a form of cooperation which is substantially beneficial to both the present generation and the long-term future.</p><h1><strong>7. Conclusion</strong></h1><p>Governments should be spending much more on averting threats from nuclear war, engineered pandemics, and AI. This conclusion follows from standard cost-benefit analysis. We need not assume longtermism, or even that future generations matter. In fact, even entirely self-interested Americans have reason to hope that the U.S. government adopts a catastrophe policy guided by cost-benefit analysis.</p><p>Longtermists should push for a similar goal: a government catastrophe policy guided by cost-benefit analysis and citizens\u2019 altruistic willingness to pay. This policy is achievable and would be democratically acceptable. It would also reduce existential risk by almost as much as a strong longtermist policy. This is especially so if longtermists succeed in making the long-term future a major moral priority of our time and if citizens\u2019 altruistic willingness to pay for benefits to the long-term future increases commensurately. Longtermists should commit to acting in accordance with a CBA-plus-AWTP policy in the political sphere. This commitment would help bring about a catastrophe policy that is much better than the status quo, for the present generation and long-term future alike.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9f59gy0bpda\"><sup><a href=\"#fn9f59gy0bpda\">[40]</a></sup></span></p><h1><strong>8. References</strong></h1><p>Aldy, J. E., and Viscusi, W. K. (2008). \u2018Adjusting the Value of a Statistical Life for Age and Cohort Effects\u2019, in <i>Review of Economics and Statistics</i> <i>90</i>(3): 573\u2013581.</p><p>Amadae, S. M., and Avin, S. (2019). Autonomy and Machine Learning as Risk Factors at the Interface of Nuclear Weapons, Computers and People. In V. Boulanin (Ed.), <i>The Impact of Artificial Intelligence on Strategic Stability and Nuclear Risk</i>&nbsp;(Vols 1, Euro-Atlantic Perspectives, pp. 105\u2013118).</p><p>Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Man\u00e9, D. (2016). \u2018Concrete Problems in AI Safety\u2019, <i>arXiv</i>. <a href=\"http://arxiv.org/abs/1606.06565\">http://arxiv.org/abs/1606.06565</a>&nbsp;</p><p>Armstrong, S., Bostrom, N., and Shulman, C. (2016). \u2018Racing to the precipice: A model of artificial intelligence development\u2019, in <i>AI &amp; Society</i> <i>31</i>(2): 201\u2013206.</p><p>Baum, S. D. (2015). \u2018The far future argument for confronting catastrophic threats to humanity: Practical significance and alternatives\u2019, in <i>Futures</i> <i>72</i>: 86\u201396.</p><p>Beckstead, N. (2013). <i>On the Overwhelming Importance of Shaping the Far Future</i>&nbsp;[PhD Thesis, Rutgers University]. <a href=\"http://dx.doi.org/doi:10.7282/T35M649T\">http://dx.doi.org/doi:10.7282/T35M649T</a>&nbsp;</p><p>Beckstead, N. (2015). \u2018How much could refuges help us recover from a global catastrophe?\u2019, in <i>Futures</i> <i>72</i>: 36\u201344.</p><p>Bipartisan Commission on Biodefense. (2021). <i>The Apollo Program for Biodefense: Winning the Race Against Biological Threats</i>. Bipartisan Commission on Biodefense. <a href=\"https://biodefensecommission.org/wp-content/uploads/2021/01/Apollo_report_final_v8_033121_web.pdf\">https://biodefensecommission.org/wp-content/uploads/2021/01/Apollo_report_final_v8_033121_web.pdf</a>&nbsp;</p><p>Bipartisan Commission on Biodefense. (2022). <i>The Athena Agenda: Advancing the Apollo Program for Biodefense</i>. Bipartisan Commission on Biodefense. <a href=\"https://biodefensecommission.org/wp-content/uploads/2022/04/Athena-Report_v7.pdf\">https://biodefensecommission.org/wp-content/uploads/2022/04/Athena-Report_v7.pdf</a>&nbsp;</p><p>Bostrom, N. (2013). \u2018Existential Risk Prevention as Global Priority\u2019, in <i>Global Policy</i> <i>4</i>(1): 15\u201331.</p><p>Bostrom, N., Douglas, T., and Sandberg, A. (2016). \u2018The Unilateralist\u2019s Curse and the Case for a Principle of Conformity\u2019, in <i>Social Epistemology</i> <i>30</i>(4): 350\u2013371.</p><p>Carlsmith, J. (2021). \u2018Is Power-Seeking AI an Existential Risk?\u2019, <i>arXiv</i>. <a href=\"http://arxiv.org/abs/2206.13353\">http://arxiv.org/abs/2206.13353</a>&nbsp;</p><p>Caviola, L., Althaus, D., Mogensen, A. L., and Goodwin, G. P. (2022). \u2018Population ethical intuitions\u2019, in <i>Cognition</i> <i>218</i>: 104941.</p><p>Centre for Long-Term Resilience. (2021). <i>Future Proof: The Opportunity to Transform the UK\u2019s Resilience to Extreme Risks</i>. <a href=\"https://11f95c32-710c-438b-903d-da4e18de8aaa.filesusr.com/ugd/e40baa_c64c0d7b430149a393236bf4d26cdfdd.pdf\">https://11f95c32-710c-438b-903d-da4e18de8aaa.filesusr.com/ugd/e40baa_c64c0d7b430149a393236bf4d26cdfdd.pdf</a>&nbsp;</p><p>Claxton, K., Ochalek, J., Revill, P., Rollinger, A., and Walker, D. (2016). \u2018Informing Decisions in Global Health: Cost Per DALY Thresholds and Health Opportunity Costs\u2019. University of York Centre for Health Economics. <a href=\"https://www.york.ac.uk/media/che/documents/policybriefing/Cost%20per%20DALY%20thresholds.pdf\">https://www.york.ac.uk/media/che/documents/policybriefing/Cost%20per%20DALY%20thresholds.pdf</a>&nbsp;</p><p>Cotra, A. (2020). \u2018Forecasting Transformative AI with Biological Anchors, Part 4: Timelines estimates and responses to objections\u2019. <a href=\"https://docs.google.com/document/d/1cCJjzZaJ7ATbq8N2fvhmsDOUWdm7t3uSSXv6bD0E_GM\">https://docs.google.com/document/d/1cCJjzZaJ7ATbq8N2fvhmsDOUWdm7t3uSSXv6bD0E_GM</a>&nbsp;</p><p>Cotra, A. (2022). \u2018Two-year update on my personal AI timelines\u2019. <i>AI Alignment Forum</i>. <a href=\"https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines\">https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines</a>&nbsp;</p><p>Coupe, J., Bardeen, C. G., Robock, A., and Toon, O. B. (2019). \u2018Nuclear Winter Responses to Nuclear War Between the United States and Russia in the Whole Atmosphere Community Climate Model Version 4 and the Goddard Institute for Space Studies ModelE\u2019, in <i>Journal of Geophysical Research: Atmospheres</i> <i>124</i>(15): 8522\u20138543.</p><p>Cutler, D. M., and Summers, L. H. (2020). \u2018The COVID-19 Pandemic and the $16 Trillion Virus\u2019, in <i>Journal of the American Medical Association</i> <i>324</i>(15): 1495\u20131496.</p><p>Dafoe, A. (2018). \u2018AI Governance: A Research Agenda\u2019. Future of Humanity Institute, University of Oxford. <a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf\">https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf</a>&nbsp;</p><p>DeepMind. (2020). \u2018Specification gaming: The flip side of AI ingenuity\u2019. DeepMind. <a href=\"https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity\">https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity</a>&nbsp;</p><p>Executive Order No. 12,291, Code of Federal Regulations, Title 3 127 (1982). <a href=\"https://www.govinfo.gov/app/details/CFR-2012-title3-vol1/CFR-2012-title3-vol1-eo13563/summary\">https://www.govinfo.gov/app/details/CFR-2012-title3-vol1/CFR-2012-title3-vol1-eo13563/summary</a>&nbsp;</p><p>Executive Order No. 13,563, Code of Federal Regulations, Title 3 215 (2012). <a href=\"https://www.archives.gov/federal-register/codification/executive-order/12291.html\">https://www.archives.gov/federal-register/codification/executive-order/12291.html</a>&nbsp;</p><p>Favaloro, P., and Berger, A. (2021). \u2018Technical Updates to Our Global Health and Wellbeing Cause Prioritization Framework\u2014Open Philanthropy\u2019. <i>Open Philanthropy</i>. <a href=\"https://www.openphilanthropy.org/research/technical-updates-to-our-global-health-and-wellbeing-cause-prioritization-framework/\">https://www.openphilanthropy.org/research/technical-updates-to-our-global-health-and-wellbeing-cause-prioritization-framework/</a>&nbsp;</p><p>Grace, K., Salvatier, J., Dafoe, A., Zhang, B., and Evans, O. (2018). \u2018When Will AI Exceed Human Performance? Evidence from AI Experts\u2019 in <i>Journal of Artificial Intelligence Research</i>, <i>62</i>, 729\u2013754.</p><p>Graham, J. D. (2008). \u2018Saving Lives through Administrative Law and Economics\u2019, in <i>University of Pennsylvania Law Review</i> <i>157</i>(2): 395\u2013540.</p><p>Greaves, H., and MacAskill, W. (2021). \u2018The Case for Strong Longtermism\u2019. <i>GPI Working Paper</i>, <i>No. 5-2021.</i>&nbsp;<a href=\"https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism/\">https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism/</a>&nbsp;</p><p>Hellman, M. E. (2008). \u2018Risk Analysis of Nuclear Deterrence\u2019, in <i>The Bent of Tau Beta Pi</i> <i>99</i>(2): 14\u201322.</p><p>Hendrycks, D., Carlini, N., Schulman, J., and Steinhardt, J. (2022). \u2018Unsolved Problems in ML Safety\u2019. <i>arXiv</i>. <a href=\"http://arxiv.org/abs/2109.13916\">http://arxiv.org/abs/2109.13916</a>&nbsp;</p><p>Hirth, R. A., Chernew, M. E., Miller, E., Fendrick, A. M., and Weissert, W. G. (2000). \u2018Willingness to pay for a quality-adjusted life year: In search of a standard\u2019, in <i>Medical Decision Making: An International Journal of the Society for Medical Decision Making</i> <i>20</i>(3): 332\u2013342.</p><p>H.R.5376\u2014Inflation Reduction Act of 2022, (2022). <a href=\"https://www.congress.gov/bill/117th-congress/house-bill/5376\">https://www.congress.gov/bill/117th-congress/house-bill/5376</a>&nbsp;</p><p>Jebari, K. (2015). \u2018Existential Risks: Exploring a Robust Risk Reduction Strategy\u2019, in <i>Science and Engineering Ethics</i> <i>21</i>(3): 541\u2013554.</p><p>Kniesner, T. J., and Viscusi, W. K. (2019). \u2018The Value of a Statistical Life\u2019, in <i>Oxford Research Encyclopedia of Economics and Finance</i>. Oxford University Press. <a href=\"https://oxfordre.com/economics/view/10.1093/acrefore/9780190625979.001.0001/acrefore-9780190625979-e-138\">https://oxfordre.com/economics/view/10.1093/acrefore/9780190625979.001.0001/acrefore-9780190625979-e-138</a>&nbsp;</p><p>Krakovna, V. (2018). \u2018Specification gaming examples in AI\u2019. <i>Victoria Krakovna</i>. <a href=\"https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/\">https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/</a>&nbsp;</p><p>Langosco, L., Koch, J., Sharkey, L., Pfau, J., Orseau, L., and Krueger, D. (2022). \u2018Goal Misgeneralization in Deep Reinforcement Learning\u2019. <i>arXiv</i>. <a href=\"http://arxiv.org/abs/2105.14111\">http://arxiv.org/abs/2105.14111</a>&nbsp;</p><p>Lehman, J., Clune, J., Misevic, D., Adami, C., Altenberg, L., Beaulieu, J., Bentley, P. J., Bernard, S., Beslon, G., Bryson, D. M., Cheney, N., Chrabaszcz, P., Cully, A., Doncieux, S., Dyer, F. C., Ellefsen, K. O., Feldt, R., Fischer, S., Forrest, S., \u2026 Yosinski, J. (2020). \u2018The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities\u2019, in <i>Artificial Life</i> <i>26</i>(2): 274\u2013306.</p><p>MacAskill, W. (2022). <i>What We Owe The Future: A Million-Year View</i>&nbsp;(Oneworld).</p><p>Matheny, J. G. (2007). \u2018Reducing the Risk of Human Extinction\u2019, in <i>Risk Analysis</i> <i>27</i>(5): 1335\u20131344.</p><p>Metaculus. (2022a). \u2018By 2100, will the human population decrease by at least 10% during any period of 5 years?\u2019. <i>Metaculus</i>. <a href=\"https://www.metaculus.com/questions/1493/global-population-decline-10-by-2100/\">https://www.metaculus.com/questions/1493/global-population-decline-10-by-2100/</a>&nbsp;</p><p>Metaculus. (2022b). \u2018If a global catastrophe occurs, will it be due to biotechnology or bioengineered organisms?\u2019. <i>Metaculus</i>. <a href=\"https://www.metaculus.com/questions/1502/ragnar%25C3%25B6k-question-series-if-a-global-catastrophe-occurs-will-it-be-due-to-biotechnology-or-bioengineered-organisms/\">https://www.metaculus.com/questions/1502/ragnar%25C3%25B6k-question-series-if-a-global-catastrophe-occurs-will-it-be-due-to-biotechnology-or-bioengineered-organisms/</a>&nbsp;</p><p>Metaculus. (2022c). \u2018Will there be a global thermonuclear war by 2070?\u2019. <i>Metaculus</i>. <a href=\"https://www.metaculus.com/questions/3517/will-there-be-a-global-thermonuclear-war-by-2070/\">https://www.metaculus.com/questions/3517/will-there-be-a-global-thermonuclear-war-by-2070/</a>&nbsp;</p><p>Michigan, et al. &nbsp;V. Environmental Protection Agency, et al. (No. 14-46); Utility Air Regulatory Group v. Environmental Protection Agency, et al. (No. 14-47); National Mining Association v. Environmental Protection Agency, et al. (No. 14-49), No. 14-46 (135 Supreme Court of the United States 2699 29 July 2015).</p><p>Millett, P., and Snyder-Beattie, A. (2017). \u2018Existential Risk and Cost-Effective Biosecurity\u2019, in <i>Health Security</i> <i>15</i>(4): 373\u2013383.</p><p>Mills, M. J., Toon, O. B., Lee-Taylor, J. M., and Robock, A. (2014). \u2018Multi-Decadal Global Cooling and Unprecedented Ozone Loss Following a Regional Nuclear Conflict\u2019, in <i>Earth\u2019s Future</i> <i>2</i>(4): 161\u2013176.</p><p>Muehlhauser, L. (2021). \u2018Treacherous turns in the wild\u2019. <a href=\"https://lukemuehlhauser.com/treacherous-turns-in-the-wild/\">https://lukemuehlhauser.com/treacherous-turns-in-the-wild/</a>&nbsp;</p><p>Murphy, T. (2013). \u2018The First Level of Super Mario Bros is Easy with Lexicographic Orderings and Time Travel\u2019. <a href=\"http://www.cs.cmu.edu/~tom7/mario/mario.pdf\">http://www.cs.cmu.edu/~tom7/mario/mario.pdf</a>&nbsp;</p><p>National Standards to Prevent, Detect, and Respond to Prison Rape, 77 Federal Register 37106 (June 20, 2012) (codified at 28 Code of Federal Regulations, pt. 115). (2012).</p><p>Neumann, P. J., Cohen, J. T., and Weinstein, M. C. (2014). \u2018Updating cost-effectiveness\u2014The curious resilience of the $50,000-per-QALY threshold\u2019, in <i>The New England Journal of Medicine</i> <i>371</i>(9): 796\u2013797.</p><p>Nondiscrimination on the Basis of Disability in State and Local Government Services, 75 Federal Register 56164 (Sept. 15, 2010) (codified at 28 Code of Federal Regulations, pt. 35). (2010).</p><p>Nuclear Threat Initiative. (2020a). <i>Preventing the Next Global Biological Catastrophe</i>&nbsp;(Agenda for the Next Administration: Biosecurity). Nuclear Threat Initiative. <a href=\"https://media.nti.org/documents/Preventing_the_Next_Global_Biological_Catastrophe.pdf\">https://media.nti.org/documents/Preventing_the_Next_Global_Biological_Catastrophe.pdf</a>&nbsp;</p><p>Nuclear Threat Initiative. (2020b). <i>Reducing Nuclear Risks: An Urgent Agenda for 2021 and Beyond</i>&nbsp;(Agenda for the Next Administration: Nuclear Policy). Nuclear Threat Initiative. <a href=\"https://media.nti.org/documents/Reducing_Nuclear_Risks_An_Urgent_Agenda_for_2021_and_Beyond.pdf\">https://media.nti.org/documents/Reducing_Nuclear_Risks_An_Urgent_Agenda_for_2021_and_Beyond.pdf</a>&nbsp;</p><p>Ohio v. U.S. Dept. Of the Interior, 880 F. 2d 432 (Court of Appeals, Dist. of Columbia Circuit 1989).</p><p>OpenAI. (2017). \u2018Learning from Human Preferences\u2019. <a href=\"https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/\">https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/</a>&nbsp;</p><p>Ord, T. (2015). \u2018Moral Trade\u2019, in <i>Ethics</i> <i>126</i>(1): 118\u2013138.</p><p>Ord, T. (2020). <i>The Precipice: Existential Risk and the Future of Humanity</i>&nbsp;(Bloomsbury).</p><p>Our World in Data. (2019). \u2018Number of deaths by cause, United States, 2019\u2019. <i>Our World in Data</i>. <a href=\"https://ourworldindata.org/grapher/annual-number-of-deaths-by-cause?country=~USA\">https://ourworldindata.org/grapher/annual-number-of-deaths-by-cause?country=~USA</a>&nbsp;</p><p>Our World in Data. (2022). \u2018Daily and total confirmed COVID-19 deaths, United States\u2019. <i>Our World in Data</i>. <a href=\"https://ourworldindata.org/grapher/total-daily-covid-deaths\">https://ourworldindata.org/grapher/total-daily-covid-deaths</a>&nbsp;</p><p>Parfit, D. (1984). <i>Reasons and Persons</i>&nbsp;(Clarendon Press).</p><p>PopulationPyramid. (2019). \u2018Population pyramid for the United States of America, 2033\u2019. PopulationPyramid.Net. <a href=\"https://www.populationpyramid.net/united-states-of-america/2033/\">https://www.populationpyramid.net/united-states-of-america/2033/</a>&nbsp;</p><p>Posner, E. A., and Sunstein, C. R. (2017). \u2018Moral Commitments in Cost-Benefit Analysis\u2019, in <i>Virginia Law Review</i> <i>103</i>: 1809\u20131860.</p><p>Posner, R. (2004). <i>Catastrophe: Risk and Response</i>&nbsp;(Oxford University Press).</p><p>Reisner, J., D\u2019Angelo, G., Koo, E., Even, W., Hecht, M., Hunke, E., Comeau, D., Bos, R., and Cooley, J. (2018). \u2018Climate Impact of a Regional Nuclear Weapons Exchange: An Improved Assessment Based On Detailed Source Calculations\u2019, in <i>Journal of Geophysical Research: Atmospheres</i> <i>123</i>(5): 2752\u20132772.</p><p>Robock, A., Oman, L., and Stenchikov, G. L. (2007). \u2018Nuclear winter revisited with a modern climate model and current nuclear arsenals: Still catastrophic consequences\u2019, in <i>Journal of Geophysical Research</i> <i>112</i>(D13).</p><p>Rodriguez, L. (2019a). \u2018How bad would nuclear winter caused by a US-Russia nuclear exchange be?\u2019. <i>Rethink Priorities</i>. <a href=\"https://rethinkpriorities.org/publications/how-bad-would-nuclear-winter-caused-by-a-us-russia-nuclear-exchange-be\">https://rethinkpriorities.org/publications/how-bad-would-nuclear-winter-caused-by-a-us-russia-nuclear-exchange-be</a>&nbsp;</p><p>Rodriguez, L. (2019b). \u2018How likely is a nuclear exchange between the US and Russia?\u2019 <i>Rethink Priorities</i>. <a href=\"https://rethinkpriorities.org/publications/how-likely-is-a-nuclear-exchange-between-the-us-and-russia\">https://rethinkpriorities.org/publications/how-likely-is-a-nuclear-exchange-between-the-us-and-russia</a>&nbsp;</p><p>S.3799\u2014PREVENT Pandemics Act, (2022). <a href=\"https://www.congress.gov/bill/117th-congress/senate-bill/3799\">https://www.congress.gov/bill/117th-congress/senate-bill/3799</a>&nbsp;</p><p>Sandberg, A., and Bostrom, N. (2008). \u2018Global Catastrophic Risks Survey\u2019. Technical Report #2008-1; Future of Humanity Institute, Oxford University. <a href=\"https://www.fhi.ox.ac.uk/reports/2008-1.pdf\">https://www.fhi.ox.ac.uk/reports/2008-1.pdf</a>&nbsp;</p><p>Schubert, S., Caviola, L., and Faber, N. S. (2019). \u2018The Psychology of Existential Risk: Moral Judgments about Human Extinction\u2019, in <i>Scientific Reports</i> <i>9</i>(1): 15100.</p><p>Scope of Review, 5 U.S. Code \u00a7706(2)(A) (2012). <a href=\"https://www.govinfo.gov/app/details/USCODE-2011-title5/USCODE-2011-title5-partI-chap7-sec706/summary\">https://www.govinfo.gov/app/details/USCODE-2011-title5/USCODE-2011-title5-partI-chap7-sec706/summary</a>&nbsp;</p><p>Seitz, R. (2011). \u2018Nuclear winter was and is debatable\u2019, in <i>Nature</i> <i>475</i>(7354): 37.</p><p>Sevilla, J., Heim, L., Ho, A., Besiroglu, T., Hobbhahn, M., and Villalobos, P. (2022). \u2018Compute Trends Across Three Eras of Machine Learning\u2019. <i>arXiv</i>. <a href=\"http://arxiv.org/abs/2202.05924\">http://arxiv.org/abs/2202.05924</a>&nbsp;</p><p>Shah, R., Varma, V., Kumar, R., Phuong, M., Krakovna, V., Uesato, J., and Kenton, Z. (2022). \u2018Goal Misgeneralization: Why Correct Specifications Aren\u2019t Enough For Correct Goals\u2019. arXiv. <a href=\"http://arxiv.org/abs/2210.01790\">http://arxiv.org/abs/2210.01790</a>&nbsp;</p><p>Steinhardt, J. (2022). \u2018AI Forecasting: One Year In\u2019. <i>Bounded Regret</i>. <a href=\"https://bounded-regret.ghost.io/ai-forecasting-one-year-in/\">https://bounded-regret.ghost.io/ai-forecasting-one-year-in/</a>&nbsp;</p><p>Stein-Perlman, Z., Weinstein-Raun, B., and Grace, K. (2022). \u20182022 Expert Survey on Progress in AI\u2019. <i>AI Impacts</i>. <a href=\"https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/\">https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/</a>&nbsp;</p><p>Sunstein, C. R. (2020). <i>How Change Happens</i>&nbsp;(MIT Press).</p><p>Teran, N. (2022). \u2018Preventing Pandemics Requires Funding\u2019. <i>Institute for Progress</i>. <a href=\"https://progress.institute/preventing-pandemics-requires-funding/\">https://progress.institute/preventing-pandemics-requires-funding/</a>&nbsp;</p><p>The Nucleic Acid Observatory Consortium. (2021). \u2018A Global Nucleic Acid Observatory for Biodefense and Planetary Health\u2019. <i>arXiv</i>. <a href=\"http://arxiv.org/abs/2108.02678\">http://arxiv.org/abs/2108.02678</a>&nbsp;</p><p>The White House. (2022a). \u2018A Return to Science: Evidence-Based Estimates of the Benefits of Reducing Climate Pollution\u2019. <i>The White House</i>. <a href=\"https://www.whitehouse.gov/cea/written-materials/2021/02/26/a-return-to-science-evidence-based-estimates-of-the-benefits-of-reducing-climate-pollution/\">https://www.whitehouse.gov/cea/written-materials/2021/02/26/a-return-to-science-evidence-based-estimates-of-the-benefits-of-reducing-climate-pollution/</a>&nbsp;</p><p>The White House. (2022b). \u2018Joint Statement of the Leaders of the Five Nuclear-Weapon States on Preventing Nuclear War and Avoiding Arms Races\u2019. <i>The White House</i>. <a href=\"https://www.whitehouse.gov/briefing-room/statements-releases/2022/01/03/p5-statement-on-preventing-nuclear-war-and-avoiding-arms-races/\">https://www.whitehouse.gov/briefing-room/statements-releases/2022/01/03/p5-statement-on-preventing-nuclear-war-and-avoiding-arms-races/</a>&nbsp;</p><p>The White House. (2022c). \u2018The Biden Administration\u2019s Historic Investment in Pandemic Preparedness and Biodefense in the FY 2023 President\u2019s Budget\u2019. <i>The White House</i>. <a href=\"https://www.whitehouse.gov/briefing-room/statements-releases/2022/03/28/fact-sheet-the-biden-administrations-historic-investment-in-pandemic-preparedness-and-biodefense-in-the-fy-2023-presidents-budget/\">https://www.whitehouse.gov/briefing-room/statements-releases/2022/03/28/fact-sheet-the-biden-administrations-historic-investment-in-pandemic-preparedness-and-biodefense-in-the-fy-2023-presidents-budget/</a>&nbsp;</p><p>Thornley, E. (2022). \u2018Calculating expected American life-years saved by averting a catastrophe in 2033\u2019. <a href=\"https://docs.google.com/spreadsheets/d/1mguFYc06mw2Bdv85Viw6CQq4mt-mqPdiU4mQRi1s0Yo\">https://docs.google.com/spreadsheets/d/1mguFYc06mw2Bdv85Viw6CQq4mt-mqPdiU4mQRi1s0Yo</a>&nbsp;</p><p>U.K. Treasury. (2003). <i>The Green Book: Appraisal and Evaluation in Central Government</i>. TSO. <a href=\"https://webarchive.nationalarchives.gov.uk/ukgwa/20080305121602/http://www.hm-treasury.gov.uk/media/3/F/green_book_260907.pdf\">https://webarchive.nationalarchives.gov.uk/ukgwa/20080305121602/http://www.hm-treasury.gov.uk/media/3/F/green_book_260907.pdf</a>&nbsp;</p><p>U.S. Department of Transportation. (2021a). <i>Departmental Guidance on Valuation of a Statistical Life in Economic Analysis</i>. U.S. Department of Transportation. <a href=\"https://www.transportation.gov/office-policy/transportation-policy/revised-departmental-guidance-on-valuation-of-a-statistical-life-in-economic-analysis\">https://www.transportation.gov/office-policy/transportation-policy/revised-departmental-guidance-on-valuation-of-a-statistical-life-in-economic-analysis</a>&nbsp;</p><p>U.S. Department of Transportation. (2021b). <i>Departmental Guidance: Treatment of the Value of Preventing Fatalities and Injuries in Preparing Economic Analyses</i>. <a href=\"https://www.transportation.gov/sites/dot.gov/files/2021-03/DOT%20VSL%20Guidance%20-%202021%20Update.pdf\">https://www.transportation.gov/sites/dot.gov/files/2021-03/DOT%20VSL%20Guidance%20-%202021%20Update.pdf</a>&nbsp;</p><p>U.S. Environmental Protection Agency. (2010). <i>Valuing Mortality Risk Reductions for Environmental Policy: A White Paper (2010)</i>. <a href=\"https://www.epa.gov/sites/default/files/2017-08/documents/ee-0563-1.pdf\">https://www.epa.gov/sites/default/files/2017-08/documents/ee-0563-1.pdf</a>&nbsp;</p><p>U.S. National Security Commission on Artificial Intelligence. (2021). <i>Final Report</i>. <a href=\"https://www.nscai.gov/wp-content/uploads/2021/03/Full-Report-Digital-1.pdf\">https://www.nscai.gov/wp-content/uploads/2021/03/Full-Report-Digital-1.pdf</a>&nbsp;</p><p>U.S. Office of Management and Budget. (2022). <i>Budget of the U.S. Government: Fiscal Year 2023</i>. 04/08/2022. <a href=\"https://www.whitehouse.gov/wp-content/uploads/2022/03/budget_fy2023.pdf\">https://www.whitehouse.gov/wp-content/uploads/2022/03/budget_fy2023.pdf</a>&nbsp;</p><p>U.S. Social Security Administration. (2022). <i>Actuarial Life Table</i>. Social Security. <a href=\"https://www.ssa.gov/oact/STATS/table4c6.html\">https://www.ssa.gov/oact/STATS/table4c6.html</a>&nbsp;</p><p>Vinding, M. (2020). <i>Suffering-Focused Ethics: Defense and Implications</i>&nbsp;(Ratio Ethica).</p><p>Watson, C., Watson, M., Gastfriend, D., and Sell, T. K. (2018). \u2018Federal Funding for Health Security in FY2019\u2019, in <i>Health Security</i> <i>16</i>(5): 281\u2013303.</p><p>Wiblin, R., &amp; Ord, T. (2020). \u2018Toby Ord on The Precipice and humanity\u2019s potential futures\u2019. <i>The 80,000 Hours Podcast with Rob Wiblin</i>. <a href=\"https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/\">https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/</a>&nbsp;</p><p>Wiener, J. B. (2016). \u2018The Tragedy of the Uncommons: On the Politics of Apocalypse\u2019, in <i>Global Policy</i> <i>7</i>(S1): 67\u201380.</p><p>Xia, L., Robock, A., Scherrer, K., Harrison, C. S., Bodirsky, B. L., Weindl, I., J\u00e4germeyr, J., Bardeen, C. G., Toon, O. B., and Heneghan, R. (2022). \u2018Global food insecurity and famine from reduced crop, marine fishery and livestock production due to climate disruption from nuclear war soot injection\u2019, in <i>Nature Food</i> <i>3</i>(8): 586\u2013596.</p><h1><strong>9. Footnotes</strong></h1><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2ylqni72csq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2ylqni72csq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;By \u2018longtermists\u2019, we mean people particularly concerned with ensuring that humanity\u2019s long-term future goes well.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8efk14fzzbw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8efk14fzzbw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Posner (2004) is one precedent in the literature. In another respect we echo Baum (2015), who argues that we need not appeal to far-future benefits to motivate further efforts to prevent catastrophes.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnku9o8g99yp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefku9o8g99yp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;A war counts as thermonuclear if and only if three&nbsp;countries each detonate at least 10 nuclear devices of at least 10 kiloton yield outside of their own territory or two countries each detonate at least 50 nuclear devices of at least 10 kiloton yield outside of their own territory.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7tawwoceikd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7tawwoceikd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See, for example, (Coupe et al. 2019; Mills et al. 2014; Robock, Oman, and Stenchikov 2007; Xia et al. 2022). Some doubt that nuclear war would have such severe atmospheric effects (Reisner et al. 2018; Seitz 2011).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7ds6mc4jjuf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7ds6mc4jjuf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Metaculus forecasters estimate that there is a 32% probability that the human population drops by at least 10% in a period of 5 years or less by 2100 (Metaculus 2022a), and a 30% probability conditional on this drop occurring that it is caused by an engineered pathogen (Metaculus 2022b). Multiplying these figures gets us 9.6%. This calculation ignores some minor technicalities to do with the possibility that there will be more than one qualifying drop in population.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnq2i8sctnkg8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefq2i8sctnkg8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;COVID-19 spread to almost every community, as did the 1918 Flu. Engineered pandemics could be even harder to suppress. Rabies and septicemic plague kill almost 100% of their victims in the absence of treatment (Millett and Snyder-Beattie 2017: 374).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnalsc0v7mcmk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefalsc0v7mcmk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The survey defines \u2018high-level machine intelligence\u2019 as machine intelligence that can accomplish every task better and more cheaply than human workers.</p><p>Admittedly, we have some reason to suspect these estimates. As Cotra (2020: 40\u201341) notes, machine learning researchers' responses in a previous survey (Grace et al. 2018) were implausibly sensitive to minor reframings of questions.</p><p>In any case, recent progress in AI has exceeded almost all expectations. On two out of four benchmarks, state-of-the-art performance in June 2022 was outside the 90% credible interval of an aggregate of forecasters\u2019 predictions made in August 2021 (Steinhardt 2022).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno23uvsfqw5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo23uvsfqw5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Here we assume that a full-scale nuclear war would kill at least 5 billion people and hence qualify as a global catastrophe (Rodriguez 2019a; Xia et al. 2022: 1).</p><p>The risk is not&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10\\%+3\\%+5\\%=18\\%\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">3</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">18</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span></span></span></span></span></span>, because each of Ord\u2019s risk-estimates is conditional on humanity not suffering an existential catastrophe from another source in the next 100 years (as is made clear by Ord 2020: 173\u201374). If we assume statistical independence between risks, the probability that there is no global catastrophe from AI, engineered pandemics, or nuclear war in the next 100 years is at most&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(1-0.1)\\times(1-0.03)\\times(1-0.05)\u224883\\%\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.03</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.05</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u2248</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">83</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span></span></span></span></span></span>. The probability that there is some such global catastrophe is then at least 17%. There might well be some positive correlation between risks (Ord 2020: 173\u201375), but plausible degrees of correlation will not significantly reduce total risk.</p><p>Note that the 17% figure does not incorporate the upward adjustment for the (significant, in our view) likelihood that an engineered pandemic constitutes a global catastrophe but not an existential catastrophe.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmekx3nzs16\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmekx3nzs16\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;If the risk over the next century is 17%&nbsp;and the risk per decade is constant, then the risk per decade is&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"x\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span></span></span></span></span></span>&nbsp;such that&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1-(1-x)^{10}=17\\%\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">17</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span></span></span></span></span></span>. That gives us&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"x\u22481.85\\%\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u2248</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1.85</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span></span></span></span></span></span>.</p><p>There are reasons to doubt that the risk this decade is as high as the risk in future decades. One might think that \u2018crunch time\u2019 for AI and pandemic risk is more than a decade off. One might also think that most nuclear risk comes from scenarios in which future technological developments cast doubt on nations\u2019 second-strike capability, thereby incentivising first-strikes. These factors are at least partly counterbalanced by the likelihood that we will be better prepared for risks in future decades.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk0isr41eoxl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk0isr41eoxl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The projected number of Americans at least 10 years old in 2033 is 6.88% smaller than the number of Americans in 2023 (PopulationPyramid 2019).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhz139osjyd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhz139osjyd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Our World in Data (2019) records a mean of approximately 41,000 road injury deaths per year in the United States over the past decade.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbce5vwjjant\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbce5vwjjant\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This Budget includes many of the recommendations from the Apollo Program for Biodefense and Athena Agenda (Bipartisan Commission on Biodefense 2021, 2022).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnf8lccgruonu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreff8lccgruonu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Avin and Amadae (2019) survey ways in which AI may exacerbate nuclear risk and offer policy recommendations, including the recommendation not to incorporate AI into NC3. The U.S. National Security Commission on Artificial Intelligence (2021: 98) make a similar recommendation.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6i6fiq643ps\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6i6fiq643ps\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Those taken up already include extending New START (Strategic Arms Reduction Treaty) and issuing a joint declaration with the other members of the P5 \u2013 China, France, Russia, and the U.K. \u2013 that a \u201cnuclear war cannot be won and must never be fought\u201d (The White House 2022b).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2k7zv6r5vgh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2k7zv6r5vgh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;It is worth noting that the dynamics of nuclear risk are complex, and that experts disagree about the likely effects of these interventions. What can be broadly agreed is that nuclear risk should receive more investigation and funding.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnu76fcclx80r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefu76fcclx80r\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The Biden administration\u2019s 2023 Budget requests $88.2 billion over five years (The White House 2022c; U.S. Office of Management and Budget 2022). We can suppose that another five years of funding would require that much again. A Nucleic Acid Observatory covering the U.S. is estimated to cost $18.4 billion to establish and $10.4 billion per year to run (The Nucleic Acid Observatory Consortium 2021: 18). Ord (2020: 202\u20133) recommends increasing the budget of the Biological Weapons Convention to $80 million per year. Our listed interventions to reduce nuclear risk are unlikely to cost more than $10 billion for the decade. AI safety and governance might cost up to $10 billion as well. The total cost of these interventions for the decade would then be $319.6 billion.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbz7c7t37zb8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbz7c7t37zb8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;We can observe how much people pay for products that reduce their risk of death, like bike helmets, smoke alarms, and airbags. We can also observe how much more people are paid to do risky work, like service nuclear reactors and fly new planes (Kniesner and Viscusi 2019).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4wnmjrx05ud\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4wnmjrx05ud\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;US agencies rely mainly on hedonic wage studies, which measure the wage-premium for risky jobs. European agencies tend to rely on stated preference methods (Kniesner and Viscusi 2019: 10).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn08d44a0ug6s3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref08d44a0ug6s3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Updating for inflation and growth in real incomes, the U.S. Environmental Protection Agency\u2019s central estimate for 2021 is approximately $12.2 million. The U.S. Department of Health and Human Services\u2019 2021 figure is about $12.1 million (Kniesner and Viscusi 2019).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn83dcbq1lmmf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref83dcbq1lmmf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Researchers and analysts in the US frequently cite a $50,000-per-quality-adjusted-life-year (QALY) threshold for funding medical interventions, but this figure lacks any particular normative significance and has not been updated to account for inflation and real growth in incomes since it first came to prominence in the mid-1990s (Neumann, Cohen, and Weinstein 2014). The \u00a320,000-\u00a330,000-per-QALY range recommended by the U.K.\u2019s National Institute for Health and Care Excellence suffers from similar defects (Claxton et al. 2016). More principled estimates put a higher value on years of life (Aldy and Viscusi 2008; Favaloro and Berger 2021; Hirth et al. 2000). In any case, simply updating the $50,000-per-QALY threshold to account for inflation and growth since 1995 would imply a value of more than $100,000-per-QALY. At $100,000-per-QALY, the value of reducing GCR a decade from now by 0.1pp is at least&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0.001\u00d7$100,000\u00d714,583,317,092\u00d7(5/9)\u00d7(1/1.07^{10})\u2248\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.001</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-texatom MJXc-space2\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">$</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">100</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">000</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">14</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">583</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">317</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">092</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">9</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1.07</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.605em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u2248</span></span></span></span></span></span></span>$412billion. (14,583,317,092 is the expected number of American life-years saved by preventing a global catastrophe in 2033, based on a projected US population pyramid (PopulationPyramid 2019) and life-expectancy statistics (U.S. Social Security Administration 2022). See <a href=\"https://docs.google.com/spreadsheets/d/1mguFYc06mw2Bdv85Viw6CQq4mt-mqPdiU4mQRi1s0Yo/edit#gid=135523877\">Thornley (2022)</a>.) That figure justifies the suite of interventions we recommend below. We believe that many interventions are also justified on the more demanding $50,000-per-QALY figure.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnr82rsvx7t2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefr82rsvx7t2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>0.005\u00d7$11,800,000\u00d7330,000,000\u00d7(5/9)\u00d7(1/1.03<sup>10</sup>)\u2248$8.04 trillion, which is over 20 times the cost of $400 billion.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9vc46kzrfy\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9vc46kzrfy\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This is especially so in the case of pandemics, and in fact the pandemic-preventing interventions that we list are justified even considering only their effects on the risk of pandemics about as damaging as COVID-19. The total cost of the COVID-19 pandemic for the U.S. has been estimated at $16 trillion (Cutler and Summers 2020), which suggests that it is worth the U.S. spending up to $32 billion per year to decrease the annual risk of such pandemics by 0.2pp (and Cutler and Summers\u2019 estimate is based on an October 2020 projection of 625,000 deaths. At the time of writing, Our World in Data (2022) has total confirmed US COVID-19 deaths at over 1 million). Our listed pandemic-preventing interventions are projected to cost less than $32 billion per year, and they would plausibly reduce annual risk by more than 0.2pp. After all, the observed frequency of pandemics as bad as COVID-19 is about one per century, suggesting an annual risk of 1% per year. A 0.2pp-decrease then means a 20%-decrease in baseline risk, which seems easily achievable via the interventions that we recommend. And since our listed pandemic-preventing interventions can be justified in this way, the case for funding them does not depend on difficult forecasts of the likelihood of unprecedented events, like a pandemic constituting a global catastrophe. Instead, we can appeal to the observed frequency of pandemics about as damaging as COVID-19.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmmytcurf8pc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmmytcurf8pc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;There is a case for including benefits to non-U.S. citizens in cost-benefit analyses of GCR-reducing interventions. After all, saving the lives of non-U.S. citizens is morally important. And the Biden administration already includes costs to non-U.S. citizens in its <i>social cost of carbon</i>&nbsp;(SCC): its estimate of the harm caused by carbon dioxide emissions (The White House 2022a). The SCC is a key input to the U.S. government\u2019s climate policy, and counting costs to non-U.S. citizens in the SCC changes the cost-benefit balance of important decisions like regulating power plant emissions, setting standards for vehicle fuel efficiency, and signing on to international climate agreements.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fne8jboxq3ehm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefe8jboxq3ehm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Describing this policy as \u2018longtermist\u2019 is simplifying slightly. Some longtermists prioritise preventing future suffering over increasing the probability that future generations exist (see, for example, Vinding 2020).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnugpwbu65vcs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefugpwbu65vcs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Here is a related recommendation: longtermists should assess interventions\u2019 cost-effectiveness using standard cost-benefit analysis when proposing those interventions to governments. They should not assess cost-effectiveness using longtermist assumptions and then appeal to cost-effectiveness thresholds from standard cost-benefit analysis to argue for government funding (see, e.g., Matheny 2007: 1340). If governments funded every intervention justified on these grounds, their level of spending on catastrophe-preventing interventions would be unacceptable to a majority of their citizens.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6hrrfb8asor\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6hrrfb8asor\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Bostrom (2013: 18\u201319) makes something like this point, as does Posner (2004: 152\u201353).</p><p>Why think that the non-existence of future generations would be an overwhelming moral loss? The best-known argument goes as follows: the expected future population is enormous (Greaves and MacAskill 2021: 6\u20139; MacAskill 2022: 1), the lives of future people are good in expectation (MacAskill 2022: 9), and \u2013 all else equal \u2013 it is better if the future contains more good lives (MacAskill 2022: 8). We should note, however, that longtermism is a big tent and that not all longtermists accept these claims.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4fa07c7xrt4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4fa07c7xrt4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Since the Reagan administration, executive orders have required U.S. agencies to conduct cost-benefit analyses of major regulations (Executive Order No. 13,563 2012), and to demonstrate that the benefits of the regulation outweigh the costs (Executive Order No. 12,291 1982). U.S. courts have struck down regulations for being insufficiently sensitive to the results of cost-benefit analyses (Graham 2008: 454, 479; Posner and Sunstein 2017: 1820), citing a clause in the Administrative Procedure Act which requires courts to invalidate regulations that are \u201carbitrary [or] capricious\u201d (Scope of Review 2012). The Supreme Court has indicated that agencies may not impose regulations with costs that \u201csignificantly\u201d exceed benefits (Michigan v. EPA <i>Michigan, et al. &nbsp;V. Environmental Protection Agency, et al. (No. 14-46); Utility Air Regulatory Group v. Environmental Protection Agency, et al. (No. 14-47); National Mining Association v. Environmental Protection Agency, et al. (No. 14-49)</i>, 2015). For more, see (Graham 2008; E. A. Posner and Sunstein 2017).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnx54p04qc4q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefx54p04qc4q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;In this respect, reducing GCR is akin to mitigating climate change.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndakoog5v4ss\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdakoog5v4ss\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;However, we should note that nuclear war is an <i>existential risk factor</i>&nbsp;(Ord 2020: 175\u201380): a factor that increases existential risk. That is because nuclear wars that are not themselves existential catastrophes make humanity more vulnerable to other kinds of existential catastrophe. Since nuclear war is an existential risk factor, preventing nuclear war has effects on total existential risk not limited by nuclear war\u2019s direct contribution to existential risk.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9law4kh303c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9law4kh303c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;We should note, though, that there are other reasons why a strong longtermist policy might prioritise nuclear risk. One is that a nuclear war might negatively affect the characteristics of the societies that shape the future.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn510mud6zv08\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref510mud6zv08\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The minor complication is that an engineered pathogen could cause an existential catastrophe (the destruction of humanity\u2019s long-term potential) <i>without</i>&nbsp;infecting more than 1,000 people. Since this outcome is very unlikely, we can safely ignore it here.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyx75t39r0w\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyx75t39r0w\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;At the time of writing, the PREVENT Pandemics Act (S.3799 - PREVENT Pandemics Act 2022) is yet to pass the Senate or the House, and it includes only about $2 billion in new spending to prevent future pandemics. Biden\u2019s Build Back Better Act originally included $2.7 billion of funding for pandemic prevention (Teran 2022), but this funding was cut when the legislation became the Inflation Reduction Act (H.R.5376 - Inflation Reduction Act of 2022 2022).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna7t0jphf8mu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa7t0jphf8mu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Ord (2020: 312) estimated that global spending on reducing existential risk from AI in 2020 was between $10 and $50 million per year.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fng4getjlqwl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefg4getjlqwl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See Beckstead (2015) and Jebari (2015) for more detail.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncrjj4bvzzym\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcrjj4bvzzym\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The welfare loss is most direct on an unrestricted preference-satisfaction theory of welfare: if a person has a moral commitment compromised, they thereby have a preference frustrated and so suffer a welfare loss. But compromised moral commitments also lead to welfare losses on other plausible theories of welfare. These theories will place some weight on positive and negative experiences, and having one\u2019s moral commitments compromised is typically a negative experience.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn123cjmrjdt8h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref123cjmrjdt8h\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Here are two reasons why one might think that AWTP should be excluded from cost-benefit calculations, along with responses. First, one might think that AWTP for benefits to other people should be excluded (U.S. Environmental Protection Agency 2010: 18\u201319). Most of us care not only about the benefits that other people receive, but also about the costs that they bear. If benefits but not costs are included, we all pay more for benefits than we would like to, on average. If both benefits and costs are included, they cancel each other out. This point is correct as far as it goes, but it gives us no reason to exclude AWTP for pure longtermist goods from cost-benefit calculations. Future generations will not have to pay for the pure longtermist goods that we fund (U.S. Environmental Protection Agency 2010: 19).</p><p>Second, one might think that charities (rather than governments) should assume the responsibility of upholding citizen\u2019s moral commitments. This thought is analogous to the thought that private companies (rather than governments) should provide for citizens\u2019 needs, and the response is analogous as well: some collective action problems require government action to solve. Citizens may be willing to bear costs for the sake of some moral commitment if and only if it can be ensured that some number of other people are contributing as well (Posner and Sunstein 2017: 1840).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6vxpau7pszj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6vxpau7pszj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;In their cost-benefit analysis of the \u2018Water Closet Clearances\u2019 regulation, the U.S. Department of Justice (DOJ) appealed to non-wheelchair-users\u2019 willingness to pay to make buildings more accessible for wheelchair users. The DOJ noted that, even if non-wheelchair-users would be willing to pay just pennies on average to provide disabled access, the benefits of the regulation would justify the costs (Nondiscrimination on the Basis of Disability in State and Local Government Services 2010). In another context, the DOJ estimated U.S. AWTP to prevent rape, and noted that the estimated figure justified a regulation designed to reduce the incidence of prison rape (National Standards to Prevent, Detect, and Respond to Prison Rape 2012). And on the legal side, the U.S. Department of the Interior had a damage measure struck down by a court of appeals for failing to incorporate the <i>existence value </i>of pristine wilderness: the value that people derive from just knowing that such places exist, independently of whether they expect to visit them (<i>Ohio v. U.S. Dept. Of the Interior</i>&nbsp;1989). Based on this case, Sunstein and Posner (2017: 1858\u20131860) suggest that excluding AWTP from cost-benefit analyses may suffice to render regulations \u201carbitrary [and] capricious\u201d, in which case courts are required by the Administrative Procedure Act to invalidate them (Scope of Review 2012).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvnvpypgizy\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvnvpypgizy\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Baum (2015: 93) makes a point along these lines: longtermists can use the inspirational power of the far future to motivate efforts to ensure it goes well.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0ko16dwc14o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0ko16dwc14o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;In this respect, the situation is analogous to Parfit\u2019s (1984: 7) hitchhiker case.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9f59gy0bpda\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9f59gy0bpda\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;For helpful comments, we thank Mackenzie Arnold, Tomi Francis, Jakob Graabak, Hannah Lovell, Andreas Schmidt, Philip Trammell, Risto Uuk, Nikhil Venkatesh, an anonymous reviewer for Oxford University Press, and audience members at the 10<sup>th</sup>&nbsp;Oxford Workshop on Global Priorities Research.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnm482lc9rpwg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefm482lc9rpwg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;And recall that the above figures assume a conservative 0.1pp reduction in GCR as a result of implementing the whole suite of interventions. We think that a 0.5pp reduction in GCR is a more reasonable estimate, in which case the benefit-cost ratio of the suite is over 5. The most cost-effective interventions within the suite will have even more favourable benefit-cost ratios.</p></div></li></ol>", "user": {"username": "elliottthornley"}}, {"_id": "fNKmP2bq7NuSLpCzD", "title": "Let's make the truth easier to find", "postedAt": "2023-03-20T04:28:41.387Z", "htmlBody": "<p>Over a year ago, I posted an<a href=\"https://forum.effectivealtruism.org/posts/ckcoSe3CS2n3BW3aT/what-ea-projects-could-grow-to-become-megaprojects?commentId=u7hPPF5r993iKYL5e\">&nbsp;<u>answer somewhere</u></a> that received no votes and no comments, but I still feel that this is one of the most important things that our world needs&nbsp;<i>right now</i>.</p><p>I wish to persuade you of a few things here:</p><ul><li>that truthseeking is an important problem,</li><li>that it's unsolved and difficult,&nbsp;</li><li>that there are probably things we can do to help solve it that are cost-effective, and I have one specific-but-nebulous idea in this vein to describe.</li></ul><h1><strong>Truthseeking is important</strong></h1><p>Getting the facts wrong has consequences big and small. Here are some examples:</p><h3>Ukraine war</h3><p>On Feb. 24 last year, over 100,000 soldiers found themselves unexpectedly crossing the border into Ukraine, which they were told was full of Nazis, because one man in Moscow believed he could take the country in a few days. A propaganda piece was even published accidentally about<a href=\"https://twitter.com/DPiepgrass/status/1502740542613950467\">&nbsp;<u>the Russian victory</u></a>: \"Ukraine has returned to Russia. Its statehood will be [...] returned to its natural state of part of the Russian world.\" Consensus opinion is that Putin made a grave mistake. Instead of a quick win, Putin took about 7% more of Ukraine than he had already taken, then lost some of that, while taking over 100,000 Russian casualties and losing over 8,000 pieces of heavy military equipment (<a href=\"https://github.com/leedrake5/Russia-Ukraine\"><u>visually confirmed</u></a>) \u2014 all in the first year. He later lost some of the land he stole, though he still has about 6,000 nuclear weapons including the<a href=\"https://www.businessinsider.com/the-real-purpose-of-russias-poseidon-nuclear-doomsday-device-2019-2\">&nbsp;<u>Poseidon \"doomsday device\"</u></a>, and hopes veiled nuclear threats plus hundreds of thousands of conscripts will bring \"victory\". And by \"victory\", I mean<a href=\"https://www.metaculus.com/questions/9936/100k-russo-ukrainian-war-deaths-in-2022/\">&nbsp;well <u>over 100,000 people have already been killed</u></a>, plus many more indirectly, while Ukraine suffers hundreds of billions of dollars in damage, and the world economy suffers similarly, including Russia itself.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fNKmP2bq7NuSLpCzD/tzart2qflfcb14mg9z5e\">Residential area of Mariupol after<a href=\"https://dpiepgrass.medium.com/putins-rain-of-terror-db7a4adc7fa8\">&nbsp;<u>one month of Russian assault</u></a></p><h3>Global warming</h3><p>In 1979, the&nbsp;<a href=\"https://geosci.uchicago.edu/~archer/warming_papers/charney.1979.report.pdf\"><u>Charney report</u></a> summarized the emerging consensus that CO<sub>2</sub> causes global warming. Still, Republicans didn't want subsidies on renewables or carbon taxes, while Democrats and the media went on treating&nbsp;<a href=\"https://www.carbonbrief.org/solar-wind-nuclear-amazingly-low-carbon-footprints/\"><u>carbon-free</u></a> nuclear power as if it were more dangerous than fossil fuels, even though its safety profile looks almost like this:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fNKmP2bq7NuSLpCzD/rpuret5imjt5pigvv6vp\"></p><p>For decades after the Charney report, no new nuclear reactors were approved in the U.S., and it took over 30 years for solar and wind power to become economical. In the meantime, as CO<sub>2</sub> accumulated in the atmosphere, human CO<sub>2</sub> emissions nearly doubled:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fNKmP2bq7NuSLpCzD/oszsu2dm4e8ygbmi1gkk\"></p><p>(View charts:&nbsp;<a href=\"https://ourworldindata.org/grapher/annual-co2-emissions-per-country?time=earliest..2021&amp;country=~OWID_WRL\"><u>CO2 emissions</u></a>,&nbsp;<a href=\"https://ourworldindata.org/grapher/global-energy-substitution?time=1965..latest\"><u>primary energy</u></a>)</p><p>44 years later, Germany still plans to close its nuclear plants years before its coal plants, though after the attack on the Nord Stream pipeline, they<a href=\"https://www.theguardian.com/world/2022/sep/27/germany-delays-exit-from-nuclear-power-to-offset-energy-shortfall\">&nbsp;<u>put off closing</u></a> the last two nuclear plants\u2026 for a few months. In those 44 years, Earth's surface warmed 0.76\u00b0C and its land warmed nearly 1.2\u00b0C (<a href=\"https://data.giss.nasa.gov/gistemp/graphs_v4/\"><u>GISS</u></a>).<a href=\"https://www.carbonbrief.org/guest-post-why-does-land-warm-up-faster-than-the-oceans/\">&nbsp;<u>Land will continue warming faster than sea</u></a>, and<a href=\"https://ourworldindata.org/co2-emissions\">&nbsp;<u>CO2 emissions keep trending upward</u></a>, although much of that CO2 will stay in the atmosphere even if we stop adding more.</p><h3>Covid Vaccines</h3><p>My brother and I tried to convince my 74-year-old father he should get vaccinated, but my dad's sources told him 5,000 people had already been killed by Covid vaccines, and he trusted his sources far more than either of us. I argued that 5,000 entries in the VAERS database is unsurprising due to base rates: natural deaths happen all the time, and some should by coincidence happen after vaccination. He dismissed the argument, saying that no one would give a vaccine to someone who was about to die. He ignored my followup email and would go on to ignore many, many more of my responses and questions, though he did later tell me that&nbsp;<i>actually</i> Covid vaccines had killed 100,000 people.</p><p>In September 2021, my uncle and former legal guardian fell down in his garden. Passers-by saw this and called an ambulance. At the hospital, he tested positive for Covid and was treated for it. He had chosen to be unvaccinated on the strength of anti-vax arguments. Later, he was put on a ventilator and a brain scan suggested a stroke. Finally, he died. His widow and my father concluded that a stroke killed him\u2014though the widow lamented to me, eyes watering slightly, that perhaps if the doctors had allowed her to give him ivermectin, he would still be alive today. I suggested she fill out a form to get more information about what happened to him in the hospital; she declined.</p><p>Soon afterward, my father's favorite anti-vaxxer, Marcus Lamb, died of Covid and his widow did not suggest any other cause or comorbidity in his death. My father's opinion was unaffected. That Christmas I sent my dad a copy of<a href=\"https://www.lesswrong.com/posts/yFJ7vCjefBxnTchmG/outline-of-galef-s-scout-mindset\">&nbsp;<u>Scout Mindset</u></a>, but it was too late; he became more convinced than ever that vaccines were the Real Threat. Just after reading the book, he told me on the phone that the author of Scout Mindset \"overthinks things\" and immediately sent a 4-page anti-vax<a href=\"https://en.wikipedia.org/wiki/Gish_gallop\">&nbsp;<u>gish gallop</u></a> he wrote himself. Later, after ignoring all of the comments and dozens of questions I responded with, he said I don't have a \"scout mentality\". Though in 2021 he insisted that the risk of him getting Covid was minimal, he got Covid in 2022 as I expected\u2014and thus, he said, better immunity than any vaccine.</p><p>I hope all this illustrates the real-world importance of epistemology, or in other words, truthseeking.</p><p>We can't solve all epistemological problems. People will always act kinda dumb. But by broadly chipping away at epistemological problems, by \"raising the sanity waterline\", I think we can make society function better.</p><h1><strong>Truthseeking is difficult</strong></h1><p>It seems to me that most people think figuring out the truth is easy. That's how people talk about politics online: one person talks about how obviously not dangerous Covid is, or how obviously dangerous vaccines are; another talks about how obviously wrong the first person is about that.</p><p>I spent some years arguing with people I call \"global warming dismissives\", who you might know as \"skeptics\" or \"deniers\". For instance, have you heard that global warming is caused by cosmic rays? By the sun? By the \"pressure law\" or \"underground rivers of lava\", as I've heard? By natural internal variability? By CO2 emissions from volcanoes? By \"natural cycles\"? Or maybe CO2 emissions from oceans? That CO2 doesn't cause warming? That CO2 causes warming, but CO2 levels naturally change quickly and erratically over the centuries for unknown reasons,<a href=\"https://debunkhouse.wordpress.com/2010/03/28/co2-ice-cores-vs-plant-stomata/\">&nbsp;<u>as shown by plant stomata</u></a>? That CO2 causes warming, but only very little? That the lines of evidence showing global warming in the last 100 years are all fraudulent, except for the UAH record which shows less warming? That maybe global warming is happening, and maybe humans are causing it, but it's a good thing and nothing should be done?&nbsp;</p><p>This is a wide variety of perspectives, but people with these beliefs all tend to like the web site WattsUpWithThat. They are united not by what they believe, but what they disbelieve: their denial. (But one of the most popular stories is that the word \"denier\" is meant to associate them with \"holocaust deniers\", so anyone who says \"denier\" is an intolerant bigot. Since these are not<a href=\"https://en.wikipedia.org/wiki/Scientific_skepticism\">&nbsp;<u>skeptics in the scientific sense</u></a>, I call them \"dismissives\" instead).</p><p>They're not dumb.<a href=\"https://www.pnas.org/doi/10.1073/pnas.1704882114\">&nbsp;<u>Many of them are highly intelligent</u></a>. And you may think they are fringe, but their beliefs are extremely influential. Despite the<a href=\"https://dpiepgrass.medium.com/1970s-agw-consensus-3123a34e5105\">&nbsp;<u>1970s consensus</u></a>, a 2010 poll (over 30 years later) found that just<a href=\"https://www.pewresearch.org/politics/2010/10/27/little-change-in-opinions-about-global-warming/\">&nbsp;<u>34% of Americans</u></a> thought humans cause global warming.</p><p>One way to counter this sort of thing is the SkepticalScience approach\u2014a web site run by volunteers, devoted to refuting myths about global warming. I do think this site has been influential. But it seems to me that most topics that have \"dismissives\" or \"pseudoscientists\" don't have any equivalent of SkepticalScience. Also, the way SkepticalScience makes its judgements is opaque; readers are faced with a puzzle of how much they should trust it; it is not obvious that SkepticalScience is more trustworthy or impartial than WattsUpWithThat. Plus, current volunteers at SkS often don't have the energy to keep up-to-date with the latest science, leaving much of the content several years out of date.</p><p>When you branch out from global warming to look at other questions of fact, such as \"whether homeopathy works and is not a placebo effect\", \"whether 9/11 was an inside job\", \"whether nuclear power is at least as bad as fossil fuels\", \"whether all vaccines are bad (or just all Covid vaccines)\", \"whether humans evolved naturally from an earlier primate\", and so on, it should least be clear that the truth isn't easy for people to discern, and taking the<a href=\"https://www.lesswrong.com/tag/inside-outside-view\">&nbsp;<u>outside view</u></a>,&nbsp;<strong>you</strong> should not expect to be good at it either.</p><h2><strong>And it's getting harder</strong></h2><p>In the past, photos have been strong evidence of things. Slowly, \"photoshopping\" became a popular means of falsifying evidence, but this kind of forgery takes a lot of skill to do well. Thanks to AI image generation, that's changing fast. Thanks to deepfakes and video-generation models, even video soon won't be good evidence like it used to be. And thanks to huge language models, it is becoming practical to produce BS in immense quantities. Right now, many Russian claims have the virtue of being<a href=\"https://twitter.com/DPiepgrass/status/1553257164386418688\">&nbsp;<u>obviously suspicious</u></a>; we can't count on that forever.</p><p>I would rest easier if we had more ways to counter this sort of thing.</p><h1><strong>Truthseeking is an unsolved problem</strong></h1><p>I read and enjoyed<a href=\"https://www.lesswrong.com/rationality\">&nbsp;<u>Rationality: A-Z</u></a>, all the most popular<a href=\"https://jasoncrawford.org/guide-to-scott-alexander-and-slate-star-codex\">&nbsp;<u>essays by Scott Alexander</u></a>, Scout Mindset and other scattered materials, and I have to say: for a movement that considers Bayesian reasoning central, it's weird that I've never seen a rationalist essay on the topic of the evidential value of a pair of sources, and almost no essays about how to choose your initial priors, or how update them&nbsp;<i>in detail</i>.</p><p>Consider a pair of sources: your friend Alice tells you X, and then your friend Bob tells you X. Is that two pieces of evidence for X, or just one? This seems like an issue that is both very thorny and very important to the question \"how to do Bayesian reasoning\", but I've never seen a rationalist essay about it. (I don\u2019t claim that no essay exists; it\u2019s just that being \u201crationalist\u201d for five years somehow wasn\u2019t long enough to have seen such an essay.)</p><p>Consider also the question of how to decide how much to trust&nbsp;<i>persons</i> or&nbsp;<i>organizations</i>. This is a central question in the modern era because the territory is much bigger than in our ancestral environment (the whole planet, over 7 billion people and endless specialties), so any knowledgeable person must rely&nbsp;<strong>mainly</strong> on reports from others. An essay related to this is<a href=\"https://slatestarcodex.com/2014/04/28/the-control-group-is-out-of-control/\">&nbsp;<u>The Control Group is Out Of Control</u></a>, which tells us that even in science (let alone politics), this is a hard problem. Even if you're trying to stick with a high standard of evidence by citing scientific papers, you might end up citing the bullshit papers by mistake. In my time exploring the climate deniosphere, I've come across a large number of bad papers or contra-consensus papers. Even&nbsp;<i>if</i> the consensus is<a href=\"https://dpiepgrass.medium.com/scrutinizing-the-consensus-numbers-70faf9200a0c\">&nbsp;<u>97%</u></a> on global warming you should expect this, because there are over 100,000 peer-reviewed papers about climate, a substantial fraction of which give an opinion on global warming. It's not much different in the anti-vax space, where anti-vaxxers have at least one \"<a href=\"https://www.publichealthpolicyjournal.com\"><u>peer-reviewed journal</u></a>\".</p><p>Even in a community working as hard as rationalists do to be rational, we\u2019re still amateurs and hobbyists, so even when key questions have perfectly good answers out there in the broader rationalist community, most of us don't have the time or discipline to discover and learn those answers. You can't expect&nbsp;<i>amateurs</i> to have read and internalized thousands of pages of text on their craft; they have day jobs. So even in rationalism, or in the EA community, I think we have plenty of room to improve.</p><p>If you encounter a garbage paper, how do you know it's garbage? Right now, you have to suspect that \"something is wrong\" with it and its journal. You might, say, comb through it looking for error(s). But that's hard and time-consuming! Who has time for that? No, in today's world we are&nbsp;<i>almost forced</i> to rely on more practical methods such as the following: we notice that the&nbsp;<i>conclusion</i> of the paper is highly implausible, and so we look for reasons to reject it. I want to stress that although this is perfectly normal human behavior,&nbsp;<i>it is&nbsp;<strong>exactly</strong> like what anti-science people do</i>. You show them a scientific paper in support of the scientific consensus and they respond: \"that can't be true, it's bullshit!\" They are convinced \"something is wrong\" with the information, so they reject it. If, however, there were some way to learn about the fatal flaws in a paper just by searching for its title on a web site, people could separate the good from the bad in a&nbsp;<i>principled</i> way, rather than mimicking the epistemically bad behavior of their opponents.</p><p>The key to making this possible is&nbsp;<i>sharing work</i>. Each person should not decide independently whether a paper is good or not. We kind of know this already; for instance we might simply decide to trust one community member\u2019s evaluation of some papers. Since this is similar to what other communities (which, we know, often reach wrong conclusions) do, I propose being suspicious about this approach.</p><h2><strong>The information-misinformation imbalance</strong></h2><p>Misinformation has a clear business model:<a href=\"https://www.theguardian.com/technology/2022/jan/27/anti-vaxxers-making-at-least-25m-a-year-from-publishing-on-substack\">&nbsp;<u>Substack may pay over a million dollars annually to the two most popular antivaxxers</u></a>. And while Steve Kirsch likely earns over $100,000 on Substack subscriptions per year, Gift of Fire's<a href=\"https://medium.com/microbial-instincts/debunking-steve-kirschs-latest-claims-97e1c40f5d74\">&nbsp;<u>debunking of Steve Kirsch's most important claim</u></a> has 363 \"claps\" on Medium (Medium allows up to 50 claps per reader, so we know at least 8 people liked it).</p><p>An individual who investigates stuff, but isn't popular, has nowhere they can put their findings&nbsp;<i>and expect others to find them</i>. Sure, you can put up a blog or a Twitter thread, but that hardly means anyone will look at it. Countless times I have been pointed to an outlandish claim treated as obviously true by some science dismissive, Googled for debunkings, and come up empty-handed. There seems to be a point where bullshit is popular enough that lots of people know about it and share it as fact, but either (1) not popular enough for anyone to debunk or (2) debunked by&nbsp;<i>someone who is not popular enough for Google to show</i>.</p><h2><strong>Individual research is demanding on the individual, but often insufficient</strong></h2><p>Even if you find a debunking, that's just the beginning. How can you tell whether the original post is correct or the debunk is correct? Ideally you would read both and decide which one is more believable. But this is time-consuming and not even sufficient: What if the original writer has good a debunk of the debunk that you didn't discover? What if the debunking wasn't very good, making the original seem undeservedly good? How do you know which author is more trustworthy, beyond the plausible-soundingness of their arguments? Maybe somebody is misrepresenting. It seems like, to do a good job, you need to investigate the character of each of the authors involved.</p><p>Or suppose you start from a question, like \"is the linear no-threshold hypothesis a reasonable model of radiation damage?\" If you do the research yourself, you might not find an answer conclusively, except perhaps one that confirms your own bias. When I investigated this question, I found that&nbsp;<i>some search queries</i> on Google Scholar mostly gave me results rejecting the hypothesis, while others gave a bunch of results that defended it. It took years before I stumbled upon something more important:<a href=\"https://twitter.com/DPiepgrass/status/1569508398202515458\">&nbsp;<i><u>actual (approximate) numbers</u></i><u> about risks of radiation</u></a>.</p><p>Most people don't actually have enough time to find obscure facts or verify them; doing it well is hard, time-consuming, and not a topic covered by&nbsp;<i>The Sequences</i>: Yudkowsky treated rationalism as an&nbsp;<i>individual endeavor</i>, envisioning \"rationalist dojos\" where each&nbsp;<i>individual</i> would try to maximize their rationality. That's a fine idea, but it's an inefficient way to raise the sanity waterline. We could reach more accurate beliefs via&nbsp;<i>organized effort</i> than if we each, individually, browse the internet for clues as to what the truth might be.</p><p>So, how can we pool our resources to find truths more efficiently?&nbsp;</p><h1><strong>Computer-aided truthseeking would be easier</strong></h1><p>We have some computer tools for truthseeking, but they have a lot of limitations:</p><ul><li>Wikipedia only covers \"noteworthy\" topics, excludes primary sources and original research, mostly doesn't cover bunk or debunks, and is subject to the biases of its semi-anonymous authors and editors.</li><li>Google weights pages by a secret formula, one affected largely by PageRank. Search results may not be accurate or trustworthy, and trustworthy results by obscure/unpopular authors may not be found or highly-ranked. Information isn't categorized (e.g. I can't specify \"government sources\"). Google has a bias toward the present day, and it's impossible to tell what a search query&nbsp;<i>would have returned</i> had it been input years ago.</li><li>Google scholar finds only academic papers, which tend to be hard to understand because the general public is not their intended audience, the authors are rarely skilled communicators and they often prefer \"sounding scientific\" over plain language. Only sometimes do search results directly address your question. And while papers are more scientific than the average newspaper article... the replication crisis is a thing, and<a href=\"https://slatestarcodex.com/2014/04/28/the-control-group-is-out-of-control/\">&nbsp;<u>The Control Group is Out of Control</u></a>. So one can't simply find a paper and expect it to provide the whole answer, even if it's a meta-analysis.</li><li>StackOverlow and Quora offer answers, but answer scores/rankings are weighted by the opinions of readers, which is not a reliable source of truth.</li><li>Lots of information just isn't publicly available on the internet (e.g. copyrighted books, reports, certain building codes), or is only available in machine-readable form (e.g. csv files) where it's hard to find or visualize.</li></ul><p>What if there were a tool that was more directly useful for finding truth?</p><p>I think we should build such a thing. Done properly, a comprehensive tool would be useful not just for rationalists, but for the general public too.</p><h2><strong>Expected value</strong></h2><p>Since I'm posting to EA forum, I feel like I should say something about how \"valuable\" a tool like this could be in dollars or QALYs or something... but I don't know how. How would you measure the value of Wikipedia, or its impact on catastrophic risks? Clearly it's valuable, but I don't know how to even begin to measure it.&nbsp;</p><p>I view the current epistemic environment as a risk similar in magnitude to climate change. Higher global temperatures create a myriad of risks and costs, none of which can directly cause a catastrophe by themselves. Instead, higher temperatures raise the general risk of conflict, strife and poverty in the world by making life harder or less pleasant for a variety of people. I have funded clean energy efforts not because climate change will directly cause human extinction, but because the rising \"risk waterline\" is a general threat to the health of society and to the natural environment.</p><p>The current epistemic environment is risky in the same way. Some people storm the capitol building because the election was \"stolen\". Others fight against the perceived threat of nuclear energy, or even wind turbines. Another large group risks their lives over vaccines. Still others fight against utilitarianism. Some are \"red pilled\", others \"black pilled\". People seem driven by tribalism, memes and underfunded journalism.&nbsp;<i>Society offers limited means for the average person to improve upon this</i>.</p><p>But compared to climate change, epistemological interventions are highly neglected.</p><p>My idea is just one of many projects that could help raise the sanity waterline, but if successful, I think its value would be comparable to Wikipedia. It would cover domains that Wikipedia doesn't cover in depth, and reaching people that Wikipedia doesn't reach for culture-war reasons. The value of Wikipedia is hard to measure, but intuitively large. My idea is the same way.</p><h1><strong>My idea</strong></h1><p>I think a site should be built for crowdsourced evidence storage, mirroring, transformation, visualization, aggregation, analysis, bayesian evaluation, and search.</p><p>This would be a hard project. It would require a bunch of math (software bayesian reasoning), game theory (to thwart malicious users and bad epistemics), UX design (so it's pleasant to use), \"big data\" (the database would become immense if the project is successful), and a very large amount of code. I am definitely not qualified to design it all myself, but I'll put down some ideas to give you a flavor of my idea.</p><p>Let's call it the \"evidence dump\".</p><p>It would be one of the first attempts at \u201c<a href=\"https://en.wikipedia.org/wiki/Computational_epistemology\">computational epistemology</a>\u201d\u2014to my knowledge. Wikipedia\u2019s page on computational epistemology lists publications going back several decades, yet I\u2019d never heard of it (I just Googled it to see what I would get). But Google gives zero results for \"applied computational epistemology\" which is what I'm going for: a site that uses epistemological theory to aggregate millions of judgements.</p><p>I hope that this site will be informed by rationalist thinking and epistemological theory, and that it will in turn help inform rationalists and AI researchers about epistemological processes relevant to them. For example, emergent failure modes of the site\u2019s algorithms should teach us something. (It is not clear to me whether improving AGI epistemology would be good or bad with respect to x-risk, but otherwise, improving AI epistemology seems like a good thing.)</p><p>I don\u2019t expect the system to change the mind of extremists or people like my father. But my hope is that</p><ul><li>People who do care about truth will appreciate that this is a tool that helps them (or at least, will help them after it becomes popular; how to go from zero to popular is a total mystery)</li><li>Some people will appreciate that it gives them a voice to share their knowledge and experiences, even when there isn\u2019t a big userbase</li><li>It will gently guide various people away from extreme political thinking</li><li>The rest of society will somehow be affected by truthseekers getting better information, e.g. if Metaculus forecasters collectively record, aggregate and view all their observations on the system, maybe the result would be more accurate forecasts, due to each individual forecaster seeing more relevant, better-organized information, and those better forecasts in turn would benefit society. Granted, Metaculus already has an ordinary comment system and it\u2019s unclear how to convince people to record more evidence here than they already record there.</li></ul><h2><strong>Evidence storage</strong></h2><p>The evidence dump is like Twitter for evidence, summaries and claims.</p><p>You can post evidence in two basic forms: either as an eyewitness account (like that story about my father), or as documentation of evidence/claims posted elsewhere.</p><p><strong>Direct evidence entry example:</strong></p><ul><li>Category: crime [posted by Username on March 6, 2022 at 6PM]<ul><li>On March 4, 2022 at ~11 PM, I saw a man break the passenger-side window of a Ford Taurus on Frankfurt St near 12 Avenue in &lt;City&gt; [software autolinks to a map]</li><li>He rummaged inside, put something in his pant pocket, and ran off</li></ul></li><li>Category: Common knowledge among: software developers<ul><li>Dynamically-typed languages typically run more slowly than statically-typed languages.</li></ul></li></ul><p>They say anecdotes are not evidence, but if a million people could be persuaded to record thousands of anecdotes, I think some real signals will be found in the noise. If, in turn, those signals are evaluated within a system designed well enough, perhaps correct analysis will win more views and popularity than incorrect analysis.</p><p>\"Common knowledge\" could be a potentially useful sub-database of the site, if the site becomes popular. One way this could work is that new users are invited to tell us their fields of expertise (with some limits on how many fields one can be an expert in), rate the truth of statements from other people in the same field, and then to \"Name a fact that almost everyone in your field knows, which isn't known or consensus among the general public.\"</p><p>Another sub-database that the site could have is a sort of \"yellow pages\" of products, databases and tools, e.g. \"This URL has a database of climate science data\", \"This URL has a tool for doing task X\".</p><p><strong>Claim/summary example:</strong></p><ul><li>Wikipedia:<a href=\"https://en.wikipedia.org/wiki/2022_Russian_invasion_of_Ukraine\">&nbsp;<u>2022 Russian invasion of Ukraine</u></a><ul><li>Claim: The full-scale invasion began on the morning of 24 February,<a href=\"https://en.wikipedia.org/wiki/2022_Russian_invasion_of_Ukraine#cite_note-32\"><sup><u>[29]</u></sup></a> when Russian president Vladimir Putin announced in his public address a \"special military operation\" for the \"demilitarisation and denazification\" of Ukraine.<a href=\"https://en.wikipedia.org/wiki/2022_Russian_invasion_of_Ukraine#cite_note-33\"><sup><u>[30]</u></sup></a><a href=\"https://en.wikipedia.org/wiki/2022_Russian_invasion_of_Ukraine#cite_note-Waxman_2022-34\"><sup><u>[31]</u></sup></a><ul><li>[29] Nikolskaya, Polina; Osborn, Andrew (24 February 2022).<a href=\"https://www.reuters.com/world/europe/russias-putin-authorises-military-operations-donbass-domestic-media-2022-02-24/\">&nbsp;<u>\"Russia's Putin authorises 'special military operation' against Ukraine\"</u></a>.<a href=\"https://en.wikipedia.org/wiki/Reuters\">&nbsp;<i><u>Reuters</u></i></a>. Moscow.<a href=\"https://web.archive.org/web/20220224032217/https://www.reuters.com/world/europe/russias-putin-authorises-military-operations-donbass-domestic-media-2022-02-24/\">&nbsp;<u>Archived</u></a> from the original on 24 February 2022. Retrieved 31 May 2022.</li><li>[30] Grunau, Andrea; von Hein, Matthias; Theise, Eugen; Weber, Joscha (25 February 2022).<a href=\"https://www.dw.com/en/fact-check-do-vladimir-putins-justifications-for-going-to-war-against-ukraine-add-up/a-60917168\">&nbsp;<u>\"Fact check: Do Vladimir Putin's justifications for going to war against Ukraine add up?\"</u></a>.<a href=\"https://en.wikipedia.org/wiki/Deutsche_Welle\">&nbsp;<u>Deutsche Welle</u></a>.<a href=\"https://web.archive.org/web/20220225162831/https://www.dw.com/en/fact-check-do-vladimir-putins-justifications-for-going-to-war-against-ukraine-add-up/a-60917168\">&nbsp;<u>Archived</u></a> from the original on 25 February 2022. Retrieved 1 June 2022.</li><li>[31] Waxman, Olivia B. (3 March 2022).<a href=\"https://time.com/6154493/denazification-putin-ukraine-history-context/\">&nbsp;<u>\"Historians on What Putin Gets Wrong About 'Denazification' in Ukraine\"</u></a>.<a href=\"https://en.wikipedia.org/wiki/Time_(magazine)\">&nbsp;<i><u>Time</u></i></a>.<a href=\"https://en.wikipedia.org/wiki/ISSN_(identifier)\">&nbsp;<u>ISSN</u></a>&nbsp;<a href=\"https://www.worldcat.org/issn/0040-781X\"><u>0040-781X</u></a>.<a href=\"https://en.wikipedia.org/wiki/OCLC_(identifier)\">&nbsp;<u>OCLC</u></a>&nbsp;<a href=\"https://www.worldcat.org/oclc/1311479\"><u>1311479</u></a>.<a href=\"https://web.archive.org/web/20220303211420/https://time.com/6154493/denazification-putin-ukraine-history-context/\">&nbsp;<u>Archived</u></a> from the original on 3 March 2022. Retrieved 1 June 2022.</li></ul></li></ul></li></ul><p>Claims from Wikipedia should be generated automatically or semi-automatically.</p><ul><li>Academic paper:<a href=\"https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2009EO030002\">&nbsp;<u>Doran 2009&nbsp;</u></a></li><li>Topic: Climate change | consensus<ul><li>Summary: 75 of 77 respondents to the Doran 2009 survey of mostly North American scientists \u201cwho listed climate science as their area of expertise and who also have published more than 50% of their recent peer-reviewed papers on the subject of climate change\u201d agreed that \u201chuman activity is a significant contributing factor in changing mean global temperatures.\u201d&nbsp;</li><li>However, the total number in the category was 79. A reasonable interpretation of this data is that when asked \u201cdo you think that mean global temperatures have generally risen,\u201d two said no and then didn\u2019t answer the question about whether \u201chuman activity is a significant contributing factor\u201d and therefore weren\u2019t counted. This interpretation of the data suggests a 95% consensus rather than 97%.</li></ul></li></ul><p>Ideally, the site could gather information about scientific papers automatically:</p><ul><li>Journal: Eos, Vol. 90, No. 3, 20 January 2009</li><li>Not retracted.</li><li>923 citations</li></ul><p>It's often tempting for users to provide more than just a summary, but also some kind of analysis, as seen here where N=79 but somehow \"75 of 77\" agreed with the consensus statement. I suppose the analysis part (\"A reasonable interpretation is...\") should be split off somehow from the summary proper, and that the site must make such splitting easy to do.</p><p>It's important that claims and summaries be brief, because they are re-used in aggregation and analyses. If a summary needs to be long, it can be broken into parts that can be referenced and rated individually.</p><p>Different people would be able to submit competing or complementary summaries of the same source document, with some kind of voting to find the best summaries, and some way to flag errors in summaries and downrank them on the basis of accuracy.</p><p>Summarizing claims by cranks and conspiracy theorists is good and encouraged; at this level the site is merely cataloging, not judging.</p><p>Claims based on books should provide an excerpt from the book that supports the claim, perhaps as a photo.</p><h2><strong>Mirroring and transformation</strong></h2><p>I want a database of databases (or database of datasets). Let's consider Covid-19 pandemic data: each country (or province) produces its own <i>bespoke</i> database which can typically be viewed on a bespoke country-specific web site (which provides some visualizations and not others), and is typically also offered in a bespoke electronic form.</p><p>At a minimum, one could publish links to all the various web sites and machine-readable datasets about a topic on the evidence dump.</p><p>Often there is someone who volunteers to gather and transform data from many places with a bespoke program published on GitHub. If so, someone should be able to import the output of that program into the evidence dump (and other people should be able to review the output and claim that it accurately reflects particular sources or not: \"data about X matches source Y\").</p><p>Ideally, the evidence dump itself would support some kind of data import process that involves transformations on the source data to produce a common schema.</p><h2><strong>Aggregation</strong></h2><p>Sources can be grouped. Ideally we would find ways to do this semi-automatically. Example:</p><ul><li>Studies addressing the question \"&lt;question&gt;?\"<ul><li>Study 1<ul><li>Claim/conclusion</li></ul></li><li>Study 2<ul><li>Claim/conclusion</li></ul></li><li>Study 3<ul><li>Claim/conclusion</li></ul></li><li>Study 4<ul><li>Claim/conclusion</li></ul></li></ul></li></ul><p>More broadly, lists of things in categories are useful. People should be able to publish lists about any topic, and other people should be able to vote about certain aspects of the items on the lists. If it's a list of tools for task X, for example, people could vote on how good tool T is for task X.</p><h2><strong>Analysis</strong></h2><p>To publish an analysis, users would take a bunch of claims, and write some logic involving those claims to produce a conclusion.</p><p>In the first draft of this proposal, I suggested a very precise writing style similar to legalese:</p><ul><li>Humans have been adding CO2 to the atmosphere, mostly by burning fossil fuels<ul><li>Reference(s) on this topic</li></ul></li><li>Natural carbon sources and sinks, such as oceans and plants, are net absorbers of CO2 from the atmosphere every year<ul><li>Reference(s) on this topic</li></ul></li><li>There are no major CO2 emitters except humans (including man-made machines) and natural carbon sinks</li><li>Therefore, given humans are responsible for the increase of CO2 in the atmosphere in the modern era (20th/21st centuries)<ul><li>This reasoning relies on the law of conservation of mass and the fact that CO2 is not created or destroyed in the atmosphere itself.</li></ul></li></ul><p>Here it says \"net absorbers of CO2 from the atmosphere\" rather than just \"net absorbers of CO2\"; there is an exhaustivity clause (third point); it has the phrase \"including man-made machines\" to clarify that we're not just talking about what comes directly from human bodies; and the law of conservation of mass is mentioned.</p><p>I was thinking that politically sensitive topics need careful, complete, qualified wording that is correct and complete so that the conclusion cannot be reasonably contested, and then the voting system would help such careful treatments to rise to the top.</p><p>But this seems unacceptable: people don't want to read or write stuff like this. Clear but easy-to-read statements should rise to the top, or the site can't gain popularity. However, caveats and clarifications are important; they need to be included and preserved. I am unsure how to achieve the right balance.</p><p>The result of an analysis can be used to support other analyses, forming a DAG (directed acyclic graph) of analyses, i.e. a network of proposed conclusions.</p><p>But before we move on, notice that the third clause is a negative statement (\"There are no...\"). They say you can't prove a negative, but you kinda can: as long as no one can find credible evidence against it, negative statements should generally stand.</p><p>I don't quite know how, but&nbsp;</p><ul><li>An analysis should be somewhat resistant to cherry picking. For example, an author could cherry pick two papers that support a desired conclusion, but other users should be able to add additional scientific papers that throw a wrench into the analysis.</li><li>Users should be able to suggest constructive modifications to an analysis to fix flaws or omissions; other users and the author could then vote on them.</li><li>There must be ways to resolve conflicts in the underlying data, and I'm not sure how. For instance, let's say we aggregate all the papers on ivermectin efficacy against Covid. But, some papers said it's effective, others said it's not effective, and others said it may be effective but statistical significance wasn't reached. The simplest possible technique would be to count the number of papers saying \"effective\" vs \"ineffective\"/\"harmful\", and since more papers suggest \"effective\" than \"ineffective\"/\"harmful\", \"effective\" wins. But this is insufficient:<ul><li>the papers vary wildly in quality, sample size, and statistical methods</li><li>a couple were fraudulent/retracted</li><li>there are more possible interpretations than just \"effective\" and \"ineffective\", e.g.<a href=\"https://astralcodexten.substack.com/i/43667275/the-synthesis\">&nbsp;<u>WORMS</u></a>!</li></ul></li><li>It seems to me that the site needs to have a concept of&nbsp;<i>paradigms</i> of interpretation (hypotheses or theories) and bring them together to compete against each other. For ivermectin, one paradigm would be \"Ivermectin is effective against Covid\" and another would be \"Ivermectin is effective against worms, which incidentally helps people with Covid if they also have worms. However, ivermectin is not very effective against Covid itself\". A third would be \"it's complicated. While many papers suggest it is effective, the body of evidence about ivermectin includes some fraudulent papers and papers with poor methodology, papers whose results may have been affected by Strongyloides infections, and was on the whole affected by publication bias\u201d. Then, users can help rate how well the available evidence fits each paradigm.<ul><li>Statements of uncertainty, like \"It's unclear if ivermectin works\", are inappropriate paradigms, but you could say \"The evidence that ivermectin works is similar in strength to the evidence that it doesn't work\". Or, we might see similar scores (assigned by crowd review) to \"Ivermectin is effective\" and \"Ivermectin is ineffective\" paradigms, which also implies that it's unclear if ivermectin works.</li><li>Or, let's consider global warming, where paradigms could include \"it's caused by the sun\", \"it's caused by natural internal variability\", \"it's caused by greenhouse gases emitted by humans\", and \"the apparent temperature changes were faked by a global conspiracy\". Any user can evaluate any piece of evidence against any paradigm, hopefully allowing the correct paradigm to rise to the top.</li></ul></li><li>Creating an analysis should not be much more onerous than it needs to be; a sufficiently friendly user interface is necessary and may be tricky to achieve.<ul><li>A user should be able to convert an&nbsp;<i>essay</i> they have written into an&nbsp;<i>analysis</i>. For this purpose I'm guessing that users will need to be able to edit multiple levels of analysis at once, and/or have editable claims or 3rd-party evidence nested inside an analysis.</li></ul></li></ul><p>One more thing is that it seems like an analysis should be able to support or cast doubt on a source: you should be able to argue \"source X produces lots of bunk because &lt;reasons&gt;\", and if the community agrees, \"source X\" gets a lower score/reputation, and that lower score would then affect the rating of other analyses that rely on that same source.</p><h2><strong>Shopping</strong></h2><p>Wait, shopping? Does this really belong here?</p><p>A few months ago I wanted to find a 12V microwave, stove, or slow cooker with input power between 200W and 700W (with 110V AC as a nice-to-have) and capacity over 2L.</p><p>These are impossible products to find. When you search for this sort of product, you find food warmers under 100 watts, microwaves over 1000 watts, propane stoves, space heaters, food thermoses, and contraptions I don't even recognize. After looking at a few&nbsp;<i>hundred</i> product links and a couple dozen product pages I found... not one single item matching this description. Sites such as Google Shopping, Amazon, and AliExpress do not seem to support even the most basic constraints on search results, such as \"these word(s) must be in the product title\" or even \"these word(s) must be somewhere in the product description\". I'm baffled and frustrated that online shopping could be this bad decades after the concept was invented.</p><p>Seems to me that if a shopping site could actually find what you're looking for, that could be worth money.</p><p>There are some keys to this sort of thing working well.</p><ol><li>We need products in a database with a consistent schema. This requires some code; if a product has an input voltage of \"12V DC\" and an input current of \"10A\", for example, an \"input wattage\" of 120W can be inferred (the fact that it's&nbsp;<i>inferred</i> rather than&nbsp;<i>measured</i> being an additional piece of information that ought to be stored). But could we build a site with a means to standardize schemas like this? Could users and/or companies be convinced to add their household products to the database? Could we resolve disputes via voting? For example, if the specs to a dosimeter say it handles 1000\u00b5Sv/hour but a user finds that it severely undermeasures above 300\u00b5Sv/hour, this could override the manufacturer claim to become the default value in the database if the evidence is good enough (user has high reputation or other users in good standing corroborate the claim). And perhaps the site can compute a probability distribution on the max dose rate field, and feed that into the search ranking algorithm. And perhaps users could propose new fields representing different thresholds like \"max radiation measured within 2x of reality\" and \"maximum radiation reading ever witnessed\", figures that could be crowdsourced by hobbyists.</li><li>The plain-text search system should connect with the schema. If I search for \"12V microwave, stove, or slow cooker, with input power between 200W and 700W and capacity over 2L\", I imagine the site could guess some search constraints and propose them to me.</li><li>As always in internet search, ranking order is important and needs some thought.</li><li>There should be product comparison matrices, where you select some products and then can create a data table comparing their specs. With charts?</li></ol><p>I bring up \"shopping\" for a couple of reasons. First, areas like this could potentially have some kind of business model, so that the site doesn't have to be funded entirely by donations.</p><p>Second, the same features are useful for any kind of research. It makes just as much sense to make a consistent schema, an excellent search system and a comparison table for a list of mines, for example:</p><pre><code>Search: [tag 'mine'; 'Common Name' contains 'Quarry'&nbsp;               ]\nCommon Name   | Country | Type                               | Annual Production\nXYZ Quarry&nbsp;   | Germany | fossil fuel &gt; coal &gt; lignite (etc) | 134,000,000 tons (2021)\nABC Quarry&nbsp;   | DRC     | precious &gt; diamond                 | 1327 kg (2018)\n...</code></pre><p>Among the many challenges in making a system like this is crowdsourcing everything, making everything up for debate, and yet still making something with a schema consistent enough to be useful, with \"best-guess\" data accurate enough to be useful. Each&nbsp;<i>individual cell</i> in this table could potentially have dozens of pages of debate and hundreds of votes affecting it, with just the \"best guesses\" shown when a user views a table. And of course, there may be dozens or hundreds of columns that are not shown.</p><h3><strong>Bayesian evaluation</strong></h3><p>Here's where I'm stumped: we need transparent algorithms to grade the analyses and claims under various paradigms, in ways that point us toward the truth. I'm not sure how.</p><p>The voting systems on the site, whatever they are, should discourage partisan thinking (\"like/dislike\", \"agree/disagree\") in favor of analytical thinking (\"does the conclusion follows from the premises?\", \"does this claim support/refute this statement?\").</p><p>Nevertheless, I would assume that in the long run people will try to game the system via lying and motivated reasoning\u2014votes that are lies, summaries that misrepresent the source material accidentally or deliberately, fabricated sources, bad analysis, and spamming. For this reason, a user reputation system (like StackOverflow) is also needed.</p><p>It seems like there should be some way to evaluate third-party claims, but it's not clear how to do that in the plan laid out above. For example, ideally the software could detect, not just declare by fiat, that Fox News or MSNBC editorials are less reliable than Wikipedia, PBS or Reuters.</p><p>Finally, of course, I want some kind of automatic Bayesian reasoning to compute estimates of how likely various ideas are to be true, at least in some cases. How to do this... let's call it an exercise for the reader.</p><h2><strong>Search</strong></h2><p>Information will be linked together, ranked and structured in ways that facilitate searching. For example, high-rated paradigms should be listed high on search results.&nbsp;</p><p>Also, pieces of evidence will have various data fields attached, such as dates, locations, categories and tags, that should be searchable.</p><h2><strong>Charts</strong></h2><p>I'm constantly looking for charts. I like to ask Google images because sometimes it finds the answer quickly. But Google images doesn't seem to think charts/graphs are important; there are search categories for \"Clip art\", \"line drawing\" and \"GIF\" but not \"chart\". Often it finds something on Statista, which then asks for money to actually see a chart. Often the data I want to see is freely available, but the chart is not. Finding data in non-chart form is hard though; often I just give up.</p><p>Ideally, the evidence dump would have its own tools to easily visualize data. I'm inspired somewhat by<a href=\"https://youtu.be/TmhQCQr_DCA?t=867\">&nbsp;<u>Power BI</u></a>. I find parts of Power BI painful, but once you have data in the right form, you can create a pretty wide variety of useful charts and tables very quickly and easily. Plus, you can combine multiple charts and tables and control widgets in a \"report\".</p><p>I imagine users creating visualizations via some drag-and-drop interface, and then publishing them with an informative title, so other people can find them via site search or via Google image search. The result: a volunteer-run interactive Statista without the paywall.</p><h2><strong>Theory of growth?</strong></h2><p>It would be unwise to build a site run by a volunteer community without a plausible theory for how the community will materialize. This will be a system with network effects, where a very big site is about quadratically more valuable than a small one.</p><p>I haven't really worked this part out, but I imagine a key aspect is that early versions of the site need to seem somehow more useful, valuable or fun than an ordinary blog for the people writing stuff in it. I guess it should initially be tailored to only one piece of its mission \u2014 one that doesn't depend a lot on network effects.</p><h2><strong>This won't happen</strong></h2><p>I'm a senior software developer (CTO, technically) with about 20 years' experience. My job is to build high-quality software that&nbsp;<i>doesn't benefit the world in any way</i>. I'd rather make different software, but for that I need funding. As it is, I was too busy for the last six months to even publish this article. Assuming&nbsp;<i>charity</i> funding isn't available, I'm looking for ideas on</p><ul><li>for-profit business models to build&nbsp;<i>any part</i> of this idea that could eventually become part of a non-profit evidence dump.</li><li>people who could act as my cofounder in a startup in a market related to this</li><li>specifics of the user interface, data model or epistemological methods (if you have expertise in computational epistemology, please help!)</li><li>ways this idea could interact/intersect with related projects such as Metaculus and LessWrong</li></ul>", "user": {"username": "dpiepgrass"}}]