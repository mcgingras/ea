[{"_id": "Tph425HEYkdks49WP", "postedAt": "2023-01-07T22:11:12.948Z", "postId": "EJ5a2ApokQqGB98P8", "htmlBody": "<blockquote><p><strong>I don't think alignment is a problem that can be solved. I think we can do better and better. But to have it be existentially safe, the bar seems really, really high and I don't think we're going to get there. So we're going to need to have some ability to coordinate and say let's not pursue this development path or let's not deploy these kinds of systems right now.</strong></p></blockquote><p>Holden makes a similar point in \"<a href=\"https://www.alignmentforum.org/posts/vZzg8NS7wBtqcwhoJ/nearcast-based-deployment-problem-analysis#fn4\">Nearcast-based 'deployment problem' analysis</a>\" (2022):</p><blockquote><p>I don\u2019t like the framing of \u201csolving\u201d \u201cthe\u201d alignment problem. I picture something like \u201cTaking as many measures as we can (<a href=\"https://www.alignmentforum.org/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very\">see previous post</a>) to make catastrophic misalignment as unlikely as we can for the specific systems we\u2019re deploying in the specific contexts we\u2019re deploying them in, then using those systems as part of an ongoing effort to further improve alignment measures that can be applied to more-capable systems.\u201d In other words, I don\u2019t think there is a single point where the alignment problem is \u201csolved\u201d; instead I think we will face a number of \u201calignment problems\u201d for systems with different capabilities. (And I think there could be some systems that are very easy to align, but just not very powerful.) So I tend to talk about whether we have \u201csystems that are both aligned and transformative\u201d rather than whether the \u201calignment problem is solved.\u201d</p></blockquote>", "parentCommentId": null, "user": {"username": "Will Aldred"}}]