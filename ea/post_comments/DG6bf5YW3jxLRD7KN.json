[{"_id": "rCBWaz5hK5uyaEiF4", "postedAt": "2023-09-16T15:45:02.461Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p>Good post.</p><p>You want to give a regulator the power to decide which large training runs are safe. I think this policy's effects depend tremendously on the regulator\u2014if it's great at distinguishing safe stuff from dangerous stuff and it makes great choices, the policy is great; if not, it's not. I feel pretty uncertain about how good it would be, and I suspect some disagreements about this policy are actually disagreements about how good the regulator would be. It feels hard to evaluate a proposal that leaves so much up to the regulator.</p><p>(Maybe it would help to have a concrete illustrative line to help readers get a sense of what you think the regulator would ideally do, like \"LLMs and bio stuff with training compute &gt; 1e24 FLOP are banned, everything else is not.\" Ideally the regulator would be more sophisticated than that, of course.)</p>", "parentCommentId": null, "user": {"username": "zsp"}}, {"_id": "SrzQeimKzuRtbBCdj", "postedAt": "2023-09-16T15:45:17.119Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p>Small stuff:</p><p>(1) Watermarking</p><blockquote><p>Required watermarking and traceability on advanced models, so that we can match AI outputs to specific AI models and developers.</p></blockquote><p>Watermarking is mostly an open technical problem\u2014there's no great existing best-practice that government can just require labs to implement. (I know you mean that government should require the limited stuff we know how to do.)</p><p>(2) Incident reporting</p><blockquote><p>Some ideas for increasing the government\u2019s&nbsp;<strong>visibility</strong> into AI development are</p></blockquote><p>Government should also facilitate or require&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/qkK5ejystp8GCJ3vC/incident-reporting-for-ai-safety\"><u>incident reporting</u></a>.</p><p>(3) Overhang</p><p><img style=\"width:59.89%\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/SrzQeimKzuRtbBCdj/bpjc0gpqoqm8flpurwn6\"></p><p>If the pause threshold is in terms of FLOP rather than E-FLOP, the dotted green line and horizontal blue line should actually be slightly upward sloping.</p><p>(If the pause threshold is in terms of E-FLOP, then (a) the FLOP threshold needs to decrease over time and (b) capabilities still increase during the pause because of inference-time improvements in algorithms and increases in compute.)</p><p>(Also I want to flag that assuming progress is linear in log(E-FLOP) over time by default is a reasonable simplification for your purposes, but it is a simplification.)</p>", "parentCommentId": null, "user": {"username": "zsp"}}, {"_id": "f9C4kwM8LkuiiyuhC", "postedAt": "2023-09-16T20:30:28.469Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p>This is a good insight - I definitely feel like lack of trust (due partly to uncertainty) in the proposed regulator is a big blocker for me feeling at all on board with pause/regulation more broadly. Especially relevant given that I think the <a href=\"https://www.aipolicy.us/work\">original CAIP proposals</a> missed the mark by some margin. I acknowledge that Thomas is writing in his personal capacity, but I think that the link is still relevant.</p>", "parentCommentId": "rCBWaz5hK5uyaEiF4", "user": {"username": "tommcgrath"}}, {"_id": "pmcjQgqrBkz99fE86", "postedAt": "2023-09-16T21:08:18.293Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<blockquote><p>I think the <a href=\"https://www.aipolicy.us/work\">original CAIP proposals</a> missed the mark by some margin</p></blockquote><p>Their original criteria for \"frontier AI\" was very broad, but an expansive definition makes sense if you think the regulator will be great\u2014you give it lots of discretion to reject unsafe stuff but it can quickly approve safe stuff. I think disagreements about CAIP's central proposal come down to different intuitions about how good the regulator would be\u2014I think Thomas thinks the regulator would quickly approve almost all clearly-safe stuff, so an expansive scope does little harm.</p>", "parentCommentId": "f9C4kwM8LkuiiyuhC", "user": {"username": "zsp"}}, {"_id": "DXfAoG9GzyyXxGHHc", "postedAt": "2023-09-17T02:06:05.716Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p>Thanks for writing this up!</p><p>I hope to write a post about this at some point, but since you raise some of these arguments, I think the most important cruxes for a pause are:</p><ol><li>It seems like in many people's models, the reason the \"snap back\" is problematic is that the productivity of safety research is much higher when capabilities are close to the danger zone, both because the AIs that we're <i>using </i>to do safety research are better and because the AIs that we're <i>doing the safety research on </i>are more similar to the ones in the danger zone. If the \"snap back\" reduces the amount of calendar time during which we think AI safety research will be most productive in exchange for giving us more time overall, this could easily be net negative. On the other hand, a pause might just \"snap back\" to somewhere on the capabilities graph that's still outside the danger zone, and lower than it would've been without the pause for the reasons you describe.</li><li>A huge empirical uncertainty I have is: how elastic is the long-term supply curve of compute? If, on one extreme end, the production of computing hardware for the next 20 years is set in stone, then at the end of the pause there would be a huge jump in how much compute a developer could use to train a model, which seems pretty likely to produce a destabilizing/costly jump. At the other end, if compute supply were very responsive to expected AI progress and a pause would mean a big cut to e.g. Nvidia's R&amp;D budget and TSMC shelved plans for a leading-node fab or two as a result, the jump would be much less worrying in expectation. I've heard that the industry plans pretty far in advance because of how much time and money it takes to build a fab (and how much coordination is required between the different parts of the supply chain), but it seems like at this point a lot of the future expected revenue to be won from designing the next generations of GPUs comes from their usefulness for training huge AI systems, so it seems like there should at least be some marginal reduction in long-term capacity if there were a big regulatory response.</li></ol>", "parentCommentId": null, "user": {"username": "levin"}}, {"_id": "pQSrDkGZjrtnCQDet", "postedAt": "2023-09-18T04:36:33.568Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p>Thanks for sharing this Thomas!</p><p>I would like to hear your thoughts on the centralisation of power this would cause. Historically it seems like multiple parties owning the means of production, and having wide latitude with how to employ this capital, has been a key driver of human progress and protector of liberty. In the event of TAI I worry that a powerful regulator like this would effectively centralise a huge degree of control over society and the economy in the hands of the regulator. For example, the regulator might adopt a definition of safety that incorporates political notions (e.g. what makes an output 'toxic'), resulting in only ideologically compliant firms being are allowed to run large models, and hence giving these firms allied to the administration a major advantage over competitors. Note that this is not regulatory capture, and hence I don't expect conflict of interest rules to resolve it.</p><p>Of course if the alternative is literally extinction then perhaps some people's answer is (to some degree) 'so be it'.</p>", "parentCommentId": null, "user": {"username": "Larks"}}, {"_id": "Hx6TJkmiFseE3HwYi", "postedAt": "2023-09-18T18:02:08.246Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p>Yeah, this sounds right to me. At present I feel like a regulator would end up massively overrepresenting at least one of (a) the EA community and (b) large tech corporations with pretty obviously bad incentives.</p>", "parentCommentId": "pmcjQgqrBkz99fE86", "user": {"username": "tommcgrath"}}, {"_id": "Aa3xovhvCk4fvm7Tj", "postedAt": "2023-09-18T18:06:26.297Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p>Hmm, I don't see what goes wrong if the regulator overrepresents EA. And overrepresenting the major labs is suboptimal but I'd guess it's better than no regulation\u2014it decreases multipolarity among labs and (insofar as major labs are relatively safe and want to require others to be safe) improves safety directly.</p>", "parentCommentId": "Hx6TJkmiFseE3HwYi", "user": {"username": "zsp"}}, {"_id": "b7hmsK7XTTJDeGGEZ", "postedAt": "2023-09-18T18:32:10.663Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p>A regulator overrepresenting EA seems bad to me (not an EA) because:</p><ol><li>I don't agree with a lot of the beliefs of the EA community on this subject and so I'd expect an EA-dominated regulator to take actions I don't approve of.</li><li>Dominance by a specific group makes legitimacy much harder.</li><li>The EA community is pretty strongly intertwined with the big labs so most of the concerns from there carry over.</li></ol><p>I don't expect (1) to be particularly persuasive for you but maybe (2) and (3) are. I find some of the points in <a href=\"https://1a3orn.com/sub/essays-regulation-stories.html\">Ways I Expect AI Regulation To Increase X-Risk</a> relevant to issues with overrepresentation of big labs. I think the overrepresentation of big labs would lead to a squashing of open-source, for instance, which I think is currently beneficial and would remain beneficial on the margin for a while.</p><p>More generally, I don't particularly like the flattening of specific disagreements on matters of fact (and thus subsequent actions) to \"wants people to be safe\"/\"doesn't want people to be safe\". I expect that most people who disagree about the right course of action aren't doing so out of some weird desire to see people harmed/replaced by AI (I'm certainly not) and it seems a pretty unfair dismissal.</p>", "parentCommentId": "Aa3xovhvCk4fvm7Tj", "user": {"username": "tommcgrath"}}, {"_id": "9TAwBPBWAmykyoaaq", "postedAt": "2023-09-18T18:36:01.189Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p>OK.</p><p>Re \"want to require others to be safe\"\u2014that was poorly worded, I meant <i>wants to require everyone to follow specific safety practices they already follow, possibly to slow competitors in addition to safety reasons</i>.</p>", "parentCommentId": "b7hmsK7XTTJDeGGEZ", "user": {"username": "zsp"}}, {"_id": "NHvF4aAc6mTKHq2pn", "postedAt": "2023-09-18T18:49:17.031Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p>Cool, apologies if that came across a bit snarky (on rereading it does to me). I think this was instance N+1 of this phrasing and I'd gotten a bit annoyed by instances 1 through N which you obviously bear no responsibility for! I'm happy to have pushed back on the phrasing but hope I didn't cause offence.</p><p>A more principled version of (1) would be to appeal to moral uncertainty, or to the idea that a regulator should represent all the stakeholders and I worry than an EA-dominated regulator would fail to do so.</p>", "parentCommentId": "9TAwBPBWAmykyoaaq", "user": {"username": "tommcgrath"}}, {"_id": "u8jfMJhKtJbFiHoqg", "postedAt": "2023-09-23T06:01:56.402Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p>I'll be looking forward to hearing more about your work on whistleblowing! I've heard some <a href=\"https://forum.effectivealtruism.org/posts/iqDt8YFLjvtjBPyv6/some-things-i-heard-about-ai-governance-at-eag#Whistleblowing\">promising takes</a> about this direction. Strikes me as broadly good and currently neglected.</p>", "parentCommentId": null, "user": {"username": "Rocket"}}, {"_id": "ku2jwEXAWDnakkqWk", "postedAt": "2023-09-23T11:48:06.749Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p>Naively I would trade a lot of clearly-safe stuff being delayed or temporarily prohibited for even a minor decrease in chance of safe-seeming-but-actually-dangerous stuff going through, which pushes me towards favoring a more expansive scope of regulation.</p><p>(in my mind the potential loss of decades of life improvements currently pale vs potential non-existence of all lives in the longterm future)</p><p>Don't know how to think about it when accounting for public opinion though, I expect a larger scope will gather more opposition to regulation, which could be detrimental in various ways, the most obvious being decreased likelihood of such regulation being passed/upheld/disseminated to other places.</p>", "parentCommentId": "pmcjQgqrBkz99fE86", "user": {"username": "Leksu"}}, {"_id": "tDpntmCbEHd8dgcHn", "postedAt": "2023-09-23T15:47:48.826Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p>Great stuff Thomas.</p><blockquote><p>I think a pause on AI progress wouldn\u2019t be very helpful unless used in concert with other effective governance interventions, such as the ones that I have outlined above.</p></blockquote><p>Agree.</p><blockquote><p>A longer pause that lasts until we are confident that we have robust AI safety measures in place that allow for safe deployment would be helpful. I\u2019m currently in favor of building the capacity of the world to create a long pause on AI.&nbsp;</p><p>As a result, I\u2019m only excited about versions of a pause that don\u2019t return to \u201cAI progress as usual\u201d, after the pause is over.&nbsp;</p></blockquote><p>Yes, I don't think anyone is seriously proposing a fixed-expiry pause at this point (FLI's \"6 month\" letter was really just a foot-in-the-d\u0336o\u0336o\u0336r\u0336Overton-Window I think). Pause in my thinking is basically shorthand for \"global indefinite pause of frontier AI development, until global consensus is reached on an x-safety solution (including solving the alignment problem, preventing misuse, and ensuring multi-agent coordination); including accepting that this may not be possible such that the pause becomes effectively permanent<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdhyddhmds8\"><sup><a href=\"#fndhyddhmds8\">[1]</a></sup></span>\".</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndhyddhmds8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdhyddhmds8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>but fear not we can still have a good future including all the nice things, it might just take a bit longer</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "gmSC2yAirCWm35M52", "postedAt": "2023-09-23T17:57:10.965Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p>Any realistic pause would only be lifted once there is a consensus on a potential solution to x-safety (or at least, say, full solutions to all jailbreaks, mechanistic interpretability and alignment up to the (frozen) frontier). If compute limits are in place during the pause, they can gradually be ratcheted up, with evals performed on models trained at each step, to avoid any such sudden snap back.</p>", "parentCommentId": "DXfAoG9GzyyXxGHHc", "user": {"username": "Greg_Colbourn"}}, {"_id": "utchkCkmCQCnLGQnt", "postedAt": "2023-10-12T10:27:51.105Z", "postId": "DG6bf5YW3jxLRD7KN", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/users/tlevin?mention=user\">@tlevin</a> I would be interested in you writing up this post, though I'd be even more interested in hearing your thoughts on the regulatory proposal Thomas is proposing.</p><p>Note that both of your points seem to be arguing against a <i>pause</i>, whereas my impression is that Thomas's post focuses more on implementing a national regulatory body.</p><p>(I read Thomas's post as basically saying like \"eh, I know there's an AI pause debate going on, but actually this pause stuff is not as important as getting good policies. Specifically, we should have a federal agency that does licensing for frontier AI systems, hardware monitoring for advanced chips, and tracking of risks. If there's an AI-related emergency or evidence of imminent danger, then the agency can activate emergency powers to swiftly respond.\"</p><p>I think the \"snap-back\" point and the \"long-term supply curve of compute\" point seem most relevant to a \"should we pause?\" debate, but they seem less relevant to Thomas's regulatory body proposal. Let me know if you think I'm missing something, though!)</p>", "parentCommentId": "DXfAoG9GzyyXxGHHc", "user": {"username": "Akash"}}]