[{"_id": "hkv6ELzynSYsZF4JM", "postedAt": "2022-09-04T23:17:03.337Z", "postId": "HzxJ8v2zv68WQHAeH", "htmlBody": "<p>As an aside, if both average-ism and totalism lead to results that seem discordant with our moral intuitions, why do we need to choose <em>between</em> them?  Wouldn't it make sense to look for a function combining some elements of each of these?</p>\n", "parentCommentId": null, "user": {"username": "david_reinstein"}}, {"_id": "zB4B7bMpXshhMAg4K", "postedAt": "2022-09-05T22:07:14.631Z", "postId": "HzxJ8v2zv68WQHAeH", "htmlBody": "<p>There's a <a href=\"https://www.iffs.se/media/2264/an-impossibility-theorem-for-welfarist-axiologies-in-ep-2000.pdf\">proof </a>showing that any utilitarian ideology violates either the repugnant or sadistic conclusion (or anti-egalitarianism, incentivizing an unequal society), so you can't cleverly avoid these two conclusions with some fancy math. To add, any fancy view you create will be in some sense unmotivated - you just came with a formula that you like, but <i>why</i> would such a formula be true? Totalism and averagism seem to be the two most interpretable utilitarian ideologies, with totalism caring only about pain/pleasure (and not <i>by whom</i> this pain/pleasure is experienced) and averagism being the same except <i>population-neutral</i>, not incentivizing a larger population unless it has higher average net pleasure. Anything else is kind of an arbitrary view invented by someone who is too into math.</p>", "parentCommentId": "hkv6ELzynSYsZF4JM", "user": {"username": "RedStateBlueState"}}, {"_id": "TbuFNJc3xesurv5fH", "postedAt": "2022-09-06T00:49:27.717Z", "postId": "HzxJ8v2zv68WQHAeH", "htmlBody": "<p>The anti-egalitarianism one seems to me to be the least obviously necessary of the three <sup class=\"footnote-ref\"><a href=\"#fn-4F67JcEaSnByTkwMt-1\" id=\"fnref-4F67JcEaSnByTkwMt-1\">[1]</a></sup>. It doesn't seem obviously wrong that for this abstract concept of 'utility' (in the hedonic sense), there may be cases and regions in which it's better to have one person with a bit more and another with a bit less.</p>\n<p>But more importantly, I think, why is it so bad that it is 'unmotivated'.  In many domains we think that 'a balance of concerns' or 'a balance of inputs' yields the best outcome under the constraints.</p>\n<p>So why shouldn't a reasonable moral valuation ('axiology') involve some balance of interest in total welfare and interest in average welfare? It's hard to know where that balancing point should lie (although maybe some principles could be derived). But that still doesn't seem to invalidate it...  any more than my liking some combination of work and relaxation, or believing that beauty lies in a balance between predictability and surprise, etc.</p>\n<p>I wouldn't think 'invented by someone too into math' (if that's possible :) ). If anything I think the opposite.  I am accepting that a valuation of what is moral could be valid and defensible even if it can't be stated in as stark axiomatic terms as the extreme value systems.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-4F67JcEaSnByTkwMt-1\" class=\"footnote-item\"><p>Although many EAs seem to be ok with the repugnant conclusion also. <a href=\"#fnref-4F67JcEaSnByTkwMt-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "parentCommentId": "zB4B7bMpXshhMAg4K", "user": {"username": "david_reinstein"}}, {"_id": "mGBqYRiAavxbzyAPp", "postedAt": "2022-09-06T03:44:33.191Z", "postId": "HzxJ8v2zv68WQHAeH", "htmlBody": "<p>In other domains, when we combine different metrics to yield one frankenstein metric, it is because these different metrics are all partial indicators of some underlying measure we cannot directly observe. The whole point of ethics is that we are trying to directly describe this underlying measure of \"good\", and thus it doesn't make sense to me to create some frankenstein view.&nbsp;</p><p>The only instance I would see this being ok is in the context of moral uncertainty, where we're saying \"I believe there is some underlying view but I don't know what it is, so I will give some weight to a bunch of these plausible theories\". Which maybe is what you're getting at? But in that case, I think it's necessary to believe that each of the views you are averaging over could be approximately true on its own, which IMO really isn't the case with a complicated utilitarianism formula, especially since we know there is no formula out there that will give us all we desire. Though this is another long philosophical rabbit hole, I'm sure.</p>", "parentCommentId": "TbuFNJc3xesurv5fH", "user": {"username": "RedStateBlueState"}}, {"_id": "t444htT64CmYKHJLr", "postedAt": "2023-08-01T11:16:42.558Z", "postId": "HzxJ8v2zv68WQHAeH", "htmlBody": "<p>Thanks for this. i just had a similar idea, and ofc I'm glad to see another EA had a similar insight before. I am no expert on the field, but I agree that this <em>\"atemporal avg utilitarianism\"</em> seems to be underrated; I wonder why.\nThe greatest problem I see with this view, at first, is that it makes the moral goodness of future actions depend on the population and the goodness of the past. I suspect this would also make it impossible (or intractable) to model goodness as a social welfare function. But then... if the moral POV is the \"POV of the universe\", or the POV of nowhere, or of the impartial observer... maybe that's justified?\nAnd it'd explain the Asymmetry and the use of thresholds for adding people.</p>\n<p>I suspect this view is immune to the repugnant conclusion / mere addition paradox. The most piercing objection from total view advocates against avg utilitarianism is that it implies a sadistic conclusion: adding a life worth living makes the world <em>worse</em> if this life is below the average utility; and adding a life with negative value is good if it is superior to the world average.\nBut if the overall avg utility is positive, or if you add a constraint forbidding adding negative lives... this makes it  less likely to find examples where this view implies a \"sadistic\" conclusion</p>\n", "parentCommentId": null, "user": {"username": "Ramiro"}}]