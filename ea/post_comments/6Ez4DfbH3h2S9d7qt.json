[{"_id": "kJAGMndL9HuFcG2BT", "postedAt": "2024-01-11T19:36:55.788Z", "postId": "6Ez4DfbH3h2S9d7qt", "htmlBody": "<blockquote><p>We can haggle about some of the details of Yudkowsky's pessimism here... but I'm sympathetic to the broad vibe: if roughly all the power is held by agents entirely indifferent to your welfare/preferences, it seems unsurprising if you end up getting treated poorly. Indeed, a lot of the alignment problem comes down to this.</p></blockquote><p>I agree with the weak claim that if literally every powerful entity in the world is entirely indifferent to my welfare, it is unsurprising if I am treated poorly. But I suspect there's a stronger claim underneath this thesis that seems more relevant to the debate, and also substantially false.</p><p>The stronger claim is: adding powerful entities to the world who don't share our values is selfishly bad, and the more of such entities we add to the world, the worse our situation becomes (according to our selfish values). We know this stronger claim is likely false because\u2014assuming we accept the deeper atheism claim that humans have non-overlapping utility functions\u2014the claim would imply that ordinary population growth is selfishly bad. Think about it: by permitting ordinary population growth, we are filling the universe with entities who don't share our values. Population growth, in other words, causes our relative power in the world to decline.</p><p>Yet, I think a sensible interpretation is that ordinary population growth is <i>not </i>bad on these grounds. I doubt it is better, selfishly, for the Earth to have 800 million people compared to 8 billion people, even though I would have greater relative power in the first world compared to the second. [ETA: see <a href=\"https://www.lesswrong.com/posts/cnv5g7jCWLw9LYKxa/an-even-deeper-atheism-3?commentId=p27TaAwTeESgp7RMt\">this comment</a> for why I think population growth seems selfishly good on current margins.]</p><p>Similarly, I doubt it is better, selfishly, for the Earth to have 8 billion humans compared to 80 billion human-level agents, 90% of which are AIs. Likewise, I'm skeptical that it is worse for my values if there are 8 billion slightly-smarter-than human AIs who are individually, on average, 9 times more powerful than humans, living alongside 8 billion humans.</p><p>(This is all with the caveat that the details here matter a lot. If, for example, these AIs have a strong propensity to be warlike, or aren't integrated into our culture, or otherwise form a natural coalition against humans, it could very well end poorly for me.)</p><p>If our argument for the inherent danger of AI applies equally to ordinary population growth, I think something has gone wrong in our argument, and we should probably reject it, or at least revise it.</p>", "parentCommentId": null, "user": {"username": "Matthew_Barnett"}}, {"_id": "7dMxgBBXpJfRxihKm", "postedAt": "2024-01-15T11:38:52.820Z", "postId": "6Ez4DfbH3h2S9d7qt", "htmlBody": "<p>Nice post, Joe!</p><blockquote><p>\"I reject Yudkowsky's story that some particular AI will foom and become dictator-of-the-future; rather, I think there will be a multi-polar <i>ecosystem</i> of different AIs with different values. Thus: problem solved?\" Well, hmm: what values in particular? Is it all still ultimately an office-supplies thing? If so, it depends how much you like a complex ecosystem of staple-maximizers, thumb-tack-maximizers, and so on \u2013 fighting, trading, etc. \"Better than a monoculture.\" Maybe, but how much?<a href=\"https://forum.effectivealtruism.org/s/Jhrsg27sxCxHh44f6/p/6Ez4DfbH3h2S9d7qt#fn-ZSHZdtdYagDgNY2Lm-9\"><sup>[9]</sup></a> Also, are all the humans still dead?</p></blockquote><p>In my mind, there is a sense in which this last question is analogous to <a href=\"https://en.wikipedia.org/wiki/Neanderthal\">Neanderthals</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefua7ib99g0ej\"><sup><a href=\"#fnua7ib99g0ej\">[1]</a></sup></span>&nbsp;asking a few hundreds of thousands of years ago whether they would still be around now. They are not, but is this any significant evidence that the world has gone through a much less valuable trajectory? I do not think so. What arguably matters is whether there are still beings around with the desire and ability to increase welfare. So I would instead ask, \"are all intelligent welfarists dead?\", where intelligent could be interpreted as sufficiently intelligent to eventually leverage (via successors or not) the <a href=\"https://forum.effectivealtruism.org/topics/universe-s-resources\">cosmic endowment</a> to increase welfare. My question is equivalent to yours nearterm, since humans are the only intelligent welfarists now, but the answers may come apart in the next few decades thanks to (even more) intelligent sentient AI. To the extent the answers to the 2 questions differ, it seems important to focus on the right one.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnua7ib99g0ej\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefua7ib99g0ej\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Or individuals of another species of the genus <a href=\"https://en.wikipedia.org/wiki/Homo\">Homo</a>. There are 12 besides Homo Sapiens!</p></div></li></ol>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}]