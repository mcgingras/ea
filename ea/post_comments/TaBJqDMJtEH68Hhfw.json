[{"_id": "uSnBegCi8EXNKcJRD", "postedAt": "2022-11-22T22:08:59.046Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>Context: This post popped into my head because I was having a conversation with Peter Hartree about whether a specific argument by Peter Thiel made sense. And I was claiming that at a glance, a specific Thiel-argument seemed locally invalid to me, in the sense of <a href=\"https://www.lesswrong.com/posts/WQFioaudEH8R7fyhm/local-validity-as-a-key-to-sanity-and-civilization\">Local Validity as a Key to Sanity and Civilization</a>.</p><p>And Peter's response was that Thiel has a good enough track record that we should be very reluctant to assume he's wrong about something like this, and should put in an effort to steel-man him and figure out what alternative, more-valid things he <i>could</i> have meant.</p><p>And I'm OK with trying that out as an intellectual exercise. (Though I've said before that <a href=\"https://www.lesswrong.com/posts/MdZyLnLHuaHrCskjy/itt-passing-and-civility-are-good-charity-is-bad\">steelmanning can encourage fuzzy thinking and misunderstandings</a>, and we should usually prioritize fleshmanning / passing people's Ideological Turing Test, rather than just trying to make up arguments that seem more plausible <i>ex nihilo</i>.)</p><p>But I felt an urge to say \"this EA thing where we steel-man and defer to impressive people we respect, rather than just blurting out when a thing doesn't make sense to us until we hear a specific counter-argument, is part of our Bigger Problem\". I think this problem keeps cropping up in EA across a bunch of domains \u2014 not just \"why didn't the EA Forum host a good early discussion of at least one SBF red or yellow flag?\", but \"why do EAs keep getting into deference cascades that lead them to double- and triple-count evidence for propositions?\", and \"why are EAs herding together and regurgitating others' claims on AI alignment topics rather than going off in dozens of strange directions to test various weird inside-view ideas and convictions and <a href=\"https://www.lesswrong.com/posts/PqMT9zGrNsGJNfiFR/alignment-research-field-guide\">build their own understanding?</a>\".</p><p>(Do people not realize that humanity doesn't know shit about alignment yet? I feel like people keep going into alignment research and being surprised by this fact.)</p><p>It all feels like one thing to me \u2014 this idea that it's fine for me to blurt out an objection to a local thing Thiel said, even though I respect him as a thinker and am a big fan of some of his <a href=\"https://www.lesswrong.com/posts/ReB7yoF22GuerNfhH/thiel-on-secrets-and-indefiniteness\">ideas</a>. Because blurting out objections is just the standard way for EAs to respond to anything that seems off to them.</p><p>I then want to be open that I may be <i>wrong</i> about Thiel on this specific point, and I want to listen to counter-arguments. But I don't think it would be good (for my epistemics or for the group's epistemics) to go through any special mental gymnastics or justification-ritual in order to blurt out my first-order objection in the first place.</p><p>I <i>wanted</i> to say all that in the Peter Thiel conversation. But then I worried that I wouldn't be able to communicate my point because people would think that I'm darkly hinting at Peter Thiel being a bad actor. (Because they're conflating the problem \"EAs aren't putting a high enough prior on people being bad actors\" with this other problem, and not realizing that organizing their mental universe around \"bad actors vs. good actors\" can make it <i>harder</i> to spot early signs of bad actors, and also harder to do a lot of other things EA ought to try to do.)</p><p>So I wrote this post. :)</p>", "parentCommentId": null, "user": {"username": "RobBensinger"}}, {"_id": "c4AFeBHYkMrHhtL2B", "postedAt": "2022-11-22T23:43:26.038Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>I think the issue you\u2019re addressing is a real and important one. However, I think current norms are a response to disadvantages of blurting, both on an individual and movement level. As you note, most people\u2019s naive divergent first impressions are wrong, and on issues most salient to the community, there\u2019s usually somebody else who\u2019s thought about it more. If we added lots more blurting, we\u2019d have an even greater problem with finding the signal in the noise. This adds substantial costs in terms of reader energy, and it also decreases the reward for sharing carefully vetted information because it gets crowded out by less considered blurting.</p>\n<p>Hence, the current equilibria, in which ill-considered blurting gets mildly socially punished by people with better-considered views frustrated by the blurter, leading to pre-emptive self-censorship and something of a runaway \u201cstay in your lane\u201d feedback loop that can result in \u201cemperor has no clothes\u201d problems like this one. Except it wasn\u2019t a child or \u201cblurter\u201d who exposed SBF - it was his lead competitor, one of the most expert people on the topic.</p>\n<p>I\u2019ve said it before and I\u2019ll say it again, EA\u2019s response to this fraud cannot be - not just shouldn\u2019t, but can\u2019t - to achieve some combination of oracular predictive ability and perfect social coordination for high-fidelity information transmission. It just ain\u2019t gonna happen. We should assume that we cannot predict the next scandal. Instead we should focus on finding general-purpose ways to mitigate or prevent scandal without having to know exactly how it will occur.</p>\n<p>This comes down to governance. It\u2019s things like good accounting, finding ways to better protect grantees in the case that their funder goes under, perhaps increased transparency of internal records of EA orgs, that sort of thing.</p>\n", "parentCommentId": null, "user": {"username": "AllAmericanBreakfast"}}, {"_id": "8RGjmdrofLLDo5oFu", "postedAt": "2022-11-23T02:22:14.851Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>Over a year ago, I thought it was completely inexplicable that SBF hired a Chief Regulatory Officer who was a lawyer known only for involvement in online fraud. There is no legitimate reason to hire such a person. And even apart from the fraud, his resume was not the typical profile of someone who a legitimate multi-billionaire would hire at a high level. An actual multi-billionaire could have poached away the general counsel from a Fortune 100 company. Why settle for someone like this? <a href=\"https://coingeek.com/tether-links-to-questionable-market-makers-yet-another-cause-for-concern/\">https://coingeek.com/tether-links-to-questionable-market-makers-yet-another-cause-for-concern/</a>&nbsp;</p><p>I finally worked up the courage to hint at this point 4 months ago ( <a href=\"https://forum.effectivealtruism.org/posts/KBw6wKDbvmqacbB5M/crypto-markets-ea-funding-and-optics?commentId=wcvYZtw7b4xvrdetL\">https://forum.effectivealtruism.org/posts/KBw6wKDbvmqacbB5M/crypto-markets-ea-funding-and-optics?commentId=wcvYZtw7b4xvrdetL</a> ), and then was a little more direct 2 months ago ( <a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest?commentId=odHG4hhM2FSGiXXFQ\">https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest?commentId=odHG4hhM2FSGiXXFQ</a>).&nbsp;</p><p>Lo and behold, &nbsp;that guy was probably in on the whole thing---he also served as general counsel to Alameda! <a href=\"https://www.nbcnews.com/news/ftxs-regulatory-chief-4-job-titles-2-years-was-really-rcna57965\">https://www.nbcnews.com/news/ftxs-regulatory-chief-4-job-titles-2-years-was-really-rcna57965</a>&nbsp;</p>", "parentCommentId": null, "user": {"username": "Stuart Buck"}}, {"_id": "HpkafLrnqWMDscn4H", "postedAt": "2022-11-23T02:31:15.148Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>I see no mention in either of your forum posts of the aforesaid lawyer?</p>", "parentCommentId": "8RGjmdrofLLDo5oFu", "user": {"username": "EliezerYudkowsky"}}, {"_id": "x85wgXrWykPond4ky", "postedAt": "2022-11-23T02:56:06.501Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>My post from 4 months ago linked to a story about the lawyer, which is why I said I merely <i>hinted </i>at this point. The post from 2 months ago didn't expressly mention it, but a followup post definitely did in detail (I deleted the post soon thereafter because I got a few downvotes and I got nervous that maybe it was over the line).&nbsp;</p>", "parentCommentId": "HpkafLrnqWMDscn4H", "user": {"username": "Stuart Buck"}}, {"_id": "JJckMXZy6spfEyMr2", "postedAt": "2022-11-23T03:05:50.096Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>I find Stuart Buck extremely impressive.</p><p>He has a lot of comments going against the grain at the time, and pointing out issues with SBF.</p><p>His contributions is focused and he's not salesy.</p><p>It\u2019s worth pausing and <a href=\"https://goodscienceproject.org/about/\">taking a look at his org</a>, which looks like one of the most impressively advised I've ever seen (and low overlap with EA figures, which is healthy).<br>&nbsp;</p>", "parentCommentId": "x85wgXrWykPond4ky", "user": {"username": "2150 Shattuck Ave"}}, {"_id": "cqY3oonmd2XJdqkFk", "postedAt": "2022-11-23T03:27:54.096Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>\ud83d\ude22</p>", "parentCommentId": "x85wgXrWykPond4ky", "user": {"username": "RobBensinger"}}, {"_id": "xbwWCes8DCi4scuqN", "postedAt": "2022-11-23T03:32:40.669Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>Thanks! There's a very natural reason for all of this: <a href=\"https://twitter.com/stuartbuck1/status/1595254445683654657\">https://twitter.com/stuartbuck1/status/1595254445683654657</a>&nbsp;</p>", "parentCommentId": "JJckMXZy6spfEyMr2", "user": {"username": "Stuart Buck"}}, {"_id": "NhwXixfa8qaybqDLM", "postedAt": "2022-11-23T11:05:18.905Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>Oh, this is excellent! I do a version of this, but I haven't paid enough attention to what I do to give it a name. \"Blurting\" is perfect.</p><p>I try to make sure to always notice my immediate reaction to something, so I can more reliably tell what my more sophisticated reasoning modules transforms that reaction into. Almost all the search-process imbalances (eg. filtered recollections, motivated stopping, etc.) come into play during the sophistication, so it's inherently risky. But refusing to reason past the blurt is equally inadvisable.</p><p>This is interesting from a predictive-processing perspective.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft66nf4yf2gb\"><sup><a href=\"#fnt66nf4yf2gb\">[1]</a></sup></span>&nbsp;The first thing I do when I hear someone I respect tell me their opinion, is to compare that statement to my prior mental model of the world. That's the fast check. If it conflicts, I aspire to mentally blurt out that reaction to myself.</p><p>It takes longer to generate an alternative mental model (ie. sophistication) that is able to predict the world described by the other person's statement, and there's a lot more room for bias to enter via the mental equivalent of <a href=\"https://en.wikipedia.org/wiki/Multiple_comparisons_problem\">multiple comparisons</a>. Thus, if I'm overly prone to conform, that bias will show itself <i>after</i> I've already blurted out \"huh!\" and made note of my prior. The blurt helps me avoid the failure mode of conforming and feeling like that's what I believed all along.</p><p>Blurting is a faster and more usefwl variation on <a href=\"https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence\">writing down your predictions in advance</a>.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt66nf4yf2gb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft66nf4yf2gb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Speculation. I'm not very familiar with predictive processing, but the claim seems plausible to me on alternative models as well.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Emrik"}}, {"_id": "K93kDmy3kmzWcsLfa", "postedAt": "2022-11-23T11:13:23.695Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>I posted for the ~first time in the EA forum after the SBF stuff, and was pretty disappointed by the voting patterns: almost all critical posts get highly upvoted (well, taking into account the selection effect where I wouldn't see negative-karma posts), seemingly regardless of how valid or truthseeking or actionable they are. And then the high-karma comments very often just consist of praise for writing up that criticism, or ones that take the criticism for granted and expand on it, while criticism of criticism gets few upvotes.</p><p>(Anyway, after observing the voting patterns <a href=\"https://forum.effectivealtruism.org/posts/sEpWkCvvJfoEbhnsd/the-ftx-crisis-highlights-a-deeper-cultural-problem-within?commentId=mjXF8XfzeWofpq6qn\">on this comment thread of mine</a>, I see little reason with engaging on this forum anymore. I find the voting patterns on LW healthier.)</p>", "parentCommentId": null, "user": {"username": "tobias-daenzer"}}, {"_id": "dh9umgC3fshM4qCpe", "postedAt": "2022-11-23T15:20:34.652Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>[Epistemic status: I'm writing this in the spirit of blurting things out. I think I'm pointing to something real, but I may be wrong about all the details.]</p><blockquote><ul><li>lack of social incentive to blurt things out when you're worried you might be wrong;</li><li>lack of social incentive to build up your own inside-view model (especially one that disagrees with all the popular views among elite EAs);</li></ul></blockquote><p>You are correct that there is an incentive problem here. But the problem is not just lack of incentive, but actual incentive to fall in line.&nbsp;</p><p>Because funding is very centralised in EA, there are strong incentives to agree with the people who control the money. The funders are obviouly smarter than just selecting only \"yes\"-sayers, but they are also humans with emotions and limited time. There are types of ideas, projects, criticism that don't appeal to them. This is not meant as criticism of individuals but as criticism of the structure. Because given the structure I don't see how thing s could be otherwise.</p><p>This shapes the community in two major ways.&nbsp;</p><ol><li>People who don't fit the mould of what the funders like, don't get funded.</li><li>People are self-censoring in order to fit what <i>they think</i> the mould is.<br>&nbsp;</li></ol><p>I think the only way out of this is to have less centralised funding. Some steps that may help:</p><ul><li>Close the EA Funds. Specifically, don't collect decentralised funding into centralised funds.&nbsp;</li><li>Encourage more people to earn-to-give and encourage all earning-to-givers to make their own funding decisions.&nbsp;</li><li>Maybe set up some infrastructure to help funders find projects? Maybe EA Funds could be replaced by some type of EA GoFundMe platform? I'm not sure what would be the best solution. But if I where to build something like this, I would start talking to earning-to-givers about what would appeal to them.</li></ul><p>&nbsp;</p><p>Ironically FTX fund actually got this right. Their re-grant program was explicitly designed to decentralise funding decisions.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Linda Linsefors"}}, {"_id": "vXihu3Hj5X9EFRwKx", "postedAt": "2022-11-23T18:50:11.695Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>I like the idea of less centralized funding, and I think giving more influence to a diverse set ofmid-size funders is a critical part of reducing the risk of undue influence bya few megadonors. But the implementation may be complicated.&nbsp;</p><p>I feel that most people who are not \"professional\" EAs (for lack of a better word, and definitely including myself) would be pretty bad at playing grantmaker without devoting quite a bit of time to it. And most people who have enough income to be systematically important funders are spending a lot of time/energy at their day jobs and are unlikely to have bandwidth to do more than -- at most -- &nbsp;evaluate others' evaluations of funding candidates.</p><p>I can think of two possible ways to get some of what you're looking for if the assumptions above are correct:</p><ul><li>It may be plausible to have a centralized fund designed to accept donations from small-to-midsize EtGers set up in a way that minimizes the influence of \"the people who control (most) of the money.\" In other words, the fund would be independent of CEA/EV, would not invite anyone who has Open Phil's ear to be a grantmaker, would not accept money from any potential megadonors, etc. That would at least create one \"independent\" funding stream.</li><li>It may be preferable to have an organization offering <i>several</i> funds in a specified area -- e.g., separate pools of money for EA infrastructure managed separately by Jim, Pam, and Dwight. Presumably, the fund managers could talk to and cooperate with each other, but there would likely be more independence and diversity of approach than under a committee system. (I think that is true even if a committee almost always approves the lead grantmaker's recommendation; the very existence of an approval requirement can significantly shape an individual's exercise of initial discretion.) Perhaps there could be a mechanism for Jim and Pam to go to an oversight board if they felt Dwight was planning to fund something harmful or unusually silly.</li></ul><p>Those ideas would have costs as well as benefits. In particular, I've always appreciated that the EA community tries to minimize the amount of time/resources organizations need to devote to fundraising as opposed to substantive work. I get the sense that top leadership in many \"mainstream\" charities spends a lot of its bandwidth on fundraising and donor management. Decentralization would require a step back from that. But I think either of the two ideas above might achieve some of your goals with significantly lower downsides than having (e.g.) several dozen different midsize EtGers evaluating grant proposals.&nbsp;</p>", "parentCommentId": "dh9umgC3fshM4qCpe", "user": {"username": "Jason"}}, {"_id": "gnybGuD8ytnubAmum", "postedAt": "2022-11-23T19:28:34.914Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>I think this is one of those places where a relative outsider may have a comparative advantage in some cases. Speaking for myself, I think I would be less likely to blurt if my employer had anything to do with EA, if I was hoping to get grants, if my sense of social identity was strongly tied to the EA community, etc. Of course, it means I know less and am more likely to say something stupid, too . . .</p><p>As a thought experiment, and recognizing the huge risk of hindsight bias, consider if a well-educated and well-informed outsider had been asked to come up a year ago with a list of events that would have been moderately serious to catastrophic for EA and estimated probabilities. I think there is a very high chance they would have come up with \"loss of a megadonor,\" and a high chance they would have come up with \"and I note that SBF's assets are in a high volatility field with frequent bankruptcies, in a very young company.\"I think there is a very high chance that they would have come up with \"major public scandal involving a key figure,\" and a high chance they would have come up with a list of key figures that included SBF. I suspect that most outsiders would have assigned a higher probability to these things than most insiders, although this is impossible to test at this point.</p><p>I don't think it likely \"SBF is discovered to be running a massive fraud\" would explicitly be on most hypothetical outsiders' list, but I think it is more likely to have appeared than if an insider had prepared the list. That is due to (1) self-censoring, and (2) less deference to / weighing of what key figures in the community thought of SBF. At least when you're talking about assessing serious to catastrophic risks, I think you want to get <i>both </i>the \"insider\" and \"outsider\" views.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Jason"}}, {"_id": "SxNrr9Z23atqNgvRf", "postedAt": "2022-11-23T19:43:44.759Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>Linda - interesting ideas.&nbsp;</p><p>As you note, centralized funding in EA, and fear of offending the funding decision-makers, can create incentives to pre-emptively self-censor, and can reduce free-spirited 'blurting'.</p><p>Additional suggestion: One way to help reduce this effect would be for EA funding decision-makers to give a little more feedback about why grant proposals get turned down. Several programs at the moment take the view that it's reasonable to say 'We just don't have enough time or staff to give any individualized feedback about grant proposals that we don't fund; sorry/not sorry'. &nbsp;</p><p>Someone may have spent several hours (or days) writing a grant proposal, and the proposal judges/funders may have spent a couple of hours reading it, but they can't spend five minutes writing an explanation of why it's turned down?</p><p>This lack of feedback can lead grant applicants to assume that there may have been personal, reputational, or ideological bias in the decision-making process. This might lead them to be extra-cautious in what they say in future on EA Forum, or in other contexts, for fear of offending the funding decision-makers.</p><p>tldr: if EA funders have time to read grant proposals and to take them seriously, then they have time to give honest, candid, constructive feedback on those proposals; this would help reduce fear and anxiety around the funding process, and would probably reduce self-censorship, and would promote a more resilient culture of blurting.</p>", "parentCommentId": "dh9umgC3fshM4qCpe", "user": {"username": "geoffreymiller"}}, {"_id": "8rWiHcTfBinGtWguB", "postedAt": "2022-11-23T20:45:10.122Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<blockquote><p>I feel that most people who are not \"professional\" EAs (for lack of a better word, and definitely including myself) would be pretty bad at playing grantmaker without devoting quite a bit of time to it.</p></blockquote><p>I think you overestimate the difference between you and \"professional\" EAs. Good grant making both hard and time consuming for <i>everyone</i>.&nbsp;<br><br>If someone is doing grant evaluation as their full-time job, then they are probably better at it than you, because they can spend more time on it. But as far as I know, most EA grants are evaluated by people doing this as some side volunteering. They usually have an EA job, but that job is often something different than grant making.&nbsp;<br><br>I think OpenPhil is the only org that employ full time grant makers? But you can't even apply to OpenPhil unless you either fitted into any of their pre-defined programs or if know the right people. The only time I asked OpenPhil for money (I went to their office hour at EA Global) they politely told me that they would not even evaluate my project, because it was too small to be worth their time. To be clear, I'm not writing this to complain. I'm not saying that they did the wrong judgment. Having paid professional evaluators looking at every small project is expensive.&nbsp;<br><br>I just hate that people like yourself think that there are some grant experts out there, looking at all the grant, making much better evaluations than you could have done. Because there isn't. That's not how things are currently run.</p><blockquote><p>In particular, I've always appreciated that the EA community tries to minimize the amount of time/resources organizations need to devote to fundraising as opposed to substantive work. I get the sense that top leadership in many \"mainstream\" charities spends a lot of its bandwidth on fundraising and donor management.</p></blockquote><p>I agree. I know some academics and have an idea of how much time and effort they spend on grant making. We don't want to end up in that situation.<br><br>I think this can be solved by having an EA wide standard for grant applications. If I were in charge, it would be a google doc template. If I want funding, I can fill it in with my project, and then send it to all the relevant mid-sized funders.&nbsp;</p>", "parentCommentId": "vXihu3Hj5X9EFRwKx", "user": {"username": "Linda Linsefors"}}, {"_id": "LTCBfjvjZpYaC5BD7", "postedAt": "2022-11-23T21:28:17.120Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>I would prefer more people give through donor lotteries rather than deferring to EA funds or some vague EA vibe. Right now <a href=\"https://funds.effectivealtruism.org/stats/overview\">I think EA funds do like $10M / year</a> vs <a href=\"https://www.givingwhatwecan.org/donor-lottery/161434944147562\">$1M / year through lotteries</a>, and probably in my ideal world the lottery number would be at least 10x higher.</p><p>I think that EAs are consistently underrating the value of this kind of decentralization. With all due respect to EA funds I don't think it's reasonable to say \"thinking more about how to donate wouldn't help because obviously I should just donate &nbsp;100% to EA funds.\" (That said, I don't have a take about whether EA funds should shut down. I would have guessed not.)</p><p>I think that's probably the lowest hanging fruit, though it might only help modestly. The effect size depends on how much the problem is lacking diverse sources of money vs lacking diverse sources of attention.</p>", "parentCommentId": "dh9umgC3fshM4qCpe", "user": {"username": "Paul_Christiano"}}, {"_id": "CmMyKCvoyLBhb2sGk", "postedAt": "2022-11-23T21:30:15.342Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>I totally agree with you regarding the value of feedback.</p><blockquote><p>Someone may have spent several hours (or days) writing a grant proposal, and the proposal judges/funders may have spent a couple of hours reading it, but they can't spend five minutes writing an explanation of why it's turned down?</p></blockquote><p>I'm also confused by this. I'm guessing it's more about the discomfort around giving negative feedback, than it is about time?&nbsp;<br><br>I'm verry much in favour of acknowledging the cost associated with the energy drain of dealing with negative emotions. There are lots of things around the emotional cost of applications that could be improved, if we agreed that this is worth caring about.<br><br><strong>Clarification (because based on past experience this seems to be necessary):</strong> I don't think the feelings of fellow EA is the only thing that matters, or even the top priority or anything like that. What I do think is that we are losing both valuable people and productivity (who could have contributed to the mission) because we ignore that personal emotions is a thing that exists.</p>", "parentCommentId": "SxNrr9Z23atqNgvRf", "user": {"username": "Linda Linsefors"}}, {"_id": "XfL2ksmAjbsTLCoxN", "postedAt": "2022-11-23T22:27:42.133Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>Is there somewhere we can see how the winners of donor lotteries have been donating their winnings?</p>\n", "parentCommentId": "LTCBfjvjZpYaC5BD7", "user": {"username": "John_Maxwell_IV"}}, {"_id": "8z3tbdQi65PKcpByN", "postedAt": "2022-11-23T23:32:40.806Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>Strong agree, and note that it's not obvious after browsing both GWWC and EAF how to donate to lotteries currently, or when they'll be running next. I'd like to see them run more regularly and placed prominently on the websites.</p>", "parentCommentId": "LTCBfjvjZpYaC5BD7", "user": {"username": "plex"}}, {"_id": "xRdL7LsCmbJHisjaL", "postedAt": "2022-11-23T23:50:48.684Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>Personally the FTX regrantor system felt like a nice middle ground between EA Funds and donor lotteries in terms of (de)centralization. I'd be excited to donate to something less centralized than EA Funds but more centralized than a donor lottery.</p>", "parentCommentId": "LTCBfjvjZpYaC5BD7", "user": {"username": "elifland"}}, {"_id": "wy2iygRanKqt3iQEg", "postedAt": "2022-11-24T00:13:57.440Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>By \"professional EA,\" I meant that -- at least by and large -- the fund managers have relevant professional expertise in the subject area of the fund. An investment banker, law firm partner, neurosurgeon, or corporate CEO is very unlikely to have that kind of experience. My assumption is that those folks will take significantly longer to adequately evaluate a grant proposal than someone with helpful background knowledge from professional experience. And given the requirements of most jobs that pay enough to make independent grantmaking viable, I don't think most people would have enough time to devote to adequately evaluating grants without strong subject-matter background. In contrast, I imagine that I would do a better job evaluating grant proposals in my field of expertise (law) than the bulk of the EA Funds managers, even if the specific subject matter of the grant was a branch of law I hadn't touched since law school.</p><p>I'm a <i>public-sector</i> lawyer, so no one has dissauded me from independent grantmaking. I don't have nearly the money to have ever thought about it!</p><p>In case someone is interested in the idea of a \"Donors Choose\" type system: unless the proposed grantees had their own 501(c)(3)s, what you're describing would need some degree of 501(c)(3) organizational oversight/control/overhead to keep the tax deduction in the US (which higher-income individuals definitely care about). A straight-up \"EA GoFundMe\" wouldn't be any more tax-deductible than vanilla GoFundMe is. Certain types of grants -- those that could be seen as closer to gifts to individuals rather than compensation for work to be performed -- might need heightened scrutiny to avoid &nbsp;problems with private inurement (benefit).</p>", "parentCommentId": "8rWiHcTfBinGtWguB", "user": {"username": "Jason"}}, {"_id": "ZnX4F6LhBgrkxZHhu", "postedAt": "2022-11-24T00:49:42.881Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>Maybe something like the S-process used by SFF?<br><a href=\"https://survivalandflourishing.fund/s-process\">https://survivalandflourishing.fund/s-process</a><br><br>It would be cool to have a grant system where anyone can list them selves as fund manager, and donors can pick which fund managers decisions they want to back with their donations. &nbsp;If I remember correctly, the s-process could facilitate something like that.</p>", "parentCommentId": "xRdL7LsCmbJHisjaL", "user": {"username": "Linda Linsefors"}}, {"_id": "wmzbTZd6YyoSqLA9K", "postedAt": "2022-11-24T01:08:28.297Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>501(c)(3)s can be acceded via fiscal sponsorship. There is already a network of agreement between EA orgs to re-grant to each other for tax reasons, mostly thanks to Rethink<br>https://rethink.charity/donate&nbsp;<br><br>In order to tap into this, an individual needs to be paid by though an org that is tapped into this network. &nbsp;For AI Safety projects I think AI Safety Support would be willing to provide this service (I know they already done this for two projects and one person). I don't know what the options are for other cause areas, but if this becomes a major bottleneck, then it seems like a good plan would be to set up orgs to financially host various projects.</p>", "parentCommentId": "wy2iygRanKqt3iQEg", "user": {"username": "Linda Linsefors"}}, {"_id": "BpLWBQghmE8kieToj", "postedAt": "2022-11-24T01:48:06.655Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>At present, the Funds evaluate hundreds of grant applications a year, generally seeking high four to low six figures in funding (median seems to be mid-five). In a world where the Funds were largely replaced by lotteries, where would those applicants go? Do we predict that the lottery winners would largely take over the funding niche currently filled by the Funds? If so, how would this change affect the likelihood of funding for smaller grant applicants?</p>", "parentCommentId": "LTCBfjvjZpYaC5BD7", "user": {"username": "Jason"}}, {"_id": "7raBKLqaweGT9dLSw", "postedAt": "2022-11-24T11:52:41.516Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<p>Ideally, EigenTrust or something similar should be able to help with regranting once it takes off, no? : )</p>", "parentCommentId": "8z3tbdQi65PKcpByN", "user": {"username": "Emrik"}}, {"_id": "BwK6SJG5ccqxHphdA", "postedAt": "2022-11-29T00:07:29.711Z", "postId": "TaBJqDMJtEH68Hhfw", "htmlBody": "<blockquote><p>If we added lots more blurting, we\u2019d have an even greater problem with finding the signal in the noise.</p></blockquote><p>The EA Forum is a hub for a wide variety of approaches and associated perspectives: global development randomista, anti-factory-farming activist, pandemic preparedness lobbyist, AI alignment researcher, Tomasik-style \"what if electrons are conscious?\" theorist, etc. On top of that, it has a karma system, post curation, and many options for filtering/tagging and subscribing to particular kinds of content.</p><p>So both in terms of the forum's infrastructure and in terms of its content and audience, I have a hard time imagining a more ideal venue for high-quality 101-level questions and conversations. What specific signal are people worried about losing? Is there any way to &nbsp;(e.g., with tags or mod curation or new features) to encourage freer discussion on this forum <i>and</i> preserve that signal?</p><p>(One bit of social tech that might help here is just to flag at the top of your comment what the epistemic status of your statements is. That plus a karma system addresses most of the \"wasting others' time\" problem, IMO.)</p><blockquote><p>Except it wasn\u2019t a child or \u201cblurter\u201d who exposed SBF - it was his lead competitor, one of the most expert people on the topic.</p></blockquote><p>Sure. SBF isn't my real crux for thinking EA is largely bottlenecked on blurting. It's an illustrative example of <i>one </i>reason we're likely to benefit from more blurting.</p><p>Discovering the fraud sooner probably wouldn't have been trivial, especially if the fraud started pretty recently; but &nbsp;there are many outcomes that fall short of full discovery and yet are a fair bit better than the status quo. (As well as many other dimensions on which I expect blurting to improve EA's ability to help the world.)</p><blockquote><p>I\u2019ve said it before and I\u2019ll say it again, EA\u2019s response to this fraud cannot be - not just shouldn\u2019t, but can\u2019t - to achieve some combination of oracular predictive ability and perfect social coordination for high-fidelity information transmission. It just ain\u2019t gonna happen. We should assume that we cannot predict the next scandal. Instead we should focus on finding general-purpose ways to mitigate or prevent scandal without having to know exactly how it will occur.</p></blockquote><p>I fully agree! But it sounds like SBF already should have been a fairly scandalous name throughout EA, based on reports about the early history of Alameda. Never mind whether we could have predicted the exact specifics of what happened at FTX; why did the Alameda info stay bottled up for so many years, such that I and others are only hearing about it now? This seems like a misstep regardless of how it would have changed our relationship to SBF.</p>", "parentCommentId": "c4AFeBHYkMrHhtL2B", "user": {"username": "RobBensinger"}}]