[{"_id": "ExxvJ38phZSrqwZdB", "postedAt": "2023-08-27T05:24:04.350Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I fully agree with the title of this post, although I do think Yudkowsky can be valuable if you treat him as an \"interesting idea generator\", as long as you treat said ideas with a very skeptical eye.&nbsp;</p><p>I've only had time to comprehensively debunk <a href=\"https://titotal.substack.com/p/could-a-superintelligence-deduce\">one</a> of his overconfident mistakes, but there are a more mistakes or flaws I've noticed but haven't gotten around to fleshing out in depth, which I'll just list here:</p><p>Yudkowsky treats his case for the \u201cmany worlds hypothesis\u201d as a slam-dunk that proves the triumph of Bayes, but in fact it is only half-done. He presents good arguments against \u201ccollapse is real\u201d, but fails to argue that this means many worlds is the truth, rather than one of the other many interpretations which do not involve a real collapse. stating that he's solved the problem is flatly ridiculous.&nbsp;</p><p>The description of Aumanns agreement theorem in \u201c<a href=\"https://www.lesswrong.com/posts/vrHRcEDMjZcx5Yfru/i-defy-the-data\">defy the data</a>\u201d is false, leaving behind important caveats that render his use of it incorrect.&nbsp;</p><p>In general, Yudkowsky talks about bayes theorem a lot, but his descriptions of practical bayesianism are firmly stuck in the 101 level, lacking, for example, any discussion on how to deal with uncertain priors or uncertain likelihood ratios. I don't know if he is unaware of how bayesian statistics are actually used or if he just thinks it was too complicated to explain, but it has lead to a lot of rationalists adopting a form of \"pseudo-bayesianism\" that bears little resemblance to how it is used in science.&nbsp;</p><p>Yud talks a lot about \u201c<a href=\"https://www.lesswrong.com/posts/MwQRucYo6BZZwjKE7/einstein-s-arrogance\">Einsteins arrogance</a>\u201d, in a way that obsfucates the actual evidence of Einsteins belief, and if i recall he has implied that using bayes theorem can justifiably get you to the same level of arrogance. In fact, general relativity was a natural extension of special relativity (which had a ton of empirical evidence in it's favour). Einsteins arrogance was justified by the nature of laws of physics and is in no way comparable to the type of speculative forecasts used by Yud and company.&nbsp;</p><p>The implications of the \u201cAI box experiment\u201d have been severely overstated. It does not at all prove that an AGI cannot be boxed, only that a subset of people are highly persuadable. \u201crationalists are gullible\u201d fits the evidence provided just as well. &nbsp;</p><p>I haven't even touched his twitter account, which I feel is just low-hanging fruit.&nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "KRKf3w6ve54i5Ptc2", "postedAt": "2023-08-27T06:06:24.013Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Eliezer's perspective on animal consciousness is especially frustrating because of the real harm it's caused to rationalists' openness to caring about animal welfare.</p><p>Rationalists are much more likely than highly engaged EAs to either dismiss animal welfare outright, or just not think about it since AI x-risk is \"obviously\" more important. (For a case study, just look at how this author's post on fish farming was received between the <a href=\"https://forum.effectivealtruism.org/posts/kwxE7HYjRpYwSEiKb/underwater-torture-chambers-the-horror-of-fish-farming\">EA Forum</a> and <a href=\"https://www.lesswrong.com/posts/Nnis3dFYZRRAzWuEA/underwater-torture-chambers-the-horror-of-fish-farming\">LessWrong</a>.) Eliezer-style arguments about the \"implausibility\" of animal suffering abound. Discussions of the implications of AI outcomes on farmed or wild animals (i.e. almost all currently existing sentient beings) are few and far between.</p><p>Unlike Eliezer's overconfidence in physicalism and FDT, Eliezer's overconfidence in animals not mattering has serious real-world effects. Eliezer's views have huge influence on rationalist culture, which has significant influence on those who could steer future TAI. If the alignment problem will be solved, <strong>it'll be really important for those who steer future TAI to care about animals, and be motivated to use TAI to improve animal welfare</strong>.</p>", "parentCommentId": null, "user": {"username": "Ariel Simnegar"}}, {"_id": "hx5LkfNPcA9DiYzTk", "postedAt": "2023-08-27T07:12:12.946Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks a lot. This was a very convincing and valuable take-down of Eliezer. I tend to think, like you, that Eliezer's way of reasoning from first principles has done real damage to epistemic practices in EA circles. Just try to follow the actual evidence, for rationality's sake. It isn't more complicated than that.</p>", "parentCommentId": null, "user": {"username": "oivavoi"}}, {"_id": "QhfjnCvwQypsJNhbP", "postedAt": "2023-08-27T07:20:20.118Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Generally, some good points across the board that I agree with. Talking with some physicist friends helped me debunk the many worlds thing Yud has going. Similarly his animal consciousness stuff seems a bit crazy as well. I will also say that I feel that you're coming off way to confident and inflammatory when it comes to the general tone. The AI Safety argument you provided was just dismissal without much explanation. Also, when it comes to the consciousness stuff I honestly just get kind of pissed reading it as I feel you're to some extent hard pandering to dualism.</p>\n<p>I totally agree with you that Yudkowsky is way overconfident in the claims that he makes. Ironically enough it also seems that you to some extent are as well in this post since you're overgeneralizing from insufficient data. As a fellow young person, I recommend some more caution when it comes to solid claims about stuff where you have little knowledge (you cherry-picked data on multiple occasions in this post).</p>\n<p>Overall you made some good points though, so still a thought-provoking read.</p>\n", "parentCommentId": null, "user": {"username": "Jonas Hallgren"}}, {"_id": "ZS2HNhif7Mr8PtuuT", "postedAt": "2023-08-27T07:27:55.897Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I will say that I thought the consciousness p zombie distinction was very interesting and a good example of overconfidence as this didn't come across in my previous comment.</p>\n", "parentCommentId": "QhfjnCvwQypsJNhbP", "user": {"username": "Jonas Hallgren"}}, {"_id": "SzbwRSsE3BfnAyyjf", "postedAt": "2023-08-27T08:11:46.803Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>Talking with some physicist friends helped me debunk the many worlds thing Yud has going.</p></blockquote><p>Yudkowsky may be criticized for being overconfident in the many-worlds interpretation, but to feel that you have \u201cdebunked\u201d it after talking to some physicist friends shows excessive confidence in the opposite direction. Have you considered how your views about this question would have changed if e.g. David Wallace had been among the physicists you talked to?</p><p><s>Also, my sense is that \u201cYud\u201d was a nickname popularized by members of the SneerClub subreddit (one of the most intellectually dishonest communities I have ever encountered). Given its origin, using that nickname seems disrespectful toward Yudkowsky.</s></p>", "parentCommentId": "QhfjnCvwQypsJNhbP", "user": {"username": "Pablo_Stafforini"}}, {"_id": "QRpvyuK83xTgEbiCe", "postedAt": "2023-08-27T09:19:26.065Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I don\u2019t have a link because Twitter is very difficult to search now if you don\u2019t have an account (if someone wants to provide one be my guest, there\u2019s one discussion thread involving Zach Weinersmith that says so for instance), but Yudkowsky currently uses and seems to like the nickname at this point.</p>\n", "parentCommentId": "SzbwRSsE3BfnAyyjf", "user": {"username": "Devin Kalish"}}, {"_id": "onuedPrQJHHGsFvh3", "postedAt": "2023-08-27T09:36:31.819Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks for the update: I have retracted the relevant part of my previous comment.</p>", "parentCommentId": "QRpvyuK83xTgEbiCe", "user": {"username": "Pablo_Stafforini"}}, {"_id": "KeWjLWpTeziiomhHz", "postedAt": "2023-08-27T09:53:22.137Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>I fully agree with the title of this post, although I do think Yudkowsky can be valuable if you treat him as an \"interesting idea generator\", as long as you treat said ideas with a very skeptical eye.&nbsp;</p></blockquote><p>Fwiw I think the <a href=\"https://slatestarcodex.com/2019/02/26/rule-genius-in-not-out/\">rule thinkers in</a> philosophy popular in EA and rat circles has itself been quite harmful. Yeah, there's some variance in how good extremely smart people are at coming up with original takes, but for the demonstrably smart people I think 'interesting idea generation' is more of a case of 'we can see them reasoning hypercarefully and scrupulously about their area of competence almost all the time they speak on it, sometimes they also come up with genuinely novel ideas, and when those ideas are outside their realm of expertise maybe they slightly underresearch and overindex on them'. I'm thinking of uncontroversially great thinkers like Feynman, Einstein, Newton, as well as more controversially great thinkers like Bryan Caplan, Elon Musk, here.</p><p>There is an opportunity cost to noise, and that cost is higher to a community the louder and more prominently it's broadcast within that community. You, the OP and many others have gone to substantial lengths to debunk almost casually thrown out views by EY that, as others have said, have made their way into rat circles almost unquestioned. Yet the cycle keeps repeating because 'interesting idea generation' gets so much shrift.</p><p>Meanwhile, there are many more good ideas than there is bandwidth to look into them. In practice, this means for every bad idea a Yudkowsky or Hanson overconfidently throws out, some reasonable idea generated by someone more scrupulous but less good at self-marketing gets lost.</p>", "parentCommentId": "ExxvJ38phZSrqwZdB", "user": {"username": "Arepo"}}, {"_id": "rrneZmaasggzFHxyG", "postedAt": "2023-08-27T10:11:08.154Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I like the general point about recognizing Eliezer\u2019s flaws and breaking through lazy dogmas that have been allowed to take hold just because he said them. I think it\u2019s important for readers to know that Eliezer is arrogant, in case that doesn\u2019t come across in his writing, but I don\u2019t think these examples make the case that he\u2019s frequently or egregiously wrong. Just sometimes wrong.</p>\n<p>I am annoyed by the effect of that one Facebook post on the entire rationalist community\u2019s opinions of animals, but I can\u2019t put all the blame on Eliezer for that. He wrote one comment on the Forum that he shared to facebook, and in it he admitted that eating animals is a sin his society lets him get away with and that he wouldn\u2019t eat animals if he felt he could get adequate nutrition otherwise. He\u2019s not making strong claims about animal consciousness\u2014 just giving his take. I think he\u2019s rationalizing in places, and I think a lot of people were grateful for the excuse not to give matter any more thought, but I don\u2019t think it\u2019s fair to act like he goes around parading this view when he doesn\u2019t. The only text we have is that decade-old comment.</p>\n<p>Seems like a low blow to say his strength isn\u2019t in forming true beliefs, the thing he wrote the Sequences about, when most of your complaints are about him being arrogant or not respecting expertise, not being probably wrong especially often.</p>\n", "parentCommentId": null, "user": {"username": "Holly_Elmore"}}, {"_id": "tRJtKoPv2d7yu9cDk", "postedAt": "2023-08-27T10:17:51.405Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>Wolfgang Schwartz</p></blockquote><p>It's <i>Schwarz</i>.</p><blockquote><p>Evidential decision theory would say that you shouldn\u2019t smoke because smoking gives you evidence that you\u2019ll have a shorter life.</p></blockquote><p>Not so important, but I feel obliged to mention that this has been forcefully argued against by e.g. <a href=\"https://www.cambridge.org/core/books/rational-decision-and-causality/A031649124A6F84581818025E60BA024\">Eells (1982)</a> and <a href=\"https://www.cambridge.org/core/books/evidence-decision-and-causality/7077949D2CD42E99C08D4FBFE5321148\">Ahmed (2014)</a>. In short, smoking will plausibly be preceded by a desire to smoke, and at the point of observing your own desire to do so, smoking or not does not provide additional evidence of cancer.</p>", "parentCommentId": null, "user": {"username": "Sylvester Kollin"}}, {"_id": "s3FxhsszKjTrPHQWQ", "postedAt": "2023-08-27T10:20:49.808Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I have a distinct memory, albeit one which could plausibly be false, of Eliezer once stating that he was '100% sure that nonhuman animals aren't conscious' because of his model of consciousness. If he said it, it's now been taken down from whichever site it appeared on. I'm now genuinely curious whether anyone else remembers this (or some actual exchange on which my psyche might have based it)</p>", "parentCommentId": "rrneZmaasggzFHxyG", "user": {"username": "Arepo"}}, {"_id": "ztXxEYdtAur2sgBat", "postedAt": "2023-08-27T10:25:09.256Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Actually, I think a comparison to Musk is pretty apt here. I frequently see Musk saying very incorrect things, and I don't think his object level knowledge of engineering is very good. But he is a good at selling ideas and building hype, which has translated into funding for actual engineers to build rockets and electric cars in a way that probably wouldn't have happened without his hype skills.&nbsp;</p><p>In the same way, Yud's skills at persuasive writing have accelerated both AI research and AI safety research (see Altman credited him for boosting openAI). The problem is that he is not actually very good at AI safety research himself (or any sub-set of the problems), and his beliefs and ideas on the subject are generally flawed. It would be like if you hired elon musk directly to build a car in your garage.&nbsp;</p><p>At this point, I think the field of AI safety is big enough that you should stick to spokespeople who are actual experts in AI, and don't make grand incorrect statements on an almost weekly basis.&nbsp;</p>", "parentCommentId": "KeWjLWpTeziiomhHz", "user": {"username": "titotal"}}, {"_id": "CCgyJ9kLAxLS8Ea4Q", "postedAt": "2023-08-27T12:06:49.470Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>A couple of other examples, both of which have been discussed on LessWrong before:</p><ul><li>In Eliezer's book <a href=\"https://equilibriabook.com/\"><i>Inadequate Equilibria</i></a>, he gives a central anecdote that by reading econ bloggers he confidently realized the Bank of Japan was making mistakes worth trillions of dollars. He further claimed that a change in leadership meant that the Bank of Japan soon after pursued his favored policies, immediately leading to \"real GDP growth of 2.3%, where the previous trend was for falling RGDP\" and validating his analysis.&nbsp;<ul><li>If true, this is really remarkable. Let me reiterate: He says that by reading econ blogs, he was able to casually identify an economic policy of such profound importance that the country of Japan was able to reverse declining GDP immediately.&nbsp;</li><li>In fact, one of his central points in the book is not just that he was able to identify this opportunity, but that he could be <i>justifiably confident</i> in his knowledge despite not having any expertise in economic policy. His intention with the book is to explain how and why he can be correct about things like this.&nbsp;</li><li>The problem? His anecdote falls apart at the <a href=\"https://www.lesswrong.com/posts/woCPxs8GxE7H35zzK/noting-an-error-in-inadequate-equilibria\">slightest fact check</a>.&nbsp;<ul><li>Japan's GDP was not falling when he says it was.</li><li><strong>There was no discernible change in GDP growth after the change in leadership and enactment of his preferred policies, while he claimed a huge jump.</strong></li></ul></li><li>This was pointed out in a widely upvoted <a href=\"https://www.lesswrong.com/posts/woCPxs8GxE7H35zzK/noting-an-error-in-inadequate-equilibria\">LessWrong post</a> (300+ karma) early this year.&nbsp;</li><li>Eliezer has yet to respond or correct his book.&nbsp;</li></ul></li><li><a href=\"https://www.lesswrong.com/posts/yCuzmCsE86BTu9PfA/there-are-no-coherence-theorems\">He misunderstands what expected utility theorems in economics say</a>. He has written very confidently across many years that they provide proofs about dominated strategies and money pumping (see appendix of link), which they do not. &nbsp;&nbsp;<ul><li>He did at least reply to this post, but his replies further demonstrate his confusions about expected utility theorems. For example, in his <a href=\"https://www.lesswrong.com/posts/yCuzmCsE86BTu9PfA/there-are-no-coherence-theorems?commentId=4HbK8ZPqDSaun9EXh\">first attempted refutation of the post</a> he <a href=\"https://www.lesswrong.com/posts/yCuzmCsE86BTu9PfA/there-are-no-coherence-theorems?commentId=7bEtCKnubf9mBWLyA\">confused one property for another</a>.&nbsp;</li><li>This contributes to his and other's beliefs about AI doom by default.&nbsp;<ul><li>Other people in the Rationality and AI Safety communities have absorbed his misunderstanding and his confidence level in it.&nbsp;</li></ul></li></ul></li></ul>", "parentCommentId": null, "user": null}, {"_id": "mscCTLnBoTNjRwpKM", "postedAt": "2023-08-27T12:11:11.921Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I appreciate the spirit of this post as I am not a Yudkowsky fan, think he is crazy overconfident about AI, am not very keen on rationalism in general, and think the EA community sometimes gets overconfident in the views of its \"star\" members. But some of the philosophy stuff here seems not quite right to me, though none of its egregiously wrong, and on each topic I agree that Yudkowsky is way, way overconfident. (Many professional philosophers are way overconfident too!)</p>\n<p>As a philosophy of consciousness PhD: the view that animals lack consciousness is definitely an extreme minority view in the field, but it it's not a view that no serious experts hold. Daniel Dennett has denied animal consciousness for roughly Yudkowsky like reasons <em>I think</em>.  (EDIT: Actually maybe not: see my discussion with Michael St. Jules below. Dennett is hard to interpret on this, and also seems to have changed his mind to fairly definitively accept animal consciousness more recently. But his earlier stuff on this at the very least opposed to confident assertions that we just <em>know</em> animals are conscious, and any theory that says otherwise is crazy.) And more definitely Peter Carruthers (<a href=\"https://scholar.google.com/citations?user=2JF8VWYAAAAJ&amp;hl=en&amp;oi=ao\">https://scholar.google.com/citations?user=2JF8VWYAAAAJ&amp;hl=en&amp;oi=ao</a>) used to defend the view that animals lack consciousness because they lack a capacity for higher-order thought. (He changed his mind in the last few years, but I personally didn't find his explanation as to why made much sense.) Likewise, it's far from obvious  that higher-order thought views imply any animals other than humans are conscious. And still less obvious that they imply all mammals are conscious.* Indeed a standard objection to HOT views, mentioned in the Stanford Encyclopedia of Philosophy page on them last time I checked, is that they are incompatible with animal consciousness.  Though that does of course illustrate that you are right that most experts take it as obvious that mammals are conscious.</p>\n<p>As for the zombies stuff: you are right that Yudkowsky is mistaken and mistaken for the reasons you give, but it's not a \"no undergraduate would make this\" error. Trust me. I have marked undergrads a little, though I've never been a Prof. Far worse confusion is common. It's not even \"if an undergrad made this error in 2nd year I'd assume they didn't have what it takes to become a prof\". Philosophy is really hard and the error is quite subtle, plus many philosophers of mind do think you can get from the possibility of zombies to epiphenomenalism given plausible further assumptions, so when Yudkowsky read into the topic he probably encountered lots of people assuming accepting the possibility of zombies commits you to epiphenomenalism. But yes, the general lesson of  \"Dave Chalmers, not an idiot\" is obviously correct.</p>\n<p>As for functional decision theory. I read Wolfgang Schwarz's critique when it came out, and for me the major news in it was that a philosopher as qualified as Wolfgang thought it was potentially publishable given revisions. It is <em>incredibly</em> hard to publish in good philosophy journals, at the very top end they have rejection rates of &gt;95%.  I have literally never heard of a non-academic doing do without even an academic coauthor. I'd classify it as a genuinely exceptional achievement to write something Wolfgang gave a revise and resubmit verdict to with no formal training in philosophy. I say this not because I think it means anyone should defer to Yudkowsky and Soares-again, I think their confidence on AI doom is genuinely crazy, but just because it feels a bit unfair to me to see what was actually an impressive achievement denigrated.</p>\n<p>*My own view is that IF animals  are not capable of higher-order thought there isn't even a fact of the matter about whether they are conscious, but that only justifies downweighting their interests to a less than overwhelming degree, and so doesn't really damage arguments for veganism. Though it would affect how much you should prioritise animals v. humans.</p>\n", "parentCommentId": null, "user": {"username": "Dr. David Mathers"}}, {"_id": "eM4FFnHL5W4Z7Fjdh", "postedAt": "2023-08-27T12:46:09.398Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>This is I think a really good comment. &nbsp;The animal consciousness stuff I think is a bit crazy. &nbsp;If Dennett thinks that as well . . . well, I never gave Dennett much deference. &nbsp;</p><p>I was exaggerating a bit when I said that no undergraduate would make that error. &nbsp;</p><p>I don't think that Schwarz saying he might publish it is much news. &nbsp;I have a friend who is an undergraduate in his second year and he has 5 or 6 published philosophy papers--I'm also an undergraduate and I have one forthcoming. &nbsp;</p><p>Do we know what journal Eliezer was publishing in? &nbsp;I'd expect it not to get published in even a relatively mediocre journal, but I might be wrong.&nbsp;</p>", "parentCommentId": "mscCTLnBoTNjRwpKM", "user": {"username": "Omnizoid"}}, {"_id": "9xKBdFMD3KbzAD5Wm", "postedAt": "2023-08-27T12:47:23.710Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Yeah, though we can imagine that everyone feels similar urge to smoke, but it's only the people with the lesion who ultimately decide to smoke.&nbsp;</p>", "parentCommentId": "tRJtKoPv2d7yu9cDk", "user": {"username": "Omnizoid"}}, {"_id": "ReESqqheJp4jYLBtH", "postedAt": "2023-08-27T12:49:10.372Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Eliezer has a huge number of controversial beliefs--about AI, physics, Newcombe's problem, zombies, nanotech, etc. &nbsp;Many of these are about things I know nothing about. &nbsp;But there are a few things where he adopts deeply controversial views that I know something about. &nbsp;And almost every time--well above half the time--that I know enough to fact check him, he turns out to be completely wrong in embarrassing ways.&nbsp;</p>", "parentCommentId": "rrneZmaasggzFHxyG", "user": {"username": "Omnizoid"}}, {"_id": "igmekhxYmgm2TEdMh", "postedAt": "2023-08-27T12:51:48.862Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Eliezer talks about lots of topics that I don't know anything about. &nbsp;So I can only write about the things that I do know about. &nbsp;There are maybe five or six examples of that, and I think he has utterly crazy views in perhaps all except one of those cases. &nbsp;</p><p>I can't fat check him on physics or nanotech, for instance.&nbsp;</p>", "parentCommentId": "QhfjnCvwQypsJNhbP", "user": {"username": "Omnizoid"}}, {"_id": "AqbhiTka2jo9DoB9s", "postedAt": "2023-08-27T12:54:29.530Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote>\n<p>There\u2019s a 1 in a googol chance that he\u2019ll blackmail someone who would give in to the blackmail and a googol-1/googol chance that he\u2019ll blackmail someone who won\u2019t give in to the blackmail.</p>\n</blockquote>\n<p>Did you mean the opposite of this? Sounds like you are saying he would almost never blackmail someone who WOULD give in and almost always blackmail someone who WOULDNT give in.</p>\n", "parentCommentId": null, "user": {"username": "david_reinstein"}}, {"_id": "fpL3v4exsEGTjSohw", "postedAt": "2023-08-27T12:57:45.347Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Yes, sorry!&nbsp;</p>", "parentCommentId": "AqbhiTka2jo9DoB9s", "user": {"username": "Omnizoid"}}, {"_id": "8H2AhvtaP89gfSH7a", "postedAt": "2023-08-27T13:06:40.398Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Note for readers: this was also posted on <a href=\"https://www.lesswrong.com/posts/TjyyngWFYvQWPpNNj/?commentId=Ghkp6ejqooZswEFXs#comments\">LessWrong</a>, where it received a very different reception and a bunch of good responses. Summary: the author is confidently, egregiously wrong (or at least very confused) about most of the object-level points he accuses Eliezer and others of being mistaken or overconfident about.&nbsp;</p><p>Also, the writing here seems much more like it is deliberately engineered to get you to believe something (that Eliezer is bad) than anything Eliezer has ever actually written. If you initially found such arguments convincing, consider examining whether you have been \"duped\" by the author.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Max H"}}, {"_id": "fJH2JuY5LcagSWxSc", "postedAt": "2023-08-27T13:10:23.972Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Phew. Please fix when you have a moment thanks. (Otherwise people may start to think they are not understanding things and give up reading.)</p>\n", "parentCommentId": "fpL3v4exsEGTjSohw", "user": {"username": "david_reinstein"}}, {"_id": "GcL3wuPxwDAqfttEq", "postedAt": "2023-08-27T13:14:52.684Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks!</p>\n<p>I don't know the journal Schwarz rejected it for, no. I f your friend has 5 or 6 publications as an undergrad then either they are a genius, or they are unusually talented and also very ruthless about identifying small, technical objections to things famous people have said, or they are publishing in extremely mediocre journals. The second and third things ares probably not what's going on when Wolfgang gives an R&amp;R to the Yudkowsky/Soares fdt paper. It is an attempt to give a big new fundamental theory, not a nitpick. And regardless of the particular journal Wolfgang was reviewing for, I don't think (could be wrong though!), that the reason why it is easy to get published in the crappiest journals is because really sharp philosophers with multiple publications in top 5-10 journals drop their standards to a trivial level when reviewing for them. No doubt they drop their standards somewhat, but those journals probably have worse reviewers quite a lot of the time. (That's only a guess though.)</p>\n<p>More importantly, a bit of googling to me revealed that Soares, though not Yudkowsky, is a coauthor on a paper defending fdt in <em>Journal of Philosophy</em>. (With Ben Levinstein who is an actual philosophy prof.) That alone takes fdt well out of the crank zone in my view. J Phil is a clear top 10 journal, probably top 5. It probably rejects around 95% of the papers sent to it. Admittedly there's a limit to how much credit Eliezer should get for a paper he didn't write, but insofar as fdt is \"his\" idea (don't know how much he developed it versus Soares and other MIRI people), this is the greenest of Philosophy green flags.</p>\n", "parentCommentId": "eM4FFnHL5W4Z7Fjdh", "user": {"username": "Dr. David Mathers"}}, {"_id": "6ksuSZkHnbqhWGv7o", "postedAt": "2023-08-27T13:18:18.604Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Okay yeah, fair. &nbsp;Here's my friends publication record <a href=\"https://philpeople.org/profiles/amos-wollen\">https://philpeople.org/profiles/amos-wollen</a> &nbsp;</p><p>Though worth noting that the other author rejected it. &nbsp;It's not clear how common it is for one reviewer to be willing to submit your paper after heavy revisions is.&nbsp;</p>", "parentCommentId": "GcL3wuPxwDAqfttEq", "user": {"username": "Omnizoid"}}, {"_id": "pAcY5PDwemF5c7eLL", "postedAt": "2023-08-27T13:19:32.631Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Fixed!&nbsp;</p>", "parentCommentId": "fJH2JuY5LcagSWxSc", "user": {"username": "Omnizoid"}}, {"_id": "4W2dHB3X5oFrkYvSm", "postedAt": "2023-08-27T13:21:10.987Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I obviously disagree that this is the conclusion of the LessWrong comments, many of which I think are just totally wrong! &nbsp;Notably, I haven't replied to many of them because the LessWrong bot makes it impossible for me to post above once per hour because I have negative Karma on recent posts.&nbsp;</p>", "parentCommentId": "8H2AhvtaP89gfSH7a", "user": {"username": "Omnizoid"}}, {"_id": "pBQyMsPdjKWH2kGHN", "postedAt": "2023-08-27T13:33:38.307Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Putting aside whether or not what you say is correct, do you think it's possible that you have fallen prey to the overconfidence that you accuse Eliezer of? This post was very strongly written and it seems a fair number of people disagree with your arguments.</p>", "parentCommentId": "4W2dHB3X5oFrkYvSm", "user": {"username": "jackmalde"}}, {"_id": "WRowtaBRHQsp5yJia", "postedAt": "2023-08-27T13:36:32.256Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I mean, it's always possible. &nbsp;But the views I defend here are utterly mainstream. &nbsp;Virtually no people in academia think either FDT, Eliezer's anti-zombie argument, or animal nonconsciousness are correct. &nbsp;</p>", "parentCommentId": "pBQyMsPdjKWH2kGHN", "user": {"username": "Omnizoid"}}, {"_id": "vpeuAJcgsMe6ZaSzg", "postedAt": "2023-08-27T13:47:26.186Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I don't think you've summarised the LessWrong comments well. Currently, they don't really engage with the substantive content of the post and/or aren't convincing to me. They spend a lot of time criticising the tone of the post. The comments here by Dr. David Mathers are a far better critique than anything on LessWrong.</p>\n<p>I do agree that the post title goes too far compared to what is actually argued.</p>\n<blockquote>\n<p>Also, the writing here seems much more like it is deliberately engineered to get you to believe something (that Eliezer is bad) than anything Eliezer has ever actually written. If you initially found such arguments convincing, consider examining whether you have been \"duped\" by the author.</p>\n</blockquote>\n<p>This paragraph seems in bad faith without substantiating, currently it's just vague rhetoric. What do you mean by \"deliberately engineered to get you to believe something\"? That sounds to me like a way of framing \"making an argument\" to sound malicious.</p>\n", "parentCommentId": "8H2AhvtaP89gfSH7a", "user": {"username": "jooke"}}, {"_id": "CARKn6cfDk6zcZ7sL", "postedAt": "2023-08-27T13:48:54.070Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p><a href=\"https://philpeople.org/profiles/amos-wollen\">Fixed link</a></p>\n", "parentCommentId": "6ksuSZkHnbqhWGv7o", "user": {"username": "jooke"}}, {"_id": "Fn27xofBMe4749C4i", "postedAt": "2023-08-27T13:49:04.937Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>You've commented 12 times so far on that post, including on all 4 of the top responses. My advice: try engaging from a perspective of inquiry and seeking understanding, rather than agreement / disagreement. This might take longer than making a bunch of rapid-fire responses to every negative comment, but will probably be more effective.&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/TjyyngWFYvQWPpNNj/eliezer-yudkowsky-is-frequently-confidently-egregiously?commentId=Ghkp6ejqooZswEFXs\">My own experience</a> commenting and getting a response from you is that there's not much room for disagreement on decision theory - the issue is more that you don't have a solid grasp of the basics of the thing you're trying to criticize, and I (and others) are explaining why. I don't mind elaborating more for others, but I probably won't engage further with you unless you change your tone and approach, or articulate a more informed objection.</p>", "parentCommentId": "4W2dHB3X5oFrkYvSm", "user": {"username": "Max H"}}, {"_id": "rpKZMuabqfccB9DNC", "postedAt": "2023-08-27T13:56:19.667Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Another pernicious aspect of Eliezer's Zombie discussion is his insinuation that differing views from him on the matter imply that one should not take seriously their other views. Even if Yudkowsky is right and others are fantastically wrong on zombies, this provides but a very small credence update as to how we should consider their other views being accurate. History is littered with brilliant and useful people who have been famously and impressively wrong on some specific matters.</p>\n", "parentCommentId": null, "user": {"username": "Brad West"}}, {"_id": "GQaetmLC82hhPHRgW", "postedAt": "2023-08-27T14:02:07.442Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Fair point that many rejected things probably received one \"revise and resubmit\".</p>\n<p>The link to your friend's philpapers page I'd broken, but I googled him and I <em>think</em> mediocre journals is probably, mostly the right answer, mixed a bit with \"your friend is very, talented\" (Though to be clear even 5 mediocre pubs is impressive for <em>a 2nd year undergrad</em>, and I would predict your friend can go to a good grad school if he wants to. ) Philosophia is a generalist journal I never read a single paper in in the 15 or so years I was reading philosophy papers generally, which is a bad sign. I'd never heard of \"Journal of Ayn Rand Studies\" but I can think of at most 1 possible examples of a good journal dedicated to a single philosopher and my guess is most people competent to review philosophy paper either hate Rand or have never read her. (This is the one journal of the 4 that even an undergrad pub in might not mean much, beyond the selection effect of mostly only fairly talented students trying to publish in the first place.) I'd never heard of Journal of Value Inquiry either. But I did find a Leiter Reports poll ranking it 18th out of moral and political philosophy journals, do publishing in it is probably a non-trivial achievement. Never heard of History and Philosophy of the Life Sciences, nor would I expected to have even if it was good. Your friend's paper looks like a straightforward historical discussion of what Darwin himself said evolution implied about epistemology rather than a defence of an original philosophical view though.</p>\n", "parentCommentId": "6ksuSZkHnbqhWGv7o", "user": {"username": "Dr. David Mathers"}}, {"_id": "hQJKk4damq3DjswZj", "postedAt": "2023-08-27T14:07:13.408Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I personally commented with an object-level <a href=\"https://www.lesswrong.com/posts/TjyyngWFYvQWPpNNj/eliezer-yudkowsky-is-frequently-confidently-egregiously?commentId=Ghkp6ejqooZswEFXs\">objection</a>; plenty of others have done the same.<br><br>I mostly take issue with the factual claims in the post, which I think is riddled with errors and misunderstandings (many of which have been pointed out), but the language is also unnecessarily emotionally charged and inflammatory in many places. A quick sampling:</p><blockquote><p><br>But as I grew older and learned more, I realized it was all bullshit.</p><p><br>it becomes clear that his view is a house of cards, built entirely on falsehoods and misrepresentations.<br><br>And I spend much more time listening to Yukowsky\u2019s followers spout nonsense than most other people.<br><br>&nbsp;(phrased in a maximally Eliezer like way): ... (condescending chuckle)</p></blockquote><p><br>I am frankly pretty surprised to see this so highly-upvoted on the EAF; the tone is rude and condescending, more so than anything I can recall Eliezer writing, and much more so than the usual highly-upvoted posts here.<br><br>The OP seems more interested in arguing about whatever \"mainstream academics\" believe than responding to (or even understanding) object-level objections. But even on that topic, they make a bunch of misstatements and overclaims. From a <a href=\"https://forum.effectivealtruism.org/posts/ZS9GDsBtWJMDEyFXh/eliezer-yudkowsky-is-frequently-confidently-egregiously?commentId=WRowtaBRHQsp5yJia\">comment</a>:</p><blockquote><p><br>But the views I defend here are utterly mainstream. &nbsp;Virtually no people in academia think either FDT, Eliezer's anti-zombie argument, or animal nonconsciousness are correct. &nbsp;</p></blockquote><p><br>(Plenty of people who disagree with the author and agree or partially agree with Eliezer about the object-level topics are in academia. Some of them even post on LessWrong and the EAF!)<br><br><br><br>&nbsp;</p>", "parentCommentId": "vpeuAJcgsMe6ZaSzg", "user": {"username": "Max H"}}, {"_id": "uqvg4sjk8WJkjoLsG", "postedAt": "2023-08-27T14:14:35.001Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I am as much a naturalist dualist as you are (see<a href=\"https://www.lesswrong.com/posts/nY7oAdy5odfGqE7mQ/freedom-under-naturalistic-dualism\"> here</a>), and I also find extremely suprising how confidently you write about fish suffering (even chickens are a doubtful case!). As a naturalistic dualist, you know how hard is to assess conscience (the ultimate noumenon).</p><p>My intuition is that conscient experience grows far more than linearly (perhaps exponentially!) with the size of the supporintg neural network. If this happens, the ample mayority of concience is concentrated in the apex taxa, while aggregate moral value of lower taxa is small (even if the number of individuals is massively bigger).&nbsp;</p><p>Additionally, I made a <a href=\"https://forum.effectivealtruism.org/posts/a3PY2Q3T9wSGnrB5t/evil\">some comments about your overconfidence on foreign policy</a>, where all moral issues are dependent on historical counterfactuals. It is very easy to show that politician's arguments in this field are often absurd (the Chomsky method), but in general public declarations are purely instrumental.</p><p>On the other hand, the American Hegemony is the basis of a global integrated economy, and it has been an age of peace and global income convergence. Could you argue against American Foreing policy in a consequalist basis? What is the alternative you have in mind?&nbsp;</p>", "parentCommentId": null, "user": {"username": "Arturo Macias"}}, {"_id": "7fs5nHEkK6AGgPAJ9", "postedAt": "2023-08-27T16:47:50.616Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>The first object-level issue the author talks about is whether the brain is close to the Landaeur limit. &nbsp;No particular issue is cited, only that somebody else claimed a lot of authority and claimed I was wrong about something, what exactly is not shown.</p><p>The brain obviously cannot be operating near the Landaeur limit. &nbsp;Thousands of neurotransmitter molecules and thousands of ions need to be pumped back to their original places after each synaptic flash. &nbsp;Each of these is a thermodynamically irreversible operation and it staggers the imagination that every ion pumped en masse back out of some long axon or dendrite, after ions flooded en masse into it to propagate electrical depolarization, is part of a well-designed informational algorithm that could not be simplified. &nbsp;Any calculation saying that biology is operating close to the Landaeur limit has reached a face-value absurdity.</p><p>Of course, this may not seem to address anything, since OP failed to state <i>what</i> I was putatively wrong about and admits to not understanding it themselves; I can't refute what isn't shown.</p><p>The first substantive criticism OP claims to understand theirself is on Zombies.</p><p>I say:</p><blockquote><p>Your \"zombie\", in the philosophical usage of the term, is putatively a being that is exactly like you in <i>every</i> respect\u2014identical behavior, identical speech, identical brain; every atom and quark in <i>exactly</i> the same position, moving according to the same causal laws of motion\u2014<i>except</i> that your zombie is not conscious.</p></blockquote><p>The author would have you believe this is a ludicrous straw position.</p><p>I invite anyone to simply read the opening paragraphs of the SEP encyclopedia entry on P-zombies:</p><blockquote><p>If zombies are to be counterexamples to physicalism, it is not enough for them to be behaviorally and functionally like normal human beings: plenty of physicalists accept that merely behavioral or functional duplicates of ourselves might lack qualia. Zombies must be like normal human beings in <i>all</i> physical respects, and they must have the physical properties that physicalists suppose we have. This requires them to be subject to the causal closure of the physical, which is why their supposed lack of consciousness is a challenge to physicalism. If instead they were to be conceived of as creatures whose behavior could not be explained physically, physicalists would have no reason to bother with the idea: there is plenty of evidence that, as epiphenomenalists hold, our movements actually are explicable in physical terms (see e.g. Papineau 2002).</p></blockquote><p>This is a debate that has gone on for very long in philosophy. &nbsp;I'd say it's gone on too long.</p><p>But whether or not particular thought experiments, by seeming metaphysically possible, license other conclusions about metaphysics, is exactly the entire substance. &nbsp;The <i>base</i> thought experiment is not or should not be in dispute: it's a being whose physics duplicate the physics of a human being <i>including the causal closure of what is said to be 'physics', </i>i.e., all of the <i>causes</i> of behavior are included into the p-zombie. &nbsp;Some people go on at fantastic length from this to say that it demonstrates the possibility of an extra consciousness that they call \"epiphenomenal\", and some say that it demonstrates the possibility of a nonphysical consciousness that they <i>don't call</i> \"epiphenomenal\", but it's <i>my position</i> that somewhere along the way of a long argument they have dropped the ball on the original thought experiment; whatever they call \"consciousness\" that isn't in the supposed p-zombie, it can't be among the causes of why we talk about consciousness, or why our verbally reportable stream of thought talks about consciousness, etc, because the zombie behaves outwardly like we do and also includes the minimal closure of the causes of that physical behavior.</p><p>The author of the above post has misrepresented what my zombies argument was about. &nbsp;It's not that I think philosophers openly claim that p-zombies demonstrate epiphenomenalism; it's that I think philosophers are confused about what this thought experiment demonstrates.</p><p>The author having been shown to be wrong on the first points addressed, which I chose in order rather than selectively sampling, I hope you accept this as obvious evidence that the rest would be no better if you looked into them in detail or I responded in detail. &nbsp;For a post claiming to show that I'm often grossly wrong, actual quotes from me, with linked context and dates attached, are remarkably thin on the ground.</p><p>You will mark that in this comment I first respond to a substantive point and show it to be mistaken before I make any general criticism of the author; which can then be supported by that previously shown, initial, first-thing, object-level point. &nbsp;You will find every post of the Less Wrong sequences written the same way.</p><p>As the entire post violates basic rules of epistemic conduct by opening with a series of not-yet-supported personal attacks, I will not be responding to the rest in detail. &nbsp;I'm sad about how anything containing such an egregious violation of basic epistemic conduct got this upvoted, and wonder about sockpuppet accounts or alternatively a downfall of EA. &nbsp;The relevant principle of epistemic good conduct seems to me straightforward: if you've got to make personal attacks (and sometimes you do), make them <i>after </i>presenting your object-level points that support those personal attacks. &nbsp;This shouldn't be a difficult rule to follow, or follow much better than this; and violating it this hugely and explicitly is sufficiently bad news that people should've been wary about this post and hesitated to upvote it for that reason alone.</p>", "parentCommentId": null, "user": {"username": "EliezerYudkowsky"}}, {"_id": "uiMjaRDTK36dboHqq", "postedAt": "2023-08-27T16:52:30.984Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>As Ahmed notes (chapter 4.3.1), if the lesion doesn't work through your beliefs and desires, smoking is not a genuine option, and so this is not an argument against evidentialism.</p>", "parentCommentId": "9xKBdFMD3KbzAD5Wm", "user": {"username": "Sylvester Kollin"}}, {"_id": "C5hza2CMTR3Gvbsyo", "postedAt": "2023-08-27T17:14:13.469Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I would very much prefer it if one didn't <a href=\"https://en.wikipedia.org/wiki/Appeal_to_consequences\">appeal to the consequences</a> of the belief about animal moral patienthood, and instead argue whether animals in fact <em>are</em> moral patients or not, or whether the question is well-posed.</p>\n<p>For this reason, I have strong-downvoted your comment.</p>\n", "parentCommentId": "KRKf3w6ve54i5Ptc2", "user": {"username": "niplav"}}, {"_id": "7EsaR47SqFrbyeGDn", "postedAt": "2023-08-27T17:42:21.444Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>If you're going to claim he is 'egregiously' wrong I would hope for clearer examples, like that he said the population of China was 100 million, or that the median apartment in Brooklyn cost $100k, or something like that. These three examples seem both cherrypicked - anyone with a long career as a genuine intellectual innovator will make claims on a wide variety of subjects, so three is nothing like what is required to claim 'frequent' - and ambiguous.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Larks"}}, {"_id": "GYFAxRfinBrF3Dxcc", "postedAt": "2023-08-27T18:02:24.607Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>I feel like there are two types of thinkers, the first we might call innovators and the second systematizers. Innovators are the kinds of people who think of wacky, out-of-the-box ideas, but are less likely to be right. They enrich the state of discourse by being clever, creative, and coming up with new ideas, rather than being right about everything. A paradigm example is Robin Hanson\u2014no one feels comfortable just deferring to Robin Hanson across the board, but Robin Hanson has some of the most ingenious ideas.</p><p>Systematizers, in contrast, are the kinds of people who reliably generate true beliefs on lots of topics. A good example is Scott Alexander. I didn\u2019t research Ivermectin, but I feel confident that Scott\u2019s post on Ivermectin is at least mostly right.</p><p>I think people think of Eliezer as a systematizer. And this is a mistake, because he just makes too many errors. He\u2019s too confident about things he\u2019s totally ignorant about. But he\u2019s still a great innovator. He has lots of interesting, clever ideas that are worth hearing out. In general, however, the fact that Eliezer believes something is not especially probative. Eliezer\u2019s skill lies in good writing and ingenious argumentation, not forming true beliefs.</p></blockquote><p>Although this is a pretty gross oversimplification, it does touch on some valuable points.</p><p>I don't think it makes sense to assume an equivalence between Hanson and Yudkowsky. Hanson has <a href=\"https://www.lesswrong.com/posts/Ks2wmthZM9vttjfk4/problems-with-robin-hanson-s-quillette-article-on-ai\">bumbled</a> into the AI arena in a way that Yudkowsky never would. It's possible that Hanson has only developed skill at making groundbreaking discoveries in economics and sociology, and stumbled instead of hitting the ground running when he ventured into AI, and that Yud did a similar thing with consciousness ethics. But even if true, that's where the similarities would end, because both human neurotypes, <i>and</i> Hanson and Scott Alexander and Yudkowsky's personal histories and skills, are each vastly more complicated than the oversimpified innovator-systematizer dichotomy that this post leaves as its concluding argument.</p>", "parentCommentId": null, "user": {"username": "trevorw96"}}, {"_id": "SqHezrWArtCcpvvp2", "postedAt": "2023-08-27T18:36:05.110Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>my guess is most people competent to review philosophy paper either hate Rand or have never read her</p></blockquote><p>I believe that to be true, and to be a very good sign of what kind of an ivory tower philosophy has become.</p>", "parentCommentId": "GQaetmLC82hhPHRgW", "user": {"username": "Micha\u0142 Zab\u0142ocki"}}, {"_id": "DE8zkzXmFihbfw4pK", "postedAt": "2023-08-27T18:38:37.651Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>My understanding from Eliezer's writing is that he's an illusionist (and/or a higher-order theorist) about consciousness. However, illusionism (and higher-order theories) are compatible with mammals and birds, at least, being conscious. It depends on the specifics.</p><p>I'm also an illusionist about consciousness and very sympathetic to the idea that some kinds of higher-order processes are required, but I do think mammals and birds, at least, are very probably conscious, and subject to consciousness illusions. My understanding is that Humphrey (<a href=\"https://mitpress.mit.edu/9780262047944/sentience/\"><u>Humphrey, 2022</u></a>,&nbsp;<a href=\"https://aeon.co/essays/how-blindsight-answers-the-hard-problem-of-consciousness\"><u>Humphrey, 2023a</u></a>,&nbsp;<a href=\"https://nextbigideaclub.com/magazine/sentience-invention-consciousness-bookbite/40738/\"><u>Humphrey, 2023b</u></a>,&nbsp;<a href=\"https://eprints.lse.ac.uk/80740/1/Humprey_Invention%20of%20consciousness.pdf\"><u>Humphrey, 2017</u></a>,&nbsp;<a href=\"https://www.newyorker.com/news/annals-of-inquiry/nicholas-humphreys-beautiful-theory-of-mind\"><u>Romeo, 2023</u></a>,&nbsp;<a href=\"https://www.researchgate.net/profile/Nicholas-Humphrey/publication/30529581_Seeing_Red_A_Study_in_Consciousness/links/576bb8be08ae5b9a62b624cc/Seeing-Red-A-Study-in-Consciousness.pdf\"><u>Humphrey, 2006</u></a>,&nbsp;<a href=\"https://press.princeton.edu/books/paperback/9780691156378/soul-dust\"><u>Humphrey, 2011</u></a>) and <a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/\"><u>Muehlhauser (2017)</u></a> (a report for Open Phil, but representing his own views) would say the same. Furthermore, I think the<i> standard</i> interpretation of illusionism doesn\u2019t require consciousness illusions or higher-order processes in conscious subjects at all, and instead a system is conscious if connecting a sufficiently sophisticated introspective system to it the right way would lead to consciousness illusions, and this interpretation would plausibly attribute consciousness more widely, possibly quite widely (<a href=\"https://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/f0020011/art00004\"><u>Blackmore, 2016</u></a> (<a href=\"https://www.susanblackmore.uk/articles/delusions-of-consciousness/\"><u>available submitted draft</u></a>),&nbsp;<a href=\"https://www.keithfrankish.com/blog/illusionism-and-compassion/\">Frankish, 2020</a>, <a href=\"https://youtu.be/txiYTLGtCuM?t=5042\"><u>Frankish, 2021</u></a>,&nbsp;<a href=\"https://youtu.be/xZxcair9oNk?t=3242\"><u>Frankish, 2023</u></a>,&nbsp;<a href=\"https://www.pnas.org/doi/10.1073/pnas.2102421118\">Graziano, 2021</a>, <a href=\"https://link.springer.com/article/10.1007/s11229-022-03710-1\"><u>Dung, 2022</u></a>).</p><p>If I recall correctly, Eliezer seemed to give substantial weight to relatively sophisticated self- and other-modelling, like cognitive empathy and passing the mirror test. Few animals seem to pass the mirror test, so that would be reason for skepticism.</p><p>However, maybe they\u2019re just not smart enough to infer that the reflection is theirs, or they don\u2019t rely enough on sight. Or, they may recognize themselves in other ways or to at least limited degrees. Dogs can remember what actions they\u2019ve spontaneously taken (<a href=\"https://www.nature.com/articles/s41598-020-67302-0\"><u>Fugazza et al., 2020</u></a>) and recognize their own bodies as obstacles (<a href=\"https://www.nature.com/articles/s41598-021-82309-x\"><u>Lenkei, 2021</u></a>), and grey wolves show signs of self-recognition via a scent mirror test (<a href=\"https://www.tandfonline.com/doi/abs/10.1080/03949370.2020.1846628\"><u>Cazzolla Gatti et al., 2021</u></a>, layman summary in&nbsp;<a href=\"https://www.psychologytoday.com/ca/blog/animal-minds/202101/wolves-demonstrate-self-awareness-in-sniff-test\"><u>Mates, 2021</u></a>). Pigeons can discriminate themselves from conspecifics with mirrors, even if they don\u2019t recognize the reflections as themselves (<a href=\"https://www.frontiersin.org/articles/10.3389/fpsyg.2021.669039/full\"><u>Wittek et al., 2021</u></a>,&nbsp;<a href=\"https://link.springer.com/article/10.1007/s10071-008-0161-4\"><u>Toda and Watanabe, 2008</u></a>). Mice are subject to the rubber tail illusion and so probably have a sense of body ownership (<a href=\"https://www.jneurosci.org/content/36/43/11133.long\"><u>Wada et al., 2016</u></a>).</p><p>Furthermore, <a href=\"https://doi.org/10.1258/002367795780739999\"><u>Carey and Fry (1995)</u></a> show that pigs generalize the discrimination between non-anxiety states and drug-induced anxiety to non-anxiety and anxiety in general, in this case by pressing one lever repeatedly with anxiety, and alternating between two levers without anxiety (the levers gave food rewards, but only if they pressed them according to the condition). Similar experiments were performed on rodents, as discussed in&nbsp;<a href=\"https://atrium.lib.uoguelph.ca/xmlui/bitstream/handle/10214/9465/Sanchez-Suarez_Walter_201601_PhD.pdf?sequence=1&amp;isAllowed=y\"><u>S\u00e1nchez-Su\u00e1rez, 2016</u></a>, in section 4.d., starting on p.81. Rats generalized from hangover to morphine withdrawal and jetlag, and from high doses of cocaine to movement restriction, from an anxiety-inducing drug to aggressive defeat and predator cues. Of course, anxiety has physical symptoms, so maybe this is what they're discriminating, not the negative affect.</p><p>&nbsp;</p><p>There are also of course many non-illusionist theories of consciousness that attribute consciousness more widely that are defended (although I'm personally not sympathetic, unless they're illusionist-compatible), and theory-neutral or theory-light approaches. On theory-neutral and theory-light approaches, see <a href=\"https://fcmconference.org/img/CambridgeDeclarationOnConsciousness.pdf\"><u>Low, 2012</u></a>,&nbsp;<a href=\"https://www.wellbeingintlstudiesrepository.org/cgi/viewcontent.cgi?article=1068&amp;context=acwp_arte\"><u>Sneddon et al., 2014</u></a>,&nbsp;<a href=\"https://efsa.onlinelibrary.wiley.com/doi/10.2903/sp.efsa.2017.EN-1196\"><u>Le Neindre et al., 2016</u></a>,&nbsp;<a href=\"https://rethinkpriorities.org/invertebrate-sentience-table\"><u>Rethink Priorities, 2019</u></a>,&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/nous.12351\"><u>Birch, 2020</u></a>,&nbsp;<a href=\"https://eprints.lse.ac.uk/115949/1/art00001.pdf\"><u>Birch et al., 2022</u></a>,&nbsp;<a href=\"https://www.frontiersin.org/articles/10.3389/fvets.2022.788289/full\"><u>Mason and Lavery, 2022</u></a>, generally giving more weight to the more recent work.</p>", "parentCommentId": null, "user": {"username": "MichaelStJules"}}, {"_id": "ZebL23Jwt7F7qcemy", "postedAt": "2023-08-27T18:47:04.670Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Maybe the examples are ambiguous but they don't seem cherrypicked to me. Aren't these some of the topics Yudskowky is most known for discussing? It seems to me that the cherrypicking criticism would apply to opinions about, I don't know, monetary policy, not issues central to AI and cognitive science.</p>", "parentCommentId": "7EsaR47SqFrbyeGDn", "user": {"username": "seanrichardson@outlook.com"}}, {"_id": "b9c9h6wzbRr4PHxuH", "postedAt": "2023-08-27T18:55:55.151Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I think the objection comes from the seeming asymmetry between over-attributing and under-attributing consciousness. It's fine to discuss our independent impressions about some topic, but when one's view is a minority position and the consequences of false beliefs are high, isn't there some obligation of epistemic humility?</p>", "parentCommentId": "C5hza2CMTR3Gvbsyo", "user": {"username": "seanrichardson@outlook.com"}}, {"_id": "9HJwCd8GB9jRDyMZG", "postedAt": "2023-08-27T18:58:23.795Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Some other discussion of his views on (animal) consciousness <a href=\"https://www.lesswrong.com/posts/KFbGbTEtHiJnXw5sk/i-really-don-t-understand-eliezer-yudkowsky-s-position-on\">here</a> (and in the comments).</p>", "parentCommentId": null, "user": {"username": "MichaelStJules"}}, {"_id": "7s6jL3dMp4s8zdHvf", "postedAt": "2023-08-27T19:11:22.388Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks for describing your reasons. My criterion for moral patienthood is described by this <a href=\"https://reducing-suffering.org/mr-rogers-on-unconditional-love/\">Brian Tomasik quote</a>:</p><blockquote><p>When I realize that an organism feels happiness and suffering, at that point I realize that the organism matters and deserves care and kindness. In this sense, you could say <strong>the only \"condition\" of my love is sentience</strong>.</p></blockquote><p>Many other criteria for moral patienthood which exclude animals have been proposed. These criteria always suffer from some combination of the following:</p><ol><li>Arbitrariness. For example, \"human DNA is the criterion for moral patienthood\" is just as arbitrary as \"European DNA is the criterion for moral patienthood\".</li><li>Exclusion of some humans. For example, \"high intelligence is the criterion for moral patienthood\" excludes people who have severe mental disabilities.</li><li>Exclusion of hypothetical beings. For example, \"human DNA is the criterion for moral patienthood\" would exclude superintelligent aliens and intelligent conscious AI. Also, if some people you know were unknowingly members of a species which looked/acted much like humans but had very different DNA, they would suddenly become morally valueless.</li><li>Collapsing to sociopathy or nihilism. For example, \"animals don't have moral patienthood because we have power over them\" is just nihilism, and if a person used that justification to act the way we do towards farmed animals towards other humans, they'd be locked up.</li></ol><p>The most parsimonious definition of moral patient I've seen proposed is just \"a sentient being\". I don't see any reason why I should add complexity to that definition in order to exclude nonhuman animals. The only motivation I can think of for doing this would be to compromise on my moral principles for the sake of the pleasure associated with eating meat, which is untenable to a mind wired the way mine is.</p>", "parentCommentId": "C5hza2CMTR3Gvbsyo", "user": {"username": "Ariel Simnegar"}}, {"_id": "Wp5jpBzsevRQfj9KN", "postedAt": "2023-08-27T19:48:07.245Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>None of these issues are \"central\" to AI or the cognitive science that's relevant to AI, AI alignment, or human upskilling. The author's area of interest is more about consciousness, animal welfare, and qualia.&nbsp;</p><p>The issues are the sole thing justifying Omnizoid's rather heated indictments against Yudkowsky, such as:</p><blockquote><p>Eliezer has swindled many of the smartest people into believing a whole host of wildly implausible things. Some of my favorite writers\u2014e.g. Scott Alexander\u2014seem to revere Eliezer. It\u2019s about time someone exposed the mountain of falsehoods on which his arguments rest. If one of the world\u2019s most influential thinkers is just demonstrably wrong about lots of topics, often in ways so egregious that they demonstrate very basic misunderstandings, then that\u2019s quite newsworthy, just as it would be if a presidential candidate supported a slate of terrible policies.</p></blockquote><p>Most readers will only read the accusations in the introduction and then bounce off the evidence backing them; because all of them are topics that, like string theory, only a handful of people on earth are capable of engaging with them. It just so happens that the author is one of them. Virtually nobody can read the actual arguments behind this post without dedicating &gt;4 hours of their life to it, which makes it pretty well optimized to attract attention and damage Yudkowsky's reputation as much as possible with effectively zero accountability.</p>", "parentCommentId": "ZebL23Jwt7F7qcemy", "user": {"username": "trevorw96"}}, {"_id": "BnDbo5enemHufMDzw", "postedAt": "2023-08-27T20:15:30.434Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I thought the final three paragraphs were the best part of this post and I wish you had led with them!</p><p><i>\"Consider two types of thinkers...I think that 'hero worship' is often a problem in this community because people - including myself - have mistaken innovators for systematizers...Sounds plausible, right? Let's take Eliezer as an example (sorry, Eliezer!)...\"</i></p><p>This practice of \"This generally pretty great person/org in our community [probably] has a FLAW!!\" *upvote upvote upvote upvote* doesn't seem very healthy to me<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefi2t9t5n9bqp\"><sup><a href=\"#fni2t9t5n9bqp\">[1]</a></sup></span>, whereas \"<a href=\"https://forum.effectivealtruism.org/posts/jgspXC8GKA7RtxMRE/on-living-without-idols\">Here's a mistake I think I've made</a> and I think lots of us are making\" (and <a href=\"https://forum.effectivealtruism.org/posts/f77iuXmgiiFgurnBu/run-posts-by-orgs\">sharing the post in advance</a> with anyone identifiable singled out for criticism) seems very helpful :)</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fni2t9t5n9bqp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefi2t9t5n9bqp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Although if Eliezer is as condescending as you make out, part of me thinks it's fair play to be somewhat ridiculing in response. Still, an eye for an eye makes the whole world blind, ya know.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "aprilsun"}}, {"_id": "mfcanXMsGRgbkcJ4M", "postedAt": "2023-08-27T20:22:03.967Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>It's frustrating to read comments like this because they make me feel like, if I happen agree with Eliezer about something, my own agency and ability to think critically is being questioned before I've even joined the object-level discussion.</p><p>Separately, this comment makes a bunch of mostly-implicit object-level assertions about animal welfare and its importance, and a bunch of mostly-explicit assertions about Eliezer's opinions and influence on rationalists and EAs, as well as the effect of this influence on the impacts of TAI.</p><p>None of these claims are directly supported in the comment, which is fine if you don't want to argue for them here, but the way the comment is written might lead readers who agree with the implicit claims about the animal welfare issues to accept the explict claims about Eliezer's influence and opinions and their effects on TAI with a less critical eye than if these claims were otherwise more clearly separated.</p><p>For example, I don't think it's true that a few FB posts / comments have had a \"huge influence\" on rationalist culture. I also think that worrying about animal welfare specifically when thinking about TAI outcomes is less important than you claim. If we succeed in being able to steer TAI at all (unlikely, <a href=\"https://www.lesswrong.com/posts/qrrEtrbLcmqr3b5uf/without-a-trajectory-change-the-development-of-agi-is-likely\">in my view</a>), animals will do fine - so will everyone else. At a minimum, there will also be no more global poverty, no more malaria, and no more animal suffering. Even if the specific humans who develop TAI don't care at all about animals themselves (not exactly likely), they are unlikely to completely ignore the concerns of everyone else who does care. But none of these disagreements have much or any bearing on whether I think animal suffering is real (I find this at least plausible) and whether that's a moral horror (I think this is very likely, if the suffering is real).</p>", "parentCommentId": "KRKf3w6ve54i5Ptc2", "user": {"username": "Max H"}}, {"_id": "QFEL4PTztNkJDjCbk", "postedAt": "2023-08-27T21:02:01.912Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>If we succeed in being able to steer TAI at all (unlikely, <a href=\"https://www.lesswrong.com/posts/qrrEtrbLcmqr3b5uf/without-a-trajectory-change-the-development-of-agi-is-likely\">in my view</a>), animals will do fine - so will everyone else</p></blockquote><p>I'm not personally convinced fwiw; this line of reasoning has some plausibility but feels extremely out-of-line with approximately every reasonable reference class TAI could be in.</p>", "parentCommentId": "mfcanXMsGRgbkcJ4M", "user": {"username": "Linch"}}, {"_id": "sCtL6gPDk5M3YjGjF", "postedAt": "2023-08-27T21:22:45.644Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I think this post has some good points about overconfidence and over-deferral, but (as some others have pointed out) it seems unnecessarily inflammatory and includes jibes and rhetorical attacks&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum#What_we_discourage__and_may_delete_or_edit_out_\">I\u2019d rather not see on the EA Forum</a>. Examples have been pointed out by&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ZS9GDsBtWJMDEyFXh/eliezer-yudkowsky-is-frequently-confidently-egregiously?commentId=hQJKk4damq3DjswZj\">Max H here</a>:</p><blockquote><p>the language is also unnecessarily emotionally charged and inflammatory in many places. A quick sampling:</p></blockquote><blockquote><p>&gt; But as I grew older and learned more, I realized it was all bullshit.</p><p>&gt; it becomes clear that his view is a house of cards, built entirely on falsehoods and misrepresentations.</p><p>&gt; And I spend much more time listening to Yukowsky\u2019s followers spout nonsense than most other people.</p><p>&gt; (phrased in a maximally Eliezer like way): ... (condescending chuckle)</p></blockquote><p>I also think that you should retitle the post; I do not think that the contents defend the title to a reasonable extent, and therefore the title both feels misleading and somewhat like clickbait.</p><p>The moderators have decided to move the post to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Y8gkABpa9R6ktkhYt/forum-user-manual#Why_might_a_post_not_be_on_the_Frontpage__\">Personal Blog</a> \u2014 the connection to EA and doing good better is not that clear. I\u2019ll also discuss with the rest of the moderation team to see if there\u2019s anything else we should do about this post.</p>", "parentCommentId": null, "user": {"username": "Lizka"}}, {"_id": "YsEJzb3JQs2BjQ3kN", "postedAt": "2023-08-27T22:08:19.234Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I apologize for phrasing my comment in a way that made you feel like that. I certainly didn't mean to insinuate that rationalists lack \"agency and ability to think critically\" -- I actually think rationalists are better at this than almost any other group! I identify as a rationalist myself, have read much of the sequences, and have been influenced on many subjects by Eliezer's writings.</p><p>I think your critique that my writing gave the impression that my claims were all self-evident is quite fair. Even I don't believe that. Please allow me to enumerate my specific claims and their justifications:</p><ol><li>Caring about animal welfare is important (99% confidence): <a href=\"https://forum.effectivealtruism.org/posts/ZS9GDsBtWJMDEyFXh/eliezer-yudkowsky-is-frequently-confidently-egregiously?commentId=7s6jL3dMp4s8zdHvf\">Here's</a> the justification I wrote to niplav. Note that this confidence is greater than my confidence that animal suffering is real. This is because I think moral uncertainty means caring about animal welfare is still justified in most worlds where animals turn out not to suffer.</li><li>Rationalist culture is less animal-friendly than highly engaged EA culture (85% confidence): I think this claim is pretty evident, and it's corroborated <a href=\"https://forum.effectivealtruism.org/posts/FjSxNiNRqgFG6By8Q/\">here</a> by many disinterested parties.\"</li><li>Eliezer's views on animal welfare have had significant influence on views of animal welfare in rationalist culture\" (75% confidence):<ol><li>A fair critique is that sure, the sequences and HPMOR have had huge influence on rationalist culture, but the claim that Eliezer's views <i>in domains that have nothing do with rationality</i> (like animal welfare) have had outsize influence on rationalist culture is much less clear.</li><li>My only pushback is the experience I've had engaging with rationalists and reading LessWrong, where I've just seen rationalists reflecting Eliezer's views on many domains other than \"rationality: A-Z\" over and over again. This very much includes the view that animals lack consciousness. Sure, Eliezer isn't the only influential EA/rationalist who believes this, and he didn't originate that idea either. But I think that in the possible world where Eliezer was a staunch animal activist, rationalist discourse around animal welfare would look quite different.</li></ol></li><li>Rationalist culture has significant influence on those who could steer future TAI (80% confidence):<ol><li><a href=\"https://www.nytimes.com/2021/02/13/technology/slate-star-codex-rationalists.html\">NYT</a>: \"two of the world\u2019s prominent A.I. labs \u2014 organizations that are tackling some of the tech industry\u2019s most ambitious and potentially powerful projects \u2014 grew out of the Rationalist movement...Elon Musk \u2014 who also worried A.I. could destroy the world and met his partner, Grimes, because they shared an interest in a <a href=\"https://www.nytimes.com/2020/07/25/style/elon-musk-maureen-dowd.html\">Rationalist thought experiment</a> \u2014 founded OpenAI as a DeepMind competitor. Both labs hired from the Rationalist community.\"</li><li><a href=\"https://twitter.com/sama/status/1621621724507938816?lang=en\">Sam Altman</a>:\"certainly [Eliezer] got many of us interested in AGI, helped deepmind get funded at a time when AGI was extremely outside the overton window, was critical in the decision to start openai, etc\".</li></ol></li></ol><p>On whether aligned TAI would create a utopia for humans and animals, I think the <a href=\"https://longtermrisk.org/against-wishful-thinking/\">arguments</a> for <a href=\"https://reducing-suffering.org/utopia/\">pessimism</a>--<i>especially</i> about the prospects for animals--are serious enough that having TAI steerers care about animals is very important.</p>", "parentCommentId": "mfcanXMsGRgbkcJ4M", "user": {"username": "Ariel Simnegar"}}, {"_id": "tr7mbtM6A9ZxeJJQE", "postedAt": "2023-08-27T22:10:27.811Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Hi Eliezer. &nbsp;I actually do quite appreciate the reply because I think that if one writes a piece explaining why someone else is systematically in error, it's important that the other person can reply. That said . . .&nbsp;</p><p>You are misunderstanding the point about causal closure. &nbsp;If there was some isomorphic physical law, that resulted in the same physical states of affairs as is resulted in by consciousness, the physical would be causally closed. &nbsp;I didn't say that your description of what a zombie is was the misrepresentation. &nbsp;The point you misrepresented was when you said \"It is furthermore claimed that if zombies are \"possible\" (a term over which battles are still being fought), then, purely from our knowledge of this \"possibility\", we can deduce a priori that consciousness is extra-physical, in a sense to be described below; the standard term for this position is \"epiphenomenalism\".\"</p><p>No, the term is non-physicalism. &nbsp;This does not entail epiphenomenalism. &nbsp;If you say the standard term for believers in zombies is epiphenomenalists, then even if you have a convincing argument for why believers in zombies must be epiphenomenalists (which you don't) then it is still totally misleading to say the standard term is something totally different from what it is. &nbsp;I think I have a convincing argument for why Objective List Theorists should accept hypersensitivity--the idea that slight changes in well-being supervene on arbitrarily small changes in welfare goods--but it would be misleading to say \"the standard term for the belief in objective list theory is belief in hypersensitivity.\" &nbsp;</p><p>The quote you give from the SEP page is \"If zombies are to be counterexamples to physicalism, it is not enough for them to be behaviorally and functionally like normal human beings: plenty of physicalists accept that merely behavioral or functional duplicates of ourselves might lack qualia.\" &nbsp;But here behavior and function are about the external outputs of the thing--you could have a behavioral and functional duplicate of me made with silicon. &nbsp;However, it wouldn't be physically identical because it would be physically different--made of different stuff. &nbsp;That is the point that is being made. &nbsp;</p><p>If I am wrong why is it that Chalmers and the SEP page both deny that you have to be an epiphenomenalist to be a nonphysicalist?&nbsp;</p><p>You said \"It is furthermore claimed that if zombies are \"possible\" (a term over which battles are still being fought), then, purely from our knowledge of this \"possibility\", we can deduce a priori that consciousness is extra-physical, in a sense to be described below; the standard term for this position is \"epiphenomenalism\".</p><p>(For those unfamiliar with zombies, I emphasize that <i>this is not a strawman.</i>&nbsp; See, for example, <a href=\"http://plato.stanford.edu/entries/zombies/\">the SEP entry on Zombies</a>. \"</p><p>However, the SEP entry, as I note in the article, explicitly says that you do not have to be an epiphenomenalist to be a zombie believer. &nbsp;</p><p>\"True, the friends of zombies do not seem compelled to be epiphenomenalists or parallelists about the actual world. They may be interactionists, holding that our world is not physically closed, and that as a matter of actual fact nonphysical properties do have physical effects.\"</p><p>As for the final points sockpuppets, if a moderator would like to look into whether there are sockpuppet accounts, be my guest. &nbsp;I'd be willing to bet at 9.5 to .5 odds that if a moderator looked into it, they would not find lots of newly created accounts&nbsp;</p><p>I'd also be happy to bet about whether, if we ask a philosopher of mind like Chalmers, Goff, or Chappell which of us is correct about the zombie argument, they would say me!&nbsp;</p><p>Finally, you suggest that saying bad things about people before addressing the object level is bad conduct. &nbsp;Why? &nbsp;You never give a reason for this. &nbsp;It seems to me that if a post is arguing that some public figure should not be deferred to as much as he is currently being deferred to, on account of his frequent errors, there is nothing wrong with describing that that is your aim at the outset. &nbsp;</p>", "parentCommentId": "7fs5nHEkK6AGgPAJ9", "user": {"username": "Omnizoid"}}, {"_id": "TRgiQKiNgR5tKAwEA", "postedAt": "2023-08-27T22:13:31.751Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Yes, sorry I should have had it start in personal blog. &nbsp;I have now removed the incendiary phrasing that you highlight.</p>", "parentCommentId": "sCtL6gPDk5M3YjGjF", "user": {"username": "Omnizoid"}}, {"_id": "nFymBNFCNsuueEKNo", "postedAt": "2023-08-27T22:15:59.780Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I tried very hard to phrase everything as clearly as possible. &nbsp;But if people's takeaway is \"people who know about philosophy of mind and decision theory find Eliezer's views there deeply implausible and indicative of basic misunderstandings,\" then I don't think that's the end of the world. &nbsp;Of course, some would disagree.&nbsp;</p>", "parentCommentId": "Wp5jpBzsevRQfj9KN", "user": {"username": "Omnizoid"}}, {"_id": "qrCvrdiCdKXvfgEMq", "postedAt": "2023-08-27T22:54:12.619Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>animal consciousness is especially frustrating because of the real harm it's caused to rationalists' openness to caring about animal welfare.</p></blockquote><p><br>I think you might be greatly overestimating Eliezer's influence on this.</p><p>According to <a href=\"https://en.wikipedia.org/wiki/Ethics_of_eating_meat\">Wikipedia</a>: \"In a 2014 survey of 406 US philosophy professors, approximately 60% of ethicists and 45% of non-ethicist philosophers said it was at least somewhat \"morally bad\" to eat meat from mammals. A 2020 survey of 1812 published English-language philosophers found that 48% said it was permissible to eat animals in ordinary circumstances, while 45% said it was not.\"</p><p>It really does not surprise me that people who give great importance to rationality value animals much less than the median EA, given that non-human animals probably lack most kinds of advanced meta-level thinking and might plausibly not be \"aware of their own awareness\".</p><p>Even in EA, there are many great independent thinkers who are uncertain about whether animals should be members of the \"moral community\"</p><blockquote><p><a href=\"https://forum.effectivealtruism.org/s/HKybonRPkBdXMDxLY/p/ehZK259et52Xnvw5F#Aspiring_to_radical_empathy\">My own reflections and reasoning about philosophy of mind have, so far, seemed to indicate against the idea that e.g. chickens merit moral concern.</a></p></blockquote><p>&nbsp;</p><blockquote><p><a href=\"https://forum.effectivealtruism.org/posts/LqS7thgawhsBhCQHJ/why-i-m-not-vegan\">Now, I don't think animals matter as much as humans. I think there's a very large chance they don't matter at all, and that there's just no one inside to suffer, but to be safe I'll assume they do.</a></p></blockquote><p><br>I think that sometimes in EA we risk forgetting how fringe veganism is, and I don't think Yudkowsky's arguments on the importance of animal suffering influence a lot of the views in the rationalist community on the subject. Especially considering people at leading AI labs that might steer TAI, they seem to be very independent thinkers and often critical of Yudkowsky's arguments (otherwise they wouldn't be working at leading AI labs in the first place)</p>", "parentCommentId": "KRKf3w6ve54i5Ptc2", "user": {"username": "Lorenzo Buonanno"}}, {"_id": "BAtQgKunCLDNxN9oH", "postedAt": "2023-08-27T22:55:19.899Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>It does seem like a misjudgment, cuz the point of \"my friends are sucked into a charismatic cult leader\" doesn't necessarily have a lot to do with object level conclusions? It's about framing, the way attention is directed. An example of what I mean is \"believing true things is hard and evolution's spaghetti code is unusually bad at it\" is a frame (a characterization of an open problem), and you don't just throw it away when you say \"this particular study was very credulously believed because no one had tried replicating it by the time thinking fast and slow was published, but you should've smelled/predicted something was wrong back then\". If you're worried about overconfidence or overdeferrence amongs your friend group, it's pretty unrealistic for them to just take the wrong outputs at face value-- people correcting someone's mistakes is just the peer review process working as intended! If you really want to be concerned about this, you should show us that \"if you're starting from correcting his object level mistake, then you're not being maximally efficient or clear in your own pursuit of answers\". I think that would work!&nbsp;</p><p>Apparently some old school news anchor, like 1950s of some kind, said \"we don't tell people what to think. we tell them what to think about\". This seems obviously to me to be the true source of fraught cult leader stuff, if there is any!!!&nbsp;</p>", "parentCommentId": null, "user": {"username": "quinn"}}, {"_id": "2o72Nih4D7eZ8n4jg", "postedAt": "2023-08-27T23:18:40.933Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thank you. I don't have any strong objections to these claims, and I do think pessimism is justified. Though my guess is that a lot of people at places like OpenAI and DeepMind <i>do</i> care about animal welfare pretty strongly already. Separately, I think that it would be much better in expectation (for both humans and animals) if Eliezer's views on pretty much every other topic were <i>more</i> influential, rather than less, inside those places.</p><p>My negative reaction to your initial comment was mainly due to the way critiques (such as this post) of Eliezer are often framed, in which the claims \"Eliezer's views are overly influential\" and \"Eliezer's views are incorrect / harmful\" are combined into one big attack. I don't object to people making these claims in principle (though I think they're both wrong, in many cases), but when they are combined it requires more effort to separate and refute.<br><br>(Your comment wasn't a particularly bad example of this pattern, but it was short and crisp and I didn't have any other major objections to it, so I chose to express the way it made me feel on the expectation that it would be more likely to be heard and understood compared to making the point in more heated disagreements.)</p>", "parentCommentId": "YsEJzb3JQs2BjQ3kN", "user": {"username": "Max H"}}, {"_id": "aLEfaAZZ98m4uu9Ey", "postedAt": "2023-08-27T23:31:27.660Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I really appreciate this mod comment. I was seriously wracking my brain, trying to think of how to put into words exactly what was wrong with this post, and nothing I pulled out was anywhere near as concise or precise as this comment. This was done extremely well, something that I'm able to recognize after the fact but nowhere near able to do myself.</p><p>I still wish it had come sooner. It's also worth noting that this post made a really serious effort to optimize for maximizing damage to the reputation to at least one of the major Schelling points in the Rationality community (the Lesswrong Sequences), which many people in EA also use for upskilling in preparation for high-EV projects. It's not clear how many people read the arguments at the beginning and assumed they were true, rather than that the author had insulated themself from accountability. Rather than hosting bad-faith attacks on other communities for more than 20 hours and following up with heavy-handed actions, like moving to personal blog, that couldn't have been done sooner than ~20 hours, it would make more sense to take lighter actions sooner, such as just posting a superior-quality disclaimer warning very early on in the process (and, again, this comment nailed the situation in a really effective way). This will maximize the number of people who can make informed decisions about the content itself, rather than just go wherever it was designed to take them.</p><p>I'm not sure how that something like this would be, as I'm not a moderator and I don't know what the base rates are for moderator capability, as well as other factors that I'm not aware of; I know that this situation was ultimately handled well but possibly could have been done better.</p><p>Yudkowsky made a long reply, but it also contained this point:</p><blockquote><p>As the entire post violates basic rules of epistemic conduct by opening with a series of not-yet-supported personal attacks, I will not be responding to the rest in detail. &nbsp;I'm sad about how anything containing such an egregious violation of basic epistemic conduct got this upvoted, and wonder about sockpuppet accounts or alternatively a downfall of EA.</p></blockquote><p>Although forum cybersecurity is very important since <a href=\"https://forum.effectivealtruism.org/posts/L8kEmQgghxS9LXF3H/ea-is-underestimating-intelligence-agencies-and-this-is\">we don't know what kinds of people will be the adversaries in the future</a>, I don't think this is as much of an indictment or a dissappointment as Yudkowsky thinks. As a thought experiment, if EA were to grow by a factor of two each year, then that would mean, at any given time, a disproportionately large proportion of people in EA would have only had their first exposure to EA ideas less than 6 months ago. However, it would still mean that it is even more important to acknowledge that, at any given time, many newer users aren't yet ready to handle complex bad-faith arguments and mind games that inevitably end up getting cooked up by an intelligent disgruntled person here or there; this would happen even if EA is doing everything right, and continuously getting better every day, by expanding and bringing in many new people, which is already hard enough as is <a href=\"https://forum.effectivealtruism.org/posts/uBSwt2vEGm4RisLjf/holden-karnofsky-s-recent-comments-on-ftx#:~:text=Having%20seen%20the,better%20term%2C%20punchable%3F\">in the current ambiently-hostile environment</a>.</p>", "parentCommentId": "sCtL6gPDk5M3YjGjF", "user": {"username": "trevorw96"}}, {"_id": "5kQxrnHwakrgkdtno", "postedAt": "2023-08-27T23:32:38.312Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Re Chalmers agreeing with you, he would, he said as much in the LessWrong comments and I recently asked him in person and he confirmed it. In Yudkowsky\u2019s defense it is a very typical move among illusionists to argue that Zombiests can\u2019t really escape epiphenomenalism, not just some ignorant outsider\u2019s move (I think I recall Keith Frankish and Francois Kammerer both making arguments like this). That said I remain frustrated that the post hasn\u2019t been updated to clarify that Chalmers disagrees with this characterization of his position.</p>\n", "parentCommentId": "tr7mbtM6A9ZxeJJQE", "user": {"username": "Devin Kalish"}}, {"_id": "ymEtKbd5LGcwCQf3q", "postedAt": "2023-08-27T23:35:47.158Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>If I was trying to list central historical claims that Eliezer made which were controversial at the time I would start with things like:</p>\n<ul>\n<li>AGI is possible.</li>\n<li>AI alignment is the most important issue in the world.</li>\n<li>Alignment will not be easy.</li>\n<li>People will let AGIs out of the box.</li>\n</ul>\n", "parentCommentId": "ZebL23Jwt7F7qcemy", "user": {"username": "Larks"}}, {"_id": "MpSA9NLMcECHvCKkj", "postedAt": "2023-08-28T00:18:39.383Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Yes, there are some arguments of questionable efficacy for the conclusion that zombieism entails epiphenomenalism. &nbsp;But notably:</p><ol><li>Eliezer hasn't given any such argument.&nbsp;</li><li>Eliezer said that deniers of zombieism are by definition zombieists. &nbsp;That's just flatly false.&nbsp;</li></ol>", "parentCommentId": "5kQxrnHwakrgkdtno", "user": {"username": "Omnizoid"}}, {"_id": "s4zGJF4HqRFGcNmGP", "postedAt": "2023-08-28T00:23:12.525Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I'd disagree with the notion that \"this post made a really serious effort to optimize for maximizing damage to the reputation to at least one of the major Schelling points in the Rationality community.\" &nbsp;The thing I was optimizing for was getting people to be more skeptical about Eliezer's views, not ruining his career or reputation. &nbsp;In fact, as I said in the article, I think he often has interesting, clever, and unique insights and has made the world a better place.&nbsp;</p><p>See also my reply to Eliezer. &nbsp;In short, if you're writing a post arguing for why we should trust someone less, I don't know why you can't start out with the broad claim and then give the reasons. &nbsp;Eliezer doesn't defend that practice--he just asserts that it's basic rationality. &nbsp;</p>", "parentCommentId": "aLEfaAZZ98m4uu9Ey", "user": {"username": "Omnizoid"}}, {"_id": "txD7L2HFoz8gPccy8", "postedAt": "2023-08-28T01:42:09.839Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>For what it's worth, both Holden and Jeff express considerable moral uncertainty regarding animals, while Eliezer does not. Continuing Holden's quote:</p><blockquote><p>My own reflections and reasoning about philosophy of mind have, so far, seemed to indicate <i>against</i> the idea that e.g. chickens merit moral concern. And my intuitions value humans astronomically more. However, I <strong>don\u2019t think either my reflections or my intuitions are highly reliable</strong>, especially given that many thoughtful people disagree. And if chickens do indeed merit moral concern, <strong>the amount and extent of their mistreatment is staggering</strong>. With <a href=\"https://www.openphilanthropy.org/blog/worldview-diversification\"><u>worldview diversification</u></a> in mind, <strong>I don\u2019t want us to pass up the </strong><a href=\"https://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms\"><strong><u>potentially considerable opportunities to improve their welfare</u></strong><u>.</u></a></p><p>I think <strong>the uncertainty we have on this point warrants putting significant resources into </strong><a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\"><strong><u>farm animal welfare</u></strong></a>, as well as working to generally <strong>avoid language that implies that only humans are morally relevant</strong>.</p></blockquote><p>I agree with you that it's quite difficult to quantify how much Eliezer's views on animals have influenced the rationalist community and those who could steer TAI. However, I think it's significant--if Eliezer were a staunch animal activist, I think the discourse surrounding animal welfare in the rationalist community would be different. I elaborate upon why I think this in <a href=\"https://forum.effectivealtruism.org/posts/ZS9GDsBtWJMDEyFXh/eliezer-yudkowsky-is-frequently-confidently-egregiously?commentId=YsEJzb3JQs2BjQ3kN\">my reply to Max H</a>.</p>", "parentCommentId": "qrCvrdiCdKXvfgEMq", "user": {"username": "Ariel Simnegar"}}, {"_id": "YKHreu84m3RCDjKvg", "postedAt": "2023-08-28T04:25:08.092Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Philosophia has I think a publication rate decently below 50%. &nbsp;</p>", "parentCommentId": "GQaetmLC82hhPHRgW", "user": {"username": "Omnizoid"}}, {"_id": "5bN3chcMGcwybh5cZ", "postedAt": "2023-08-28T04:26:02.507Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Or a sign that knowing about philosophy decreases support for Rand.&nbsp;</p>", "parentCommentId": "SqHezrWArtCcpvvp2", "user": {"username": "Omnizoid"}}, {"_id": "WLRjizmJEbzG7Y9sr", "postedAt": "2023-08-28T04:41:11.315Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I don't think I really overgeneralized from limited data. &nbsp;Eliezer talks about tons of things, most of which I don't know about. &nbsp;I know a lot about maybe 6 things that he talks about and expresses strong views on. &nbsp;He is deeply wrong about at least four of them.&nbsp;</p>", "parentCommentId": "QhfjnCvwQypsJNhbP", "user": {"username": "Omnizoid"}}, {"_id": "vt5Hhg57jdEoddNrf", "postedAt": "2023-08-28T04:44:05.735Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I tend to think Hanson more reliably generates true beliefs than Eliezer.</p>", "parentCommentId": "GYFAxRfinBrF3Dxcc", "user": {"username": "Omnizoid"}}, {"_id": "BfdtycgyCqTjsJ25E", "postedAt": "2023-08-28T05:01:33.389Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>Physicalists think once you\u2019ve specified the way that matter behaves, that is sufficient to explain consciousness. Consciousness, just like tables and chairs, can be fully explained in terms of the behavior of physical things.</p><p>Non-physicalists think that the physicalists are wrong about this. Consciousness is its own separate thing that is not explainable just in terms of the way matter behaves. There are more niche views like idealism and panpsychism that we don\u2019t need to go into, which say that consciousness is either fundamental to all particles or the only thing that exists, so let\u2019s ignore them. The main view about consciousness is called dualism, according to which consciousness is non-physical and there are some psychophysical laws, that result in consciousness when there are particular physical arrangements.</p></blockquote><p>&nbsp;</p><p>This sort framing, which conflates&nbsp;</p><ul><li>broad metaphysical positions&nbsp;</li><li>theses about the nature of mental states&nbsp;</li><li>theses about the causal structure of consciousness</li></ul><p>is ironically an excellent example of LessWrong received wisdom leaking into the water supply. These are of course not unrelated topics, but they're not the same.&nbsp;</p><p>1.</p><p>Physicalism is the thesis that only the physical exists. It is an <i>extremely broad</i> class of theories, differentiated in large part (but not exclusively) by disputes over what counts as physical. The main alternatives are dualism and neutral monism (though this is arguably still physicalism). Idealism is deader than dead.&nbsp;</p><p><strong>Physicalism is not and does not entail illusionism</strong>.&nbsp;</p><p>Illusionism, aka eliminativism about consciousness, is <i>very</i> fringe and the vast majority of physicalists reject it.&nbsp;</p><p>2.</p><p>Dualism is not \"the main view\" of consciousness. A slim majority of philosophers are physicalists. &nbsp;</p><p>3.</p><p>Panpsychism is a thesis about what sort of physical systems have mental states (namely: all of them), not what mental states are or their causal structure. It is entirely compatible with both physicalism and property dualism. (And I suppose with substance dualism as well, though I'm not sure what would motivate that particular combination.)</p><p>4.&nbsp;</p><p>Dualism is not emergentism. On the contrary, emergentism is typically (though not always) a <i>physicalist</i> position - and the claim that emergence entails substance dualism is one of the main lines of argument against it!</p>", "parentCommentId": null, "user": {"username": "prisonpent"}}, {"_id": "nT3bBqfJLdHLM8eXC", "postedAt": "2023-08-28T05:27:30.042Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>his <a href=\"https://acyclicity\">first attempted refutation of the post</a>&nbsp;</p></blockquote><p>this link is broken</p>", "parentCommentId": "CCgyJ9kLAxLS8Ea4Q", "user": {"username": "prisonpent"}}, {"_id": "kLba8hvQ9jYMB4K7N", "postedAt": "2023-08-28T07:31:57.710Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Sorry, Pablo, I meant that I got a lot more epistemically humble, I should have thought about how I phrased it more. It was more that I went from the opinion that many worlds is probably true to: \"Oh man, there are some weird answers to the Wigner's friend thought experiment and I should not give a major weight to any.\" So I'm more like maybe 20% on many worlds?&nbsp;<br><br>That being said I am overconfident from time to time and it's fair to point that out from me as well. Maybe you were being overconfident in saying that I was overconfident? :D</p>", "parentCommentId": "SzbwRSsE3BfnAyyjf", "user": {"username": "Jonas Hallgren"}}, {"_id": "7w935tPT2gs8Dn5cZ", "postedAt": "2023-08-28T07:33:23.672Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I didn't mean it in this sense. I think the lesson you drew from it is fair in general, I was just reacting to the things I felt you pulled under the rug, if that makes sense.</p>", "parentCommentId": "WLRjizmJEbzG7Y9sr", "user": {"username": "Jonas Hallgren"}}, {"_id": "FHNp3eKvyCvxHjv2u", "postedAt": "2023-08-28T09:26:07.579Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I have mixed feelings about this mod intervention. On the one hand, I value the way that the moderator team (including Lizka) play a positive role in making the forum a productive place, and I can see how this intervention plays a role of this sort.</p><p>On the other hand:</p><ol><li>Minor point: I think Eliezer is often condescending and disrespectful, and I think it's unlikely that anyone is going to successfully police his tone. I think there's something a bit unfortunate about an asymmetry here.</li><li>More substantially: I think procedurally it's pretty bad that the moderator team act in ways that discourages criticism of influential figures in EA (and Eliezer is definitely such a figure). I think it's particularly bad to suggest concrete specific edits to critiques of prominent figures. I think there should probably be quite a high bar set before EA institutions (like forum moderators) discourage criticism of EA leaders (esp with a post like this that engages in quite a lot of substantive discussion, rather than mere name calling). (ETA: Likewise, with the choice to re-tag this as a personal blogpost, which substantially buries the criticism. Maybe this was the right call, maybe it wasn't, but it certainly seems like a call to be very careful with.)</li><li>I personally agree that Eliezer's overconfidence is dangerous, given that many people do take his views quite seriously (note this is purely a comment on his overconfidence; I think Eliezer has other qualities that are praiseworthy). I think that the way EA has helped to boost Eliezer's voice has, in this particular respect, plausibly caused harm. Against that backdrop, I think it's important that there be able to be robust pushback against this aspect of Eliezer.</li></ol><p>I don't know what the right balance is here, and maybe the mod team/Lizka have already found it. But this is far from clear to me.</p><p>(P.S. While I was typing this, I accidentally refreshed, and I was happy to discover that my text had been autosaved. It's a nice reminder of how much I appreciate the work of the entire forum team, including the moderators, to make using the forum a pleasant experience. So I really do want to emphasise that this isn't a criticism of the team, or Lizka in particular. It's an attempt to raise an issue that I think is worth reflection in terms of future mod action).</p>", "parentCommentId": "sCtL6gPDk5M3YjGjF", "user": null}, {"_id": "d5Yp6GvAe6E4cKBxS", "postedAt": "2023-08-28T09:41:40.486Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>The relevant principle of epistemic good conduct seems to me straightforward: if you've got to make personal attacks (and sometimes you do), make them <i>after </i>presenting your object-level points that support those personal attacks. &nbsp;This shouldn't be a difficult rule to follow, or follow much better than this; and violating it this hugely and explicitly is sufficiently bad news that people should've been wary about this post and hesitated to upvote it for that reason alone.</p></blockquote><p>This might well be a reasonable norm to follow, and it <i>might </i>well even be the type of norm that enlightened rational actors can converge on as good, but I think it's far from settled practice, and I don't think Omnizoid is defecting on established norms at least in this instance (in the way that e.g., doxxing or faking data is widely considered defecting in most internet discussions).</p>", "parentCommentId": "7fs5nHEkK6AGgPAJ9", "user": {"username": "Linch"}}, {"_id": "BSYiEGqbwFBWaLM2S", "postedAt": "2023-08-28T09:42:55.685Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>In the UK, where I did all of my philosophy education bar a brief trip to Australia for a couple of months, Rand is not a significant presence in the wider culture in any way, so she wouldn't naturally come up unless she already had credibility within academic philosophy. Though there were a lot of Americans around in Oxford obviously, and maybe they had read Rand. &nbsp;</p>", "parentCommentId": "SqHezrWArtCcpvvp2", "user": {"username": "Dr. David Mathers"}}, {"_id": "Jr2ne5oGZh3AkDFMb", "postedAt": "2023-08-28T09:44:54.125Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I agree that much of the language is inflammatory, and this is blameworthy. I disagree that the connection to EA and doing good better is unclear, conditional upon the writer being substantively correct. And historically, the personal blogpost/frontpage distinction has not been contingent on correctness. (But I understand you're operating under pretty difficult tradeoffs, need to move fast, etc, so wording might not be exact).&nbsp;</p>", "parentCommentId": "sCtL6gPDk5M3YjGjF", "user": {"username": "Linch"}}, {"_id": "tWCnHeZhq5qhPzdEh", "postedAt": "2023-08-28T09:47:10.626Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Yeah it seems pretty obvious to me that there are far worse things you could've said if you wanted to optimize for reputational damage, assuming above 75th percentile creativity and/or ruthlessness.&nbsp;</p>", "parentCommentId": "s4zGJF4HqRFGcNmGP", "user": {"username": "Linch"}}, {"_id": "MEu7W9HgJTz5Ess6H", "postedAt": "2023-08-28T09:48:30.666Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>'Chalmers, Goff, or Chappell' &nbsp;This is stacking the deck against Eliezer rather unfairly; none of these 3 are physicalists, even though physicalism is the plurality, and I think still slight majority position in the field: <a href=\"https://survey2020.philpeople.org/survey/results/4874\">https://survey2020.philpeople.org/survey/results/4874</a>&nbsp;</p>", "parentCommentId": "tr7mbtM6A9ZxeJJQE", "user": {"username": "Dr. David Mathers"}}, {"_id": "7vezS7ooFh2oeppjy", "postedAt": "2023-08-28T10:00:41.033Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>What do you mean by \"illusionism\"? I understand \"eliminativism\" where people say there is no such thing as (phenomenal) consciousness. But that is obviously incompatible with birds, mammals or humans(!) being (phenomenally) conscious. When I hear \"consciousness is an illusion\" in ordinary English, it sounds like them same claim: there's no such thing. But in fact, people mean something else, and I've never been quite sure what. Sometimes it seems just to be \"nothing shows up in perceptual phenomenology except external stuff, but people mistakenly believe that qualia are properties instantiated by the experience and show up in phenomenology\", but that makes all phenomenal externalists like Tye, Dretske, Mike Martin (etc.) \"illusionists\", which is not a way any of them has ever self-identified as far as I know.&nbsp;</p>", "parentCommentId": "DE8zkzXmFihbfw4pK", "user": {"username": "Dr. David Mathers"}}, {"_id": "cj9LzybyygjeDPinq", "postedAt": "2023-08-28T10:26:20.522Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I strongly disagree with the claim that the connection to EA and doing good is unclear. The EA community's beliefs about AI have been, and continue to be, strongly influenced by Eliezer. It's very pertinent if Eliezer is systematically wrong and overconfident about being wrong because, insofar as there's some level of defferal to Elizer on AI questions within the EA community which I think there clearly is, it implies that most EAs should reduce their credence in Elizer's AI views.&nbsp;</p>", "parentCommentId": "sCtL6gPDk5M3YjGjF", "user": {"username": "Nathan_Barnard"}}, {"_id": "ri2AvkHdmoPdfiaMw", "postedAt": "2023-08-28T11:08:30.846Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks, fixed.&nbsp;</p>", "parentCommentId": "nT3bBqfJLdHLM8eXC", "user": null}, {"_id": "xuLeao8mKNPADvs2P", "postedAt": "2023-08-28T11:32:30.101Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>This is not the subarea of consciousness research I am most expert in, and I am not a very good philosopher, but I have long had the suspicion that \"emergent\" doesn't really mean anything precise at all, but is just a term used by scientists who want to (possibly sensibly) avoid thinking about metaphysics. I mean, I'm sure you can find philosophers using it, but if I see a philosopher say it, I don't feel like I immediately know what they mean, whereas I do (at least roughly) with \"physicalism\" \"dualism\" \"panpsychism\" \"elminativism\".<br>&nbsp;</p>", "parentCommentId": "BfdtycgyCqTjsJ25E", "user": {"username": "Dr. David Mathers"}}, {"_id": "gntqmrfxzY2swmxDh", "postedAt": "2023-08-28T12:03:18.674Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Have you read much philosophy?<br><br>The criticisms you made of FDT don't seem like especially devastating criticisms (I admit I might be biased as someone who has argued for an FDT-like theory myself, though notably with important differences).<br><br>Admittedly, I'm coming from a perspective where sometimes in philosophy <i>every</i> position has quite strong counter-arguments.<br><br>So I guess I feel that you jumped a bit too quickly from \"strong counterarguments\" to \"conclusively debunked\". Like if you were writing a philosophy paper, &nbsp;you'd have to explain why common responses didn't hold water.</p><hr><p>Regarding physicalism, I've argued that Eliezer (and LW in general) haven't fully engaged with the arguments for the existence of <a href=\"https://www.lesswrong.com/posts/TniCuWCDxQeqFSxut/arguments-for-the-existence-of-qualia-1\">non-physical consciousness</a>. At the same time, I think Eliezer made a really strong (and well-argued) point that if we believe in epiphenomenalism then we have no reason to believe that our reports of consciousness have any connection to the phenomenon of consciousness. I haven't seen this point made so clearly elsewhere<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpls2x17mkn\"><sup><a href=\"#fnpls2x17mkn\">[1]</a></sup></span>. So even though I have my criticisms of him on this topic, I still find his writings here very impressive.</p><hr><p>On the topic of AI Risk, while I agree with the criticism that Eliezer is too pessimistic about the possibility of us solving this problem, I kind of feel that if you're being fair you've got to give him a <i>lot</i> of credit for taking existential risks from AI seriously when essentially no-one was arguing about it seriously, apart from maybe thinking that we'd cross that bridge when we get to it.</p><p>He's certainly made mistakes here (like not predicting the rise of generative AI), but seeing how much support existential risks from AI now has from credible academics including two <a href=\"https://www.safe.ai/statement-on-ai-risk\">Turing Prize winners</a> means that Eliezer is looking pretty good out of all of this, at least in my books. I mean, there isn't a scientific consensus on this issue yet, but regardless of how it resolves, I still think it was incredibly impressive for him to realise how strong the arguments were for this being something to worry about.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpls2x17mkn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpls2x17mkn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>That said, when I studied philosophy, I never took a philosophy of consciousness subject.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "quvovrFErGfBg9aBD", "postedAt": "2023-08-28T13:21:35.630Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>At the same time, I think Eliezer made a really strong (and well-argued) point that if we believe in epiphenomenalism then we have no reason to believe that our reports of consciousness have any connection to the phenomenon of consciousness. I haven't seen this point made so clearly elsewhere</p></blockquote><p>Chalmers <a href=\"https://consc.net/papers/nature.pdf\">here</a> says something like that (\u201cIt is certainly at least strange to suggest that consciousness plays no causal role in my utterances of \u2018I am conscious\u2019. Some have suggested more strongly that this rules out any <i>knowledge</i> of consciousness\u2026 The oddness of epiphenomenalism is exacerbated by the fact that the relationship between consciousness and reports about consciousness seems to be something of a lucky coincidence, on the epiphenomenalist view \u2026\u201d)</p><p>I liked how Eliezer made that argument more grounded and rigorous by presenting it in the context of Bayesian epistemology.</p><p>Also, that Chalmers section that I excerpted above is intermingled by a bunch of counterarguments, and Chalmers eventually says \u201cI think that there is no knockdown objection to epiphenomenalism here.\u201d I think Chalmers loses points for that\u2014I think the arguments are totally knockdown and the counterarguments are galaxy-brained copium, and I say kudos to Eliezer for just outright stating that.</p><p>I also think that getting the right answer in a controversial domain and stating it clearly is much more important and praiseworthy than being original (in this context). So even if Eliezer\u2019s point is already in the literature, I don\u2019t care. (The fact that you can\u2019t get academic publications and tenure from that activity is one of many big systematic problems with academia, IMO.)</p><p><i>(I am not a philosopher of consciousness.)</i></p>", "parentCommentId": "gntqmrfxzY2swmxDh", "user": {"username": "steve2152"}}, {"_id": "jRjvHwdCFkDcvfQPm", "postedAt": "2023-08-28T13:25:41.375Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Can you clarify the basis on which a post about an influential figure in the EA community that according to you makes some good points about overconfidence and over-deferral is not clearly connected to EA and doing good better? I genuinely cannot make sense of this decision or its stated justification.&nbsp;</p><p>Your comment only goes into specifics about the tone and rhetoric in parts of the post. Are these factors relevant to which section a post belongs to? If so, can you clarify how?&nbsp;</p>", "parentCommentId": "sCtL6gPDk5M3YjGjF", "user": null}, {"_id": "5FKr3wRYEBAcKJGBb", "postedAt": "2023-08-28T13:27:37.292Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>My sense of the consciousness literature is that it's a standard objection to dualism that it &nbsp;leads to epiphenomenalism (given reasonable assumptions about physics), and given epiphenomenalism, 'we have no reason to believe that our reports of consciousness have any connection to the phenomenon of consciousness', so dualism falsely implies that we don't know that we are conscious and must be wrong. Though it's a long time since I read this stuff. I think it's discussed in ch.5 of The Conscious Mind under \"The Paradox of Phenomenal Judgment\". I've just checked Chalmers' later 'The Character of Consciousness' and sc.10 of ch.5 describes this as a standard objection to epiphenomenalist forms of dualism (and bear in mind the original version of that chapter goes back to the mid-90s), though he doesn't actually cite any examples of people arguing this*. My sense is that ch.8 and 9 of Character of Consciousness are designed, among other things to address this sort of objection. My memory is that it's discussed in Jackson's original paper on the Knowledge Argument (the Mary the colour scientist in the black and white room thought experiment). Insofar as Eliezer thinks he has discovered something that \"philosophers\" have overlooked here, I think he's just wrong. But by the same token, the general complaint \"dualism entails epiphenomenalism, which is silly and self-undermining because it blocks knowledge of consciousness\" is not some wild out to lunch belief of Eliezer's; it's well within the phil. mind mainstream! (Note that this is compatible with the original post here being right that Eliezer's specific detailed reasoning on this topic is not good.)&nbsp;<br><br>What's actually going on is that (not all) philosophers regard this as such a decisive objection to epiphenomenalist varieties of dualism or dualism in general, that the latter have definitely been refuted aren't worth discussing (etc.) Though note that almost certainly some physicalists, maybe even many, &nbsp;<i>do</i> think this. But as long as there are still dualists who disagree, they will keep arguing about it in journals and treating dualists (relatively!) respectfully in published work: they have both selfish career incentives to do so (it makes for publications), but also, presumably they want to <i>demonstrate</i> the wrongness of dualism (ideally to dualists, certainly to bystanders.) There's not much point in them taking the \"this is like creationism or flat Earthism\" high ground, when 1 out of 3 of their colleagues are dualists, whatever the physicalists own personal inside view credence in dualism. (Though to be clear, there are no doubt many physicalists who think dualism is wrong but not *obviously* wrong to.)&nbsp;<br><br>Actually, insofar as Eliezer has said \"don't listen to silly philosophers, they don't even get the devastating objections to dualism that I outline here\", I actually think he has made the same mistake that you correctly diagnose Omnizoid as having made on FDT. As you say very often in philosophy <i>all</i> known views face objections that look, on their own devastating, but it's also not clear that there is a further, different alternative that doesn't face those objections**. &nbsp;In the clearest cases of this, logical paradoxes like the liar or the sorites, we can show to a fairly high standard of rigor that actually <i>something</i> that seems obvious and maybe even a \"conceptual truth\" must be false. So at least in those cases, the answer \"dumb philosophers have missed an alternative that avoids all problems\" is not very plausible. But in cases where things are murky and less rigorous there is always a temptation to take the objection that feels most convincing to you and go \"how could anyone possibly endorse view X, given objection Y\", even if all the known alternatives to X face big problems too. This should be resisted, but not many people seem to be very good at resisting it, not even professional philosophers.&nbsp;<br><br>*Maybe people said it all the time in conversation but it didn't really make it into print by the mid-90s, because pre-Chalmers few people defended dualism at length?&nbsp;<br><br>**In my <i>personal</i> view there often is such an alternative: namely \"there is no determinate fact of the matter who is right on this issue, because the meaning of our terms isn't precise enough to settle it\". But that's not usually what philosophers as a group think about most of the disputes.&nbsp;</p>", "parentCommentId": "gntqmrfxzY2swmxDh", "user": {"username": "Dr. David Mathers"}}, {"_id": "mZKox5kyprJQMMRvW", "postedAt": "2023-08-28T13:54:10.889Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I find this comment much more convincing than the top-level post.</p>\n", "parentCommentId": "CCgyJ9kLAxLS8Ea4Q", "user": {"username": "niplav"}}, {"_id": "ReCvmGTCgrqWBQsxz", "postedAt": "2023-08-28T14:10:20.153Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Some counterarguments are sufficiently strong that they are decisive. &nbsp;These are good examples. &nbsp;</p><p>I have read quite a lot of philosophy. &nbsp;Less than academics--I'm currently an undergrad--but my major is philosophy and it's my primary interest, such that I spend lots of time reading and writing about it.</p>", "parentCommentId": "gntqmrfxzY2swmxDh", "user": {"username": "Omnizoid"}}, {"_id": "HHoCC555L4aS8LjrL", "postedAt": "2023-08-28T14:16:58.030Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I strongly upvoted this article because I believe that it is incredibly important for EA to be able to point out the proverbial emperors lacking clothes. I would have preferred for the tone to be more reserved, but overall the OP has provided a large amount of citations, examples, arguments and evidence to back up their critique. &nbsp;It also matches with my experience of your writing on my area of expertise(physics), which features misleading distortions and excessive overconfidence.&nbsp;</p><p>I think as the founder of rationalism, you benefit from a large population of fans that are willing to give you the benefit of the doubt and be biased towards your positions, and for the intellectual health of the community this needs to be balanced with substantial, evidence-based skepticism and critique, a standard which I believe the OP has lived up to here.</p>", "parentCommentId": "7fs5nHEkK6AGgPAJ9", "user": {"username": "titotal"}}, {"_id": "3bkCPrJawQpkehQv2", "postedAt": "2023-08-28T14:21:54.204Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>You're response in the decision theory case was that there's no way that a rational agent could be in that epistemic state. &nbsp;But we can just stipulate it for the purpose of the hypothetical. &nbsp;</p><p>In addition, the scenario doesn't require absurdly low odds. &nbsp;Suppose that a demon has a 70% chance of creating people who will chop their legs off. &nbsp;You've been created and your actions will affect no one else. &nbsp;FDT implies that you have strong reason to chop your legs off even though it doesn't benefit you at all. &nbsp;</p>", "parentCommentId": "Fn27xofBMe4749C4i", "user": {"username": "Omnizoid"}}, {"_id": "HLS87WCEMXBRQHTWN", "postedAt": "2023-08-28T14:23:37.525Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Just want to say, I also agree that much of the original language was inflammatory. &nbsp;I think I have fixed it to make it less inflammatory, but do let me know if there are other parts that you think are inflammatory. &nbsp;</p>", "parentCommentId": "Jr2ne5oGZh3AkDFMb", "user": {"username": "Omnizoid"}}, {"_id": "8g4z3DJEAsg5Mho4Q", "postedAt": "2023-08-28T14:25:13.003Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>We could ask a physicalist too--Frankish, Richard Brown, etc.&nbsp;</p>", "parentCommentId": "MEu7W9HgJTz5Ess6H", "user": {"username": "Omnizoid"}}, {"_id": "my2WiR6NpQAMtvB36", "postedAt": "2023-08-28T14:45:46.927Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>You're response in the decision theory case was that there's no way that a rational agent could be in that epistemic state.<br>&nbsp;</p></blockquote><p>I did not say this.</p><blockquote><p>But we can just stipulate it for the purpose of the hypothetical.</p></blockquote><p><br>OK, in that case, the agent in the hypothetical should probably consider whether they are in a short-lived simulation.<br>&nbsp;</p><blockquote><p>FDT implies that you have strong reason to chop your legs off even though it doesn't benefit you at all. &nbsp;</p></blockquote><p><br>No, it <i>might</i> say that, depending on (among other things) what exactly it means to value your own existence.</p>", "parentCommentId": "3bkCPrJawQpkehQv2", "user": {"username": "Max H"}}, {"_id": "2LwYWDwsbfEjzikmM", "postedAt": "2023-08-28T15:16:37.134Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Disagreed, animal moral patienthood <a href=\"https://forum.effectivealtruism.org/s/HSA8wsaYiqdt4ouNF/p/XXLf6FmWujkxna3E6\">competes</a> with all the other possible interventions effective altruists could be doing, and does so symmetrically (the opportunity cost cuts in both directions!).</p>\n", "parentCommentId": "b9c9h6wzbRr4PHxuH", "user": {"username": "niplav"}}, {"_id": "E3EovXiZhCKbPqqfj", "postedAt": "2023-08-28T16:24:54.921Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Rather than denying consciousness per se, (strong) illusionists would deny that there\u2019s something like <i>phenomenal</i> consciousness, where that's defined (at least in part) in terms of qualitative properties, like the <i>quality</i> of reddishness in experiences or red, classic qualia (private, intrinsic, ineffable, and subjective, etc.) or even nonphysical properties. Humans and other animals can still be conscious, if understood in terms of the illusions of phenomenal/qualitative properties, either directly (actually having such illusions) or indirectly (would have these illusions, with the right additional machinery connected in the right way).</p><p>The hard problem of consciousness is typically defined as the problem of explaining why there's phenomenal consciousness or why consciousness has these phenomenal/qualitative properties. Illusionists (strong illusionists) believe this is misguided because there are no such phenomenal/qualitative properties, and we replace the hard problem with the problem of explaining <i>why (many) people believe</i> consciousness has these phenomenal/qualitative properties, despite <i>not</i> having them. I think <a href=\"https://philpapers.org/rec/FRAIAA-4\">Frankish, 2016</a> (<a href=\"https://keithfrankish.github.io/articles/Frankish_Illusionism%20as%20a%20theory%20of%20consciousness_eprint.pdf\">preprint</a>) is a standard reference. He also contrasts <i>weak</i> illusionism as denying classic qualia but not phenomenality per se, while strong illusionism also denies phenomenality:</p><blockquote><p>Weak illusionism holds that these properties are, in some sense, genuinely qualitative: there really are phenomenal properties, though it is an illusion to think they are ineffable, intrinsic, and so on. Strong illusionism, by contrast, denies that the properties to which introspection is sensitive are qualitative: it is an illusion to think there are phenomenal properties at all.</p></blockquote><p>I think illusionism about consciousness usually refers to strong illusionism.</p><p>I'm not familiar with the writing of Tye, Dretske, Mike Martin, but what you wrote suggests to me that they're weak illusionists and so deny classic qualia, but not strong illusionists, so don't deny phenomenality generally.</p><p>FWIW, I've seen Michael Graziano, Walter Veit and Heather Browning each self-describe as an illusionist (or something similar) and say they don't like the term and don't like to use it because it's misleading and confusing.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdxyo6tyc9q\"><sup><a href=\"#fndxyo6tyc9q\">[2]</a></sup></span>&nbsp;Illusionists are not saying there\u2019s no such thing as consciousness and are frequently misinterpreted that way, among other ways, like a Cartesian theatre. \"Consciousness illusion\" is also probably a confusing term for similar reasons, and something like \"illusion of phenomenality\" would be better.</p><p>&nbsp;</p><p>I'd also add that being an illusionist doesn't make experiences of red stop <i>seeming</i> to have qualitative features, so it seems to me that some such beliefs are \"wired-in\" and instinctual or intuitive, or, as <a href=\"https://doi.org/10.1007/s10670-019-00204-4\">Kammerer (2022)</a> puts it, <i>cognitively impenetrable</i>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref183x52vmix3\"><sup><a href=\"#fn183x52vmix3\">[1]</a></sup></span>&nbsp;You can't get rid of these illusions just by understanding that they are illusions or even how they work, just like you can't for <a href=\"https://en.wikipedia.org/wiki/M%C3%BCller-Lyer_illusion\">the M\u00fcller-Lyer illusion</a>, with which <a href=\"https://doi.org/10.1007/s10670-019-00204-4\">Kammerer (2022)</a> illustrates.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn183x52vmix3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref183x52vmix3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See also <a href=\"https://link.springer.com/referenceworkentry/10.1007/978-3-319-47829-6_1596-1\">Dawson, 2017</a> for cognitive impenetrability in general, not just in this context.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndxyo6tyc9q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdxyo6tyc9q\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://philpapers.org/rec/GRACE-3\">Graziano, 2016</a> (<a href=\"https://grazianolab.princeton.edu/sites/g/files/toruqf3411/files/graziano/files/jcs_graziano_2016.pdf\">ungated</a>) wrote:</p><blockquote><p>In the target article of this special issue, Frankish describes an approach to consciousness called illusionism that is shared by many theories of consciousness. The attention schema theory has much in common with illusionism. It clearly belongs to the same category of theory, and is especially close to the approach of Dennett (1991). But I confess that I baulk at the term \u2018illusionism\u2019 because I think it miscommunicates. To call consciousness an illusion risks confusion and unwarranted backlash. To me, consciousness is not an illusion but a useful caricature of something real and mechanistic. My argument here concerns the rhetorical power of the term, not the underlying concepts.</p></blockquote><p>It goes on further about resulting misunderstandings the term can cause.</p><p>&nbsp;</p><p><a href=\"https://doi.org/10.1080/21507740.2023.2188292\">Veit and Browning (2023)</a> (<a href=\"https://philsci-archive.pitt.edu/21817/1/Defending%20Sentientism%20Preprint.pdf\">preprint</a>) wrote, responding to some misunderstandings of illusionism:</p><blockquote><p>While we consider ourselves akin to illusionists, we do not typically use the term, since it invites just these kinds of confusions among those less familiar with the position.</p></blockquote></div></li></ol>", "parentCommentId": "7vezS7ooFh2oeppjy", "user": {"username": "MichaelStJules"}}, {"_id": "dfBsDz6XnxELJ7CsQ", "postedAt": "2023-08-28T17:27:53.937Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I agree. In case of interest: I have published a paper on exactly this question:&nbsp;https://link.springer.com/article/10.1007/s11229-022-03710-1</p><p>There, I argue that if illusionism/eliminativism is true, the question which animals are conscious can be reconstructed as question about particular kinds of non-phenomenal properties of experience. For what it\u2019s worth, Keith Frankish seems to agree with the argument and, I\u2019d say, Francois Kammerer does agree with the core claim (although we have disagreements about distinct but related issues).&nbsp;</p>", "parentCommentId": "E3EovXiZhCKbPqqfj", "user": {"username": "LeonardDung"}}, {"_id": "iyvpaLyPyq23Tngoq", "postedAt": "2023-08-28T17:31:38.413Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks. (Pleased to see most of this stuff postdates my DPhil and therefore it's less embarrassing I haven't read it!). I guess I feel I don't really have enough gasp on what phenomenal consciousness is, beyond definition by examples to feel like I entirely understand what is meant by \"there's consciousness, but not <i>phenomenal</i> consciousness\".&nbsp;</p>", "parentCommentId": "E3EovXiZhCKbPqqfj", "user": {"username": "Dr. David Mathers"}}, {"_id": "FEZMq8QnL7EKfFtSg", "postedAt": "2023-08-28T17:36:09.489Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p><strong>Apologies if this is derailing the thread by butting in with my thoughts r.e. illusionism. I'd love to find people to discuss these theories and share thoughts/notes outside of this thread. Please get in touch if interested :) maybe we could do a review and adversial/collaborative collaboration or something</strong></p><p>I will say as a former student of philosophy, and someone who likes reading philosophy a lot more than the median person (though perhaps less than the median EA!) I've never been able to get my head around illusionism. Like, I just really don't understand how many people (at least in the rationalist/EA space) don't seem to grok the Hard Problem. I really think one of the best arguments for 'qualia realism' or the belief that consciousness is a phenomenon demanding of an extra-physical explanation (or perhaps a more convincing one than current theories allow) is a 'Moorean' argument:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref66uufaae1k\"><sup><a href=\"#fn66uufaae1k\">[1]</a></sup></span></p><blockquote><ol><li>If illusionism is true then I am not conscious</li><li>I am conscious</li><li>Therefore, illusionism is false</li></ol></blockquote><p>I could try and throw in references and arguments but, if I'm being honest, like every person I do not have the time to re-evaluate each philosophy tradition and argument from scratch, and I find this form of argument very, very strong against the illusionist school, be that Frankish, Dennett, Hofstadter, and illusionist-inclined rationalists (though I'm actually not sure Yudkowsky is an illusionist here?).</p><p>Of course, the illusionists would say that the term is confusing, and that they're not eliminativists. They'd say that there's a difference between <i>consciousness1</i> - what we're all experiencing which they'd agree exists, and <i>consciousness2 </i>- which is the non-physical/mysterious/subjective/qualitative what-it-is-likeness. This is one of many things, I think, that causes debates on consciousness to often lead to people <a href=\"https://www.lesswrong.com/posts/NyiFLzSrkfkDW4S7o/why-it-s-so-hard-to-talk-about-consciousness\">talking past each other</a>.</p><p><strong>Some final thoughts to end (and as I said at the beginning, maybe to discuss on a different place and time):</strong></p><ol><li>I'd recommend trying Sam Harris' <i>Waking Up </i>app to explore what introspection tells you. I think it'd be better than many other approaches to meditation which might be a bit 'new-agey' for many EA/LW types. I think the experiences and insight I've had with meditation are much, much more convincing to me than what can come across as very confusing, unintuitive, and esoteric arguments in contemporary philosophy of mind.</li><li>Sam Harris particularly mentions Douglas Harding, and how Dennett and Hofstadter completely misunderstand his point. I'm 100% with Harding over Dennett and Hofstadter here. One of Harding's students, Richard Lang, has a 'Headless Way' course on <i>Waking Up </i>and I think it's brilliant.</li><li>In his <a href=\"https://80000hours.org/podcast/episodes/david-chalmers-nature-ethics-consciousness/\">great appearance</a> on the 80k podcast, Chalmers brings up an inconsistent triad for the illusionist-inclined to deal with:</li></ol><blockquote><p>I mean, you better not hold, number one, that consciousness is required for moral status; two, that consciousness is entirely an illusion; and that, three, some beings have moral status.</p></blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Though I suspect that this is again a definitional dispute on what we actually mean by the term 'consciousness'</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn66uufaae1k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref66uufaae1k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See the conclusion section in <a href=\"https://consc.net/papers/debunking.pdf\">this</a> recent Chalmers essay. Kammerer has responded <a href=\"https://www.francoiskammerer.com/wp-content/uploads/2022/04/KAMMERER-2022-How-can-you-be-so-sure-postprint.pdf\">here</a>, but I haven't read that yet</p></div></li></ol>", "parentCommentId": "E3EovXiZhCKbPqqfj", "user": {"username": "JWS"}}, {"_id": "jpwapoHhstoyBNjks", "postedAt": "2023-08-28T17:40:03.460Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I think people generally or often have <a href=\"https://en.wikipedia.org/wiki/What_Is_It_Like_to_Be_a_Bat%3F\">Nagel's what-it-is-likeness</a> in mind as the definition of phenomenal consciousness (or at least without classic qualia or nonphysical properties).</p><p>If I recall correctly, Frankish (<a href=\"https://keithfrankish.github.io/articles/Frankish_Quining%20diet%20qualia_eprint.pdf\">paper</a>, <a href=\"https://www.youtube.com/watch?v=zrk5TfwiY-s\">video</a>) called this 'diet qualia' and argued that attempts to define phenomenality in more specific terms generally reduce to either classic qualia or 'zero qualia' (I think purely functionalist terms, compatible with strong illusionism).</p>", "parentCommentId": "iyvpaLyPyq23Tngoq", "user": {"username": "MichaelStJules"}}, {"_id": "uSobdvhYWjb3PZCcB", "postedAt": "2023-08-28T17:50:10.204Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Sure, but even the Nagel thing is kind of a metaphor. I find it easy to class which mental states it does or doesn't apply to, but it's not something I can really characterize in other terms? I don't know, I've become less certain I know what all this terminology means the longer I've thought about it over the years.</p>", "parentCommentId": "jpwapoHhstoyBNjks", "user": {"username": "Dr. David Mathers"}}, {"_id": "iFwWsjt8mmt3yDATH", "postedAt": "2023-08-28T18:26:22.240Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I'm not sure how much you wanted to get into the object-level here, but I'll leave a few quick responses to a few points from the (strong) illusionist perspective (or what I understand it to be):</p><p>&nbsp;</p><blockquote><ol><li>If illusionism is true then I am not conscious</li><li>I am conscious</li><li>Therefore, illusionism is false</li></ol></blockquote><p>I assume this is supposed to refer to <i>phenomenal</i> consciousness specifically, not consciousness in general, because (strong) illusionists don't deny consciousness in general, and consciousness can be understood in different terms. And, it's worth noting that people have other illusions that we find hard to disabuse ourselves of on some level, like <a href=\"https://en.wikipedia.org/wiki/M%C3%BCller-Lyer_illusion\">the M\u00fcller-Lyer illusion</a>, with which <a href=\"https://doi.org/10.1007/s10670-019-00204-4\">Kammerer (2022b)</a> illustrates. It's <i>intuitively obvious</i> that one line is longer than the other, but <i>it's also false.</i> The same could be the case for phenomenality (assuming the definition doesn't collapse to one compatible with strong illusionism). <a href=\"https://link.springer.com/article/10.1007/s11098-022-01804-7\">Kammerer (2022a)</a> (which you linked to) describes other ways in which we are obviously conscious that are compatible with illusionism: functional and normative.</p><p>&nbsp;</p><blockquote><p>\"I mean, you better not hold, number one, that consciousness is required for moral status; two, that consciousness is entirely an illusion; and that, three, some beings have moral status.\"</p></blockquote><blockquote><p>Though I suspect that this is again a definitional dispute on what we actually mean by the term 'consciousness'</p></blockquote><p>I would say that this is largely definitional. Consciousness is not <i>entirely</i> an illusion according to strong illusionists; phenomenality (qualitativeness, what-it-is-likeness), classic qualia and dualism are illusions. You can just use the illusionist's conception of consciousness to ground moral status. This is the approach <a href=\"https://link.springer.com/article/10.1007/s11229-022-03710-1\"><u>Dung (2022)</u></a>, Frankish and Muehlhauser take, and a 'conservative' approach described in <a href=\"https://philarchive.org/rec/KAMTNC-2\">Kammerer, 2019</a>. That's also what I'd do, and I'd imagine the vast majority of strong illusionists would do.</p><p>That being said, I think (stance-independent) moral realism is false anyway (and did so before I became an illusionist), and strong illusionists probably have more reason to be moral antirealists of some kind than most, because similar or even the same debunking arguments would apply to both phenomenality and stance-independent moral claims, e.g. that pain is bad.</p>", "parentCommentId": "FEZMq8QnL7EKfFtSg", "user": {"username": "MichaelStJules"}}, {"_id": "EQ3qsuZGgQyP6FvoW", "postedAt": "2023-08-28T18:42:58.914Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>It means your preference ordering says that it's very good for you to be alive. &nbsp;</p><p>We can stipulate that you get decisive evidence that you're not in a simulation.&nbsp;</p>", "parentCommentId": "my2WiR6NpQAMtvB36", "user": {"username": "Omnizoid"}}, {"_id": "bZedmfpw9N5dwkanq", "postedAt": "2023-08-28T19:36:27.739Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>but is just a term used by scientists who want to (possibly sensibly) avoid thinking about metaphysics</p></blockquote><p>It's certainly that, but I don't think it's <i>just</i> that. I've seen at least one instance (though I can't remember where) of someone explicitly not-rejecting the possibility of natural laws that switch on, so to speak, above a certain scale.</p>", "parentCommentId": "xuLeao8mKNPADvs2P", "user": {"username": "prisonpent"}}, {"_id": "Z9sGz9w92in9dja2M", "postedAt": "2023-08-28T19:50:26.775Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I won't comment on the overall advisability of this piece, but I think you're confused about the decision theory (I'm about ten years behind state of the art here, and only barely understood it ten years ago, so I might be wrong).</p><p>The blackmail situation seems analogous to the Counterfactual Mugging, which was created to highlight how Eliezer's decision theories sometimes (my flippant summary) suggest you make locally bad decisions in order to benefit versions of you in different Everett branches. Schwartz objecting \"But look how locally bad this decision is!\" isn't telling Eliezer anything he doesn't already know, and isn't engaging with the reasoning. I think I would pay Omega in Counterfactual Mugging; I agree Schwartz's case is harder, but provisionally I think it unintentionally adds a layer of Pascal's Wager + torture vs. dust specks by making the numbers so extreme, which are two totally unrelated reasoning vortices.</p><p>I think the \"should you procreate to make your father procreate?\" question only works if your father's cognitive algorithms are perfectly correlated with yours, which no real father's are. To make the example fair, it should be more like \"You were created by Omega, a god who transcends time. It resolved to created you if and only if It predicted that you would procreate, and It is able to predict everything perfectly. Now should you procreate?\" I would also accept \"You were created by a clone of yourself in the exact same situation, down to the atom, that you find yourself in now, including worrying about being created by a clone of yourself and so on. Should you procreate?\" In both of these, the question seems much more open than with a normal human father.</p><p>If Eliezer's decision theories make no sense and are ignoring easy disproofs, then everyone else who finds them compelling (or at least not obviously wrong) after long study, including people like Wei Dai, Abram Demski, Scott Garrabrant, Benya Fallenstein, etc, is also bizarrely and inexplicably wrong. This is starting to sound less like \"Eliezer is a uniquely bad reasoner\" and more like \"there's something in the water supply here that makes extremely bright people with math PhDs make simple dumb mistakes that any rando can notice.\"</p>", "parentCommentId": null, "user": {"username": "Scott Alexander"}}, {"_id": "WCjYiN96ZrfdHgx3v", "postedAt": "2023-08-28T19:54:34.433Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>but it's not something I can really characterize in other terms?</p></blockquote><p>Well, that's the whole issue, isn't it? Qualia are the things that can't be fully characterized by their relations.</p>", "parentCommentId": "uSobdvhYWjb3PZCcB", "user": {"username": "prisonpent"}}, {"_id": "ri4xGEekZG9qLSBXB", "postedAt": "2023-08-28T20:11:29.073Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>If your action affects what happens in other Everett branches, such that there are actual, concretely existing people whose well-being is affected by your action to blackmail, then that is not relevantly like the case given by Schwarz. &nbsp;That case seems relevantly like the twin case, where I think there might be a way for a causal decision theorist to accomodate the intuition, but I am not sure. &nbsp;</p><p>We can reconstruct the case without torture vs dust specks reasoning, because that's plausibly a cofounder. &nbsp;Suppose a demon is likely to create people who will cut off their legs once they exist. &nbsp;Suppose being created by the demon is very good. &nbsp;Once you're created, do you have any reason to cut off your legs, assuming it doesn't benefit anyone else? &nbsp;No!&nbsp;</p><p>In the twin case, suppose that there are beings named Bob. &nbsp;Each being named Bob is almost identical to the last one--there choices are 99.9% correlated--and can endure great cost to create another Bob when he dies. &nbsp;It seems instrumentally irrational not to bare great costs. &nbsp;</p><p>I think it's plausible that most people are just not very good at generating true beliefs about philosophy, just as they're not good at generating true beliefs about physics. &nbsp;Philosophy is really fricking hard! &nbsp;So the phenomenon \"lots of smart people with a math background rather than a philosophy background hold implausible views about philosophy,\" isn't news. &nbsp;However, if someone claims to be the expert on physics, philosophy, decision theory, and AI, and then they turn out to be very confused about philosophy, then that is a mark against their reasoning abilities. &nbsp;</p><p>It's true that there is a separate interesting question about how so many smart people go so wrong about philosophy (note, I'd dispute the characterization that these are errors basic enough that a rando can figure them out--I think it wouldn't have been obvious to me what the errors were if it weren't for MacAskill and Schwarz who are very much non-randos). &nbsp;But the thing I intended to convey in this article is that, I know a lot about philosophy, such that I think I'm pretty good at assessing when people are totally wrong in the area of philosophy. &nbsp;This isn't true of many things. &nbsp;And the substantial majority of the time, when Eliezer has a controversial philosophical view, it turns out to be badly confused.&nbsp;</p>", "parentCommentId": "Z9sGz9w92in9dja2M", "user": {"username": "Omnizoid"}}, {"_id": "gcgQbp7yArGpF2FFh", "postedAt": "2023-08-28T20:13:35.231Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>And, it's worth noting that people have other illusions that we find hard to disabuse ourselves of on some level ... It's <i>intuitively obvious</i> that one line is longer than the other, but <i>it's also false.</i></p></blockquote><p>Sure, but that only establishes that \"it's intuitively obvious\" is not an infinitely strong reason for belief. It remains a strong one. To overcome the Moorean argument you need to provide arguments for illusionism which are stronger.</p>", "parentCommentId": "iFwWsjt8mmt3yDATH", "user": {"username": "prisonpent"}}, {"_id": "4tvPvWmCy9sBdiGLd", "postedAt": "2023-08-28T20:15:11.275Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>So then chop your legs off if you care about maximizing your total amount of experience of being alive across the multiverse (though maybe check that your measure of such experience is well-defined before doing so), or don't chop them off if you care about maximizing the fraction of high-quality subjective experience of being alive that you have.</p><p>This seems more like an <a href=\"https://www.lesswrong.com/tag/udassa\">anthropics</a> issue than a question where you need any kind of fancy decision theory though. It's probably better to start by understanding decision theory without examples that involve existence or not, since those introduce a bunch of weird complications about the nature of the multiverse and what it even means to exist (or fail to exist) in the first place.</p>", "parentCommentId": "EQ3qsuZGgQyP6FvoW", "user": {"username": "Max H"}}, {"_id": "FAefrpev7wGnGad2b", "postedAt": "2023-08-28T20:19:13.555Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Let's stipulate you have good evidence that you are the only being in the universe, and no one else will exist in the future. &nbsp;You don't care about what happens to anyone else.&nbsp;</p>", "parentCommentId": "4tvPvWmCy9sBdiGLd", "user": {"username": "Omnizoid"}}, {"_id": "KSFp9mjFe6jTEdkiF", "postedAt": "2023-08-28T20:58:12.408Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote>\n<p>This is starting to sound less like \"Eliezer is a uniquely bad reasoner\" and more like \"there's something in the water supply here that makes extremely bright people with math PhDs make simple dumb mistakes that any rando can notice.\"</p>\n</blockquote>\n<p>Independently of all the wild decision theory stuff, I don't think this is true at all. It's more akin to how for a few good years, people thought Mochizuki might have proven the ABC conjecture. It's not that he was right - just that he wrapped everything in so much new theory and terminology, that it took years for people to understand what he meant well enough to debunk him. He was still entirely wrong.</p>\n", "parentCommentId": "Z9sGz9w92in9dja2M", "user": {"username": "Guy Raveh"}}, {"_id": "5dbdhSyy95AvArNFT", "postedAt": "2023-08-28T21:10:17.641Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Based on this essay it seems like by \"completely wrong in embarrassing ways\" you mean that he's not knowledgeable about or respectful of what the local experts think. It's not like we know they are right on most of these questions.&nbsp;</p>", "parentCommentId": "ReESqqheJp4jYLBtH", "user": {"username": "Holly_Elmore"}}, {"_id": "qRdjyGWmELY49B8tT", "postedAt": "2023-08-28T21:12:15.798Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I don't think so. &nbsp;I argued in detail against each of Eliezer's views. &nbsp;I think I do know that Eliezer is wrong about zombies, decision theory, and animal consciousness. &nbsp;I didn't just point to what experts believe, I also explained why Eliezer is wrong.&nbsp;</p>", "parentCommentId": "5dbdhSyy95AvArNFT", "user": {"username": "Omnizoid"}}, {"_id": "SPrtJgiLRFZ9AkWpD", "postedAt": "2023-08-28T21:13:59.793Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>My read on what you meant by \"wrong about zombies\" was that he didn't understand what the field was claiming with the use of certain words and was dismissing a strawman.&nbsp;</p>", "parentCommentId": "qRdjyGWmELY49B8tT", "user": {"username": "Holly_Elmore"}}, {"_id": "ktBq47C4FkqD2CaLg", "postedAt": "2023-08-28T21:16:12.208Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>He didn't understand how the field was using a certain word. &nbsp;If a person uses words incorrectly, based on a misreading, and then interprets arguments as being obviously wrong based on their misinterpretation, they are making errors, not just failing to agree with the consensus view. &nbsp;</p>", "parentCommentId": "SPrtJgiLRFZ9AkWpD", "user": {"username": "Omnizoid"}}, {"_id": "3eeSiqpchpEksGXt7", "postedAt": "2023-08-28T21:24:41.793Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Misunderstanding someone else's claim doesn't strike me as an \"egregious error\". I don't feel he should have to understand the entirety of the academic view to have his own view. Although I agree he was mistaken to dismiss that view using words he had misunderstood.</p>", "parentCommentId": "ktBq47C4FkqD2CaLg", "user": {"username": "Holly_Elmore"}}, {"_id": "RBYN62g4a4wNJfrSx", "postedAt": "2023-08-28T21:35:14.049Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I've had some time to think about this post and it's reception both here and on LessWrong. There's a lot of discussion about the object-level claims and I don't think I have too much to say about adjudicating them above what's been said already, so I won't. Instead, I want to look at why this post is important at all.</p><p>&nbsp;</p><p><strong>1: Why does it matter if someone is wrong, frequently or egregiously?</strong></p><p>I think this post thinks that its thesis matters because of the reach of Eliezer's influence on the rationalist and EA communities. It certainly seems historically true given Eliezer's position as one of the key founders of the Rationalist movement, but I don't know how strong it is now, or how that question could be operationalised in a way where people could change their minds about it.</p><p>If you think Eliezer believes some set of beliefs <i><strong>X </strong></i>that are 'egregiously wrong' then it's probably worth writing separate posts about those issues rather than a hit piece. If you think that the issue is dangerous community epistemics surrounding Eliezer, then it'd probably be better if you focused on establishing that <i>before </i>bringing up the object level, or even bringing up the object level at all.</p><p>This has been a theme of quite a few posts recently (i.e. last year or so) on the Forum, but I think I'd like to see some more thoughts explaining what people mean by 'deference' or 'epistemic norms', and ideally some more concrete evidence about them being good or bad beyond personal anecdotes/vibes at an EAG.</p><p><strong>2: Did it need to be said in this way?</strong></p><p>Ironically, a lot of what Omnizoid critcises Eliezer for is stuff I find myself thinking about Omnizoid takes some of the times! I definitely think this post could have had a better light-to-heat ratio if it was worded and structured differently, and I think it's to your credit Omni that you recongised this, but bad that you posted it in its original state on both Forums.</p><p><strong>3: Why is Eliezer so confident?</strong></p><p>I've never met Eliezer or interacted in the same social circles, so I don't know to what extent personality figures into it. I Eliezer most clearly argues <i>for </i>this approach in his book <i>Inadequate Equilibria</i> he argues against what he calls 'modest epistemology' (see <a href=\"https://equilibriabook.com/against-modest-epistemology/\">Chapter 6</a>), I think he'd rather believe strongly and update accordingly<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefr3y9jnk2or\"><sup><a href=\"#fnr3y9jnk2or\">[1]</a></sup></span>&nbsp;if proven wrong than slowly approach the right belief via gradient descent. This would explain why he's confident when he's both right and both wrong.</p><p><strong>4: Why is the EA/LW reaction so different?</strong></p><p>So it's reaction is definitely more 'positive' on the EA Forum than LessWrong. But I wouldn't say it's 'positive' (57 karma, 106 votes, 116 comments) at time of writing. That's decidedly 'mixed' at best, so I don't think you can view this post as 'EA Forum says yeah screw Eliezer' and LessWrong says 'boo stupid EA Forum', I think both community's views are more nuanced than that.</p><p>I do get a sense that while many on LW disagree with Eliezer on a lot, everyone there respects him and his accomplishments, whereas there is an EAF contigent that really doesn't like Eliezer or his positions/vibes, and are happy to see him 'taken down a peg or two'. I think there's a section of LW that doesn't like this section of EA, hence Eliezer's claim about the 'downfall of EA' in his response.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn8inystbgw\"><sup><a href=\"#fnn8inystbgw\">[2]</a></sup></span></p><p><strong>5: An attempted innoculation?</strong></p><p>I think this is again related to the perception of outsiders of EA. Lots of our critics, fairly or unfairly, hone in on Eliezer and his views that are more confident/outside the overton window and run wit those to tarnish both EA and rationalism. Maybe this post is attempting to show internally and externally that Eliezer isn't/shouldn't be highly respected in the community to innoculate against this criticism, but I'm not sure it does that well.</p><p>&nbsp;</p><p>I think having this meta-level discussion about what discussion <i>we're actually having</i>, or want to have, helps move us in the light-not-heat direction. All in all, I think the better discussion is to try and find measure on Eliezer's. I think my main point is 1) - I think there are some good object-level discussions, and it's worth being wary of the confidence the community places in primary figures, but on balance on reflection I don't think this was the right way to go about it.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnr3y9jnk2or\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefr3y9jnk2or\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Again, not making a claim on whether he does or not</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn8inystbgw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn8inystbgw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Epistemic status - reading vibes, very unconfident</p></div></li></ol>", "parentCommentId": null, "user": {"username": "JWS"}}, {"_id": "szCpHNFyMp6LvpKCb", "postedAt": "2023-08-28T21:36:39.120Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>If you claim to be justified in having a near zero credence in some view, and the reason for that is because you don't know what words mean that are totally standard among people who are informed about the subject matter, and then you go on to dismiss those who are informed who disagree with you, that seems pretty eggregious.&nbsp;</p>", "parentCommentId": "3eeSiqpchpEksGXt7", "user": {"username": "Omnizoid"}}, {"_id": "gqcF3pas2kHChwMDr", "postedAt": "2023-08-28T21:42:25.008Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks for this comment. &nbsp;I agree with 2. &nbsp;On 3, it seems flatly irrational to have super high credences when experts disagree with you and you do not have any special insights.</p><p>If an influential person who is given lots of deference is often wrong, that seems notable. &nbsp;If people were largely influenced by my blog, and I was often full of shit, expressing confident views on things I didn't know about, that would be noteworthy. &nbsp;</p><p>Agree with 4.&nbsp;</p><p>On 5, I wasn't intending to criticize EA or rationalism. &nbsp;I'm a bit lukewarm on rationalism, but enthusiastically pro EA, and have, in fact, written lengthy responses to many of the critics of EA. &nbsp;Really my aim was to show that Eliezer is worthy of much less deference then he currently is given, and to argue the object level--that many of his view, commonly believed in the community, are badly mistaken.&nbsp;</p>", "parentCommentId": "RBYN62g4a4wNJfrSx", "user": {"username": "Omnizoid"}}, {"_id": "C94KkSfLkjo7JR9Z5", "postedAt": "2023-08-28T22:04:50.346Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>But all three parts of this \"takedown\" are about questions of philosophy / metaphysics? &nbsp;How do you suggest that I \"follow the actual evidence\" and avoid \"first principles reasoning\" when we are trying to learn about the nature of consciousness or the optimal way to make decisions??</p>", "parentCommentId": "hx5LkfNPcA9DiYzTk", "user": {"username": "Jackson Wagner"}}, {"_id": "RR9RSokXPojgbx2ZD", "postedAt": "2023-08-28T22:06:56.666Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I don't want to get into a long back-and-forth here, but for the record I still think you're misunderstanding what I flippantly described as \"other Everett branches\" and missing the entire motivation behind Counterfactual Mugging. It is definitely not supposed to directly make sense in the exact situation you're in. I think this is part of why a variant of it is called \"updateless\", because it makes a principled refusal to update on which world you find yourself in in order to (more flippant not-quite-right description) program the type of AIs that would weird games played against omniscient entities.</p><p>If the demon would only create me conditional on me cutting off my legs after I existed, and it was the specific class of omniscient entity that FDT is motivated by winning games with, then I would endorse cutting off my legs in that situation.&nbsp;</p><p>(as a not-exactly-right-but-maybe-helpful intuition pump, consider that if the demon isn't omniscient - but simply reads the EA Forum - or more strictly can predict the text that will appear on the EA Forum years in the future - it would now plan to create me but not you, and I with my decision theory would be better off than you with yours. And surely omniscience is a stronger case than just reads-the-EA-Forum!)</p><p>If this sounds completely stupid to you, and you haven't yet read the LW posts on Counterfactual Mugging. I would recommend starting there; otherwise, consider finding a competent and motivated FDT proponent (ie not me) and trying to do some kind of double-crux or debate with them, I'd be interested in seeing the results.</p>", "parentCommentId": "ri4xGEekZG9qLSBXB", "user": {"username": "Scott Alexander"}}, {"_id": "GncnibxF9GPaCzfdr", "postedAt": "2023-08-28T22:07:28.752Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I agree, and I think your point applies equally well to the original Eliezer Zombie discussion, as to this very post. &nbsp;In both cases, trying to extrapolate from \"I totally disagree with this person on [some metaphysical philosophical questions]\" to \"these people are idiots who are wrong all the time, even on more practical questions\", seems pretty tenuous.</p>", "parentCommentId": "rpKZMuabqfccB9DNC", "user": {"username": "Jackson Wagner"}}, {"_id": "kbCrExAzbJpFvSoao", "postedAt": "2023-08-28T22:07:54.004Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Were there bright people who said they had checked his work, understood it, agreed with him, and were trying to build on it? Or just people who weren't yet sure he was wrong?</p>", "parentCommentId": "KSFp9mjFe6jTEdkiF", "user": {"username": "Scott Alexander"}}, {"_id": "PK5RGXK7GJwys2DFk", "postedAt": "2023-08-28T22:15:00.494Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I suggest maybe re-titling this post to:<br>\"I strongly disagree with Eliezer Yudkowsky about the philosophy of consciousness and decision theory, and so do lots of other academic philosophers\"<br><br>or maybe:<br>\"Eliezer Yudkowsky is Frequently, Confidently, Egregiously Wrong, About Metaphysics\"<br><br>or consider:<br>\"Eliezer's ideas about Zombies, Decision Theory, and Animal Consciousness, seem crazy\"<br><br>Otherwise it seems pretty misleading / clickbaity (and indeed overconfident) to extrapolate from these beliefs, to other notable beliefs of Eliezer's -- such as cryonics, quantum mechanics, macroeconomics, various political issues, various beliefs about AI of course, etc. &nbsp;Personally, I clicked on this post really expecting to see a bunch of stuff like \"in March 2022 Eliezer confidently claimed that the government of Russia would collapse within 90 days, and it did not\", or \"Eliezer said for years that X approach to AI couldn't possibly scale, but then it did\".</p><p>Personally, I feel that beliefs within this narrow slice of philosophy topics are unlikely to correlate to being \"egregiously wrong\" in other fields. &nbsp;(Philosophy is famously hard!! &nbsp;So even though I agree with you that his stance on animal consciousness seems pretty crazy, I don't really hold this kind of philosophical disagreement against people when they make predictions about, eg, current events.)</p>", "parentCommentId": null, "user": {"username": "Jackson Wagner"}}, {"_id": "TjRpXPHRboiTgxLR4", "postedAt": "2023-08-28T22:24:36.764Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Fair. I think the stronger arguments for (strong) illusionism are of the following form:</p>\n<ol>\n<li>Physicalism seems true and dualism (including property dualism and epiphenomenalism) false for various reasons.</li>\n<li>No other (physicalist) theory besides strong illusionism seems able to address the meta-problem of consciousness or even on the right path.</li>\n<li>No theory has an adequate solution to the hard problem of consciousness and some debates between them seem empirically unresolvable (e.g. where the line is between report(ability)/access and phenomenal consciousness), but every theory other than strong illusionism needs to solve it.</li>\n<li>There are specific illusionist explanations of some posited phenomenal or classic qualia properties.</li>\n<li>There don't seem to be any strong arguments against illusionism (other than possibly mere intuition that phenomenal consciousness is real).</li>\n</ol>\n<p>To be clear, I'm leaving out all of the details, none of the above is obvious, and most or all of it is controversial. I think part of 3 isn't controversial (no full solution yet, and non-illusionist theories need it).</p>\n<p>On 4, ineffability and privacy seem easy to explain. First, we don't today know enough of the details of how our brains make the discriminations they do, so we can't fully communicate or compare them in practice yet anyway. Second, even if I understood and could communicate how my brain makes the discriminations it does, this doesn't allow you to put yourself in the same brain states or generally make the same discriminations in the same way. You could potentially build an AI that could or modify your brain accordingly, but this hasn't been possible yet, and it wouldn't really be \"you\" making those discriminations. I can't subject you to my illusions just by explanation, so ineffability is true in practice. With a full enough description, we could compare and privacy wouldn't hold.</p>\n<p>Also, I don't take \"intuitively obvious\" to be a strong reason for belief, but I am unusually skeptical.</p>\n", "parentCommentId": "gcgQbp7yArGpF2FFh", "user": {"username": "MichaelStJules"}}, {"_id": "5imvKfxMrviaoHf9Z", "postedAt": "2023-08-28T22:30:20.237Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I'm sympathetic to that. I just also get a whiff of \"it's my group's prerogative to talk about this and he didn't pay proper deference\". As a point of comparison, I'm sympathetic to theologians who thought the new atheists were total yokels who didn't understand any of the subtleties of their religions and their arguments, because they often didn't. But I also think the new atheists were more right and I don't think it would have been a good use of time for them to understand more. I'm not trying to be insulting to academic philosophy but rather insist that the world of these topics doesn't need to revolve around it.<br><br>Eliezer was wrong to mischaracterize other people's views. But I don't think he was especially wrong for not knowing what the academic landscape was on a topic before opining on it himself.</p>", "parentCommentId": "szCpHNFyMp6LvpKCb", "user": {"username": "Holly_Elmore"}}, {"_id": "f2hnZGC9QHQAoq6Qa", "postedAt": "2023-08-28T22:59:01.089Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>To be fair to the OP, I don't think that he was saying you should not consider the views of Yudkowsky- in fact he admits that Yudkowsky has some great thoughts and that he is an innovator.<br><br>OP observes that he himself for a long time <i>reflexively deferred</i> to Yudkowksy. I think his objective with his post was to point out some questions on which he thought Yudkowsky was &nbsp;pretty clearly wrong (although it is not clear that he accomplished this). His goal was not to urge people not to read or consider Yudkowsky, but rather to urge people not to reflexively defer to him.</p>", "parentCommentId": "GncnibxF9GPaCzfdr", "user": {"username": "Brad West"}}, {"_id": "DN4dDkkRdqXZxFEsP", "postedAt": "2023-08-28T23:53:11.677Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>OK. Simultaneously believing that and believing the truth of the original setup seems dangerously close to believing a contradiction.</p><p>But anyway, you don't really need all those stipulations to decide not to chop your legs off; just don't do that if you value your legs. (You also don't need FDT to see that you should defect against CooperateBot in a prisoner's dilemma, though of course FDT will give the same answer.)<br><br>A couple of general points to keep in mind when dealing with thought experiments that involve thorny or exotic questions of (non-)existence:</p><ul><li>\"Entities that don't exist don't care that they don't exist\" is a vacuously true, for most ordinary definitions of non-existence. If you fail to exist as a result of your decision process, that's generally not a problem for you, unless you also have unusual preferences over or beliefs about the precise nature of existence and non-existence.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref21yf41kyisl\"><sup><a href=\"#fn21yf41kyisl\">[1]</a></sup></span></li><li>If you make the universe inconsistent as a result of your decision process, that's also not a problem for you (or for your decision process). Though it may be a problem for the universe creator, which in the case of a thought experiment could be said to be the author of that thought experiment.&nbsp;<br><br>An even simpler view is that logically inconsistent universes don't actually exist at all - what would it even mean for there to be a universe (or even a thought experiment) in which, say, 1 + 2 = 4? Though if you accepted the simpler view, you'd probably also be a physicalist.</li></ul><p><br>I continue to advise you to avoid confidently pontificating on decision theory thought experiments that directly involve non-existence, until you are more practiced at applying them correctly in ordinary situations.<br><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn21yf41kyisl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref21yf41kyisl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>e.g. unless you're Carissa Sevar</p></div></li></ol>", "parentCommentId": "FAefrpev7wGnGad2b", "user": {"username": "Max H"}}, {"_id": "rtskCg7Rnh6Q56khM", "postedAt": "2023-08-29T01:00:44.332Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>&gt;that makes extremely bright people with math PhDs make simple dumb mistakes that any rando can notice</p><p>Bright math PhDs that have already been selected for largely buying into Eliezer's philosophy/worldview, which changes how you should view this evidence. Personally I don't think FDT is wrong as much as just talking past the other theories and being confused about that, and that's a much more subtle mistake that very smart math PhDs could very understandably make</p>", "parentCommentId": "Z9sGz9w92in9dja2M", "user": {"username": "keith_wynroe"}}, {"_id": "LhmPWraHoyCiSr4Eo", "postedAt": "2023-08-29T01:10:31.552Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>The coherence theorem part seems particularly egregious to me given how load-bearing it seems to be to a lot of his major claims. A frustration I have personally is that he seems to claim a lot that no one ever comes to him with good object-level objections to his arguments, but then when they do like in that thread he just refuses to engage&nbsp;</p>", "parentCommentId": "CCgyJ9kLAxLS8Ea4Q", "user": {"username": "keith_wynroe"}}, {"_id": "jdAYKeyj6jBzWKxkp", "postedAt": "2023-08-29T01:25:01.548Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>In your shoes, I'd remove \"egregiously\" from the title, but I'm not great at titles and also occupy a different epistemic status than you (eg I think FDT is better than CDT or EDT).</p>", "parentCommentId": "HLS87WCEMXBRQHTWN", "user": {"username": "Linch"}}, {"_id": "mvPo6mMxfnNLo9Cdh", "postedAt": "2023-08-29T01:47:20.043Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I should have been clearer.</p>\n<p>I wasn\u2019t claiming Eliezer\u2019s point was original, just especially clear and well-argued.</p>\n<p>Then again, maybe if I looked into the literature I\u2019d find others making this point in an even clearer and better argued fashion.</p>\n", "parentCommentId": "5FKr3wRYEBAcKJGBb", "user": {"username": "casebash"}}, {"_id": "tAofLDGbKr5Wa6Zk9", "postedAt": "2023-08-29T01:59:10.265Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>FDT isn\u2019t cherry-picked as Eliezer has described himself as a decision theorist and his main contribution is TDT (which latter developed into FDT).</p>\n", "parentCommentId": "7EsaR47SqFrbyeGDn", "user": {"username": "casebash"}}, {"_id": "ctCNn8EBHhZ4FJiRh", "postedAt": "2023-08-29T03:37:57.943Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Oh sorry, yeah I misunderstood what point you were making. &nbsp;I agree that you want to be the type of agent who cuts off their legs--you become better off in expectation. &nbsp;But the mere fact that the type of agent who does A rather than B gets more utility on average does not mean that you should necessarily do A rather than B. &nbsp;If you know you are in a situation where doing A is guaranteed to get you less utility than B, you should do B. &nbsp;The question of which agent you should want to be is not the same as which agent is acting rationally. &nbsp;I agree with MacAskill's suggestion that FDT is the result of conflating what type of agent to be with what actions are rational. &nbsp;FDT is close to the right answer for the second and a crazy answer for the first imo. &nbsp;</p><p>Happy to debate someone about FDT. &nbsp;I'll make a post on LessWrong about it. &nbsp;</p><p>One other point, I know that this will sound like a cop-out, but I think that the FDT stuff is the weakest example in the post. &nbsp;I am maybe 95% confident that FDT is wrong, while 99.9% confident that Eliezer's response to zombies fails and 99.9% confident that he's overconfident about animal consciousness.</p>", "parentCommentId": "RR9RSokXPojgbx2ZD", "user": {"username": "Omnizoid"}}, {"_id": "HKeeEtkhnDzTStbxD", "postedAt": "2023-08-29T04:01:40.289Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Well put! &nbsp;Though one nitpick: I didn't defer to Eliezer much. &nbsp;Instead, I concluded that he was honestly summarizing the position. &nbsp;So I assumed physicalism was true because I assumed, wrongly, that he was correctly summarizing the zombie argument.&nbsp;</p>", "parentCommentId": "f2hnZGC9QHQAoq6Qa", "user": {"username": "Omnizoid"}}, {"_id": "7hnKjCfabBdFxLmiQ", "postedAt": "2023-08-29T05:34:32.827Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Sorry if I misunderstood your point. I agree this is the strongest objection against FDT. I think there is some sense in which I can become the kind of agent who cuts off their legs (ie by choosing to cut off my legs), but I admit this is poorly specified.</p><p>I think there's a stronger case for, right now, having heard about FDT for the first time, deciding I will follow FDT in the future. Various gods and demons can observe this and condition on my decision, so when the actual future comes around, they will treat me as an FDT-following agent rather than a non-FDT-following agent. Even though future-created-me isn't exactly in a position to influence the (long-since gone) demon, current me is in a position to make this decision for future relevant situations, and should decide to follow FDT in general. Part of this decision I've made involves being the kind of person who would take the FDT option in hypothetical scenarios.</p><p>Then there's the additional question of whether to defect against the demons/gods later, and say \"Haha, back in August 2023 I resolved to become an FDT agent, and I fooled you into believing me, but now that I've been created I'm just going to not cut off my legs after all\". I think of this as - suppose every past being created by the demon has cut off its legs, ie the demon has a 100% predictive success rate over millions of cases. So the demon would surely predict if I would do this. That means I should (now) try really hard not to do this. Cf. Parfit's Hitchhiker. Can I bind my future self like this? I think empirically yes - I think I have enough honor that if I tell hypothetical demon gods now that I'm going to do various things, I can actually do them when the time comes. This will be \"irrational\" in some sense, but I'll still end up with more utility than everyone else.&nbsp;</p><p>Is there some sense in which, if I decide not to cut off my legs, I would wink out of existence? I admit feeling a superstitious temptation to believe this (a non-superstitious justification might be wondering if I'm the real me, or a version of me in the omniscient demon's simulation to predict what I would do). I think the literal answer is no but that it's practically useful to keep my superstitious belief in this to allow myself to do the irrational thing that gets me more utility. But this is a weird enough sidetrack that I'm really not sure I'm still in normal Eliezer-approved-decision-theory-land at all.</p><p>I think an easier question is whether you should program an AI to always keep its pre-emptive bargains with gods and demons; here the answer is just straightforwardly yes. You don't have to assume that your actions alter your algorithm, you can just alter the algorithm directly. I think this is what Eliezer is most interested in, though I'm not sure.</p>", "parentCommentId": "ctCNn8EBHhZ4FJiRh", "user": {"username": "Scott Alexander"}}, {"_id": "8mJNYXJD5ZaAk7Hdc", "postedAt": "2023-08-29T06:52:24.565Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>reposting a reply by Omnizoid from Lesswrong:<br><br>\"Philosophy is pretty much the only subject that I'm very informed about. &nbsp;So as a consequence, I can confidently say Eliezer is eggregiously wrong about most of the controversial views I can fact check him on. &nbsp;That's . . . worrying.\"<br><br>And my reply to that:<br><br>Some other potentially controversial views that a philosopher might be able to fact-check Eliezer on, based on skimming through <a href=\"https://www.lesswrong.com/rationality\">an index of the sequences</a>:</p><ul><li>Assorted confident statements about the obvious supremacy of Bayesian probability theory and how Frequentists are obviously wrong/crazy/confused/etc. &nbsp;(IMO he's right about this stuff. &nbsp;But idk if this counts as controversial enough within academia?)</li><li>Probably a lot of assorted philosophy-of-science stuff about the <a href=\"https://www.lesswrong.com/s/zpCiuR4T343j9WkcK/p/fhojYBGGiYAFcryHZ\">nature of evidence</a>, the idea that <a href=\"https://www.lesswrong.com/s/fxynfGCSHpY4FmBZy\">high-caliber rationality ought to operate \"faster than science\"</a>, etc. &nbsp;(IMO he's right about the big picture here, although this topic covers a lot of ground so if you looked closely you could probably find some quibbles.)</li><li>The <a href=\"https://www.lesswrong.com/s/5uZQHpecjn7955faL/p/8QzZKw9WHRxjR4948\">claim / implication that talk of \"emergence\" or the study of \"complexity science\" is basically bunk</a>. &nbsp;(Not sure but seems like he's probably right? &nbsp;Good chance the ultimate resolution would probably be \"emergence/complexity is a much less helpful concept than its fans think, but more helpful than zero\".)</li><li>A lot of assorted references to cognitive and evolutionary psychology, including probably a number of studies that haven't replicated -- I think Eliezer has expressed regret at some of this and said he would write the sequences differently today. &nbsp;But there are probably a bunch of somewhat-controversial psychology factoids that Eliezer would still confidently stand by. &nbsp;(IMO you could probably nail him on some stuff here.)</li><li>Maybe some assorted claims about the nature of evolution? &nbsp;What it's optimizing for, what it produces (\"<a href=\"https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/XPErvb8m9FapXCjhA\">adaptation-executors, not fitness-maximizers</a>\"), where the logic <a href=\"https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/XC7Kry5q6CD9TyG4K\">can &amp; can't be extended</a> (can corporations be said to evolve? &nbsp;EY says no), whether group selection happens in real life (EY says <a href=\"https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/QsMJQSFj7WfoTMNgW\">basically never</a>). &nbsp;Not sure if any of these claims are controversial though.</li><li>Lots of confident claims about the idea of \"intelligence\" -- that it is a coherent concept, an important trait, etc. &nbsp;(Vs some philosophers who might say there's no one thing that can be called intelligence, or that the word intelligence has no meaning, or generally make the kinds of arguments parodied in \"<a href=\"https://arxiv.org/abs/1703.10987\">On the Impossibility of Supersized Machines</a>\". &nbsp;Surely there are still plenty of these philosophers going around today, even though I think they're very wrong?)</li><li>Some pretty pure philosophy about <a href=\"https://www.lesswrong.com/s/SGB7Y5WERh4skwtnb\">the nature of words/concepts</a>, and \"the relationship between cognition and concept formation\". &nbsp;I feel like philosophers have a lot of hot takes about linguistics, and the way we structure concepts inside our minds, and so forth? &nbsp;(IMO you could at least definitely find some quibbles, even if the big picture looks right.)</li><li>Eliezer confidently dismissing what he calls a key tenet of \"postmodernism\" in several <a href=\"https://www.lesswrong.com/s/p3TndjYbdYaiWwm9x/p/BwtBhqvTPGG2n2GuJ\">places</a> -- the idea that different \"truths\" can be true for different cultures. &nbsp;(IMO he's right to dismiss this.)</li><li>Some pretty confident (all things considered!) <a href=\"https://www.lesswrong.com/posts/iGH7FSrdoCXa5AHGs/what-would-you-do-without-morality\">claims </a>about <a href=\"https://www.lesswrong.com/s/9bvAELWc8y2gYjRav/p/vy9nnPdwTjSmt5qdb\">moral anti-realism</a> and the proper ethical attitude to take towards life? &nbsp;(I found his writing helpful and interesting but idk if it's the last word, personally I feel very uncertain about this stuff.)</li><li>Eliezer's confident rejection of religion at many points. &nbsp;(Is it too obvious, in academic circles, that all major religions are false? &nbsp;Or is this still controversial enough, with however many billions of self-identified believers worldwide, that you can get credit for calling it?)</li><li>It also feels like some of the more abstract AI alignment stuff (about the fundamental nature of \"agents\", what it means to have a \"goal\" or \"values\", etc) might be amenable to philosophical critique.</li></ul><p>Maybe you toss out half of those because they aren't seriously disputed by any legit academics. &nbsp;But, I am pretty sure that at least postmodern philosophers, \"complexity scientists\", people with bad takes on philosophy-of-science / philosophy-of-probability, and people who make \"On the Impossibility of Supersized Machines\"-style arguments about intelligence, are really out there! &nbsp;They at least consider themselves to be legit, even if you and I are skeptical! &nbsp;So I think EY would come across with a pretty good track record of correct philosophy at the end of the day, if you truly took the entire reference class of \"controversial philosophical claims\" and somehow graded how correct EY was (in practice, since we haven't yet solved philosophy -- how close he is to your own views?), and compared this to how correct the average philosopher is.</p>", "parentCommentId": "PK5RGXK7GJwys2DFk", "user": {"username": "Jackson Wagner"}}, {"_id": "b7qZptfcBTxnScea6", "postedAt": "2023-08-29T07:22:01.953Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Yeah, I know it is <i>sometimes</i> used by philosophers with specific precise meanings, it's just I've never been sure that there is a standard precise(ish) meaning.&nbsp;</p>", "parentCommentId": "bZedmfpw9N5dwkanq", "user": {"username": "Dr. David Mathers"}}, {"_id": "nfKkdLdiJasK8k9hL", "postedAt": "2023-08-29T07:27:23.809Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>On some definitions of \"qualia\" yes. I.e. not if you talk in the Tye/Byrne way where \"qualia\" turn out just to be perceived external properties that show up in the phenomenology, for example. And not, <i>necessarily</i> if qualia just means \"property of a conscious experience that shows up in the phenomenology\". But some people do think that about qualia in the second sense, and probably some people do endorse the stronger claim that this is part of the definition of \"qualia\".&nbsp;<br><br>Still having glanced at the Frankish paper I <i>think</i> I get what's going on now. Frankish is (I think, didn't read just glanced!) doing something like claiming standard dualist thought experiments show that ordinary people think there is more to consciousness than what goes on physically and functionally, then arguing that this makes that part of the meaning of \"phenomenally conscious\", so if there's nothing beyond the physical and the functional, there is no phenomenal consciousness by definition.&nbsp;</p>", "parentCommentId": "WCjYiN96ZrfdHgx3v", "user": {"username": "Dr. David Mathers"}}, {"_id": "b6WSfoEpCjXCkPmcg", "postedAt": "2023-08-29T07:29:41.495Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Cool, that makes sense.&nbsp;</p>", "parentCommentId": "mvPo6mMxfnNLo9Cdh", "user": {"username": "Dr. David Mathers"}}, {"_id": "PwDnLPDihNRwk7Pi4", "postedAt": "2023-08-29T07:49:51.383Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>'Were there bright people who said they had checked his work, understood it, agreed with him, and were trying to build on it?'<br><br>Yes, I think. Though my impression (Guy can make a better guess of this than me, since he has maths background) is that they were an extreme minority in the field, and all socially connected to Mochizuki: &nbsp;<a href=\"https://www.wired.com/story/math-titans-clash-over-epic-proof-of-the-abc-conjecture/\">https://www.wired.com/story/math-titans-clash-over-epic-proof-of-the-abc-conjecture/</a><br><br>'Between 12 and 18 mathematicians who have studied the proof in depth believe it is correct, wrote <a href=\"https://www.maths.nottingham.ac.uk/personal/ibf/\"><u>Ivan Fesenko</u></a> of the University of Nottingham in an email. But only mathematicians in \u201cMochizuki\u2019s orbit\u201d have vouched for the proof\u2019s correctness, Conrad <a href=\"http://www.math.columbia.edu/~woit/wordpress/?p=9871&amp;cpage=1#comments\"><u>commented</u></a> in a blog discussion last December. \u201cThere is nobody else out there who has been willing to say even off the record that they are confident the proof is complete.\u201d'<br><br>In any case with FDT, it might not really be an either/or of 'people who endorse it are clearly mistaken' &nbsp;v. 'the critiques are clearly mistaken'. Often in philosophy, all known views have significant costs, but its unclear what that means about what you should accept/reject. In any case, as I've said elsewhere in this comment section, FDT has now been defended in Journal of Philosophy, so in terms of academic philosophy it is very definitely out of the crank category sociologically (rightly or wrongly): <a href=\"https://philpapers.org/rec/LEVCDI\">https://philpapers.org/rec/LEVCDI</a><br><a href=\"https://leiterreports.typepad.com/blog/2022/07/best-general-philosophy-journals-2022.html\">https://leiterreports.typepad.com/blog/2022/07/best-general-philosophy-journals-2022.html</a><br>That makes me fairly confident FDT has <i>something</i> going for it.&nbsp;</p>", "parentCommentId": "kbCrExAzbJpFvSoao", "user": {"username": "Dr. David Mathers"}}, {"_id": "26o5Cfo3JtyKbTAEF", "postedAt": "2023-08-29T07:59:45.877Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>FWIW, I'm confused about Dennett's current position on animal consciousness. Still, my impression is that he does attribute consciousness to many other animals, but believes that human consciousness is importantly unique because of language and introspection.</p><p>In <a href=\"https://youtu.be/3aIfyl6fO34?list=PLY_s7b9LrR8UCcPqL59XuIII68ALjgLt7&amp;t=4597\">this panel discussion</a>, Dennett seemed confident that chickens and octopuses are conscious, directly answering that they are without reservation, and yes on bees after hesitating, but acknowledging their sophisticated capacities and going back to gradualism and whether what they do \"deserves to be called consciousness at all\".</p><p>&nbsp;</p><p>Some other recent writing by him or about his views:</p><blockquote><p>But Dennett thinks these things are like evolution, essentially gradualist, without hard borders. The obvious answer to the question of whether animals have selves is that they sort of have them. He loves the phrase \u201csort of.\u201d Picture the brain, he often says, as a collection of subsystems that \u201csort of\u201d know, think, decide, and feel. These layers build up, incrementally, to the real thing. Animals have fewer mental layers than people\u2014in particular, they lack language, which Dennett believes endows human mental life with its complexity and texture\u2014but this doesn\u2019t make them zombies. It just means that they \u201csort of\u201d have consciousness, as measured by human standards.</p></blockquote><p><a href=\"https://www.newyorker.com/magazine/2017/03/27/daniel-dennetts-science-of-the-soul\">https://www.newyorker.com/magazine/2017/03/27/daniel-dennetts-science-of-the-soul</a></p><p>&nbsp;</p><blockquote><p>To appreciate what I see to be Chalmers\u2019 second contribution, we first need to distinguish two different illusions: the malignant theorists\u2019 illusion and the benign user illusion. Chalmers almost does that. He asserts: \u2018To generate the hard problem of consciousness, all we need is the basic fact that there is something it is like to be us\u2019 (2018, p. 49). No, all we need is the fact that we think there is something it is like to be us. Dogs presumably do not think there is something it is like to be them, even if there is. It is not that a dog thinks there isn\u2019t anything it is like to be a dog; the dog is not a theorist at all, and hence does not suffer from the theorists\u2019 illusion. The hard problem and meta-problem are only problems for us humans, and mainly just for those of us humans who are particularly reflective. In other words, dogs aren\u2019t bothered or botherable by problem intuitions. Dogs \u2014 and, for that matter, clams and ticks and bacteria \u2014 do enjoy (or at any rate do not suffer from) a sort of user illusion: they are equipped to discriminate and track only some of the properties in their environment.</p></blockquote><p><a href=\"https://www.ingentaconnect.com/content/imp/jcs/2019/00000026/f0020009/art00004\">https://www.ingentaconnect.com/content/imp/jcs/2019/00000026/f0020009/art00004</a></p><p>&nbsp;</p><blockquote><p>I have long stressed the fact that human consciousness is vastly different from the consciousness of any other species, such as apes, dolphins, and dogs, and this \u201chuman exceptionalism\u201d has been met with little favor by my fellow consciousness theorists. Yes, of course, human beings, thanks to language, can do all sorts of things with their consciousness that their language-less cousin species cannot, but still, goes the common complaint, I have pushed my claims into extreme versions that are objectionable, and even offensive. Not wanting to stir up more resistance than necessary to my view, I have on occasion strategically soft-pedaled my claims, allowing animals to be heterophenomenological subjects (of sorts) thanks to their capacity to inform experimenters (if not tell them), but now, my thinking clarified by Rosenthal\u2019s, I want to recant that boundary blurring and re-emphasize the differences, which I think Rosenthal may underestimate as well. \u201cThoughts are expressible in speech,\u201d he writes (p. 155), but what about the higher-order thoughts of conscious animals? Are they? They are not expressed in speech, and I submit that it is a kind of wishful thinking to fill the minds of our dogs with thoughts of that sophistication. So I express my gratitude to Rosenthal for his clarifying account by paying him back with a challenge: how would he establish that non-speaking animals have higher-order thoughts worthy of the name? Or does he agree with me that the anchoring concept of consciousness, human consciousness, is hugely richer than animal consciousness on just this dimension?</p></blockquote><p><a href=\"https://davidrosenthal.org/Dennett-on-Seeming-to-Seem.pdf\">https://davidrosenthal.org/Dennett-on-Seeming-to-Seem.pdf</a></p>", "parentCommentId": "mscCTLnBoTNjRwpKM", "user": {"username": "MichaelStJules"}}, {"_id": "kCdXrJxujjEymtjiv", "postedAt": "2023-08-29T08:47:59.961Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Maybe that is right. Dennett is often quite slippery (I think he believes that precision actually makes philosophy worse a lot of the time.)&nbsp;<br><br>He also just may have changed his position. The SEP article on Animal Consciousness at one point refers to 'Dennett (who argues that consciousness is unique to humans)', but the reference is to a paper from 1995. Looking at the first page of the paper they cite, I think it was the one I vaguely remembered as \"Dennett denies animal consciousness for Yudkowsky-like reasons\". But having skimmed some of the paper again, I found it hard to tell this time if the reading of it as flat-out denying that animals are conscious was right. It seemed like Dennett *might* just be saying \"we don't know, but it's not obvious, and for some animals, there probably isn't even a fact of the matter\". (This is basically my view too, I think, except that unlike Dennett I don't think this much damages the case for animal rights.) But even that is inconsistent with \"anyone who thinks mammals aren't conscious is totally out-of-step with experts in the field, I think.\" And it's possible the stronger reading of Dennett as actually denying animal consciousness is correct: I only skimmed it, and the SEP thinks so.&nbsp;</p>", "parentCommentId": "26o5Cfo3JtyKbTAEF", "user": {"username": "Dr. David Mathers"}}, {"_id": "gKZN2LSrqAGzTG67d", "postedAt": "2023-08-29T11:18:05.899Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Disagree, or at least it doesn't have to be like that: I think deciding to smoke can give further evidence for the strength of the desire which at least could be further evidence that you are genetically predisposed to cancer if a common genetic cause between desire to smoke and getting cancer exists.&nbsp;</p>", "parentCommentId": "tRJtKoPv2d7yu9cDk", "user": {"username": "Dr. David Mathers"}}, {"_id": "orSqbCjfHztpAv4Fx", "postedAt": "2023-08-29T11:22:02.224Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>How long have you been studying philosophy? My views sometimes changed quite radically in grad school: I used to think problem of evil was \"decisive\", but now I think multiverse theodicies <i>might</i> work. I used to think Moore's proof of the existence of the external world was question-begging garbage as an undergrad, but then I read Scott Soames' account of its historical significance in grad school one day, and decided, no, actually Moore was totally right, and it's a deep insight. I used to buy Chalmers' 2-d zombie argument <i>against</i> [lol I originally idiotically wrote <i>for</i> here] materialism, and then one day at a conference in either my 2nd year of masters or 1st year of PhD, I decided that no, actually, I am now a physicalist. When I was an undergrad, I had idealist sympathies at one point, but now I think idealism is the dumbest view ever.&nbsp;</p>", "parentCommentId": "ReCvmGTCgrqWBQsxz", "user": {"username": "Dr. David Mathers"}}, {"_id": "SohCqBsxYfNZFF8FB", "postedAt": "2023-08-29T13:30:18.867Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>That seems correct to me. Perhaps not by coincidence, I also think the case against FDT is the weakest of his three, with some of the counterexamples being cases where I'm happy to bite the bullet, and the others seeming no worse than the objections to CDT, EDT, TDT, UDT etc.</p>", "parentCommentId": "tAofLDGbKr5Wa6Zk9", "user": {"username": "Larks"}}, {"_id": "oMoaLd3Y5qMtuQzDq", "postedAt": "2023-08-29T14:14:41.323Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I know you said you didn't want to repeatedly go back and forth, but . . .&nbsp;</p><p>Yes, I agree that if you have some psychological mechanism by which you can guarantee that you'll follow through on future promises--like programming an AI--then that's worth it. &nbsp;It's better to be the kind of agent who follows FDT (in many cases). &nbsp;But the way I'd think about this is that this is an example of rational irrationality, where it's rational to try to get yourself to do something irrational in the future because you get rewarded for it. &nbsp;But remember, decision theories are theories about what's rational, not theories about what kind of agent you should be. &nbsp;</p><p>I think we agree with both of the following claims:&nbsp;</p><ol><li>If you have some way to commit in advance to follow FDT in cases like the demon case or the bomb case, you should do so. &nbsp;</li><li>Once you are in those cases, you have most reason to defect. &nbsp;</li><li>Given that you can predict that you'll have most reason to defect, you can sort of psychologically make a deal with your future self where you say \"NO REALLY, DON'T DEFECT, I'M SERIOUS.\" &nbsp;</li></ol><p>My claim though, is that decision theory is about 2, rather than 1 or 3. &nbsp;No one disputes that the kinds of agents who two box do worse than the kinds of agents who one box--the question is about what you should do once you're in that situation.&nbsp;</p><p>If an AI is going to encounter Newcombe's problem a lot, everyone agrees you should program it to one box.&nbsp;</p>", "parentCommentId": "7hnKjCfabBdFxLmiQ", "user": {"username": "Omnizoid"}}, {"_id": "6F66MEovM6JaCbKHy", "postedAt": "2023-08-29T15:08:04.396Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I guess on #3, I suggest reading <i>Inadequate Equilibria. </i>I think it's given me more insight into Eliezer's approach to making claims. The <a href=\"https://www.lesswrong.com/posts/woCPxs8GxE7H35zzK/noting-an-error-in-inadequate-equilibria\">Bank of Japan example</a> he uses in the book is probably, ironically, one of the clearest examples of an uncorrect, egregious and overconfident mistake. I think the question of when to trust your own judgement over experts, of how much to incorporate expert views into your own, and how to identify experts in the first place is an open and unsolved issue (perhaps insoluble?).</p><p>Point taken on #5, was definitely my most speculative point.</p><p>I think it comes back to Point #1 for me. If your core aim was: <i>\"to show that Eliezer is worthy of much less deference then he currently is given\" </i>then I'd want you to show how much deference is given to him over and above the validity of his ideas spreading in the community, its mechanisms, and why that's a potential issue more than litigating individual object-level cases. Instead, if your issue is the commonly-believed views in the community that you think are incorrect, then you could have argued against those beliefs without necessarily invoking or focusing on Eliezer. In a way the post suffers from kinda trying to be both of those critiques at once, at least in my opinion. That's at least the feedback I'd give if you wanted to revisit this issue (or a similar one) in the future.</p>", "parentCommentId": "gqcF3pas2kHChwMDr", "user": {"username": "JWS"}}, {"_id": "dgZiLABEsWPmyeEGm", "postedAt": "2023-08-29T18:22:20.751Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I guess any omniscient demon reading this to assess my ability to precommit will have learned I can't even precommit effectively to not having long back-and-forth discussions, let alone cutting my legs off. But I'm still interested in where you're coming from here since I don't think I've heard your exact position before.</p><p>Have you read <a href=\"https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality\">https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality</a> ? Do you agree that this is our crux?</p><p>Would you endorse the statement \"Eliezer, using his decision theory, will usually end out with more utility than me over a long life of encountering the sorts of weird demonic situations decision theorists analyze, I just think he is less formally-rational\" ?&nbsp;</p><p>Or do you expect that you will, over the long run, get more utility than him?</p>", "parentCommentId": "oMoaLd3Y5qMtuQzDq", "user": {"username": "Scott Alexander"}}, {"_id": "grdMgsmZefSEAJavp", "postedAt": "2023-08-29T19:25:10.787Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I realize that my comment was somewhat poorly worded. I do not mean that you can follow the evidence in an absolute and empirical sense when forming a belief about the nature of consciousness. What you can do, however, and which Eliezer doesn't do, is to pay attention to what the philosophers who spend their lives working on this question are saying, and take their arguments seriously. The first principle approach is kind of \"I have an idea about consciousness which I think is right so I will not spend too much time looking at what actual philosophers are saying\".&nbsp;</p><p>(I did a master's degree in philosophy before turning to a career in social science, so at least I know enough about contemporary analytic philosophy to know what I don't know)</p><p>My comment \"just follow the actual evidence\" was not regarding consciousness or metaphysics, but regarding broader epistemic tendencies in the EA community. This tendency is very much Eliezer-ish in style: An idea that one knows best, because one is smart. If one has a set of \"priors\" one thinks are reasonably well-founded one doesn't need to look too much at empirical evidence, arguments among researchers or best practices in relevant communities outside of EA.&nbsp;</p><p>A case in point that comes to mind was some time ago when EAs debated whether it is a good idea that close colleagues in EA orgs have sex with each other. Some people pointed out that this is broadly frowned upon in most high-risk or high-responsibility work settings. Eliezer and other EAs thought they knew better, because, hey - first principles and we know ethics and we are smart! So the question then becomes: who should we trust on this, Eliezer and some young EAs in their early twenties or thirties, or high-powered financial firms and intelligence agencies who have fine-tuned their organizational practices over decades? Hm, tough one.</p><p>There are obviously huge differences between metaphysics, empirical evidence on various social issues, and sexual ethics in organizations. But the similarity is the first principles style of thought that is common in EA: we have good priors so no need to listen too much to outsiders.&nbsp;</p><p>I broadly agree with what the authors of \"Doing EA better\" wrote in their essay on this btw. They expressed similar points in a better and more precise way.&nbsp;</p><p>Given that Eliezer has had such a huge influence on epistemic practices in EA I therefore think it is valuable with takedowns like this. Eliezer is not that smart, actually, and his style of thinking has led EAs astray epistemically.</p>", "parentCommentId": "C94KkSfLkjo7JR9Z5", "user": {"username": "oivavoi"}}, {"_id": "QLCevppMFisW9BqBE", "postedAt": "2023-08-29T20:04:37.167Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks for your response Michael (and your one below to prisonpent). I'll try to keep it to the point and pre-commit to not responding further as I don't think this is the right place to have a debate about illusionism,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8rouchteub2\"><sup><a href=\"#fn8rouchteub2\">[1]</a></sup></span>&nbsp;but since you presented somewhat of a case for the illusionist I thought I might present the other side.</p><p>To me, phenomenal consciousness refers to the first-person perspective, which obviously exists. That first person perspective can make mistakes about the nature of the world, as in the M\u00fcller-Lyer case, but I have the experience nonetheless. In the Kammerer(a) piece, he argues that the <i>argument from the anomalousness of phenomenal consciousness</i> is a piece of evidence in favour of illusionism, but I take it one as in favour of non (reductive) physicalism. One man's modus ponens and all that.</p><p>I often find (strong) illusionist writings utterly baffling. I actually re-skimmed Quining Qualia before writing this, and it was really difficult for me to understand<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5241k4xosaf\"><sup><a href=\"#fn5241k4xosaf\">[2]</a></sup></span>&nbsp;even when consulting with GPT-4 in philosopher mode. In Kammerer(a) he refers to a 'quasi-phenomenal state', which I have no idea what that is. Again, viewing phenomenal consciousness as the first-person perspective, that just sounds like saying I have 'a fake first-person perspective'. To me that's the same as saying the first-person perspective doesn't exist, and since it clearly does, there is evidence that illusionist theories do not explain and therefore they are bad theories both philosophically and scientifically.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8rouchteub2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8rouchteub2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'd be happy to pick this up in an alternate forum though :)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5241k4xosaf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5241k4xosaf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is partly a me problem, but is also a philosophy problem. Sometimes technical language is needed, but the language of a lot of academic philosophy on all sides often seems to be needlessly obscurantist to me.</p></div></li></ol>", "parentCommentId": "iFwWsjt8mmt3yDATH", "user": {"username": "JWS"}}, {"_id": "LSwWWD3cnrBmBAeBs", "postedAt": "2023-08-29T23:16:05.831Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I'm not sure exactly what you mean by \"first-person perspective\", but strong illusionists might not deny that it exists, if understood in functionalist terms, say.</p>\n<p>Frankish says it is like something to be a bat, in terms of a bat's first-order responses or reactive patterns to things, but a bat can't know what it's like to be a bat, because they don\u2019t have (sufficiently sophisticated) introspection on those first-order responses. Dennett says even bacteria have a kind of \"user-illusion\", because they can discriminate, but only \"particularly reflective\" humans are subject to the theorists' illusion and worry about things like the hard problem of consciousness. So, we could define first-person perspective in terms of responses or discriminations, and in a way compatible with strong illusionism. This would attribute first-person perspectives extremely widely, e.g. even to bacteria.</p>\n<p>If by first-person perspective, you mean introspection, then illusionists wouldn't deny that humans have it.</p>\n<p>If by first-person perspective, you mean classic qualia (private, ineffable, intrinsic, etc.), then an illusionist would deny that this exists.</p>\n<p>Strong illusionists would also deny phenomenality, of course, in case that's different from classic qualia, but some attempted definitions of phenomenality (including what specific physicalist theories define consciousness as, e.g. broadcasting to a global workspace) actually could be understood as defining quasi-phenomenal states, and so compatible with illusionism.</p>\n<p>A theory-neutral defintion of quasi-phenomenal states could be that they're real things, processes or responses (physical or otherwise) on which introspection (of the right kind) leads to beliefs in phenomenal properties, e.g. these quasi-phenomenal states appear epistemically to us as to be phenomenal. If introspection is reliable and can access phenomenal states, then these accessed phenomenal states would be quasi-phenomenal states under this definition. Illusionists would claim that introspection is not reliable, no phenomenal states actually exist, and so the beliefs in phenomenality are mistaken, hence illusions.</p>\n<p>I think it would be wrong to take phenomenal properties as evidence that must be explained, and doing so begs the question against illusionism. What we have evidence of is the appearance of (our beliefs in) phenomenal properties, and illusionism tries to explain that without requiring the actual existence of phenomenal properties. Sometimes (maybe usually) appearances and beliefs are accurate instead of illusions, and the best explanation is based on what they represent actually existing.</p>\n", "parentCommentId": "QLCevppMFisW9BqBE", "user": {"username": "MichaelStJules"}}, {"_id": "g725hSEzgrdoaJZz3", "postedAt": "2023-08-30T00:19:59.884Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I would agree with the statement \"if Eliezer followed his decision theory, and the world was such that one frequently encountered lots of Newcombe's problems and similar, you'd end up with more utility.\" &nbsp;I think my position is relatively like MacAskill's in the linked post where he says that FDT is better as a theory of the agent you should want to be than what's rational. &nbsp;</p><p>But I think that rationality won't always benefit you. &nbsp;I think you'd agree with that. &nbsp;If there's a demon who tortures everyone who believes FDT, then believing FDT, which you'd regard as rational, would make you worse off. &nbsp;If there's another demon who will secretly torture you if you one box, then one boxing is bad for you! &nbsp;It's possible to make up contrived scenarios that punish being rational--and Newcombe's problem is a good example of that.</p><p>Notably, if we're in the twin scenario or the scenario that tortures FDTists, CDT will dramatically beat FDT. &nbsp;</p><p>I think the example that's most worth focusing on is the demon legs cut off case. &nbsp;I think it's not crazy at all to one box, and have maybe 35% credence that one boxing is right. &nbsp;I have maybe 95% credence that you shouldn't cut off your legs in the demon case, and 80% confidence that the position that you can is crazy, in the sense that if you spent years thinking about it while being relatively unbiased you'd almost certainly give it up.&nbsp;</p>", "parentCommentId": "dgZiLABEsWPmyeEGm", "user": {"username": "Omnizoid"}}, {"_id": "5RP2yc9jshBX3NKTQ", "postedAt": "2023-08-30T02:31:53.718Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Also, the commentary <a href=\"https://keithfrankish.github.io/articles/Frankish_Not%20disillusioned_eprint.pdf#page=13\">here</a> on Nicholas Humphrey's views may be illustrative of definitional issues. Humphrey denies the label illusionism for his theory, but Frankish responds that his theory really is illusionist. Also, Schwitzgebel and Nida-R\u00fcmelin attempted to define phenomenality as common features of multiple example mental states (and/or by contrast with unconscious states), but Frankish argues that this doesn't work to define phenomenality (at least not in a way incompatible with illusionism):</p><p>&nbsp;</p><blockquote><p>For, precisely because his definition is so innocent, it is not incompatible with illusionism. As I stressed in the target article, illusionists do not deny the existence of the mental states we describe as phenomenally conscious, nor do they deny that we can introspectively recognize these states when they occur in us. Moreover, they can accept that these states share some unifying feature. But they add that this feature is not possession of phenomenal properties (qualia, what-it\u2019s-like-ness, etc.) in the substantive sense created by the phenomenality language game. Rather, it is possession of introspectable properties that dispose us to judge that the states possess phenomenal properties in that substantive sense (of course, we could call this feature \u2018phenomenality\u2019 if we want, but I take it that phenomenal realists will not want to do that). Now, the challenge of the target article was to articulate a concept of phenomenality that is recognizably substantive (and so not compatible with illusionism) yet stripped of all commitments incompatible with physicalism. Schwitzgebel hasn\u2019t done this, since his conception is not substantive.</p><p>Nevertheless, Schwitzgebel has succeeded in something perhaps more important. He has defined a neutral explanandum for theories of consciousness, which both realists and illusionists can adopt. (I have referred to this as consciousness in an inclusive sense. We might call it simply consciousness, or, if we need to distinguish it from other forms, putative phenomenal consciousness.) In doing this, Schwitzgebel has performed a valuable service.</p></blockquote><p>&nbsp;</p><blockquote><p>However, I deny that it is the sort of feature realists think it is. It is not some intrinsic quality, akin to the property characterized by the phenomenality language game. Rather, it is (roughly) the property of having a cluster of introspective representational states and dispositions that create the illusion that one is acquainted with some intrinsic quality. I am sure that this is not what Nida-R\u00fcmelin thinks the procedure picks out, but I don\u2019t see how she can rule out the possibility.</p></blockquote>", "parentCommentId": "QLCevppMFisW9BqBE", "user": {"username": "MichaelStJules"}}, {"_id": "LproPzoSyhnGYThZi", "postedAt": "2023-08-30T03:24:35.121Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I think rather than say that Eliezer is wrong about decision theory, you should say that Eliezer's goal is to come up with a decision theory that helps him get utility, and your goal is something else, and you have both come up with very nice decision theories for achieving your goal.</p><p>(what <i>is</i> your goal?)</p><p>My opinion on your response to the demon question is \"The demon would never create you in the first place, so who cares what you think?\" That is, I think your formulation of the problem includes a paradox - we assume the demon is always right, but also, that you're in a perfect position to betray it and it can't stop you. What would <i>actually</i> happen is the demon would create a bunch of people with amputation fetishes, plus me and Eliezer who it knows wouldn't betray it, and it would never put you in the position of getting to make the choice in real life (as opposed to in an FDT algorithmic way) in the first place. The reason you find the demon example more compelling than the Newcomb example is that it starts by making an assumption that undermines the whole problem - that is, that the demon has failed its omniscience check and created you who are destined to betray it. If your problem setup contains an implicit contradiction, you can prove anything.</p><p>I don't think this is as degenerate a case as \"a demon will torture everyone who believes FDT\". If that were true, and I expected to encounter that demon, I would simply try not to believe FDT (insofar as I can voluntarily change my beliefs). While you can always be screwed over by weird demons, I think decision theory is about what to choose in cases where you have all of the available knowledge and also a choice in the matter, and I think the leg demon fits that situation.</p>", "parentCommentId": "g725hSEzgrdoaJZz3", "user": {"username": "Scott Alexander"}}, {"_id": "AyKszfCyhvfYSaucj", "postedAt": "2023-08-30T11:47:56.146Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>'The blackmail situation seems analogous to the Counterfactual Mugging, which was created to highlight how Eliezer's decision theories sometimes (my flippant summary) suggest you make locally bad decisions in order to benefit versions of you in different Everett branches. Schwartz objecting \"But look how locally bad this decision is!\" isn't telling Eliezer anything he doesn't already know, and isn't engaging with the reasoning'<br><br>I just control-F searched the paper Schwarz reviewed, for \"Everett\", \"quantum\", \"many-worlds\" and \"branch\" and found zero hits. Can't really blame Schwarz for ignoring an argument that does not appear in the paper! There's no mention of these in the Soares and Levinstein FDT paper that did <i>get</i> published in J Phil either.</p>", "parentCommentId": "Z9sGz9w92in9dja2M", "user": {"username": "Dr. David Mathers"}}, {"_id": "TheWNW45cyn737Pyi", "postedAt": "2023-08-30T12:17:42.992Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>The remark.about Everett branches rather gives the game away. Decision theories rest on assumptions about the nature of the universe and of the decider, so trying to formulate a DT that will work.perfectly in any universe is hopeles.</p>\n", "parentCommentId": "Z9sGz9w92in9dja2M", "user": {"username": "TAG"}}, {"_id": "Ajwaq3XHWum2WeG74", "postedAt": "2023-08-30T12:47:26.959Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>The argument is basically saying that if X can't be explained by physicalism, then X is an illusion. That's treating physicalism as unfalsifable.</p>\n", "parentCommentId": "TjRpXPHRboiTgxLR4", "user": {"username": "TAG"}}, {"_id": "pSrHBMJZC7Heh9w65", "postedAt": "2023-08-30T13:10:19.100Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I don\u2019t know much about philosophy to participate in the zombies &amp; animal consciousness debate meaningfully. (It takes me hours to get people who think there\u2019s a 30% chance microns have qualia to start to understand the reason why they\u2019re likely not. And the word \u201cconsciousness\u201d is a not a good one, as people mean totally different things when they use it. And Yudkowsky eats fish but not octopi and some other seafood, because he think there\u2019s a high enough chance octopi have consciousness. But, this is not my area, this is just something that\u2019s fun to talk about.)</p>\n<p>But the critique of FDT doesn\u2019t seem valid at all.</p>\n<p>If a simulated copy of you gives in to a threat, it makes sense to identically blackmail the real you. If you don\u2019t, it doesn\u2019t make sense to spend resources on reducing your utility.</p>\n<p>If you\u2019re the kind of agent who gives in to blackmail, everyone across the multiverse will extract everything you have from you, and you\u2019ll get quite a negative utility for all the threats you didn\u2019t have resources left to give in to. If you don\u2019t give in to threats, you\u2019ll get much less threats and won\u2019t lose as much.</p>\n<p>If you\u2019re an AI trained with machine learning that does what a logical decision theory says you should do, you get a lower loss then an AI that does something else, and you get selected.</p>\n", "parentCommentId": null, "user": {"username": "Samin"}}, {"_id": "cJfowdgXbAgbxLDCJ", "postedAt": "2023-08-30T13:18:04.086Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Seems like you are thinking of a case without full introspection. Both Eells and Ahmed provide convincing tickle defences in this case as well. See <a href=\"https://www.andrew.cmu.edu/user/coesterh/TickleDefenseIntro.pdf\">Oesterheld (2022)</a> for a review of the arguments (especially sections 6.3 and 6.4).&nbsp;</p>", "parentCommentId": "gKZN2LSrqAGzTG67d", "user": {"username": "Sylvester Kollin"}}, {"_id": "EfPTrascgDPJwS3B8", "postedAt": "2023-08-30T14:45:49.935Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>At this point I have to admit that we've gotten beyond my knowledge of this stuff, and I can't really follow your comment! Though when I glanced at the Oesterheld I think I get what's going on, sort of, though it's not clear to me why decision theory should start with the assumption of perfect introspection, and I also <i>suspect</i> you can come up with a case where your X-ing provides evidence that Y bad thing will happen but does not causally influence Y, that gets around this while still looking bad for evidential decision theory. But that's only a guess, as again, I have no background on this stuff beyond generic philosophy education.&nbsp;</p>", "parentCommentId": "cJfowdgXbAgbxLDCJ", "user": {"username": "Dr. David Mathers"}}, {"_id": "okCayGFC3xMZemzfg", "postedAt": "2023-08-30T14:47:13.626Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Eliezer quoted the SEP entry as support for his position and you, in your response, cut off the part of said quote which contained the support and only responded to the remaining part which did not contain the supporting point (eg the key words: causal closure). This seems bad-faith to me even though I think you're right that Eliezer did not account for interactionist dualism (though I disagree that it is necessarily a critical error, I don't think one should be expected to note every possibilty no matter how low prob in the course of an argumentation.)</p>", "parentCommentId": "tr7mbtM6A9ZxeJJQE", "user": {"username": "Pseudotruth"}}, {"_id": "bHpPeXa8jzBtHw6tq", "postedAt": "2023-08-30T15:47:16.319Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>No, it isn't just saying that. That understates the case for both physicalism and illusionism that I outlined.</p>\n<p>We have good independent reasons to believe physicalism and against alternatives, and I mentioned this, but didn't give examples. Here are some:</p>\n<ol>\n<li>There's the good empirical track record of physicalism generally and specifically in giving adequate explanations for the seemingly nonphysical.</li>\n<li>There are the questions of where, when, how and why nonphysical properties arise, whether that's from or with a collection of particles in a system, over a human's development from conception, or in our evolutionary history, that nonphysicalist theories struggle to give sensible answers to. If the nonphysical is fundamental and there at all levels (panpsychism), then we have the combination problem: how does the nonphysical combine to make minds like ours?</li>\n<li>There's the expansion of the physical to include what's empirically reliable and testable to very high precision and for which we have precise fundamental accounts, including interactions with other fundamental physical properties (although not necessarily all such interactions, e.g. we don't yet have a good theory of quantum gravity). For example, gravity, quantum superposition and quantum entanglement might have seemed unphysical before, but they've become part of our physical ontology because of their reliability and our very good (but incomplete) understanding of them and their relationships with other things. Of course, maybe the seemingly nonphysical properties of minds will eventually come to gain the same status, but it\u2019s nowhere close to that now. We shouldn\u2019t be hasty to assume the existence of things that don't meet this bar, because the evidence for them is far weaker.</li>\n</ol>\n<p>The illusionist also argues (or would want to, but currently lacks the details to make it very convincing) that there's a specific adequate (physicalist) explanation for the appearance of X that doesn\u2019t require the existence of X. If the appearance of X doesn't depend on its existence, then the appearance of X isn't reliable evidence for its existence. Without any other independent argument for the existence of X (as seems to be the case for phenomenality and classic qualia), then it becomes like any other verified illusion, and our reasons to believe in X become very weak.</p>\n", "parentCommentId": "Ajwaq3XHWum2WeG74", "user": {"username": "MichaelStJules"}}, {"_id": "PxLzjTGFJSCqWizn2", "postedAt": "2023-08-30T17:36:02.712Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Unfortunate. I find the author's first two sections weak but I find the third section about animal consciousness to be interesting, concrete, falsifiable, written clearly, and novel-to-me.</p>", "parentCommentId": "7fs5nHEkK6AGgPAJ9", "user": {"username": "Sinclair Chen"}}, {"_id": "tYXdfKTpJNhwqM36v", "postedAt": "2023-08-30T21:00:44.074Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I might add that Mochizuki also isn't a crank (as far as I understand, at least) - just someone who made it difficult to realise that he was wrong in his very big claim despite being a smart person.</p>\n", "parentCommentId": "PwDnLPDihNRwk7Pi4", "user": {"username": "Guy Raveh"}}, {"_id": "eC2qx9neiwfaEioxB", "postedAt": "2023-08-31T00:02:18.397Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>The demon case shows that there are cases where FDT loses, as is true of all decision theories. &nbsp;IF the question is which decision theory will programming into an AI generate most utility, then that's an empirical question that depends on facts about the world. &nbsp;If it's once you're in a situation which &nbsp;will get the most utility, well, that's causal decision theory. &nbsp;</p><p>Decision theories are intended as theories of what is rational for you to do. &nbsp;So it describes what choices are wise and which choices are foolish. &nbsp;I think Eliezer is confused about what a decision theory is, but that is a reason to trust his judgment less. &nbsp;</p><p>In the demon case, we can assume it's only almost infallible, so every million times it makes a mistake. &nbsp;The demon case is a better example, because I have some credence in EVT, and EVT entails you should one box. &nbsp;I am waaaaaaaaaaaay more confident FDT is crazy than I am that you should two box.&nbsp;</p>", "parentCommentId": "LproPzoSyhnGYThZi", "user": {"username": "Omnizoid"}}, {"_id": "tHcRfBRff39gksXon", "postedAt": "2023-08-31T00:05:51.787Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>He didn't quote it--he linked to it. &nbsp;I didn't quote the broader section because it was ambiguous and confusing. &nbsp;The reason not accounting for interactionist dualism matters is because it means that he misstates the zombie argument, and his version is utterly unpersuasive.&nbsp;</p>", "parentCommentId": "okCayGFC3xMZemzfg", "user": {"username": "Omnizoid"}}, {"_id": "LBfpStXHGex5pB46b", "postedAt": "2023-08-31T03:56:00.369Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I would be shocked if Eliezer ever said he was \u201c100% sure\u201d about anything. It would just sound gauche coming from him.</p>\n<blockquote>\n<p>Using the log odds exposes the fact that reaching infinite certainty requires infinitely strong evidence, just as infinite absurdity requires infinitely strong counterevidence.</p>\n</blockquote>\n<blockquote>\n<p>Furthermore, all sorts of standard theorems in probability have special cases if you try to plug 1s or 0s into them\u2014like what happens if you try to do a Bayesian update on an observation to which you assigned probability 0.</p>\n</blockquote>\n<blockquote>\n<p>So I propose that it makes sense to say that 1 and 0 are not in the probabilities; just as negative and positive infinity, which do not obey the field axioms, are not in the real numbers.</p>\n</blockquote>\n<p><a href=\"https://www.lesswrong.com/posts/QGkYCwyC7wTDyt3yT/0-and-1-are-not-probabilities\">https://www.lesswrong.com/posts/QGkYCwyC7wTDyt3yT/0-and-1-are-not-probabilities</a></p>\n", "parentCommentId": "s3FxhsszKjTrPHQWQ", "user": {"username": "Holly_Elmore"}}, {"_id": "EXtANEeowzAXwPHJR", "postedAt": "2023-08-31T04:13:17.277Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Wait sorry it\u2019s hard to see the broader context of this comment on account of being on my phone and comment sections being hard to navigate on ea forum.  I don\u2019t know if I said eliezer had 100% credence, but if I did, that was wrong.</p>\n", "parentCommentId": "LBfpStXHGex5pB46b", "user": {"username": "Omnizoid"}}, {"_id": "7gqTGB9f2KbkhJigH", "postedAt": "2023-08-31T14:06:16.288Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>but every theory other than strong illusionism needs to solve [the hard problem].</p></blockquote><p>I agree in the sense that other theories can't simply <i>dissolve</i> it, but that's almost tautological. If you mean that other theories need to solve it <i>in order to justify belief in them</i>, or in other words if you mean that if we were all certain the hard problem would never be adequately resolved we would be forced to accept illusionism, then I don't think that's correct at all.&nbsp;</p><p>Consider what we might call \"the hard problem of physics\": why this? Why anything? What puts the fire in the equations? Short of some galaxybrained PSR maneuver, which seems more and more dubious by the century, I doubt we're ever going to get an answer. It is completely inexplicable that anything should exist.</p><p>And yet it does. It's there, it's obviously there, everything you've ever seen or felt or thought bears witness to it, and someone who claims otherwise on the grounds that it doesn't make any sense has <i>entirely</i> misunderstood the nature of their situation.&nbsp;</p><hr><p>This is also how I feel about illusionism. Phenomenal experience is the <i>only</i> thing we have direct access to: all arguments, all inferences, all sense data, ultimately cash out in some regularity in the phenomenal content of consciousness. Whatever its ontological status, it's the epistemic ground of everything else. You can't justify the claim that phenomenal consciousness doesn't exist by pointing to patterns of phenomena, any more than you can demonstrate the nonexistence language in an essay or offer a formal disproof of modus ponens.&nbsp;</p><p>So these illusionists explanations are, well, not really explanations of consciousness. They're explanations of a coarse world model in terms of a finer one, but the coarse world model wasn't the thing I wanted explained. On the contrary, it was a tiny provisional step towards an explanation: there are many lawlike regularities in the structure of my experiences, so I hypothesize a common cause and call it \"my brain\". It's a very successful hypothesis, and I'd like to know why - given that the world is more than just its shadow on the mind<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefesy50sebvik\"><sup><a href=\"#fnesy50sebvik\">[1]</a></sup></span>, why should the structure of one reflect the other?&nbsp;</p><p>The illusionist response of \"actually your hypothesis is the evidence and your data are just hypotheses\" misses the point entirely.&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnesy50sebvik\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefesy50sebvik\">^</a></strong></sup></span><div class=\"footnote-content\"><p>the dumbest possible solution, but I can't rule it out</p></div></li></ol>", "parentCommentId": "TjRpXPHRboiTgxLR4", "user": {"username": "prisonpent"}}, {"_id": "4rtfwX288rbZzDLG8", "postedAt": "2023-08-31T16:35:45.034Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I agree - but that's why it stuck in my mind so strongly. I remember thinking how incongruous it was at the time.</p>", "parentCommentId": "LBfpStXHGex5pB46b", "user": {"username": "Arepo"}}, {"_id": "zQSQYwCHug3pBDQhv", "postedAt": "2023-08-31T18:10:41.609Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>The analogy to the \"hard problem of physics\" is interesting, and my stance towards the problem is the same as yours.</p><p>&nbsp;</p><p>However, I don't think the analogy really works.</p><blockquote><p>This is also how I feel about illusionism. Phenomenal experience is the <i>only</i> thing we have direct access to: all arguments, all inferences, all sense data, ultimately cash out in some regularity in the phenomenal content of consciousness. Whatever its ontological status, it's the epistemic ground of everything else.</p></blockquote><p>Is phenomenality itself necessary/on the causal path here? Illusionists aren't denying consciousness, that it has contents, that there's regularity in its contents or that it's the only thing we have direct access to. Illusionists are just denying the phenomenal nature of consciousness or phenomenal properties. I would instead say, more neutrally:</p><blockquote><p>Experience (whatever it is) is the <i>only</i> thing we have direct access to: all arguments, all inferences, all sense data, ultimately cash out in some regularity in the content of consciousness (whatever it is). Whatever its ontological status, it's the epistemic ground of everything else.</p></blockquote><p>Note also that the information in or states of a computer (including robots and AIs) also play a similar role for the computer. And, a computer program can't necessarily explain how it does everything it does. \"Ineffability\" for computers, like us, could just be <a href=\"https://link.springer.com/referenceworkentry/10.1007/978-3-319-47829-6_1596-1\">cognitive impenetrability</a>: some responses and contents are just <i>wired in</i>, and their causes are not accessible to (certain levels of) the program. For \"us\", everything goes through our access consciousness.</p><p>So, what exactly do you mean by phenomenality, and what's the extra explanatory work phenomenality is doing here? What isn't already explained by the discriminations and responses by our brains, non-phenomenal (quasi-phenomenal) states or just generally physics?</p><p>If you define phenomenality <i>just</i> by certain physical states, effects or responses, or functionalist or causal abstractions thereof, say, then I think you'd be <i>defining away</i> phenomenality, i.e. \"zero qualia\" according to Frankish (<a href=\"https://keithfrankish.github.io/articles/Frankish_Quining%20diet%20qualia_eprint.pdf\">paper</a>, <a href=\"https://www.youtube.com/watch?v=zrk5TfwiY-s\">video</a>).</p>", "parentCommentId": "7gqTGB9f2KbkhJigH", "user": {"username": "MichaelStJules"}}, {"_id": "n5AyaCqvaccc6xpsh", "postedAt": "2023-08-31T22:27:49.513Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I do not find the argument against the applicability of the Complete Class theorem in that post convincing. See Charlie Steiner's reply in the comments.</p><blockquote><p>You just have to separate \"how the agent internally represents its preferences\" from \"what it looks like the agent us doing.\" You describe an agent that dodges the money-pump by simply acting consistently with past choices. <i>Internally</i> this agent has an incomplete representation of preferences, plus a memory. But <i>externally</i> it looks like this agent is acting like it assigns equal value to whatever indifferent things it thought of choosing between first.</p></blockquote><p>Decision theory is concerned with external behaviour, not internal representations. All of these theorems are talking about whether the agent's <i>actions</i> can be consistently <i>described </i>as maximising a utility function. They are not concerned whatsoever with how the agent actually mechanically represents and thinks about its preferences and actions on the inside. To decision theory, agents are black boxes. Information goes in, decision comes out. Whatever processes may go on in between are beyond the scope of what the theorems are trying to talk about.</p><p>So</p><blockquote><p>Money-pump arguments for Completeness (understood as the claim that sufficiently-advanced artificial agents will have complete preferences) assume that such agents will not act in accordance with policies like \u2018if I previously turned down some option&nbsp;X, I will not choose any option that I strictly disprefer to&nbsp;X.\u2019 But that assumption is doubtful. Agents with incomplete preferences have good reasons to act in accordance with this kind of policy: (1) it never requires them to change or act against their preferences, and (2) it makes them immune to all possible money-pumps for Completeness.&nbsp;</p></blockquote><p>As far as decision theory is concerned, <i>this is a complete set of preferences</i>. Whether the agent makes up its mind as it goes along or has everything it wants written up in a database ahead of time matters not a peep to decision theory. The only thing that matters is whether the agent's resulting <i>behaviour</i> can be coherently described as maximising a utility function. If it quacks like a duck, it's a duck.</p>", "parentCommentId": "CCgyJ9kLAxLS8Ea4Q", "user": {"username": "Laplace"}}, {"_id": "tQsyZ65Gv9mg7zrCM", "postedAt": "2023-09-01T02:35:48.797Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I thought we already agreed the demon case showed that FDT wins in real life, since FDT agents will consistently end up with more utility than other agents.</p><p>Eliezer's argument is that you can become the kind of entity that is programmed to do X, by choosing to do X. This is in some ways a claim about demons (they are good enough to predict even the choices you made with \"your free will\"). But it sounds like we're in fact positing that demons are that good - I don't know how to explain how they have 999,999/million success rate otherwise - so I think he is right.</p><p>I don't think the demon being wrong one in a million times changes much. 999,999 of the people created by the demon will be some kind of FDT decision theorist with great precommitment skills. If you're the one who isn't, you can observe that you're the demon's rare mistake and avoid cutting off your legs, but this just means you won the lottery - it's not a generally winning strategy.</p><blockquote><p>Decision theories are intended as theories of what is rational for you to do. &nbsp;So it describes what choices are wise and which choices are foolish.&nbsp;</p></blockquote><p>I don't understand why you think that the choices that get you more utility with no drawbacks are foolish, and the choices that cost you utility for no reason are wise.</p><p>On the Newcomb's Problem post, Eliezer explicitly said that he doesn't care why other people are doing decision theory, he would like to figure out a way to get more utility. Then he did that. I think if you disagree with his goal, you should be arguing \"decision theory should be about looking good, not about getting utility\" (so we can all laugh at you) rather than saying \"Eliezer is confidently and egregiously wrong\" and hiding the fact that one of your main arguments is that he said we should try to get utility instead of failing all the time and then came up with a strategy that successfully does that.</p>", "parentCommentId": "eC2qx9neiwfaEioxB", "user": {"username": "Scott Alexander"}}, {"_id": "Jb9vBmybQConznXcB", "postedAt": "2023-09-01T06:29:57.958Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I really appreciate this post, and think you did a great job writing it. This is one of the most comprehensive summaries of animal consciousness research I have seen, and I will likely be referring back to it. If you're interested, I have compiled a few sources that try to demonstrate that \"animals are conscious\" is the consensus view among people who study it. (I was dating someone who weakly believed that animals weren't conscious, so I sent him a 7 page email on animal consciousness).</p><p>I would summarize the errors you're describing as such:</p><ul><li>Zombie error: Misunderstanding the views of others when refuting their arguments</li><li>Decision theory error: His theory has contradictions and weird implications?</li><li>Animal error: Epistemics. Refusing to engage with evidence.</li></ul><p>&nbsp;</p><p>The zombie and animal errors feel like fundamental, egregious errors. The decision theory error just feels like a philosophical disagreement? Your critique of it sounds like a lot of philosophical critiques of other philosophical theories. So a disagreement, but not evidence of egregious errors. But I'm not a philosopher and haven't read philosophy in a long, long time. So I may be mistaken about the nature of your disagreement.</p>", "parentCommentId": null, "user": {"username": "Masha Winchell"}}, {"_id": "GEXEqLDpwaNET5Nnk", "postedAt": "2023-09-01T11:22:18.748Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>The only thing that matters is whether the agent's resulting <i>behaviour</i> can be coherently described as maximising a utility function.</p></blockquote><p>If you're only concerned with externals, <i>all</i> behaviour can be interpreted as maximising a utility function. Consider an example: an agent pays $1 to trade vanilla for strawberry, $1 to trade strawberry for chocolate, and $1 to trade chocolate for vanilla. Considering only externals, can this agent be represented as an expected utility maximiser? Yes. We can say that the agent's preferences are defined over entire histories of the universe, and the history it's enacting is its most-preferred.</p><p>If we want expected-utility-maximisation to rule anything out, we need to say something about the objects of the agent's preference. And once we do that, we can observe violations of Completeness.</p>", "parentCommentId": "n5AyaCqvaccc6xpsh", "user": {"username": "elliottthornley"}}, {"_id": "LJCZS5FSjhbjt6ySb", "postedAt": "2023-09-01T12:03:16.529Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p><i>all</i> behaviour can be interpreted as maximising a utility function.</p></blockquote><p>Yes, it indeed can be. However, the less coherent the agent acts, the more cumbersome it will be to describe it as an expected utility maximiser. Once your utility function specifies entire histories of the universe, its description length goes through the roof. If describing a system as a decision theoretic agent is that cumbersome, it's probably better to look for some other model to predict its behaviour. A rock, for example, is not well described as a decision theoretic agent. You can technically specify a utility function that does the job, but it's a ludicrously large one.</p><p>The less coherent and smart a system acts, the longer the utility function you need to specify to model its behaviour as a decision theoretic agent will be. In this sense, expected-utility-maximisation does rule things out, though the boundary is not binary. It's telling you what kind of systems you can usefully model as \"making decisions\" if you want to predict their actions.</p><p>If you would prefer math that talks about the actual internal structures agents themselves consist of, decision theory is not the right field to look at. It just does not address questions like this at all. Nowhere in the theorems will you find a requirement that an agent's preferences be somehow explicitly represented in the algorithms it \"actually uses\" to make decisions, whatever that would mean. It doesn't know what these algorithms are, and doesn't even have the vocabulary to formulate questions about them. It's like saying we can't use theorems for natural numbers to make statements about counting sheep, because sheep are really made of fibre bundles over the complex numbers, rather than natural numbers. The natural numbers are talking about our count of the sheep, not the physics of the sheep themselves, nor the physics of how we move our eyes to find the sheep. And decision theory is talking about our model of systems <i>as</i> agents that make decisions, not the physics of the systems themselves and how some parts of them may or may not correspond to processes that meet some yet unknown embedded-in-physics definition of \"making a decision\".</p>", "parentCommentId": "GEXEqLDpwaNET5Nnk", "user": {"username": "Laplace"}}, {"_id": "XcKKiNsqJdAdxJtyT", "postedAt": "2023-09-01T17:15:24.271Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>Is phenomenality itself necessary/on the causal path here?</p></blockquote><p>I have no idea what the causal path is, or even whether causation is the right conceptual framework here. But it has no bearing on whether phenomenal experiences exist: they're particular things out there in the world (so to speak), not causal roles in a model.&nbsp;</p><blockquote><p>Note also that the information in or states of a computer (including robots and AIs) also play a similar role for the computer.</p></blockquote><p>It plays a similar role, for very generous values of \"similar\", in the computer qua physical system, sure. And I am perfectly happy to grant that \"I\" qua human organism am almost certainly a causally closed physical system like any other. (Or rather, the joint me-environment system is). But that's not what I'm talking about.&nbsp;</p><blockquote><p>For \"us\", everything goes through our access consciousness.</p></blockquote><p>I'm not talking about access consciousness either! That's just one particular sort of mental state in a vast landscape. The existence of the landscape - as a really existing thing with really existing contents, not a model &nbsp;- is the heart of the mystery.&nbsp;</p><blockquote><p>what's the extra explanatory work phenomenality is doing here?</p></blockquote><p>My whole point is that it <i>doesn't </i>do explanatory work, and expecting it to is a conceptual confusion. The sun's luminosity does not explain its composition, the fact that looking at it causes retinal damage does not explain its luminosity, the firing of sensory nerves does not explain the damage, and the qualia that constitute \"hurting to look at\" do not explain the brain states which cause them.</p><p>Phenomenality is raw data: the thing to be explained. Not what I do, not what I say, not the exact microstate of my brain, not even the structural features of my mind - but the stuff being structured, and the fact there is any.</p><blockquote><p>If you define phenomenality <i>just</i> by certain physical states, effects or responses, or functionalist or causal abstractions thereof</p></blockquote><p>I don't define phenomenality! I point at it. It's that thing, right there, all the time. The stuff in virtue of which all my inferential knowledge is inferential knowledge <i>about something</i>, and not just empty formal structure. The relata which introspective thought relates<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9p7evryzf16\"><sup><a href=\"#fn9p7evryzf16\">[1]</a></sup></span>. The stuff at the bottom of the logical positivists' glass. You know, the thing.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9p7evryzf16\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9p7evryzf16\">^</a></strong></sup></span><div class=\"footnote-content\"><p>And again, I am only pointing at particular examples, not defining or characterizing or even trying to offer a conceptual prototype: qualia need not have anything to do with introspection, linguistic thought, inference, or any other sort of higher cognition. In particular, \"seeing my computer screen\" and \"being aware of seeing my computer screen\" are not the same quale.</p></div></li></ol>", "parentCommentId": "zQSQYwCHug3pBDQhv", "user": {"username": "prisonpent"}}, {"_id": "Zb225MQG4tJmDvQFd", "postedAt": "2023-09-01T19:17:17.989Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>But it seems to me that phenomenal aspects themselves aren't the raw data by which we know things. If you accept the causal closure of the physical, non-phenomenal aspects of our discriminations and cognitive responses are already enough to explain how we know things, or the phenomenal aspects just are physical aspects (possibly abstracted to functions or dispositions), which would be consistent with illusionism.</p>\n<p>Or, do you mean that knowing itself is not entirely physical?</p>\n", "parentCommentId": "XcKKiNsqJdAdxJtyT", "user": {"username": "MichaelStJules"}}, {"_id": "sgyMj9iFJiqpLsG6h", "postedAt": "2023-09-02T09:57:48.826Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote><p>If you accept the causal closure of the physical</p></blockquote><p>I think the causal closure of the physical is very, very likely, given the evidence. I do not accept it as axiomatic. But if it turns out that it implies illusionism, i.e. that it implies the evidence does not exist, then it is self-defeating and should be rejected.</p><blockquote><p>Or, do you mean that knowing itself is not entirely physical?</p></blockquote><p>I am referring to my phenomenology, not (what I believe to be) the corresponding behavioral dispositions. E.g. so far as I know my visual field can be simultaneously all blue and all dark, but never all blue and all red. We have a clear path towards explaining why that would be true, and vague hints that it might be possible to explain why, given that it's true, I can think the corresponding thoughts and say the corresponding words. &nbsp;But explaining how I can make that judgement is <i>not</i> an explanation of why I have visual qualia to begin with.&nbsp;</p><p>Whether these are also physical in some broader sense of the word, I can't say.</p>", "parentCommentId": "Zb225MQG4tJmDvQFd", "user": {"username": "prisonpent"}}, {"_id": "4HekSKEZ8Zc4Ki5ng", "postedAt": "2023-09-02T12:41:22.640Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>That all sounds approximately right but I'm struggling to see how it bears on this point:</p><blockquote><p>If we want expected-utility-maximisation to rule anything out, we need to say something about the objects of the agent's preference. And once we do that, we can observe violations of Completeness.</p></blockquote><p>Can you explain?</p>", "parentCommentId": "LJCZS5FSjhbjt6ySb", "user": {"username": "elliottthornley"}}, {"_id": "z7mbsWFEjwve7c3cT", "postedAt": "2023-09-02T15:44:43.083Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>We all agree that you should get utility. &nbsp;You are pointing out that FDT agents get more utility. &nbsp;But once they are already in the situation where they've been created by the demon, FDT agents get less utility. &nbsp;If you are the type of agent to follow FDT, you will get more utility, just as if you are the type of agent to follow CDT while being in a scenario that tortures FDTists, you'll get more utility. &nbsp;The question of decision theory is, given the situation you are in, what gets you more utility--what is the rational thing to do. &nbsp;Eliezer's turns you into the type of agent who often gets more utility, but that does not make it the right decision theory. &nbsp;The fact that you want to be the type of agent who does X doesn't make doing X rational if doing X is bad for you and not doing X is rewarded artificially. &nbsp;</p><p>Again, there is no dispute about whether on average one boxers or two boxers get more utility or which kind of AI you should build.&nbsp;</p>", "parentCommentId": "tQsyZ65Gv9mg7zrCM", "user": {"username": "Omnizoid"}}, {"_id": "hxBRPzSuFChbJ6AMi", "postedAt": "2023-09-02T21:27:49.483Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I think this response misses the woods for the trees here. It's true that you can fit <i>some</i> utility function to behaviour, if you make a more fine-grained outcome-space on which preferences are now coherent etc. But this removes basically all of the predictive content that Eliezer etc. assumes when invoking them.</p><p>In particular, the use of these theorems in doomer arguments absolutely does implicitly care about \"internal structure\" stuff - e.g. one major premise is that non-EU-maximising AI's will reflectively iron out the \"wrinkles\" in their preferences to better approximate an EU-maximiser, since they will notice that their e.g. incompleteness leads to exploitability. The OP argument shows that an incomplete-preference agent will be inexploitable <i>by its own lights. </i>The fact that there's some completely different way to refactor the outcome-space such that from the outside it looks like an EU-maximiser is just irrelevant.</p><p>&gt;If describing a system as a decision theoretic agent is that cumbersome, it's probably better to look for some other model to predict its behaviour</p><p>This also seems to be begging the question - if I have something I think I can describe as a non-EU-maximising decision-theoretic agent, but which has to be described with an incredibly cumbersome utility function, why do we not just conclude that EU-maximisation is the wrong way to model the agent, rather than throwing out the belief that is should be modelled as an agent. If I have a preferential gap between A and B, and you have to jump through some ridiculous hoops to make this look EU-coherent ( \"he prefers [A and Tuesday and feeling slightly hungry and saw some friends yesterday and the price of blueberries is &lt;\u00a31 and....] to [B and Wednesday and full and at a party and blueberries &gt;\u00a31 and...]\" ), seems like the correct conclusion is not to throw away me being a decision-theoretic agent, but me being well-modelled as an EU-maximiser</p><p>&gt;The less coherent and smart a system acts, the longer the utility function you need to specify...</p><p>These are two very different concepts? (Equating \"coherent\" with \"smart\" is again kinda begging the question). Re: coherence, it's just tautologous that the more complexly you have to partition up outcome-space to make things look coherent, the more complex the resulting utility function will be. Re: smartness, if we're operationalising this as \"ability to steer the world towards states of higher utility\", then it seems like smartness and utility-function-complexity are by definition independent. Unless you mean more \"ability to steer the world in a way that seems legible to us\" in which case it's again just tautologous</p>", "parentCommentId": "LJCZS5FSjhbjt6ySb", "user": {"username": "keith_wynroe"}}, {"_id": "zMGSkijf4eGn8bSvy", "postedAt": "2023-09-05T12:22:37.556Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I don't feel particularly good that the various concerns about this mod decision were not, as far as I can tell, addressed by mods. I accept that this decision has support from some people, but a number of people have also expressed concern. My own concern got 69 upvotes and 24 agree votes. Nathan, Linch, and Sphor all raise concerns too. I think a high bar should be set for mod action against critiques of EA leaders, but I also think that mods would ideally be willing to engage in discussion about this sort of action (even if only to provide reassurance that they generally support appropriate critique but that they feel this instance wasn't appropriate for X, Y and Z reasons).</p><p>ETA: Lizka has now written a thoughtful and reflective response <a href=\"https://forum.effectivealtruism.org/posts/ZS9GDsBtWJMDEyFXh/eliezer-yudkowsky-is-frequently-confidently-egregiously?commentId=z5KuXKK4HjCfkJz2T\">here</a> (and also explained why it took a while for any such response to be written).</p>", "parentCommentId": "sCtL6gPDk5M3YjGjF", "user": null}, {"_id": "z5KuXKK4HjCfkJz2T", "postedAt": "2023-09-06T00:11:12.458Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks for the pushback. Writing some notes, and speaking only for myself (I don\u2019t know what the other moderators think).&nbsp;</p><ol><li><strong>I think my note</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1mfocb4bop5j\"><sup><a href=\"#fn1mfocb4bop5j\">[1]</a></sup></span><strong>&nbsp;about Personal Blog-ing this post was unambiguously bad.</strong> In practice, the decision was made because I was trying to avoid delaying the comment, someone proposed (in the moderator slack) that this post was only loosely connected to doing good effectively and should be in Personal Blog, and I didn\u2019t question it further.&nbsp;</li><li><strong>I think we probably shouldn\u2019t have moved the post to Personal Blog</strong>, but I\u2019m not totally sure. I\u2019ve flip-flopped a bit about this. (I just moved the post back, although I think this doesn\u2019t change anything at this point.) I think the bigger error is that the distinction is so messy \u2014 I had written a doc trying to clarify things last year (it was mostly focused on whether productivity-hack-style posts should go on the Frontpage or not), and we thought a bit about it when we added the Community section, but this hasn\u2019t been resolved. I think we probably should have prioritized clearing this up earlier, but I\u2019m once again unsure.&nbsp;</li><li>Relatedly,&nbsp;<strong>I don\u2019t think moving the post to \u201cPersonal Blog\u201d substantially lowered the post\u2019s visibility&nbsp;</strong>(I\u2019m not sure it did anything except put a little \u201cpersonal blog\u201d icon on it), given that the post is also in the Community section. If it were&nbsp;<i>not</i> a Community post, then I think logged-out users wouldn\u2019t see it on the Frontpage, but I think nothing really changes for Community posts. (Not totally confident in this; I'll check with the rest of the Online Team.)</li><li>I agree that in an ideal world, I (or someone else from the moderation team) would have responded sooner to the replies on my comment. But I was traveling, very busy, and didn\u2019t think the visibility of the post was actually lowered (see #3, and see the number of comments on the post), so I didn\u2019t prioritize this issue. (I also suspect that this got ughy, although ughiness mainly pushed back my response time today, when I came back to the thread and saw newer comments.) I don\u2019t know if I endorse the trade-offs I made, but it\u2019s hard for me to tell.</li><li>Setting aside Personal Blog \u2014 re: the fact that this is criticism of an influential figure in EA, and moderators should avoid responding to posts like that. I think it\u2019s very important to protect criticism, but I also think the moderators are currently&nbsp;<i>over</i>-correcting for this kind of consideration a bit (see e.g.&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/AAZqD2pvydH7Jmaek/lorenzo-buonanno-s-shortform#CDPS2JQKziWsWg73D\"><u>this</u></a>), and I honestly think that I want to discourage the kinds of rhetorical attacks that I saw in this post. I want to protect whistleblowing, red-teaming, disagreement, serious critical engagement with the quality of someone\u2019s work, etc., but I don\u2019t want to encourage the sense that if you frame your post as criticism, then it will be featured even if it is inflammatory and misleading.&nbsp;</li></ol><p>(I\u2019m still swamped and traveling, so might continue to be slow to respond.)</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1mfocb4bop5j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1mfocb4bop5j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cThe moderators have decided to move the post to Personal Blog \u2014 the connection to EA and doing good better is not that clear\u201d</p></div></li></ol>", "parentCommentId": "FHNp3eKvyCvxHjv2u", "user": {"username": "Lizka"}}, {"_id": "rtXRE4nZJc5ZpTHK2", "postedAt": "2023-09-06T00:12:35.780Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks for commenting \u2014 I agree with your main point, and wrote more <a href=\"https://forum.effectivealtruism.org/posts/ZS9GDsBtWJMDEyFXh/eliezer-yudkowsky-is-frequently-confidently-egregiously?commentId=z5KuXKK4HjCfkJz2T\">here</a>.&nbsp;</p>", "parentCommentId": "cj9LzybyygjeDPinq", "user": {"username": "Lizka"}}, {"_id": "yuap6oShS3T4sGSef", "postedAt": "2023-09-06T00:13:59.608Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks for this comment \u2014 I left a longer reply <a href=\"https://forum.effectivealtruism.org/posts/ZS9GDsBtWJMDEyFXh/eliezer-yudkowsky-is-frequently-confidently-egregiously?commentId=z5KuXKK4HjCfkJz2T\">here</a>.&nbsp;</p>", "parentCommentId": "Jr2ne5oGZh3AkDFMb", "user": {"username": "Lizka"}}, {"_id": "am9bGTA3LTxuX3Q3p", "postedAt": "2023-09-06T00:15:50.024Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks for this comment \u2014 I left a longer comment <a href=\"https://forum.effectivealtruism.org/posts/ZS9GDsBtWJMDEyFXh/eliezer-yudkowsky-is-frequently-confidently-egregiously?commentId=z5KuXKK4HjCfkJz2T\">here</a>. In brief, I hadn't thought very hard about the decision to move the post to Personal Blog, and was in fact mostly focused on the rhetorical/inflammatory aspects of the post, and only briefly considered the strength of its relevance to EA.</p>", "parentCommentId": "jRjvHwdCFkDcvfQPm", "user": {"username": "Lizka"}}, {"_id": "S6w8ixvkbL9fye5dG", "postedAt": "2023-09-06T00:16:49.548Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks for editing your post.&nbsp;</p><p>I've moved the post back to Frontpage (although I don't think this changes much) \u2014 see <a href=\"https://forum.effectivealtruism.org/posts/ZS9GDsBtWJMDEyFXh/eliezer-yudkowsky-is-frequently-confidently-egregiously?commentId=z5KuXKK4HjCfkJz2T\">this comment</a>. We don't generally move posts to Frontpage if the authors mark them as Personal Blog themselves. Do you want us to move this post back?&nbsp;</p>", "parentCommentId": "TRgiQKiNgR5tKAwEA", "user": {"username": "Lizka"}}, {"_id": "rCCtoE5cxsuNt6sf4", "postedAt": "2023-09-06T17:26:46.476Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks for sharing your reasoning, openly acknowledging a mistake and explaining how it happened.</p><p>Note: the below is an observation of a structural problem, rather than any individual. person Moderation is not an easy job and I do believe that the Forum mods are doing their best.&nbsp;</p><p>Overall it sounds like the Forum team may not have enough capacity to adequately deal with issues like this (according to your description it sounds like despite traveling and being busy, you were ultimately the person responsible for this).&nbsp;</p><p>This could result in a sub-optimal situation that decisions like this are either delayed, or made quickly (with a higher chance of mistakes). I think this is bad because the Forum is actively used by hundreds of community members, and time spent critiquing mod decisions is valuable time that isn't being spent on object-level issues.&nbsp;</p><p>In my opinion, it seems like it should be higher priority for the Forum team to expand the number of dedicated moderators who are \"on call\" to prevent situations like this in the future.</p><p>Some notes on mod capacity:</p><ul><li>From my understanding the forum has hired some paid moderators in the past year or two, but it seems like it may not be sufficient (possibly because of a <a href=\"https://forum.effectivealtruism.org/posts/GqvSoerAurEbiP9GE/moderator-appreciation-thread\">increase</a> in forum usage over the same time period)</li><li>I am also aware that the Forum is <a href=\"https://forum.effectivealtruism.org/posts/auhi3JoiqGhi5PqnQ/ama-we-re-the-forum-team-and-we-re-hiring-ask-us-anything#___We_re_hiring_for_a_Content_Specialist___Deadline_extended__apply_by_July_26_\">trying to hire another Content Specialist</a>, although it is unclear whether they are replacing Lizka or adding more capacity.&nbsp;</li></ul>", "parentCommentId": "z5KuXKK4HjCfkJz2T", "user": {"username": "Grumpy Squid"}}, {"_id": "gKwrRYqFcBNPyavgu", "postedAt": "2023-09-06T18:35:34.160Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Moderation issues are annoying (and I agree they are too quick to go after disagreeable-but-insightful people) but adding new dedicated paid moderators seems quite expensive. Most of the time there isn't a huge issue so their time would be wasted, and even when there was an issue you don't get certainty of improved performance - the new people might sometimes have worse ideas than the old guard. My guess (?) is the EA forum is already an outlier on the admin-hours / user-hours ratio.</p>", "parentCommentId": "rCCtoE5cxsuNt6sf4", "user": {"username": "Larks"}}, {"_id": "6nZ3ArmXJqjDFCWbz", "postedAt": "2023-09-06T19:55:31.578Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I think having adding something like 1 FTE or 2 x 0.5 FTE moderators wouldn't be that expensive - would add ~5% to the Forums' overall budget (currently $2M per year per a recent comment). Onboarding and recruiting would take some time, but the process for hiring moderators (AFAIK) is less time-consuming if they are in a contract role.&nbsp;</p><p>It's true that new moderators could make worse decisions, but they could also be trained by existing moderators, read up on past instances of moderation that worked / didn't, and initially run decisions by more experienced mods to reduce the chance of decreasing quality. It seems like moderators who joined in 2022 did a <a href=\"https://forum.effectivealtruism.org/posts/GqvSoerAurEbiP9GE/moderator-appreciation-thread\">pretty good job</a>, at least Forum leadership's standards.&nbsp;</p>", "parentCommentId": "gKwrRYqFcBNPyavgu", "user": {"username": "Grumpy Squid"}}, {"_id": "ivDgCGo3pYg7rzZ6s", "postedAt": "2023-09-06T20:03:17.447Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>RE outlier - Do you mean an outlier in that there are more admin hours put in than other places?&nbsp;</p><p>I don't think that is true, at least from my impression of a couple other places, but this is a weak impression.&nbsp;</p><p>I would make the case that we probably don't want to compare the Forum to most other online communities, because unlike in other places, people are writing &amp; sharing substantive research and trying to, in some sense, do work. Of course, there is a community / social element to it as well, but I think there is a case to see the Forum as more than just that. As a result, I think it's okay for the mod team to be an outlier.&nbsp;</p><p>I'll also say that in general, I believe CEA as an organization undervalues / underinvests in infrastructural investments for the EA movement and community (e.g. the Groups team, Events team and Community health had been chronically understaffed until early 2022. The Forum only had ~3 FTE until 2021 and only had capacity to maintain rather than build new features. I'd argue the Community Health team is still very understaffed relative to their remit.)</p>", "parentCommentId": "gKwrRYqFcBNPyavgu", "user": {"username": "Grumpy Squid"}}, {"_id": "6ReDFugWRbfow7Pdv", "postedAt": "2023-09-06T21:08:09.406Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>What do you think CEA over-invests in? If you take away Online, Groups, Events and CH as all undervalued there's not much of CEA left.</p>", "parentCommentId": "ivDgCGo3pYg7rzZ6s", "user": {"username": "Larks"}}, {"_id": "mtYF4b5xASP74yLmh", "postedAt": "2023-09-06T21:26:59.235Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Clarification: I think they underinvest in staff specifically, and sometimes overinvest in specific programs (e.g. conference spend in 2022, maybe the university groups program from 2021-2022). But overall I think they just underinvest in these things across the board, despite having had the capacity to raise more funds / hire more.&nbsp;</p><p>If they were unable to raise more funds (which I'm very skeptical of since it appears they haven't actively fundraised non-OP funders), then I'd have wanted them to scale down to have fewer projects, and put more resources into fewer programs. &nbsp;</p>", "parentCommentId": "6ReDFugWRbfow7Pdv", "user": {"username": "Grumpy Squid"}}, {"_id": "27JWL7zb6CfQms4ys", "postedAt": "2023-09-07T00:40:34.989Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>I appreciate the thoughtful reply. However, I don't agree with 5, which I take to be the most important claim in this reply.</p><p>Side comment: my claim isn't that moderators should avoid responding to posts that criticise prominent figures in EA. But my claim is that moderators should be cautious about acting in ways that discourage critique. I think this creates a sort of default presupposition that formal mod action should not be taken against critiques that include substantive discussion, as this one did.</p><p>I don't particularly find the comparison to the \"modest proposal\" post fruitful, because the current post just seem like a very different categories of post. I think it's perfectly possible to not take action on substantive criticisms of leaders while taking action on \"modest proposal\" style posts.</p><p>While it might be reasonable to want to discourage the sort of rhetorical attacks seen in this post if all else were equal, I don't think all else was equal in this case. And while I agree that \"criticism\" of leaders shouldn't permit all sins, the post seemed to me to have enough substantive discussion that it shouldn't be grouped into the general category of \"inflammatory and misleading\".</p>", "parentCommentId": "z5KuXKK4HjCfkJz2T", "user": null}, {"_id": "56GoA7DW37SvZyski", "postedAt": "2023-09-07T11:11:33.412Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Also, just to say: I think these judgement calls are easy to make in the abstract, but I'm glad I don't have to make them quickly in reality when they actually have implications.<br><br>I do think the wrong call was made here, but I also think the mod team acts in good faith and is careful and reflective in their actions. I am discussing things here because I think this is how we can collectively work towards a desirable set of moderation norms. I am not mentioning these things to criticise the mod team as individuals or indeed as a group.</p>", "parentCommentId": "27JWL7zb6CfQms4ys", "user": null}, {"_id": "CB3YCSr8ZGejdhhwd", "postedAt": "2023-09-07T12:29:42.232Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<blockquote>\n<p>However, if someone claims to be the expert on physics, philosophy, decision theory, and AI, and then they turn out to be very confused about philosophy, then that is a mark against their reasoning abilities.</p>\n</blockquote>\n<p>It's more effective to show they are confused about maths, physics and AI, since it is much easier to establish truth/consensus in those fields.</p>\n", "parentCommentId": "ri4xGEekZG9qLSBXB", "user": {"username": "TAG"}}, {"_id": "KrcvbSQEbqhCzodTo", "postedAt": "2023-09-08T12:06:06.372Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Thanks for the honesty Lizka. I appreciate it.&nbsp;</p><p>I'm very disappointed about the low priority the mod team assigns to being responsive to and engaging with critical feedback about their decisions from forum users. It's very surprising to me that in this situation, with all the substantive and popular comments and votes pushing back on a potentially consequential decision and multiple people following up publicly a week later about their disappointment that the situation was left unaddressed, you are undecided on whether this situation needed addressing sooner. I find myself less interested in this forum due to this reason.&nbsp;</p>", "parentCommentId": "z5KuXKK4HjCfkJz2T", "user": null}, {"_id": "6FEsf7fgkKkS69tfw", "postedAt": "2023-09-08T15:51:33.769Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Writing only my personal perspective on the moderation team's approach, I haven't checked this with other moderators or advisors</p><blockquote><p>my claim isn't that moderators should avoid responding to posts that criticise prominent figures in EA. But my claim is that moderators should be cautious about acting in ways that discourage critique.</p></blockquote><p>My view is that all moderators agree with this! There are just many reasonable places to draw this line, though, and both different users and different moderators have different preferences and perspectives on what the bar should be and what counts as \"prominent figures in EA\".</p><p>In the past, we have received feedback from some users that we should have intervened in the opposite direction in other threads about prominent figures.</p>", "parentCommentId": "27JWL7zb6CfQms4ys", "user": {"username": "Lorenzo Buonanno"}}, {"_id": "aNLYaksGHLzYekYvn", "postedAt": "2023-09-08T16:08:54.223Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Writing only my personal perspective. I haven't checked this with other moderators or advisors.</p><blockquote><p>Overall it sounds like the Forum team may not have enough capacity to adequately deal with issues like this (according to your description it sounds like despite traveling and being busy, you were ultimately the person responsible for this). [...] In my opinion, it seems like it should be higher priority for the Forum team to expand the number of dedicated moderators who are \"on call\" to prevent situations like this in the future.</p></blockquote><p>You might be happy to hear that this already happened to a significant amount!</p><p>There are now <a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum#The_moderation_team\">six</a> active moderators, plus advisors, which is ~2x as many as there were at some points. Three of the active moderators joined in August, I think ~three weeks before this post, and the content specialist role you linked to starts with \"to work with me (Lizka)\", so I don't think that she's looking for a replacement.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefb3amd3gdb3m\"><sup><a href=\"#fnb3amd3gdb3m\">[1]</a></sup></span></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnb3amd3gdb3m\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefb3amd3gdb3m\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I have no insider info, just going by public posts.</p></div></li></ol>", "parentCommentId": "rCCtoE5cxsuNt6sf4", "user": {"username": "Lorenzo Buonanno"}}, {"_id": "kwkAvKYuX5mEErXPR", "postedAt": "2023-09-08T16:25:45.883Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Writing only my personal perspective. I haven't checked this with other moderators or advisors.</p><blockquote><p>adding something like 1 FTE or 2 x 0.5 FTE moderators wouldn't be that expensive</p></blockquote><p>I think an important cost would be the opportunity cost for what those moderators could be doing.</p><p>For me personally, the<a href=\"https://forum.effectivealtruism.org/posts/9t7St3pfEEiDsQ2Tr/nailing-the-basics-theories-of-change\"> theory of change</a> for spending more time on moderation is often not that clear. My personal theory of change is that the main value I provide via moderation is to save time/energy for Lizka and JP to focus a bit more on <a href=\"https://forum.effectivealtruism.org/s/HqxvGsczdf4yLB9FG\">projects that I think are extremely valuable</a>. (Edit: This is just my personal view! I don't work for CEA, and I think <a href=\"https://forum.effectivealtruism.org/posts/hvrwNmXtWRgHGwzaz/ea-forum-content-and-moderator-positions#Why_moderation_\">they disagree with this</a>!)</p><blockquote><p>seems like moderators who joined in 2022 did a <a href=\"https://forum.effectivealtruism.org/posts/GqvSoerAurEbiP9GE/moderator-appreciation-thread\">pretty good job</a></p></blockquote><p>As one of these mods, I think I also made some <a href=\"https://forum.effectivealtruism.org/posts/AAZqD2pvydH7Jmaek/lorenzo-buonanno-s-shortform#CDPS2JQKziWsWg73D\">pretty</a> <a href=\"https://forum.effectivealtruism.org/posts/FZFzqPYpTpGGRhyrj/does-ea-get-the-best-people-hypotheses-call-for-discussion?commentId=o3mahDSh4wuHTvsXh\">clear</a> <a href=\"https://forum.effectivealtruism.org/posts/iJSYZJJrLMigJsBeK/lizka-s-shortform?commentId=ZARhAgnMt7joZkbxb\">mistakes</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdijiqtaonlw\"><sup><a href=\"#fndijiqtaonlw\">[1]</a></sup></span>, even one year into this, that I think more experienced mods wouldn't have made. I think the new mods went through a better selection process, though, so I'm optimistic that it will take less time for them to make better decisions.</p><p>Tangentially related to this point, I think 99% of the moderation action on this forum comes from users (via voting, commenting, and reporting posts). I think that's how it should be, and I'm really impressed by how well users of this forum moderate discussions, compared to e.g. serious subreddits, Twitter spheres, or Hacker News.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndijiqtaonlw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdijiqtaonlw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I was also the moderator who pushed the most to move this to personal blog, as I (wrongly) didn't see a strong connection between this post and doing good better.</p></div></li></ol>", "parentCommentId": "6nZ3ArmXJqjDFCWbz", "user": {"username": "Lorenzo Buonanno"}}, {"_id": "FDZ6zp4QF4uBgQ4Mx", "postedAt": "2023-09-08T16:57:53.237Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>My personal thoughts, as I was the mod who most pushed to move this to personal blog<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaxtyg1sbb6p\"><sup><a href=\"#fnaxtyg1sbb6p\">[1]</a></sup></span>. I haven't checked this with other mods:</p><ul><li>My main actionable general takeaway from this incident is that we should try to write longer and more detailed notes when taking any moderation action. We should treat moderation notes as <a href=\"https://en.wikipedia.org/wiki/High-context_and_low-context_cultures\">low context</a> communication, and we should try to expand more on things like \"violates norms\" or \"is not clearly related to doing more good\". I'm very guilty of this, e.g. I think this was a core mistake <a href=\"https://forum.effectivealtruism.org/posts/FZFzqPYpTpGGRhyrj/does-ea-get-the-best-people-hypotheses-call-for-discussion?commentId=o3mahDSh4wuHTvsXh\">here</a> and <a href=\"https://forum.effectivealtruism.org/posts/iJSYZJJrLMigJsBeK/lizka-s-shortform?commentId=ZARhAgnMt7joZkbxb\">here</a>. In particular, we should always try to make it clear that criticism is welcome on the forum.</li></ul><p>My less actionable and less general thoughts on this specific case:</p><ul><li>I strongly believe that this decision was <strong>not</strong> a blunder, even if it probably was a mistake:<ul><li>As many people agreed than disagreed with the moderation comment (It was 21 agreed to 18 disagreed as of 3 days ago. After the post edits and recent discussion it's 22 to 23. People might be biased to agree, but I don't think more than to disagree in this specific case.)</li><li>The author agreed with the decision</li><li>People who agree have no reason to comment and are less likely to see the moderation comment in the first place</li></ul></li><li>In this case, there were several considerations, which made things messy. From my perspective, this post as posted was somewhat borderline on these axes, and I can see reasonable and contradicting perspectives on:&nbsp;<ul><li>The post relevance to doing more good</li><li>The post breaking forum norms (i.e. the insults that have since been edited)</li><li>Yudkowsky relationship with EA and if that raises or lowers the bar for acceptable criticism. As an influential voice, we should allow more criticism; as a critic of large parts of EA, like AI labs and animal welfare, we should make sure criticism is kind and doesn't discourage people from criticizing EA.</li></ul></li><li>I think, in retrospect, the ideal action might have been to take mod action in the form of writing a comment asking the author to edit the post (as they did) to keep the good parts and reduce the insults (and maybe clarify the practical relevance to doing more good).</li><li>I think the main reasons why we didn't reply earlier to comments are that:<ul><li>The poster agreed with the decision, so there wasn't much to change</li><li>Moving the post to personal blog for whatever reason didn't remove it from the frontpage, even for logged out users (idk if this is a bug, but it just showed a little icon next to the post, which didn't seem important to fix)</li><li>It's obvious to moderators that criticizing anyone is ok (while following norms) so we <a href=\"https://en.wikipedia.org/wiki/Illusion_of_transparency\">didn't feel the need to spell it out</a></li><li>I weakly wanted to reach more of a consensus in the mod team, and hear the perspectives of all moderators</li></ul></li><li>I was wrong in not seeing any relevance to EA. EY is much more relevant to EA for many more users than I would have thought, and <a href=\"https://forum.effectivealtruism.org/posts/EFpKcbaZNyZNk4qWD/jonas-vollmer-s-shortform?commentId=hjqzE7ki2hdYwjZZB\">social reality</a> is much more important than I thought, and arguably is a core reason for the community section.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxrauxptbu6q\"><sup><a href=\"#fnxrauxptbu6q\">[2]</a></sup></span></li><li>I feel that the \"silent majority\" that reads but doesn't write on the forum wants <strong>relatively</strong> more moderation than people who write lots of comments, so we should weakly keep that in mind when getting feedback in terms of \"how much to moderate\" (but the feedback in terms of \"how to moderate\" is very useful)</li><li>We should probably have replied earlier, even if we didn't reach a consensus on whether it was the right call or not, potentially just to surface that we were not sure it was.</li><li>Mostly unrelated to the above, but I really liked some of the comments in this thread. I am grateful for the standards that many commenters hold themselves to when posting, and the time they invest in sharing their expertise and thoughtful perspectives even in threads that would naturally have a tendency to devolve into fights.</li></ul><p>Apologies for writing this quickly<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref71i25fopur3\"><sup><a href=\"#fn71i25fopur3\">[3]</a></sup></span>, and again I want to emphasize that this is just my personal perspective, and I haven't asked for feedback from other mods or advisors.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnaxtyg1sbb6p\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefaxtyg1sbb6p\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As I (wrongly) didn't see a strong connection between this post and doing good better</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxrauxptbu6q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxrauxptbu6q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I might have overreacted because I have seen people loving to hate on Yudkowsky for &gt;10 years. There used to be a subreddit dedicated to it. I haven't found comments on either side of those discussions to be particularly true, necessary or kind. I would want this forum to have less of that, but this is my personal view and shouldn't have influenced mod action</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn71i25fopur3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref71i25fopur3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'm writing this from EAGxBerlin</p></div></li></ol>", "parentCommentId": "z5KuXKK4HjCfkJz2T", "user": {"username": "Lorenzo Buonanno"}}, {"_id": "PgYjH8LCubqR6QdGE", "postedAt": "2023-09-08T17:00:55.368Z", "postId": "ZS9GDsBtWJMDEyFXh", "htmlBody": "<p>Hi sphor,</p><p>I'm sorry about this, especially that this worsened your experience on the forum, I quickly wrote some reasons why it took us so long <a href=\"https://forum.effectivealtruism.org/posts/ZS9GDsBtWJMDEyFXh/eliezer-yudkowsky-is-frequently-confidently-egregiously?commentId=FDZ6zp4QF4uBgQ4Mx\">here</a></p>", "parentCommentId": "KrcvbSQEbqhCzodTo", "user": {"username": "Lorenzo Buonanno"}}]