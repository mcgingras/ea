[{"_id": "w7gKumkRYj4JDYqqk", "postedAt": "2015-03-26T18:01:15.329Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<p>Oh yes, thank you for this article! Since 2011-ish I\u2019ve gradually discovered these various mental coping strategies. I wish I had had a summary like yours back then!</p>\n<p>The scene from Schindler\u2019s List is powerful. Back in 2011 or maybe 2012 I often spent sleepless hours in bed crying and wondering why I\u2019m not doing more than I am, why I\u2019m the only one who has made these moral inferences, whether I\u2019m crazy or overlooking something, and how I\u2019m supposed to do it all alone. It would take me till early 2014 to find out about EA and find out that there are renowned philosophers who\u2019ve made the same inferences, so I couldn\u2019t be all that crazy.</p>\n<p>What has helped me a lot is a specific kind of compartmentalization. I find it intellectually painful to compartmentalize, so I\u2019m very hesitant to do it purposefully, but after a longer deliberative process I\u2019ve found that my strong emotions paralyzed me more than they motivated me, and that I was sufficiently motivated anyway, so instead of wasting time crying, I trained thinking about things without feeling them. Or \u201ctrained\u201d is an exaggeration, it\u2019s more that it took me a tiny bit of effort to bring my empathy to this paralyzing emotional level in the first place, so that I could just avoid putting in this bit of effort. It\u2019s still pretty hard when I see something visually, but it\u2019s easier when I just think about it.</p>\n<p>These emotions are not to scale anyway, and I don\u2019t think I\u2019m atrophying my ability to empathize doing this, so I think this is a kind of compartmentalization that will not negatively impact my decision-making.</p>\n<p>As Bertrand Russell said:</p>\n<blockquote>\n<p>Three passions, simple but overwhelmingly strong, have governed my life: the longing for love, the search for knowledge, and unbearable pity for the suffering of [sentient beings]. These passions, like great winds, have blown me hither and thither, in a wayward course, over a great ocean of anguish, reaching to the very verge of despair.</p>\n</blockquote>\n<p>It\u2019s just too unbearable to not suppress it and still function.</p>\n<p>There\u2019s also the factor that many EAs who do earning to give still keep savings. I think this also gives them the ability to give large amounts with regularity sustainably. It might also give them the ability to be more enterprising in their job search and thus better at maximizing their income. There are a lot of people who already feel a need to donate more but don\u2019t dare to out of self-preservation-related fears. Those may be in some cases disproportionate, but self-preservation is probably an instinct that is easier to pacify than to vanquish.</p>\n<p>Conversely, there is also the problem of coping with other people who are very strongly compartmentalized. There, <a href=\"http://effective-altruism.com/ea/fh/six_ways_to_get_along_with_people_who_are_totally/2qw\">Robert\u2019s and my exchange on another article</a> seems relevant.</p>\n<p>I\u2019ve also written another introduction to EA article for people with low compartmentalization, which I haven\u2019t posted here yet. I\u2019ll probably do that one of these days.</p>\n<p>PS: You copied a lot of in-line style. The article will look prettier when you strip it.</p>\n", "parentCommentId": null, "user": {"username": "Telofy"}}, {"_id": "g7NrnjCZNkCYjv9bu", "postedAt": "2015-03-26T19:36:01.105Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<p>Thanks for writing this, Lukas.</p>\n", "parentCommentId": null, "user": {"username": "RyanCarey"}}, {"_id": "hXtHtDYx8BWWNZLP2", "postedAt": "2015-03-26T21:29:16.140Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<p>Thanks, great points! I got some help and managed to fix the layout. </p>\n", "parentCommentId": "w7gKumkRYj4JDYqqk", "user": {"username": "Lukas_Gloor"}}, {"_id": "oj6TXHyTMfJeSBX67", "postedAt": "2015-03-27T08:18:46.203Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<p>Compassion is also helpful - sometime visualising the suffering of the world and trying to identify with it in a cleaner way than involving guilt can generate some really powerful energy in the right direction. Doing this is the only way I can keep going while recognising that I'm a bottom 1-10% performer of the people that I've met in the movement - which is quite ego-depleting!!</p>\n<p><a href=\"http://old-shambhala.shambhala.org/teachers/pema/tonglen1.php\">Tonglen</a> is a Tibetan buddhist practice that seeks to build compassion in this way.</p>\n", "parentCommentId": "w7gKumkRYj4JDYqqk", "user": {"username": "tomstocker"}}, {"_id": "ssyTdHe6rMRa7fQKZ", "postedAt": "2015-03-27T09:00:31.282Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<p>Yeah, I\u2019ve been hard pressed to find any use for guilt. There is this justice-related defense against helping (\u201cWhy am I supposed to help if [random other person] doesn\u2019t!\u201d), and showing someone how they profit from a system of exploitation can break thought that. But otherwise empathy/compassion is my central motivation. I know that this unbearable suffering exists. I just don\u2019t let it take the emotional shape that overwhelms and paralyzes me with its sheer intensity.</p>\n<p>I hope the ego depletion doesn\u2019t keep you from aspiring to become a top 1\u201310% performer in the movement?</p>\n", "parentCommentId": "oj6TXHyTMfJeSBX67", "user": {"username": "Telofy"}}, {"_id": "9oxPxRWkrEWmGERaW", "postedAt": "2015-03-27T10:15:22.125Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<p>I'm part of the target audience, I think, but this post isn't very helpful to me. Mistrust of arguments which tell me to calm down may be a part of it, but it seems like you're looking for reasons to excuse caring for other things than effective altruism, rather than weighing the evidence for what works better for getting EA results.</p>\n<p>Your &quot;two considerations&quot;,</p>\n<blockquote>\n<ol>\n<li>If you view EA as a possible goal to have, there is nothing contradictory about having other goals in addition.</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol>\n<li>Even if EA becomes your only goal, it does not mean that you should necessarily spend the majority of your time thinking about it, or change your life in drastic ways. (More on this below.)</li>\n</ol>\n</blockquote>\n<p>, look like a two-tiered defence against EA pressures rather than convergence on a single right answer on how to consider your goals. Maybe you mean that some people are 'partial EAs' and others are 'full EAs (who are far from highly productive EA work in willpower -space)', but it isn't very clear.</p>\n<p>Now, on 'partial EAs': If you agree that effective altruism = good (if you don't, adjust your EA accordingly, IMO), then agency attached to something with different goals is bad compared to agency towards EA. Even if those goals can't be changed right now, they would still be worse, just like death is bad even if we can't change it (yet (except maybe with cryonics)). If you are a 'partial EA' who feels guilty about not being a 'full EA', this seems like an accurate weighing of the relative moral values, only wrong if the guilt makes you weaker rather than stronger. Your explanation doesn't look like a begrudging acceptance of the circumstances, it looks almost like saying 'partial EAs' and 'full EAs' are morally equivalent.</p>\n<p>Concerning 'full EAs who are far from being very effective EAs in willpower -space&quot;, this triggers many alarm bells in my mind, warning of the risk of it turning into an excuse to <a href=\"http://lesswrong.com/lw/uh/trying_to_try/\">merely try</a>. You reduce effective effective altruists' productivity to a personality trait (and 'skills' which in context sound unlearnable), which doesn't match <a href=\"https://80000hours.org/career-guide/big-picture/dont-follow-your-passion/\">80,000hours' conclusion that people can't estimate well how good they are at things or how much they'll enjoy things before they've tried</a>.</p>\n<p>Your statement on compartmentalisation (and Ben Kuhn's original post) both just seem to assume that because denying yourself social contact because you could be making money itself is bad, therefore compartmentalisation is good. But the reasoning for this compartmentalisation - it causes happiness, which causes productivity - isn't (necessarily) compartmentalised, so why compartmentalise at all? Your choice isn't just between a delicious candy bar and deworming someone, it's between a delicious candy bar which empowers you to work to deworm two people and deworming one person. This choice isn't removed when you use the compartmentalisation heuristic, it's just hidden. You're &quot;freeing your mind from the moral dilemma&quot;, but <strong>that is exactly what evading cognitive dissonance is</strong>.</p>\n<p>I don't have a good answer. I still have an ugh field around making actual decisions and a whole bunch of stress, but this doesn't sound like it should convince anyone.</p>\n", "parentCommentId": null, "user": {"username": "Philip_W"}}, {"_id": "frj7RmvxPY635b5dN", "postedAt": "2015-03-27T10:42:29.505Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<p>other things stop that, like raw ability, health, resource and availability of time due to prior commitments, but thanks for the encouragement! </p>\n<p>Now its been written about I can see the comparison between these feelings and the feelings people often have of <em>'oh, well, there's no way I can compete with Bill Gates so why bother'.</em> I haven't let them affect me that much but now I can see that they shouldn't affect me at all / even come in to play quite clearly. Thank you Telofy! :)</p>\n", "parentCommentId": "ssyTdHe6rMRa7fQKZ", "user": {"username": "tomstocker"}}, {"_id": "gJRrfdvmXjt3pLhGc", "postedAt": "2015-03-27T17:32:09.526Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<p>Thanks for this feedback! You bring up a very important point with the danger of things turning into merely &quot;pretending to try&quot;. I see this problem, but at the same time I think many people are far closer to the other end of the spectrum. </p>\n<blockquote>\n<p>I'm part of the target audience, I think, but this post isn't very helpful to me. Mistrust of arguments which tell me to calm down may be a part of it, but it seems like you're looking for reasons to excuse caring for other things than effective altruism, rather than weighing the evidence for what works better for getting EA results.</p>\n</blockquote>\n<p>I suspect that many people don't really get involved in EA in the first place because they're on some level afraid that things will grow over their head. And I know of cases where people gave up EA at least partially because of these problems. This to me is enough evidence that there are people who are putting too much pressure on themselves and would benefit from doing it less. Of course, there is a possibility that a post like this one does more harm because it provides others with &quot;ammunition&quot; to rationalize more, but I doubt this would make much of a difference \u2013 it's unfortunately easy to rationalize in general and you don't need that much &quot;ammunition&quot; for it. </p>\n<blockquote>\n<p>Your &quot;two considerations&quot;, look like a two-tiered defence against EA pressures rather than convergence on a single right answer on how to consider your goals. </p>\n</blockquote>\n<p>That's what they are. I think there's no other criterion that make your goals the &quot;right&quot; ones other than that you would in fact choose these goals upon careful reflection. </p>\n<blockquote>\n<p>Maybe you mean that some people are 'partial EAs' and others are 'full EAs (who are far from highly productive EA work in willpower -space)', but it isn't very clear.</p>\n</blockquote>\n<p>Yes, that's what I meant. And I agree it's unclear because it's confusing that I'm talking only about 2) in all of what follows, I'll try to make this more clear. So to clarify, most of my post addresses 2), &quot;full EAs (who are far from highly productive in willpower-space)&quot;, and 1) is another option that I mention and then don't explore more because the consequences are straightforward. I think there's absolutely nothing wrong with 1), if your goals are different from mine then that doesn't necessarily mean you're making a mistake about your goals. I personally focus on suffering and don't care about preventing death, all else being equal, but I don't (necessarily) consider you irrational for doing so. </p>\n<blockquote>\n<p>Now, on 'partial EAs': If you agree that effective altruism = good (if you don't, adjust your EA accordingly, IMO), then agency attached to something with different goals is bad compared to agency towards EA. Even if those goals can't be changed right now, they would still be worse, just like death is bad even if we can't change it (yet (except maybe with cryonics)).</p>\n</blockquote>\n<p>I'm arguing within a framework of moral anti-realism. I basically don't understand what people mean by the term &quot;good&quot; that could do the philosophical work they expect it to do. A partial EA is someone who would refuse to self-modify to become more altruistic IF this conflicts with other goals (like personal happiness, specific commitments/relationships, etc). I don't think there's any meaningful and fruitful sense in which these people are doing something bad or making some sort of mistake, all you can say is that they're being less altruistic as someone with a 100%-EA goal, and they would reply: &quot;Yes.&quot; </p>\n<blockquote>\n<p>Concerning 'full EAs who are far from being very effective EAs in willpower -space&quot;, this triggers many alarm bells in my mind, warning of the risk of it turning into an excuse to merely try. You reduce effective effective altruists' productivity to a personality trait (and 'skills' which in context sound unlearnable), which doesn't match 80,000hours' conclusion that people can't estimate well how good they are at things or how much they'll enjoy things before they've tried.</p>\n</blockquote>\n<p>I accept that there's a danger that my post can be read as such, but that's not what I'm saying. Not all skills are learnable to the same extent, but of course there is also  a component to how much people try! And I would also second the advice that it's important to try things even if they seem very hard to do at first. But the thing is, some people have tried and failed and feel miserable about it, or even the thought of trying makes them feel miserable, so that certainly cannot be ideal because these people aren't being productive at that point. </p>\n<blockquote>\n<p>Your statement on compartmentalisation (and Ben Kuhn's original post) both just seem to assume that because denying yourself social contact because you could be making money itself is bad, therefore compartmentalisation is good. But the reasoning for this compartmentalisation - it causes happiness, which causes productivity - isn't (necessarily) compartmentalised, so why compartmentalise at all? Your choice isn't just between a delicious candy bar and deworming someone, it's between a delicious candy bar which empowers you to work to deworm two people and deworming one person. This choice isn't removed when you use the compartmentalisation heuristic, it's just hidden. You're &quot;freeing your mind from the moral dilemma&quot;, but that is exactly what evading cognitive dissonance is.</p>\n</blockquote>\n<p>Human brains are not designed to optimize towards a single goal. It can drive you crazy. For some, it works, for others, it probably does not. I'm not saying &quot;if you're stressed sometimes, do less EA stuff&quot;. Maybe being stressed is the lesser problem. My point is: &quot;If you're stressed to the point that the status quo is not sustainable, then change something and don't feel bad about it&quot;. </p>\n<p>To sum up, I'm aware that rationalizing is a huge danger \u2013 it always amazes me just how irrational people can become when they are protecting a cherished belief \u2013 but I think that there are certain people who really aren't in danger of setting their expectations too low, because they have a problem with doing the opposite. </p>\n", "parentCommentId": "9oxPxRWkrEWmGERaW", "user": {"username": "Lukas_Gloor"}}, {"_id": "bqtvHLer4eFdLRLNA", "postedAt": "2015-03-30T06:45:46.159Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<p>Thanks for replying. (note: I'm making this up as I go along. I'm forgoing self-consistency for accuracy).</p>\n<blockquote>\n<p>You bring up a very important point with the danger of things turning into merely &quot;pretending to try&quot;. I see this problem, but at the same time I think many people are far closer to the other end of the spectrum.</p>\n</blockquote>\n<p>Merely trying isn't the same as pretending to try. It isn't on the same axis as emotionally caring, it's the (lack of) agency towards achieving a goal. Someone who is so emotionally affected by EA that they give up is definitely someone who 'merely tried' to affect the world, because you can't just give up if you care in an agentic sense.</p>\n<p>What we want is for people to be emotionally healthy - not caring too much or too little, and with control over how affected they are - but with high agency. Telling people they don't need to be like highly agentic EA people affects both, and to me at least it isn't obvious if you meant that people should still try their hardest to be highly agentic but merely not beat themselves up over falling short.</p>\n<blockquote>\n<blockquote>\n<p>Your &quot;two considerations&quot;, look like a two-tiered defence against EA pressures rather than convergence on a single right answer on how to consider your goals.</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<p>That's what they are. I think there's no other criterion that make your goals the &quot;right&quot; ones other than that you would in fact choose these goals upon careful reflection.</p>\n</blockquote>\n<p>Whose &quot;right&quot; are we talking about, here? If it's &quot;right&quot; according to effective altruism, that is obviously false: someone who discovers they like murdering is <em>wrong</em> by EA standards (as well as those of the general population). &quot;Careful reflection&quot; also isn't enough for humans to converge on an answer for themselves. If it was, tens of thousands of philosophers should have managed to map out morality, and we wouldn't need the likes of MIRI.</p>\n<p>Why should (some) people who are partial EAs not be pushed to become full EAs? Or why should (some) full EAs not be pushed to become partial EAs? Do you expect people to just happen to have the morality which has highest utility^1 by this standard? I suppose there is the trivial solution where people should always have the morality they have, but in that case we can't judge people who like murdering.</p>\n<blockquote>\n<p>I think there's absolutely nothing wrong with 1), if your goals are different from mine then that doesn't necessarily mean you're making a mistake about your goals. I personally focus on suffering and don't care about preventing death, all else being equal, but I don't (necessarily) consider you irrational for doing so.</p>\n</blockquote>\n<p>People's goals can be changed and/or people can be wrong about their goals, depending on what you consider proper &quot;goals&quot;. I'm sufficiently confident that I'm either misunderstanding you or that you're wrong about your morality that I can point out that the best way to achieve &quot;minimise suffering, without caring about death&quot; is to kill things as painlessly as possible (and by extension, to kill everything everywhere). I would expect people who believe they are suffering-minimisers to be objectively wrong.</p>\n<blockquote>\n<p>I'm arguing within a framework of moral anti-realism. I basically don't understand what people mean by the term &quot;good&quot; that could do the philosophical work they expect it to do. A partial EA is someone who would refuse to self-modify to become more altruistic IF this conflicts with other goals (like personal happiness, specific commitments/relationships, etc). I don't think there's any meaningful and fruitful sense in which these people are doing something bad or making some sort of mistake, all you can say is that they're being less altruistic as someone with a 100%-EA goal, and they would reply: &quot;Yes.&quot;</p>\n</blockquote>\n<p>Just because there is no objective morality, that doesn't mean people can't be wrong about their own morality. We can observe that people can be convinced to become more altruistic, which contradicts your model: if they were true partial EAs, they would refuse because anything other than what they believe is worse. I don't expect warring ideological states to be made up of people who all happened to be born with the right moral priors at the right time to oppose one another; their environment is much more likely to play a deciding role in what they believe. And environments can be changed, for example by telling people that they're wrong and you're right.</p>\n<p>Regarding your second confusion, not knowing how &quot;good&quot; works in a framework of moral anti-realism. Basically, in that case, every agent has its morality where doing good is &quot;good&quot; and doing bad is bad. What's good according to the cat is bad according to the mouse. Humans are sort of like agents and we're all sort of similar, so our moralities tend to always be sort of the same. So much so that I can say many things are good according to humanity, and have it make a decent amount of sense. In common speech, we drop the &quot;according to [x]&quot;. Also note that agents can judge each other just as they can judge objects. We can say that Effective Altruism is good and murder is bad, so we can say that an agent becoming more likely to do effective altruism is good and one becoming less likely to commit murder is good.</p>\n<blockquote>\n<p>But the thing is, some people have tried and failed and feel miserable about it, or even the thought of trying makes them feel miserable, so that certainly cannot be ideal because these people aren't being productive at that point.</p>\n</blockquote>\n<p>That isn't trivial. If 1 out of X miserable people manages to find a way to make things work eventually they could be more productive than Y people who chose to give up on levelling up and to be 'regular' EAs instead, with Y greater than X, and in that case we should advice people to keep trying even if they're depressed and miserable. But more importantly, it's a false choice: it should be possible to have people be less miserable but still to continue trying, and you could give advice on how to do that, if you know it. Signing up for a CFAR workshop might help, or showing some sort of clear evidence that happiness increases productivity. Compared to <a href=\"http://lesswrong.com/lw/4su/how_to_be_happy/\">lesswrong</a> <a href=\"http://lesswrong.com/lw/bq0/be_happier/\">posts</a>, this is very light on evidence.</p>\n<blockquote>\n<p>Human brains are not designed to optimize towards a single goal. It can drive you crazy. For some, it works, for others, it probably does not.</p>\n</blockquote>\n<p>This looks like you're contradicting yourself, so I'm not sure if I understand you correctly. But if you mean the first two sentences, do you have a source for that, or could you otherwise explain why you believe it? It doesn't seem obvious to me, and if it's true I need to change my mind.</p>\n<p>[1] This may include their personal happiness, EA productivity, right not to have their minds overwritten, etc.</p>\n", "parentCommentId": "gJRrfdvmXjt3pLhGc", "user": {"username": "Philip_W"}}, {"_id": "WGrZcgvnS6HvcqBFD", "postedAt": "2015-03-30T12:50:03.586Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<blockquote>\n<p>Someone who is so emotionally affected by EA that they give up is definitely someone who 'merely tried' to affect the world, because you can't just give up if you care in an agentic sense.</p>\n</blockquote>\n<p>I strongly disagree. Why would people be so deeply affected if they didn't truly care? The way I see it, when you give up EA because it's causing you too much stress, what happens constitutes a failure of goal-preservation, which is irrational, but after you've given up, you've become a different sort of agent. Just because you don't care/try anymore does not mean that caring/trying <em>in the earlier stages</em> was somehow fake. </p>\n<p>Giving up is not a rational decision made by your system-2*, it's a coping mechanism triggered by your system-1 feeling miserable, which then creates changes/rationalizations in system-2 that could become permanent. As I said before (and you expressed skepticism), humans <a href=\"http://lesswrong.com/lw/l3/thou_art_godshatter/\">are not designed to efficiently pursue a single goal</a>. A neuroscientist of the future, when the remaining mysteries of the human brain will be solved, will not be able to look at people's brains and read out a clear utility-function. Instead, what you have is a web of situational heuristics (system-1), combined with some explicitly or implicitly represented beliefs and goals (system-2), which can well be contradictory. There is often no clear way to get out a utility-function. Of course, people can decide to do what they can <a href=\"http://lesswrong.com/lw/8gc/stanovich_the_robots_rebellion_minireview/\">to self-modify towards becoming more agenty</a>, and some succeed  quite well despite of all the messy obstacles your brain throws at you. But if your ideal self-image and system-2 goals are too far removed from your system-1 intuitions and generally the way your mind works, then this will create a tension that leads to unhappiness and quite likely cognitive dissonance somewhere. If you keep going without changing anything, the outcome won't be good for neither you nor your goals. </p>\n<p>You mentioned in your earlier comment that lowering your expectations is <em>exactly what evading cognitive dissonance is</em>. Indeed! But look at the alternatives: If your expectations are impossible to fulfill for you, then you cannot reduce cognitive dissonance by improving your behavior. So either you lower your expectations (which preserves your EA-goal!), or you don't, in which case the only way to reduce the cognitive dissonance is by rationalizing and changing your goal. By choosing strategies like &quot;Avoiding daily dilemmas&quot;, you're not changing your goals, your only changing the expectations you set for yourself in regard to these goals. </p>\n<blockquote>\n<p>[\u2026] to me at least it isn't obvious if you meant that people should still try their hardest to be highly agentic but merely not beat themselves up over falling short.</p>\n</blockquote>\n<p>Have you considered that for some people, the most agenty thing to do would be to change their decision-procedure so it becomes less &quot;agenty&quot;? </p>\n<p>An analogy (adapted from Parfit): You have a combination-safe at home and get robbed, the robbers want the combination from you. They threaten to kill your family if you don't comply. The safe contains something that is extremely valuable to you, e.g. you and your family would be in gigantic trouble if it got stolen. You realize that the robbers are probably going to kill you and your family anyway after they got the safe open, because you all have seen their faces. What do you do? Now, imagine you had a magic pill that temporarily turns you, a rational, agenty person, into an irrational person whose actions don't make sense. Imagine that this state would be transparent to the robbers, e.g. they would know with certainty that you're not faking it and therefore realize that they can't get the combination from you. Should you take the pill, or would you say &quot;I'm rational, so I can never decide to try to become less rational in the future&quot;? Of course, you'd take the pill, because the rational action for your present self here is rendering your future self irrational. </p>\n<p>Likewise: If you notice that trying to be more agenty is counterproductive in important regards, the right/rational/agenty thing for you to do would be to try become a bit less agenty in the future. The robbers in the EA examples is your system-1 and personality vs. your idealized self-image/system-2/goal. With EA being too demanding, you don't even have to change your goals, it suffices to adjust your expectations to yourself. Both would have the same desired effect, the main difference being that, when you don't change your goals, you would want to become more agenty again if you discovered a means to overcome your obstacles in a different way. </p>\n<blockquote>\n<p>Whose &quot;right&quot; are we talking about, here? If it's &quot;right&quot; according to effective altruism, that is obviously false: someone who discovers they like murdering is wrong by EA standards (as well as those of the general population).</p>\n</blockquote>\n<p>We were talking about whether and to what extent people's goals contain EA-components. If part of people's goals contradict EA tenets, then of course they cannot be (fully) EA. I do agree with your implicit point that &quot;right according to x&quot; is a meaningful notion if &quot;x&quot; is sufficiently clearly defined. </p>\n<blockquote>\n<p>&quot;Careful reflection&quot; also isn't enough for humans to converge on an answer for themselves. If it was, tens of thousands of philosophers should have managed to map out morality, and we wouldn't need the likes of MIRI.</p>\n</blockquote>\n<p>Are you equating &quot;morality&quot; with &quot;figuring out an answer for one's goals that converges for all humans&quot;? If yes, then I suspect that the reference of &quot;morality&quot; fails because goals probably don't converge (completely). Why is there so much disagreement in moral philosophy? To a large extent, people seem to be trying to answer different questions. In addition, some people are certainly being irrational at what they're trying to do, e.g. they fail to distinguish between things that they care about terminally and things they only care about instrumentally, or they might fail to even ask fundamental questions. </p>\n<blockquote>\n<p>People's goals can be changed and/or people can be wrong about their goals, depending on what you consider proper &quot;goals&quot;.</p>\n</blockquote>\n<p>I agree, see my 2nd footnote in my original post. The point where we disagree is whether you can infer from an existing disagreement about goals that at least one participant is necessary being irrational/wrong about her goals. I'm saying that's not the case. </p>\n<blockquote>\n<p>I'm sufficiently confident that I'm either misunderstanding you or that you're wrong about your morality [...]</p>\n</blockquote>\n<p>I probably thought about my values more than most EAs and have gone through unusual lengths to lay out my arguments and reasons. If you want to try to find mistakes, inconsistencies or thought experiment that would make me change them, feel free to send me a PM here or on FB. </p>\n<blockquote>\n<p>Humans are sort of like agents and we're all sort of similar, so our moralities tend to always be sort of the same.</p>\n</blockquote>\n<p>With lots of caveats, e.g. people will be more or less altruistic, and that's part of your &quot;morality-axis&quot; as well if morality=your_goals. In addition, people will disagree about the specifics of even such &quot;straightfoward&quot; things as what &quot;altruism&quot; implies. Is it altruistic to give someone a sleeping pill against their will if they plan to engage in some activity you consider bad for them? Is it altruistic to turn rocks into happy people? People will disagree about what they would choose here, and it's entirely possible that they are not making any meaningful sort of mistake in the process of disagreeing. </p>\n<blockquote>\n<p>That isn't trivial. If 1 out of X miserable people manages to find a way to make things work eventually they could be more productive than Y people who chose to give up on levelling up and to be 'regular' EAs instead, with Y greater than X, and in that case we should advice people to keep trying even if they're depressed and miserable.</p>\n</blockquote>\n<p>OK, but even so, I would in such a case at least be right about the theoretical possibility of there being people to whom my advice applies correctly. For what it's worth, I consider it dangerous that EA will be associated with a lot of &quot;bad press&quot; if people drop out due to it being too stressful. All my experience with pitching EA so far indicates that it's bad to be too demanding. Sure, you can say you shouldn't be demanding towards <em>newcomers</em>, not established EAs, but you won't be able to keep up a memetic barrier there.</p>\n<blockquote>\n<p>But more importantly, it's a false choice: it should be possible to have people be less miserable but still to continue trying, and you could give advice on how to do that, if you know it.</p>\n</blockquote>\n<p>As a general point, I object to your choice of words: I don't think my posts ever argued for people to stop trying. I'm putting a lot of emphasis on getting the things right that you <em>do</em> do, like e.g. splitting separate motivations for donating so people don't end up donating to fuzzies only due to rationalizing or actual giving-up. I agree with the sentiment that advice is better the more it helps people stay well and still remain super-productive, I tried to give some advice that goes into that direction, e.g. the &quot;Viewing life as a game&quot; is also useful for when you're thinking about EA all the time, but of course I don't have all the advice either; I also welcome more contributions, and from what I've heard, CFAR has helped a lot of people in these regards. </p>\n<ul>\n<li>I'm using terms like &quot;system-2&quot; in a non-technical sense here. </li>\n</ul>\n", "parentCommentId": "bqtvHLer4eFdLRLNA", "user": {"username": "Lukas_Gloor"}}, {"_id": "tX59ocRe5hxCdeoRs", "postedAt": "2015-04-11T07:21:57.515Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<p>Sorry for not replying sooner.</p>\n<p>tl;dr: Effective Altruism shouldn\u2019t be a <em>job</em> you do because it\u2019s The Right Thing To Do, which you come home from tired and drained, you should integrate it with your life and include your own wellbeing in the decision process.</p>\n<blockquote>\n<p>I strongly disagree. Why would people be so deeply affected if they didn't truly care?</p>\n</blockquote>\n<p>They do truly care, in the emotional sense. They just can't be modeled like a utility-maximiser which values it greatly compared to their own mental well-being. You call the aberration 'irrationality', but that isn't an explanation. A model which does offer an explanation (simpler than an ad-hoc rule) is therefore strictly better. Given how predictable and intentional it is, I think it makes more sense to model it as a rational action of an agent which values the well-being of humanity less than the emotions generated by caring (about the well-being of humanity or something else).</p>\n<p>Suppose we have an agent. It has a utility function over its 'emotional states', and these emotional states are a priori linked to the environment in certain ways. It has a strong utility penalty for changing these links, but it is able to. In that case, if we place the agent in an environment which causes misery, then if it becomes unlikely that the situation will change, the agent will change the way the environment links to misery to prevent future misery. The link between the environment and the emotions is &quot;caring for things in the environment&quot;, with all expected behaviours, but the agent does not terminally value the environment in this model.</p>\n<p>We should also consider that people sometimes do start emotionally caring again if a problem stops appearing hopeless. This could be modeled by a utility boost for switching back to the &quot;proper&quot; emotional link-ups (though smaller than the utility loss for becoming jaded, because otherwise you would just always shield yourself from nasty emotions and switch back for the positive ones afterwards), which means that there is a complete map of &quot;proper&quot; emotional link-ups in the utility function, albeit lower-ranked than the emotional hookups. The agent's true optimum would therefore mean having &quot;proper&quot; emotional link-ups, and an environment identical to that of an agent which has the proper emotional link-ups as its utility function.</p>\n<p>This matches the data quite nicely, methinks. Better than &quot;irrationality&quot;, anyway.</p>\n<blockquote>\n<p>Giving up is not a rational decision made by your system-2*, it's a coping mechanism triggered by your system-1 feeling miserable, which then creates changes/rationalizations in system-2 that could become permanent.</p>\n</blockquote>\n<p>Agenty/rational behaviour isn't exclusive to system 2. How does system 1 decide when to trigger this coping mechanism? Or to beg the question less, how is existence parsed into the existence or nonexistence of a trigger?</p>\n<blockquote>\n<p>As I said before (and you expressed skepticism), <a href=\"http://lesswrong.com/lw/l3/thou_art_godshatter/\">humans are not designed to efficiently pursue a single goal</a>.</p>\n</blockquote>\n<p>That does not follow from the linked page. It states that our utility function (such that it exists) is very complex, not that there isn't a way to make one value dominant. For example, humans can be convinced to efficiently pursue the singular goal of watching flashing lights of a slot machine, getting heroin into their bloodstream, etc.</p>\n<p>Was that the evidence you have for the claim that humans aren't designed to efficiently pursue a single goal? Or do you have more evidence?</p>\n<blockquote>\n<p>A neuroscientist of the future, when the remaining mysteries of the human brain will be solved, will not be able to look at people's brains and read out a clear utility-function.</p>\n</blockquote>\n<p>It is trivially true that a utility function-based agent exists (in a mathematical sense) which perfectly models someone's behaviour. It may not be the simplest, but it must exist.</p>\n<blockquote>\n<p>Instead, what you have is a web of situational heuristics (system-1), combined with some explicitly or implicitly represented beliefs and goals (system-2),</p>\n</blockquote>\n<p>&quot;Non-technical&quot; is one thing, an entirely different sorting from the common usage which AFAIK has no basis in cognitive science is quite another. How did you manage to come upon those definitions? Never mind that situational heuristics by construction have implicitly represented beliefs and goals (ducking if someone's fist moves towards your face: I believe that their fist will continue to move in the rough direction it is going and I do not want to be hit in the face).</p>\n<blockquote>\n<p>There is often no clear way to get out a utility-function.</p>\n</blockquote>\n<p>That's evidence against a simple UF, only a little against complex UFs. And since Thou Art Godshatter, a complex UF is expected.</p>\n<blockquote>\n<p>Of course, people can decide to do what they can to self-modify towards becoming more agenty, and some succeed quite well despite of all the messy obstacles your brain throws at you. But if your ideal self-image and system-2 goals are too far removed from your system-1 intuitions and generally the way your mind works, then this will create a tension that leads to unhappiness and quite likely cognitive dissonance somewhere. If you keep going without changing anything, the outcome won't be good for neither you nor your goals.</p>\n</blockquote>\n<p>How could you possibly know this? How do you know what &quot;keeping going&quot; is for those who are going to read this?</p>\n<blockquote>\n<p>You mentioned in your earlier comment that lowering your expectations is exactly what evading cognitive dissonance is. Indeed! But look at the alternatives: If your expectations are impossible to fulfill for you, then you cannot reduce cognitive dissonance by improving your behavior. So either you lower your expectations (which preserves your EA-goal!), or you don't, in which case the only way to reduce the cognitive dissonance is by rationalizing and changing your goal.</p>\n</blockquote>\n<p>This is very different from how I would describe it, to the point that I have trouble understanding you. Am I correct in interpreting this as you expecting people to use some kind of EA utility quota, where &quot;expectations&quot; are a moral standard you want yourself to reach? That's... well, I guess it explains why people have donation quota, but it's very different from how I think about it by default.</p>\n<p>If you're a utilitarian, it is also Wrong, because either you're not optimising for the right utility before meeting the quota or you're necessarily doing worse after passing the quota than if you hadn't passed it. Problem cases are failing to optimise for the right utility function - one which places great instrumental value on their emotional health. A utility quota masks that problem by allowing people to patch up their emotional health during down time, but it is not a solution: For example, problem cases would still be damaging their emotional health while 'working', requiring a longer time to fix than if they took action to minimise emotional health damage while working, which is not allowed under the utility quota model because it's &quot;slacking off during work time&quot;. Someone whose quota allows them to just barely be okay would be in a constant struggle between their misaligned &quot;EA utility quota&quot; and their free time which tries to make them happy, as opposed to someone who has a more properly aligned EA utility quota, who also partially optimises work to be something they enjoy and as a consequence can get system 1 involved in creative thinking during work and spend more time working, leading to better happiness and productivity. (Disclaimer: no large scale test that I know of).</p>\n<p>In my opinion, there is always cognitive dissonance in this entire paradigm of utility quotas. You're making yourself act like two agents with two different moralities who share the same body but get control at different times. There is cognitive dissonance between those two agents. Even if you try to always have one agent in charge, there's cognitive dissonance with the part you're denying.</p>\n<blockquote>\n<p>By choosing strategies like &quot;Avoiding daily dilemmas&quot;, you're not changing your goals, your only changing the expectations you set for yourself in regard to these goals.</p>\n</blockquote>\n<p>These &quot;expectations&quot;, as you use them, are the goals you actually engage in. I agree you're not changing your true goals by changing your expectations, but you're doing something which is suboptimal by your own standards, which you don't see because you can't naturally empathise with everything in your future light-cone and system 2 is saying that it's all right.</p>\n<p>(part 1/2)</p>\n", "parentCommentId": "WGrZcgvnS6HvcqBFD", "user": {"username": "Philip_W"}}, {"_id": "ndqCgPDprRkwPeExj", "postedAt": "2015-04-11T07:22:07.865Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<p>part 2/2</p>\n<blockquote>\n<p>Have you considered that for some people, the most agenty thing to do would be to change their decision-procedure so it becomes less &quot;agenty&quot;? [...] your idealized self-image/system-2/goal.</p>\n</blockquote>\n<p>Yes, I have, for myself, and I declined, because I didn't see how it would help. Your analogy is indeed an analogy for what you believe, but it is not evidence. I asked you why you advise reducing one's agency, and the mere fact that it is theoretically possible for it to be a good idea doesn't demonstrate that it in fact is.</p>\n<p>Note that in the analogy, if the robbers aren't stupid, they will kill one of your family members because taking the pill is a form of non-compliance, wait for the pill to wear off, and then ask the same question minus one. If the hostage crisis <em>is</em> a good analogy for your internal self, what is to stop &quot;system 1&quot; from breaking its promises or being clever? That's basically how addiction works: take a pill a day or start withdrawal. Doing that? Good. Now take one more a day or start withdrawal. Replace \u201cpills\u201d with \u201ctime/effort not spent on altruism\u201d and you\u2019re doomed.</p>\n<blockquote>\n<p>With EA being too demanding, you don't even have to change your goals, it suffices to adjust your expectations to yourself. </p>\n</blockquote>\n<p>The utility quota strikes again. Here, your problem is that EA can be &quot;too demanding&quot; - apparently there is some kind of better morality by which you can say that EA is being Wrong, but somehow you don't decide to use that morality for EA instead.</p>\n<blockquote>\n<blockquote>\n<p>&quot;Careful reflection&quot; also isn't enough for humans to converge on an answer for themselves. If it was, tens of thousands of philosophers should have managed to map out morality, and we wouldn't need the likes of MIRI.</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<p>Are you equating &quot;morality&quot; with &quot;figuring out an answer for one's goals that converges for all humans&quot;? If yes, then I suspect that the reference of &quot;morality&quot; fails because goals probably don't converge (completely). Why is there so much disagreement in moral philosophy? To a large extent, people seem to be trying to answer different questions. In addition, some people are certainly being irrational at what they're trying to do, e.g. they fail to distinguish between things that they care about terminally and things they only care about instrumentally, or they might fail to even ask fundamental questions.</p>\n</blockquote>\n<p>No, in this case I'm referring to true morality, whatever it might be. If your explanation was true - if the divergence in theories was <em>because of</em> goals failing to converge or people answering different questions - we would expect philosophers to actually answer their questions. However, what we see is that philosophers do not manage to answer their own questions: every moral theory has holes and unanswered questions, not just differences of opinion between the author and others.</p>\n<p>If there were moral consensus, then obviously there would be a single morality, so the lack of a consensus carries some evidential weight, but not much. People are great at creating disagreements over nothing, and ethics is complex enough to be opaque, so we would expect moral disagreement in both worlds with a single coherent morality for humanity and worlds without one.</p>\n<blockquote>\n<p>I agree, see my 2nd footnote in my original post. The point where we disagree is whether you can infer from an existing disagreement about goals that at least one participant is necessary being irrational/wrong about her goals. I'm saying that's not the case.</p>\n</blockquote>\n<p>And my point is that the inference is obsolete: since neither person has psychological knowledge thirty years ahead of their time, both are necessarily wrong and irrational about their goals.</p>\n<blockquote>\n<blockquote>\n<p>I'm sufficiently confident that I'm either misunderstanding you or that you're wrong about your morality [...]</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<p>I probably thought about my values more than most EAs and have gone through unusual lengths to lay out my arguments and reasons. If you want to try to find mistakes, inconsistencies or thought experiment that would make me change them, feel free to send me a PM here or on FB.</p>\n</blockquote>\n<p>Why not do this publicly? Why not address the thought experiment I proposed? </p>\n<blockquote>\n<p>In addition, people will disagree about the specifics of even such &quot;straightfoward&quot; things as what &quot;altruism&quot; implies. Is it altruistic to give someone a sleeping pill against their will if they plan to engage in some activity you consider bad for them? Is it altruistic to turn rocks into happy people? People will disagree about what they would choose here, and it's entirely possible that they are not making any meaningful sort of mistake in the process of disagreeing.</p>\n</blockquote>\n<p>Those things are less straightforward than string theory, in the sense of Kolmogorov complexity. The fact that we can compress those queries into sentences which are simpler to introduce one to than <em>algebra</em> is testimony to how similar humans are.</p>\n<blockquote>\n<p>OK, but even so, I would in such a case at least be right about the theoretical possibility of there being people to whom my advice applies correctly.</p>\n</blockquote>\n<p>Yes, but you couldn't act on it without the benefit of hindsight. It is also theoretically possible that the moon is made out of cheese and that all information to the contrary has been spread by communist mice.</p>\n<blockquote>\n<p>For what it's worth, I consider it dangerous that EA will be associated with a lot of &quot;bad press&quot; if people drop out due to it being too stressful.</p>\n</blockquote>\n<p>This should be included in the productivity calculation, naturally. Just like your own mental wellbeing naturally should be part of EA optimisation.</p>\n<blockquote>\n<p>All my experience with pitching EA so far indicates that it's bad to be too demanding.</p>\n</blockquote>\n<p>And all your experience has had the constant factor of being pitched by you, someone who believes &quot;optimising for EA&quot; being tiring and draining is all part of the plan.</p>\n<p>Yes, if &quot;optimising for EA&quot; drains you, you should do less of it, because you aren't optimising for EA (unless there's an overlap, which there probably is, in which case you should keep doing the things which optimise for EA).</p>\n<blockquote>\n<p>As a general point, I object to your choice of words: I don't think my posts ever argued for people to stop trying.</p>\n</blockquote>\n<p>You're telling people not to try to optimise their full lives to EA right now. If that is what they were trying before, then you are arguing for people to stop trying, QED.</p>\n<p>On the topic of choice of words, though, in the original post you write &quot;The same of course also applies to women.&quot; - this implies that the author of the quote did not intend his statement to apply to women, despite using a (or at the time perhaps <em>the</em>) grammatically correct way to refer to an unspecified person of any gender (&quot;he&quot;). Considering you use a gendered pronoun to refer to unspecified people of any gender as well (&quot;she&quot;), I'm confused why you would wrongly 'correct' someone out like that.</p>\n", "parentCommentId": "tX59ocRe5hxCdeoRs", "user": {"username": "Philip_W"}}, {"_id": "SbHsTkDgszL7PKKiT", "postedAt": "2015-04-12T11:59:00.446Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<p>It seems obvious to me that we're talking past each other, meaning we're in many cases trying to accomplish different things with our models/explanations. The fact that this doesn't seem obvious to you suggests to me that I'm either bad at explaining, or that you might be interpreting my comments uncharitably. I agree with your tl;dr, btw! </p>\n<blockquote>\n<p>This matches the data quite nicely, methinks. Better than &quot;irrationality&quot;, anyway.</p>\n</blockquote>\n<p>You're presupposing that the agent would <em>not</em> modify any of its emotional links if it had the means to do so. This assumption might apply in some cases, but it seems obviously wrong as a generalization. Therefore, your model is incomplete. Reread what I wrote in the part &quot;On goals&quot;. I'm making a distinction between &quot;utility-function_1&quot;, which reflects all the decisions/actions an agent <em>will make</em> in all possible situations, and &quot;utility-function_2&quot;, which reflects all the decisions/actions an agent <em>would want to make</em> in all possible situations. You're focusing on &quot;utility-function_1&quot;, and what you're saying is entirely accurate \u2013 I like your model in regard to what it is trying to do. However, I find &quot;utility-function_2s&quot; much more interesting and relevant, which is why I'm focusing on them. Why don't you find them interesting?  </p>\n<blockquote>\n<p>Agenty/rational behaviour isn't exclusive to system 2. How does system 1 decide when to trigger this coping mechanism?</p>\n</blockquote>\n<p>Again, we have different understandings of rationality. The way I defined &quot;goals&quot; in the section &quot;On goals&quot;, it is only the system 2 that is <em>defining what is rational</em>, and system 1 heuristics can be &quot;rational&quot; if they are calibrated in a way that produces outcomes that are good in regard to the system 2 goals, given the most probable environment the agent will encounter. This part seems to be standard usage, in fact.</p>\n<p>Side note: Your theory of rationality is quite Panglossian, it is certainly possible to interpret <em>all</em> of human behavior as &quot;rational&quot; (as e.g. Gigerenzer does), but that would strike me as a weird/pointless thing to do. </p>\n<blockquote>\n<p>Was that the evidence you have for the claim that humans aren't designed to efficiently pursue a single goal? Or do you have more evidence?</p>\n</blockquote>\n<p>This claim strikes me as really obvious, so I'm wondering whether you might be misunderstanding what I mean. Have you never noticed just how bad people are at consequentialism? When people first hear of utilitarianism/EA, they think EA implies to give away all their wealth until they are penniless, instead of considering future earning prospects. Or they think having children and indoctrinating them with utilitarian beliefs is an effective way to do good, ignoring that you could just use the money to teach other people's children more effectively. The point EY makes in the Godshatter article is all about how evolution programmed many goals into humans even though evolution only (metaphorically) has one goal. </p>\n<p>What I'm saying is the following: Humans are bad at being consequentialists, they are bad at orienting their entire lives, i.e. jobs/relationships/free-time/self-improvement... towards one well-defined aim. Why? Well for one thing, if you ask most people &quot;what is your goal&quot;, they'll have a hard time to even answer! In addition, evolution programmed us to value many things, so someone whose goal is solely &quot;reduce as much suffering as possible&quot; cannot count on evolutionarily adaptive heuristics, and thus, biases against efficient behavioral consequentialism are to be expected.</p>\n<blockquote>\n<p>It is trivially true that a utility function-based agent exists (in a mathematical sense) which perfectly models someone's behaviour. It may not be the simplest, but it must exist.</p>\n</blockquote>\n<p>I know, what I was trying to say by &quot;A neuroscientist of the future, when the remaining mysteries of the human brain will be solved, will not be able to look at people's brains and read out a clear utility-function&quot; \u2013 emphasis should be on CLEAR \u2013 was that there is no representation in the brain where you look at it and see &quot;ah, that's where the utility function is&quot;. Instead (and I tried to make this clear with my very next sentence, which you quoted), you have to look at <em>the whole agent</em>, where <em>eventually</em>, all the situational heuristics, the emotional weights you talked about, and the agent's beliefs, together <em>imply</em> a utility-function in the utility-function_1-sense. Contrast this to a possible AI: It should be possible to construct an AI with a more clearly represented utility function, or a &quot;neutral&quot; prototype-AI where scientists need to fill in the utility-function part with whatever they care to fill in. When we speak of &quot;agents&quot;, we have in mind an entity that knows what it's goals are. When we represent an FAI or a paperclip maximizer, we assume this entity knows what its goal is. However, most humans do not know what their goals are. This is interesting, isn't it? Paperclippers would not enagage in discussions on moral philosophy, because they know what their goals are. Humans are often confused about their goals (and about morality, which some take to imply more than just goals). So your model of human utility functions should incorporate this curious fact of confusion. I was very confused by it myself, but I now think I understand what is going on. </p>\n<blockquote>\n<p>In my opinion, there is always cognitive dissonance in this entire paradigm of utility quotas. You're making yourself act like two agents with two different moralities who share the same body but get control at different times. There is cognitive dissonance between those two agents. Even if you try to always have one agent in charge, there's cognitive dissonance with the part you're denying.</p>\n</blockquote>\n<p>I find this quite a good description, actually. One is your system 2, &quot;what you would want to do under reflection&quot;, the other is &quot;primal, animal-like brain&quot;, to use outdated language. I wouldn't call the result &quot;cognitive dissonance&quot; necessarily. If you rationally understand what is going on, and realize that rebelling against your instincts/intuitions/emotional set-up is going to lead to a worse outcome than trying to reconcile your conflicting motivations, then the latter is literally the most sensible thing for your system 2 to attempt to do. </p>\n", "parentCommentId": "tX59ocRe5hxCdeoRs", "user": {"username": "Lukas_Gloor"}}, {"_id": "2iLWpeWbBWFE7JkMv", "postedAt": "2015-04-12T12:41:49.672Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<blockquote>\n<p>If the hostage crisis is a good analogy for your internal self, what is to stop &quot;system 1&quot; from breaking its promises or being clever?</p>\n</blockquote>\n<p>There are problems to every approach. Talk about your commitment to others, they will remind you. I'm not saying this whole strategy always works, but I'm quite sure there are many people for whom it is the best idea to try.</p>\n<p>Regarding the &quot;utility quota&quot;, what I mean by &quot;personal moral expecations&quot;: Basically, this just makes the point that it is useless to beat yourself up over things you cannot change. And yet we often do this, feel sad about things we probably couldn't have done differently. (One interesting hypothesis for this reaction is described <a href=\"http://www.patheos.com/blogs/unequallyyoked/2014/08/purity-anxiety-and-effective-altruism.html\">here</a>.)</p>\n<blockquote>\n<p>People are great at creating disagreements over nothing, and ethics is complex enough to be opaque, so we would expect moral disagreement in both worlds with a single coherent morality for humanity and worlds without one.</p>\n</blockquote>\n<p>Note that if this were true, you still need reasons why you expect there to be just one human morality. I know what EY wrote on the topic, and I find it question-begging and unconvincing. What EY is saying is that human utility-function_1s are complex and similar. What I'm interested in, and what I think you and EY should also be interested in, are utility-function_2s. But that's another discussion, I've been meaning to write up my views on the topic of metaethics and goal-uncertainty, but I expect it'll take me at least a few months until I get around to it.</p>\n<p>This doesn't really prove my case by itself, but it's an awesome quote nevertheless, so I'm including it here (David Hume, Enquiry): </p>\n<blockquote>\n<p>\u201cIt might reasonably be expected in questions which have been canvassed and disputed with great eagerness, since the first origin of science and philosophy, that the meaning of all the terms, at least, should have been agreed upon among the disputants; and our enquiries, in the course of two thousand years, been able to pass from words to the true and real subject of the controversy. For how easy may it seem to give exact definitions of the terms employed in reasoning, and make these definitions, not the mere sound of words, the object of future scrutiny and examination? But if we consider the matter more narrowly, we shall be apt to draw a quite opposite conclusion. From this circumstance alone, that a controversy has been long kept on foot, and remains still undecided, we may presume that there is some ambiguity in the expression, and that the disputants affix different ideas to the terms employed in the controversy.\u201d</p>\n</blockquote>\n<blockquote>\n<p>Why not do this publicly? Why not address the thought experiment I proposed?</p>\n</blockquote>\n<p>Lack of time, given that I've already written a lot of text on the topic. And because I'm considering to publish some of it at some point in the future, I'm wary of posting long excerpts of it online. </p>\n<blockquote>\n<p>Those things are less straightforward than string theory, in the sense of Kolmogorov complexity. The fact that we can compress those queries into sentences which are simpler to introduce one to than algebra is testimony to how similar humans are.</p>\n</blockquote>\n<p>Isn't the whole point of string theory that it is pretty simple (in terms of Kolmogorov complexity that is, not in whether I can understand it)? If anything, this would be testimony to how good humans are at natural speech as opposed to math. Although humans aren't <em>that</em> good at natural speech, because they often don't notice when they're being confused or talking past each other. But this is being too metaphorical. </p>\n<p>I don't really understand your point here. Aren't you presupposing that there is one answer people will converge on with these cases? I've talked to very intelligent people about these sorts of questions, and we've narrowed down all the factual disagreements we could think of. Certainly it is possible that I and the people I was talking to (who disagreed with my views), were missing something. But it seems more probable that answers just don't always converge. </p>\n<blockquote>\n<p>And all your experience has had the constant factor of being pitched by you, someone who believes &quot;optimising for EA&quot; being tiring and draining is all part of the plan.</p>\n</blockquote>\n<p>What, I thought you were saying that, at least more so than I'm saying it. </p>\n<blockquote>\n<p>You're telling people not to try to optimise their full lives to EA right now. If that is what they were trying before, then you are arguing for people to stop trying, QED.</p>\n</blockquote>\n<p>Differentiate between 1) &quot;ways of trying to accomplish a goal, e.g. in terms of decisional algorithms or habits&quot; and 2) &quot;pursuing a goal by whichever means are most effective&quot;. I did not try to discourage anyone from 2), and that's clearly what is relevant. I'm encouraging people to stop trying a particular variant of 1) because I believe that particular variant of 1) works for some people (to some extent), but not for all of them. It's a spectrum of course, not just two distinct modes of going about the problem. </p>\n<blockquote>\n<p>Considering you use a gendered pronoun to refer to unspecified people of any gender as well (&quot;she&quot;), I'm confused why you would wrongly 'correct' someone out like that.</p>\n</blockquote>\n<p>Let's not get into this, this discussion is already long enough. I can say that I see the point of your last remark, I did not mean to imply that Williams himself, given the time the text was written, was being sexist.</p>\n", "parentCommentId": "ndqCgPDprRkwPeExj", "user": {"username": "Lukas_Gloor"}}, {"_id": "Njd4Nur2c6h64dxxz", "postedAt": "2015-04-20T21:06:15.097Z", "postId": "4fPxQjq6GFZgurSsf", "htmlBody": "<p>Just read this, it expresses well what I meant by &quot;humans are not designed to pursue a single goal&quot;: <a href=\"http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/\">http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/</a></p>\n", "parentCommentId": "tX59ocRe5hxCdeoRs", "user": {"username": "Lukas_Gloor"}}]