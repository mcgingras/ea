[{"_id": "xi2d8fFyBqo5vw7tm", "postedAt": "2022-11-25T04:12:28.682Z", "postId": "5iQoR8mhEpvRT43jv", "htmlBody": "<p>Thanks Peter! I appreciate the work you've put in to synthesising a large and growing set of activities.</p><p>Nicholas Moes and Caroline Jeanmaire wrote a piece, <a href=\"https://forum.effectivealtruism.org/posts/tmxkRFx6HyhhvHdz4/a-map-to-navigate-ai-governance\">A Map to Navigate AI Governance</a>, which set out Strategy as 'upstream' of typical governance activities. Michael Aird in <a href=\"https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=SxLjkfqZwZqb9PvM5\">a shortform post</a> about x-risk policy 'pipelines' also set (macro)strategy upstream of other policy research, development, and advocacy activities.</p><p>One thing that could be interesting to explore is the current and ideal relationships between the work groups you describe here.&nbsp;</p><p>For example, in your government analogy, you describe Strategy as the executive branch, and each of the other work groups as agencies, departments, or specific functions (e.g., HR), which would be subordinate.&nbsp;</p><p>Does this reflect your thinking as well? Should AI strategy worker / organisations be deferred to by AI governance workers / organisations?</p>", "parentCommentId": null, "user": {"username": "AlexanderSaeri"}}, {"_id": "GhDxJeDfifC9dpBWE", "postedAt": "2022-11-25T04:52:19.021Z", "postId": "5iQoR8mhEpvRT43jv", "htmlBody": "<blockquote><p>For example, in your government analogy, you describe Strategy as the executive branch, and each of the other work groups as agencies, departments, or specific functions (e.g., HR), which would be subordinate.&nbsp;</p><p>Does this reflect your thinking as well? Should AI strategy worker / organisations be deferred to by AI governance workers / organisations?<br>&nbsp;</p></blockquote><p>Thanks Alex! I agree that it could be interesting to explore the current and ideal relationships between the work groups. I'd like to see that happen in the future.<br><br>I think that deferring sounds a bit strong, but I suspect that many workers/organisations in AI governance (and in other work groups) would like strategic insights from people working on AI Strategy and Movement Building. For instance, on questions like:&nbsp;</p><ul><li>What is the AI Safety community's agreement with/enthusiasm for specific visions, organisations and research agendas?</li><li>What are the key disagreements between competing visions for AI risk mitigation and the practical implications?</li><li>Which outcomes are good metrics to optimise for?</li><li>Who is doing/planning what in relevant domains, and what are the practical implications for a subset of workers/organisations plans?</li></ul><p>With that said, I don't really have any well-formed opinions about how things should work just yet!</p>", "parentCommentId": "xi2d8fFyBqo5vw7tm", "user": {"username": "Peterslattery"}}, {"_id": "dq3GcyRSJJHfYA3oq", "postedAt": "2022-11-27T01:08:26.787Z", "postId": "5iQoR8mhEpvRT43jv", "htmlBody": "<p>You explained the difference between strategy and governance as governance being a more specific thing, but I'm not sure it's a good idea in a specific place to separate and specialize in that way. What good does it bring to have governance separated from strategy ? Should experts in governance not communicate directly with experts in strategy (like should they only interface through public reports given from one group for the other?)<br><br>It seems to be governance was already a field thinking about the total strategy as well as specific implementations. I personally think of AI safety &nbsp;as Governance, Alignment and field building, and with the current post I don't see why I should update that.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Jonathan Claybrough"}}, {"_id": "z75DYtRb97YCca9Zv", "postedAt": "2022-11-28T02:01:38.029Z", "postId": "5iQoR8mhEpvRT43jv", "htmlBody": "<p>Thanks for commenting, Jonathan, it was helpful to hear your thoughts.<br><br>Sorry if I have been unclear. This post is not intended to show how I think things should be. I am instead trying to show how I think things are right now.<br><br>When doing that, I didn't mean to suggest that experts in governance don't communicate directly with experts in strategy.&nbsp;<br><br>As I said: \"Many people in the AI Safety community are involved in more than one work group.\" I also tried to show that there is an overlap in my Venn diagram.&nbsp;<br><br><i>&gt; I personally think of AI safety &nbsp;as Governance, Alignment and field building, and with the current post I don't see why I should update that. &nbsp;</i><br><br>I think that this is fine, and perhaps correct. However, right now, I still think that it is useful to tease those areas apart. &nbsp;<br><br>This is because I think that a lot of current work is related to strategy but not governance or the groups you mentioned. For instance, &nbsp;\u201cWhat is happening in areas relevant to AI?\u201d or how we should forecast progress based on <a href=\"https://www.alignmentforum.org/posts/wgio8E758y9XWsi8j/grokking-forecasting-tai-with-biological-anchors\">biological anchors</a>.<br><br>Let me know what you think.</p>", "parentCommentId": "dq3GcyRSJJHfYA3oq", "user": {"username": "Peterslattery"}}, {"_id": "wwo5MipG7LGENKCmr", "postedAt": "2022-11-30T19:18:23.257Z", "postId": "5iQoR8mhEpvRT43jv", "htmlBody": "<p>Thanks for writing this up!&nbsp;</p><p>As someone who is getting started in AIS movement building, this was great to read!</p><blockquote><p>i) I conceptualised the AI Safety community differently from some of my readers</p></blockquote><p>I would be curious, how does your take differ from others' takes?</p><p>I have read <a href=\"https://forum.effectivealtruism.org/posts/eggdG27y75ot8dNn7/three-pillars-for-avoiding-agi-catastrophe-technical\">Three pillars for avoiding AGI catastrophe: Technical alignment, deployment decisions, and coordination</a> and I feel the two posts are trying to answer slightly different questions but would be keen to learn about some other way people have conceptualised the problem.</p>", "parentCommentId": null, "user": {"username": "gergogaspar"}}, {"_id": "oDGXctRAhhf4mQmDr", "postedAt": "2022-12-07T08:22:05.165Z", "postId": "5iQoR8mhEpvRT43jv", "htmlBody": "<p>Hey, thanks for asking about this! I am writing this in a bit of a rush, so sorry if it's disjointed!<br><br>Yeah, I don't really have a good answer for that question. &nbsp;What happened was that I asked for feedback on an article outlining my theory of what AI Safety movement building should focus on. &nbsp;</p><p>The people who responded starting mentioning terms like funding, field building and community building and 'buying time'. I was unsure if/where all of these concepts overlapped with AI community/movement building.&nbsp;</p><p>So it was more so the case that I felt unclear on my take and other people's takes than I noticed some clear and specific differences between them.&nbsp;</p><p>When I posted this, it was partially to test my ideas on what the AI Safety community is and how it functions and learn if people had different takes from me. So far, Jonathan Claybrough is the only one who has offered a new take.</p><p>I am not sure how my take aligns with <a href=\"https://forum.effectivealtruism.org/posts/eggdG27y75ot8dNn7/three-pillars-for-avoiding-agi-catastrophe-technical\">Three pillars for avoiding AGI catastrophe: Technical alignment, deployment decisions, and coordination</a> etc.&nbsp;</p><p>At this stage, I haven't really been able to read and integrate much of the general perspective on what the AI Safety community as a whole should do. I think that this is ok for now anyway, &nbsp;because I want to focus on the smaller space of what movement builders' should think about.&nbsp;</p><p>I am also unsure if I will explore the macro strategy space much in the future because of the trade-offs. I think that in the long term, people are going to need to think about strategy at different levels of scope (e.g., around governance, overall strategy, movement building in governance etc). It's going to be very hard for me to have a high fidelity model of macro-strategy and all the concepts and actors etc while also really understanding what I need to do for movement building. &nbsp; &nbsp;</p><p>I therefore suspect that in the future, I will probably rely on an expert source of information to understand strategy (e.g., what do these three people or this community survey suggest I should think) rather than try to have my own in depth understanding. It's perhaps like a recruiter for a tech company is probably just going to rely on a contact to learn what the company needs for its hires, rather than understanding the companies structure and goals in great detail. However, I could be wrong about all of that and change my mind once I get more feedback!</p>", "parentCommentId": "wwo5MipG7LGENKCmr", "user": {"username": "Peterslattery"}}, {"_id": "eD3kX5imTav3nkcwK", "postedAt": "2022-12-07T14:55:59.466Z", "postId": "5iQoR8mhEpvRT43jv", "htmlBody": "<p>Fair enough, thank you!</p>", "parentCommentId": "oDGXctRAhhf4mQmDr", "user": {"username": "gergogaspar"}}]