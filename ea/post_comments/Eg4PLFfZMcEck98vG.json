[{"_id": "bfPw65duDS3TYewHJ", "postedAt": "2024-01-31T20:09:15.103Z", "postId": "Eg4PLFfZMcEck98vG", "htmlBody": "<p><strong>Executive summary</strong>: AI development involves risks at three key stages: training (misalignment), deployment (misuse), and diffusion (systemic issues). Competitive pressures exacerbate risks across all stages.</p><p><strong>Key points</strong>:</p><ol><li>Training risks involve AI goals misaligning from human values, causing issues when systems are deployed.</li><li>Deployment risks involve humans intentionally misusing powerful AI systems for harm.</li><li>Diffusion risks involve AI diffusing through the economy and causing unintentional systemic issues like loss of human control.</li><li>Competitive pressures make actors more likely to cut corners on safety at each stage.</li><li>Risks across stages could manifest simultaneously in complex ways.</li><li>Understanding this framework helps integrate different AI risk proposals into a cohesive whole.</li></ol><p>&nbsp;</p><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}, {"_id": "Lu3hCgFWAfbrJK4Gh", "postedAt": "2024-02-01T05:55:40.259Z", "postId": "Eg4PLFfZMcEck98vG", "htmlBody": "<p>You may have also seen Sam Clarke's <a href=\"https://forum.effectivealtruism.org/posts/e55QpEExmtkRjw9CD/classifying-sources-of-ai-x-risk\">classification of AI x-risk sources</a>, just sharing for others :)&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Lu3hCgFWAfbrJK4Gh/e3f81hh0mmxlrkb76dmu\"></figure><p>Wei Dai and Daniel Kokotajlo's <a href=\"https://www.lesswrong.com/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk\">older longlist</a> might worth perusing too?</p>", "parentCommentId": null, "user": {"username": "Mo Nastri"}}, {"_id": "MFvktKMMBmCLsTh8W", "postedAt": "2024-02-01T09:52:16.085Z", "postId": "Eg4PLFfZMcEck98vG", "htmlBody": "<p>Yes; it could be useful if Stephen briefly explained how his classification relates to other classifications. (And which advantages it has - I guess simplicity is one.)</p>", "parentCommentId": "Lu3hCgFWAfbrJK4Gh", "user": {"username": "Stefan_Schubert"}}, {"_id": "hXcADoBJJTsDNwaon", "postedAt": "2024-02-08T13:27:58.602Z", "postId": "Eg4PLFfZMcEck98vG", "htmlBody": "<p>This is a very informative article. The three stage model simplifies the risks for better understanding of what would happen.\nThe alignment stage is a very crucial stage in AI development and deployment. All the risks in all the three stages are equally very catastrophic. We could never be 'ready for what is to come' but we could surely curb it at the alignment stage!</p>\n<p>Thank you for the piece!</p>\n", "parentCommentId": null, "user": {"username": "Maureen Ondieki"}}]