[{"_id": "uqwxXbPJmge8wJqGG", "postedAt": "2023-08-16T08:03:14.778Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<p>This is interesting and useful, thanks for doing it!</p>", "parentCommentId": null, "user": {"username": "EmersonSpartz"}}, {"_id": "PhqLpTv8mWiXRn3NJ", "postedAt": "2023-08-20T09:42:12.644Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<p>\" A major consideration is that individuals\u2019 explicit statements of their reasons for their beliefs and attitudes may simply reflect post hoc rationalizations or ways of making sense of their beliefs and attitudes, as respondents may be unaware of the true reasons for their responses.\"</p>\n<p>I suspect that a major factor is simply: \"are people like me concerned about AI?\"/\"what do people like me believe?\"</p>\n<p>This to me points out the issue that AI risk communication is largely done by nerdy white men (sorry guys - I'm one of those too) and we should diversify the messengers.</p>\n", "parentCommentId": null, "user": {"username": "SiebeRozendal"}}, {"_id": "ggQDPnDGoS9Qi9Q2W", "postedAt": "2023-08-21T15:37:45.155Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<p>Thanks for the comment!&nbsp;</p><p>I think more research into whether public attitudes towards AI might be influenced by the composition of the messengers would be interesting. It would be relatively straightforward to run an experiment assessing whether people's attitudes differ in response to different messengers.</p><p>That said, the hypothesis (the AI risk communication being largely done by nerdy white men influences attitudes via public perceptions of whether 'people like me' are concerned about AI) seems to conflict with the available evidence. Both our previous surveys in this series (as well as this one) found no significant gender differences of this kind. Indeed, we found that women were more supportive of a pause on AI, more supportive of regulation, more inclined towards expecting harm from AI than men. Of course, future research attitudes towards more narrow attitudes regarding AI risk could reveal other differences.</p>", "parentCommentId": "PhqLpTv8mWiXRn3NJ", "user": {"username": "David_Moss"}}, {"_id": "FKbnR9xPsCkvXqeXR", "postedAt": "2023-08-21T18:06:08.286Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<p>Good point, thanks David</p>\n", "parentCommentId": "ggQDPnDGoS9Qi9Q2W", "user": {"username": "SiebeRozendal"}}, {"_id": "3Svr9kwyd53EjQ3E5", "postedAt": "2023-09-08T14:05:10.990Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<p>I think this is valuable research, and a great write up, so I'm curating it.</p><p>I think this post is so valuable because having accurate models of what the public currently believe seems very important for AI comms and policy work. For instance, I personally found it surprising how few people disbelieve AI being a major risk (only 23% disbelieve it being an <i>extinction</i> level risk), and how few people dismiss it for \"Sci-fi\" reasons. I have seen fears of \"being seen as sci-fi\" as a major consideration around AI communications within EA, and so if the public are not (or no longer) put off by this then that would be an important update for people working in AI comms to make.</p><p>I also like how clearly the results are presented, with a lot of the key info contained in the first graph.</p>", "parentCommentId": null, "user": {"username": "Will Howard"}}, {"_id": "RYfvZoTDM7jqBwPCv", "postedAt": "2023-09-09T17:29:34.252Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<p>Thanks!</p><blockquote><p>For instance, I personally found it surprising how few people disbelieve AI being a major risk (only 23% disbelieve it being an <i>extinction</i> level risk)</p></blockquote><p>Just to clarify, we don't find in this study that only 23% of people disbelieve AI is an extinction risk. This study shows that <i>of those who disagreed with the CAIS statement</i> 23% <i>explained this in terms of AI not causing extinction</i>.&nbsp;</p><p>So, on the one hand, this is a percentage of a smaller group (only 26% of people disagreed with the CAIS statement in our previous survey) not everyone. On the other hand, it could be that more people also disbelieve AI is an extinction risk, but that wasn't their cited reason for disagreeing with the statement, or maybe they agree with the statement but don't believe AI is an extinction risk.</p><p>Fortunately, <a href=\"https://forum.effectivealtruism.org/posts/Rg7h7G3KTvaYEtL55/us-public-perception-of-cais-statement-and-the-risk-of#\">our previous survey</a> looked at this more directly: we found <a href=\"https://forum.effectivealtruism.org/posts/Rg7h7G3KTvaYEtL55/us-public-perception-of-cais-statement-and-the-risk-of#Perceived_likelihood_of_human_extinction_from_AI\">13%</a> expressed that there was literally 0 probability of extinction from AI, though around 30% indicated 0-4% (the median was 15%, which is not far off some EA estimates). We can provide more specific figures on request.&nbsp;</p>", "parentCommentId": "3Svr9kwyd53EjQ3E5", "user": {"username": "David_Moss"}}, {"_id": "DSpCeRvmdx5fXhnDx", "postedAt": "2023-09-11T14:16:20.480Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<p>In 2015, one survey found 44% of the American public would consider AI an existential threat. In February 2023 it was<a href=\"https://www.monmouth.edu/polling-institute/reports/monmouthpoll_us_021523/\"> 55%.</a></p>", "parentCommentId": "3Svr9kwyd53EjQ3E5", "user": {"username": "DavidNash"}}, {"_id": "zhp6YEtTbPwypphof", "postedAt": "2023-09-11T15:44:05.668Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<p>I think Monmouth's question is not exactly about whether the public <i>believe</i> AI to be an existential threat. They asked:<br>\"<i>How worried are you</i> that machines with artificial intelligence could eventually pose a<br>threat to the existence of the human race \u2013 <i>very, somewhat, not too, or not at all worried</i>?\" The 55% you cite is those who said they were \"Very worried\" or \"somewhat worried.\"</p><p>Like the earlier <a href=\"https://forum.effectivealtruism.org/posts/hBSSn33BZggfkqErj/new-survey-46-of-americans-are-concerned-about-extinction-69?commentId=9FxqHyLkrZWbe5xkK\">YouGov poll</a>, this conflates an <i>affective</i> question (how worried are you) with a <i>cognitive</i> question (what do you believe will happen). That's why we deliberately split these &nbsp;in <a href=\"https://forum.effectivealtruism.org/s/6wddPaAdfJH2tGs2Z/p/ConFiY9cRmg37fs2p#Worry_about_AI\">our own polling</a>, which cited Monmouth's results, and also asked about explicit probability estimates in our later polling which we cited above.</p>", "parentCommentId": "DSpCeRvmdx5fXhnDx", "user": {"username": "David_Moss"}}, {"_id": "AGBeuwCwFHD2LFaWX", "postedAt": "2023-09-12T13:52:30.333Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<p>thanks for such detailed research with us :) i am newbie and excited to be here</p>", "parentCommentId": null, "user": {"username": "daniel99"}}, {"_id": "i8ybQyFaqkdx2ndoc", "postedAt": "2023-09-17T17:56:38.623Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<p>I think not mentioning climate change, though technically it's correct to not consider it a likely extinction risk, was probably the biggest problem with the statement. It may have given it a tinge or vibe of right-wing political polarization, as it feels like it's almost ignoring the elephant in the room, and that puts people on the defensive. Perhaps a broader statement could have mentioned \"risks to our species or our civilization such as nuclear war, climate change and pandemics\", which broadens the kind of risks included. After all, some very extreme climate change scenarios <i>could</i> be an X-risk, much like nuclear war <i>could</i> \"only\" cause a civilization collapse, but not extinction. Plus, these risks are correlated and entangled (climate change could cause new pandemics due to shifting habitats for pathogens, AI could raise international tensions and trigger a nuclear war if put in charge of defense, and so on). An acknowledgement of that is important.</p><p>There is unfortunately some degree of \"these things just sound like Real Serious Problems and that thing sounds like a sci-fi movie plot\" going on here too, and I don't think you can do much about that. The point of the message should not be compromised on that - part of the goal is exactly to make people think \"this might not be sci-fi any more, but your reality\".</p>", "parentCommentId": null, "user": {"username": "Simone S."}}, {"_id": "6rcgjhCNvkerxyQta", "postedAt": "2023-09-17T23:38:52.954Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<p>My guess is that mentioning climate change will get detractors on the right who dislike the mention of climate, AND detractors on the left who consider it to diminish the importance of climate change.</p>", "parentCommentId": "i8ybQyFaqkdx2ndoc", "user": {"username": "Linch"}}, {"_id": "GiLfeqk7vBCDmSYSz", "postedAt": "2023-09-18T22:09:29.661Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<p>Investigating the effects of talking (or not talking) about climate change in different EA/longtermist context seems to be neglected (e.g. through surveys/experiments and/or focus groups), despite being tractable with few resources.&nbsp;</p><p>It seems like we don't actually know either the direction or magnitude of the effect, and are mostly guessing, despite the stakes potentially being quite high (considered across EA/longtermist outreach).</p>", "parentCommentId": "6rcgjhCNvkerxyQta", "user": {"username": "David_Moss"}}, {"_id": "8JEdyG52JWqqeJNfo", "postedAt": "2023-09-24T07:09:05.778Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<p>My impression is that on the left there is a strong current that tries to push the idea of EAs being one and the same with longtermists, and both being lost after right-wing coded worries and ignoring the real threats (climate change). Statements about climate change not being an <i>existential</i> threat are often misinterpreted (even if technically true, they come off as dismissal). In practice, to most people, existential (everyone dies) and civilizational (some people survive, but they're in a Mad Max post-apocalyptic state) risks are both so awful that they count as negative infinity, and warrant equal effort to be averted.</p><p>I'm going to be real, I don't trust much the rationality of anyone who right now believes that climate change is straight up <i>fake</i>, as some do - that is a position patently divorced from reality. Meanwhile there's plenty of people who are getting soured up on AI safety because they see it presented as something that is being used to steal attention from it. I don't know precisely how can one assess the relative impacts of these trends, but I think it would be very urgent to determine it.</p>", "parentCommentId": "6rcgjhCNvkerxyQta", "user": {"username": "Simone S."}}, {"_id": "wahhce9zyHSvmpzfh", "postedAt": "2023-09-25T18:45:36.159Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<p>Thanks for engaging.</p><blockquote><p>In practice, to most people, existential (everyone dies) and civilizational (some people survive, but they're in a Mad Max post-apocalyptic state) risks are both so awful that they count as negative infinity, and warrant equal effort to be averted.</p></blockquote><p>So I think this is a very reasonable position to have. I think it's the type of position that should lead someone to be comparatively much less interested in the \"biology can't kill everyone\"-style arguments, and comparatively more concerned about biorisk and AI misuse risk compared to AGI takeover risk. Depends on the details of the collapse<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8i6npt7ysup\"><sup><a href=\"#fn8i6npt7ysup\">[1]</a></sup></span>&nbsp;and what counts as \"negative infinity\", you might also be substantially more concerned about nuclear risk as well.</p><p>But I don't see a case for <strong>climate change risk</strong> specifically approaching anywhere near those levels, especially on timescales less than 100 years or so. My understanding is that the academic consensus on climate change is <i><strong>very</strong></i> far from it being a near-term(or medium-term) civilizational collapse risk, and when academic climate economists argue about the damage function, the boundaries of debate are on the order of percentage points<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdr9em5c2yz5\"><sup><a href=\"#fndr9em5c2yz5\">[2]</a></sup></span>&nbsp;of GDP. Which is terrible, sure, and arguably qualify as a GCR, but pretty far away from a Mad Max apocalyptic state<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftzv325ef4jr\"><sup><a href=\"#fntzv325ef4jr\">[3]</a></sup></span>. So on the object-level, such claims will <i>literally</i> be wrong. That said, I think the wording of the CAIS statement \"societal-scale risks such as ...\" is broad enough to be inclusive of climate change, so someone editing that statement to include climate change won't directly be lying by my lights.</p><blockquote><p>I'm going to be real, I don't trust much the rationality of anyone who right now believes that climate change is straight up <i>fake</i>, as some do - that is a position patently divorced from reality.</p></blockquote><p>I'm often tempted to have views like this. But as my friend roughly puts it, \"once you apply the standard of 'good person' to people you interact with, you'd soon find yourself without any allies, friends, employers, or idols.\"&nbsp;<br><br>There are many commonly-held views that I think are either divorced from reality or morally incompetent. Some people think AI risk isn't real. Some (actually, most) people think there are literal God(s). Some people think there is no chance that chickens are conscious. Some people think chickens are probably conscious but it's acceptable to torture them for food anyway. Some people think vaccines cause autism. Some people oppose Human-Challenge Trials. Some people think it's immoral to do cost-effectiveness estimates to evaluate charities. Some people think climate change poses an extinction-level threat in 30 years. Some people think it's acceptable to value citizens &gt;1000x the value of foreigners. Some people think John Rawls is internally consistent. Some people have strong and open racial prejudices. Some people have secret prejudices that they don't display but drives much of their professional and private lives. Some people think good internet discourse practices includes randomly calling other people racist, or Nazis. Some people think evolution is fake. Some people believe in fan death.</p><p>And these are just <i>viewpoints</i>, when arguably it's more important to do good actions than to have good opinions. Even though I'm often tempted to want to only interact or work with non-terrible people, in terms of practical political coalition-building, I suspect the only way to get things done is by being willing to work with fairly terrible (by my lights) people, while perhaps still being willing to exclude extremely terrible people. Our starting point is the crooked timbers of humanity, the trick is creating the right incentive structures and/or memes and/or coalitions to build something great or at least acceptable.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8i6npt7ysup\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8i6npt7ysup\">^</a></strong></sup></span><div class=\"footnote-content\"><p>ie is it really civilizational collapse if it's something that affects the Northern hemisphere massively but results in Australia and South America not having &gt;50% reduction in standard of living? Reasonable people can disagree I think.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndr9em5c2yz5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdr9em5c2yz5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Maybe occasionally low tens of percentage points? I haven't seen anything that suggests this, but I'm not well-versed in the literature here.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntzv325ef4jr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftzv325ef4jr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>World GDP per capita was <strong>50%</strong> lower in 2000, and I think most places in 2000 did not resemble a post-apocalyptic state, with the exception of a few failed states.</p></div></li></ol>", "parentCommentId": "8JEdyG52JWqqeJNfo", "user": {"username": "Linch"}}, {"_id": "Bwze8mR62Ar2svgDR", "postedAt": "2023-09-26T18:17:20.626Z", "postId": "RYNtykh5xM467zRNj", "htmlBody": "<blockquote><p>But I don't see a case for <strong>climate change risk</strong> specifically approaching anywhere near those levels, especially on timescales less than 100 years or so.&nbsp;</p></blockquote><p>&nbsp;</p><p>I think the thing with climate change is that unlike those other things it's not just a vague possibility, it's a certainty. The uncertainty lies in the precise entity of the risk. At the higher end of warming it gets damn well dangerous (not to mention, it can be the trigger for other crises, e.g. imagine India suffering from killer heatwaves leading to additional friction with Pakistan, both nuclear powers). So it's a baseline of merely \"a lot dead people, a lot lost wealth, a lot things to somehow fix or repair\", and then the tail outcomes are potentially much much worse. They're considered unlikely but of course we <i>may</i> have overlooked a feedback loop or tipping point too much. I honestly don't feel as confident that climate change isn't a big risk to our civilization when it's likely to stress multiple infrastructures at once (mainly, food supply combined with a need to change our energy usage combined with a need to provide more AC and refrigeration as a matter of survival in some regions combined with sea levels rising which may eat on valuable land and cities).</p><blockquote><p>I'm often tempted to have views like this. But as my friend roughly puts it, \"once you apply the standard of 'good person' to people you interact with, you'd soon find yourself without any allies, friends, employers, or idols.\"&nbsp;</p></blockquote><p>I'm not saying \"these people are evil and irredeemable, ignore them\". But I'm saying they <i>are</i> being fundamentally irrational about it. \"You can't reason a person out of a position they didn't reason themselves in\". In other words, I don't think it's worth worrying about not mentioning climate change merely for the sake of not alienating them when the result is it will alienate <i>many more</i> people on other sides of the spectrum. Besides, those among those people who think like you might also go \"oh well these guys are wrong about climate change but I can't hold it against them since they had to put together a compromise statement\". I think as of now many minimizing attitudes towards AI risk are also irrational, but it's still a much newer topic and a more speculative one, with less evidence behind it. I think people might still be in the \"figuring things out\" stage for that, while for climate change, opinions are very much fossilized, and in some cases determined by things other than rational evaluation of the evidence. Basically, I think in this specific circumstance, there is no way of being neutral: either mentioning <i>or</i> not mentioning climate change gets read as a signal. You can only pick which side of the issue to stand on, and if you think you have a better shot with people who ground their thinking in evidence, then the side that believes climate change is real has more of those.</p>", "parentCommentId": "wahhce9zyHSvmpzfh", "user": {"username": "Simone S."}}]