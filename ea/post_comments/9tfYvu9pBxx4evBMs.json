[{"_id": "f225vBmNg3f5yLsLQ", "postedAt": "2024-03-09T15:48:30.414Z", "postId": "9tfYvu9pBxx4evBMs", "htmlBody": "<p>Impressions:</p>\n<ul>\n<li>None of these seem to have the relevant AI governance expertise</li>\n<li>Some nonprofit expertise</li>\n<li>Mostly corporate expertise</li>\n<li>I wonder what's happening with the OpenPhil board seat</li>\n</ul>\n", "parentCommentId": null, "user": {"username": "SiebeRozendal"}}, {"_id": "HYhwsZCmfen9PpSos", "postedAt": "2024-03-09T16:22:29.770Z", "postId": "9tfYvu9pBxx4evBMs", "htmlBody": "<p>Also worth reading:</p>\n<ul>\n<li><a href=\"https://www.nytimes.com/2024/03/07/technology/openai-executives-role-in-sam-altman-ouster.html\">this article</a> by the NY Times</li>\n<li>OpenAI's press release](<a href=\"https://openai.com/blog/review-completed-altman-brockman-to-continue-to-lead-openai\">https://openai.com/blog/review-completed-altman-brockman-to-continue-to-lead-openai</a>) where they summarize the report of the investigation, which contains very little information. Here's their conclusion:</li>\n</ul>\n<blockquote>\n<p>WilmerHale found the prior Board implemented its decision on an abridged timeframe, without advance notice to key stakeholders, and without a full inquiry or an opportunity for Mr. Altman to address the prior Board\u2019s concerns. WilmerHale found that the prior Board acted within its broad discretion to terminate Mr. Altman, but also found that his conduct did not mandate removal.</p>\n</blockquote>\n<p>Sam Altman seems to have won the battle and gained a very strong grip on OpenAI, but his reputation has taken a large hit.</p>\n", "parentCommentId": null, "user": {"username": "SiebeRozendal"}}, {"_id": "joRMZhzunvHbhdRvb", "postedAt": "2024-03-09T19:57:01.509Z", "postId": "9tfYvu9pBxx4evBMs", "htmlBody": "<blockquote><p>I wonder what's happening with the OpenPhil board seat</p></blockquote><p>I'm pretty sure that's gone now. I.e. that the initial $30m for a board seat arrangement wasn't actually legally binding wrt future members of the board, it was just maintained by who the current members would allow. So now there are no EA aligned board members there is no pressure or obligation to add any.</p><p>I could be wrong about this but I'm reasonably confident</p>", "parentCommentId": "f225vBmNg3f5yLsLQ", "user": {"username": "Will Howard"}}, {"_id": "SdfFxaJnR6YewDFKC", "postedAt": "2024-03-09T20:20:36.121Z", "postId": "9tfYvu9pBxx4evBMs", "htmlBody": "<p>My understanding is that since Holden left there\u2019s not been any formal \u201cOpen Phil board seat\u201d</p>\n", "parentCommentId": "joRMZhzunvHbhdRvb", "user": {"username": "bec_hawk"}}, {"_id": "FDRQbP2TYkZ88MeYo", "postedAt": "2024-03-10T15:42:10.254Z", "postId": "9tfYvu9pBxx4evBMs", "htmlBody": "<p>Anyone with thoughts on what went wrong with EA's involvement in OpenAI? It's probably too late to apply any lessons to OpenAI itself, but maybe not too late elsewhere (e.g., Anthropic)?</p>\n", "parentCommentId": null, "user": {"username": "Wei_Dai"}}, {"_id": "uZqcfQBciW99Mpuat", "postedAt": "2024-03-10T17:29:38.298Z", "postId": "9tfYvu9pBxx4evBMs", "htmlBody": "<p>I think answers to this are highly downstream of object-level positions.</p><p>If you think timelines are short and scaled-up versions of current architectures will lead to AGI, then 'what went wrong' is contributing to vastly greater chance of extinction.</p><p>If you don't agree with the above, then 'what went wrong' is overly dragging EA's culture and perception to be focused on AI-Safety, and causing great damage to all of EA (even non-AI-Safety parts) when the OpenAI board saga blew up in Toner &amp; McCauleys' faces.</p><p>Lessons are probably downstream of this diagnosis.</p><p>My general lesson aligns with <a href=\"https://forum.effectivealtruism.org/posts/LX9wGhZnCdLuXHPih/how-ea-can-be-better-at-communications\">Bryan's recent post </a>- man is EA bad about communicating what it is, and despite the OpenAI fiasco not being an attempted EA-coup motivated by Pascal's mugging longtermist concerns, it seems so many people have that as a 'cached explanation' of what went on. Feels to me like that is a big own goal and was avoidable.</p><p>Also on OpenAI, I think it's bad that people like Joshua Achiam who do good work at OpenAI seem to really dislike EA. That's a really bad sign - feels like the AI Safety community could have done more not to alienate people like him maybe.</p>", "parentCommentId": "FDRQbP2TYkZ88MeYo", "user": {"username": "JWS"}}, {"_id": "MKPXKFXKkbwqEhg8o", "postedAt": "2024-03-10T21:18:12.910Z", "postId": "9tfYvu9pBxx4evBMs", "htmlBody": "<p>At the risk of sounding, it's really not clear to me that anything \"went wrong\" - from my outside perspective, it's not like there was a clear mess-up on the part of EA's anywhere here, just a difficult situation managed to the best of people's abilities.</p>\n<p>That doesn't mean that it's not worth pondering whether there's any aspect that had been handled badly, or more broadly what one can take away from this situation (although we should beware to over-update on single notable events). But, not knowing the counterfactuals, and absent a clear picture of what things \"going right\" would have looked like, it's not evident that this should be chalked up as a failing on the part of EA.</p>\n", "parentCommentId": "FDRQbP2TYkZ88MeYo", "user": {"username": "Nick K."}}, {"_id": "C3yziL4di6zDo8Fs5", "postedAt": "2024-03-11T07:53:46.461Z", "postId": "9tfYvu9pBxx4evBMs", "htmlBody": "<p>I recommend adding \"Sam Altman\" to the title, it can act as a TLDR. The current phrasing has a bit of a \"click here to know more\" vibe for me (like an ad) (probably unintentionally)</p>\n", "parentCommentId": null, "user": {"username": "hibukki"}}, {"_id": "guDCAmY8fSfqcRGaE", "postedAt": "2024-03-11T09:00:41.970Z", "postId": "9tfYvu9pBxx4evBMs", "htmlBody": "<p>Personally I think the other members are actually the bigger news here, seeing as Sam being added back seemed like a foregone conclusion (or at least, the default outcome, and him not being added back would have been news).</p><p>But anyway, my goal was just to link to the post without editorialising too much so that people can discuss it on the forum. For this I think a policy of copying the exact title from the article is good in general.</p>", "parentCommentId": "C3yziL4di6zDo8Fs5", "user": {"username": "Will Howard"}}, {"_id": "43qwg4RBioggtfhL8", "postedAt": "2024-03-11T09:35:47.684Z", "postId": "9tfYvu9pBxx4evBMs", "htmlBody": "<p>The thing that stands out to me as clearly seeming to go wrong is the lack of communication from the board during the whole debacle. Given that the <a href=\"https://manifold.markets/sophiawisdom/why-was-sam-altman-fired\">final best guess</a> at the reasoning for their decision seems like something could have explained<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"bya3fpqadgm\" role=\"doc-noteref\" id=\"fnrefbya3fpqadgm\"><sup><a href=\"#fnbya3fpqadgm\">[1]</a></sup></span>, it does seem like an own goal that they didn't try to do so at the time.</p><p>They were getting clear pressure from the OpenAI employees to do this for instance, this was one of the main complaints in the <a href=\"https://www.nytimes.com/interactive/2023/11/20/technology/letter-to-the-open-ai-board.html\">employee letter</a>, and from talking to a couple of OAI employees I'm fairly convinced that this was sincere (i.e. they were just as in the dark as everyone else, and this was at least one of their main frustrations).</p><p>I've heard a few people make a comparison to other CEO-stepping-down situations, where it's common for things to be relatively hush hush and \"taking time out to spend with their family\". I think this isn't a like for like comparison, because in those cases it's usually a mutual agreement between the board and the CEO for them both to save face and preserve the reputation of the company. In the case of a sudden unilateral firing it seems more important to have your reasoning ready to explain publicly (or even privately, to the employees).</p><p>It's possible of course that there are some secret details that explain this behaviour, but I don't think there's any reason to be overly charitable in assuming this. If there was some strategic tradeoff that the board members were making it's hard to see what they were trading off against because they don't seem to have ended up with anything in the deal<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"2\" data-footnote-id=\"2hf1oaswmbo\" role=\"doc-noteref\" id=\"fnref2hf1oaswmbo\"><sup><a href=\"#fn2hf1oaswmbo\">[2]</a></sup></span>. I also don't find \"safety-related secret\" explanations that compelling because I don't see why they couldn't have said this (that there was a secret, not what it was). Everyone involved was very familiar with the idea that AI safety infohazards might exist so this would have been a comprehensible explanation.</p><p>If I put myself in the position of the board members I can much more easily imagine feeling completely out of my depth in the situation that happened and ill-advisedly doubling down on this strategy of keeping quiet. It's also possible they were getting bad advice to this effect, as lawyers tend to tell you to keep quiet, and there is general advice out there to \"not engage with the twitter mob\".</p><ol class=\"footnote-section footnotes\" data-footnote-section=\"\" role=\"doc-endnotes\"><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"1\" data-footnote-id=\"bya3fpqadgm\" role=\"doc-endnote\" id=\"fnbya3fpqadgm\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"bya3fpqadgm\"><sup><strong><a href=\"#fnrefbya3fpqadgm\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Several minor fibs from Sam, saying different things to different board members to try and manipulate them. This does technically fit with the \"not consistently candid\" explanation but that was very cryptic without further clarification and examples</p></div></li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"2\" data-footnote-id=\"2hf1oaswmbo\" role=\"doc-endnote\" id=\"fn2hf1oaswmbo\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"2hf1oaswmbo\"><sup><strong><a href=\"#fnref2hf1oaswmbo\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>To frame this the other way, if they had kept quiet and then been given some lesser advisory position in the company afterwards you could more easily reason that some face-saving dealing had gone on</p></div></li></ol>", "parentCommentId": "MKPXKFXKkbwqEhg8o", "user": {"username": "Will Howard"}}, {"_id": "RsQCYHQDGYPHj5tgN", "postedAt": "2024-03-11T15:44:16.098Z", "postId": "9tfYvu9pBxx4evBMs", "htmlBody": "<p>I wish I could signal boost this comment more. D'Angelo, Toner, and McCauley had a reason which was enough to persuade Ilya to take the drastic move of summarily removing Brockman from the board and then firing Sam, without any foresight and no communication whatsoever to the rest of the company, OpenAI stakeholders, or even Emmett (he threatened to resign unless they told him,<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"v5qzrmarz2q\" role=\"doc-noteref\" id=\"fnrefv5qzrmarz2q\"><sup><a href=\"#fnv5qzrmarz2q\">[1]</a></sup></span>&nbsp;though not sure where that left off), which lost them legitimacy both internally and externally, and eventually lost them everything.</p><p>I honestly don't have a plausible reason for it, or an belief they might have had which would help to square this circle, especially since I don't buy the <i>\"AGI has been achieved internally\" </i>stuff. I honestly think that your explanation of not realising it was going to go so nuclear,<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"2\" data-footnote-id=\"165v87u41rj\" role=\"doc-noteref\" id=\"fnref165v87u41rj\"><sup><a href=\"#fn165v87u41rj\">[2]</a></sup></span>&nbsp;and then just doing whatever the lawyers told them, is what happened. But if so, the lawyers' strategy of <i>\"say nothing, do nothing, just collect evidence and be quiet\"</i> was just an absolute disaster both for their own strategy and EA's reputation and legitimacy as a whole.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"3\" data-footnote-id=\"cmeaght5x07\" role=\"doc-noteref\" id=\"fnrefcmeaght5x07\"><sup><a href=\"#fncmeaght5x07\">[3]</a></sup></span>&nbsp;It honestly just seems like staggering incompetency to me,<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"4\" data-footnote-id=\"w8n3qam1gz\" role=\"doc-noteref\" id=\"fnrefw8n3qam1gz\"><sup><a href=\"#fnw8n3qam1gz\">[4]</a></sup></span>&nbsp;and the continued silence is just the most perplexing part of the entire saga, and I'm still (obviously) exercised by it.</p><p>Toner and McCauley remain as 2 members of the <a href=\"https://www.governance.ai/people\">5 person advisory board</a> for The Centre for the Governance of AI. Any implications are left as an exercise for the reader.</p><ol class=\"footnote-section footnotes\" data-footnote-section=\"\" role=\"doc-endnotes\"><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"1\" data-footnote-id=\"v5qzrmarz2q\" role=\"doc-endnote\" id=\"fnv5qzrmarz2q\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"v5qzrmarz2q\"><sup><strong><a href=\"#fnrefv5qzrmarz2q\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p><a href=\"https://www.bloomberg.com/news/articles/2023-11-21/altman-openai-board-open-talks-to-negotiate-his-possible-return\">https://www.bloomberg.com/news/articles/2023-11-21/altman-openai-board-open-talks-to-negotiate-his-possible-return</a></p></div></li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"2\" data-footnote-id=\"165v87u41rj\" role=\"doc-endnote\" id=\"fn165v87u41rj\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"165v87u41rj\"><sup><strong><a href=\"#fnref165v87u41rj\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>But how could they not realise this???</p></div></li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"3\" data-footnote-id=\"cmeaght5x07\" role=\"doc-endnote\" id=\"fncmeaght5x07\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"cmeaght5x07\"><sup><strong><a href=\"#fnrefcmeaght5x07\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>The dangers of policy set by lawyers reminds me of the Heads of Harvard, UPenn, and MIT having a disaster in front of Congress about the Israel/Palestine/free-speech/genocide hearing, possibly because they were repeating some legally cleared lines about what the could say to limit liability instead of saying <i>\"genocide is bad and advocating it is against our code of conduct\"</i></p></div></li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"4\" data-footnote-id=\"w8n3qam1gz\" role=\"doc-endnote\" id=\"fnw8n3qam1gz\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"w8n3qam1gz\"><sup><strong><a href=\"#fnrefw8n3qam1gz\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Nothing has really changed my mind here</p></div></li></ol>", "parentCommentId": "43qwg4RBioggtfhL8", "user": {"username": "JWS"}}, {"_id": "rmnYsy3ktnLDdp7Y7", "postedAt": "2024-03-13T16:43:31.549Z", "postId": "9tfYvu9pBxx4evBMs", "htmlBody": "<p>Just in case someone interested in this has not done so yet, I think Zvi\u2018s post about it was worth reading.</p>\n<p><a href=\"https://thezvi.substack.com/p/openai-the-board-expands\">https://thezvi.substack.com/p/openai-the-board-expands</a></p>\n", "parentCommentId": null, "user": {"username": "MaxRa"}}]