[{"_id": "ARsBKZHnagX72WY44", "postedAt": "2017-11-02T02:05:29.078Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<p>Excellent to see some challenge to this framework! I was particularly pleased to see this line: &quot;in the \u2018major arguments against working on it\u2019 section they present info like \u2018the US government spends about $8 billion per year on direct climate change efforts\u2019 as a negative <em>in itself</em>.&quot; I've often thought that 80k communicates about this oddly -- after all, for all we know, maybe there's room for $10 billion to be spent on climate change before returns start diminishing.</p>\n<p>However, having looked through this, I'm not sure I've been convinced to update much <em>against</em> neglectedness. After all, if you clarify that the % changes in the formula are really meant to be elasticities (which you allude to in the footnotes, and which I agree isn't clear in the 80k article), then surely lots of the problems actually go away? (i.e. thinking about diminishing marginal returns is important and valid, but that's also consistent with the elasticity view of neglectedness, isn't it?)</p>\n<p>Why I still think I'm in favour of including neglectedness: because it matters for counterfactual impact. I.e. with a crowded area (e.g. climate change), it's more likely that if you had never gone into that area, someone else would have come along and achieved the same outcomes as you (or found out the same results as you). And this likelihood drops if the area is neglected.</p>\n<p>So a claim that might usefully update my views looks something like this hypothetical dialogue:</p>\n<ul>\n<li><p>Climate change has lots of people working on it (bad)</p>\n</li>\n<li><p>However there are sub-sectors of climate change work that are high impact and neglected (good)</p>\n</li>\n<li><p>But because lots of other people work on climate change, if you hadn't done your awesome high-impact neglected climate change thing, someone else probably would have since there are so many people working in something adjacent (bad)</p>\n</li>\n<li><p>But [some argument that I haven't thought of!]</p>\n</li>\n</ul>\n", "parentCommentId": null, "user": {"username": "Sanjay"}}, {"_id": "PhLWdQQF6zuGajrYj", "postedAt": "2017-11-02T14:40:32.942Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<p>Hi Sacha, thanks for writing this, good food for thought. I'll get back to you properly next week after EA Global London (won't have any spare time for at least 4 days).</p>\n<p>I just wanted to point out quickly that we do have personal fit in our framework and it can give you up to a 100x difference between causes: <a href=\"https://80000hours.org/articles/problem-framework/#how-to-assess-personal-fit\">https://80000hours.org/articles/problem-framework/#how-to-assess-personal-fit</a></p>\n<p>I also wonder if we should think about the effective resources dedicated to solving a problem using a Cobb-Douglas production function: Effective Resources = Funding ^ 0.5 * Talent ^ 0.5. That would help capture cases where an increase in funding without a commensurate increase in talent in the area has actually increased the marginal returns to an extra person working on the problem.</p>\n", "parentCommentId": null, "user": {"username": "Robert_Wiblin"}}, {"_id": "azcpJpaJHaSEh9qqz", "postedAt": "2017-11-02T21:48:43.035Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<p>A few of the points made in this piece are similar to the points I make here: <a href=\"https://casparoesterheld.com/2017/06/25/complications-in-evaluating-neglectedness/\">https://casparoesterheld.com/2017/06/25/complications-in-evaluating-neglectedness/</a></p>\n<p>For example, the linked piece also argues that returns may diminish in a variety of different ways. In particular, it also argues that the returns diminish more slowly if the problem is big and that clustered value problems only produce benefits once the whole problem is solved.</p>\n", "parentCommentId": null, "user": {"username": "caspar42"}}, {"_id": "F7bqw9rFCew3mZKuC", "postedAt": "2017-11-03T16:54:57.266Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<blockquote>\n<p>then surely lots of the problems actually go away? (i.e. thinking about diminishing marginal returns is important and valid, but that's also consistent with the elasticity view of neglectedness, isn't it?)</p>\n</blockquote>\n<p>Can you expand on this? I only know of elasticity from reading around it after's Rob's in response to the first draft of this essay, so if there's some significance to it that isn't captured in the equations given, I maybe don't know it. If it's just a case of relabelling, I don't see how it would solve the problems with the equations, though - unused variables and divisions by zero seem fundamentally problematic.</p>\n<blockquote>\n<p>But because lots of other people work on climate change, if you hadn't done your awesome high-impact neglected climate change thing, someone else probably would have since there are so many people working in something adjacent (bad)</p>\n</blockquote>\n<blockquote>\n<p>But [</p>\n</blockquote>\n<p>this only holds to the extent that the field is proportionally less neglected - a priori you're less replaceable in an area that's 1/3 filled than one which is half filled, even if the former has a far higher absolute number of people working in it.</p>\n<blockquote>\n<p>]</p>\n</blockquote>\n<p>which is just point 6 from the 'Diminishing returns due to problem prioritisation' section applied. I think all the preceding points from the section could apply as well - eg the more rational people tend to work on (eg) AI-related fields, the better comparative chance you have of finding something importantly neglected within climate change (5), your awesome high-impact neglected climate change thing might turn out to be something which actually <em>increases</em> the value of subsequent work in the field (4), and so on.</p>\n<p>To be clear, I do think neglectedness will roughly track the value of entering a field, ceteris literally being paribus. I just think it's one of a huge number of variables that do so, and a comparatively low-weighted one. As such, I can't see a good reason for EAs having chosen to focus on it over several others, let alone over trusting the estimates from even a shallow dive into what options there are for contributing to an area.</p>\n", "parentCommentId": "ARsBKZHnagX72WY44", "user": {"username": "Arepo"}}, {"_id": "qAJjbX3Bpn963EbzC", "postedAt": "2017-11-03T17:20:50.236Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<p>What do you think of neglectedness popping up in Owen's <a href=\"https://www.fhi.ox.ac.uk/estimating-cost-effectiveness/\">model</a> when he was not trying to produce it? And his general <a href=\"http://www.fhi.ox.ac.uk/law-of-logarithmic-returns/\">logarithmic returns</a>? I do agree with you that even if the cause area is not neglected, there could be cost effective interventions, as I argue <a href=\"http://effective-altruism.com/ea/1g9/should_we_be_spending_no_less_on_alternate_foods/\">here</a>. But I would still say that within interventions, neglectedness is an important indicator of cost effectiveness.</p>\n", "parentCommentId": null, "user": {"username": "Denkenberger"}}, {"_id": "4FWuckRTF8gSbsdWa", "postedAt": "2017-11-05T01:46:05.683Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<p>I suggest summarising your reasoning as well as your conclusion in your tl;dr e.g. adding something like the following: &quot;as neglectedness is not a useful proxy for impact w/r/t many causes, such as those where progress yields comparatively little or no \u2018good done\u2019 until everything is tied together at the end, or those where progress benefits significantly from economies of scale.&quot;</p>\n", "parentCommentId": null, "user": null}, {"_id": "coHdeHSwnxTavDXPN", "postedAt": "2017-11-05T03:36:27.517Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<p>That's a good piece - we thought about many of these issues when working on the framework, and I agree it's not all clearly explained on the page.</p>\n", "parentCommentId": "azcpJpaJHaSEh9qqz", "user": {"username": "Benjamin_Todd"}}, {"_id": "A7aw5zJon3fFumkJf", "postedAt": "2017-11-06T19:36:07.942Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<blockquote>\n<p>To be clear, I do think neglectedness will roughly track the value of entering a field, ceteris literally being paribus.</p>\n</blockquote>\n<p>On reflection I don't think I believe this. The same assumption of rationality that says that people will tend to pick the best problems in a cause area to work on suggests that (a priori) they would tend to pick the best cause area to work on, in which case more people working on a field would indicate that it was <em>more</em> worth working on.</p>\n", "parentCommentId": "F7bqw9rFCew3mZKuC", "user": {"username": "Arepo"}}, {"_id": "Xf8iqD7zcitJgSqe4", "postedAt": "2017-11-06T20:04:13.850Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<p>Just read this. Nice point about future people. </p>\n<p>It sounds like we agree on most of this, though perhaps with differing emphasis - yy feeling is that neglectedness such a weak heuristic that we should abandon it completely, and at the very least avoid making it a core part of the idea of effective altruism. Are there cases where you would still advocate using it?</p>\n", "parentCommentId": "azcpJpaJHaSEh9qqz", "user": {"username": "Arepo"}}, {"_id": "Cm5BMsvaTgaTHhXWh", "postedAt": "2017-11-06T20:16:15.804Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<p>Ta Holly - done.</p>\n", "parentCommentId": "4FWuckRTF8gSbsdWa", "user": {"username": "Arepo"}}, {"_id": "YbL39fSHA9c2K3pnJ", "postedAt": "2017-11-06T22:35:08.358Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<p>Even if people pick interventions at random, the more people who enter a cause, the more the best interventions will get taken (by chance), so you still get diminishing returns even if people aren't strategically selecting.</p>\n", "parentCommentId": "A7aw5zJon3fFumkJf", "user": {"username": "Benjamin_Todd"}}, {"_id": "ApPgWjKFbXYLmHgxm", "postedAt": "2017-11-08T17:28:23.429Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<blockquote>\n<p>I certainly don\u2019t think, for example, that Givewell\u2019s recommendations for combating various neglected tropical diseases are based (even in significant fraction) on them being neglected, that ACE recommend factory farming campaigns because so few animal welfare charities address them, or that FHI are so concerned about AI because other academics weren\u2019t.</p>\n</blockquote>\n<p>A few brief thoughts:</p>\n<p>1) My understanding is that there are two different prioritization frameworks: the 80,000 Hours framework (for cause areas) and the GiveWell framework (for measurable interventions). The <a href=\"https://80000hours.org/articles/problem-framework/\">80,000 Hours framework</a> looks at how much good would be done by solving a problem (scale), how much of the problem would be solved by doubling the money/talent currently going towards it (tractability), and how much money/talent is currently going towards it (neglectedness). The <a href=\"https://www.givewell.org/how-we-work/criteria\">GiveWell framework</a> looks at whether there is strong evidence that an intervention has a positive impact (evidence of effectiveness), how much you have to spend on the intervention to have some standardized unit of impact such as saving a life (cost effectiveness), and whether additional donations would enable the charity to expand its implementation of the intervention (room for more funding). The GiveWell framework does not seem to explicitly consider any of the 80,000 Hours factors: if there is an intervention that is evidence-based and cost effective that is in need of more funding, then it does well under the GiveWell framework even if there are significant resources going towards the underlying problem (i.e. it is not neglected), the problem as a whole is relatively unimportant (i.e. it is small in scale), or the portion of the problem that would be solved is relatively small (i.e. it is not tractable in the 80,000 Hours sense). However, if an intervention performs poorly on all three 80,000 Hours factors (i.e. it is relatively unimportant, doubling the resources going towards it would not solve a substantial portion of it, and it would take significantly more resources to achieve that doubling), then it is unlikely to be considered cost effective by GiveWell.</p>\n<p>2) Neglectedness is generally important for two reasons. The first is room for more funding: if there is already enough funding to implement an intervention to the maximum extent feasible given other constraints, then additional donations will either be diverted to a different intervention or will be spent on the intervention only after some delay. In both cases, there is a significant opportunity cost if there is a comparably effective alternative intervention with room for more funding. The second is diminishing marginal returns: an intervention will generally be implemented first in settings where it has a higher impact, which means that as more is spent on an intervention, the intervention tends to expand to settings where it is less effective. </p>\n<p>3) GiveWell implicitly considers diminishing marginal returns as part of its cost effectiveness factor. For example, in its cost effectiveness estimate for bednets, it looks at baseline malaria rates in countries where the Against Malaria Foundation (AMF) operates (see A6 and A13 of <a href=\"https://docs.google.com/spreadsheets/d/1ulAmsDjWKgV07DiUuvVCRRJOVLw-PCjXMe-Eq2J3DtY/edit#gid=1364064522\">this spreadsheet</a>). As AMF expands to countries with lower malaria rates, GiveWell will increase its cost per life saved estimate for AMF (if other factors remain the same). In addition to its implicit consideration of diminishing marginal returns, GiveWell explicitly considers room for more funding.</p>\n<p>4) By contrast, the 80,000 Hours framework does not appear to take into account diminishing marginal returns or room for more funding. To see this, consider the following example.\nSuppose that there is a village of people living in poverty. Any person in that village who receives cash transfers gains additional utility as follows:</p>\n<p>Total Cash Received...............    Total Additional Utility</p>\n<p>$1000...........................................   5</p>\n<p>$2000...........................................    9</p>\n<p>$3000...........................................    12</p>\n<p>$4000...........................................    14</p>\n<p>$5000...........................................    15</p>\n<p>Currently each person is receiving $2,000, which means that each of them is gaining 9 additional units of utility. You are considering transferring an extra $1,000 to each of them. Under the 80,000 Hours framework, you would start by calculating how much good would be done by solving the problem (scale). In this case, eliminating poverty would result in 15 additional units of utility per person. (I am including the impact of the first $2,000.) The next step is to look at the portion of the problem that would be solved by doubling spending (tractability). In this case, if spending were doubled to $4,000 per person, then each person would gain 5 additional units of utility (since they would go from having 9 additional units of utility to having 14 additional units of utility). Thus, doubling spending would result in solving 1/3 of the problem being solved (gaining 5 additional units of utility out of 15 possible additional units). The last step is to determine how far your money would go towards the doubling (neglectedness). In this case, donating $1,000 to each person would take you half way to the doubling (which would require $2,000 per person). Multiplying the three factors together, you get that your donation of $1,000 to each person would result in a gain of 2.5 additional units of utility per person (15 additional units scale x 1/3 tractability x 1/2 neglectedness). However, your donation will actually result in a gain of 3 additional units of utility per person (12 additional units compared to 9 additional units). Why does the 80,000 Hours formula give a lower number? Because it assumes that the average per dollar impact of the money needed to double spending (in this case $2,000 per person) is as large as the average per dollar of impact of a smaller amount of money (in this case $1,000 per person). In other words, it assumes constant marginal returns (rather than diminishing marginal returns). (The same issue exists with respect to room for more funding: if in doubling spending, you fill the entire funding gap for the most effective intervention and have to spend the remainder of the money on a less effective intervention, then the average effectiveness of the money spent doubling will be less than the average effectiveness of money used to fill the funding gap.) </p>\n<p>5) ACE's focus on factory farming is in fact based partly on neglectedness (see <a href=\"https://animalcharityevaluators.org/donation-advice/why-farmed-animals/\">here</a> and <a href=\"https://animalcharityevaluators.org/advocacy-interventions/prioritizing-causes/#detailed\">here</a>).</p>\n<p><em>Note: I am not affiliated with GiveWell or 80,000 Hours, so nothing in this comment should be taken as an official description of their frameworks.</em></p>\n", "parentCommentId": null, "user": {"username": "RandomEA"}}, {"_id": "ETko4E3qnjqdoZ5yL", "postedAt": "2017-11-25T07:31:36.233Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<blockquote>\n<p>The same assumption of rationality that says that people will tend to pick the best problems in a cause area to work on suggests that (a priori) they would tend to pick the best cause area to work on</p>\n</blockquote>\n<p>This was an insightful comment for me, and the argument does seem correct at first glance. I guess the reason I'd still disagree is because I observe people thinking about within-cause choices very differently from how they think about across-cause choices, so they're more rational in one context than the other. A key part of effective altruism's value, it seems to me, is the recognition of this discrepancy and the argument that it should be eliminated.</p>\n<blockquote>\n<p>in which case more people working on a field would indicate that it was <em>more</em> worth working on.</p>\n</blockquote>\n<p>I think if you really believe people are rational in the way described, more people working on a field doesn't necessarily give you a clue as to whether more people should be working on it or not, because you expect the number of people working on it to roughly track the number of people who ought to work on it -- you think the people who are not working on it are <em>also</em> rational, so there must be circumstances under which that's correct, too.</p>\n", "parentCommentId": "A7aw5zJon3fFumkJf", "user": {"username": "BenMillwood"}}, {"_id": "hbRAWhiqyCpSv76Nz", "postedAt": "2017-11-25T07:37:50.001Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<p>To clarify, this only applies if everyone <em>else</em> is picking interventions at random, but you're still managing to pick the best remaining one (or at least better than chance).</p>\n<p>It also seems to me like it applies across causes as well as within causes.</p>\n", "parentCommentId": "YbL39fSHA9c2K3pnJ", "user": {"username": "BenMillwood"}}, {"_id": "pqAHCeBS6r7gACSCK", "postedAt": "2017-11-25T08:07:49.880Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<p>I'm going to write a relatively long comment making a relatively narrow objection to your post. Sorry about that, but I think it's a particularly illustrative point to make. I disagree with these two points against the neglectedness framing in particular:</p>\n<ol>\n<li>that it could divide by zero, and this is a serious problem</li>\n<li>that it splits a fraction into unnecessarily conditional parts (the &quot;dragons in Westeros&quot; problem).</li>\n</ol>\n<p>Firstly in response to (1), this is a legitimate illustration that the framework only applies where it applies, but it seems like in practice like it isn't an obstacle. Specifically, the framing works well when your proposed addition is small relative to the existing resource, and it seems like that's true of most people in most situations. I'll come back to this later.</p>\n<p>More importantly, I feel like (2) misses the point of what the framework was developed <em>for</em>. The goal is to get a better handle on what kinds of things to look for when evaluating causes. So the fact that the fraction simplifies to &quot;good done per additional resource&quot; is sort of trivial \u2013 that's the goal, the metric we're trying to optimize. It's hard to measure that directly, so the value added by the framework is the claim that certain conditionalizations of the metric (if that's the right word) yield questions that are easier to answer, and answers that are easier to compare.</p>\n<p>That is, we write it as &quot;scale times neglectedness times solvability&quot; because we find empirically that those individual factors of the metric tend to be more predictable, comparable and measurable than the metric as a whole. The applicability of the framework is absolutely contingent on what we in-practice discover to be the important considerations when we try to evaluate a cause from scratch.</p>\n<p>So while there's no <em>fundamental</em> reason why neglectedness, particularly as measured in the form of the ratio of percentage per resource, needs to be a part of your analysis, it just turns out to be the case that you can often find e.g. two different health interventions that are otherwise very comparable in how much good they do, but with very different ability to consume extra resources, and that drives a big difference in their attractiveness as causes to work on.</p>\n<p>If ever you <em>did</em> want to evaluate a cause where the existing resources were zero, you could just as easily swap the bad cancellative denominator/numerator pair with another one, say the same thing in absolute instead of relative terms, and the rest of the model would more or less stand up. Whether that should be done in general for evaluating other causes as well is a judgement call about how these numbers vary in practice and what situations are most easily compared and contrasted.</p>\n", "parentCommentId": null, "user": {"username": "BenMillwood"}}, {"_id": "TAMYd5T5AXry4gHTZ", "postedAt": "2020-09-29T12:32:58.793Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<p>I know this is a late reply to an old comment but it would be awesome to know in how far you think you have addressed the issues raised? Or if you did not address them what was you reason for discarding them?</p><p>I am working through the cause prio literature at the moment and I don't really feel that 80k addresses all (or most) of the substantial concerns raised. For instance, the assessments of climate change and AI safety are great examples where 80k's considerations can be quite easily attacked given conceptual difficulties in the underlying cause prio framework/argument.</p>", "parentCommentId": "coHdeHSwnxTavDXPN", "user": {"username": "alexherwix"}}, {"_id": "LviqAGafCmdHv4aE6", "postedAt": "2020-09-29T14:45:48.023Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<p>This will mainly need to wait for a separate article or podcast, since it's a pretty complicated topic.</p>\n<p>However, my quick impression is that the issues Caspar mentions are mentioned in the problem framework article.</p>\n<p>I also agree that their effect is probably to narrow the difference between AI safety and climate change, however I don't think they flip the ordering, and our 'all considered' view of the difference between the two was already narrower than a naive application of the INT framework implies \u2013 for the reasons mentioned <a href=\"https://forum.effectivealtruism.org/posts/YxgDypoHSPYhhfcMx/factors-other-than-itn?commentId=DLFHje8688pBA3G8a\">here</a> \u2013 so I don't think it really alters our bottom lines (in part because we were already aware of these issues). I'm sorry, though, that we're not clearer that our 'all considered' views are different from 'naive INT'.</p>\n", "parentCommentId": "TAMYd5T5AXry4gHTZ", "user": {"username": "Benjamin_Todd"}}, {"_id": "2RzGA6yBE8XokTib8", "postedAt": "2020-09-29T16:30:21.582Z", "postId": "JRaMhtJkAYkBvz6tC", "htmlBody": "<p>Thanks for the quick reply!&nbsp;</p><p>Yeah, an article or podcast on the framework and possible pitfalls would be great. I generally like ITN for broad cause assessments (i.e., is this interesting to look at?) but the quantitative version that 80k uses does seem to have some serious limitations if one digs more deeply into the topic. I would be mostly concerned about people new to EA either having false confidence in numbers or being turned off by an overly simplistic approach. But you obviously have much more insight into peoples reactions and I am looking forward to how you develop and improve on the content in the future!</p>", "parentCommentId": "LviqAGafCmdHv4aE6", "user": {"username": "alexherwix"}}]