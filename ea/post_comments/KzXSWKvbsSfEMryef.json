[{"_id": "8NGFLdoYKYjuepbBa", "postedAt": "2024-02-28T20:16:52.683Z", "postId": "KzXSWKvbsSfEMryef", "htmlBody": "<p>I would add that acausal influence is not only not Pascalian, but that it can make other things that may seem locally Pascalian or at least quite unlikely to make a positive local difference \u2014 like lottery tickets, voting and maybe an individual's own x-risk reduction work \u2014 become reasonably likely to make a large difference across a multiverse, because of variants of the Law of Large Numbers or Central Limit Theorem. This can practically limit risk aversion. See <a href=\"https://globalprioritiesinstitute.org/hayden-wilkinson-can-an-evidentialist-be-risk-averse/\"><u>Wilkinson, 2022</u></a><u> (</u><a href=\"https://forum.effectivealtruism.org/posts/CJtGbGZBvBvEr8jBP/can-an-evidentialist-be-risk-averse-hayden-wilkinson\"><u>EA Forum post</u></a><u>).</u></p>", "parentCommentId": null, "user": {"username": "MichaelStJules"}}, {"_id": "EfLBgnbHrJxWvCCFL", "postedAt": "2024-02-28T20:33:20.363Z", "postId": "KzXSWKvbsSfEMryef", "htmlBody": "<p>On cluelessness: if you have complex cluelessness as <i>deep uncertainty</i> about your expected value conditional on the possibility of acausal influence, then it seems likely you should still have complex cluelessness as deep uncertainty all-things-considered, because deep uncertainty will be infectious, at least if its range is higher (or incomparable) than that assuming acausal influence is impossible.</p><p>For example, suppose</p><ol><li>10% to acausal influence, and expected value of some action conditional on it of -5*10^50 to 10^51, a range due to deep uncertainty.</li><li>90% to no acausal influence, and expected value of 10^20 conditional on it.</li></ol><p>Then the unconditional expected effects are still roughly -5*10^49 to 10^50, assuming the obvious intertheoretic comparisons between causal and acausal decision theories from <a href=\"https://philpapers.org/archive/MACTEW-2.pdf\"><u>MacAskill et al., 2021</u></a>, and so deeply uncertain. If you don't make intertheoretic comparisons, then you could still be deeply uncertain, but it could depend on how exactly you treat normative uncertainty.</p><p>If you instead use precise probabilities even with acausal influence and the obvious intertheoretic comparisons, then it would be epistemically suspicious if the expected value conditional on acausal influence were ~0 and didn't dominate the expected value without acausal influence. One little piece of evidence biasing you one way or another gets multiplied across the (possibly infinite) multiverse under acausal influence.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefs818hfk9ki\"><sup><a href=\"#fns818hfk9ki\">[1]</a></sup></span></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fns818hfk9ki\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefs818hfk9ki\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Maybe the expected value is also infinite without acausal influence, too, but a reasonable approach to infinite aggregation would probably find acausal influence to dominate, anyway.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "MichaelStJules"}}, {"_id": "KgDRLGpouDMWhh9dR", "postedAt": "2024-02-28T20:46:30.171Z", "postId": "KzXSWKvbsSfEMryef", "htmlBody": "<p>This doesn't necessarily totally eliminate all risk aversion, because the outcomes of actions can also be substantially correlated across correlated agents for various reasons, e.g. correlated agents will tend to be biased in the same directions, the difficulty of AI alignment is correlated across the multiverse, the probability of consciousness and moral weights of similar moral patients will be correlated across the multiverse, etc.. So, you could only apply the LLN or CLT after conditioning separately on the different possible values of such common factors to aggregate the conditional expected value across the multiverse, and then you recombine.</p>", "parentCommentId": "8NGFLdoYKYjuepbBa", "user": {"username": "MichaelStJules"}}, {"_id": "b38GbBrmybFjabDtm", "postedAt": "2024-02-29T20:37:58.591Z", "postId": "KzXSWKvbsSfEMryef", "htmlBody": "<p><strong>Executive summary</strong>: This post addresses common objections and questions about evidential cooperation in large worlds (ECL), which argues we should cooperate with distant civilizations that use similar reasoning.</p><p><strong>Key points</strong>:</p><ol><li>ECL combines reasonable ideas from decision theory and assumes a large universe. It is counterintuitive but worth considering.</li><li>There are good arguments against causal decision theory and for noncausal theories that support ECL.</li><li>ECL does not seem to be a Pascal's mugging. The ideas behind it are not that unlikely.</li><li>ECL's implications may be dampened by uncertainty over cooperation actions. But commitments to benefit others have broad appeal.</li><li>ECL likely increases the value of cooperative AI systems. Quantifying the exact implications needs more research.</li><li>Issues like complex cluelessness apply broadly. Infinite ethics causes problems for everyone.</li></ol><p>&nbsp;</p><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}, {"_id": "jdohWsaJmnrFdBcEk", "postedAt": "2024-03-01T04:01:53.832Z", "postId": "KzXSWKvbsSfEMryef", "htmlBody": "<p>I downvoted this post for the lack of epistemic humility. I don't mind people playing with thought experiments as a way to yield insight, but they're almost never a way of generating 'evidence'. Saying things like CDT 'falls short' because of them is far too strong. Under a certain set of assumptions, it arguably recommends an action that some people intuitively take issue with - that's not exactly a self-evident deficiency.</p><p>Personally, I can't see any reason to reject CDT based on the arguments here (or any I've seen elsewhere). They seem to rely on sleights such amphiboly, vagueness around what we call causality, asking me to accepting magic or self-effacing premises in some form, and an assumption of perfect selfishness/attachment to closed individualism, both of which I'd much sooner give up than accept a theory that basically defies known physics. For example:</p><blockquote><p><i><strong>Betting on the past.</strong></i>&nbsp;In my pocket (says Bob) I have a slip of paper on which is written a proposition P. You must choose between two bets. Bet 1 is a bet on P at 10:1 for a stake of one dollar. Bet 2 is a bet on P at 1:10 for a stake of ten dollars. So your pay-offs are as in [the table below]. Before you choose whether to take Bet 1 or Bet 2 I should tell you what P is. It is the proposition that the past state of the world was such as to cause you now to take Bet 2.</p><p>Ahmed argues that any <i>causal </i>decision theory worthy of the name would recommend taking Bet 1, simply because taking Bet 1 <i>causally </i><a href=\"https://en.wikipedia.org/wiki/Strategic_dominance\"><i><u>dominates</u></i></a>&nbsp;taking Bet 2.<a href=\"https://forum.effectivealtruism.org/posts/KzXSWKvbsSfEMryef/evidential-cooperation-in-large-worlds-potential-objections#fnqspw2otyk9s\"><sup>[9]</sup></a>&nbsp;</p></blockquote><p>In this scenario I don't actually see the intuition that supposedly pushes CDTs to take bet one. It's not clearly phrased, so maybe I'm not parsing it as intended, but it seems to me like the essence is supposed to be that the piece of paper says 'you will take bet 2.' If I take bet 2, it's true, if not, it's false. Since I'm financially invested in the proposition being true, I 'cause' it to be so. I don't see a case for taking bet 1, or any commitment to evidential weirdness from eschewing it.</p><blockquote><p><i><strong>The psychopath button.</strong></i>&nbsp;Paul is debating whether to press the \"kill all psychopaths\" button. It would, he thinks, be much better to live in a world with no psychopaths. Unfortunately, Paul is quite confident that only a psychopath would press such a button. Paul very strongly prefers living in a world <i>with </i>psychopaths to dying. Should Paul press the button?</p></blockquote><p>This is doing some combination of amphiboly and asking us to accept self-effacing premises.&nbsp;</p><p>There are many lemmas to this scenario, depending on how we interpret it, and none given me any concern for CDT:</p><ul><li>We might just reject the premises as being underspecified or false. It's very unclear what the result of pressing the button would be, so if Paul were me I would need to ask a great deal of questions first to have any confidence that this were really a positive EV action. But if I were satisfied that it was, I don't think I'm a psychopath, and wouldn't see any real reason why I <i>should</i> assume that I am one just because I'm doing something that harms a few to help many.</li><li>Otherwise, if we're imagining that pressing this button is extremely net good, then rather than switching to a belief system with spooky acausality, we might relax the highly unrealistic assumption that Paul is necessarily totally selfish (even if he does turn out to be a psychopath, they are perfectly <a href=\"https://www.quora.com/Can-psychopaths-sociopaths-be-good-people/answer/David-Horst-4\">capable of being altruistic</a>), and believe that he would push the button and risk death. In such a scenario, a CDT Paul would altruistically push the button and this seems optimal for his goals.</li><li>If we insist on the claim that he <i>is</i> totally selfish, then we're more or less defining him as a psychopath (see the <a href=\"https://dictionary.cambridge.org/dictionary/english/psychopath\">dictionary definition</a>, 'no feeling for other people'), so pressing the button would provide no relevant evidence but would definitely kill him. In such a scenario CDT Paul would selfishly <i>not</i> press it and this seems optimal for his goals.</li><li>If we modify the definition of psychopathy to 'both being willing to kill other people and also being totally selfish', then if Paul were to press it expecting it to kill him but improve the world, he would (by evidencing himself to be a psychopath and therefore choosing self-sacrifice for the sake of others) be evidencing himself to be <i>not</i> a psychopath. So the premises in this interpretation are self-effacing, and CDT Paul could reasonably go either way; or he might reasonably believe that such premises are either inconsistent or insufficient to allow him to decide.</li><li>Maybe we define psychopathy being willing to kill other people at all even in extremely bizarre circumstances, and even altruistically at great cost to oneself (which is a strange definition, but we're running out of options) and independent of that definition, we again insist insist on the claim that Paul is totally selfish. Then pressing the button will probably kill almost the entire world's population including Paul, and be a very bad thing to do. CDT Paul would have both altruistic and selfish reason <i>not </i>to push the button, and this seems optimal for his goals.</li></ul><p>Finally if we somehow still insist that pressing the button will necessarily make the world better and that CDT will require Paul to press it while other decision theories will not, this seems like a strike against all those other decision theories. Why would we want to promote algorithms that worsen the world?&nbsp;</p><p>If we're imagining that we get to determine how an AGI thinks, I would rather than give it an easily comprehensible and somewhat altruistic motivation than a perfectly selfish motivation with greater complexity that's supposed to undo that selfishness.</p><p>Newcomb's problem is similar. I won't go through all the lemmas in detail, because there are many and some are extremely convoluted, but an approximation of my view is that it's incredibly underspecified how we know that Omega supposedly knows the outcome and that he's being honest with us about his intentions, and what we should do as a consequence. For example:</p><ul><li>If we imagine he simulated us countless times, he <i>can't </i>know the outcome, because each simulation introduces a new variable (himself, with knowledge of N+1 simulations, where in the previous simulation he had had knowledge of N) that we could use to randomise generate a result that would seem random to him. So asserting him to be actually perfect requires magic and hence gives us no meaningful result under any decision theory.</li><li>If we don't try to sabotage him via randomness and we believe that he's doing endless simulations then a CDT without either the assumption of perfect selfishness or of closed individualism (without which, the 'next guy' is indistinguishable including to me from myself) will one-box because it will cause the <i>next</i> guy to get more money. Both of these assumptions seem reasonable to drop - or rather, I think they're unreasonable to hold in the first place - and in this case everyone ends up richer because we one-box without any metaphysical spookiness.&nbsp;</li><li>If we believe we have full information about his simulation process and insist on being perfectly selfish, then in the first simulation we should obviously 2-box, since there's nothing to lose (assuming he has no other way of knowing our intent; otherwise see below). Then, knowing that we're the Sim 2, we should two-box, since we can infer that Omega will have predicted we do that based on Sim 1. And so on, ad infinitum - we always two-box.</li><li>If we assert some more mundane scenario, like Omega is just a human psychologist with a good track record, then we need to assume his hit rate is much lower for it to be non-magic. Then his best strategy (if he's trying to optimise for correct predictions and not messing with us in some other way) is certainly going to be something trivial like 'always assume they'll one-box', or perhaps something fractionally more sophisticated like 'assume they'll one-box iff they've ever posted on Less Wrong'. That's going to dominate any personality-reading he does - and implies we should two-box.</li><li>If he's a machine learning algorithm who looks at a huge amount of my personal historical data and generates a generally accurate prediction, then we're bordering back on magic for him to have collected all that data without me knowing what was going to happen.&nbsp;<ul><li>If I <i>did</i> know or had a strong suspicion, then if the expected payoff was sufficient then as a CDT I should have always publicly acted in ways that imply I would one day one-box (and then ultimately two-box and expect to get the maximum reward).&nbsp;</li><li>If I <i>didn't</i> know, then choosing non-CDT examples throughout my life just in case I ever run into such an ML algorithm opens me to other easy exploitation. For eg, I have actually played this game with non-CDT friends, and 'exploited' them by just lying about which box I put how much money in (often after hinting in advance that I plan to do so). If they distrust me - even forewarned by a comment such as this - enough to call my bluff in such scenarios, they're giving evidence to the ML algorithm that it should assume they're two-boxers, and hence losing the purported benefit of rejecting CDT. Regardless, when we unexpectedly reach the event, we have no reason not to then two-box.</li></ul></li></ul><p>I realise there are many more scenarios, but these arguments feel liturgical to me. If the rejection of CDT can't be explained in a single well defined and non-spooky case of how it evidently fails by its own standards, I don't see any value in generating or 'clearing up confusions about' ever more scenarios, and strongly suspect those who try of motivated reasoning.</p>", "parentCommentId": null, "user": {"username": "Arepo"}}]