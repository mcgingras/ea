[{"_id": "PxYxRiQ2wpMkF8a3t", "postedAt": "2024-03-19T12:50:59.763Z", "postId": "XMrEvXrhAA2eKncJp", "htmlBody": "<blockquote><p>Perhaps it\u2019s just the case that the process of moral reflection tends to cause convergence among minds from a range of starting points, via something like <i>social logic</i>&nbsp;plus <i>shared evolutionary underpinnings</i>.</p></blockquote><p>Yes. And there are many cases where evolution has indeed converged on solutions to other problems<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"bmrh10mf6zw\" role=\"doc-noteref\" id=\"fnrefbmrh10mf6zw\"><sup><a href=\"#fnbmrh10mf6zw\">[1]</a></sup></span>.</p><ol class=\"footnote-section footnotes\" data-footnote-section=\"\" role=\"doc-endnotes\"><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"1\" data-footnote-id=\"bmrh10mf6zw\" role=\"doc-endnote\" id=\"fnbmrh10mf6zw\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"bmrh10mf6zw\"><sup><strong><a href=\"#fnrefbmrh10mf6zw\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Some examples:</p><p>(Copy-pasted from Claude 3 Opus. They pass my eyeball fact-check.)</p></div><ol><li>Wings: Birds, bats, and insects have all independently evolved wings for flight, despite having very different ancestry.</li><li>Eyes: Complex camera-like eyes have evolved independently in vertebrates (like humans) and cephalopods (like octopuses and squids).</li><li>Echolocation: Both bats and toothed whales (like dolphins) have evolved the ability to use echolocation for navigation and hunting, despite being unrelated mammals.</li><li>Venomous spines: Both porcupines (mammals) and hedgehogs (also mammals, but not closely related to porcupines) have evolved sharp, defensive spines.</li><li>Fins: Sharks (cartilaginous fish) and dolphins (mammals) have independently evolved similar fin shapes and placement for efficient swimming.</li><li>Succulence: Cacti (native to the Americas) and euphorbs (native to Africa) have independently evolved similar water-storing, fleshy stems to adapt to arid environments.</li><li>Flippers: Penguins (birds), seals, and sea lions (mammals) have all evolved flipper-like limbs for swimming, despite having different ancestries.</li><li>Ant-eating adaptations: Anteaters (mammals), pangolins (mammals), and numbats (marsupials) have independently evolved long snouts, sticky tongues, and strong claws for eating ants and termites.</li></ol></li></ol>", "parentCommentId": null, "user": {"username": "Peter_Hartree"}}, {"_id": "pjmwknLaJx8t4ZpAr", "postedAt": "2024-03-25T11:19:51.298Z", "postId": "XMrEvXrhAA2eKncJp", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/4xwWDLfMenw48TR8c/long-reflection-reading-list?commentId=SsfZwANGRzPeEmnAA\">This comment</a> I just made on Will Aldred's <a href=\"https://forum.effectivealtruism.org/posts/4xwWDLfMenw48TR8c/long-reflection-reading-list\">Long Reflection Reading List</a> seems relevant for this topic.&nbsp;</p><p>Overall, I'd say there's for sure going to be <i>some</i> degree of moral convergence, but it's often overstated, and whether the degree of convergence is strong enough to warrant going for the AI strategies you discuss in your subsequent posts (e.g., <a href=\"https://forum.effectivealtruism.org/posts/TmPSYEdQ6Mrb9oMLA/ai-strategy-given-the-need-for-good-reflection\">here</a>) would IMO depend on a tricky weighting of risks and benefits (including the degree to which alternatives seem promising).</p><blockquote><p>Does moral realism imply the convergent morality thesis? Not strictly, although it\u2019s suggestive. And even if you believe both, presumably there\u2019s some causal mechanism behind convergent morality. Personally, though, I find many intuitions that used to make me sympathetic to realism now make me sympathetic to the convergent morality thesis.</p></blockquote><p>I agree with this endnote.&nbsp;</p><p>For my anti-realism sequence, I've actually made the stylistic choice of <i>defining</i> (one version of) moral realism as implying moral convergence (at least under ideal reasoning circumstances). That's notably different from how philosophers typically define it. I went for my idiosnycratic definition because, when I tried to find out what are the action-guiding versions of moral realism (<a href=\"https://forum.effectivealtruism.org/posts/TwJb75GtbD4LvGiku/what-is-moral-realism\">here</a>), many ways in which philosophers have defined \"moral realism\" in the literature don't actually seem relevant for what we should do as effective altruists. I could only come up with two (very different!) types of moral realism that would have clear implications for effective altruism.&nbsp;<br><br>(1) Non-naturalist moral realism based on the (elusive?) concept of irreducible normativity.</p><p>(2) Naturalist moral realism where the true morality is what people who are interested in \"doing the most moral/altruistic thing\" would converge on under ideal reflection conditions.<br><br>(See <a href=\"https://forum.effectivealtruism.org/posts/SotZAFkGbgBEFBnQX/moral-uncertainty-and-moral-realism-are-in-tension#fn-ehEhra8iuK8YM5T6Y-8\">this endnote</a> where I further justify my choice of (2) against some possible objections.)</p><p>I think (1) just doesn't work as a concept, and (2) is almost certainly false at least in its strongest form. But yeah, there's going to be <i>degrees of</i> convergence, and moral reflection (even at the individual level without convergence) is relevant also from within a moral anti-realist reasoning framework.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Lukas_Gloor"}}, {"_id": "HHqETgoLFe72Yg2Ct", "postedAt": "2024-03-28T08:49:36.169Z", "postId": "XMrEvXrhAA2eKncJp", "htmlBody": "<blockquote>\n<p>Then I think for practical decision-making purposes we should apply a heavy discount to world A) \u2014 in that world, what everyone else would eventually want isn\u2019t all that close to what I would eventually want. Moreover what me-of-tomorrow would eventually want probably isn\u2019t all that close to what me-of-today would eventually want. So it\u2019s much much less likely that the world we end up with even if we save it is close to the ideal one by my lights. Moreover, even though these worlds possibly differ significantly, I don\u2019t feel like from my present position I have that much reason to be opinionated between them; it\u2019s unclear that I\u2019d greatly imperfect worlds according to the extrapolated volition of some future-me, relative to the imperfect worlds according to the extrapolated volition of someone else I think is pretty reasonable.</p>\n</blockquote>\n<ol>\n<li>You seem to be assuming that people's extrapolated views in world A will be completely uncorrelated with their current views/culture/background, which seems a strange assumption to make.</li>\n<li>People's extrapolated views could be (in part) selfish or partial, which is an additional reason that extrapolated views of you at different times may be closer than that of strangers.</li>\n<li>People's extrapolated views not converging doesn't directly imply \"it\u2019s much much less likely that the world we end up with even if we save it is close to the ideal one by my lights\" because everyone could still get close to what they want through trade/compromise, or you (and/or others with extrapolated views similar to yours) could end up controlling most of the future by winning the relevant competitions.</li>\n<li>It's not clear that applying a heavy discount to world A makes sense, regardless of the above, because we're dealing with \"<a href=\"https://www.alignmentforum.org/posts/dt4z82hpvvPFTDTfZ/six-ai-risk-strategy-ideas#Logical_vs_physical_risk_aversion\">logical risk</a>\" which seems tricky in terms of decision theory.</li>\n</ol>\n", "parentCommentId": null, "user": {"username": "Wei_Dai"}}, {"_id": "xxnApiuNDYWCyRgp2", "postedAt": "2024-03-28T10:36:07.654Z", "postId": "XMrEvXrhAA2eKncJp", "htmlBody": "<p>4 is a great point, thanks.</p><p>On 1--3, I definitely agree that I may prudentially prefer some possibilities than others. I've been assuming that from a consequentialist moral perspective the distribution of future outcomes still looks like <a href=\"https://forum.effectivealtruism.org/posts/iuzcdfsKQEEoPj2Gf/beyond-maxipok-good-reflective-governance-as-a-target-for\">the one I give in this post</a>, but I guess it should actually look quite different. (I think what's going on is that in some sense I don't really believe in world A, so haven't explored the ramifications properly.)</p>", "parentCommentId": "HHqETgoLFe72Yg2Ct", "user": {"username": "Owen_Cotton-Barratt"}}]