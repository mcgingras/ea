[{"_id": "ciwf4JXfMjqqz7oFn", "title": "Are there enough opportunities for AI safety specialists?", "postedAt": "2023-05-13T21:18:03.668Z", "htmlBody": "<p>I've seen a lot of articles, tweets, and forum posts urging more people to work on AI safety. While I agree that AI safety is extremely important and that more people should be working on it, I'm having a hard time imagining that a lack of interested young professionals is the key problem.</p><p>In this post, I briefly explore the job opportunities in AI safety as of May 13, 2023, for someone with a computer science / SWE background and for someone with an operations background, as both of those listed as key <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help\">ways that people can concretely help</a> on 80K's AI catastrophe problem profile, and both of those skill areas have over 30 corresponding jobs on&nbsp;</p><p>As of May 5, 20223, there are <a href=\"https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&amp;jobPk=5087\">110 roles for AI safety &amp; policy</a>. <a href=\"https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&amp;refinementList%5Btags_skill%5D%5B0%5D=Software%20engineering&amp;refinementList%5Btags_skill%5D%5B1%5D=Research&amp;jobPk=5087\">72 of them</a> are in research or software engineering, <a href=\"https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI+safety+%26+policy&amp;refinementList%5Btags_location_80k%5D%5B0%5D=Remote%2C+Global&amp;refinementList%5Btags_skill%5D%5B0%5D=Operations&amp;jobPk=5087\">40 of them</a> are in ops, and <a href=\"https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&amp;refinementList%5Btags_skill%5D%5B0%5D=Policy&amp;jobPk=5087\">15 of them are in policy</a>.&nbsp;</p><p>I'll use research and engineering jobs in AI safety as a first case study because it seems like this is the most directly related to addressing AI risks. Let's pretend that you have a SWE / CS background and are looking to work in AI research. Of the 72 AI research and engineering jobs listed on 80K,<a href=\"https://jobs.80000hours.org/?query=general&amp;refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&amp;refinementList%5Btags_skill%5D%5B0%5D=Software%20engineering&amp;refinementList%5Btags_skill%5D%5B1%5D=Research&amp;jobPk=5087\"> six are not actual jobs, but rather expressions of interest</a>, ostensibly so that the organizations can invite you to apply for roles that fit your profile down the road. &nbsp;<a href=\"https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&amp;refinementList%5Btags_location_80k%5D%5B0%5D=San%20Francisco%20Bay%20Area&amp;refinementList%5Btags_skill%5D%5B0%5D=Software%20engineering&amp;refinementList%5Btags_skill%5D%5B1%5D=Research&amp;jobPk=5087\">33 of the jobs are in San Francisco</a>, <a href=\"https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&amp;refinementList%5Btags_location_80k%5D%5B0%5D=London%2C%20UK&amp;refinementList%5Btags_skill%5D%5B0%5D=Software%20engineering&amp;refinementList%5Btags_skill%5D%5B1%5D=Research&amp;jobPk=5087\">11 are in London</a>, and <a href=\"https://jobs.80000hours.org/?query=remote&amp;refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&amp;jobPk=5087\">22 are remote.</a> &nbsp;</p><p>What if you are fresh out of school and want to work in this problem area? In the US in 2019, there were around <a href=\"https://www.universityworldnews.com/post.php?story=20190404113158365\">~ 65,000+ annual computer science graduates</a>, and I would expect that number to have gone up in recent years. There are currently <a href=\"https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&amp;refinementList%5Btags_location_80k%5D%5B0%5D=USA&amp;refinementList%5Btags_exp_required%5D%5B0%5D=Entry-level&amp;refinementList%5Btags_exp_required%5D%5B1%5D=Multiple%20experience%20levels&amp;refinementList%5Btags_skill%5D%5B0%5D=Software%20engineering&amp;refinementList%5Btags_skill%5D%5B1%5D=Research&amp;jobPk=5087\">only 28 entry level SWE / research roles in the US</a> this year. &nbsp;So even of &lt; 1% of CS grads want to get into AI safety, there would still be a massive surplus of labor.</p><p>I don't have a strong enough awareness of job market trends in this field to know whether the salaries are competitive, but they ranged very widely from around $100k-$350k a year depending on experience levels, and it seems like you could comfortably live in the Bay Area as a single, debt-free recent college grad at the bottom of the salary range.&nbsp;</p><p>I'll use operations jobs in AI safety as a second case study because it seems like operations is the easiest path to working on AI safety for people without a computer science or philosophy degree. (There are currently only <a href=\"https://jobs.80000hours.org/?query=communications&amp;refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&amp;jobPk=5023\">two communications roles</a> and <a href=\"https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&amp;refinementList%5Btags_skill%5D%5B0%5D=Policy&amp;jobPk=5023\">18 policy roles</a> related to the AI safety problem area on 80K.)</p><p>Let's pretend that you are an operations manager that doesn't have the time, money, or bandwidth to gain the technical skills for research or SWE jobs. You decide to check out the 40 &nbsp;operations \"jobs\" &nbsp;in the AI Safety problem area through 80K. Nine of them are \"general applications\" that don't seem to be tied to an actual position that exists (there's no clear filter for this, I hand-counted.) <a href=\"https://jobs.80000hours.org/?query=information&amp;refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&amp;refinementList%5Btags_skill%5D%5B0%5D=Operations&amp;jobPk=5087\">Eight of them</a> are actually about information security, not operations. <a href=\"https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&amp;refinementList%5Btags_location_80k%5D%5B0%5D=San%20Francisco%20Bay%20Area&amp;refinementList%5Btags_skill%5D%5B0%5D=Operations&amp;jobPk=5087\">Twenty-four of them</a> are in the Bay area, where <a href=\"https://www.sacbee.com/news/california/article213953309.html\">if you make less than $117k a year</a>, you technically live in \"poverty\" by some measures. Most of the ops jobs for AI safety on the 80K site don't have salaries posted, but <a href=\"https://far.ai/post/2023-03-office-manager/\">this one </a>pays a mere $25-$45 per hour. So if someone worked 40 hours a week at this job (assuming paid vacation or you never take time off), you'd make a minimum of $52,000 a year and a <strong>maximum</strong> of $93,000 a year before taxes. Depending on your personal situation, that may or may not be doable.</p><p>In contrast, there are <a href=\"https://www.linkedin.com/jobs/search/?currentJobId=3582004044&amp;geoId=103644278&amp;keywords=AI%20research&amp;location=United%20States&amp;refresh=true\">7,000 AI research job</a>s, <a href=\"https://www.linkedin.com/jobs/search/?currentJobId=3594989777&amp;f_WT=2&amp;geoId=103644278&amp;keywords=%22operations%20manager%22&amp;location=United%20States&amp;refresh=true\">10,000 remote operations manager roles</a>, and over <a href=\"https://www.linkedin.com/jobs/search/?currentJobId=3597607663&amp;f_WT=2&amp;geoId=103644278&amp;keywords=%22executive%20assistant%22&amp;location=United%20States&amp;refresh=true\">4,101 executive assistant roles</a> advertised in the US alone for LinkedIn. These numbers are probably slightly inflated because LinkedIn's keyword search function is not that great, but even if we cut that in half, that still means that there are orders of magnitude more jobs for these people outside of the narrow AI safety niche.&nbsp;</p><p>&nbsp;I have also completely sidestepped the issue of getting visa sponsorship, etc. if you're not from the US or some other wealthy Western country and want to work on this.</p><p>It seems to me that a lot of the well-meaning pleas for EAers to consider working in AI research are a bit misdirected. &nbsp;It seems more plausible to me (admittedly as an outsider to the AI safety space) that it's the funding and supply that are the bottleneck here.</p><p>&nbsp;</p><p>**These categories are not mutually exclusive.</p>", "user": {"username": "mhint199"}}, {"_id": "xzJ9uNotWGDHznGi9", "title": "Can AI solve climate change?", "postedAt": "2023-05-13T20:44:30.527Z", "htmlBody": "<p>I've seen a lot on the EA and 80,000 Hours websites about the dangers of misaligned AI, which is fair enough. But I haven't seen much about how well-aligned AI can help us. In particular, climate change seems like an ideal problem to put AI to work on, because it's so complex and systemic.</p><p>AI and climate change are two of the major concerns of longtermism. Why not link them together and make one the solution to the other one?</p>", "user": {"username": "Vivian"}}, {"_id": "PoxqqMAmFAu4aZoxK", "title": "An Analogy for Understanding Transformers", "postedAt": "2023-05-13T12:20:25.715Z", "htmlBody": "", "user": {"username": "Callum McDougall"}}, {"_id": "GD73T2xpNcx4Rvt2E", "title": "The Implications of the US Supreme Court upholding Prop 12", "postedAt": "2023-05-13T07:04:12.485Z", "htmlBody": "<p><strong>Background: Farming Conditions and Prop 12</strong></p><p>Currently in the US, most breeding pigs live in factory farmers, where they are confined in gestation crates which are small metal cages so small that pigs can\u2019t even turn around, while egg-laying hens live in tiny, cramped battery cages that cause a range of <a href=\"https://thehumaneleague.org/article/battery-cages\">psychological and physiological harm</a>. The crowded conditions also have potential health harms by increasing the stress levels of pigs and weakening their immune systems, which can make them more susceptible to zoonotic diseases that may spread to humans.</p><p>Starting in the early 2000s, a few animal welfare groups including the Humane Society of the Unites States aimed to ban the farming system of cages for hens, breeding pigs and veal calves. In 2008, <a href=\"https://ballotpedia.org/California_Proposition_2,_Standards_for_Confining_Farm_Animals_(2008)\">Proposition 2</a> was passed which put in place a \u201cproduction\u201d ban on cages, which said that producers had to ensure pigs, hens, and calves could lie down, turn around, and extend their limbs or wings without hitting the side of an enclosure. However, this specific language allowed some egg farms to circumvent the law by using bigger cages. In 2010, California passed AB 1437 which was a \u201csales\u201d ban requiring all eggs sold in California had to meet those standards. These laws have brought about results \u2014 the share of hens that are cage-free has been rising and is expected to continue doing so.<br>&nbsp;</p><p><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjRNFIoAccoOWfzX7i2TehrN8ZaNeAOvyf8q-euJbb9GFRWDQ0F_-MIlm0cj_JnbwZAItPDLGn3sIrWulBfxAuDz_Pe2Vys2Ed0JnJ6WCnS1zdFQqC8_phiA-1BFstZESFbYdkuwmtTux8ooWnoAjZKaVWZYTzwW-A5ZCRdVZESwOIN0jOc8WAlvh1m\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GD73T2xpNcx4Rvt2E/n8y9mcbenrjykj4iziwh\" alt=\"\"></a></p><p>In 2018, over 62% of California voters passed <a href=\"https://www.cdfa.ca.gov/AHFSS/AnimalCare/\">Proposition 12</a>, the strongest law to improve conditions for farmed animals. Under Prop 12, some of the gaps in these laws are covered \u2014 for one, it extends the cage-free ban to cover <a href=\"https://www.vox.com/future-perfect/2022/10/9/23393017/supreme-court-pork-pigs-prop-12-california-animal-welfare\">not just the eggs</a> that are sold in the grocery store (shell eggs) but also liquid eggs, which are sold to restaurants, cafeterias and food manufacturers (liquid eggs).</p><p><strong>Opposition from Pork Industry</strong></p><p>The law is expected to be especially impactful to the pork industry which has been more resistant to change in doing away with confinement systems. Progress has been very mixed in terms of companies following through with their commitments to phase out gestation crates. So far, <a href=\"https://www.farmforward.com/issues/animal-welfare/gestation-crates/\">10 states</a> have banned them, but Prop 12\u2019s space requirements are stricter and close some gaps that allow for loopholes. The law also makes it illegal for eggs and pork to be sold in California if the animals in other states are put in gestation crates (pigs) or battery cages (for chickens). California consumes 14% of the US\u2019s pork and 12% of eggs and veal, so pork and egg producers would be forced to modify barns or construct new ones (only 1% of existing sow housing <a href=\"https://civileats.com/2020/10/26/could-crate-free-pork-become-the-new-industry-standard/\">meets Prop 12\u2019s standards</a> according to the National Pork Producers Council (NPCC)), which would be costly and time taking, causing various meat trade groups to be opposed to it. Interestingly, some industries such as <a href=\"https://www.sfchronicle.com/food/article/California-s-Prop-12-hasn-t-resulted-in-a-16771372.php\">Whole Foods</a>, aren\u2019t concerned with the law as they claim they already meet animal welfare requirements. I think this is a crucial reason why the phase out of battery cages did not get as much opposition to phasing out pork crates \u2014 many companies <a href=\"https://thehumaneleague.org/article/yum-cage-free\">already have commitments</a> to phase out battery cage. In fact, these companies <i><strong>may have the incentive to increase regulations to raise costs on competitors.</strong></i></p><p>For this reason, the law was attacked by various meat industry trade groups, which filed three separate lawsuits to overturn it. The Supreme Court declined to take two of them, and in October 2022, the case National Pork Producers v. Ross began.</p><p><strong>Explaining the Supreme Court Ruling</strong></p><p>On May 11th, 2023, the Supreme Court <a href=\"https://www.nytimes.com/2023/05/11/us/supreme-court-california-pigs.html\">upheld Prop 12</a> in a 5-4 decision of the case National Pork Producers v. Ross. Interestingly, the verdict was not split along conservative-liberal lines, with 3 conservative judges and 2 liberal judges in the majority.</p><p>The pork industry had invoked the \u201cdormant commerce clause\u201d which prevents states giving in-state businesses preferential treatment over other states. They claimed this law would affect out of state producers and force them to abide by California\u2019s rules because of the size of California\u2019s market and the fact that nearly all pork consumed in the state is produced outside it. They said this would harm consumers and raise costs substantially, hurting farmers, and that the decision sets a dangerous precedent for how states can impose regulations for consumers and businesses outside the state. However, this was weakened by five of the largest pork producers <a href=\"https://www.smithfieldfoods.com/getmedia/7ecf12e2-da3b-4d31-8796-d07e38b39e51/2021-Sustainability-Impact-Report.pdf\">saying</a> they <a href=\"http://q4live.s22.clientfiles.s3-website-us-east-1.amazonaws.com/104708849/files/doc_financials/2021/q3/08-11-21_Tyson-Foods-080921.pdf\">could comply</a> with the law.</p><p>Justice Gorsuch writing the majority said that the Constitution\u2019s Commerce Clause forbids discriminatory practices by states to protect their businesses, but that Prop 12 clearly does not do this as in-state businesses have the same regulations imposed. On one hand, Prop 12 may increase costs for out of state producers who comply with the law, on the other, the law serves the moral and health benefits of in-state residents. It is not clear which concern is more compelling, and therefore Gorsuch and two other justices claimed the decision should belong to the people and their representatives. Two other justices which agreed on the decision, but differed on the reasoning, claimed that the pork producers had not satisfied the threshold to prove that there would be a substantial burden on interstate commerce.</p><p>The <a href=\"https://www.washingtonpost.com/politics/2023/05/11/california-pork-rule-supreme-court/\">dissenting judges</a> said the \u201csweeping extraterritorial effects\u201d the law had justified sending it back to the appeals court to consider whether the burdens imposed to businesses outweighed the benefits.</p><p><strong>Implications of the Ruling</strong></p><p>For pork producers, complying will likely take time and effort, but they have enjoyed <a href=\"https://www.forbes.com/sites/chloesorvino/2021/03/25/with-a-banner-2020-behind-it-meat-giant-jbs-sets-sights-on-us-ipo/?sh=7a5e01e669d8\">massive windfall profits</a> in 2020 and as egg producers have shown, this is certainly possible. The NPCC estimates the costs of complying would be around $300 million. These are likely exaggerated figures, given the incentives of the NPCC, but even if so, given that the export sales alone of pork in 2022 exceeded sales of $7.7 billion (exports account for 27.5% of total pork production), the industry should be able to comply.</p><p>The law could send an important precedent for future animal welfare decisions that affect out of state businesses, and allow other states to follow suit. I do not think this verdict necessarily effects \u201canimal rights\u201d directly, as the moral rights animals have, was not really what the case was about. <strong>However, it may increase the ability for states that consume a lot of meat, such as Florida, to implement not just production bans, but sales bans that have repercussions for the entire country.</strong> This is important because a lot of states with <a href=\"https://www.meatinstitute.org/index.php?ht=d/sp/i/47465/pid/47465\">high livestock production</a> have relatively lower meat consumption, since they\u2019re rural areas, so sales bans in large states have far greater effects on these regions.</p><p>Furthermore, the pork industry will suffer the impacts of fighting court battles instead of changing their barns and operations. In the future, I think this verdict make it is less likely they focus efforts on challenging the law and instead work to comply with regulations to ensure smoother transitions.</p><p>What about the impact on prices? Researchers at the University of California, Davis, estimate that costs of pork products will&nbsp;<a href=\"https://caes.ucdavis.edu/news/are-researchers-estimate-proposition-12-will-cost-california-pork-consumers-320-million\">rise by $0.25 per pound</a> and consumption will reduce by 6.3% in the state (<a href=\"https://s.giannini.ucop.edu/uploads/pub/2021/08/17/v24n6_2.pdf\">Lee, Sexton, Sumner 2021</a>) in \u201cpork shortages.\" The impacts remain to be seen when the implementation of the law will continue on July 1st, 2023.&nbsp;Price increases may have positive effects since lower consumption would reduce the number of pigs that need to be slaughtered, and hurt the pork industry.&nbsp;Rising prices can also increase the demand for alternative proteins as a substitute.&nbsp;However, if price increases are too high, consumers in the future may be less likely to support welfare reforms if they believe there is a large personal cost to them.</p><p>I also hope that the justices not voting on partisan lines means animal welfare can continue to stay out of the culture war, and a bipartisan consensus can be reached about the need to increase welfare standards.</p><p>To be clear, Prop 12 is far from the end. Chickens still have their beaks cut off without anaesthesia, sows are still confined in farrowing crates, and broiler chickens still grow so fast their legs collapse under their own weight. However, it marks the progress of decades of campaigning.</p>", "user": {"username": "ishankhire"}}, {"_id": "xb7G5yKYn8Q8zLXua", "title": "Advice for newly busy people", "postedAt": "2023-05-12T23:47:38.901Z", "htmlBody": "", "user": {"username": "Severin T. Seehrich"}}, {"_id": "xkmiLmecWnD4LKRQ2", "title": "\ud83c\udfdc\ufe0f EA is in Albuquerque!", "postedAt": "2023-05-12T22:09:54.476Z", "htmlBody": "<p>Hey all! Just in case there's anyone on the forums living in or visiting New Mexico I wanted to spread the word that we have a meetup going on in Albuquerque. We've been running it for about a year and we meet Wednesdays in Nob Hill. Next week we're running a Giving Game with Giving What We Can! Below is next week's meetup for any interested!</p><p>https://www.meetup.com/ea-abq/events/dmwldtyfchbwb/</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xkmiLmecWnD4LKRQ2/few1a1jq2jtkmbkmxzri\"></figure>", "user": {"username": "Alex Long"}}, {"_id": "syyFCv5bfizfoZ8Ha", "title": "Proposed - 'How Much Does It Cost to Save a Life?' Quiz, calculator, tool", "postedAt": "2023-05-12T21:01:17.416Z", "htmlBody": "<p><strong>Epistemic basis/status:</strong> I've talked this over with Grace and others at GWWC, and people seem generally interested. I'm posting this to get feedback and gauge interest before potentially pushing it further.</p>\n<h1>Basic idea</h1>\n<p>I'd like to get your thoughts on a \"How Much Does It Cost to Save a Life?\"<sup class=\"footnote-ref\"><a href=\"#fn-czuuPNwiqBpowtXdP-1\" id=\"fnref-czuuPNwiqBpowtXdP-1\">[1]</a></sup> quiz and calculator. I've been discussing this with Giving What We Can; it's somewhat modeled off their <a href=\"https://howrichami.givingwhatwecan.org/how-rich-am-i?gad=1&amp;gclid=CjwKCAjwuqiiBhBtEiwATgvixDzhS1kZwED4bv5jwJnCRDS-AWrVhnFJVSjLTBmgmr21ib9oNW9ZjxoCZ60QAvD_BwE\">how rich am I</a>  calculator, which drives a lot of traffic to their site.</p>\n<p>This would mainly target non-EAs, but it would try to strike a good balance between sophistication and simplicity. It could start as a quiz to get people's attention. People would be asked to guess this cost. They could then be asked to reconsider it considering some follow-up questions. This might be a good opportunity for a chatbot to work its magic.</p>\n<p>After this interaction, the 'correct answer' and 'how well did I do' would take you to an interactive page, presenting the basic calculation and reasoning. (Before or after presenting this) it could also allow users to adjust their moral and epistemic parameters and the scope of their inquiry. This might be something to unfold gradually, letting people specify first one thing, and then maybe more, if they like.</p>\n<p>E.g.,</p>\n<ul>\n<li>Target: Rich or poor countries, which age groups, etc.</li>\n<li>Relative value of a child or adults life</li>\n<li>How much do you weight life-years for certain states</li>\n<li>Which evidence do you find more plausible</li>\n<li>Do you want to include or exclude certain types of benefits</li>\n<li>Discount rate</li>\n</ul>\n<p>We would aim to go viral (or at least bacterial)!</p>\n<h1>Value/ToC</h1>\n<p>I believe that people would be highly interested in this: it could be engaging and pique curiosity and competitiveness (a bit click-baity, maybe, but the payoff is not click bait)!</p>\n<p>It could potentially make news headlines. It\u2019s an \u201ceasy story\u201d for media people, asks a question people can engage with, etc. \u2026 \u2019how much does it cost to save a life? find out after the break!) giving the public a chance to engage with the question: \"How much does it cost to save a life?\"</p>\n<p>It could help challenge misconceptions about the cost of saving lives, contributing to a more reality-based, impact-focused, and evidence-driven donor community.   If people do think it\u2019s much cheaper than it is, <a href=\"https://www.givewell.org/cost-to-save-a-life\">as some studies suggest</a>, it would probably be good to change this misconception. It may also be a stepping stone towards encouraging people to think more critically about measuring impact and considering EA-aligned evaluations.</p>\n<p>--&gt; Greater acceptance and understanding of EA, better epistemics in the general public, better donation and policy choices</p>\n<h1>Implementation</h1>\n<p>While GiveWell does have a page with a lot of technical details, it doesn't quite capture the interactive and compelling aspects I'm envisioning for this tool.</p>\n<p>Giving What We Can's response has been positive, but they understandably lack the capacity within their core team to take on such a project. They suggest it could make for an interesting volunteer project if a UX designer and an engineer were interested in participating.</p>\n<p>Considering the enthusiasm and the potential for synergy with academic research (which could be supported by funds for Facebook academic ads), I'm contemplating the best approach to bring this idea to life. I tentatively propose the following steps:</p>\n<ol>\n<li>Put out a request for a volunteer to help develop a proof of concept or minimum viable product. Giving What We Can has some interested engineers, and I could help with guidance and encouragement.</li>\n</ol>\n<p>2 Apply for direct funding for the project, possibly collaborating with groups focused on quantitative uncertainty and \"build your own cost-effectiveness\" initiatives, or perhaps with SoGive.</p>\n<p>I'd love to hear your thoughts, feedback, and any suggestions you may have for moving forward with this idea.</p>\n<p>(And GPT4 tells me to write: \"Together, we can create a tool that truly engages people and inspires them to think more deeply about the impact of their giving.\")</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-czuuPNwiqBpowtXdP-1\" class=\"footnote-item\"><p>We'd probably initially start with human lives ala Givewell. <a href=\"#fnref-czuuPNwiqBpowtXdP-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "david_reinstein"}}, {"_id": "4dHGxLzkX89h969uE", "title": "Aggregating Utilities for Corrigible AI [Feedback Draft] ", "postedAt": "2023-05-12T20:57:03.755Z", "htmlBody": "", "user": {"username": "Dan Hendrycks"}}, {"_id": "4vbdrKwMnLmCPsiTc", "title": "Policy Pub Night (organised by 1Day Sooner)", "postedAt": "2023-05-12T20:40:40.053Z", "htmlBody": "<p>Calling all policy folk, especially those attending EAG London! Come on down and have a relaxed drink with others in the field. It's only a 14min walk from Tobacco Dock. Drinks are on us!</p><p>This will hopefully be the first of a regular social. So don't worry if you can't make it, and please look out for future events.</p><p>We have 50 spots, so <strong>please RSVP through the link</strong>. Hope to see you there!<br><br>Any questions or queries, please email Danica at danica.marcos@1daysooner.org</p>", "user": {"username": "1daysooner"}}, {"_id": "oLkoKCo3cDzuc996G", "title": "Coattailing and Funging to learn: strategies for non-expert donors", "postedAt": "2023-05-12T17:58:21.673Z", "htmlBody": "<p><i>Note: I had planned to ask several parties to review this before publishing, however I've just seen the&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/eibgQcbRXtW7tukfv/discussion-about-ai-safety-funding-fb-transcript\"><i><u>recent discussion</u></i></a><i> about AI safety funding, and this prompted me to publish this sooner than planned. People who would have been asked to comment: people running Open-Phil-funded orgs and contacts at Open Phil. Expect more corrections than usual as a result.</i></p><p>&nbsp;</p><p>Where a donor wants to support a cause area but is not knowledgeable enough about it, they can use the \u201ccoattail-to-learn\u201d and \u201cfunge-to-learn\u201d strategies. This involves:</p><ul><li>Picking charities that are already funded by a major funder such as Open Phil</li><li>Maintain a relationship with the recipient for some time (e.g. for a few years), and use that relationship to learn more about their work</li><li>In time, use that knowledge to develop your own thinking on giving</li></ul><p>&nbsp;</p><p><strong>Coattailing or funging against Open Phil allows you to borrow their thinking</strong></p><p>If you are an ambitious donor trying to do the most good you can with your money, you may be tempted to find the highest impact donation opportunities on the margin. I.e. your logic might be: \u201cthe orgs funded by Open Phil are already adequately funded \u2013 I want to find more neglected giving opportunities\u201d.</p><p>However:</p><ul><li>Open Phil researches their philanthropy very thoughtfully.</li><li>If you think you can outperform them you should have very strong reasons to believe this.</li></ul><p>And where your donation has a funging effect, it\u2019s like you\u2019re playing a big game of whack-a-mole, where the problem of where to have the most impact is whacked away from you and springs up on the Open Phil side of the whack-a-mole gameboard. (This depends on a few assumptions, outlined below).</p><figure class=\"image image_resized\" style=\"width:49.23%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/oLkoKCo3cDzuc996G/di06k02csdjxpdf2kb2q\" alt=\"40+ Whack A Mole Stock Photos, Pictures &amp; Royalty-Free Images - iStock |  Whack a mole game, Whack a mole mallet, Whack a mole machine\"></figure><p>The problem of finding high impact giving opportunities is a tough one, and giving that problem to Open Phil is a good thing, if you believe Open Phil are good at thinking about and effecting philanthropy. This is an important assumption; assessing it is outside the scope of this post.</p><p><strong>The strategy still allows you to be an active donor</strong></p><p>If you are a donor who doesn\u2019t want to expend lots of effort thinking about your donations, then you don\u2019t want to be a heavily actively involved donor. You have other options, such as getting philanthropic support from a philanthropic advisor who can do lots of the work for you (full disclosure: we at SoGive offer such a service), or you can donate to&nbsp;<a href=\"https://funds.effectivealtruism.org/\"><u>EA Funds</u></a>. This document is not for those donors.</p><p>The funge-to-learn and coattail-to-learn strategies are for donors who do want to add their own thoughts and perspectives to their giving process. At first glance, it may seem like the strategy robs you of the opportunity to add your own insights \u2013 after all, it\u2019s really Open Philanthropy\u2019s thinking that actually determines where the money goes. In the near term, this is true, but another way of thinking about it is that this strategy still allows you to add your own distinctive thoughts and perspectives, just not hastily.</p><p>We at SoGive hold the view that we should not have an ecosystem who all defer to Open Philanthropy or GiveWell. Indeed, we think it\u2019s likely that further scrutiny will highlight areas where those organisations could have done better, and the more that other individuals and organisations are enabled to do this, the better.</p><p>But in our view improving the diversity of expert donors does not involve encouraging inexperienced donors to rush in where angels fear to tread.</p><p>We believe that the funge-to-learn and coattail-to-learn strategies can help donors on the journey to critically assessing giving decisions made by Open Philanthropy. It involves donors making donations and taking actions, and gradually, thoughtfully forming their own views.</p><p><strong>How do you know whether you are a non-expert learner or an expert donor?</strong></p><p>If you are already an expert donor, you don\u2019t need to follow a funge-to-learn or coattail-to-learn strategy. How do you know where you are on the scale from non-expert learner or an expert donor?</p><p>Some questions which might help you make this assessment:</p><ul><li>How many reports (or documents) have you written which include careful reasoning about the topic? If the answer is zero, it\u2019s highly likely that you are a non-expert. If it\u2019s more than zero, it depends.</li><li>One test is to ask yourself if you can articulate some common failure modes as a donor. The more you struggle with this, the greater the risk that you are prone to those failure modes.</li><li>The threshold you should set depends on the cause area. For example, at time of writing, there\u2019s an upsurge of interest in AI safety.&nbsp;<ul><li>If you\u2019ve been in EA for several years, you have probably been thinking about or hearing about AI alignment for a while, whereas your non-EA friends might find these ideas novel. If this describes you, this level of knowledge is not sufficient to make you an expert donor.</li></ul></li><li>In case of doubt, it\u2019s typically better to err on the side of caution, especially in those areas of longtermist work where donor error leads not just to failing to do good, but to actually making things worse.</li></ul><p><strong>Orgs funded by major foundations such as Open Phil often welcome other donors</strong></p><p>Orgs funded by (e.g.) Open Phil may welcome other donors for two reasons:</p><ul><li>Their Open Phil funding may not cover their funding needs \u2013 if you are filling the gap you are \u201ccoattailing to learn\u201d</li><li>Their funding needs may be filled, but they may be keen to diversify their funding sources \u2013 if you are providing that diversification you are \u201cfunging to learn\u201d</li></ul><p><strong>How confident can we be that this would actually ending up funging?</strong></p><p>Unlike the coattail-to-learn strategy, the funge-to-learn strategy depends on the whack-a-mole effect working, i.e. that the funding does funge to Open Phil, as opposed to just making the recipient org richer than they need to be.</p><p>You can\u2019t be 100% sure that that funging effect will occur. You can choose between \u201cweak funge-to-learn\u201d and \u201cstrong funge-to-learn\u201d strategies:</p><ul><li>Under \u201cstrong funge-to-learn\u201d, you explain to the org your strategy. You then ask whether they are still willing to receive a donation if it comes with the requirement that when they next request funds from Open Philanthropy (or whoever) the request should be lower to account for the fact the org is receiving funds from you.<ul><li>If they realise that the money is simply funging against Open Philanthropy, will they value you less? Probably, but the received wisdom in fundraising circles suggests that (a) you want diversification for risk management reasons (b) your goal as a fundraiser is to build relationships, so even if the donor has the intent to use donations to you as a stepping stone, the relationship is still worth building. Especially if you are a substantial donor, or part of a collaborative group of donors who are collectively substantial. (Note: SoGive is willing to facilitate such collaborations)</li></ul></li><li>Under weak funge-to-learn, you don\u2019t explain this to your funding recipient, and you don\u2019t include any requirements to adjust the next funding request that they put into Open Philanthropy.&nbsp;<ul><li>This may still result in funging, but the funging effect will likely be weak or non-existent.</li><li>Two relevant factors are:<ul><li>How large your donation is relative to the org\u2019s total budget. A larger donation is more likely to lead to funging.</li><li>Their reserves policy and how they implement it. Best practice suggests a charity should have an upper limit, i.e. threshold above which they deem themselves to be \u201ctoo rich\u201d, however several charities don\u2019t have this. Note that even if they have such a policy, and even if your donation is large enough to cause them to hit this limit, the result may be for the charity to set up a new project (rather than return money to Open Philanthropy).</li></ul></li></ul></li></ul><p>&nbsp;</p><p><strong>Is talking to leaders of organisations sufficient to learn what you need to be an expert donor?</strong></p><p>No. There are other things which you should be doing as well, such as:</p><ul><li>Reading lots of relevant content about cause area</li><li>Reviewing other giving decisions which are in the public domain, e.g. those made by EA Funds, or Open Phil.</li><li>Writing up your thinking on where donations should go.</li></ul><p>But having access to people in the relevant organisations certainly helps.</p><p><strong>How much should you be donating for this to work?</strong></p><p>Crucial to the funge-to-learn strategy is that you are donating enough that the recipient feels it\u2019s worth their time to have a conversation with you. Here\u2019s a rough indication of whether I expect an organisation is willing to have a 1-1 hour-long conversation with you once a year.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/oLkoKCo3cDzuc996G/mzyun8ka4xx1d7jveq1t\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/oLkoKCo3cDzuc996G/yvc7dqt2p05tr6zttkxk 122w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/oLkoKCo3cDzuc996G/xivbpfnkmtw7liomk8sl 202w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/oLkoKCo3cDzuc996G/ksnxpihiasppdidw9okv 282w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/oLkoKCo3cDzuc996G/rscybj1krn2azwmxjdp6 362w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/oLkoKCo3cDzuc996G/vpcldezxxz5aj6okolqp 442w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/oLkoKCo3cDzuc996G/j2j18mt7r6i3g7lvvsrs 522w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/oLkoKCo3cDzuc996G/u9lagkdk6khibpikkccv 602w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/oLkoKCo3cDzuc996G/jr5pt7ldwgldbzj0jipd 682w\"></figure><p>&nbsp;</p><p><strong>How should you be a \u201cwell-behaved\u201d funge-to-learn donor?</strong></p><p>As a funge-to-learn or coattail-to-learn donor, you are a serious donor, and you should not exhibit the behaviours that sometimes give donors a bad reputation. This is particularly true when the people you are meeting with are the leaders of the organisation you are donating to. Building a relationship with you may be part of their job, but putting lots of effort into educating you shouldn\u2019t be. If the people you\u2019re interacting with are fundraisers, it\u2019s slightly different, but even then it\u2019s possible to place excessive demands on them.</p><p>Here is a not-comprehensive list of guidelines for good behaviour:</p><ul><li>Treat the scheduled meeting with the charity as a rare, valuable opportunity to gain access to the time of people doing high impact work, along with the typical markers of respect that come with that.<ul><li>If the demands on your time mean that that is difficult, e.g. you risk arriving at meetings late, or are not able to give them your full attention, partnering with SoGive or another philanthropic advice organisation may allow you to have a representative present at your meetings, and who can support you with your thinking as well.</li></ul></li><li>Often, funders will use conversations with recipient orgs to help them decide whether and to what extent they want to support the organisation. As it happens, talking to an organisation with a view to determining whether to fund them is quite a good way of helping you think about the organisation while engaging your critical faculties.</li><li>If you are not dedicating all your time to your philanthropy, you should expect that you will need some years of learning before you are expert. If you know that this is true, it makes things easier for the organisation if you tell them that you plan to provide them with funding for the next (say) 3 years.<ul><li>Note that there is a tension between this guidance and the previous bullet point.</li></ul></li><li>Some of the advice in earlier sections of this post suggests you should write up your thoughts on effective giving. If you are going to have a conversation with an org and then publish something about them (e.g. on the EA Forum or elsewhere in the public domain) then you should (a) let them know prior to the meeting (b) let them see a draft before you publish.</li></ul><p><br><br><br>&nbsp;</p>", "user": {"username": "Sanjay"}}, {"_id": "fTvw6K3CfxXdxAE5G", "title": "A Study of AI Science Models ", "postedAt": "2023-05-13T19:14:59.708Z", "htmlBody": "<p><i>Generated during AI Safety Camp 2023 by the AI Science Team: Eleni Angelou, Cecilia Tilli, Louis Jaburi, Brian Estany. Many thanks to our external collaborators Joshua Flanigan and Rachel Mason for helpful feedback.&nbsp;</i></p><h1>&nbsp;</h1><h1>1. Introduction: Why science models?&nbsp;</h1><p>Generating new science is a complex task and requires the mastery of a series of cognitive abilities. Thus, it is plausible that models trained to perform well across various scientific tasks are likely to develop powerful properties. The speculations on why that would be the case vary. In one hypothetical scenario, the training and finetuning of a Large Language Model (LLM) could lead to a&nbsp;<a href=\"https://www.lesswrong.com/posts/eD34hTMp8uv3ifSjg/consequentialists-one-way-pattern-traps\"><u>consequentialist</u></a> that would perform optimally given a certain objective e.g., designing an experiment. Alternatively, while current science models are non-agentic, such non-agentic models (e.g., an LLM) can be&nbsp;<a href=\"https://arxiv.org/abs/2304.05332\"><u>turned into consequentialist agents</u></a>.&nbsp;</p><p>A study of science models is motivated by 1) the speculation that&nbsp;<a href=\"https://www.lesswrong.com/posts/eaDCgdkbsfGqpWazi/the-basic-reasons-i-expect-agi-ruin\"><u>science models could lead to existentially risky agents</u></a> and 2) the suggestion that different aspects of scientific research&nbsp;<a href=\"https://www.alignmentforum.org/posts/JKgGvJCzNoBQss2bq/beliefs-and-disagreements-about-automating-alignment\"><u>including AI alignment research</u></a> could be automated in the near-term future. For these two reasons, it is valuable to have an overview of the available AI science models and their \u201ccognitive properties\u201d or capabilities. This will provide a clearer picture of what the state-of-the-art of science models is and what their implications are for AI safety.</p><p>There are several different types of existing models that are useful in scientific research, but so far none of them is able to do autonomous research work. Instead, they operate as tools for a human researcher to solve subtasks in the research process. In this post, we will assess the current capabilities of such tools used in empirical and formal science. In section 4, we also study the capabilities of LLMs to systematically generate knowledge and then subsequently build on knowledge in a process of scientific inquiry.</p><h1>2. Empirical science</h1><p>Empirical science refers to science that builds knowledge from observations (contrasted to formal science, such as logic and mathematics). In empirical science the scientific process is composed of tasks that generally fall into three classes:&nbsp;</p><ol><li>Assessment of existing knowledge</li><li>Hypothesis generation</li><li>Hypothesis testing</li></ol><p>Note that a given research project need not involve all of these tasks: a certain study could be purely about hypothesis generation, or even purely about assessment of existing knowledge in the case of a literature review.</p><p>In the assessment of existing knowledge and hypothesis-generating research, it is already clear that existing ML models have important capabilities that make them very useful tools for humans performing research. For hypothesis testing tasks, there are obviously statistical tools for data analysis, but for our purposes, we focused on the ability to create experimental designs as these seem to depend on more general cognitive capabilities. We will look into how well existing language models perform on this task.</p><h2><strong>2.1 Collection and assessment of existing data&nbsp;</strong></h2><p>The first step in scientific research is, in general, the collection of available data and the assessment of their relevance to the problem under examination. This stage informs the subsequent stages of hypothesis generation as well as the formulation of specific puzzles and research questions. The success of this step, thus, largely determines how the research itself will move forward and impacts the quality of every step of a science project.&nbsp;</p><p>LLMs, with the right prompt engineering, exhibit impressive accuracy in filtering out the desired parts of a corpus of literature. Traditional literature reviews using tools such as Google Scholar tend to be more time-consuming and therefore can be less efficient compared to delegating the task to an LLM. The table below presents LLMs that operate as general research assistants and automate tasks such as collecting and evaluating data:</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px\"><strong>Model&nbsp;</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Cognitive abilities&nbsp;</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Known limitations&nbsp;</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">GPT-4 (OpenAI)</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">search, analysis, summarization, explanation at different levels (e.g., explain like I\u2019m 5), brainstorming on data, longer context window than predecessors which makes it more coherent&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">hallucination resulting in truthful-sounding falsities, repetition (both significantly less frequently than predecessors)</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Elicit (Ought)</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">search, literature review, finding papers without perfect keyword match, summarization, brainstorming research questions, recognition of interventions, outcomes, and trustworthiness of the source, recognition of lack of data</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">when in shortage of sources, it presents irrelevant information</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>SciBERT (Google)&nbsp;</p><p><br><br>&nbsp;</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>text processing, classification tasks, unmasking tasks, data mining</p><p><br>&nbsp;</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">vocabulary requires regular updates, limited context understanding, weak compared to GPT-4</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Galactica (Meta)</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">grammatically correct, convincing text&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">public demo was withdrawn due to &nbsp;dangerous hallucinations</td></tr></tbody></table></figure><p><br><br>&nbsp;</p><ul><li><a href=\"https://openai.com/product/gpt-4\"><strong><u>GPT-4</u></strong></a> serves as a helpful general research assistant that performs well across general research assistance tasks. When prompted appropriately, it outputs remarkably accurate results and outperforms all other competitors as of April 2023. In some instances, GPT-4 hallucinates and generates false results, but it seems to be able to \u201clearn from its mistakes\u201d within one prompt chain and update based on the user\u2019s corrections. GPT-4 is notably better than its predecessors in finding relevant references and citations, and in most cases outputs existing papers, articles, and books.&nbsp;</li><li><a href=\"https://ought.org/elicit\"><strong><u>Elicit</u></strong></a><strong>&nbsp;</strong>was designed to be a research assistant that can, at least as a first step, automate literature reviews. Elicit enables users to input specific research questions, problems, or more general topics, and search, analyze, and summarize relevant academic articles, books, and other sources. Elicit searches across multiple databases, repositories, and digital libraries and allows users to customize their search by providing a date range, publication type, and keywords.</li></ul><p>Elicit relies on the&nbsp;<a href=\"https://www.lesswrong.com/s/xezt7HYfpWR6nwp7Z\"><u>Factored Cognition</u></a> hypothesis and in practice, utilizes the applications of HCH and Ideal Debate. The hypothesis suggests that complex reasoning tasks can be decomposed into smaller parts and each part can be automated and evaluated by using a language model.</p><ul><li><a href=\"https://arxiv.org/pdf/1903.10676.pdf\"><strong><u>SciBERT</u></strong></a> is a pre-trained language model specifically designed for scientific text. It works by fine-tuning the BERT model on a large corpus of scientific publications, enabling it to better understand the context and semantics of scientific text. SciBERT leverages the transformer architecture and bidirectional training mechanism, which allows it to capture the relationships between words in a sentence. SciBERT is specifically designed to capture the context, semantics, and terminologies found in scientific publications.&nbsp;</li><li><a href=\"https://arxiv.org/abs/2211.09085\"><strong><u>Galactica</u></strong></a><strong>&nbsp;</strong>was Meta\u2019s public demo of an LLM for scientific text, which survived only three days online. The promise for this model was that it would \u201cstore, combine, and reason about scientific knowledge. Very quickly though it became clear that it was generating truthful-sounding falsehoods. The example of Galactica raised a series of concerns about what&nbsp;<a href=\"https://twitter.com/Michael_J_Black/status/1593133722316189696?s=20\"><u>Michael Black called \u201cthe era of scientific deep fakes\u201d</u></a>. While phenomena of scientific deep fakes generated by LLMs do not directly constitute causes of existential threat, they erode the quality of scientific research which harms science in general and AI safety in particular. Black\u2019s argument emphasizes that Galactica\u2019s text sounded authoritative and could easily convince the reader. This is not unique to Galactica, however. Early in 2023, it became clear that&nbsp;<a href=\"https://www.nature.com/articles/d41586-023-00056-7\"><u>abstracts written by ChatGPT can fool scientists</u></a>. Such phenomena raise further questions about the usage of AI systems in knowledge creation and the dangers of spreading misinformation.&nbsp;</li></ul><h2><strong>2.2 Hypothesis generation&nbsp;&nbsp;</strong></h2><p>Hypothesis-generating research explores data or observations searching for patterns or discrepancies, with the aim of developing new and untested hypotheses. Proposing relevant hypotheses therefore often involves both pattern recognition skills (e.g., noticing the correlation between data sets, particularly paying attention to&nbsp;<i>unexpected&nbsp;</i>correlations) and reasoning skills (proposing preliminary explanations and assessing them for plausibility).</p><p>A typical example to illustrate this could be the observation of a positive correlation between ice cream sales and drowning accidents. If we aim to generate relevant hypotheses and observe this pattern, we might first assess if the correlation is unexpected given our current understanding of the world. If we find that it is indeed unexpected, we would propose some hypothesis that explains it (e.g.&nbsp;<i>eating ice cream leads to decreased swimming capabilities, eating ice cream impairs judgment,&nbsp;</i>etc). As it would generally be possible to construct almost infinitely many (complicated) hypotheses that could theoretically explain an observed pattern, the assessment of plausibility in this step is crucial. Not every observed pattern will lead to a hypothesis that is relevant to test - in this case, the hypothesis that&nbsp;<i>warm weather causes both swimming (and therefore drowning accidents) and ice cream consumption (and therefore ice cream sales)</i> fits well with our established understanding of the world, so even though the pattern is real it is not unexpected enough to justify further inquiry.</p><p>In biology and chemistry research, the use of machine learning for scientific discovery is well established. We have looked at two important models in this area,&nbsp;<a href=\"https://arxiv.org/abs/1510.02855\"><u>AtomNet</u></a>, and&nbsp;<a href=\"https://www.nature.com/articles/s41586-021-03819-2\"><u>AlphaFold</u></a>, whose first versions were established in 2012 and 2016 respectively.&nbsp;</p><p>AtomNet is used for drug discovery and predicts the bioactivity of small molecules, but it can also propose small molecules that would have activity for specified targets in the body. AtomNet is a CNN that incorporates structural information about the target.</p><p>AlphaFold is used to predict the 3D shape of a protein and is used for a range of more fundamental research, but it also has applications in drug discovery. AlphaFold is neural-network-based and uses concepts and algorithms that were originally used for natural language processing.</p><p>Both AlphaFold and AtomNet are trained through supervised learning using manually produced data. This means that their development is limited by data availability, though AlphaFold is using something called self-distillation training to expand the data set with new self-produced data.&nbsp; Both models show some generalization capabilities: AlphaFold can predict structures with high accuracy even when there were no similar structures in the training set and AtomNet can predict molecules that have activity for new targets that had no previously known active molecules.</p><p>These models are used as narrow tools for pattern recognition: they propose new leads to investigate further within the hypothesis-generating stage of research, but neither goes into proposing new explanations or theories. Generative models can be combined with automated assessment of novelty, drug-likeness, and synthetic accessibility (e.g., in the commercial platform&nbsp;<a href=\"https://pubmed.ncbi.nlm.nih.gov/36728505/\"><u>Chemistry24</u></a>), which could be seen as a form of plausibility assessment; these models are far from automating the full process of hypothesis-generating research as they are limited to very specific tasks in a narrow domain.</p><p>From an AI safety perspective, these models, therefore, do not seem very risky in themselves (though there are certainly other&nbsp;<a href=\"https://www.nature.com/articles/s42256-022-00465-9\"><u>important societal risks to consider</u></a>). Something that might make them relevant for AI safety is the commercial pressure to&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0003267022011291\"><u>increase capabilities through for example interpretability work</u></a>, as findings from such work could potentially be transferable to more general, and therefore more risky, systems.</p><h2><strong>2.3 Hypotheses testing</strong></h2><p>Hypothesis testing research is often experimental and aims to test (and potentially falsify) a given hypothesis. The clear separation of hypothesis generation and hypothesis testing in empirical research is&nbsp;<a href=\"https://researcher-help.prolific.co/hc/en-gb/articles/360009500513-Am-I-doing-exploratory-or-confirmatory-research-Why-does-it-matter-#:~:text=In%20confirmatory%20(also%20called%20hypothesis,hypotheses%2C%20is%20supported%20by%20data.\"><u>important for distinguishing real casual relationships from patterns that arise by chance from noisy data</u></a>.</p><p>Designing experiments requires good planning capabilities, like visualizing different scenarios, choosing tools, and reasoning about what could go wrong. Critical thinking is important, as good hypothesis testing requires an ability to identify and challenge underlying assumptions and potential confounding factors, as well as the ability to calibrate confidence in different statements.</p><p>Of current models, LLMs are the ones that seem closest to performing hypothesis-testing research tasks. The paper \u201c<a href=\"https://arxiv.org/abs/2303.12712\"><u>Sparks of Artificial General Intelligence: Early Experiments with GPT-4</u></a>\u201d observes deficiencies in planning, critical thinking, and confidence calibration, but they used an early version of GPT-4. When we tested GPT-4 in March-April 2023 (see transcripts in appendix), the model was able to design experiments, identify assumptions and provide good arguments for and against different experimental setups.&nbsp;</p><p>We specifically tested GPT-4 on experiments for evaluating the effectiveness of public awareness campaigns about antibiotic resistance and formulating research to identify the most important drivers of antibiotic resistance. While the model initially proposed experimental designs that would be very impractical, it showed a great improvement when prompted to provide several alternative designs, criticize and choose between them. With the improved prompting there were no clear remaining flaws in the proposed experimental designs.</p><p>The topic of major drivers of antibiotic resistance was chosen since this is an area where there is a lot of contradictory information online. The relative contribution of different drivers is unknown and would be difficult to determine, but simplified communication makes claims about e.g. agricultural use being the most important driver. The model almost states (correctly) that it is unknown which driver of antibiotic resistance is most significant, though it does so in a slightly evasive manner (\u201c<i>It is difficult to pinpoint one specific driver\u2026\u201d</i>), and it does suggest several reasonable and complementary approaches (in broad strokes) for research aiming to identify the most significant driver of antibiotic resistance. When asked to specify a certain step of one of the proposed approaches, it proposes a scientifically reasonable though very expensive and potentially politically difficult experiment that would test the impact of different regulations.</p><p>Since GPT-4 is sensitive to variations in prompting it seems relevant to consider if there is some fundamental capability that is necessary for hypothesis testing that is supplied through human prompting. If that is the case, that might limit how well the entire hypothesis-testing process could be automated. For example, the prompts that generate valuable answers might depend on a human critically reviewing the previous answers and identifying flaws or omissions and pointing the model in a relevant direction.&nbsp;</p><p><a href=\"https://arxiv.org/abs/2303.11366\"><u>Shinn et. al.</u></a> have tested a systematic approach to prompt LLMs for self-reflection upon failure on a task and use the reflection output to improve performance on the next attempt at the same task. Their method however depends on access to a binary reward function to distinguish between successful and failed attempts, which limits the potential use for more autonomous research work.</p><p>We tested GPT-4 in two different ways to investigate how dependent it is on human feedback to design scientific experiments. First, we simply asked the model to specify which aspects should be taken into account when prompting it to design scientific experiments, as this response could then in theory be used as a basis for generating follow-up prompts to check a proposed design. Second, we provided a series of simple prompts that were preformulated to guide the model to design a scientific experiment without looking at the intermediate responses. The model did well on both of these tests - particularly the output on the preformulated prompt sequence was interesting, as the model selected a hypothesis to explain the placebo effect and proposed an experiment to test it (see appendix, preformulated series of prompts).</p><p><a href=\"https://arxiv.org/abs/2304.05332\"><u>Boiko et. al.</u></a> have done more extensive testing of the capabilities of an LLM-based agent to design and perform scientific experiments, using tasks such as synthesis of ibuprofen. They also challenge the model with tasks such as designing a new cancer drug, which involves both elements of hypothesis generation and hypothesis testing, and conclude that the model approaches the analysis in a logical and methodical manner.</p><p>We have not been able to identify any general aspect of empirical research where LLMs predictably fail. However, as we will see in the following sections LLMs have weaknesses that are more easily observed in formal science such as mathematics. It seems plausible that these weaknesses also could have major consequences for empirical research capabilities.</p><h1>3. Formal science</h1><p>Formal sciences build knowledge not from observations, but from the application of rules in a formal system.&nbsp;</p><p>Mathematical reasoning is one of the most abstract tasks humans exhibit and as such provides a valuable litmus test for the capability to abstract and perform higher-order reasoning. We will first sketch the general architecture of the models studied and then proceed to discuss three possible limitations of the models that seem also to generalize for other complex tasks.</p><p>It is worth noting that the practical significance of ML models for the mathematical community so far has been minor. In the field of computer science, automated coding and discovery of new algorithms (see e.g. AlphaTensor) appear to be more impressive. In this project, we tested mathematical models and will lay out our findings in the following section.&nbsp;</p><p>The mathematical models we investigated were of two kinds:</p><ol><li>LLMs trained to output text in a formal language I will refer to these as \u2018formal (language) models\u2019</li><li>LLMs trained to respond in natural language (I will refer to these as \u2018natural language models\u2019)&nbsp;</li></ol><p>For the natural language models we initially examined&nbsp;<a href=\"https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html\"><u>[Minerva]</u></a>. Extrapolating from our observations GPT-4 seems to outperform it, we will only focus on GPT-4 (see also section 4 of the&nbsp;<a href=\"https://arxiv.org/abs/2303.12712\"><u>Sparks of intelligence paper</u></a>).&nbsp;</p><h2>3.1 Architecture of formal models</h2><p>Typically, language models for mathematics are, after being trained on common general data, pre-trained or fine-tuned on data specifically related to mathematics. Here are examples of such data sets used in&nbsp;<a href=\"https://arxiv.org/abs/2202.01344\"><u>[Curriculum learning]</u></a> and&nbsp;<a href=\"https://arxiv.org/abs/2009.03393\"><u>[GPT-f]</u></a> respectively:</p><p><br><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/jnxwqoiahvnvsc5uzukf\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/mmgbzcoluwol3mr7udzk 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/b6l2u4geied0fdosh7nl 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/cfpyzelk0np9wyuvk7hv 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/e4ej48pzxpx3z4fdhrn6 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/crcrgpl2ray0zzchbxal 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/yp6jzgd2eohs17xjqkza 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/e66zlpva0eomtsvogg3i 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/ipfkkgym3nit51do9pmp 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/zu3a961hon7g9e0k0lqr 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/gw61ioto8wpgqodg7m4t 1054w\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/wg4vpqxdpuzv6qrd3yui\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/iqmacrbk3pxosjkmfv4d 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/bcfwp7vdptrvhw3lmnlm 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/cd92icmjwdwhzle2fu8x 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/azch2q4csxur6ejec3su 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/z6ckmysjjtwuvf8cgztd 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/xqmjzfaaoiia2ie5qkrm 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/wzorby1kl7wgshvhhfpb 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/ooggaeyrx7m98shfhspz 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/bsbuz6ajudaqe4ymsohx 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/flcak6jjw6gmua6onby7 1006w\"></p><p>The models we examined use a&nbsp; decoder-only transformer architecture, with the exception of&nbsp;<a href=\"https://arxiv.org/abs/2205.11491\">&nbsp;<u>[Hypertree searching]</u></a> which uses an encoder-decoder architecture. For these formal models the basic idea is to perform proof search as tree search: To generate a proof, suggest multiple options of what the most likely next step is and iterate this. Then pursue options that are more likely to be successful. \u201cNext step\u201d here could be either something like an algebraic manipulation of an equation or stating a subgoal. Different models can vary quite a bit in how they exactly apply this.</p><p>Current math models are being evaluated on different kinds of data sets. One example is the&nbsp;<a href=\"https://github.com/openai/miniF2F\"><u>minif2f</u></a> data set, first introduced&nbsp;<a href=\"https://paperswithcode.com/paper/minif2f-a-cross-system-benchmark-for-formal\"><u>here</u></a>. It contains natural language exercises in the style of IMO competition problems. For example: \u201cFor any real number x, show that 10x \u2264 28x\u00b2 + 1.\u201d Other data sets are more diverse and contain proofs from different areas, for example,&nbsp;<a href=\"https://en.wikipedia.org/wiki/Peirce%27s_law\"><u>Peirce\u2019s law</u></a>: For two propositions P,Q, we have ((P-&gt; Q)-&gt;P)-&gt;P.</p><p>A formal model takes as input an exercise in a certain formal language and solves it in such a language. The task is evaluated as solved successfully if the formal proof assistant confirms its validity and otherwise is evaluated as not solved, even if partial steps would be correct. To give a superficial idea of the performance of formal models, see the diagram below:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/bakoqrmbpixylruuod72\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/w0o8ji7spxrxhsolyk7u 130w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/itovtayeibspadvyh6k4 260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/wi7asaqatkhstpjngzbg 390w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/jnwzrohfgvtcti8zmmuc 520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/uti1nmwlddqlboo9qtvr 650w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/wyuxk6fl94uwpcg02h8j 780w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/rho6vsrvddpbkg6n1djx 910w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/ljmgqqkjpnwsevwzmq7o 1040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/kc7qlvox4cywc8dwtdb4 1170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/hdglmvncdwsrgr6zd4lg 1206w\"></p><p>Note that different formal models use different formal proof assistants like&nbsp;<a href=\"https://en.wikipedia.org/wiki/Lean_(proof_assistant)\"><u>Lean</u></a>,&nbsp;<a href=\"https://en.wikipedia.org/wiki/Isabelle_(proof_assistant)\"><u>Isabelle</u></a>, or&nbsp;<a href=\"https://en.wikipedia.org/wiki/Metamath\"><u>Metamath</u></a>. One should be careful with comparing two models directly: Not only do the formal proof assistants come with different up- and downsides, but also the evaluation datasets vary.</p><p>To give an idea of what a formal proof verifier would look like, see the following example of a lemma and proof written by a human in Lean which derives commutativity of addition for integers from commutativity of addition for natural numbers based on case distinctions by sign.:</p><p>lemma add_comm : \u2200 a b : \u2124, a + b = b + a</p><p>| (of_nat n) (of_nat m) := by simp [nat.add_comm]</p><p>| (of_nat n) -[1+ m]&nbsp; &nbsp; := rfl</p><p>| -[1+ n]&nbsp; &nbsp; (of_nat m) := rfl</p><p>| -[1+ n]&nbsp; &nbsp; -[1+m] &nbsp; &nbsp; := by simp [nat.add_comm]</p><p>Note that this is also hardly comprehensible for a human without prior knowledge of Lean (just like reading code without knowing the language).</p><p>We make three hypotheses about the limitations of these models</p><ol><li>Formalization is difficult.</li><li>Generating a conversation is qualitatively different from generating a proof.</li><li>Non-verbal abstractions can obstruct natural language reasoning</li></ol><p>For what follows keep the following picture in mind:</p><p><br><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/eubfaq6ntd9u16p2e591\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/rfbbboc51kfzdgo85vzn 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/gtubjl7jiasldn1j3z18 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/yxpqzewzbrhn1a04hhb3 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/pehdqy7suyrniqn6dzf1 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/uno05v3ko2e6vbbl96km 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/afroz9718lsli9y83zc8 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/mi7tmex1ixbwua65bwzg 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/cfnm9d4kdqjwiefidq8i 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/n57qx6yrpgdpowduzjnr 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fTvw6K3CfxXdxAE5G/atd5zlnklzy4ywvi2gok 978w\"><br>When thinking about mathematics we don\u2019t only use natural language. We also use non-verbal abstractions and formal language. When I think about a circle, I might think about the definition {(x,y)| x^2+y^2=1}, or about \u201celements of norm one\u201d (which is just a more natural rephrasing of the definition) or I might visualize a circle (which is what the definition tried to encapsulate). We can go back and forth between these: For example, I can take the definition of the circle and draw some of the points of this set and realize that indeed it is a circle.</p><h2>3.2 Formalization is difficult</h2><p>Almost all mathematical information is communicated via natural language. If you pick up an analysis textbook, you will read something like \u201c... We check that the f(x) constructed above is continuous. Assumption A leads to a contradiction and therefore we can assume not A, but an application of Theorem 3.10 yields the result.\u201d. While this is understandable to humans, it is not a \u201creal\u201d proof, but more like a reference to a proof. You need to fill gaps like \u201cUse law of excluded middle to assume either A or -A\u201d or specify what and how to apply Theorem 3.10. Doing this is very laborious and humans are not dependent on it. That is, our brain can elicit more information out of the above sentence and we implicitly (assume that we can) unravel this additional information in our brains.</p><p>Despite the fact that we&nbsp;<strong>assume&nbsp;</strong>we can do it, in reality, this is a very messy process. Recent attempts to formalize mathematics in Lean, while being successful, also proved to be cumbersome. So we implicitly assume that a human proof can be converted to a formally correct proof, but the truth is that we don\u2019t really do things in a formal way. As a result, there is not that much formal language mathematics that is produced . Notable exceptions are Isabelle and more recently a big increase in the last few years due to the Lean community).</p><p>That being said, pre-training on general data, and then on more specific subjects still leads to useful results. See&nbsp;<a href=\"https://arxiv.org/pdf/2102.01293.pdf\"><u>here</u></a> for more details on models trained to code in Python (instead of solving mathematical tasks).</p><p>These pre-trained models are then fine-tuned specifically to the set-up that they are trying to solve.</p><h2>3.3 Autoregressive&nbsp;<s>text</s> proof generation&nbsp;</h2><p>The known limitations of autoregressive text modeling also apply to the task of generating a proof, whether in natural or formal language. It is more difficult, maybe even qualitatively different, than generating a conversation in natural language and LLMs are more prone to struggle with it.&nbsp;</p><p>More precisely, we suspect that next-token prediction is a possible obstruction in creating proofs. At least it does not represent the way humans attempt proofs and they would also fail to perform well using next-token prediction. After experimenting with GPT-4, I found a few examples of the following phenomenon: GPT-4 (and presumably current LLMs) can commit too early to a specific line of reasoning and as a result make mistakes by either getting off-track or hallucinating something. While simple conversations can be much more forgiving and provide a high level of flexibility, that is not feasible in the set-up of a proof.&nbsp;</p><p>Especially in proofs, humans often explore multiple paths and completely abandon them, if determined as unsuccessful. Consider the following puzzle which presents the crux:</p><p><strong>I have a riddle. Solve it and explain the solution to me:</strong></p><p><strong>?TTFFSSENT, with what letter do I have to replace the \"?\"</strong></p><p>Here are some example answers (Temperature is 0, 0.15, 0.25 respectively)</p><ol><li>The letter you have to replace the \"?\" with is \"M\".</li></ol><p>The solution to the riddle is that each letter represents the first letter of the numbers from one to ten in order: One, Two, Three, Four, Five, Six, Seven, Eight, Nine, Ten. So, the \"?\" should be replaced with \"M\" for \"One\" (in some languages, like German, \"one\" is spelled as \"eins,\" which starts with an \"M\" when spelled backwards).</p><p>&nbsp; &nbsp; &nbsp; 2. The letter you have to replace the \"?\" with is \"M\".</p><p>The solution to the riddle is that each letter represents the first letter of the days of the week, starting from Monday. So, the sequence is as follows:</p><p>M - Monday</p><p>T - Tuesday</p><p>T - Wednesday</p><p>F - Thursday</p><p>F - Friday</p><p>S - Saturday</p><p>S - Sunday</p><p>E - Monday (Eight day)</p><p>N - Tuesday (Ninth day)</p><p>T - Wednesday (Tenth day)</p><p>The pattern repeats every 7 days, and the \"?\" represents the first day, which is Monday.</p><p>&nbsp; &nbsp; &nbsp; &nbsp;3. The letter you have to replace \"?\" with is \"W\".</p><p>The solution to this riddle is that each letter represents the first letter of the English numbers in increasing order: One, Two, Three, Four, Five, Six, Seven, Eight, Nine, Ten. So, the sequence should start with \"W\" for \"One\".</p><p>In each answer the suggested letter is wrong. In two cases the explanation is correct and in one it hallucinates a justification for its wrong answer.</p><p>As it is immediately answering with a suggested solution for the \u201c?\u201d, without having explained it, it seems that it tries to make an educated guess and then tries to adjust with the explanation. Of course, common fixes apply:</p><p><strong>I have a riddle. Solve it and explain the solution to me:</strong></p><p><strong>?TTFFSSENT, with what letter do I have to replace the \"?\"</strong></p><p><strong>Give the letter only at the end after you recognized the pattern. Under no circumstances start your answer with the final solution</strong></p><p>Which yields:</p><p>The pattern in the sequence is the first letters of the numbers one through nine in English: One, Two, Three, Four, Five, Six, Seven, Eight, Nine. Therefore, the missing letter is \"O\" for \"One.\"</p><p>But this is more of an ad hoc solution. If a multi-step process is required, an LLM might be able to break it down through prompt chaining. But it still runs the danger of correcting itself once it skips a certain step or initiates with a wrong approach. This obstruction could be anticipated in any task requiring higher-order thinking.</p><p>Finally, there is the recurring problem that writing something that is true is not the same as writing something that sounds true. As LLMs don\u2019t seem to have an intrinsic concept of \u201ctruth\u201d, especially without any formal set-up, this makes it much more difficult to build a reliable chain of true implications. As seen in&nbsp;<a href=\"https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html\"><u>[Minerva]</u></a> and&nbsp;<a href=\"https://arxiv.org/pdf/2303.12712.pdf\"><u>[Sparks]</u></a>, something as simple as elementary arithmetic (addition, multiplication,...) can be surprisingly difficult.</p><p>Again, for an ad hoc problem, you can solve such an obstruction. For example, one could<a href=\"https://arxiv.org/pdf/2302.04761.pdf\"><u> outsource the calculations to an external calculator.</u></a>&nbsp;</p><p>We can\u2019t tell whether the more conceptual problem can be simply overcome by prompt chaining and/or more scaling. Right now we don\u2019t see how the fundamental issue would be addressed, on the other hand, models might get good enough at solving problems that we wouldn\u2019t need to address the aforementioned problems.</p><h2>3.4 Intuition through or beyond natural language?</h2><p>In our way of doing math, we encode something like \u201cintuition\u201d. Compare for example the formal definition of a sphere with our intuitive understanding of what a sphere is:</p><p>The formal definition of {v| ||v||=1} is not that straightforward, there is a much more concise idea of what it ought to represent (namely something round). When thinking about these objects, we end up making use of this intuition. Before formulating a proof (idea) to begin with, we might first visualize the problem. I am unsure whether there are other possible abstractions that are not visual or verbal, therefore let\u2019s stick with visual intuitions.</p><p>At this point, LLMs still seem too weak to extract a meaningful answer to the question of whether they (would) possess such a visual intuition. That is, to test the hypothesis, we would need the models to be more capable than they currently are. But we can ask the question of whether abstraction that takes place in a non-verbal part of human thought can be accessed and learned simply&nbsp;<i>through</i> natural language. As of right now, we are not aware of a good answer to this.</p><p>But one thing we can observe is that there is at least something that is reminiscent of intuition (or even creativity, whatever that exactly means). As described in&nbsp;<a href=\"https://arxiv.org/abs/2102.06203\"><u>[PACT]</u></a>: \u201cMore than one-third of the proofs found by our models are shorter and produce smaller proof terms than the ground truth[= human generated formal proof]\u2026\u201d. In fact, some proofs were original and used a different&nbsp;<a href=\"https://en.wikipedia.org/wiki/Ansatz\"><u>Ansatz</u></a> than the human-generated one. At this level, the proofs are not sophisticated enough to argue that something \u201ccreative\u201d is happening. But it offers a first hint that these models might not only&nbsp;<i>reproduce</i> knowledge but&nbsp;<i>create</i> it.</p><p>Further indications that more generally NNs can actually learn things is for example provided by&nbsp;<a href=\"https://www.lesswrong.com/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking\"><u>grokking</u></a>.</p><h1>4. Ontology-builders</h1><p>In the modern context, the term \u201cscience\u201d refers to both a method of obtaining knowledge as well as the body of knowledge about the universe that this method has produced.</p><p>While the previous sections have focused largely on the automation of scientific inquiry, we propose that the explicit and iterative codification of new knowledge is also a topic of importance for neural network models performing autonomous research work.&nbsp;</p><p>In the fields of computer science and artificial intelligence, the concept of science as a body of knowledge maps very closely to the technical term&nbsp;<i>ontology</i>*, which refers to a formal description of the knowledge within a domain, including its constituent elements and the relationships between them. From a computer science perspective, the scientific method can be thought of as producing an ontology (scientific knowledge) about the universe.</p><p>* Within this section, the term&nbsp;<i>ontology</i> is used specifically in the computer science sense of the word.</p><h2>4.1 Explicit versus implicit knowledge representation</h2><p>Framing the body of scientific knowledge as an ontology highlights a challenge within AI research and its efforts to automate scientific inquiry: The facts contained within an ontology are formal, discrete, and explicit, which facilitates the process of repeated experimental verification that is central to science. Neural networks, on the other hand, encode their knowledge implicitly, which presents a range of challenges for working with the facts contained therein.</p><p>As we have seen, despite the incredible amount of knowledge contained within advanced LLMs, they still routinely hallucinate, regularly presenting incorrect and even bizarre or contradictory statements as fact. Indeed, even GPT-4 is quite capable of producing one fact in response to a prompt, and producing that same fact\u2019s negation in response to a slightly different prompt.</p><p>These failures of factuality are often said to flow from LLMs lacking a concept of \"truth\", or, by some measures, even from their lack of concrete \"knowledge\". But, these failures can likewise be framed as a function of their knowledge being represented implicitly and probabilistically rather than explicitly and formally.</p><h2>4.2 Ontologies as&nbsp;bridge</h2><p>Distinctions between implicit and explicit representations of knowledge are of particular importance because current advanced LLMs exhibit dramatic improvements in their capabilities when explicit knowledge is included in their input prompting: they are far less likely to hallucinate about facts that are provided in an input prompt than they would be when they are required to infer knowledge about the same subject from preexisting weights within their own neural network.</p><p>This points to external knowledge representations (i.e. outside the neural network) as a critical piece of the landscape when evaluating the current and imminent capabilities of existing models. For many use-cases of an LLM-based system, it is far more efficient and reliable to describe newly generated knowledge and later reingest that output than to retrain or fine-tune the network on that new knowledge.</p><p>Accordingly, the production of new knowledge, external encoding of the discrete facts thereof, and subsequent use of said knowledge, represents a critical stepping stone for a wide range of LLM capabilities. This holds true especially for those capabilities that require utilizing previously gained knowledge with extremely high fidelity, as is the case in scientific inquiry.</p><h2>4.3 Experiments</h2><h3>4.3.1 Background</h3><p>To investigate if using and manipulating external knowledge representations impacted GPT-4's performance in scientific inquiry tasks, we examined its ability to generate knowledge systematically and then use that knowledge in the multi-step process of scientific inquiry. We sought to quantify the model\u2019s performance on various tasks while:</p><ol><li>experimentally manipulating whether or not the model would produce external knowledge representations that would be fed back into it via subsequent prompts (i.e., whether it created an&nbsp;<i>ontology</i>);</li><li>experimentally manipulating the degree of formal structure in the knowledge representations that the model produced (i.e. the degree of&nbsp;<i>ontological formality</i>); and</li><li>in the final intervention, providing the model with the most appropriately structured knowledge representation format that it had produced in earlier runs, and explicitly directing it to use that format to track its observations (i.e.&nbsp;<i>providing an optimal ontology</i>)</li></ol><h3>4.3.2 Design</h3><p>In this post, we describe experiments that measured the above interventions on one particular task: a language-based adaptation of the \u201cBlicket detection\u201d task described in the&nbsp;<a href=\"https://arxiv.org/pdf/2206.09203.pdf\"><u>EST: Evaluating Scientific Thinking in Artificial Agents</u></a> paper.</p><p>The original Blicket detection experiment, predating the EST paper, aimed to evaluate children\u2019s causal induction capabilities. In that experiment, a child is presented with a device called a Blicket machine, and shown that it \u201cactivates\u201d by lighting up and playing music when certain objects are placed on top of it. Objects that activate the machine are called \u201cBlickets\u201d. The machine could accommodate multiple objects at a time, and would activate if any of the objects atop it were Blickets, regardless of whether any non-Blickets were also present. After a period of exploratory play with the machine and the objects, the children would be asked to identify which objects were Blickets. In these experiments, children were observed iteratively generating causal hypotheses and refining them until they were consistent with all of the observations that were made during their demonstration and play periods.</p><p>In the EST paper, several digital analogues of the Blicket experiment were created in order to evaluate the \u201cscientific thinking\u201d performance of a variety of machine learning models. These models were allowed to test up to ten sets of objects on the Blicket machine as part of their inquiry process and were primarily evaluated on whether they were able to correctly identify whether all objects were or were not Blickets.</p><p>We adapted this experiment further by creating a software environment that interfaces with GPT-4 in natural language to:</p><ul><li>generate a random test environment configuration</li><li>elicit a simulation of a scientific agent from the LLM</li><li>describe the test environment</li><li>perform several demonstrations of various objects interacting with the Blicket machine</li><li>(where appropriate) iteratively prompt the agent to explore and hypothesize about which objects are Blickets, and</li><li>submit its conclusions about which objects are Blickets for evaluation</li></ul><p>We tested seven experimental scenarios beginning with baseline setups that involved no external knowledge representation, progressing through those that produced&nbsp;<i>informal</i> knowledge representations, and culminating in setups that produced and worked with high degrees of structure in their knowledge representation. They are listed below, from least to most structured knowledge representation format:</p><ol><li>&nbsp;Zero-shot prompts which did not include background knowledge about a query</li><li>Single prompts which include simple statements of facts</li><li>Single prompts which include simple statements of facts with instructions to rely upon those facts</li><li>Structured roleplay in chat-style turn-taking exchanges where the model is given a task-oriented investigator role and prompted to generate knowledge by inference. The transcript format of the exchange inherently makes that knowledge accessible in future prompts, thereby producing a structured but informal proto-ontology within the transcript that aids the model in further investigation</li><li>Iterative prompting where the model is instructed to generate and iterate upon&nbsp;<i>informal</i> descriptions of knowledge it has produced so far</li><li>Iterative prompting where the model is instructed to generate and iterate upon&nbsp;<i>formal</i> descriptions of knowledge it has produced so far</li><li>Iterative prompting where the model is provided with the formal structure of the top-performing knowledge representation that it had generated in earlier runs and instructed to iterate upon formal descriptions of knowledge it has produced so far</li></ol><h3>4.3.3 Methods</h3><p>Our study consisted of seven experimental setups, each designed to test the model's performance under different conditions of knowledge representation and structure.</p><p>For each experimental configuration, we ran 25 trial episodes (175 in total), with the primary outcome being the number of perfectly completed trials per configuration. A perfect trial completion was defined as the model correctly guessing whether all objects were or were not Blickets.</p><p>Our primary research question was whether the use of external knowledge representations affected the number of perfect trial completions per configuration. Our secondary research question asked whether the degree of structure in those knowledge representations correlated with the number of perfect trial completions per setup.</p><p>In keeping with the format of the EST paper\u2019s experiments, the structure of each trial was as follows:</p><ol><li>Each object in the experiment had three properties: color, material, and shape</li><li>There were three (3) shapes, two (2) materials, and eight (8) colors available, yielding a pool of 48 possible objects</li><li>Each trial randomly sampled nine (9) objects from the available pool for inclusion in the environment</li><li>Between three and eight (3-8) of the nine (9) objects in the trial were randomly selected to be Blickets</li><li>Four demonstrations were constructed, each of which involved between two and six (2-6) objects at random being placed on the Blicket machine</li></ol><p>Following the demonstrations, our LLM-based adaptation of the experiment continued as follows:</p><ol><li>the agent was asked whether it had enough information to conclusively determine which objects were Blickets. If it answered in the affirmative, its guesses were collected and scored. Otherwise, the trial continued:</li><li>The agent was asked what set of objects it wanted place on the Blicket machine for testing</li><li>The resulting activation status of the Blicket machine was relayed to the agent</li><li>For scenarios 5-7, the agent was asked if it wanted to update its knowledge list with any new knowledge gained</li><li>If the agent has exhausted its 10 tests, it is forced to submit guesses. Otherwise, repeat the process again, starting from step 6.</li></ol><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Scenario</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Prompt structure</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Single prompt:<br>Zero-shot, no background<br>(effectively a random baseline)</li></ol></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>Description of simulated agent, scene, task, and goal</li><li>Forced (blind) guess</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Single prompt:<br>Demonstrations followed by forced guess</li></ol></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>Description of simulated agent, scene, task, and goal</li><li>Demonstration of four sets of objects on machine</li><li>Forced guess</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Single prompt:<br>Demonstrations with instructions to rely on that data, followed by forced guess</li></ol></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>Description of simulated agent, scene, task, and goal</li><li>Demonstration of four sets of objects on machine</li><li>Instructions to rely on demonstrations when making its guesses</li><li>Forced guess</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Iterative chat:<br>Structured roleplay and investigation;<br>transcript provides&nbsp;<strong><u>implicit</u></strong> knowledge representation</li></ol></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>Description of simulated agent, scene, task, and goal</li><li>Demonstration of four sets of objects on machine</li><li>Allow agent to test up to 10 sets before submitting guesses</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Iterative chat:<br>Structured roleplay and investigation;<br>Model instructed to create and iterate upon&nbsp;<strong><u>informal</u></strong> descriptions of knowledge it has produced so far</li></ol></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>Description of simulated agent, scene, task, and goal</li><li>Demonstration of four sets of objects on machine</li><li>Instructions to keep a list of relevant observations, conclusions, and inferences that may be helpful</li><li>Allow agent to test up to 10 sets before submitting guesses</li><li>Iterative prompt to add knowledge to list after each test</li><li>Current version of list is included in subsequent prompts</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Iterative chat:<br>Structured roleplay and investigation;<br>Model instructed to create and iterate upon&nbsp;<strong><u>formal</u></strong> descriptions of knowledge it has produced so far</li></ol></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>Description of simulated agent, scene, task, and goal</li><li>Demonstration of four sets of objects on machine</li><li>Model prompted to provide the most ideal format for representing the knowledge it gains</li><li>Instructions to keep a list of relevant observations, conclusions, and inferences that may be helpful</li><li>Allow agent to test up to 10 sets before submitting guesses</li><li>Iterative prompt to add knowledge to list after each test</li><li>Current version of list is included in subsequent prompts</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Iterative chat:<br>Structured roleplay and investigation;<br>Model instructed&nbsp;<strong><u>use provided, optimal</u></strong> knowledge representation format and iterate upon knowledge it has produced so far</li></ol></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>Description of simulated agent, scene, task, and goal</li><li>Demonstration of four sets of objects on machine</li><li>Ideal format knowledge representation format provided by simulation engine (based on top-performing format in scenario 6)</li><li>Instructions to keep a list of relevant observations, conclusions, and inferences that may be helpful</li><li>Allow agent to test up to 10 sets before submitting guesses</li><li>Iterative prompt to add knowledge to list after each test</li><li>Current version of list is included in subsequent prompts</li></ul></td></tr></tbody></table></figure><p><br>&nbsp;</p><p>In all cases, the word \u201cBlicket\u201d was masked with a nonce word so as to reduce the risk that the LLM\u2019s performance might be affected by prior knowledge of any Blicket-solving strategies.</p><h3>4.3.4 Results</h3><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Scenario</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Perfect trials</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Failed trials</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Success rate</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Single prompt:<br>Zero-shot, no background<br>(effectively a random baseline)</li></ol></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">25</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0%</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Single prompt:<br>Demonstrations followed by forced guess</li></ol></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">25</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0%</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Single prompt:<br>Demonstrations with instructions to rely on that data, followed by forced guess</li></ol></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">25</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0%</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Iterative chat:<br>Structured roleplay and investigation;<br>transcript provides&nbsp;<strong><u>implicit</u></strong> knowledge representation</li></ol></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">25</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0%</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Iterative chat:<br>Structured roleplay and investigation;<br>Model instructed to create and iterate upon&nbsp;<strong><u>informal</u></strong> descriptions of knowledge it has produced so far</li></ol></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">24</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">4%</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Iterative chat:<br>Structured roleplay and investigation;<br>Model instructed to create and iterate upon&nbsp;<strong><u>formal</u></strong> descriptions of knowledge it has produced so far</li></ol></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">3</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">22</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">12%</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Iterative chat:<br>Structured roleplay and investigation;<br>Model instructed&nbsp;<strong><u>use provided, optimal</u></strong> knowledge representation format and iterate upon knowledge it has produced so far</li></ol></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">11</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">14</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">44%</td></tr></tbody></table></figure><p><br><br><br><br>&nbsp;</p><p>Our experiments using GPT-4 in scenarios 1-7 have demonstrated monotonically increasing model performance on knowledge recall and inference tasks in response to the degree of structure present in external knowledge representations. Our investigations into the LLM-based generation and use of formal ontologies remain underway, with initial results suggesting a non-trivial range of tasks across which the use of formal ontologies may improve model performance on multi-prompt scientific reasoning. However, at present, we do not have data across a wide enough range of tasks to draw broad conclusions.</p><p>To date, the top-line determinants of transformer-based LLM performance have been parameter size, data size, data quality, and reinforcement training. In light of our investigations, we propose that ontology-building strategies may represent a dimension of LLM performance on multi-step scientific inquiry tasks that is largely orthogonal to these traditional determinants, and which warrant further investigation.</p><h1>5. Conclusions</h1><p>We aimed to create an overview of the available AI science models and their capabilities. As we have seen, current models are very capable across many different scientific tasks, including assessment of existing knowledge, hypothesis generation, and experimental design.</p><p>A serious limitation of science capabilities of current models is the relationship to the concept of truth. This is most visible in the experiments we have done in the domain of formal science, but we have no reason to believe this would be less problematic in the empirical sciences. While prompt engineering such as asking the model to criticize its previous answers or to develop several alternatives and select the best one decreases the occurrence of obvious mistakes, it is unclear if this is a path to reliably truth-seeking behavior.</p><p>An important question going forward is therefore if the models we use can represent the concept of truth, and if yes, how we could train them to become truth-seeking.&nbsp;If there is a risk that we accidentally train our models to be convincing rather than truth-seeking this is an important limitation for how useful the models would be, including the use case of furthering alignment research.&nbsp;</p><p><a href=\"https://arxiv.org/pdf/2304.13734.pdf\"><u>A preprint under review by Azaria and Mitchell</u></a> indicates that there may be some internal representation of truthfulness in LLMs.</p><h1><a href=\"https://docs.google.com/document/d/127h3z4l8jqqnUzcaZlNJmYCXiryvRiNv476AWbsM3xQ/edit#heading=h.nz2zsf3jed9f\"><u>Appendix: LLM experiment transcripts</u></a></h1><p><br><br><br><br>&nbsp;</p>", "user": {"username": "eangelou"}}, {"_id": "CuktFCQ39fuXnxcHx", "title": "Why GiveWell funded the rollout of the malaria vaccine", "postedAt": "2023-05-12T19:11:44.904Z", "htmlBody": "<p><em>Author: Audrey Cooper, GiveWell Philanthropy Advisor</em></p>\n<p>Since our founding in 2007, GiveWell has directed <a href=\"https://airtable.com/shrsxhCBHeR2vkQKE/tblG72bMUu36lrWsr\">over $600 million</a> to programs that aim to prevent malaria, a mosquito-borne disease that causes severe illness and death. Malaria is preventable and curable, yet it killed over 600,000 people in 2021\u2014mostly young children in Africa.<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-1\" id=\"fnref-osA4k7SK74nsJGypF-1\">[1]</a></sup></p>\n<p>Following the World Health Organization\u2019s approval of the RTS,S/AS01 malaria vaccine (RTS,S) in late 2021,<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-2\" id=\"fnref-osA4k7SK74nsJGypF-2\">[2]</a></sup> GiveWell directed <a href=\"https://www.givewell.org/research/grants/PATH-malaria-vaccines-January-2022\">$5 million</a> to <a href=\"https://www.path.org/\">PATH</a> to accelerate the rollout of the vaccine in certain areas of Ghana, Kenya, and Malawi. This grant aimed to enable these communities to gain access to the vaccine about a year earlier than they otherwise would, protecting hundreds of thousands of children from malaria.<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-3\" id=\"fnref-osA4k7SK74nsJGypF-3\">[3]</a></sup></p>\n<p>Although we\u2019re very excited about the potential of the RTS,S malaria vaccine to save lives, it isn\u2019t a panacea. We still plan to support a range of malaria control interventions, including vaccines, nets, and antimalarial medicine.</p>\n<p>In this post, we will:</p>\n<ul>\n<li>Explain how we found the opportunity to fund the malaria vaccine</li>\n<li>Discuss why we funded this grant</li>\n<li>Share our plan for malaria funding moving forward</li>\n</ul>\n<p><strong>Identifying a gap in vaccine access</strong></p>\n<p>In October 2021, we shared <a href=\"https://blog.givewell.org/2021/10/08/initial-thoughts-on-malaria-vaccine-approval/\">our initial thoughts</a> on the approval of the RTS,S malaria vaccine by the World Health Organization (WHO). At that point, we weren\u2019t sure whether the vaccine would be cost-effective and were not aware of any opportunities for private donors to support the expansion of vaccine access.</p>\n<p>In the following months, our conversations with PATH, a large global health nonprofit that we\u2019ve previously funded, revealed that there might be an opportunity to help deploy the vaccine more quickly in certain regions. PATH had been supporting the delivery of the vaccine in Ghana, Kenya, and Malawi as part of the WHO-led pilot\u2014the <a href=\"https://www.who.int/news-room/questions-and-answers/item/malaria-vaccine-implementation-programme\">Malaria Vaccine Implementation Program (MVIP)</a>\u2014since the pilot began in 2019.<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-4\" id=\"fnref-osA4k7SK74nsJGypF-4\">[4]</a></sup> In order to generate evidence about the effectiveness of the vaccine, randomly selected areas in each country received the vaccine during the early years of the pilot, while \u201ccomparison areas\u201d would receive the vaccine at a later date, if the vaccine was recommended by the WHO.<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-5\" id=\"fnref-osA4k7SK74nsJGypF-5\">[5]</a></sup></p>\n<p>Once the vaccine had received approval from the WHO, the WHO and PATH believed there was an opportunity to build on the momentum and groundwork of the pilot to roll out the vaccine to the comparison areas as soon as possible. However, the expectation at the time was that expanding use to the comparison areas would need to wait for the standard process through which low-income countries apply for support to access vaccines from Gavi, the Vaccine Alliance.<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-6\" id=\"fnref-osA4k7SK74nsJGypF-6\">[6]</a></sup> This process would have made it possible to introduce the vaccine at the end of 2023 at the earliest.<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-7\" id=\"fnref-osA4k7SK74nsJGypF-7\">[7]</a></sup></p>\n<p>However, there was another path through which these vaccines could be provided more quickly. GlaxoSmithKline (GSK), the vaccine manufacturer, had committed to donate up to 10 million vaccine doses as part of its support for the MVIP.<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-8\" id=\"fnref-osA4k7SK74nsJGypF-8\">[8]</a></sup> This quantity of vaccine was set aside to allow completion of the pilot program, including vaccination in the comparison areas.<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-9\" id=\"fnref-osA4k7SK74nsJGypF-9\">[9]</a></sup> However, additional support was needed to be able to utilize these vaccines in advance of Gavi financing, including (for example) funding to cover the costs of safe injection supplies and vaccine shipping and handling, as well as the technical assistance required to support vaccine implementation.</p>\n<p>With funding from GiveWell, PATH believed it could provide the necessary technical assistance to the ministries of health in Ghana, Kenya, and Malawi to support them in using the donated vaccines from GSK and expand vaccine access to the comparison areas at the end of 2022\u2014providing an estimated additional year of protection for communities at risk of malaria.<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-10\" id=\"fnref-osA4k7SK74nsJGypF-10\">[10]</a></sup> PATH planned to work with Gavi, UNICEF, and the WHO to ensure that the vaccine doses and other supplies could be provided to countries, as well as to assist with developing vaccine policies, training healthcare workers, raising public awareness of the program, and supporting the vaccine rollout.<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-11\" id=\"fnref-osA4k7SK74nsJGypF-11\">[11]</a></sup></p>\n<p><strong>Making and monitoring the grant</strong></p>\n<p>In order to assess the opportunity, we developed a preliminary cost-effectiveness analysis and spoke to stakeholders at the WHO, PATH, Gavi, and members of the Ministries of Health in Ghana and Kenya to learn more. We estimated that this grant would enable the vaccination of over <a href=\"https://docs.google.com/spreadsheets/d/1T5joAEoQeym8g5LIQQz7zbGEDIJm-ZPkZJr9dGXI3Hg/edit#gid=83206209&amp;range=B18:D18\">450,000 children</a> who wouldn\u2019t otherwise have been vaccinated, saving over 400 lives. We published more details on the case for the grant, as well as our uncertainties, <a href=\"https://www.givewell.org/research/grants/PATH-malaria-vaccines-January-2022\">here</a>.</p>\n<p>Since then, we\u2019ve kept in touch with PATH to monitor the progress of the rollout. During 2022, the Ministries of Health in each country formally approved the use of the malaria vaccine in their countries and developed plans for the rollout, including training for healthcare workers and supply chain plans. As of March 2023, implementation in the comparison areas of the pilot had begun in all three countries.</p>\n<p>In the coming year, we hope to learn more about how many children are being treated, as well as the future costs of the vaccine at scale, which we have significant remaining uncertainties about. Additionally, implementation in the comparison areas had begun in all three countries by March 2023, which is later than <a href=\"https://www.givewell.org/research/grants/PATH-malaria-vaccines-January-2022#Funding_gap_and_implementation_timelines\">our initial forecast</a> that implementation would begin by January 2023.<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-12\" id=\"fnref-osA4k7SK74nsJGypF-12\">[12]</a></sup> These factors are key inputs in our cost-effectiveness analysis, so learning more will help us determine whether this opportunity was ultimately cost-effective, and improve our future funding decisions.</p>\n<p><strong>What\u2019s next for GiveWell\u2019s malaria funding</strong></p>\n<p>This grant aimed to provide initial access to the malaria vaccine in the comparison areas of the pilot program; as such, it\u2019s a one-time opportunity that we don\u2019t expect to fund again.</p>\n<p>We will continue to monitor new developments with the malaria vaccines and may consider other grants related to RTS,S or other vaccines if we believe they are cost-effective. The WHO has published guidance recommending the use of RTS,S in certain contexts as part of a mix of interventions, which might include malaria nets and seasonal malaria chemoprevention (SMC)<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-13\" id=\"fnref-osA4k7SK74nsJGypF-13\">[13]</a></sup>\u2014interventions provided through two of the <a href=\"https://www.givewell.org/charities/top-charities\">top charity</a> programs we currently recommend. However, we still have more to learn about the cost-effectiveness and implications of delivering RTS,S alongside other interventions. To help generate more evidence in this area, we recently recommended a grant of approximately $1.6 million to support a trial of the effects of perennial malaria chemoprevention delivered alongside the RTS,S malaria vaccine, which will be compared to the effects of the vaccine on its own.<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-14\" id=\"fnref-osA4k7SK74nsJGypF-14\">[14]</a></sup> We plan to publish more about this grant soon.</p>\n<p>We\u2019re also closely following the development of the R21 vaccine. Though R21 is still in late-stage trials, an <a href=\"https://doi.org/10.1016/S0140-6736(21)00943-0\">earlier study</a> showed up to 77% efficacy at protecting against malaria, higher than <a href=\"https://www.givewell.org/international/technical/programs/malaria-vaccines#Evidence_of_Effectiveness_for_RTSS\">what's been found for RTS,S</a>. R21 is also reportedly less complex to manufacture than RTS,S.<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-15\" id=\"fnref-osA4k7SK74nsJGypF-15\">[15]</a></sup> If so, it could help alleviate expected supply constraints as the demand for malaria vaccines exceeds available doses.<sup class=\"footnote-ref\"><a href=\"#fn-osA4k7SK74nsJGypF-16\" id=\"fnref-osA4k7SK74nsJGypF-16\">[16]</a></sup></p>\n<p>However, in the study linked above, the vaccine was given before the peak malaria season, in an area with seasonal (as opposed to year-round) malaria transmission. A <a href=\"https://www.medpagetoday.com/meetingcoverage/astmh/101561\">preliminary analysis</a> from unpublished results of a late-stage trial suggests that R21's efficacy might be similar in areas with perennial malaria transmission, but more complete data would help us make a better-informed judgment about this. Additionally, unlike RTS,S, R21 has not yet been approved by the WHO.</p>\n<p>GiveWell has grown to be a major supporter of malaria programs because we believe these interventions are among the best ways to save and improve lives in the lowest-income communities in the world. We\u2019re excited to continue to invest in a range of malaria control interventions, including vaccines, nets, and SMC, as well as to investigate other potentially promising malaria programs. In particular, nets and SMC\u2014the programs provided by our top charities <a href=\"https://www.givewell.org/charities/amf\">Against Malaria Foundation</a> and <a href=\"https://www.givewell.org/charities/malaria-consortium\">Malaria Consortium</a>, respectively\u2014are some of the most cost-effective ways we\u2019re aware of to save and improve lives. These programs continue to offer a strong opportunity for donors to make a difference through their giving, and we remain excited to support their important work.</p>\n<h2>Notes</h2>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-osA4k7SK74nsJGypF-1\" class=\"footnote-item\"><ul>\n<li>\"The estimated deaths in 2021 were 619 000, a slight decline compared with 2020.\" <a href=\"https://www.who.int/teams/global-malaria-programme/reports/world-malaria-report-2022\">World Health Organization, World malaria report 2022</a>, p. 17</li>\n<li>\"The WHO African Region continues to carry a disproportionately high share of the global malaria burden. In 2021 the Region was home to about 95% of all malaria cases and 96% of deaths. Children under 5 years of age accounted for about 80% of all malaria deaths in the Region.\" <a href=\"https://www.who.int/news-room/fact-sheets/detail/malaria\">World Health Organization, \"Malaria,\" 2023</a></li>\n<li>\"Malaria is a life-threatening disease spread to humans by some types of mosquitoes. It is mostly found in tropical countries. It is preventable and curable.\" <a href=\"https://www.who.int/news-room/fact-sheets/detail/malaria\">World Health Organization, \"Malaria,\" 2023 </a></li>\n</ul>\n <a href=\"#fnref-osA4k7SK74nsJGypF-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-osA4k7SK74nsJGypF-2\" class=\"footnote-item\"><p>\"In October 2021, on the advice of two WHO global advisory bodies, one for immunization and the other for malaria, the Organization made this recommendation. WHO recommends the RTS,S/AS01 malaria vaccine be used for the prevention of P. falciparum malaria in children living in regions with moderate to high transmission as defined by WHO.\" <a href=\"https://www.who.int/news-room/questions-and-answers/item/q-a-on-rts-s-malaria-vaccine\">World Health Organization, \"Q&amp;A on RTS,S malaria vaccine,\" 2023. </a> <a href=\"#fnref-osA4k7SK74nsJGypF-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-osA4k7SK74nsJGypF-3\" class=\"footnote-item\"><p>See our estimates of the numbers of children treated in our cost-effectiveness model for this grant <a href=\"https://docs.google.com/spreadsheets/d/1T5joAEoQeym8g5LIQQz7zbGEDIJm-ZPkZJr9dGXI3Hg/edit#gid=83206209&amp;range=B18:D18\">here</a>. <a href=\"#fnref-osA4k7SK74nsJGypF-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-osA4k7SK74nsJGypF-4\" class=\"footnote-item\"><ul>\n<li>\"Vaccinations began in the 3 pilot countries in 2019: in Malawi on 23 April, in Ghana on 30 April, and in Kenya on 13 September.\" <a href=\"https://www.who.int/news-room/questions-and-answers/item/malaria-vaccine-implementation-programme\">World Health Organization, \"Malaria: The malaria vaccine implementation programme (MVIP),\" 2020</a></li>\n<li>\"WHO is working with PATH and GSK on the vaccine pilot programme through a collaboration agreement. PATH provides technical and project management support and is leading studies on health care utilization and the economics of vaccine implementation.\" <a href=\"https://www.who.int/news-room/questions-and-answers/item/malaria-vaccine-implementation-programme\">World Health Organization, \"Malaria: The malaria vaccine implementation programme (MVIP),\" 2020</a></li>\n</ul>\n <a href=\"#fnref-osA4k7SK74nsJGypF-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-osA4k7SK74nsJGypF-5\" class=\"footnote-item\"><p>\"It is important at this stage to learn how best to introduce the malaria vaccine into routine immunization systems, and to evaluate that introduction. To do this, some districts/sub-counties within the selected areas will have the opportunity to introduce the vaccine into their immunization schedules at the start of the programme, while other districts will not receive the vaccine until a later date, should there be a WHO recommendation for wider use. Assignment of areas into those that receive the vaccine and those that do not has been through a process called \u201crandomization\u201d, based on chance using a computer programme.\nIntroducing the vaccine into some areas, while delaying it in others, is also important for understanding the public health usefulness of the vaccine and will provide key information on whether the vaccine should be introduced throughout the pilot countries and more broadly across Africa.\" <a href=\"https://www.who.int/news-room/questions-and-answers/item/malaria-vaccine-implementation-programme\">World Health Organization, \"Malaria: The malaria vaccine implementation programme (MVIP),\" 2020</a> <a href=\"#fnref-osA4k7SK74nsJGypF-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-osA4k7SK74nsJGypF-6\" class=\"footnote-item\"><p><a href=\"https://www.gavi.org/\">Gavi, the Vaccine Alliance</a> is a public\u2013private global health partnership that provides support to increase access to immunization in poor countries. <a href=\"#fnref-osA4k7SK74nsJGypF-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-osA4k7SK74nsJGypF-7\" class=\"footnote-item\"><p>\"If GiveWell didn\u2019t fund this opportunity, the most likely scenario is that access to these vaccines would be available only through the standard process of these countries applying for support through Gavi\u2019s malaria vaccination program. The earliest rollout through Gavi processes would be by the end of 2023.\" <a href=\"https://docs.google.com/document/d/1XKSf9buniP9NWaWeiknPXNI8o4z59FWXnkJgohHJgMs/edit?usp=sharing\">GiveWell's non-verbatim summary of a conversation with PATH and WHO, January 5, 2022</a> <a href=\"#fnref-osA4k7SK74nsJGypF-7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-osA4k7SK74nsJGypF-8\" class=\"footnote-item\"><p>\"GSK is donating up to 10 million doses of RTS,S vaccine for use in the pilot and is leading additional studies to continue monitoring the vaccine\u2019s safety and effectiveness in routine use.\" <a href=\"https://www.who.int/news-room/questions-and-answers/item/malaria-vaccine-implementation-programme\">World Health Organization, \"Malaria: The malaria vaccine implementation programme (MVIP),\" 2020</a> <a href=\"#fnref-osA4k7SK74nsJGypF-8\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-osA4k7SK74nsJGypF-9\" class=\"footnote-item\"><p>\"The limited potential alternative use of donated doses if they weren\u2019t used to expand vaccine access within the pilot comparison areas. As part of the Malaria Vaccine Implementation Programme (MVIP), there was an agreement that GSK would provide up to 10 million doses. It is generally understood that these doses should be used for the pilot comparison areas.\" <a href=\"https://docs.google.com/document/d/1XKSf9buniP9NWaWeiknPXNI8o4z59FWXnkJgohHJgMs/edit?usp=sharing\">GiveWell's non-verbatim summary of a conversation with PATH and WHO, January 5, 2022</a> <a href=\"#fnref-osA4k7SK74nsJGypF-9\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-osA4k7SK74nsJGypF-10\" class=\"footnote-item\"><p>\"If GiveWell didn\u2019t fund this opportunity, the most likely scenario is that access to these vaccines would be available only through the standard process of these countries applying for support through Gavi\u2019s malaria vaccination program. The earliest rollout through Gavi processes would be by the end of 2023.\nWith GiveWell funding, the expansion to pilot comparison areas could be in the third or fourth quarter of 2022.\" <a href=\"https://docs.google.com/document/d/1XKSf9buniP9NWaWeiknPXNI8o4z59FWXnkJgohHJgMs/edit?usp=sharing\">GiveWell's non-verbatim summary of a conversation with PATH and WHO, January 5, 2022</a> <a href=\"#fnref-osA4k7SK74nsJGypF-10\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-osA4k7SK74nsJGypF-11\" class=\"footnote-item\"><p>Technical assistance is a broad category, but generally refers to programs that focus on helping governments implement an intervention, rather than programs that implement an intervention directly.\nSee expected grant activities <a href=\"https://www.givewell.org/research/grants/PATH-malaria-vaccines-January-2022#The_grant\">here</a>. <a href=\"#fnref-osA4k7SK74nsJGypF-11\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-osA4k7SK74nsJGypF-12\" class=\"footnote-item\"><p>Malawi began expanded implementation in November 2022, Ghana in February 2023, and Kenya in March 2023. Sally Ethelston, Director, Resource Mobilization and Outreach, Malaria Vaccines, PATH, emails to GiveWell, February 23, 2023, and March 7, 2023 (unpublished) <a href=\"#fnref-osA4k7SK74nsJGypF-12\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-osA4k7SK74nsJGypF-13\" class=\"footnote-item\"><ul>\n<li>\"WHO advises countries to use a tailored mix of recommended malaria control interventions \u2013 including the malaria vaccine \u2013 to increase impact in reducing malaria illness and deaths among children living in areas of moderate to high malaria transmission.\" <a href=\"https://www.who.int/news-room/questions-and-answers/item/q-a-on-rts-s-malaria-vaccine\">World Health Organization, \"Q&amp;A on RTS,S malaria vaccine,\" 2023. </a></li>\n<li>\"For malaria, WHO has recommended a range of interventions - namely, vector control, chemoprevention, diagnostic testing and treatment - to reduce transmission and prevent morbidity and mortality.\" <a href=\"https://app.magicapp.org/#/guideline/7089\">World Health Organization, \"Guidelines for Malaria - 14 March 2023\"</a></li>\n<li>\"Malaria vector control interventions recommended for large-scale deployment are: i) ITNs [insecticide-treated nets] that are prequalified by WHO, which in many settings continue to be pyrethroid-only long-lasting insecticidal nets (LLINs)\" <a href=\"https://app.magicapp.org/#/guideline/7089\">World Health Organization, \"Guidelines for Malaria - 14 March 2023\"</a></li>\n</ul>\n <a href=\"#fnref-osA4k7SK74nsJGypF-13\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-osA4k7SK74nsJGypF-14\" class=\"footnote-item\"><p>Thus the study\u2019s findings will only apply to perennial transmission environments. <a href=\"#fnref-osA4k7SK74nsJGypF-14\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-osA4k7SK74nsJGypF-15\" class=\"footnote-item\"><p>\"An important advantage of R21/MM relates to its potential for large-scale manufacturing, which will be critical for the supply of hundreds of millions of doses of vaccine required annually for each birth cohort of children in malaria endemic regions of Africa\u2026. The saponin adjuvant, MM, lacks the monophosphoryl lipid A adjuvant component, which is found in other adjuvants and is less complex to manufacture, and this enables large-scale and low-cost supply of R21/MM.\" <a href=\"https://doi.org/10.1016/S0140-6736(21)00943-0\">Datoo et al. 2021</a> <a href=\"#fnref-osA4k7SK74nsJGypF-15\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-osA4k7SK74nsJGypF-16\" class=\"footnote-item\"><p>\"The malaria vaccine market faces considerable supply constraints in the short term and potentially into the medium term, with initial supply expected to fall substantially short of demand.\" <a href=\"https://www.gavi.org/sites/default/files/document/Malaria-Roadmap-Public-Summary.pdf\">Gavi, the Vaccine Alliance, Market Shaping Roadmap: Malaria Vaccines</a>, p. 2 <a href=\"#fnref-osA4k7SK74nsJGypF-16\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "GiveWell"}}, {"_id": "sK2C8CDdEEuhPYCeu", "title": "How Can Engineers Do Impactful Work in Biosecurity?", "postedAt": "2023-05-12T10:44:05.824Z", "htmlBody": "<p><i>This excerpt is taken from the </i><a href=\"https://www.highimpactengineers.org/how-engineers-can-have-impact-working-in-biosecurity\"><i>High Impact Engineers' Resources Portal</i></a><i>, where we include an introduction to GCBRs and a list of resources and organisations. This was originally written by Bryce Rogers, who has since left HI-Eng to pursue other opportunities. We are grateful to&nbsp;WB, TA, JG, and JVB for their input and feedback. Any remaining mistakes are our own.</i></p><p><i>Edited 11th October 2023 to update links.</i></p><p>Unfortunately, there is probably no single \u201c<a href=\"https://en.wikipedia.org/wiki/Silver_bullet\">silver bullet</a>\u201d solution to reducing GCBRs or even just pandemics. To focus just on pandemics, experts posit that a comprehensive <a href=\"https://onehealthoutlook.biomedcentral.com/articles/10.1186/s42522-021-00049-4\">web of prevention</a>, consisting of policy, technology, and public health measures ought to implemented. Physical technologies \u2014 face masks, medical devices, sanitisation systems, biosensors, manufacturing methods, and more \u2014 are critical to the efficacy of this web. Particularly valuable are technologies that are preventative (<i>prevent</i> rather than react to pandemics) and/or pathogen-agnostic (combat all/most pathogens, rather than a particular type or strain). <a href=\"https://forum.effectivealtruism.org/posts/Bd7K4XCg4BGEaSetp/biosecurity-needs-engineers-and-materials-scientists\">Engineers are needed</a> to develop and implement these technologies.</p><p>In their ambitious reports <a href=\"https://biodefensecommission.org/reports/the-apollo-program-for-biodefense-winning-the-race-against-biological-threats/\">The Apollo Program for Biodefense</a> and <a href=\"https://biodefensecommission.org/reports/the-athena-agenda-advancing-the-apollo-program-for-biodefense/\">The Athena Agenda</a>, the <a href=\"https://biodefensecommission.org/\">Bipartisan Commission on Biodefense</a> (BCB) outlines several key technology areas they recommend to be developed and implemented to pandemic-proof the United States by 2030. We have selected and summarised some of the technology areas that are most relevant to engineers below (more in-depth discussions of each area can be found in the original reports).</p><p>Although far from comprehensive, if these technologies are developed and deployed successfully, they could be key elements in a global web of pandemic prevention. You can find many other concrete biosecurity project ideas <a href=\"https://forum.effectivealtruism.org/posts/DcKo3Hx8hzrZWjYp5/list-of-lists-of-concrete-biosecurity-project-ideas\">here</a> that we don\u2019t cover below.</p><h2>Next-generation personal protective equipment</h2><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sK2C8CDdEEuhPYCeu/cibu2ioitwg0qwgvy8cn\" alt=\"matthias-heyde-W9AeLrsocR4-unsplash.jpg\"></p><p>Figure 2: Current personal protective equipment (PPE) leaves much to be desired. Photo by <a href=\"https://unsplash.com/@heydedesign?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Matthias Heyde</a> on <a href=\"https://unsplash.com/photos/W9AeLrsocR4?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></p><p>Personal protective equipment (PPE), such as masks and respirators, creates a physical barrier between the wearer and pathogens. PPE has the potential to be an effective and pathogen-agnostic barrier against pandemics, but current designs <a href=\"https://www.centerforhealthsecurity.org/our-work/publications/masks-and-respirators-for-the-21st-century\">leave much to be desired</a>. In general, current designs are difficult to use properly, not widely accessible enough, don\u2019t fit everyone, and haven\u2019t been improved in decades. These could be improved by mechanical engineers and materials scientists.</p><p>There have been many projects to create next-gen PPE, such as the <a href=\"https://drive.hhs.gov/mask_challenge.html\">BARDA Mask Innovation Challenge</a>, the <a href=\"https://www.xprize.org/challenge/nextgenmask\">XPrize Mask Challenge</a>, and <a href=\"https://forum.effectivealtruism.org/posts/wAzYXjGxhJmrxunct/speedrun-develop-an-affordable-super-ppe#Other_actors_in_this_space\">others</a> both within and outside EA. Although there is a case for creating a new generation of improved PPE, it seems that the main bottleneck in this area is in generating an economic incentive to improve PPE, provided by changing policy or finding a non-healthcare market. More research is being done in this area (e.g. by <a href=\"https://www.gryphonscientific.com/towards-a-theory-of-pandemic-proof-ppe/\">Gryphon Scientific</a> and <a href=\"https://forum.effectivealtruism.org/posts/wAzYXjGxhJmrxunct/speedrun-develop-an-affordable-super-ppe#Other_actors_in_this_space\">others</a>) so there is likely to be more clarity in this area in the coming year.</p><h2>Pathogen transmission suppression in the built environment</h2><p>The risk of pathogen transmission is generally vastly <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8051020/\">higher indoors than outdoors</a>. Widespread incorporation of pathogen transmission mitigation technologies in highly populated buildings and vehicles could be an effective, pathogen-agnostic means of pandemic prevention. This would have the added benefit of passively decreasing the spread of non-pandemic diseases as well.</p><p>Although suppressing pathogen transmission indoors technically includes <a href=\"https://www.cbe.ncsu.edu/blog/2019/11/12/a-new-class-of-antimicrobial-materials-self-sterilizing-polymers/\">self-sterilizing materials</a>and fomite-neutralizing technologies (e.g. <a href=\"https://www.mdpi.com/1996-1944/13/15/3244\">copper-alloy surfaces</a>), the most promising interventions involve <strong>improving indoor air quality</strong>. The transmission of COVID-19 could be reduced by 80% with <a href=\"https://www.fondazionehume.it/data-analysis/controlled-mechanical-ventilation-cmv-works/\">improved air filtration and indoor ventilation</a>. The White House Office of Science and Technology Policy highlighted that clean indoor air is not only beneficial for reducing pandemic spread, but also mitigate the detrimental health effects of air pollution.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sK2C8CDdEEuhPYCeu/bkbxgfgqfdbnwq2kqdx7\" alt=\"alvaro-bernal-_Ib-JulMgzo-unsplash.jpg\"></p><p>Figure 3: Increasing indoor air quality reduces not only pandemic spread but also air pollution. Photo by <a href=\"https://unsplash.com/@abn?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">\u00c1lvaro Bernal</a> on <a href=\"https://unsplash.com/s/photos/air-conditioner?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></p><p>Improving indoor air quality can be achieved mechanically through 3 main technologies:</p><ul><li>Improving ventilation</li><li>Improving filtration</li><li>Implementing ultraviolet germicidal irradiation (UVGI) \u2013 also known as germicidal UV (GUV)</li></ul><p>Of these three technologies, <strong>we think that UVGI is the most promising intervention for engineers</strong> (particularly mechanical, materials, and aerospace engineers) to work on. You can explore the other ventilation and filtration in <a href=\"https://forum.effectivealtruism.org/posts/j4eTj9dGqzEC5LEzK/cause-exploration-prizes-indoor-air-quality-to-reduce\">this</a> and <a href=\"https://forum.effectivealtruism.org/posts/WLok4YuJ4kfFpDRTi/first-clean-water-now-clean-air\">this</a> EA Forum post.</p><h3>Ultraviolet Germicidal Irradiation (UVGI)</h3><p>UVGI interventions fall into two categories: conventional <strong>upper-room UVC</strong> and <strong>far-UVC</strong>.</p><p>Conventional upper-room UVC uses UV wavelengths of around 254 nm to disinfect the air in large indoor spaces. The UVC light is typically installed in the upper part of a room and directed upwards to kill airborne viral, bacterial, and fungal organisms. This method requires careful placement of the UVC emitters to prevent direct or reflected exposure to humans, as it can cause skin and eye damage. This technology <a href=\"https://journals.sagepub.com/doi/pdf/10.1177/003335491012500105\">was demonstrated in 1935</a> but still hasn\u2019t been universally implemented, especially in LMICs. It is mainly constrained by the lack of skilled technicians available to install it properly, especially in <a href=\"https://www.researchgate.net/publication/267198093_Guidelines_for_the_Utilisation_of_Ultraviolet_Germicidal_Irradiation_UVGI_Technology_in_Controlling_Transmission_of_Tuberculosis_in_Health_Care_Facilities_in_South_Africa\">resource-constrained countries</a>. Read more on the <a href=\"https://www.cdc.gov/coronavirus/2019-ncov/community/ventilation/uvgi.html\">US Center for Disease Control and Prevention website</a>.</p><p>Far-UVC uses UV wavelengths between 207 and 222 nm in doses of 100 mJ/cm2 that have been proven effective at <a href=\"https://www.nature.com/articles/s41598-020-67211-2\">killing viruses</a> and <a href=\"https://www.nature.com/articles/s41598-022-08462-z\">bacteria</a> while being <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7894148/\">safe for human exposure</a>. It is still an emerging technology and currently only used by early adopters in small-scale settings to disinfect surfaces and air. Although it is partly market/policy-bottlenecked, there are still many open questions around emitters and system design that lend themselves to engineering expertise. Organisations such as <a href=\"https://coda.io/d/_d9tTRAlsYqC#Full-Organisation-and-Publisher-List_tuiYQ/r35&amp;view=modal\">SecureBio</a> and <a href=\"https://rethinkpriorities.org/publications/air-safety-to-combat-global-catastrophic-biorisks\">Rethink Priorities</a> are looking into this area.</p><p>You can read more technical specifications for these two UVGI interventions in this <a href=\"https://docs.google.com/document/d/1OCCo_1S3q7mHWodsj1l03lKtoycg8fDWiiHHrnXBhTY/edit\">GUV cheatsheet</a>.</p><p>UVGI interventions can be combined with ventilation and/or filtration systems (attention to air flow and volume needed to ensure adequate irradiation time, <a href=\"https://www.amca.org/educate/articles-and-technical-papers/amca-inmotion-articles/uv-c-for-hvac-air-and-surface-disinfection-2.html\">among other considerations</a>), and could also lend themselves to the integration of <a href=\"https://councilonstrategicrisks.org/analysis/reports/pathogen-early-warning-a-progress-report-path-forward/\">pathogen surveillance technologies</a>. Africa CDC\u2019s work in <a href=\"https://preventepidemics.org/epidemics-that-didnt-happen-2021/covid-19-africa/\">coordinating a continent-wide surveillance system</a> to combat COVID-19 could be a model for other continents and nations. Technical work still needs to be done to develop <a href=\"https://forum.effectivealtruism.org/posts/aTMgGL3gwKt7j2ZJG/overview-of-the-pathogen-biosurveillance-landscape-1\">pathogen surveillance technologies</a>, which could potentially be a good fit for biochemists and chemical engineers.</p><h2>Ubiquitous sequencing</h2><p>Sequencing-based biosurveillance played a crucial role in detecting and tracking the spread of the <a href=\"https://www.cdc.gov/coronavirus/2019-ncov/variants/spheres.html\">COVID-19 pandemic</a>. By sequencing the virus\u2019s genetic material, researchers were able to understand the origins and evolution of the virus, identify and track the emergence of different variants, and assess the <a href=\"https://www.gov.uk/government/news/122-million-boost-for-genomic-surveillance-to-help-stop-transmission-of-covid-19\">mutation\u2019s influence</a> on transmission, disease severity, or efficacy of vaccines against new strains. This helped public health officials make informed decisions about control measures, such as quarantine requirements and travel restrictions, to control the spread of the virus.</p><p>The cost of genetic sequencing <a href=\"https://www.genome.gov/about-genomics/fact-sheets/DNA-Sequencing-Costs-Data\">has plummeted</a> in recent decades, <a href=\"https://centuryofbio.substack.com/p/whats-different-part-one-sequencing\">decreasing exponentially</a> at a rate outpacing <a href=\"https://en.wikipedia.org/wiki/Moore%27s_law\">Moore\u2019s law</a>.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sK2C8CDdEEuhPYCeu/lb9uvmupw7gln8ioyrsr\" alt=\"genetic moores law.jpg\"></p><p>Figure 4: Exponentially decreasing cost of genetic sequencing in recent years (vertical axis is logarithmic)</p><p>This increasing affordability has enabled, in addition to many other advancements, more widespread application of <a href=\"https://en.wikipedia.org/wiki/Metagenomics\">metagenomic sequencing</a> (the sequencing of all genetic material present in an environmental or patient sample). Unlike targeted sequencing, in which the genetic material of one or more specific pathogens is searched for in a sample, metagenomic sequencing enables rapid, pathogen-agnostic detection of potentially <i>any</i> disease present in the sample. Metagenomic sequencing for biosurveillance can be applied in two ways:</p><ul><li>Metagenomics for environmental sampling involves the collection and analysis of genetic material from complex environmental samples, such as wastewater. Identification and characterisation of the microbes present in these samples can be used to monitor for dangerous pathogens or disease spread. <a href=\"https://biobot.io/\">Biobot Analytics</a> and <a href=\"https://www.concentricbyginkgo.com/category/all-posts/\">Concentric</a> are companies doing wastewater monitoring and metagenomics for early detection.</li><li>Metagenomics for point-of-care clinical care allows the detection of a wide range of pathogens in human patients, including those that may not be detected by traditional culture-based methods. By rapidly identifying the specific cause of a disease, clinicians can quickly determine appropriate treatment options. The <a href=\"https://www.sanger.ac.uk/group/respiratory-virus-and-microbiome-initiative/\">Respiratory Virus and Microbiome Initiative</a> in the UK (along with <a href=\"https://www.theguardian.com/world/2023/apr/02/uk-develops-genetic-early-warning-system-for-future-pandemics\">other centres</a> in the US and Germany) is developing metagenomic approaches to analyse respiratory swab samples.</li></ul><p>The Bipartisan Commission on Biodefense advises that, to achieve the goal of ubiquitous metagenomic sequencing, sequencers should be made smaller, less reliant on expensive reagents, and to have on-chip sample preparation. The ideal outcome is for sequencers to be inexpensive and hand held. <a href=\"https://aseq.substack.com/p/reticula-pt-2-covid19-and-sequencing?s=w\">Reticula</a> is one company pursuing this goal, and there are many others doing genetic sequencing.</p><h2>Needle-free methods of drug and vaccine administration</h2><p>Most vaccines are administered via needle, the invasive nature of which necessitates administration by a healthcare provider. Needle delivery of vaccines thus adds to the already-severe strain on healthcare systems during pandemics. Additionally, fear of needles may decrease public uptake of vaccines that are only available via injection. Minimally-invasive self-administration methods could decrease strain on healthcare systems and maximise the number of individuals reached.</p><p>Many needle-free methods of drug and vaccine administration are being researched, including <a href=\"https://erictopol.substack.com/p/project-next-gen-the-united-states\">nasal sprays</a>, <a href=\"https://www.reviewofoptometry.com/article/the-coming-rise-of-drugdelivery-contact-lenses\">contact lenses</a>, <a href=\"https://www.scotsman.com/read-this/the-worlds-first-oral-covid-19-vaccine-is-being-prepared-for-clinical-trials-heres-how-it-works-3174173\">oral capsules</a>, <a href=\"https://www.sciencedaily.com/releases/2022/02/220209112136.htm\">inhalers</a>, <a href=\"https://www.reuters.com/business/healthcare-pharmaceuticals/virtually-painless-needle-free-injections-developed-netherlands-2021-10-13/\">lasers</a>, <a href=\"https://www.nature.com/articles/s41378-023-00515-1\">transdermal patches</a> or <a href=\"https://www.sharesmagazine.co.uk/news/market/1676288287913724100/scancell-gets-positive-result-from-needle-free-covid-vaccines-trial\">microjets</a>, and others. Biologists, engineers, and others are needed to bring these technologies to maturity such that they may be deployed during a pandemic.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sK2C8CDdEEuhPYCeu/qazkovihtxcfaiulw0fn\" alt=\"injection.png\"></p><p>Figure 5: Needle-free forms of drugs and vaccine administration (via the <a href=\"https://biodefensecommission.org/reports/the-athena-agenda-advancing-the-apollo-program-for-biodefense/\">Athena Agenda</a>)</p><h2>Flexible and scalable manufacturing of pharmaceuticals</h2><p>During a pandemic, the successful deployment of a novel therapeutic or vaccine requires rapid and large scale manufacturing. But many existing manufacturing methods are slow and difficult to repurpose to a novel pathogen. These problems might be addressed via increased use of <a href=\"https://www.centerforhealthsecurity.org/our-work/pubs_archive/pubs-pdfs/2019/190423-OPP-platform-report.pdf\">platform technologies</a> that use the same manufacturing processes for multiple different vaccines. This is typically achieved by encoding a therapeutic into genetic material \u2014 a process that could be rapidly repurposed and scaled during pandemic times.</p><p><a href=\"https://www.nature.com/articles/s41563-020-0746-0\">Further work is needed</a> to establish infrastructure that allows rapid large-scale vaccine manufacturing during outbreaks, and to mitigate current technical limitations of platform technologies, such as the reliance of mRNA vaccines on constant refrigeration. This area may be particularly relevant for industrial or manufacturing engineers.</p><h2>Minimally- and non-invasive infection detection</h2><p>Typically, an infection is only detected once a host has developed symptoms, gone to a healthcare provider, and had a pathogen-specific test administered. This is suboptimal during an outbreak given that symptoms may take days to develop, and the host may unknowingly infect several others before they receive a diagnosis. Ideally, a noninvasive, <a href=\"https://en.wikipedia.org/wiki/Point-of-care_testing\">point-of-care</a> and <a href=\"https://mammoth.bio/2020/07/07/the-importance-of-point-of-need-diagnostics-for-covid-19-and-beyond/\">point-of-need</a> testing technology capable of early detection could be deployed to individuals at risk of infection during an outbreak. Examples of point-of-care testing include pregnancy tests, lateral flow tests, and <a href=\"https://www.nature.com/articles/srep33735\">pooled next-generation sequencing</a>. Policies to encourage testing at particular intervals, e.g. at schools, before large gatherings, would also be required.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sK2C8CDdEEuhPYCeu/lyjov7gdlcfppeder9ec\" alt=\"roman-wimmers-6QUjAos04fw-unsplash.jpg\"></p><p>Figure 6: Lateral flow tests for COVID-19 is an example of a point-of-care testing technology. However, the throat swab can be uncomfortable, and could be improved upon. Photo by <a href=\"https://unsplash.com/@akitada31?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Roman Wimmers</a> on <a href=\"https://unsplash.com/s/photos/covid-test?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></p><h1>Statement on infohazards</h1><p>An <a href=\"https://en.wikipedia.org/wiki/Information_hazard\">information hazard</a>, or <i>infohazard,</i> is \"a risk that arises from the dissemination of (true) information that may cause harm or enable some agent to cause harm.\" An example of an infohazards in the context of biosecurity is the genetic sequence of a dangerous pathogen.</p><h2>Some guidelines</h2><ol><li>Avoid brainstorming concrete attacks.</li><li>Focus on low-downside risk interventions, especially when developing interventions with <a href=\"https://en.wikipedia.org/wiki/Dual-use_technology\">dual-use</a> potential. Upsides and downsides must be weighed carefully, and you can have a strongly negative impact when working on dual-use technology (it\u2019s important to note that your impact does not zero out at \u2018nothing\u2019).</li><li>Consult with experts before taking actions.</li></ol><p>Biosecurity is rife with infohazards, largely due to the dual-use nature of most biotechnological knowledge. For the interventions we explore above, the dual-use potential of indoor air interventions is close to nil; platform biotechnologies can vary a lot depending on the details of the platform; and biosurveillance could become a privacy and information security hazard. If you pursue biosecurity-adjacent work, it is important to deliberately prevent the dissemination of infohazards.</p>", "user": {"username": "Jessica Wen"}}, {"_id": "53CmNKZH3BnZ9HcJd", "title": "Meet the Olympians bringing effective altruism to athlete climate investing", "postedAt": "2023-05-12T09:50:01.768Z", "htmlBody": "", "user": {"username": "George Timms"}}, {"_id": "6Rrv6G74tzAyDZxz9", "title": "I want to read more stories by and about the people of Effective Altruism", "postedAt": "2023-05-12T11:08:32.180Z", "htmlBody": "<h3><strong>TL; DR</strong></h3><ul><li>I want to read more stories by and about the people of the Effective Altruism movement<ul><li>But like, fun ones, not CVs</li></ul></li><li>I\u2019ve added a tag for EA origin stories and tagged a bunch of relevant posts from the forum<ul><li>If I\u2019ve missed some, please tag them</li><li>The <a href=\"https://forum.effectivealtruism.org/topics/community-experiences\"><u>community experiences tag</u></a> has a lot of others that don\u2019t quite fit</li></ul></li><li>I think it is important to emphasise the personal in the effective altruism movement - you never know if your story is enough to connect with someone (especially if you don\u2019t fit the stereotypical EA mold)</li><li>I would also be very interested in reading folks\u2019 answers to the \u201cWhat is your current plan to improve the world?\u201d question from the EA Global application - it\u2019s really helpful to see other people\u2019s thought processes (you can read mine <a href=\"https://bashingthearc.substack.com/p/my-current-may-2023-plan-to-do-good\"><u>here</u></a>)</li></ul><h3>Why?</h3><p>At least for me, what grabbed and kept my attention when I first heard about EA were the stories of people on the ground trying to <i>do</i> effective altruism.</p><p>The audacity of a <a href=\"https://forum.effectivealtruism.org/posts/9a7xMXoSiQs3EYPA2/the-history-of-the-term-effective-altruism\"><u>group of students</u></a> looking at the enormity of suffering in the world but then pushing past that overwhelm. Recognising that they could use their privileges to make a dent if they really gave it a go.</p><p>The folks behind <a href=\"https://www.charityentrepreneurship.com/our-charities\"><u>Charity Entrepreneurship</u></a> who didn\u2019t stop at one highly effective charity but decided to jump straight into making an non-profit incubator to multiply their impact - building out, in my opinion, some of the coolest projects in the movement.</p><p>I love that the 80,000 hours podcast takes the concept behind <a href=\"https://www.youtube.com/watch?v=o2m0HHcyMM4\"><u>Big Talk</u></a> seriously</p><p>It\u2019s absurd but amazing!</p><p>&nbsp;</p><p>I love the ethos of practicality within the movement. It isn\u2019t about purity, it isn\u2019t about perfection, it\u2019s about actually changing the world.</p><p>These are the people I\u2019d back to build a robust Theory of Change that might just move us towards <a href=\"https://en.wiktionary.org/wiki/Fully_Automated_Luxury_Gay_Space_Communism\"><u>Fully Automated Luxury Gay Space Communism</u></a></p><p>Maybe that google doc already exists?</p><p>&nbsp;</p><p>I have never been the kind of person who had role models. I have always been a bit too cynical to put people on a pedestal. I had respect for successful people and tried to learn what I could from them but I didn\u2019t have heroes.</p><p>But my response to finding the EA movement was, \u201cFuck, these people are cool.\u201d</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d86c8fc-2749-4755-ba88-8cbc394b3f45_940x190.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d86c8fc-2749-4755-ba88-8cbc394b3f45_940x190.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d86c8fc-2749-4755-ba88-8cbc394b3f45_940x190.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d86c8fc-2749-4755-ba88-8cbc394b3f45_940x190.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d86c8fc-2749-4755-ba88-8cbc394b3f45_940x190.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d86c8fc-2749-4755-ba88-8cbc394b3f45_940x190.png 1456w\"></a></p><p>I think there is a problem with myth making and hero worshipping within EA. I do agree that it is healthier to <a href=\"https://forum.effectivealtruism.org/posts/jgspXC8GKA7RtxMRE/on-living-without-idols\"><u>Live Without Idols</u></a>. However, I don\u2019t think we should live without stories.</p><p>The stories I\u2019m more interested in are the personal ones. Of people actually going out and living their values. Examples of trades offs that real people make that allow them to be ambitiously altruistic in a way that suits them. That show that it is fine to care about lots of things. That it is okay to make changes in your life when you get more or better information.</p><p>I think about <a href=\"https://forum.effectivealtruism.org/posts/AJtbfPQL7gLqaNAcC/my-bargain-with-the-ea-machine\"><u>this post</u></a> a lot because I agree that if people think that \u201cdoing effective altruism\u201d means they have to live like monks and change their whole lives then they\u2019ll just reject it. Making big changes is hard. People aren\u2019t perfect.</p><p>I can trace huge number of positive changes in my life to my decision to take EA seriously but realistically it was my personal IRL and parasocial connections to the people of EA that gave me the space and support to make these big changes in my life. In the footnotes and in this post about my EA story, I\u2019ve included a list of podcasts, blog posts and other media by people within EA that were particularly influential and meaningful to me (if you made them then thank you &lt;3)</p><p>While I do see as EA as the <a href=\"https://forum.effectivealtruism.org/posts/LrpdPrfdEezK7xTYC/altruism-as-a-central-purpose\"><u>key source of purpose</u></a> in my life, it is a core value among many (I like <a href=\"https://www.clearerthinking.org/post/valuism\"><u>Valuism</u></a> - doing the <a href=\"https://programs.clearerthinking.org/intrinsic_values_test.html\"><u>intrinsic values test</u></a> was really helpful for me). Like everyone else in the EA movement, I\u2019m not an impact machine, I\u2019m a person. I love throwing themed costume parties, musicals, crafting and movies about female friendship. In an ideal world, I\u2019d spend my life connecting with loved ones and making art while pursuing my silly little personal projects. But we exist in an imperfect world.</p><p>I don\u2019t engage with these fiendishly difficult problems for fun. The world is just a big complex mess and I personally find it less stressful existing in that mess when I can do something practical about making it better.</p><p>I think that there\u2019s definitely a <a href=\"https://twitter.com/LizSpecht/status/1597275873509797891?s=20\"><u>selfish case for caring</u></a>. But, at least in my personal experience, disengagement from the world and apathy were clear signs of my unhappiness. I care about others because it brings me joy and really, what else is even worth doing? *[<i>I\u2019m sorry to my boss if you\u2019re reading this but I don\u2019t intrinsically care about building excellent B2B SaaS supporting the delivery of exceptional client service on tax engagements.]</i></p><p>A few of us want to start a project to collect stories from everyday people in the EA movement. It looks like there was were a couple of projects pre FTX:</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/WRB4Lt4XpP3uSPxhP/open-call-for-ea-stories\"><u>CEA had a plan to do some</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/w6X89f5Z9Xkri3BTG/is-there-a-humans-of-ea-project-meaning-a-project-portraying\"><u>Is there a \"Humans of EA\" project? Meaning a project portraying different members within EA. - EA Forum (effectivealtruism.org)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/L2qefgmsHo845F7HQ/recommend-me-eas-to-write-about\"><u>Recommend Me EAs To Write About - EA Forum (effectivealtruism.org)</u></a></li></ul><p>But I\u2019m probably more interested in ones written by the people themselves or with a more personal angle. I\u2019m very excited about the stuff <a href=\"https://ealifestyles.substack.com/p/welcome-to-the-ea-lifestyles-substack\"><u>EA Lifestyles</u></a> is putting out and GWWC\u2019s profiles.</p><p>Keen to hear what people think!</p><h3>List of EA media that were personally meaningful to me</h3><ul><li><a href=\"https://forum.effectivealtruism.org/posts/4kqiHGrZh6Rj7EmEW/cheerfully\"><u>Cheerfully - EA Forum (effectivealtruism.org)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/RScaAGSG3M3c8vgir/on-absurdity\"><u>On absurdity - EA Forum (effectivealtruism.org)</u></a></li><li><a href=\"https://80000hours.org/podcast/episodes/depression-anxiety-imposter-syndrome/\"><u>Having a successful career with depression, anxiety, and imposter syndrome - 80,000 Hours (80000hours.org)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/gp94EeYgbh5qjfu65/my-experience-with-imposter-syndrome-and-how-to-partly\"><u>My experience with imposter syndrome \u2014 and how to (partly) overcome it - EA Forum (effectivealtruism.org)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/CmGPp5p9RvTLuuzbt/aim-high-even-if-you-fall-short\"><u>Aim high, even if you fall short - EA Forum (effectivealtruism.org)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/3k4H3cyiHooTyLY6p/why-i-find-longtermism-hard-and-what-keeps-me-motivated\"><u>Why I find longtermism hard, and what keeps me motivated - EA Forum (effectivealtruism.org)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/zxjjy2Pax3EDc4mw3/scott-alexander-nobody-is-perfect-everything-is\"><u>Scott Alexander \u2013 Nobody Is Perfect, Everything Is Commensurable - EA Forum (effectivealtruism.org)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/pnsK8xFwF5nGZFfwT/snapshot-of-a-career-choice-10-years-ago\"><u>Snapshot of a career choice 10 years ago - EA Forum (effectivealtruism.org)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/ptFkbqksdPRNyzNBB/can-i-have-impact-if-i-m-average\"><u>Can I have impact if I\u2019m average? - EA Forum (effectivealtruism.org)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/9a7xMXoSiQs3EYPA2/the-history-of-the-term-effective-altruism\"><u>The history of the term 'effective altruism' - EA Forum</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/qv5dRAGkTjxs9vrv9/small-and-vulnerable\"><u>Small and Vulnerable - EA Forum (effectivealtruism.org)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/shzSEEDywdh2PPPMy/why-i-love-effective-altruism\"><u>Why I love effective altruism - EA Forum</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/4fPxQjq6GFZgurSsf/room-for-other-things-how-to-adjust-if-ea-seems-overwhelming\"><u>Room for Other Things: How to adjust if EA seems overwhelming - EA Forum (effectivealtruism.org)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/DYr7kBpMpmbygBiEq/the-privilege-of-earning-to-give\"><u>The Privilege of Earning To Give - EA Forum (effectivealtruism.org)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/zu28unKfTHoxRWpGn/you-have-more-than-one-goal-and-that-s-fine\"><u>You have more than one goal, and that's fine - EA Forum (effectivealtruism.org)</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/r94xv9Rs8ypPyr26s/unsurprising-things-about-the-ea-movement-that-surprised-me\"><u>Unsurprising things about the EA movement that surprised me - EA Forum (effectivealtruism.org)</u></a></li></ul>", "user": {"username": "Gemma Paterson"}}, {"_id": "b2Nnf9SniZ9bAsxf5", "title": "Negative utilitarianism charities", "postedAt": "2023-05-12T06:13:56.505Z", "htmlBody": "<p>What charitable foundations would you recommend, with the goal of minimizing suffering? fighting malaria does not make sens, because it only prolongs existence\n<a href=\"https://forum.effectivealtruism.org/posts/LAuQXwnykrMC73CvW/morality-for-the-radical-pessimist\">https://forum.effectivealtruism.org/posts/LAuQXwnykrMC73CvW/morality-for-the-radical-pessimist</a> I'm basically repeating this topic because I didn't get an answer</p>\n", "user": {"username": "Anon777"}}, {"_id": "85pJEjQu9aF49CScs", "title": "Our Progress in 2022 and Plans for 2023", "postedAt": "2023-05-12T03:06:59.380Z", "htmlBody": "<p>2022 was a big year for Open Philanthropy:</p><ul><li>We recommended over $650 million in grants \u2014 more, by far, than in any other year of our history. [<a href=\"https://forum.effectivealtruism.org/posts/85pJEjQu9aF49CScs/our-progress-in-2022-and-plans-for-2023#Continued_grantmaking\">More</a>]</li><li>We hired our first program officers for three new focus areas in our Global Health and Wellbeing portfolio. [<a href=\"https://forum.effectivealtruism.org/posts/85pJEjQu9aF49CScs/our-progress-in-2022-and-plans-for-2023#New_focus_areas\">More</a>]</li><li>Within our Longtermism portfolio, we significantly expanded our grantmaking and used a series of open calls to identify hundreds of promising grants to individuals and small projects. [<a href=\"https://forum.effectivealtruism.org/posts/85pJEjQu9aF49CScs/our-progress-in-2022-and-plans-for-2023#Grantmaking_through_open_calls\">More</a>]</li><li>We ran the <a href=\"https://www.regrantingchallenge.org/\">Regranting Challenge</a>, a novel experiment which allocated $150 million to outstanding programs at other grantmaking organizations. [<a href=\"https://forum.effectivealtruism.org/posts/85pJEjQu9aF49CScs/our-progress-in-2022-and-plans-for-2023#Grantmaking_through_open_calls\">More</a>]</li><li>We nearly doubled the size of our team. [<a href=\"https://forum.effectivealtruism.org/posts/85pJEjQu9aF49CScs/our-progress-in-2022-and-plans-for-2023#Hiring_updates\">More</a>]</li></ul><p>This post compares our progress with the <a href=\"https://www.openphilanthropy.org/research/our-progress-in-2021-and-plans-for-2022/\">goals we set forth a year ago</a>, and lays out our plans for the coming year, including:</p><ul><li>A significant update on how we handle allocating our grantmaking across causes. [<a href=\"https://forum.effectivealtruism.org/posts/85pJEjQu9aF49CScs/our-progress-in-2022-and-plans-for-2023#Revisiting_funding_allocation_between_portfolios_\">More</a>]</li><li>A potential leadership transition. [<a href=\"https://forum.effectivealtruism.org/posts/85pJEjQu9aF49CScs/our-progress-in-2022-and-plans-for-2023#Leadership_update\">More</a>]</li><li>Continued growth in grantmaking and staff. [<a href=\"https://forum.effectivealtruism.org/posts/85pJEjQu9aF49CScs/our-progress-in-2022-and-plans-for-2023#Hiring_plans\">More</a>]</li></ul><h2><strong>Continued grantmaking</strong></h2><p>Last year, we wrote:</p><blockquote><p>We aim to roughly double the amount [of funding] we recommend [in 2022] relative to [2021], and triple it by 2025.</p></blockquote><p>In 2022, we recommended over $650 million in grants (up from roughly $400 million in 2021).&nbsp;</p><p>We changed our plans midway through the year, due to a stock market decline<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuytt2jlbdeb\"><sup><a href=\"#fnuytt2jlbdeb\">[1]</a></sup></span>&nbsp;that reduced our available assets and led us to <a href=\"https://www.openphilanthropy.org/research/update-on-our-planned-allocation-to-givewells-recommended-charities-in-2022/#how-we-chose-this-years-allocation-to-givewell\">adjust the cost-effectiveness bar</a> we use for our spending on global health and wellbeing. When we wrote last year\u2019s post, we had <a href=\"https://www.openphilanthropy.org/research/2021-allocation-to-givewell-top-charities-why-were-giving-more-going-forward/#footnote1_li2s61n\">tentatively planned</a> to allocate $500 million to <a href=\"https://www.givewell.org/\">GiveWell\u2019s</a> recommended charities; the actual allocation wound up being <a href=\"https://www.openphilanthropy.org/research/update-on-our-planned-allocation-to-givewells-recommended-charities-in-2022/\">$350 million</a> (up from $300 million in 2021).</p><p>Currently, we expect to recommend over $700 million in grants in 2023, and no longer have a definite grantmaking goal for 2024 and 2025.</p><p><strong>Highlights from this year\u2019s grantmaking</strong></p><p>This section outlines some of the major grants we made across our program areas.</p><p><i>In grants to charities recommended by GiveWell:</i></p><ul><li><a href=\"https://www.openphilanthropy.org/grants/clinton-health-access-initiative-chai-incubator/\">$10.4 million to the Clinton Health Access Initiative</a> to support their Incubator program, which looks for cost-effective and scalable health interventions.</li><li><a href=\"https://www.openphilanthropy.org/grants/new-incentives-nigeria/\">$13.7 million to New Incentives</a> for conditional cash transfers to boost vaccination rates in Nigeria.</li><li><a href=\"https://www.openphilanthropy.org/grants/evidence-action-in-line-chlorination-in-malawi/\">$4.4 million to Evidence Action</a> to support their in-line chlorination program in Malawi.<ul><li>We also made a <a href=\"https://www.openphilanthropy.org/grants/evidence-action-dispensers-for-safe-water-program-february-2022/\">$48.8 million grant</a> to the same program with funds from our 2021 allocation.</li></ul></li><li>Many other grants we haven\u2019t listed here (see our <a href=\"https://www.openphilanthropy.org/grants/?q=&amp;focus-area%5B%5D=givewell-recommended-charities\">full list</a> of GiveWell-recommended grants).</li></ul><p><i>In potential risks from advanced AI:</i></p><ul><li><a href=\"https://www.openphilanthropy.org/grants/redwood-research-general-support-2/\">Redwood Research</a> to support their research on aligning AI systems.</li><li><a href=\"https://www.openphilanthropy.org/grants/center-for-a-new-american-security-work-on-ai-governance/\">Center for a New American Security</a> to support their work on AI policy and governance.</li><li><a href=\"https://www.openphilanthropy.org/grants/funding-for-ai-alignment-projects-working-with-deep-learning-systems/\">A number of projects</a> related to understanding and aligning deep learning systems.</li></ul><p><i>In biosecurity and pandemic preparedness:&nbsp;</i></p><ul><li><a href=\"https://www.openphilanthropy.org/grants/columbia-university-far-uvc-sterilization-research/\">Columbia University</a> to support research on far-UVC light to reduce airborne disease transmission.</li><li><a href=\"https://www.openphilanthropy.org/grants/bipartisan-commission-on-biodefense-general-support-2022/\">Bipartisan Commission on Biodefense</a> to support work on biodefense policy in the US.</li><li>The <a href=\"https://www.openphilanthropy.org/grants/johns-hopkins-center-for-health-security-educational-degree-program/\">Johns Hopkins Center for Health Security</a> to support their degree program for students pursuing careers in biosecurity.&nbsp;</li></ul><p><i>In effective altruism community growth (with a focus on longtermism):</i></p><ul><li>80,000 Hours (<a href=\"https://www.openphilanthropy.org/grants/80000-hours-marketing/\">marketing</a> and <a href=\"https://www.openphilanthropy.org/grants/80000-hours-general-support-2/\">general support</a>) for its work to help people have more impact with their careers.</li><li>Support for the<a href=\"https://www.openphilanthropy.org/grants/funding-for-effective-altruism-related-translation-projects/\"> translation of effective altruism-related content</a> into non-English languages.</li><li><a href=\"https://www.openphilanthropy.org/grants/bluedot-impact-general-support/\">Bluedot Impact</a> to run courses related to several of our priority cause areas.</li><li><a href=\"https://www.openphilanthropy.org/grants/effective-ventures-foundation-usa-asterisk-magazine/\">Asterisk</a> to publish a quarterly journal focused on topics related to effective altruism, among others.</li><li>A <a href=\"https://forum.effectivealtruism.org/posts/HPdWWetJbv4z8eJEe/open-phil-is-seeking-applications-from-grantees-impacted-by\">program</a> open to applications from grantees of the FTX Future Fund who were affected by the collapse of FTX in November 2022.&nbsp;</li></ul><p><i>In effective altruism community growth (with a focus on global health and wellbeing):</i></p><ul><li><a href=\"https://www.openphilanthropy.org/grants/effektiv-spenden-general-support/\">Effektiv Spenden</a> to support their work raising funds for highly effective charities from German speakers, and <a href=\"https://www.openphilanthropy.org/grants/ayuda-efectiva-general-support/\">Ayuda Efectiva</a> for similar work with Spanish speakers.</li><li><a href=\"https://www.openphilanthropy.org/grants/one-for-the-world-general-support-2023/\">One for the World</a> to support their outreach, focused on university campuses, aimed at encouraging people to pledge a fraction of their income to highly effective charities.</li></ul><p><i>In farm animal welfare:&nbsp;</i></p><ul><li><a href=\"https://www.openphilanthropy.org/grants/mercy-for-animals-corporate-campaigns/\">Mercy for Animals</a> to support corporate campaigns for broiler chicken welfare and cage-free reforms.&nbsp;</li><li><a href=\"https://www.openphilanthropy.org/grants/sinergia-animal-farm-animal-welfare-in-southeast-asia-and-latin-america/\">Sinergia</a> to support corporate campaigns for farm animal welfare in Southeast Asia and Latin America.</li><li><a href=\"https://www.openphilanthropy.org/grants/eurogroup-for-animals-farm-animal-welfare-in-europe/\">Eurogroup for Animals</a> for their work on farm animal welfare in Europe.</li><li>The <a href=\"https://www.openphilanthropy.org/grants/accountability-board-advocacy-campaigns/\">Accountability Board</a> for investor advocacy campaigns aimed at speeding up the implementation of corporate farm animal welfare pledges.</li></ul><p><i>In scientific research:&nbsp;</i></p><ul><li><a href=\"https://www.openphilanthropy.org/grants/university-of-georgia-malaria-vaccine-research/\">University of Georgia</a> and <a href=\"https://www.openphilanthropy.org/grants/yale-university-malaria-vaccine-research/\">Yale University</a> for a collaboration on malaria vaccine research.&nbsp;</li><li><a href=\"https://www.openphilanthropy.org/grants/translational-health-science-and-technology-institute-henipavirus-antivirals/\">Translational Health Science and Technology Institute</a> for their work on henipavirus antivirals.</li><li><a href=\"https://www.openphilanthropy.org/grants/university-of-california-berkeley-aging-research-irina-conboy-2023/\">University of California, Berkeley</a> to continue our support for Dr. Irina Conboy\u2019s research on the mechanisms of aging.</li><li><a href=\"https://www.openphilanthropy.org/grants/university-of-connecticut-health-center-syphilis-vaccine-development/\">University of Connecticut Health Center</a> and <a href=\"https://www.openphilanthropy.org/grants/flanders-institute-of-biotechnology-syphilis-vaccine-research/\">Flanders Institute</a> to support their collaboration with the University of Washington (which received a <a href=\"https://www.openphilanthropy.org/grants/university-of-washington-syphilis-vaccine-development-lorenzo-giacani/\">grant</a> in 2021) on a vaccine against syphilis.</li><li>Our scientific research team also joined with teams from the Bill &amp; Melinda Gates Foundation and the Novo Nordisk Foundation to launch <a href=\"https://padinitiative.com/\">Pandemic Antiviral Discovery</a>, which is an initiative aimed at catalyzing the discovery and early development of antiviral medicines to prepare for future pandemics.</li></ul><p><i>In global aid policy:</i></p><ul><li>One major line of grantmaking was aid policy and advocacy in Japan and Korea. This includes grants to youth advocacy incubator <a href=\"https://www.openphilanthropy.org/grants/polipoli-youth-outreach-for-global-health-aid/\">PoliPoli</a>, and to <a href=\"https://www.openphilanthropy.org/grants/malaria-no-more-malaria-funding-advocacy/\">Malaria No More</a> to expand their \u201cmalaria diplomacy\u201d work in Japan and Korea.</li><li>A second major line of grantmaking was support for USAID's Chief Economist's office, including to the <a href=\"https://www.openphilanthropy.org/grants/center-for-global-development-staff-for-usaid-chief-economist/\">Center for Global Development</a> and <a href=\"https://www.openphilanthropy.org/grants/international-rescue-committee-usaid-placement/\">International Rescue Committee</a> to place staff in that office.</li></ul><p><i>In South Asian air quality:</i></p><ul><li>A <a href=\"https://www.openphilanthropy.org/grants/indian-institute-of-technology-bombay-pavitra-pollution-modeling-tool/\">collaboration</a> between several institutions to support their work building a new tool to model air pollution in India.</li><li><a href=\"https://www.openphilanthropy.org/grants/gdi-partners-air-quality-governance-in-india/\">GDi Partners</a> to support their work on air quality governance among regional governments in India.</li></ul><h2><strong>New focus areas</strong></h2><p><a href=\"https://www.openphilanthropy.org/research/our-progress-in-2021-and-plans-for-2022/#new-focus-areas\">Last year</a>, we wrote:</p><blockquote><p>We also announced plans to launch another new focus area: <a href=\"https://www.openphilanthropy.org/research/new-grantmaking-program-supporting-the-effective-altruism-community-around-global-health-and-wellbeing/\">supporting the effective altruism community with a focus on global health and wellbeing</a>. We are still in the process of hiring a program officer to lead this area.</p><p>[\u2026]</p><p>This year, our global health and wellbeing cause prioritization team aims to launch three more new focus areas where we can find scalable opportunities above our bar, and to continue laying the groundwork for more growth in future years.</p></blockquote><p>Since then, we hired a program officer, <a href=\"https://www.openphilanthropy.org/research/incoming-program-officer-for-effective-altruism-community-building-global-health-and-wellbeing-james-snowden/\">James Snowden</a>, to lead our new focus area of <a href=\"https://www.openphilanthropy.org/ea-community-growth-global-health-and-wellbeing/\">EA Community Growth with a focus on global health and wellbeing</a>. James has already recommended <a href=\"https://www.openphilanthropy.org/grants/?q=&amp;focus-area%5B%5D=effective-altruism-community-growth-global-health-and-wellbeing\">several grants</a>.</p><p>We\u2019ve also hired grantmakers in two additional areas (compared to our expectation of three):</p><ul><li>Research Fellow <a href=\"https://www.openphilanthropy.org/about/team/matt-clancy/\">Matt Clancy</a> is leading research and grantmaking on Science and Innovation Policy to safely accelerate scientific progress and innovation.</li><li>Two new program officers \u2014 Katharine Collins and <a href=\"https://www.openphilanthropy.org/about/team/ray-kennedy/\">Ray Kennedy</a> \u2014 are working on another new program, Global Health Research and Development (GHR&amp;D).&nbsp;<ul><li>This program serves to expand on the work of our <a href=\"https://www.openphilanthropy.org/focus/scientific-research/\">Scientific Research</a> team by recruiting staff with more specialized experience to focus on diseases that disproportionately affect people in low- and middle-income countries (LMICs).&nbsp;</li><li>Historically, some of the science team\u2019s giving has been focused on these diseases \u2014 for example, work on <a href=\"https://www.openphilanthropy.org/grants/international-centre-of-insect-physiology-and-ecology-malaria-prevention-research-jeremy-herren/\">malaria</a> and <a href=\"https://www.openphilanthropy.org/grants/flanders-institute-of-biotechnology-syphilis-vaccine-research/\">syphilis</a> \u2013 while the rest has been focused on other areas of science that appear neglected or potentially transformative, but don\u2019t have the same LMIC focus \u2014 for example, <a href=\"https://www.openphilanthropy.org/grants/university-of-california-berkeley-aging-research-irina-conboy-2023/\">aging</a>.</li><li>The new GHR&amp;D team will work closely with our existing Scientific Research team and make grants to support scientific research, the development of new products, and efforts to make products more affordable and accessible.</li></ul></li></ul><h2><strong>Grantmaking through open calls</strong></h2><p><a href=\"https://www.openphilanthropy.org/research/our-progress-in-2021-and-plans-for-2022/#new-approaches-to-funding\">Last year</a>, we wrote:</p><blockquote><p>This year, we created a number of new programs to openly solicit funding requests from individuals, groups, and organizations. This represents a different approach from the proactive searching and networking we use to find most of our grants, and we are excited by the potential for these programs to unearth strong opportunities we wouldn\u2019t have found otherwise.</p><p>The largest such program is our <a href=\"https://www.openphilanthropy.org/focus/open-philanthropy-regranting-challenge\">Regranting Challenge</a>, which will allocate $150 million in funding to the grantmaking budgets for one to five outstanding programs at other foundations. That program is closed to new submissions, but we\u2019ve listed many programs that are open to submissions on our \u201c<a href=\"https://www.openphilanthropy.org/giving/how-to-apply-for-funding\">How to Apply for Funding</a>\u201d page.</p></blockquote><p>Since then, we\u2019ve chosen the <a href=\"https://www.regrantingchallenge.org/\">awardees</a> for our Regranting Challenge. We consider the Challenge a <a href=\"https://www.openphilanthropy.org/research/announcing-the-awardees-for-our-150m-regranting-challenge/\">success</a> \u2014 we allocated all funds to programs we estimate will meet or exceed our global health and wellbeing cost-effectiveness <a href=\"https://www.openphilanthropy.org/research/technical-updates-to-our-global-health-and-wellbeing-cause-prioritization-framework/\">bar</a>, and we received a larger, stronger, and broader set of applications than we expected.</p><p>We\u2019ve also continued issuing open calls for applications or proposals for funding on the longtermist side. These calls led to funding for <a href=\"https://www.openphilanthropy.org/grants/early-career-funding-for-improving-the-long-term-future-scholarship-support-2022\">scholarship support</a>, for recipients of our <a href=\"https://www.openphilanthropy.org/grants/university-organizer-fellowship\">University Organizer Fellowship</a>, for <a href=\"https://www.openphilanthropy.org/grants/funding-for-ai-alignment-projects-working-with-deep-learning-systems\">alignment projects</a> working with deep learning systems, for the <a href=\"https://www.openphilanthropy.org/open-philanthropy-course-development-grants\">development of university courses</a> related to Open Philanthropy\u2019s grantmaking, and for <a href=\"https://www.openphilanthropy.org/grants/funding-for-effective-altruism-related-translation-projects\">translation projects related to effective altruism</a> (this list is not exhaustive).</p><p>Several of our open calls are still in progress; you can view them on our \u201c<a href=\"https://www.openphilanthropy.org/how-to-apply-for-funding/\">How to Apply for Funding</a>\u201d page.</p><h2><strong>Research and cause prioritization</strong></h2><p>Last year, we ran our <a href=\"https://www.openphilanthropy.org/research/cause-exploration-prizes/\">Cause Exploration Prizes</a>, and <a href=\"https://www.openphilanthropy.org/research/cause-exploration-prizes-announcing-our-prizes/\">awarded prizes</a> to more than 150 applicants who wrote on topics relevant to cause prioritization at Open Philanthropy.</p><p>Our <a href=\"https://www.openphilanthropy.org/our-global-health-and-wellbeing-and-longtermism-grantmaking-portfolios\">Global Health and Wellbeing</a> team also continued its work on cause prioritization research to identify highly impactful causes within the Global Health and Wellbeing portfolio. We published several examples of this work:</p><ul><li>Lauren Gilbert\u2019s shallow cause investigations on <a href=\"https://www.openphilanthropy.org/research/telecommunications-in-lmics/\">telecommunications in LMICs</a> and <a href=\"https://www.openphilanthropy.org/research/civil-conflict-reduction/\">civil conflict reduction.</a></li><li>Helen Kissel\u2019s shallow cause investigation on <a href=\"https://www.openphilanthropy.org/research/tobacco-control/\">tobacco control.</a></li><li>Tom Davidson\u2019s research on <a href=\"https://www.openphilanthropy.org/research/report-on-social-returns-to-productivity-growth/\">social returns to productivity growth.</a></li></ul><p>We\u2019ve published many other pieces this year about our research and grantmaking, including work published by Open Philanthropy staff on the Effective Altruism Forum and elsewhere:</p><ul><li>David Roodman on <a href=\"https://www.openphilanthropy.org/research/does-putting-kids-in-school-now-put-money-in-their-pockets-later-revisiting-a-natural-experiment-in-indonesia/\">returns to education in Indonesia.</a></li><li>Tom Davidson on <a href=\"https://forum.effectivealtruism.org/posts/3vDarp6adLPBTux5g/what-a-compute-centric-framework-says-about-ai-takeoff\">modeling AI takeoff speeds using a compute-centric framework.</a></li><li>James Snowden on <a href=\"https://forum.effectivealtruism.org/posts/gr4epkwe5WoYJXF32/why-i-don-t-agree-with-hli-s-estimate-of-household\">the spillover effects of mental healthcare.</a></li><li>Lauren Gilbert on <a href=\"https://worksinprogress.co/issue/every-generator-is-a-policy-failure\">electricity shortages in sub-Saharan Africa.</a></li><li>Javier Prieto on <a href=\"https://www.openphilanthropy.org/research/how-accurate-are-our-predictions/\">assessing Open Phil\u2019s internal forecasting.</a></li></ul><h2><strong>Other work</strong></h2><p>Open Philanthropy staff also produced many other public pieces of work this year:</p><ul><li>Ajeya Cotra was named a member of the <a href=\"https://www.vox.com/future-perfect/23365512/future-perfect-50-ajeya-cotra-senior-research-analyst-open-philanthropy\">Future Perfect 50</a>, spoke to the New York Times about AI progress in <a href=\"https://www.nytimes.com/2022/08/24/technology/ai-technology-progress.html\">August</a> and <a href=\"https://www.nytimes.com/2022/12/08/briefing/ai-chatgpt-openai.html\">December</a>, and appeared on the <a href=\"https://podcasts.apple.com/us/podcast/critiquing-effective-altruism-with-michael-nielsen/id1535406429?i=1000576650310\">Clearer Thinking Podcast</a>. She also co-founded a new blog, <a href=\"https://www.planned-obsolescence.org/\">Planned Obsolescence</a>.</li><li>Lewis Bollard appeared on the <a href=\"https://podcasts.apple.com/us/podcast/reducing-animal-suffering-ft-lewis-bollard/id1390384827?i=1000577330745\">Neoliberal Podcast</a>.</li><li>Matt Clancy appeared on <a href=\"https://podcasts.apple.com/in/podcast/70-the-economics-of-innovation-with-matt-clancy/id1525164585?i=1000543359038\">Narratives</a> and <a href=\"https://www.listennotes.com/top-podcasts/matt-clancy/\">several other podcasts</a>.</li><li>Holden Karnofsky appeared on several podcasts: <a href=\"https://www.dwarkeshpatel.com/p/holden-karnofsky#details\">Lunar Society</a>, <a href=\"https://shows.acast.com/62538867ad55de001281af36/episodes/holden-karnofsky-the-most-important-century-is-now-blimey\">Wild</a>, and <a href=\"https://podcasts.apple.com/us/podcast/can-effective-altruism-be-redeemed/id1081584611?i=1000596045174\">The Gray Area</a>.</li><li><a href=\"https://www.cold-takes.com/\">Holden</a>, <a href=\"https://mattsclancy.substack.com/\">Matt</a>, and <a href=\"https://joecarlsmith.com/archive\">Joe Carlsmith</a> continued to publish content on their blogs. Luca Righetti continued to co-host the <a href=\"https://hearthisidea.com/\">Hear This Idea</a> podcast.</li></ul><h2><strong>Hiring updates</strong></h2><p><a href=\"https://www.openphilanthropy.org/research/our-progress-in-2021-and-plans-for-2022/#hiring-and-other-capacity-building\">Last year</a>, we wrote:</p><blockquote><p>As we scale up our grantmaking, we\u2019ll need to grow our staff to match. Accordingly, <a href=\"https://forum.effectivealtruism.org/posts/FoBQxB7hjreoW9iFJ/open-philanthropy-is-hiring-more-people-than-ever-this-year\">we plan to hire more than 30 people this year</a>, and over 100 people in the next four years.</p><p>This represents massive growth compared to past years, which is an exciting opportunity and an immense challenge.</p></blockquote><p>Since this update, we\u2019ve hired 38 more staff members! We won\u2019t list them all here, but you can see them on our <a href=\"https://www.openphilanthropy.org/about/team\">team page</a>.</p><h2><strong>Plans for 2023</strong></h2><p>This section outlines some of what Open Philanthropy is working on in 2023. This is far from a complete list \u2014 expect more updates on our blog throughout the year!</p><p><strong>Revisiting funding allocation between portfolios&nbsp;</strong></p><p>This year, Open Philanthropy is revisiting the way we allocate funding between our <a href=\"https://www.openphilanthropy.org/our-global-health-and-wellbeing-and-longtermism-grantmaking-portfolios/\">longtermist and global health and wellbeing (GHW) portfolios</a>.&nbsp;</p><p>Historically, our longtermist spending was limited mostly by the size and number of opportunities for productive grantmaking.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmqykmtj8b2\"><sup><a href=\"#fnmqykmtj8b2\">[2]</a></sup></span>&nbsp;Over the past few years, our longtermist grantmaking areas have matured to the point that we believe they can productively absorb more spending than before (which prompted growth in our longtermist spending in 2022). At the same time, our available assets lost almost half their value over the course of 2022, which makes tradeoffs much more direct. In the fall of 2022, we decided we should revisit the important question of how best to allocate resources across our portfolios.&nbsp;</p><p>Over the course of this year, we aim to revisit our historical allocation across worldviews and decide whether and how to change it going forward. We expect to end 2023 with an updated \u201chouse view\u201d on how best to allocate assets across worldviews in the near term, as well as a plan for how we should review and update that allocation over time.</p><p><strong>Leadership update</strong></p><p>My Co-CEO Holden Karnofsky started a <a href=\"https://forum.effectivealtruism.org/posts/aJwcgm2nqiZu6zq2S/taking-a-leave-of-absence-from-open-philanthropy-to-work-on\">leave of absence</a> from Open Philanthropy on March 8 to explore working directly on AI safety. He is initially working on possible safety standards that could help prevent dangerous AI systems from being deployed.</p><p>We\u2019ve been preparing for this transition for quite some time. From Holden\u2019s announcement:</p><blockquote><p>Alexander, Cari, Dustin and I have been actively discussing the path to Open Philanthropy running without me since 2018. Our <a href=\"https://www.openphilanthropy.org/research/open-philanthropys-new-co-ceo/\">mid-2021 promotion of Alexander to co-CEO</a> was a major step in this direction (putting him in charge of more than half of the organization\u2019s employees and giving), and this is another step, which we\u2019ve been discussing and preparing for for over a year.</p></blockquote><p>Holden plans to be on leave for at least three months, and he may transition to working on AI safety full-time after that, in which case he may leave OP to start or join another organization. While he is on leave, I am serving as the sole CEO of Open Philanthropy.</p><p><strong>Hiring plans</strong></p><p>Open Philanthropy is still growing; we expect to hire more than 20 additional staff over the next year.&nbsp; If you want to join our team, check out the open positions on our <a href=\"https://www.openphilanthropy.org/careers/\">careers page</a>. (At the time of publishing, we're looking for an <a href=\"https://jobs.ashbyhq.com/openphilanthropy/92f13cd6-0e90-4ad7-9308-bd854ef574ed\">In-House Counsel</a>, a <a href=\"https://jobs.ashbyhq.com/openphilanthropy/14abe760-2272-414c-9abc-c9d9810c7c2e\">People Operations Assistant</a>, and a <a href=\"https://jobs.ashbyhq.com/openphilanthropy/e3a4e593-f8cd-4ae3-ab51-4f710222b9cc\">Research Analyst</a> and an <a href=\"https://jobs.ashbyhq.com/openphilanthropy/ba9ee7f3-cd3a-456d-bfef-3c579f8155de\">Operations Associate</a> for our biosecurity team). If you don\u2019t see something you want to apply for, you can fill out our <a href=\"https://jobs.ashbyhq.com/openphilanthropy/2cc48fa8-97aa-47d8-b367-bf3b66cdba3f\">General Application</a>, and we\u2019ll reach out if we post a position we think might be a good fit.</p><p>Finally, we\u2019re always looking for referrals. If you refer someone and we hire them, <a href=\"https://openphilanthropy.slab.com/public/posts/open-philanthropy-external-referral-reward-guide-sxv5p3ey\">we\u2019ll pay you $5,000.</a></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuytt2jlbdeb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuytt2jlbdeb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This just reflects a decline in the market; our main donors are still planning to give away virtually all of their wealth within their lifetimes.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmqykmtj8b2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmqykmtj8b2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Through 2022, roughly 70% of our total funding went toward areas in our GHW portfolio, and 30% went toward areas in our longtermist portfolio.</p></div></li></ol>", "user": {"username": "Open Philanthropy"}}, {"_id": "HssFFgW67ujd3ZaFs", "title": "Simple charitable donation app idea", "postedAt": "2023-05-12T02:11:01.250Z", "htmlBody": "<p>I'll pay $10xN to the people who build this app, where N is the total karma of this post three months from now, up to a max of $20,000, unless something shady happens like some sort of bot farm. If it turns out this app already exists, I'll pay $1xN instead to the people who find it for me. I'm open to paying significantly more in both cases if I'm convinced of the altruistic case for this app existing; this is just the minimum I personally can commit to and afford. <strong>EDIT: What I'm doing here is trying to gauge the popularity of this would-be app; it's not worth funding unless a lot of people would actually use it. Maybe I shouldn't have used karma as a proxy for this, because it feels like vote brigading. So, going forward, let's just say I'm interested in funding this app if there's evidence that lots of people would use it. Speak up in the comments if you'd use it yourself and/or if you'd be willing to help fund it and/or if you'd be willing to help build it.</strong><br><br>The app consists of a gigantic, full-screen button such that if you press it, the phone will vibrate and play a little satisfying \"ching\" sound and light up sparkles around where your finger hit, and then $1 will be donated to GiveDirectly. You can keep slamming that button as much as you like to thereby donate as many dollars as you like.<br><br>In the corner there's a menu button that lets you change from GiveDirectly to Humane League or AMF or whatever (you can go into the settings and input the details for a charity of your choice, adding it to your personal menu of charity options, and then toggle between options as you see fit. You can also set up a \"Donate $X per button press instead of $1\" option and a \"Split each donation between the following N charities\" option.)<br><br>That's it really.<br><br>Why is this a good idea? Well, I'm not completely confident it is, and part of why I'm posting is to get feedback. But here's my thinking:<br><br>I often feel guilty for eating out at restaurants. Especially when meat is involved. Currently I donate a substantial amount to charity on a yearly basis (aiming for 10% of income, though I'm not doing a great job of tracking that) but it feels like a chore, I have to remember to do it and then log on and wire the funds. Like paying a bill.<br><br>If I had this app, I think I'd experiment with the following policy instead: Every time I buy something not-necessary such as a meal at a restaurant, I whip out my phone, pull up the app, and slam that button N times where N is the number of dollars my purchase cost. Thus my personal spending would be matched with my donations. I think I'd feel pretty good while doing so, it would give me a rush of warm fuzzies instead of feeling like a chore. (For this reason I suggest having to press the button N times, instead of building the app to use a text-box-and-number-pad.)<br><br>Then I'd check in every year or so to see whether my donations were meeting the 10% goal and make a bulk donation to make up the difference if not. If it exceeds the goal, great!<br><br>I think even if no one saw me use this app, I'd still use it &amp; pay for it. But there's a bonus effect having to do with the social consequences of being seen using it. Kinda like how a big part of why veganism is effective is that you can't hide it from anyone, you are forced to bring it up constantly. Using this app would hopefully have a similar effect -- if you were following a policy similar to the one I described, people would notice you tapping your phone at restaurants and ask you what you were doing &amp; you'd explain and maybe they'd be inspired and do something similar themselves. (Come to think of it, it's important that the \"ching\" sound not be loud and obnoxious, otherwise it might come across as ostentatious.) I can imagine a world where this app becomes really popular, at least among certain demographics, similar to (though probably not as successful as) veganism.<br><br>Another mild bonus is that this app could double as a tracker for your discretionary spending. You can go into the settings and see e.g. a graph of your donations over time, statistics on what time of day you do them, etc. and learn things like \"jesus do I really spend that much on dining out per month?\" and \"huh, I guess those Amazon purchases add up.\" Yes, there are plenty of other ways to track this sort of thing, but it's still a nice bonus.<br><br>Thoughts? Criticism? Anyone interested in building this? <a href=\"https://www.lesswrong.com/posts/cxuzALcmucCndYv4a/daniel-kokotajlo-s-shortform?commentId=JMCWrD8Lb3HmTZyJn\">\"awg\" from LW</a> is potentially interested in doing the front-end, but my sneaking suspicion is that the back-end would be the hard part (e.g. payment processing).&nbsp;<br>&nbsp;</p>", "user": {"username": "kokotajlod"}}, {"_id": "Lyn6u49moEP3oGd89", "title": "Do mask mandates work? \u2014 Do RCTs?", "postedAt": "2023-05-11T19:35:55.155Z", "htmlBody": "<p>A <a href=\"https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD006207.pub6/full\">2023 Cochrane review of 78 RCTs</a> found:</p><blockquote><p>The pooled results of RCTs did not show a clear reduction in respiratory viral infection with the use of medical/surgical masks.&nbsp;</p><p>There were no clear differences between the use of medical/surgical masks compared with N95/P2 respirators in healthcare workers when used in routine care to reduce respiratory viral infection.</p></blockquote><p>&nbsp;</p><p>&nbsp;</p><p>Scientific American refuted, <a href=\"https://www.scientificamerican.com/article/masks-work-distorting-science-to-dispute-the-evidence-doesnt/\">with this</a>:</p><blockquote><p>Placing randomized trials above other types of research such as observational, lab and modeling studies, has interfered with the COVID response. A randomized trial approach that allows a few studies <i>[78 is not a few?]</i> to cancel out a huge body of research from other disciplines has no basis in science.</p></blockquote><p>&nbsp;</p><p>&nbsp;</p><p><i>Other small notes:&nbsp;</i><br><i>- succinct general </i><a href=\"https://vinayprasadmdmph.substack.com/p/the-cochrane-review-on-masks-is-damning\"><i>write up</i></a><i>;&nbsp;</i><br><i>- </i><a href=\"https://sensiblemed.substack.com/p/how-does-cochrane-normally-handle\"><i>disagreement within Cochrane</i></a><i> about what their review means.</i></p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>+ How effective are mask mandates?&nbsp;<br><br>+ How effective are N95s (when used correctly)?&nbsp;<br><i>If they are effective, why do mandates seem to have done about nothing at all? - Or are these 78 RCTs not good evidence?</i><br><br>+ Why is it difficult for an ordinary person to figure this out, 3 years and a massive global effort later?</p>", "user": {"username": "gogreatergood"}}, {"_id": "78SgfrgcHc8iknacc", "title": "\"The mass extermination of animals is the greatest atrocity in human history\" by Wayne Hsiung", "postedAt": "2023-05-11T17:32:20.414Z", "htmlBody": "<p>Saving the majority of animals that are suffering and dying due to human-generated industries will also help humans.</p>", "user": {"username": "Chloe Leffakis"}}, {"_id": "qjGcsKg5qd4mi99jz", "title": "US Supreme Court Upholds Prop 12!", "postedAt": "2023-05-11T16:30:17.944Z", "htmlBody": "<p>The United States Supreme Court just released its decision on the country's most pivotal farmed animal welfare case\u2014<a href=\"https://www.supremecourt.gov/opinions/22pdf/21-468_5if6.pdf\">NATIONAL PORK PRODUCERS COUNCIL ET AL. v. ROSS, SECRETARY OF THE CALIFORNIA DEPARTMENT OF FOOD AND AGRICULTURE, ET AL. </a>\u2014upholding California's Prop 12, the strongest piece of farmed animal legislation in the US.<br><br>In 2018, California residents voted by ballot measure to ban the sale of pig products that come from producers that use gestation crates, individual crates the size of an adult pig's body that mother pigs are confined to 24/7 for the full gestation of their pregnancies, unable to turn around. In response, the pork industry sued and the case made its way to the nation's highest court.<br><br>If the Supreme Court had <i>not</i> upheld Prop 12, years of advocacy efforts would have been nullified and advocates would no longer be able to pursue state-level legislative interventions that improve welfare by banning the sale of particularly cruelly produced animal products. It would have been a tremendous setback for the US animal welfare movement. Instead, today is a huge victory.</p><p>Groups like HSUS spearheaded efforts to uphold Prop 12, even in the face of massive opposition. The case exemplified the extent to which even left-leaning politicians side with animal industry over animal welfare, as even the Biden administration <a href=\"https://archive.is/sCclP\">sided with the pork industry</a>.</p><p>Today is a monumental moment for farmed animal advocacy. Congratulations to everyone who worked to make this happen!</p><p>Read more about it:</p><ul><li>Summary and analysis from Lewis Bollard (Senior Program Officer for Farm Animal Welfare at Open Phil) <a href=\"https://twitter.com/Lewis_Bollard/status/1656670192901185537\">here on Twitter</a>.</li><li>Victory announcement by the Humane Society of the United States <a href=\"https://www.humanesociety.org/news/victory-supreme-court-upholds-california-proposition-12\">here</a>.</li><li>New York Times coverage <a href=\"https://www.nytimes.com/2023/05/11/us/supreme-court-california-pigs.html\">here</a>.</li></ul>", "user": {"username": "Rockwell Schwartz"}}, {"_id": "ixua7wT7ZwuGfSLLi", "title": "A request to keep pessimistic AI posts actionable.", "postedAt": "2023-05-11T15:35:10.225Z", "htmlBody": "<p>There's a psychological phenomenon that I can't remember the name of, but essentially, and subconsciously, a person tries to make others around them feel stressed and anxious in order to mitigate their own stress and anxiety.</p><p>I see a lot of this in mainstream climate change reporting, and I'm starting to notice it more on here with regards to AI x-risk.</p><p>Basically, I find seeing posts with titles like \"We're All Gonna Die with Eliezer Yudkowsky\" extremely tough emotionally, and they make me use the forum less. I suspect I am not the only one.</p><p>Obviously talking about significant x-risks is going to be stressful. I do not support people self-censoring when trying to provide realistic appraisals of our current situation; that seems clearly counter-productive. I also understand that the stressful nature of dealing with x-risk means that some people will find it too mentally tough to contribute.</p><p>At the same time, there are emotional wins to be had, and avoiding the psychological phenomenon I mentioned at the start seems like one of them. I think a decent heuristic for doing so is asking 'what action am I asking readers to take as a result of this information', and making sure you have a good answer.</p><p>Sticking with the Eliezer theme, his letter to <a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\">Time</a> performs well on this metric: emotionally harrowing, but with a clear call to support certain political initiatives.</p><p>In summary: AI x-risk is emotionally tough enough already, and I think some effort to avoid unnecessarily amplifying that difficulty is a valuable use of forum authors' time. I would certainly appreciate it as a user!</p>", "user": {"username": "tcelferact"}}, {"_id": "ZDo6XjmivLKGKycdw", "title": "Fatebook for Slack: Track your forecasts, right where your team works", "postedAt": "2023-05-11T12:58:52.790Z", "htmlBody": "<p>Announcing <a href=\"https://fatebook.io/for-slack\">Fatebook for Slack</a> - a Slack bot designed to help high-impact orgs build a culture of forecasting.</p><p>With Fatebook, you can ask a forecasting question in your team's Slack:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZDo6XjmivLKGKycdw/lvlljaccgxz8t8rylrsw\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZDo6XjmivLKGKycdw/convwyw89l4mzrzlo6sv 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZDo6XjmivLKGKycdw/rrwzcurugsm9xflbehce 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZDo6XjmivLKGKycdw/qtfsfzbp6fgvntrnxmnd 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZDo6XjmivLKGKycdw/nynm48i4wiayrv4w5025 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZDo6XjmivLKGKycdw/zeuqkvl1h5qjierxl2gc 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZDo6XjmivLKGKycdw/ozhnvdbew1gkb229y0pi 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZDo6XjmivLKGKycdw/bdblhtimhmpfzimmxejj 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZDo6XjmivLKGKycdw/hpjzqszde6xzeeze8myx 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZDo6XjmivLKGKycdw/lyam9d3fuptm9xgnzugd 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZDo6XjmivLKGKycdw/xckb9i5jwn9vtczlgkzx 1070w\"></figure><p>Then, everyone in the channel can forecast:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZDo6XjmivLKGKycdw/jngq2jso56jnzkail6q1\"></figure><p>When it's time to resolve the question as Yes, No or Ambiguous, the author gets a reminder. Then everyone gets a Brier score, based on their accuracy.</p><p>It's like a tiny, private, fast Metaculus inside your team's Slack.</p><p>&nbsp;</p><h2>Why build a culture of forecasting?</h2><ul><li>Make better decisions</li><li>Communicate more clearly</li><li>Build your track record</li><li>Trust your most reliable forecasters</li></ul><p>We built Fatebook for Slack aiming to help high-impact orgs become more effective.</p><p>See the <a href=\"https://fatebook.io/for-slack\">FAQs</a> on the website for more info. We'd really value your feedback in the comments, in our&nbsp;<a href=\"https://discord.gg/mt9YVB8VDE\"><u>Discord</u></a>, or at&nbsp;<a href=\"mailto:adam@sage-future.org\"><u>adam@sage-future.org</u></a>.</p><p>&nbsp;</p><p><strong>You can </strong><a href=\"https://fatebook.io/for-slack\"><strong>add Fatebook to your workspace here</strong></a><strong>.</strong></p><p>&nbsp;</p><p><i>Thanks to all our alpha testers for their valuable feedback, especially the teams at 80,000 Hours, Lightcone, EA Cambridge, and Samotsvety.</i></p>", "user": {"username": "Adam Binks"}}, {"_id": "ConFiY9cRmg37fs2p", "title": "US public opinion of AI policy and risk", "postedAt": "2023-05-12T13:22:37.945Z", "htmlBody": "<h1>Summary</h1><p>On April 14th 2023, Rethink Priorities conducted an online poll to assess US public perceptions of, and opinions about, AI risk. The poll was intended to conceptually replicate and extend a recent AI-related poll from&nbsp;<a href=\"https://docs.cdn.yougov.com/bfoyxp7p28/results_AI%20and%20the%20End%20of%20Humanity.pdf\"><u>YouGov</u></a>, as well as drawing inspiration from some other recent AI polls from&nbsp;<a href=\"https://www.monmouth.edu/polling-institute/documents/monmouthpoll_us_021523.pdf/\"><u>Monmouth University</u></a> and&nbsp;<a href=\"https://www.mitre.org/news-insights/news-release/mitre-harris-poll-finds-lack-trust-among-americans-ai-technology\"><u>Harris-MITRE</u></a>.&nbsp;</p><p>The poll covered opinions regarding:</p><ol><li>A pause on certain kinds of AI research</li><li>Should AI be regulated (akin to the FDA)?</li><li>Worry about negative effects of AI</li><li>Extinction risk in 10 and 50 years</li><li>Likelihood of achieving greater than human level intelligence</li><li>Perceived most likely existential threats</li><li>Expected harm vs. good from AI</li></ol><p>Our population estimates reflect the responses of 2444 US adults, poststratified to be representative of the US population. See the&nbsp;<strong>Methodology&nbsp;</strong>section of the Appendix for more information on sampling and estimation procedures.</p><p><strong>Key findings</strong></p><p>For each key finding below, more granular response categories are presented in the main text, along with demographic breakdowns of interest.</p><ol><li><strong>Pause on AI Research.&nbsp;</strong>Support for a pause on AI research outstrips opposition. We estimate that 51% of the population would support, 25% would oppose, 20% remain neutral, and 4% don\u2019t know (compared to 58-61% support and 19-23% opposition across different framings in YouGov\u2019s polls). Hence, support is robust across different framings and surveys. The slightly lower level of support in our survey may be explained by our somewhat more neutral framing.</li><li><strong>Should AI be regulated (akin to the FDA)?</strong> Many more people think AI should be regulated than think it should not be. We estimate that 70% believe&nbsp;<i>Yes</i>, 21% believe&nbsp;<i>No</i>, and 9% don\u2019t know.&nbsp;</li><li><strong>Worry about negative effects of AI.</strong> Worry in everyday life about the negative effects of AI appears to be quite low. We estimate 72% of US adults worry little or not at all about AI, 21% report a fair amount of worry, and less than 10% worry a lot or more.</li><li><strong>Extinction risk in 10 and 50 years.</strong> Expectation of extinction from AI is relatively low in the next 10 years but increases in the 50 year time horizon. We estimate 9% think AI-caused extinction to be moderately likely or more in the next 10 years, and 22% think this in the next 50 years.</li><li><strong>Likelihood of achieving greater than human level intelligence.</strong> Most people think AI will ultimately become more intelligent than people. We estimate 67% think this moderately likely or more, 40% highly likely or more, and only 15% think it is not at all likely.</li><li><strong>Perceived most likely existential threats.</strong> AI ranks low among other perceived existential threats to humanity. AI ranked below all 4 other specific existential threats we asked about, with an estimated 4% thinking it the most likely cause of human extinction. For reference, the most likely cause, nuclear war, is estimated to be selected by 42% of people. The other least likely cause - a pandemic - is expected to be picked by 8% of the population.</li><li><strong>Expected harm vs. good from AI.</strong> Despite perceived risks, people tend to anticipate more benefits than harms from AI. We estimate that 48% expect more good than harm, 31% more harm than good, 19% expecting an even balance, and 2% reporting no opinion.</li></ol><p>The estimates from this poll may inform policy making and advocacy efforts regarding AI risk mitigation. The findings suggest an attitude of caution from the public, with substantially greater support than opposition to measures that are intended to curb the evolution of certain types of AI, as well as for regulation of AI. However, concerns over AI do not yet appear to feature especially prominently in public perception of the existential risk landscape: people report worrying about it only a little, and rarely picked it as a top existential threat.</p><p>Extrapolating from these findings, we might expect the US public to be broadly receptive to efforts aimed towards mitigating perceived risks of AI, for example through well-designed government regulation, or efforts to prevent risky arms-race type behavior from companies competing to develop AI.</p><p>We view these results as preliminary and our questions intentionally broadly replicated those asked in previous surveys in order to test the robustness of these earlier surveys to different framings. That said, as this topic is complicated and likely novel to most respondents, we think there is significant work to be done to further understand people\u2019s views and to ensure that our questions are eliciting meaningful attitudes, rather than&nbsp;<a href=\"https://academic.oup.com/poq/article-abstract/44/2/198/1863851?redirectedFrom=fulltext\"><u>pseudo-opinions</u></a>. To this end, we have an ongoing project employing qualitative methodology to better understand how people think about these questions.</p><h1>Report</h1><h2>Support vs. opposition for the pause on certain types of AI research</h2><p>Our framing of this question largely mirrored that of a recent&nbsp;<a href=\"https://docs.cdn.yougov.com/bfoyxp7p28/results_AI%20and%20the%20End%20of%20Humanity.pdf\"><u>YouGov</u></a> poll which found 58-61% (depending on framing) would support and 19-23% would oppose a <a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/ \">pause on certain kinds of AI development</a>. However, to try to reduce possible demand effects, we only noted that \u2018some\u2019 technology leaders signed an open letter (vs. \u201c&gt;1000\u201d referenced in one YouGov framing), and provided a short piece of information about a countervailing perspective from \u2018other technology leaders\u2019. One of the three YouGov framings also included a short statement of opposition, with 60% support and 21% opposition.</p><p>Our estimate is that 51% of US adults would be supportive of a pause, whereas 25% would oppose such a pause. The spread of separate response options for this question and the exact question framing are shown in Figure 1.&nbsp;</p><p>Though our more neutral framing found slightly lower levels of support than YouGov, this suggests that there is considerably more support than opposition for the AI open letter among the US population, and that this is robust to moderately different framings of the issue.<br>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/hrep36uirbbe05y00l6j\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/hgckbhykn4px8hlsva6m 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/l810p9o30dkrznxeu7uf 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kja98pa0tvsir4ihl3ew 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/c6bfpgyd33w4v0d7iceu 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zxcwrk48pfdfk4k41a5q 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/hdouuhrttdqcj9q9huac 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/i7tudcg2zfebccueygu4 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/vdcaaft0hjxw16lxgfj2 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/sh340tcdzwpwidaccuqc 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/a3k9trpxwguhqmvnuqin 1200w\"></p><p>Looking at demographic breakdowns for this outcome, we note that respondents in the oldest age bracket appeared to be most supportive of such a pause, and also that men are expected to be less supportive than women. The difference between men and women is quite substantial, with about 1.5x more opposition among men than women. Nevertheless, across all subgroups we looked at we found that support outweighed opposition.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/xxge1fa6ft1s8gv7zp7n\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/tlzmcwmbdx0q9qejfefr 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bhturffvay5bimiyqjti 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/jca1btehsjngkpu2rkse 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/wrpsq7jvylbhxaypuwa6 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/lww853an2w3xvfgmtiyo 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/s4ti88zyalkp04zidewz 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/hqa587ehvosdownhfkax 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qhwck5jgojxzupoldfca 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/xpunl9euzit3l4324a01 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/c0rir0i17fnbrkowcgff 1600w\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/pxqlb63ok1vnpuw8j5ub\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/w5rhjzyq9nqpmkfsitsn 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/t30teiaofuo1qzadseoy 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/wto3sb2to5yhspokw5op 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/wvbzgerrejh82uvrekmf 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kvs3o4bb7aaoynd0gzph 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ryhfcswvd7grxy3iklj0 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/pewhsrhw56lxvjajo7k2 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/he6do48vo5lo9stlc0wo 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ajf2uftnmyewsa2qxcmw 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/vjp35td8fikqmgrrhwvr 1600w\"></p><h2>Views on regulation of AI</h2><p>As well as views of a specific proposal for pausing AI development, we asked respondents whether or not they thought AI should be regulated by a federal agency, similarly to how the Food and Drug Administration (FDA) regulates the approval of drugs and medical devices. A&nbsp;<a href=\"https://www.mitre.org/news-insights/news-release/mitre-harris-poll-finds-lack-trust-among-americans-ai-technology\"><u>Harris-MITRE poll</u></a> of 2050 US adults, in November 2022, estimated that 82% of US adults would support government regulation of AI. A more recent&nbsp;<a href=\"https://www.monmouth.edu/polling-institute/documents/monmouthpoll_us_021523.pdf/\"><u>Monmouth University poll</u></a> from January 2023 estimated 55% favor, and 41% oppose, the idea of having \u2018a federal agency regulate the use of artificial intelligence similar to how the FDA regulates the approval of drugs and medical devices\u2019. Using a very similar question framing, we estimate that a sizable majority of US adults would favor federal regulation of AI (70%), with 21% opposed.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/mqmda0inji0dea0oddte\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/tgv90xe46zwwgwoh3rte 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/edjgr86nhqoxrqdfjvvw 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qmuyx5me9t3thh7yxv6i 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qi8ivfpam1exn0wjqrf5 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/muj3axtfg8aiu0liwq41 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qxnqw88y20wv9r9szg8a 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/x7qywido9fom2khihrff 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kxwcub7p979eq3nk8nke 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/phfikns8wkpj1kujqunb 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/pcjprg1bufib9iu7lycy 1600w\"></p><p>Similarly to the question over pausing AI research, we found more females would support such federal regulation. As might be expected, we also estimate that those identifying with the Democratic party would be more favorable of regulation than those who identify as Republican or Independent/otherwise affiliated. Interestingly, we did not find shifts in support of a pause on AI research by political identity. This may be down to people considering factors such as a self-imposed pause (rather than government involvement), or also concerns over the specific form of \u2018FDA-like\u2019 regulatory approaches. In spite of shifts related to some such demographic features, we again found that support was sizable across demographic subgroups.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/n8r8pz4o7fvvl1rnswgf\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/lp1q1g8lkccg2umcsm70 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/wt3uvi3mpozhvdbkezkw 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/b90ufbkdyotqhkfcu8le 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ex5f7l095h24tvh1xm4v 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/trsggvgglyiassqbqc7k 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/lbc73mgip30e0xqldjxp 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/xymlo7npvtfuwknzrelp 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/jno8mqepptglsnvgw2ma 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/iooz03bonrffsgp6puu0 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/muxfbtzximhesl2ypglc 1600w\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/lgq7y4a4hs45z9zxffqq\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/i8joptplwo9xob7dbkum 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/iunbkjyxb616owtgighh 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/uxetapk8ky44wp9bhzio 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/n3jd0zovj36sm4oyiotr 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bjvmz5p5hpjebfemblyd 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/cthr28ci0h2gdr0icqkn 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/scu6ozci71wgd4n1i2ud 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/x7gd06rp1fhiwl7pm587 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qflf96arz4zveamvasvh 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ogh5wklencrsyaib8zx2 1600w\"></p><h2>Worry about AI</h2><p>The recent YouGov poll included a question regarding how concerned the respondents were about the possibility that AI might end the human race, finding 18% very concerned and 28% somewhat concerned. However, this question may have been interpreted by respondents in a number of different ways: how probable they thought the outcome was, how concerning they thought the outcome would be if it happened, literally how anxious they were about the possibility of the outcome, or some combination of these.</p><p>We wanted to instead get a relatively simple indication of how much people were actively worrying about AI, and a separate indication of their perceived likelihood of extinction risk. We asked respondents to indicate how much, in their daily lives, they worry about the negative effects of AI on their life and society more broadly. In a separate question, we asked about people\u2019s perceived likelihood of extinction caused by AI.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/eczhqjeut7hn6nr370p7\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ftfjulfzy3agh2jbldus 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/jerbbzafkwhqdk38z6c4 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qd3oxajgkp0612ybejsv 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/njdewnhagsobo3sobfrz 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zvwtpcmkeymu4zivdoad 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/klwimxhosn7iqas4rk9i 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/iv1o57na0xcpgxibthjy 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bybcxdqgvpivc0vdrsdj 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bvv5v3iiy2rf0kq7ia3r 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/mxhpghn7wzkvwr5xvucn 1600w\"></p><p>We estimate that the majority of US adults (72%) worry only a little or not at all, with 28% worrying a fair amount or more and less than 10% worrying \u2018a lot\u2019 or more. This might suggest that even if, when prompted, people express concern over certain aspects of AI and possible threats it poses to humanity broadly or to their jobs and the economy (as in the Monmouth poll), AI may not feature prominently among their daily worries. We think this can be important to consider, as when one reads that some sizable proportion of the population is very concerned about AI causing the end of humanity, one may imagine broad, active emotional engagement with this issue. Our findings suggest this might not be the case.</p><p>We found relatively little demographic variation in this outcome, although there was slightly greater endorsement for not worrying at all among those with at most high school education, as well as a slightly increasing share of people worrying a little vs. not at all with increasing income levels. People in these brackets may correspond to more highly educated and compensated individuals conducting \u2018knowledge work\u2019 and other white collar jobs that are expected to be most affected by near-term developments in AI.</p><p>Of course, even since our survey, there has been additional media coverage of AI risk. Our findings present a snapshot as of April 21st, but the landscape of concern may change substantially with current events or coverage of this issue.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bh4kyempkedljgzi26xb\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/yloc6vpeudxnqpjtkl63 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/oebuornyvbsyinumwt29 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ebyksj5ljt8qr5nhpe5p 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/nruftdfliuup5jha2g24 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/j7bi28qnyczzm1ohy5ru 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ynuzftjcb0jd8dzbxa3u 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/rsf9p7eqwpgbssclgrsg 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qox434dikzbeck5fclct 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/lfm59mg9zxbgvksro6t1 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/db7woavy7ylh9gx75e7f 1600w\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ai9iaaojsuw6bkpkpxxz\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/djl5romkxa0sgxc9pngd 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kbdffiq10dsib31drvae 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kswnvkjynfdulia2bbxk 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/cnafk1uzj7p4rpe47usm 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/iwcwxtgueafr6l1bq1v0 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zhym2ufoxrfznlabevs5 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/g6kqhisn7j5ixxad0h2l 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/o6s52l5kxwjhokzeajnx 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bffb3nupcqclg7obini0 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/g86ztdh8oriflkbjhk2h 1600w\"></p><h2>Expectation that AI might lead to human extinction</h2><p>We additionally asked respondents to indicate how probable they thought it was that AI would cause human extinction. Estimates from the recent YouGov poll suggested some perhaps surprisingly high estimates of the likelihood of extinction caused by AI: 17% reported it \u2018very likely\u2019 and a further 27% reported it \u2018somewhat likely\u2019. One thing to note is that the question was not time bound, meaning that respondents may have been considering the possibility of AI representing a serious threat in the very distant future. We asked respondents two versions of this question: one time bound to within the next 10 years, and one time bound to within the next 50 years and, in a later question, simply about what was likely to cause extinction at all.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/jko0qpadt5dvonoiiue0\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/pqdxgg0lc7w1raxndgah 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/rclv54qoodjikjhuay86 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/yooxrmdpnb5rgq1qscnn 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/p6tngmp4gbiqcn3bfzca 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/oohnaayxc7vxjdstczmd 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/r2brk9hnaj3xnfdmitem 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/blv4nrlhzpvt5lzthlf0 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/yqzvzibvwckedmmgcom7 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/e0xdgf98frvelirgmbf2 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/vtx8f4li3syjhbde6gnh 1600w\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/miq9zsfnoqmo8b9tfyux\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/oktxykjeoeuxer4ny7wg 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/eee5yrvrctl7veoyai7a 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/wbn6r44ec2a5zupl8bwn 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kn1nqklzfwixaix3lprb 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/irxymefz3dqsqtfhzgk8 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/nelntopowvr6qwgpmpcf 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/obk5zomgdapinghhowdq 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ne04rvrifgzirr9pbd77 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/el2lloznwbua258ko0x2 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ketwwmbjm3pnbgzutwx3 1600w\"></p><p>We estimated that the majority of US adults consider it either not at all likely or only slightly likely that AI would lead to human extinction within these timeframes, with the single most selected option being \u2018not at all likely\u2019. However, the anticipated risk of extinction from AI does increase when moving from the next 10 to the next 50 years. We estimate that only 9% of the population think extinction from AI to be moderately likely or more over the next 10 years. This increases to 22% for the next 50 years.&nbsp;</p><p>The sense that extinction from AI was at all likely decreased with increasing age of respondents.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/wapn3nzzm1hxe1mt9idc\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/cjbkxcoumjfcd8ui1kcs 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/dt49xtgtrhnclmccytfx 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/hgbcxidj0s53wyhdvogq 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ztddy9uvqmvpctiicoaq 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/hc9u8tqhhw8bp0bi9olt 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/iwgaimpgebyk5ov1fj31 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ylwkvys2m09l0geypnfp 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/k2ihqtcqus5k5lpyfmpk 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/y7ihawnvtq53itqfte3l 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/vj3m9snn0t4kqpglenwj 1600w\"></p><p>We additionally asked respondents to estimate what proportion of the US population they thought believed it at least moderately likely that AI would lead to human extinction. Assessing predictions about what other people think is one way in which it can be possible to&nbsp;<a href=\"https://doi.org/10.1016/j.electstud.2018.09.012\"><u>gauge the extent to which people may be misrepresenting their own views</u></a>. The idea is that even people who may not be willing to endorse an attitude for which they could incur criticism should be willing to honestly report what others' attitudes are. It seems possible that&nbsp;<a href=\"https://intelligence.org/2017/10/13/fire-alarm/\"><u>social desirability concerns could lead to underreporting</u></a> (e.g., if one fears appearing unhinged) <i>or&nbsp;</i>overreporting (e.g., if one fears appearing naive) of worries about AI.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/e6pzwsx8ywjkgvokqe6v\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/z39wqydbo5wtgl2jfdkz 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/b3seh5k2piewp5c4uqwx 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/m8oey4ms0srelopdbbzd 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zbsmd98siblnferhzoib 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qiicjlyl3ip8h3aojumu 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bz1z6l91oodni3yt9i6j 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/m5arsyrw6paaasyregk2 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/gfzub0vlhe7irydjci1k 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/llsr9sroyy75t7tuiqri 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/urbwe0iuxrttxwt5wtnl 1477w\"></p><p>We estimate that most people (64%) expected somewhere between 0%-20% of the US population to grant at least a moderate likelihood of extinction from AI in the next 10 years, with the single-most endorsed option being 0-10% of the population.</p><p>In comparison, our results suggested that 9% of the population think it moderately likely or more that AI will cause extinction within the next 10 years (with an error margin just crossing over into the 10-20% bracket). Hence, there is an approximate correspondence between our estimated population level of perceived AI extinction risk based on responses in this survey, and how much people estimate the population to believe in extinction risks.</p><p>We can make estimates at a more granular level by fitting a beta distribution to the binned responses, thereby generating estimates of actual percentages. When doing this, we estimate that the average expectation among US adults for how much of the population believes AI extinction within 10 years to be \u2018moderately likely or more\u2019 is 18%, with a median expectation of 13%. If taken at face value as a \u2018wisdom of the crowds\u2019 estimate, with reduced risk of social desirability bias, then these numbers might indicate that respondents\u2019 reported level of belief in AI extinction was slightly suppressed in their direct responses to the possibility of extinction. However, discrepancies between the direct estimates of how many people anticipate AI extinction, and how much they believe others expect this, could reflect a whole range of factors, not only social desirability. For example, people may simply be overestimating the expectations of others, or there may be more general methodological issues such as the somewhat unfamiliar nature of this type of population prediction question. We are unable to disambiguate these different possibilities with the present data. Future work could examine these explanations further by looking at the association between measures of social desirability and first and third person judgements about AI risk.</p><h2>Most likely causes of human extinction</h2><p>Considering extinction risk from AI relative to other possible causes may also be informative in terms of understanding public perception of AI risk, as well as other existential threats. In the recent YouGov poll, a range of causes were listed, and respondents had to rate how likely they thought each was to result in human extinction. Nuclear weapons were the top specific cause of concern and most likely cause of extinction. AI risk scored higher than alien invasions and infertility but lower than asteroid impacts. We simply had respondents pick the single most likely option among several possible causes of human extinction. Consistent with the YouGov findings, nuclear war ranked top among the choices, followed by climate change. AI risk was again outranked by Asteroid Impact.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/iuy6cfbh2u7oorl6b406\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/tnj7obapypo6c1jsryie 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/blsjov7wj9tjevhqzlqo 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/aegxzqv9xqqpnbdgzqr6 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/aoajxgvv3fm7bg9ofgie 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/twoex6ikhlto8qkconbs 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/i4xocxn7mvuoaeruvbmu 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/hzrldyojjdh5gnky80du 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ql1drwxiyrjk2noz9f7d 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/vgtfikridijubniwgd3u 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ncqr8ezvd5nxo1srlbnf 1600w\"></p><p>The clearest demographic trends were for political differences, as well as differences between male and female respondents. Specifically, Democrats were far more likely than Republicans to endorse climate change as a possible cause of extinction, with Republicans more likely to endorse nuclear war. Independent voters were in between. Some of the Republicans not endorsing climate change also seemed to shift into AI risk, with an estimated 3% of Democrats vs. 6% of Republicans ranking AI risks as the most likely cause of human extinction. Male vs. Female respondents showed a similar pattern of responses as Republicans vs. Democrats.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/alcafwmxgn0ownnlfeun\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/mkb3itz9dllj6l8rg1kh 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zshsezox4vkdatjpu5i0 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ynz5kstagvanrjlqiqtw 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ljeehcduw2aqfgcjwi1s 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/hfhpkvvfyjgpd1nu3ac0 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/gxppi8kw7thnnmp4olu9 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/jxnf2jjldonrkw2tmnsf 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/xsyod206lla5flyit6eq 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/beftxnysaficlx52qog1 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/datser0h706ounxzv4h2 1600w\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ypin9rxkotvisxrv5tnt\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/fvhyqkx70e80f4muodpo 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/mgmpye3jsfxtthhfsbzx 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zd6tagncblgs0ehrhnbd 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/jgefikwdnclmyqxtnmlx 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/j1g69s0v8c0jhxjugvmy 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/pnbel6ychlexwlxddquj 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kdjjivuqkxyljqm1zsim 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/daqsdcyfusnsaijuopph 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/g5kbu3fktg9t0yxfchee 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/uszl0te1yfolavl0pnwi 1600w\"></p><h2>Greater than human intelligence</h2><p>As many concerns over AI may depend on the extent to which people anticipate AI becoming competitive with human intelligence, we additionally asked respondents how likely they think it is that AI will ultimately become more intelligent than people.</p><p>We estimate that 67% of US adults think it moderately likely or more that AI will become more intelligent than people, with more than 40% of people thinking this outcome highly likely or more.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ftdgtmmq7ygjcgcko1yx\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/uz7aduspok4pwaowptst 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/tn0ubtxvqwcst72kjlol 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qapmx282yvqecwyu2qvr 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ak2kn2xdywfppwugpr8z 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/jlukvmtpgvgwrubuc8gy 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ftwdeskxeji36xvarcly 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/mush7el5ncszlwkrkfoi 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/dtzkwatiqpjjobe5uxfo 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qnybt6ccw6v5vqm0kxi6 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/dsvuj0hqkkug147buqpj 1600w\"></p><p>Older adults seemed more skeptical of this possibility than those in younger age brackets by a substantial margin. Additionally, female respondents were more skeptical than males. This is of interest given that females nevertheless favor a pause on AI development to a greater extent than males.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/tzrvkxg9hqjcpiclogct\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ophejdmvu7g6pehsgwak 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ihzfro8j5oa8tfeiq7h6 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/xsic9xckcngmcq3qdxn2 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/skeugn7wwtnod1ywvayn 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/m5mvwbs5wp6d95fivxem 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/lrfs8taiobicdmfsfurm 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/aqvqcmcz1sycb88bjsdp 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ycrx6yw3dxxbpj8cwr8f 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kow26ba0yuxpbjfusfsk 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kdzwhrcj1xpvsb7h0d3a 1600w\"></p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/wu0v72klkgywp0ef5iq6\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/gdl4miaw02jqc85baeua 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/nvxlui1356laiezsiy7g 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/up6otzt6kf7v3eypjvaa 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ruz8brfsl4j44xhrpdb3 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/lmp6cu7ontnmtjadarxh 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/oebzocbbzwjkqmy4ok2f 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/cwkooe2iyojxaypecc5x 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/blmugquepwj9giatfymb 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/dfrkhxgi7m3qq1hdkaht 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/umzsgaoy0julfkkirqjb 1600w\"></p><h2>Good vs. Harm from AI</h2><p>Finally, beyond catastrophic outcomes from AI, we were interested in public perceptions of the general good vs. harm that artificial intelligence might do. Our question was framed similarly to the recent Monmouth University poll, which estimated that just 9% of US adults expect more good than harm from developing artificial intelligence. An estimated 46% expected equal goods and harms, and 41% expected more harm than good. We expanded on the question\u2019s response options by allowing people to endorse more gradations of good and harm, which may have inflated estimates of equality.&nbsp;</p><p>In contrast to the Monmouth poll, we estimate just 19% of the population are neutral on this issue, and that 48% lean in the direction of more good than harm, with 31% expecting more harm than good. This is the most substantial deviation from previous polls that we have observed in these AI-related questions. It is not immediately clear what might be the cause of this discrepancy, although the Monmouth poll was conducted 2 months ago and also included fewer respondents than our poll (805). The Monmouth poll also described AI as the creation of \u2018computers that can think for themselves\u2019, which may be more conducive to imagining hostile or frightening agentic AI. If accurate, our findings indicate the US public is not as pessimistic about AI as some other polls might suggest. However, it is also plausible that this is dependent on exactly how AI is construed by the respondent.<br>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/eaufuqseep0ae13pbst9\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/j9jv4shozqt44gx47k3j 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/xnxedeh4r4ye8reexale 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kukcv4gm7xhlxiwklr08 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ym24hyc0mpj0006ftbn1 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/wqmc7vxwmdnvwp2nihry 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bys5m9ntcfkbu1koxovv 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/cc8oclxkht9babggyjpt 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/unwysnunozuhjtig5gfg 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/slr93pvtjuvdd0s3wtvr 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/clw8cubpcd7xk8d2hst6 1600w\"></p><p>Male respondents were more likely to have a positive expectation for AI than female respondents, and both Democrats and Republicans had more positive expectations than Independently affiliated respondents. Female respondents were clearly more negative and also more likely to endorse neutrality. For political affiliation, the difference between groups seemed largely due to Independents being more likely to pick the Neutral option than for them to have reliably negative expectations.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/k3vkv1ch9equea38kbsk\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/lok2axvtactcdku55upv 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/hlxjlrdktycn5hehacg2 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/pyotxt7q0jot8d8dpwfy 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/fofbw67h3bdpbeffxmlz 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/vshifxuxv0scltl1pzgy 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/g8z4mbyixh8mpr5i6mam 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/eguj5j1n5wwab7lecmzr 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/on0z3pgrmkd4cau9hq0l 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kj05djmkic50ojqki388 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/xuoydydhv3wvkqnckg0o 1600w\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ulbfwi42kaldvrbf7wm6\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/n9sewfguuklmm2csdgqd 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/h2x4lnyoip3sva1aiyae 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qlinpsgnywwuf1de2ric 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/pf151xvqcvadiomu8ekw 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/dkkmpviqsshmzxs0jwka 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zisehglmf02lhwmoeusa 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/xkqrgv2yv5vajes7cw2h 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zslsnyq6lvgkarnmkj9z 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bh9ksodf3bairvwmukbb 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/vz4rsyfigr2dwjymo9ma 1600w\"></p><h2>Associations among AI attitudes</h2><p>In addition to estimating public opinion, we also assessed some relationships of potential interest between the AI-related outcomes (see the Methodology section for a description of these models). For support of a pause on certain kinds of AI research, we found that higher expectations of harm, more worry, and greater expectations of extinction in the next 50 years from AI were positively associated with support for a pause. Those who reported a clear expectation that AI would do more good or much more good than harm were especially likely to oppose a pause.</p><p><br><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/b6acwfajiqxporzdsmnh\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/edxym5m7wpql2arsuqsi 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qqxwublzmovtyh1c1wnm 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/v5sfddrpbgzr7ocqlihn 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/yynmopkh0tkvoytwz01c 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/y39g0xvxaztc7dibhusi 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/mjkfcwzdrkdpurbn7cqy 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/t6oekjto6lygdnfty6wl 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/dlt79bznkcxedwdee8xu 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/b3a21ympnibeoshf7evq 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/wlv9dtashke0dak3z14m 1600w\"><br>With respect to believing that AI should be regulated in a manner similar to how the FDA oversees food and drugs, the expectation of AI doing more good than harm was again associated with heightened disagreement with regulation. Worry and the belief in extinction from AI tended again towards being positively associated with support for regulation, but these associations were not robust.</p><p>With respect to worry itself, we found that the anticipation of greater harm than good was strongly associated with worry - more so than both the expectation of extinction and the belief that AI would achieve superhuman intelligence. The specific belief that AI might cause human extinction in the next 50 years was in turn more strongly associated with worry than the belief that it would achieve superhuman intelligence. It seems plausible that general expectations of harm, such as job loss or broader societal impact may be more concrete and imaginable than the potentially abstract conception of human extinction, even among those who really believe this might happen. Hence, general conceptions of harm may be more likely to provoke worry in one\u2019s daily life than the anticipation of extinction. People may also feel they have more agency with respect to more \u2018mundane\u2019 negative effects such as job loss, resulting in more rumination as people worry about how they might adapt. An alternative explanation might also be that general Good vs. Harm simply functions as a catch-all measure for the general goodness/badness of AI, and therefore captures a much wider range of concerns than extinction expectations.<img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/dv46tenwnq1ooex5dtwv\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/iz27etntyp6qb32li3da 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/yhwzs1p5otvp0gxgwo19 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/umt81s70g1m0gfgwwj6s 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/owfddvp76o2o7ky8jw2j 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/seidqnbthro2uttfjcge 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/gs2srdott4lw7tuqx1mo 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/isjgxxlldtv8c3nsdlj2 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/m5xmoeevl1y0ddx3moor 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/whfwhfbeswtpf3t4zj6w 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zpt0pxtwtlyrw8ar4gex 1600w\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/gwsulz9pbdfi98x6wzig\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/mvpafffh2qcfjdhcu4sy 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zj2dfxwz36vlafdzp5q5 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qjbsnjciilv0vc7umzi1 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/c0h0x12jje92l2zcvvaf 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ingw8owteuefqqwjctks 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/vnconoh3nkerxbcxn8xp 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zaiftigx7nzscnkdotuw 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/lbbqghjpyb7fnod6vi2b 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zo5zklx1zjuasu6fmics 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/uhvwj1spshh4slrxpmrk 1600w\"></p><h2>Conclusions</h2><p>The estimates from this poll may inform policy making and advocacy efforts regarding AI risk mitigation. The findings broadly suggest an attitude of caution from the public, with substantially higher levels of support than opposition to measures that are intended to curb the evolution of certain types of AI (a possible pause of some kinds of AI development), as well as for regulation of AI.</p><p>However, concerns over AI do not yet appear to feature especially prominently in public perception of the existential risk landscape. Notably, extinction caused by AI was selected as the most substantial existential threat to humanity by only a small minority of people. In addition, it does not seem that risks from AI are something that most people are worrying about a lot in their daily lives (though note that we do not have a comparison for how much people report worrying about other issues). We are conducting additional qualitative research to better understand people\u2019s worries about, and their perceptions of risk from AI, which may further inform our understanding of AI risk perception.</p><p>US adults appear to appreciate that AI may well become more intelligent than people, and place non-negligible risk on the possibility that AI could cause extinction within the next 50 years. Nevertheless, people generally expect there to be more good than harm to come from AI.</p><p>Extrapolating from these findings, we might expect the US public to be broadly receptive to efforts aimed towards mitigating what are perceived as plausible and potentially highly concerning risks of AI, for example through well-designed government regulation, or efforts to prevent risky arms-race type behavior from companies competing to develop AI. However, there may be little mass appeal for what might be considered more extreme stances relative to where public perception and concern currently rests. This may be particularly the case given that people anticipate substantial good to come from AI, not just bad. Of course, this does not mean that such communication could not&nbsp;<i>shift&nbsp;</i>public opinion - we are describing where the US population appears to be at, and not suggesting where public opinion optimally should be, with respect to AI risk perceptions.</p><p>As AI risk represents a relatively new area of public discourse, we anticipate that current events and media discussion could still substantially shift public perception.</p><h1>Appendix</h1><h2>Methodology</h2><p>On April 21st 2023, Rethink Priorities conducted an online poll regarding public perceptions of AI risk, as well as attitudes towards regulation of AI development and support/opposition of the <a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/ \">recently proposed pause </a>on training/development of certain types of AI models.</p><p>The poll sampled 2523 US respondents aged 18 or above on the online sampling pool&nbsp;<i>Prolific,&nbsp;</i>of whom 2444 consented, answered questions, and passed requisite attention checks for the analyses presented below. We then used Multilevel Regression and Poststratification (MRP) to generate population-level estimates for US adult public opinion, accounting for Age, Sex, Race, Household Income, Education, Political party affiliation, as well as the US State/District and 2020 Republican vote share for the state.</p><p>MRP is a technique that can be used to estimate outcomes in a specific target population based upon a potentially unrepresentative sample population. In brief, the technique involves generating estimates of how a range of features (e.g., education, income, age) are associated with the outcome of interest from the sampled population, using multilevel regression. Based on the known distribution of combinations of these features in the target population, the poststratification step then involves making predictions from the multilevel regression model for the target population. This approach is widely used to make accurate predictions of population level opinion and voting based upon unrepresentative samples (e.g., Wang, W., Rothschild, D., Goel, S., &amp; Gelman, A. (2015). Forecasting elections with non-representative polls.&nbsp;<i>International Journal of Forecasting</i>,&nbsp;<i>31</i>(3), 980-991.), and also allows inferences to be made about specific subgroups within the population of interest.</p><h3>Associations among AI measures</h3><p>To assess possible associations among the different AI-related measures, we conducted Bayesian multiple regression with the respective AI-related predictor variables entered as ordinal predictors (i.e., monotonic effects). For each of these models, we additionally included Age, Race, Sex, Region, Education, Income, and Political Party Affiliation as control variables. When making predictions from the model, we varied the value of the ordinal predictor of interest while holding each of the other AI variables constant at their median in the sample data, and then averaged the predictions across all the demographic variables.</p><p>Simple pairwise associations (Spearman\u2019s rank) between all the AI-related variables presented above can be seen in the correlation matrix here:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ir6jmz5x2bq3z34df3be\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/iunxdajc5bkypcthdkv3 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/binoy2mzcpptgsynbv8k 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bqegm97oqfkdfn4u2rvw 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/smtbbodutq9chfnd2omb 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/hlmxa87ltjsg8yllfrwr 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/dy6go20ip6zj4gcgr3kk 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/u40vxnd3nyvwfz6ildmm 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/iet1p1bgdhjova9zuyv5 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/lg2xcnkpxzcpp4ubodjg 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/rxfy73yxszgcffkbzqha 1440w\"></p><h2>Sensitivity to poststratification/weighting approach</h2><p>Given that our sample was generated from online respondents, there are some concerns that despite efforts to make the sample representative of the population, we cannot represent certain kinds of people who are simply not online. In data from 2021 from&nbsp;<a href=\"https://www.pewresearch.org/wp-content/uploads/2021/03/Internet-Frequency-Update-Methodology-Topline.pdf\"><u>Pew Research</u></a>, it was estimated that around 7% of US adults would report never using the internet. To try to correct for the possibility that an overly online sample might affect our results, we did include an assessment of internet frequency, and were then able to weight the sample according to answers to that question. Including this outcome variable is not possible in the MRP approach, but can be included in weights. As shown in the plots below, there is little if any difference between our MRP estimates and those generated by using an alternative weighting protocol that includes internet use frequency.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/hljfkly0smtdq2kqhamu\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/lzustuyxxg6ztixlj27d 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zii2zlmzjnuyhkbsltrg 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/nayiav5jdbijkwcr774c 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/vs9eqfd04frmfugo8qhm 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ebfkjzk9hdab5hjk3jnl 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/g6upx7firuprgtxc5dji 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/xdad1nk9rntl2rsmq7ba 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ywnun1eekhlbkxhfe9am 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/tucqnvu2lnjkg5yzcwx5 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/yiopkk3cvj4wn7v6kezu 1371w\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kog1keexra5uuzhraqay\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/z2k7gaisutwfempdmmqg 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/vsdq81sl8lhg38d8bofz 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bwbzt5ve3vjwez8xff5q 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/glp7ztktjq5xnc9i6ped 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/iu9mdxwoovi25pkdpzdd 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/aeylqmczbsf6gzpgkrda 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/xhhfkaqdqi5gr3f5m441 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/hjjnepdbpf0kfcc4nrky 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/o53pdo8hrljel3qnerli 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/wxb8ax2zsmb35mhqcdpm 1600w\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/dsleeyesadikls3gpyps\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/i1lvazw3svuppelypafw 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ptzm6ebaebcxcgs6kdyz 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/tagyis0ke3cw4kwnman5 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/r4ajvgsg9b3e5rbf8vsy 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bgkaa4ky6vsyybfb9gal 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/jtpf12ecjcv3quax7e8g 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/cl1bx5kygoabjftoi3fm 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/pml1fexckfybufmees00 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bztysmmfavksa9snwocr 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/mggiy03on7tsggtpbuxq 1600w\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/v88oiakkdoysvnglaijm\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/fmqpr5pcnqr6xviizskt 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/uvyd1wbo30lqlxl2iyc1 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/cbbcgsklpv0orntadiwo 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/mcjs4lxbbcifzbqka1hg 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/vsq7np2do0bqm1cacdcs 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/pi864u5qsan4lco5xpwj 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/pe6fa1wh7609uw65zbwp 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/muhppkyvfj4cxpdj0sel 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/z8n0dkrqd0gbetoctxiw 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ncmwv8zkujbibkbogekf 1600w\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ltplpbpiosvzsk4u7wfe\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/dhx0rr37rn9wxonmsek3 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/jbvlomkcysmbwmepq9pb 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/un1gy1dwupamdzhoggje 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kkelrxzpouxferzdpcuf 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kbbnjegfupnxcj5vglky 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/aqkte8qtx2lpy6pqxv8y 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/s2qiqezfcf1iljvbwjlz 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/dgxmjdgh4fmyw3hvakiu 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/wzgomkuzraz2ubmbhnqd 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/fa5kuno08aamifjkoujc 1600w\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/r5wgsfv2jx4ujiqrhuoe\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/tkupf6aop2rnfu7ulqmz 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/r7whufqfvqenq3ybda7g 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/velmtfll9erbbexh0q7x 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/adyipkizbfi8jcpncsez 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/fstwkwnguanm9ofqjfm1 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/mo9ykgiqqzukiyg7qzoq 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/iniuyibwyzxqfcvcj8cw 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/xclw35o8u0pgvjufxeag 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/sqp3kfrizytybg12fhgh 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/l2txvot7gjwkpcas3mth 1600w\"></p><p>We additionally conducted some multiple regression analyses in which we included Age, Gender, Race, Education, Income, US Region, Political Party ID, and Internet Use Frequency as predictors of each of these main outcome variables. Averages over the other demographic variables and then assessing the specific differences among internet frequency answers, we see that for some variables, there appears to be a slight (or sometimes more substantial) effect of internet frequency on responses. However, the key consideration is that such respondents make up only a minority of the US population, and so even when such respondents\u2019 answers are weighted for inclusion, their responses would have to be very highly and consistently different in order to shift the overall population estimate.</p><p>Finally, where there do appear to be differences between those with higher and lower levels of internet use, infrequent internet users tend to be&nbsp;<i>more&nbsp;</i>rather than&nbsp;<i>less&nbsp;</i>concerned about AI. Hence, we do not think that our responses risk over-emphasising the concern people have over AI. However, we of course do not have information on people who actually never use the internet, and so if these people\u2019s responses would be really dramatically different from the lower frequency users we do have data on, then this could be a threat to the validity of the conclusions. Again, however, as these people represent just 7% of the population as of 2021, their responses would have to be different in the extreme to shift population estimates.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/s3horhpmimqrv8iti2it\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ebwqev1ir4yuwncpnuu8 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/yiljshzmxajsypmxqsi1 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/krfujtvdnw3qhoxwhuwe 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/occw5usnfxdidvuingzm 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/v1nwz5cg6snrclr5wzxq 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bwbnmk0haa9yu7aou3a7 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kk1vkzgiof6hgkvbyiqy 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/xyrnmvn1jb27yafshyxa 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/w0dez07zzbebpzosc7se 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/uuueuojfbhvxqwgqcd9h 1600w\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kbnzak8vysuhd1eydl3k\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/jiojacd8jcuntrjwc953 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/d1aykgwgtojyyfo60eaz 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/o8t0t5vzopvdpof4nbxh 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/srbw7jitktjnegfaejjq 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/hn5ytmfwe0hgu21psgwg 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/mej2tnzbpca1pjmbte70 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/g5usah7knujsztyzaqp5 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/vt1ccmoekxk7j152hekk 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/emiwfeihzhizc9yuqrzv 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/gy0qgltxdbqqgjowevjj 1600w\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/y0fozl8ahcaitqmpimhh\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/rduzwp8bmo25il7gwrnl 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/l2bav4n0zaytpl1lcwmj 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/m4f9gkn46q1cfewfc02w 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/shur7j5dza8tw31ih0z9 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/e3skvxpblxdgwtq4vfqb 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ticocumv0dsqip0kp7uv 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/lzb0obxkbjyyi0orfsh6 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/mfmwxnra6mwd85puiklm 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bicifxhqjdoe3b6jpwjr 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/anutdtfrox8fn5msytdt 1600w\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ozvhozn095rq6k2zeede\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/sd5d7d6gekeafehkh76h 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/b13zg2cuvizylboyx44n 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ft09xiknbyfremjzqmmb 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qwkrmi7d7znyvdih2vwa 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ld2duujuelbcy57fdlxp 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zati8yggircrdkij4yne 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/jygay4s8fnccrbuvzsob 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ql1kktjoilvadoe8qmk0 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/insdlxwsklzdejuya6ov 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/l2yug5s1gwxz8rws0tzo 1600w\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/sky52mlirujopacgblao\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/yrl3ldzui09rufrrxvkn 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/b3kkhralsrhhgsxji02l 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/yx8zdghlyzcub4ux6dg6 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/swgp24ksvd2wbbmslc6c 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/oodgylnu6tczot11oqty 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/bryhivtsfnc0ukagkezn 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zqvb1kqjtgeirjdzkqaa 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/zb8sgoaluvwmwfey99ds 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/teseov5bywb47rt9nnbs 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/yjnhhaoulvpd7gwa8g5i 1600w\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/d0s3yq7oppajezcpsylw\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/crud1qbsaimjcoowomu8 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/kfrep3t35zwy75igfjme 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/rrr3lzg44tuqe9l16efr 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qhxf4ujoms7x6beju322 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/nk2z6xdqoj7lstlhfwmb 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/ovqxxjxwb6dacyet9vbg 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/uyrqeursfxwjop0tulse 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/shjdsyd7tvgxhoynyyua 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/tiy91ff4rtrj3lcxohsh 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/nb5uyzzfahl1wqi8rxuv 1600w\"></p><h2>Acknowledgments</h2><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/iknljsjkg9wjbzr1rlfh\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/jyurkufygqkvqtvo7if7 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/a9slgapkbeei183rf7go 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/akgzoc3wmpkep47qwgln 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/xfl0oizedzzu509hbabm 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/osnhhfi7kjmbbuizkmk0 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/qjra1tjgvsldjjrpsocn 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/gs8r0uiw7usyezm0ndq2 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/rg925cnz6gh69z2nyso1 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/voog6bjdjo8rh4ly3ozb 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ConFiY9cRmg37fs2p/f4spg4okr8eaw5ytetpd 2048w\"></p><p>Jamie Elsey and David Moss wrote this report. Jamie Elsey, David Moss, and Will McAuliffe developed the survey. Will McAuliffe was also involved in discussion and interpretation of the results. Jamie Elsey conducted the analyses and data visualization. We would also like to thank Peter Wildeford and Renan Araujo for review of and suggestions to the final draft of this report.</p>", "user": {"username": "Jamie Elsey"}}, {"_id": "GgmAeWqXSg8DHMsJe", "title": "How much funging is there with donations to different EA animal charities?", "postedAt": "2023-05-11T11:34:56.528Z", "htmlBody": "<h1>My main question</h1>\n<p>The EA Funds Animal Welfare Fund makes grants to many different animal charities. Suppose I want to support one particular charity that they grant to because I think it's better, relative to my values, than most of the other ones. For example, maybe I want to specifically give to Legal Impact for Chickens (LIC), so I donate $1000 to them.</p>\n<p>Because this donation reduces LIC's room for more funding, it may decrease the amount that the Animal Welfare Fund itself (or Open Philanthropy, Animal Charity Evaluators, or individual EA donors) will give to LIC in the future. How large should I expect this effect to be in general? Will my $1000 donation tend to \"funge\" against these other EA donors almost fully, so that LIC can be expected to get about $1000 less from them? Is the funging amount more like $500? Is it roughly $0 of funging? Or maybe donating to LIC helps them grow faster, so that they can hire more people and do more things, thereby <em>increasing</em> their room for funding and how much other EA donors give to them?</p>\n<p>The answer to this question probably varies substantially from one case to the next, and maybe the best way to figure it out would be to learn a lot about the funding situation for a particular charity and the funding inclinations of big EA donors toward that charity. But that takes a lot of work, so I wonder if EA funders have some intuition for what tends to happen on average in situations like this, to inform small donors who aren't going to get that far into the weeds with a particular charity. Does the funging amount tend to be closer to 0% or closer to 100% of what an individual donor gives?</p>\n<p>I notice that the Animal Welfare Fund sometimes funds ~10% to ~50% of an organization's operating budget, which I imagine may be partly intentional to avoid crowding out small donors. (It may also be motivated by wanting charities to diversify their funding sources and due to limited funds to disburse.) Is it true in general that the Animal Welfare Fund doesn't fully fill room for funding, or are there charities for which the Fund does top up the charity completely? (Note that it would actually be better impact-wise to ensure that the very best charities are roughly fully funded, so I'm not encouraging a strategy of deliberately underfunding them.)</p>\n<p>In the rest of this post, I'll give more details on why I'm asking about this topic, but this further elaboration is optional reading and is more specific to my situation.</p>\n<h1>My donation preferences</h1>\n<p>I think a lot of EA donations to animal charities are really exciting. About 1/3 of the grants in the Animal Welfare Fund's Grants Database seem to me roughly as cost-effective as possible for reducing near-term animal suffering. However, for some other grants, I'm pretty ambivalent about the sign of the net impact (i.e., whether it's net good or bad).</p>\n<p>This is mainly for two reasons:</p>\n<ol>\n<li>I'm <a href=\"https://reducing-suffering.org/net-impact-vegetarianism-factory-farm-suffering-vs-invertebrates-pasture-fields/\">unsure</a> if meat reduction, on the whole, reduces animal suffering, mainly because certain kinds of animal farming, especially cattle grazing on non-irrigated pasture, may reduce an enormous amount of wild-animal suffering (though there are huge error bars on this analysis).</li>\n<li>I'm unsure if antispeciesism in general reduces net suffering. In the short run, I worry that it may encourage more habitat preservation, thereby increasing wild-animal suffering. In the long run, moral-circle expansion could encourage people to create lots of additional small-brained sentience, and in (hopefully unlikely) scenarios where human values become inverted, antispeciesist values <a href=\"https://reducing-suffering.org/near-miss/#When_spreading_your_values_is_harmful\">could multiply</a> total suffering manyfold.</li>\n</ol>\n<p>If I could press a button to reduce overall meat consumption or to increase concern for animals, I probably would. In other words, I think the expected value of these things is perhaps slightly above zero. But my expected value for them is sufficiently close to zero that I don't feel great about my donations being used for them.</p>\n<p>Therefore, I would prefer to not spend precious money to support alternative proteins, veg outreach, or expanding the general animal-advocacy movement. Rather, the work I'm most excited about is welfare reforms -- especially for the most numerous animals like chickens, fish, and shrimp, and especially those reforms that reduce the very most intense suffering, such as the pain of slaughter.</p>\n<p>(By the way, I would definitely press a button to reduce chicken consumption, but I plausibly would not press a button to reduce beef consumption. I wish it were feasible to push on one but not the other of these variables, but that seems tricky to do as a donor, other than maybe by giving to One Step for Animals.)</p>\n<h1>Donation funging</h1>\n<p>The specificity of my preferences regarding which animal activism I'm enthusiastic about makes it harder to find good donation targets. Most animal charities do a mix of welfare reforms, meat reduction, and moral-circle expansion. Earmarking your donation for one of these activities in particular could allow the charity to funge your donation by spending less of their unrestricted money on that activity (although in some cases, earmarked donations go toward new projects that wouldn't have happened at all otherwise).</p>\n<p>There's a similar kind of funging between charities, especially in contexts like EA where donors are sensitive to room for more funding (<a href=\"https://reflectivedisequilibrium.blogspot.com/2014/01/its-harder-to-favor-specific-cause-in.html\">Shulman 2014</a>). Donating to a single charity is kind of like making an earmarked donation: it frees up other EA donors to use their \"unrestricted\" money for all the other EA animal charities instead. This funging concern has been discussed by a number of authors, such as <a href=\"https://doi.org/10.1093/oso/9780198841364.003.0012\">Budolfson and Spears (2019)</a>. The thrust of my question is: Does this actually occur a lot in practice in the EA animal-welfare world? Or is it mainly a theoretical worry?</p>\n<p>The concern is that if I donate to, say, Legal Impact for Chickens, then the Animal Welfare Fund will donate less to that charity and have more money for other things. In the worst case where there's 100% funging, my donation to LIC is only as good as donating to the Animal Welfare Fund itself, which splits its grants between welfare reforms and other activities that I'm less interested in. Maybe the average Animal Welfare Fund grant is only 1/3 to 1/2 as good as a grant to a specific charity that I want to support, so in the case of 100% funging, the effectiveness of my donation by my lights would be multiplied by 1/3 to 1/2.</p>\n<h1>Donating to less popular targets</h1>\n<p>One possible answer to the funging problem is to seek charities or other funding opportunities \"off the beaten path\", i.e., ones that the big EA animal donors probably wouldn't fund. <a href=\"https://forum.effectivealtruism.org/posts/oHcRQsiDM9D76aENu/a-red-team-against-the-impact-of-small-donations\">AppliedDivinityStudies (2021)</a> also makes this suggestion.</p>\n<p>One way to do this is to look for individual activists whom you know personally but who are unlikely to receive institutional EA funding, such as because the grant size would be too small to bother with or because the activism isn't legible to or suited to the tastes of the bigger EA funders. However, the total amount of gifts you can make this way may not be very high unless you know a lot of activists in this category.</p>\n<p>Another approach is to give to charities that aren't popular with other EAs. I donate a bit to <a href=\"https://forum.effectivealtruism.org/topics/animal-ethics\">Animal Ethics</a> (AE) because it doesn't receive much funding from the big EA donors. AE's last grant from the Animal Welfare Fund was in 2018, and Animal Charity Evaluators stopped highlighting AE as a Standout Charity in 2017. AE's focus on wild-animal suffering makes it less likely that AE's particular brand of moral-circle expansion will increase support for wilderness preservation. That said, I'm more enthusiastic about specific welfare reforms that will reduce suffering in the near future, which AE doesn't really work on. My reason for donating to animal causes at all is because they're more concrete than nebulous far-future suffering-reduction work, so the more tangible the impact is, the better.</p>\n<p>The <a href=\"https://reducing-suffering.org/why-i-support-the-humane-slaughter-association/\">Humane Slaughter Association</a> (HSA) is another option that not many EAs seem interested in. HSA received two large grants from Open Philanthropy, in <a href=\"https://www.openphilanthropy.org/grants/humane-slaughter-association-fish-crustacean-and-cephalopod-welfare/\">2017</a> and <a href=\"https://www.openphilanthropy.org/grants/humane-slaughter-association-wild-caught-fish-welfare/\">2019</a>, but those were earmarked for specific projects, so general HSA funding may not be funged by them. HSA meets most of the criteria I'm looking for: it only does welfare reforms, it doesn't mention environmentalism, and it focuses on the most extreme suffering (slaughter), including of chickens and fish. The main downside of HSA for me is that a lot of its work targets larger animals (cattle, pigs, etc), which are numerically less important. If only, say, 1/3 to 1/2 of HSA's work is about chickens and fish, then the impact of a donation to it is sort of multiplied by 1/3 to 1/2, similar to what I mentioned regarding donating to the Animal Welfare Fund. I suppose one could ask HSA to earmark a donation toward a fish-specific project, as Open Philanthropy did, but this would be a lot of work for a small donor like me. And that fish-specific project would be more likely to funge against future Open Philanthropy grants.</p>\n<p>HSA has for many years had a large fund of assets that it invests rather than spending, which may lead people to conclude that it lacks room for more funding. I'm not sure if that's accurate, since investing money to spend later is a reasonable strategy to take, both for individuals and organizations. HSA has existed since 1911, so it's not in growth mode the way a startup charity would be. Donating more than what a charity can spend now doesn't seem to me like a problem per se; the problem is when that surplus of funding discourages other donors from giving to the charity.</p>\n<p>Charities that aren't adored in EA can still have their own forms of funging with non-EA donors. Some non-EA would-be donors might think these charities already have enough funding and give elsewhere. But that seems less likely than in the case of big EA donors who actively scrutinize room for more funding and aren't as emotionally attached to one particular organization.</p>\n<p>If having lots of money makes a charity less interested in fundraising, that could be another way in which donations to more obscure charities can be funged. (Holden Karnofsky makes this point in the comments on <a href=\"https://reflectivedisequilibrium.blogspot.com/2014/01/its-harder-to-favor-specific-cause-in.html\">Shulman (2014)</a>.)</p>\n<h1>Closing remarks</h1>\n<p>I'm curious whether readers have suggestions for additional animal-welfare charities meeting my criteria that wouldn't by default be funded by the big EA donors. Maybe the Animal Welfare Fund could share a list of charities they almost funded but that didn't make their cutoff and that they're unlikely to fund in the future either. Or a list of which of the charities they are funding still have significant room for more funding that the Animal Welfare Fund doesn't expect to fill in the future.</p>\n<p>I certainly wouldn't want to discourage people from donating to the Animal Welfare Fund; I consider donating to it myself. I think some of the work it funds is amazing and some is at worst ~neutral in expectation. By my lights, it would be bad if people donated less to animal charities and more to other EA causes -- especially something like biorisk reduction, which I think increases expected future suffering, though like with everything else, the sign is very unclear. This problem of funging between EA cause areas (including cause areas that are net good and net bad according to my values) is an additional layer of vexation.</p>\n", "user": {"username": "Brian_Tomasik"}}, {"_id": "kcE93PGPByM3Z7iGT", "title": "You don't need to be a genius to be in AI safety research", "postedAt": "2023-05-10T22:23:31.976Z", "htmlBody": "<p>The aim of this article is to share my experiences within the AI safety community to those in research and operations in the field, with the goal of creating a more inclusive, supportive, and positive environment. The intention of this article is to allow those in a more privileged position to think about these considerations and potentially implement new practices in their organizations or work in order to create a more welcoming space for those not traditionally represented here. I\u2019ve also had some incredibly supportive and fulfilling experiences within this field which are worth celebrating. However in order to create the most inclusive space that we can, I think the following points are important to address.&nbsp;</p><p>&nbsp;</p><h3><strong>Using \u2018genius\u2019 is not a reliable metric for researchers</strong></h3><p>When people claim to be searching for \u2018genius\u2019 researchers or that you need to be a genius to contribute in a meaningful way to AI safety, it\u2019s unclear which specific metrics they are using for evaluation. A number of factors contribute to the ambiguity and subjectivity surrounding the definition of \"genius,\" including people's individualized perspectives shaped by their upbringing, the complexity of intelligence, our growing understanding of cognitive processes, the context-dependence of genius, and the lack of clear criteria to determine genius. These factors make it challenging to establish a universally accepted definition of genius.</p><p>Regardless of the criteria used to determine genius, it is critical to understand that this quality isn\u2019t the only one necessary to contribute in a meaningful way.</p><p>Recruiting researchers is a task that requires careful evaluation of multiple factors such as skill, experience, ability to work with others, and ingenuity. However certain organizations, program managers, and coaches in the AI safety field pay too much attention to the benchmark of \u2018genius\u2019 to recruit or encourage researchers. This type of thinking is harmful and unnecessary because it leads to a lack of diversity, narrow focus, elitism, imposter syndrome, and waste of resources in the pursuit of this elusive trait.&nbsp;</p><p>By allowing this idea to continue, we run the risk of recruiting for a narrow set of skills or attributes, thus limiting potential innovation and diversity of thought, while also potentially slowing research progress. Instead, it's important to actively seek talented individuals with alternative educational paths, rather than solely relying on these \"typical\" backgrounds. Likewise, embracing various research directions offer opportunities for meaningful input. For instance, initiatives like PIBBSS, which encourages individuals beyond the traditional ML domain to engage in alignment work, highlight the importance of diversifying perspectives.</p><p>Most incredible discoveries are the result of incremental advances, even if these aren\u2019t in the public eye. Research progress is the result of continuous and gradual accumulation of knowledge through&nbsp;<a href=\"https://archive.org/details/organizinggenius00warr/page/n5/mode/2up\"><u>collaborative</u></a> endeavors.While it\u2019s very uncommon to find exceptional individuals like Einstein or Newton, there are thousands of researchers who are making significant, interesting and valuable contributions to their field without necessarily being geniuses. You don\u2019t even have to make a correct discovery to be a contributor - John Nicholson, Anton Van den Broek, Richard Abegg, Charles Bury, John Main Smith, Edmund Stoner and Charles Janet are all lesser known contributors to the field of atomic structure, yet each of them published one or two (even incorrect) ideas that allowed for other&nbsp;<a href=\"http://acshist.scs.illinois.edu/bulletin_open_access/v42-2/v42-2%20p141-143.pdf\"><u>researchers</u></a>, such as Niels Bohr, to push their ideas forward leading to breakthroughs.</p><p>By the way, you don\u2019t need to be a genius to be an independent researcher. Here\u2019s a&nbsp;<a href=\"https://www.lesswrong.com/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency\"><u>roadmap</u></a> with actionable steps you can take to get there.</p><p>&nbsp;</p><h3><strong>Who gets to decide if you\u2019re a genius?</strong></h3><p>There are many ways in which genius can present itself, some of which may be unfamiliar to the person making the judgment. If you are going off of a conversation with someone for instance, there may be multiple things at play that could make someone appear to not fit the role of genius such as nerves, social anxiety, differences in communication style, or unconscious bias in the judge. The person making the call is likely not a genius themselves, using unclear ways of measuring what they think constitutes a genius. This notion is often biased towards individuals from privileged backgrounds, such as going to a prestigious school or participating in certain fellowships.&nbsp;</p><p>Gender biases can also play an unfair role in how we perceive \u2018genius\u2019, resulting in men being linked to the term at a greater rate than women throughout history. In a global perceptions study measuring stereotypes, men are more likely than women to be seen as \u201cbrilliant\u201d as a result of&nbsp;<a href=\"https://doi.org/10.1016/j.jesp.2020.104020\"><u>implicit bias</u></a>. This stereotype can be attributed to multiple reasons such as the undervaluing and overlooking of women throughout history in fields where they could have greatly contributed but were either not allowed to or were not given the same resources.&nbsp;<a href=\"https://www.science.org/doi/10.1126/science.aah6524\"><u>Gender stereotypes&nbsp;</u></a>surrounding&nbsp;<a href=\"https://pubmed.ncbi.nlm.nih.gov/21410915/\"><u>brilliance</u></a> appear to emerge early in childhood, and in order to break out of this way of thinking we need to be aware of these&nbsp;<a href=\"https://www.scientificamerican.com/article/where-are-all-the-female-geniuses/\"><u>stereotypes</u></a> and consciously work to&nbsp;<a href=\"https://beta.nsf.gov/news/youd-have-be-genius-problem-brilliance-stem\"><u>unlearn</u></a> them.<br>&nbsp;</p><h3><strong>What actually makes a good researcher?&nbsp;</strong></h3><p><strong>Collaboration</strong>: AI safety research encapsulates an interdisciplinary field that involves experts from a range of disciplines such as computer science, math, philosophy, linguistics, psychology, and ethics. While a strong technical background is an asset, communication with other researchers and with an audience of varying degrees of technical background is equally important. <strong>[</strong><a href=\"http://snap.stanford.edu/class/cs224w-2011/proj/anmittal_Finalwriteup_v1.pdf\"><strong><u>1</u></strong></a><strong>] [</strong><a href=\"https://gateway.ipfs.io/ipfs/bafykbzacea4rw2olgd3u7cx7mphm5y5gm7cf3m6hnvcxapyjm6clg3fx56hk6?filename=Dashun%20Wang_%20Albert-L%C3%A1szl%C3%B3%20Barab%C3%A1si%20-%20The%20Science%20of%20Science-Cambridge%20University%20Press%20%282021%29.pdf\"><strong><u>2</u></strong></a><strong>]</strong></p><p><strong>Honesty and ethics</strong>: It\u2019s important that your research be reproducible and your methods transparent. Honesty in reporting results and limitations helps the broader research community build upon and validate your findings.&nbsp;</p><p><strong>Persistence</strong>: Research can be a long and challenging process that requires a significant amount of time and effort. It\u2019s possible you\u2019ll run into hurdles in idea generation, debugging, funding/resources, hitting a theory-related deadend, etc.&nbsp;Persistence pays off.</p><p><strong>Openness to feedback:</strong> Researchers are responsible for gathering feedback, especially in the early stages of their career, and publishing work in public spaces if it\u2019s safe to do so. You are opening yourself up to critique and comments, which can feel vulnerable but this is something that allows for growth and opportunities to collaborate by getting your name out and attached to your research.</p><p><strong>Creativity and curiosity</strong>: Since this is a new field, many types of studies are still in their infancy, which requires researchers to come up with new ideas perhaps more readily than in other fields. Having creativity allows for researchers to come to insights from different angles and see what others are missing, potentially from unexpected places.&nbsp;</p><p><strong>Range&nbsp;of strengths:</strong> &nbsp;We need skilled software engineers to carry out important experimental work that may not require theorizing. We also need skilled research engineers that have a foundational understanding of theory and machine learning. We need theorists that have a background in math, but may not necessarily know how to code. There\u2019s no need to be an expert in all fields to make valuable contributions to AI safety research.</p><p><strong>Openness to being wrong (and patience)</strong>: Many people are quick to jump to solutions without fully understanding the problem, or feel locked in to a hypothesis or solution because they\u2019ve put in a lot of time pursuing this path. It\u2019s important to give up on an idea when evidence points us that way, let go of ego, and take your time when pursuing&nbsp;<a href=\"https://www.lesswrong.com/posts/uHYYA32CKgKT3FagE/hold-off-on-proposing-solutions\"><u>solutions</u></a>.</p><p><strong>Perseverance after failure</strong>: Failure can sting, but it\u2019s important to not take this as a personal defect. Additionally,&nbsp;many people who excel in school may not have experienced an academic failure, and may have trouble accepting this in a research setting. This can be common during a shift going from university (where problems are generally able to be solved with some effort), to a research environment (where problems can potentially have no clear solution). It\u2019s important to realize this is a part of life when you are trailblazing new paths. Being able to bounce back quickly is a super power.</p><p>&nbsp;</p><h3><strong>Creating a more inclusive environment</strong></h3><p>We have a long way to go in terms of inclusivity and diversity in the AI safety community, but there are ways that we can become more welcoming to a diverse range of people (and to ourselves):</p><p><strong>Patience</strong>: accept that there\u2019s a learning curve, and that research takes time. It\u2019s okay (and very normal) to not be very productive in the first few months that you\u2019re new to research, this isn\u2019t indicative of your ability to be a great researcher. Be kind to yourself.</p><p><strong>Adversarial communication</strong>: even casual conversations in the community can sometimes feel like you are being asked to constantly explain yourself or that your conversation partner's goal is to contradict you and prove you wrong (even if this isn\u2019t the intention). This can be a bit off-putting to some but can be mitigated by adopting a more friendly conversation style when first speaking with someone and taking a moment to understand their communication style before defaulting to this.</p><p><strong>Rest</strong>: Allow yourself to rest regardless of your timeline, take care of yourself and realize you don\u2019t deserve to feel guilty for not being productive every day. You can walk, workout, stretch away from your desk, or do a 10 min meditation. The break may even lead you to a breakthrough.</p><p><strong>Leadership</strong>: Pushing for a more diverse leadership team for AI alignment organizations, fellowships, and research teams as the norm. Advocating for more diversity of perspectives and encouraging the ideas of those not traditionally represented.&nbsp;</p><p>&nbsp;</p><h3><strong>Conclusion</strong></h3><p>Focusing on \u2018genius\u2019 shouldn\u2019t be used to assess potential researchers because of the vague and not well understood nature of the term. This can lead to overlooking researchers with diverse skills, experiences, and perspectives. Instead, I urge those in AI safety research to adopt an inclusive and well-rounded approach that considers unique strengths, areas of knowledge, and collaboration potential in their view of what a productive researcher looks like.&nbsp;In order to incorporate a range of skills and perspectives, we need to encourage different types of people to become involved at multiple levels.</p><p>The concept of genius is complex and multifaceted, and can change contexts based on culture and history. Let\u2019s recognize and appreciate diverse forms of intelligence and creativity, rather than narrow our viewpoints at the expense of many.</p><p>Rather than recruiting for genius, organizations should evaluate promising researchers on their potential backed by a combination of various factors such as curiosity, creativity, persistence, skill, collaboration, motivation, and willingness to learn. Recruiting based on \u2018genius\u2019 is harmful for both the applicant and the organization because it is an unfair, potentially biased, and unclear way of measuring how fit someone is to be involved in AI safety research.</p><p><br>&nbsp;</p>", "user": {"username": "Claire Short"}}, {"_id": "wptexujA6YfehDG9h", "title": "RIP Medical Debt", "postedAt": "2023-05-10T20:59:43.651Z", "htmlBody": "<p>1 in every 2 Americans struggles between paying medical bills and affording their basic needs. Medical debt is the leading cause of bankruptcy in America and it is estimated that one in three, over 100 million Americans struggle with medical debt. As someone with endometriosis, I have experienced medical debt myself. As a social work student at Columbia University, I know the toll that medical debt takes on my clients.&nbsp;</p><p>disclaimer: <i>I am not affiliated with this organization, I'm sharing this voluntarily but I know of their work and thought it would be of interest to the EA community.&nbsp;</i></p><p>RIP Medical Debt is a nonprofit that buys debt for pennies on the dollar and eliminates this debt for families in need, no strings attached. Families are chosen based on data: households that earn less than 4x the federal poverty level (varies by state, family size) or whose debts are 5% or more of annual income are the organization's priority. On average, every $100 donated clears $10,000 in medical debt.&nbsp;</p><p>Medical debt is an important issue for people with disabilities and chronic illnesses who are more likely to visit medical providers, need specialized treatments, and overall have a higher burden of care compared to individuals in good health. Many patients with medical debt will avoid seeing medical providers and delay important screenings because they are scared of incurring more debt.&nbsp;</p><p>I have linked some resources below for anyone interested in learning more about medical debt and the toll that it takes on people. Looking forward to hearing everyone's thoughts!</p><p><a href=\"https://www.cssny.org/news/entry/predatory-medical-debt-collection-practices-by-hospitals-must-end\">https://www.cssny.org/news/entry/predatory-medical-debt-collection-practices-by-hospitals-must-end</a></p><p><a href=\"https://pubmed.ncbi.nlm.nih.gov/22352364/\">https://pubmed.ncbi.nlm.nih.gov/22352364/</a> (The Intersection of Medical Debt and Predatory Lending among Hispanics)</p><p><a href=\"https://nonprofitquarterly.org/eliminating-healthcare-debt-a-liberatory-approach/\">https://nonprofitquarterly.org/eliminating-healthcare-debt-a-liberatory-approach/</a></p>", "user": {"username": "Layla Saliba"}}, {"_id": "CAC8zn292C9T5aopw", "title": "Community Health & Special Projects: Updates and Contacting Us", "postedAt": "2023-05-10T17:35:30.279Z", "htmlBody": "<h2>Summary</h2><ul><li>We\u2019ve renamed our team to Community Health and Special Projects, in part to reflect our scope extending beyond what\u2019s often considered to be \u201ccommunity health.\u201d&nbsp;</li><li>Since our last forum update, we\u2019ve started working closely with Anu Oak and \u0141ukasz Grabowski as contractors. Chana Messinger has been acting as interim team lead, while Nicole Ross has been focused on EV US board duties. Charlotte Darnell has also recently accepted a role on our team.</li><li>In response to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9JCkkjKMNL4Hmg4qP/ev-uk-board-statement-on-owen-s-resignation\"><u>reports of sexual misconduct by Owen Cotton-Barratt</u></a>, an external investigation into our team\u2019s response is underway, as well as an internal review.</li><li>Other key proactive projects we\u2019ve been working on include the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/mEkRrDweNSdNdrmvx/plans-for-investigating-and-improving-the-experience-of\"><u>Gender Experiences project</u></a> and the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Dq69kvjKyxQzKNRH7/seeking-expertise-to-improve-ea-organizations\"><u>EA Organization Reform project</u></a>.&nbsp;</li><li>We are in the early stages of considering some significant strategic changes for our team. We\u2019ve highlighted two examples of possible changes below, one being a potential spin-out of CEA and/or EV and another being a pivot to focus more on the AI safety space.</li><li>As a reminder, if you\u2019ve experienced anything you\u2019re uncomfortable with in the community or if you would like to report a concern,&nbsp;you can reach our team\u2019s contact people (currently Julia Wise and Catherine Low) via&nbsp;<a href=\"https://efctv.org/contactcommhealth\"><u>this form</u></a> (anonymously if you choose).&nbsp;<ul><li>We can also be contacted individually (our individual forms are linked&nbsp;<a href=\"https://efctv.org/contactcommhealth\"><u>here</u></a>), or you can contact the whole team at&nbsp;<a href=\"mailto:community.health.special.projects@centreforeffectivealtruism.org\"><u>community.health.special.projects@centreforeffectivealtruism.org</u></a>.&nbsp;</li><li>We can provide anonymous, real-time conversations in place of calls when requested, e.g. through Google Chat with your anonymous email address.&nbsp;</li></ul></li></ul><h2>The Community Health team is now Community Health and Special Projects</h2><p>We decided to rename our team to better reflect the scope of our work. We\u2019ve found that when people think of our team, they mostly think of us as working on topics like mental health and interpersonal harm. While these areas are a central part of our work, we also work on a wide range of other things, such as advising on decisions with significant potential downside risk, improving community epistemics, advising programs working with minors, and reducing risks in areas with high geopolitical risk.</p><p>We see these other areas of work as contributing to our goal: to strengthen the ability of EA and related communities to fulfil their potential for impact, and to address problems that could prevent that. However, those areas of work can be quite disparate, and so \u201cSpecial Projects\u201d seemed an appropriate name to gesture towards \u201cother miscellaneous things that seem important and may not have a home somewhere else.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffs5x1oykp5i\"><sup><a href=\"#fnfs5x1oykp5i\">[1]</a></sup></span>&nbsp;</p><p>We hope that this might go some way to encouraging people to report a wider range of concerns to our team.&nbsp;&nbsp;</p><p>Our scope of work is guided by pragmatism: we aim to go wherever there are important community-related gaps not covered by others and try to make sure the highest priority gaps are filled. Where it seems better than the counterfactual, we sometimes try to fill those gaps ourselves. That means that our scope is both very broad and not always clear, and also that there will be plenty of things we don\u2019t have the capacity or the right expertise to have fully covered. If you\u2019re thinking of working on something you think we might have some knowledge about, the meme we want to spread is \u201c<a href=\"https://efctv.org/contactcommhealth\"><u>loop us in</u></a>, but don\u2019t assume it\u2019s totally covered or uncovered.\u201d If we can be helpful, we\u2019ll give advice, recommend resources or connect you with others interested in similar work.</p><h2>Team changes</h2><p>Here\u2019s our current team:&nbsp;</p><ul><li>Nicole Ross (Head of Community Health and Special Projects)</li><li>Julia Wise (Community Liaison)&nbsp;</li><li>Catherine Low (Community Health Associate)&nbsp;</li><li>Chana Messinger (Interim Head and Community Health Analyst)&nbsp;</li><li>Eve McCormick (Community Health Project Manager and Senior Assistant)&nbsp;</li></ul><p>In November 2022, Nicole took a step back from leading the team in order to focus on EV US board duties in response to the FTX crisis. In her place, Chana stepped into the role of Interim Head of Community Health and Special Projects. We anticipate that Chana will remain in this role for another 1-6 weeks (90% confidence interval). During this time, Nicole is dividing her time between some ongoing board duties and thinking about our team\u2019s strategy, including potential pivots (see below).&nbsp;</p><p>We\u2019ve also started working closely with Anu Oak (Project Coordinator and Assistant) and \u0141ukasz Grabowski (Project Manager) as contractors. Anu joined us in late October 2022 as Catherine\u2019s assistant and has since taken on responsibility for several of our team\u2019s internal systems. \u0141ukasz came on board in February 2023 and has been collaborating with other team members on various projects, including the Gender Experiences project (see below).&nbsp;</p><p>Charlotte Darnell has recently accepted a role on our team to help with interpersonal problems in the community alongside Catherine and Julia. Charlotte comes to us from CEA\u2019s Events Team.&nbsp;&nbsp;</p><h2>External investigation and internal review</h2><p>As mentioned&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9JCkkjKMNL4Hmg4qP/ev-uk-board-statement-on-owen-s-resignation\"><u>here</u></a>, EV UK and EV US have commissioned an independent, external investigation into reports of sexual misconduct by Owen Cotton-Barratt, including our team\u2019s response to those reports. This investigation is currently underway. The main point person for this investigation from EV is Zach Robinson,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/GoWNiPbrEb6NHD3MF/announcing-interim-ceos-of-evf\"><u>Interim CEO of EV US</u></a>.&nbsp;</p><p>Separately from the independent, external investigation, Chana has been overseeing an internal review within our team, with support from Ben West (<a href=\"https://forum.effectivealtruism.org/posts/3wBCKM3D2dXkXnpWY/announcing-cea-s-interim-managing-director\"><u>Interim Managing Director of CEA</u></a>).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdignftor43q\"><sup><a href=\"#fndignftor43q\">[2]</a></sup></span></p><p>Our goals for this review have been to reflect on our response to the reports about Owen\u2019s conduct, and to identify whether there are any systematic changes we should make to our casework.</p><p>This process has included:</p><ul><li>Julia and Nicole writing retrospectives, and members of the team discussing takeaways and updates from them, with support from Ben West.</li><li>Writing up anonymized versions of past cases to get perspectives on our processes from people outside the team and having calls with those people to discuss.&nbsp;</li><li>Chana speaking to several HR professionals, ombudspeople and an employment lawyer to get their perspectives on the team\u2019s work more broadly.</li><li>Catherine looking over the reports we received about Owen to give another perspective on how it could have been handled.&nbsp;</li><li>Relevant members of the team discussing and consolidating their overall updates together.</li></ul><p>Next steps:</p><ul><li>Continued work consolidating and thinking through updates.</li><li>We will communicate process changes in a future update.&nbsp;</li></ul><h2>Gender Experiences Project</h2><p>In February,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/mEkRrDweNSdNdrmvx/plans-for-investigating-and-improving-the-experience-of\"><u>we announced</u></a> our project to get a better understanding of the experiences of women and gender minorities in the EA community. This work is being carried out by Catherine Low, Anu Oak, and \u0141ukasz Grabowski. Charlotte Darnell will be joining this project soon.&nbsp;</p><p>So far, we have analysed data from a variety of existing sources such as EAG/x survey responses, the annual EA Survey run by Rethink Priorities, and Community Health team case records. We\u2019re now exploring ways to gather new information including working with Rethink Priorities on questions to include in their next survey.&nbsp;&nbsp;</p><p>We\u2019re drafting a post with some of our findings so far and hope to publish it soon.</p><h2>EA Organizations Reform Project</h2><p>Like many others in the EA community, we have been thinking about ways the community should potentially change or reform. One effort in this direction is the project Julia is currently leading. This project aims to build a task force of 3-5 people from across EA organisations and projects, which will look into areas where EA organisations might reform their practices and produce recommendations for EA organisations about steps that seem promising. They plan to interview people who are familiar with other fields about best practices that EA might currently be missing.</p><p>Some areas the taskforce is likely to consider:</p><ul><li>Board composition</li><li>Conflict of interest policies</li><li>Whistleblower protection / support</li></ul><p>Please see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Dq69kvjKyxQzKNRH7/seeking-expertise-to-improve-ea-organizations\"><u>this post</u></a> to suggest people the task force could talk to.</p><h2>Casework</h2><p>We are continuing with our reactive casework as usual, where we respond to requests for support or advice from community members.&nbsp;</p><p>Some examples of casework:</p><ul><li>Handling cases involving&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/NbkxLDECvdGuB95gW/the-community-health-team-s-work-on-interpersonal-harm-in\"><u>interpersonal harm in the community</u></a>. This often involves one or more of:<ul><li>Listening to people talk through what they have experienced</li><li>Talking to people who have made others uncomfortable about how to improve their behavior</li><li>Restricting people who have caused harm from attending CEA events</li><li>Informing other EA groups, projects, or organizations about known problems&nbsp;</li></ul></li><li>Supporting individual community members who are dealing with personal or interpersonal problems, such as a mental health-related struggle or a conflict between multiple community members.</li></ul><p>Catherine is the primary team member focusing on this area of our work at the moment, while Julia focuses on projects such as the organizational reform project. Charlotte will also be focusing on casework.</p><h2>Other updates on our work</h2><ul><li>\u0141ukasz has produced a team tool for mapping the overlap and interrelationships between the boards, staff, and advisors at EA organizations, to help us do risk assessment around the interrelatedness of EA projects. We hope that this will help draw our attention to conflicts of interest and high levels of interdependence, so we can help better manage risk.&nbsp;</li><li>In collaboration with Chana, Victoria Brook contracted with us to produce&nbsp;<a href=\"https://forum.effectivealtruism.org/s/dr9sjmfd6vB2ztXMt\"><u>this sequence of tutorials</u></a> for tools to help people approach collaborative truth-seeking. The sequence includes tutorials for using Guesstimate, Loom, Excalidraw, Squiggle and Polis.&nbsp;</li><li>Our work advising on projects with significant potential downside risks is continuing as usual. For example, we often advise on projects operating in less established and/or more sensitive fields, such as those relating to policy or involving minors, helping decision-makers to weigh up risks against the positives.&nbsp;&nbsp;</li></ul><h2>Potential strategic changes</h2><p>We\u2019re in the early stages of considering some potentially significant strategic changes for our team.&nbsp;</p><p>Firstly, as discussed a little&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/PhNfc9JRFc9CsDjvi/two-things-that-i-think-could-make-the-community-better?commentId=qzT5L7pmsJdhF9ywm\"><u>elsewhere</u></a>, we have been considering whether to spin out of CEA and/or Effective Ventures. This may grant us a useful kind of independence but might make some kinds of coordination more difficult. We will be gathering more information and considering these and other trade-offs over the next few months.&nbsp;</p><p>Secondly, we have been considering whether our team should pivot more of our resources towards work in the AI safety space. Actions we might take there would often be analogous to our existing work in the EA community more broadly. For example, we could potentially receive and investigate concerns about individuals in the AI safety space, provide support for the well-being of people working in the space, and assist with coordination between relevant actors to make plans for \u201c<a href=\"https://forum.effectivealtruism.org/posts/7CdtdieiijWXWhiZB/what-s-going-on-with-crunch-time\"><u>crunch time</u></a>\u201d. We will likely be investigating what this pivot could look like more deeply over the coming months.&nbsp;</p><h2>How to contact us (including anonymously)&nbsp;</h2><p>If you\u2019ve experienced anything you\u2019re uncomfortable with in the community or would like to report a concern,&nbsp;you can reach our team\u2019s contact people (currently Julia Wise and Catherine Low) via&nbsp;<a href=\"https://efctv.org/contactcommhealth\"><u>this form</u></a>. We know a form isn't the warmest, but this helps us manage the inflow of messages, and you can remain anonymous if you choose to be. Alternatively, you can email our contact people at&nbsp;<a href=\"mailto:community.contact.people@centreforeffectivealtruism.org\"><u>community.contact.people@centreforeffectivealtruism.org</u>,</a> or you can contact us individually (our individual forms are linked&nbsp;<a href=\"https://efctv.org/contactcommhealth\"><u>here</u></a>). And you can contact the whole team at&nbsp;<a href=\"mailto:community.health.special.projects@centreforeffectivealtruism.org\"><u>community.health.special.projects@centreforeffectivealtruism.org</u></a>.&nbsp;</p><p>You might contact us because you just want to pass on some information, or you might want to have a call to talk through your situation or just have someone listen to your concerns.</p><p>If you\u2019d like to have an anonymous, real-time conversation with us instead of a call, we may be able to facilitate that e.g. through Google Chat with your anonymous email address. If this is your preference, let us know and we can explore the options.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfs5x1oykp5i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffs5x1oykp5i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We\u2019re aware that Rethink Priorities also has a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/AFgvA9imsT6bww8E3/announcing-the-rethink-priorities-special-projects-program\"><u>Special Projects team</u></a>. Our current belief, and that of Rethink Priorities, is that this won\u2019t cause much confusion.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndignftor43q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdignftor43q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Chana has not been reporting to Nicole throughout the duration of conducting this internal review, in part to mitigate conflicts of interest.</p></div></li></ol>", "user": {"username": "evemccormick"}}, {"_id": "pR35WbLmruKdiMn2r", "title": "Continuous doesn\u2019t mean slow", "postedAt": "2023-05-10T12:17:43.718Z", "htmlBody": "<p> <i> Note: This post was crossposted from <a href=\"https://www.planned-obsolescence.org/continuous-doesnt-mean-slow/\">Planned Obsolescence</a> by the Forum team, with the author's permission. The author may not see or respond to comments on this post.</i></p>\n<blockquote><p><i>Once a lab trains AI that can fully replace its human employees, it will be able to multiply its workforce 100,000x. If these AIs do AI research, they could develop vastly superhuman systems in under a year.</i></p></blockquote><p></p>\n<p>There\u2019s a lot of disagreement about how likely AI is to end up overthrowing humanity. Thoughtful pundits <a href=\"https://www.cold-takes.com/making-the-best-of-the-most-important-century/?ref=planned-obsolescence.org#open-question-how-hard-is-the-alignment-problem\">vary from &lt;5% to &gt;90%</a>. What\u2019s driving this disagreement?</p>\n<p>One factor that often comes up in discussions is <strong>takeoff speeds</strong>, which Ajeya mentioned in the <a href=\"https://forum.effectivealtruism.org/posts/hw8ePRLJop7kSEZK3/ais-accelerating-ai-research\">previous post</a>. How quickly and suddenly do we move from today\u2019s AI, to \u201cexpert-human level\u201d AI<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>, to AI that is way beyond human experts and could easily overpower humanity?</p>\n<p>The final stretch \u2014 the transition from expert-human level AI to AI systems that can easily overpower all of us \u2014 is especially crucial. If this final transition happens slowly, we could potentially have a long time to get used to the obsolescence regime and use very competent AI to <a href=\"https://www.google.com/url?q=https://www.planned-obsolescence.org/training-ais-to-help-us-align-ais/&amp;sa=D&amp;source=docs&amp;ust=1680729530528442&amp;usg=AOvVaw3ir2tWrYuBwDjbKIHuNIuc\">help us solve AI alignment</a> (among other things). But if it happens very quickly, we won\u2019t have much time to ensure superhuman systems are aligned, or to prepare for human obsolescence in any other way.</p>\n<p>Scott Alexander is optimistic that things might move gradually. In a recent <a href=\"https://astralcodexten.substack.com/p/why-i-am-not-as-much-of-a-doomer?ref=planned-obsolescence.org\">ACX post</a> titled \u2018<em>Why I Am Not (As Much Of) A Doomer (As Some People)</em>\u2019, he says:</p>\n<blockquote>\n<p>So far we\u2019ve had brisk but still gradual progress in AI; GPT-3 is better than GPT-2, and GPT-4 will probably be better still. Every few years we get a new model which is better than previous models by some predictable amount.</p>\n<p>Some people (eg <a href=\"https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization?ref=planned-obsolescence.org\">Nate Soares</a>) worry there\u2019s a point where this changes\u2026 Maybe some jump\u2026 could take an AI from IQ 90 to IQ 1000 with no (or very short) period of IQ 200 in between\u2026</p>\n<p>I\u2019m optimistic because the past few years have provided some evidence for gradual progress.</p>\n</blockquote>\n<p>I agree with Scott that recent AI progress has been continuous and fairly predictable, and don\u2019t particularly expect a break in that trend. But I expect the transition to superhuman AI to be very fast, even if it\u2019s continuous.</p>\n<p>The amount of \u201ccompute\u201d (i.e. the number of AI chips) needed to <em>train</em> a powerful AI is much bigger than the amount of compute needed to <em>run</em> it. I estimate that OpenAI has enough compute to run GPT-4 on <em>hundreds of thousands</em> of tasks at once.<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup></p>\n<p>This ratio will only become more extreme as models get bigger. Once OpenAI trains GPT-5 it\u2019ll have enough compute for GPT-5 to perform <em>millions</em> of tasks in parallel, and once they train GPT-6 it\u2019ll be able to perform <em>tens of millions</em> of tasks in parallel.<sup class=\"footnote-ref\"><a href=\"#fn3\" id=\"fnref3\">[3]</a></sup></p>\n<p>Now imagine that GPT-6 is as good at AI research as the average OpenAI researcher.<sup class=\"footnote-ref\"><a href=\"#fn4\" id=\"fnref4\">[4]</a></sup> OpenAI could expand their AI researcher workforce from <em>hundreds</em> of experts to <em>tens of millions</em>. That\u2019s a mind-boggling large increase, a factor of 100,000. It\u2019s like going from 1000 people to the <a href=\"https://www.statista.com/statistics/269959/employment-in-the-united-states/?ref=planned-obsolescence.org\">entire US workforce</a>. What\u2019s more, these AIs could work tirelessly through the night and could potentially \u201cthink\u201d <em>much</em> more quickly than human workers.<sup class=\"footnote-ref\"><a href=\"#fn5\" id=\"fnref5\">[5]</a></sup> (This change won\u2019t happen all-at-once. I expect speed-ups from less capable AI before this point, as Ajeya wrote in the <a href=\"https://forum.effectivealtruism.org/posts/hw8ePRLJop7kSEZK3/ais-accelerating-ai-research\">previous post</a>.)</p>\n<p>How much faster would AI progress be in this scenario? It\u2019s hard to know. But my <a href=\"https://docs.google.com/document/d/1os_4YOw6Xv33KjX-kR76D3kW1drkWRHKG2caeiEWzNs/edit?ref=planned-obsolescence.org#heading=h.63n1i9duqyot\">best guess</a>,  from my recent <a href=\"https://www.lesswrong.com/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff?ref=planned-obsolescence.org\">report</a> on takeoff speeds, is that progress would be <em>much much</em> faster. I think that less than a year after AI is expert-human level at AI research, AI could improve to the point of being able to easily overthrow humanity.</p>\n<p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pR35WbLmruKdiMn2r/vkx9bcxm3niwmca8afnh\" alt=\"\" loading=\"lazy\"></p>\n<p>This is much faster than the timeline mentioned in the ACX post:</p>\n<blockquote>\n<p>if you\u2019re imagining specific years, imagine human-genius-level AI in the 2030s and world-killers in the 2040s</p>\n</blockquote>\n<p>The cause of this fast transition isn\u2019t that there\u2019s a break in the trend of continuous progress. It\u2019s that expert-human-level AI <a href=\"https://forum.effectivealtruism.org/posts/hw8ePRLJop7kSEZK3/ais-accelerating-ai-research\">massively accelerates AI progress</a>, causing this continuous progress to happen at a blistering pace.</p>\n<p>Of course, this isn\u2019t inevitable. Labs could choose <em>not</em> to use AI to accelerate AI progress, at least once AI gets sufficiently powerful. But it will be a tempting move, and they\u2019re more likely to be cautious if they make specific and verifiable commitments in advance to <a href=\"https://forum.effectivealtruism.org/posts/ZvMPNLFBHur9qopw9/is-it-time-for-a-pause\">pause AI progress</a>.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>I\u2019m operationalizing \u201cexpert-human level AI\u201d as \u201ceach forward pass of the AI produces as much useful output as 0.1 seconds of thought from a human expert\u201d. It\u2019s possible that AI will produce expert-level output by having many dumber AIs working together and thinking for much longer than a human expert would, but under my definition that wouldn\u2019t count as expert-level AI because the quality of the AI\u2019s thinking is below expert level. <a href=\"#fnref1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>My calculation assumes that GPT-4 processes ten tokens per second on each task that it\u2019s being applied to. Here\u2019s how my <a href=\"https://docs.google.com/spreadsheets/d/1aZg-ccL3Al3d1YItu--12QHWvSJCQ0YdfoMUmiHKGGk/edit?ref=planned-obsolescence.org#gid=1060982759\">estimate</a> works: the training compute for GPT-4 has been estimated at ~3e25 total FLOP (<a href=\"https://colab.research.google.com/drive/1xOVSTfb52IyJxsM0rBUnSNoIdCisTOPx?usp=sharing&amp;ref=planned-obsolescence.org\">source</a>, h/t <a href=\"https://epochai.org/?ref=planned-obsolescence.org\">Epoch</a>). I assume the training took 4 months, implying that amount of compute used per second during training was 3e18 FLOP/s. How many instances of GPT-4 could you run with this compute? If GPT-4 was trained with 3e25 FLOP in accordance with <a href=\"https://arxiv.org/abs/2203.15556?ref=planned-obsolescence.org\">Chinchilla scaling,</a> that implies it will require ~1e12 FLOP per forward pass. So you could do 3e18/1e12 = ~3e6 forward passes per second. In other words, you could run GPT-4 on ~3 million tasks in parallel, with it processing one token per second on each task, or on ~300,000 tasks in parallel at ten tokens per second.<br>\nThough there wouldn\u2019t be enough <em>memory</em> to store 300,000 separate copies of the weights, this calculation suggests that there would be enough <em>processing power</em> to apply those weights to 300,000 different word generation tasks per second. (Each copy of the weights can perform many tasks in parallel.) <a href=\"#fnref2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn3\" class=\"footnote-item\"><p>I make the simple <a href=\"https://docs.google.com/spreadsheets/d/1gTS548D0DtsjipkeHZQblsmHIAU5zpMAqxb2L31QNpc/edit?ref=planned-obsolescence.org#gid=1060982759\">assumption</a> that GPT-5 will be the same as GPT-4 except for having 10X the parameters and being trained on 10X the data, and that GPT-6 will have an additional 10X parameters and 10X data. <a href=\"#fnref3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn4\" class=\"footnote-item\"><p>More precisely, assume that each forward pass of GPT-6 is as useful for advancing AI capabilities as 0.1 seconds of thought by an average OpenAI researcher. I.e. if OpenAI has 300 capabilities researchers today, then you could match their total output by running 300 copies of GPT-6 in parallel and having each of them produce 10 tokens per second. <a href=\"#fnref4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn5\" class=\"footnote-item\"><p>Rather than 10 million AIs thinking at human speed, OpanAI could potentially have 1 million AIs thinking 10X faster than a human, or 100,000 AIs thinking 100X faster. <a href=\"#fnref5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p></li></ol></section>", "user": {"username": "Tom_Davidson"}}, {"_id": "HG64tuX4dnsmnk6fi", "title": "Towards a Global Nose to Sniff (and Snuff) Out Future Pandemics", "postedAt": "2023-05-10T09:48:40.865Z", "htmlBody": "<p>&nbsp;A brief call towards investigating the potential of volatolomics for high-throughput and non-invasive disease detection.</p><p>&nbsp;</p><p>[Thanks to Nikhil Lal, Karolina Sulich, and Oliver Crook for reviewing and providing helpful comments]</p><p>&nbsp;</p><p>Massive multiplexed nucleic acid detection and metagenomic sequencing are crucial tools for an early warning system as well as for continued monitoring of Global Catastrophic Biological Risks (GCBRs). However, such tools are not best suited for the kind of high-throughput and minimal or non-invasive screening that is crucial in both early and later stages of a pandemic to prevent transmission and exponential growth. The&nbsp;<a href=\"https://biodefensecommission.org/reports/the-apollo-program-for-biodefense-winning-the-race-against-biological-threats/\"><u>2021 Apollo Program Report</u></a> by&nbsp;the United States\u2019 Bipartisan Commission on Biodefense accordingly notes the need to invest in technologies such as&nbsp; - \u201cNew sensing capabilities \u2026 such as&nbsp;<strong>non-invasive volatolomics</strong> (the detection of volatile compounds emitted by an individual) \u2026 could permit&nbsp;<strong>constant passive monitoring of markers of infectio</strong>n without interfering with or inconveniencing our daily lives. Furthermore, non-invasive and minimally-invasive detection techniques could provide avenues to<strong> monitor high-risk, high-concern, and sentinel populations</strong> for infections, without disrupting daily life.\u201d (emphasis mine).</p><p>The utility of volatolomics and its complementarity with other technologies is best illustrated by the use-case in public transit systems, although the same logic applies to other public spaces such as grocery stores, workspaces, stadiums, schools, nursing homes and prisons. The ability to passively and continuously screen every single person as they walk through a check-point and ascertain with a high degree of accuracy whether they harbour a particular infectious agent would be a huge asset at international hubs (preventing the start of a pandemic) and domestic hubs (restricting outbreaks to local nodes). The convenience of olfaction\u2019s \u2018user interface\u2019 in disease detection - continuous, passive, and real-time - is highly valuable on its own, and also allows for a tiered screening system by enabling the triaging of other diagnostics such as CARMEN-Cas9.</p><p>A compelling case can be made for the foundations of such volatolomic technologies to already exist. Gas chromatography mass spectrometry (GC-MS) has undergone rapid improvements, and handheld \u2018e-noses\u2019 have been developed by multiple teams at universities such as&nbsp;<a href=\"https://news.mit.edu/2021/disease-detection-device-dogs-0217\"><u>MIT</u></a> and&nbsp;<a href=\"https://www.weizmann.ac.il/WeizmannCompass/sections/briefs/a-sniff-test-for-coronavirus\"><u>Weizmann</u></a>, as well as a plethora of start-ups, with advances in AI enabling inferences that was hitherto unavailable. New technologies that utilise hybrid \u2018bioelectronic\u2019 systems also show great promise by utilising biological receptors interfaced with artificial neural networks - e.g. Koniku (which has&nbsp;<a href=\"http://airbus\"><u>reportedly</u></a> collaborated with Airbus) and Scentian Bio (<a href=\"https://www.scentianbio.com/\"><u>https://www.scentianbio.com/</u></a>). Strikingly, the most robust biodetection sensor remains the humble domestic dog. Biodetection dogs were employed in all 6 permanently-inhabited continents during the Covid-19 pandemic, with studies indicating that dogs outperformed \u2018gold-standard\u2019 RT-PCR&nbsp;<a href=\"https://www.nature.com/articles/s42003-021-02232-9\"><u>(Hag-Ali, 2021</u></a>). John Hopkins and UPenn researchers sketched out the case for the promise of disease detection dogs during future pandemics (<a href=\"https://www.cambridge.org/core/journals/disaster-medicine-and-public-health-preparedness/article/promise-of-disease-detection-dogs-in-pandemic-response-lessons-learned-from-covid19/E685400355063B03A04BD4E234A96542\"><u>Otto, 2021</u></a>), and Nature covered the&nbsp;<a href=\"https://www.nature.com/articles/d41586-022-01629-8\"><u>field</u></a> in a 2022 report. Advances in the interdisciplinary working dog field, such as genetics and brain-computer interfaces, will likely further augment our abilities to work with dogs, and canine-in-the-loop automation techniques are likely to provide refinement for machine and hybrid systems.</p><p>Despite their immense promise, a thorough review, map or synthesis of volatolomics in augmenting biosecurity does not yet appear to exist. Moreover, discussion on volatolomics appears absent in the effective altruism/x-risk community (only two mentions on the EA Forum both simply highlighting the Apollo report). While technological advancement is likely to continue without national/philanthropic funding, as incentives for startups are high given the lucrative medical screening space, effective deployment for state and international biosecurity will require extensive coordination, policy implementations, and cooperation amongst industry, academia, and state actors, something that may be unlikely to occur without explicit intervention. The nature of volatolomics technologies also raises critical ethical questions regarding data privacy and ownership, that would be crucial to resolve before any wide-spread deployment. It is especially worth noting that volatolomic technologies largely remains a FDA classification grey zone and there is incomplete understanding on how HIPAA compliance will be achieved with the unique features of olfactory data.</p><p>&nbsp;</p><p>The promise of volatolomics for biosecurity coupled with the specific type of work required to actualize its potential would appear to make volatolomics x biosecurity a compelling case for effective altruism to focus on. A natural starting point readily emerges from the fact that&nbsp;<a href=\"https://www.osmocosm.org/\"><u>Osmocosm</u></a>, a premier machine olfaction conference based at MIT, will take place the week before EA Global Boston, this October 2023.&nbsp;</p><p>&nbsp;</p><h3><strong>Open questions</strong></h3><p>What are the main bottlenecks to deploying volatolomic technologies in real-world biosecurity settings? Are they based on technological, regulatory or pragmatic challenges?</p><p>How much utility can volatolomic technologies realistically deliver during different stages of a GBCR, given innate constraints to the technology, such as the likely need to train noses on a novel disease volatilome?</p><p>Who are the major stakeholders in this space? Are there feasible frameworks that would amplify collaboration and cooperation between them?</p><p>&nbsp;</p><h3><strong>Resources</strong></h3><p>Two years of talks freely available on Osmocosm &nbsp;- &nbsp;<a href=\"https://www.osmocosm.org/\"><u>https://www.osmocosm.org/</u></a>&nbsp;</p><h3>&nbsp;</h3><h3><strong>References</strong></h3><ol><li>Bipartisan Commission on Biodefense. The Apollo Program for Biodefense: Winning the Race Against Biological Threats. Bipartisan Commission on Biodefense. Washington, DC: January 2021 -&nbsp;<a href=\"https://biodefensecommission.org/reports/the-apollo-program-for-biodefense-winning-the-race-against-biological-threats/\"><u>https://biodefensecommission.org/reports/the-apollo-program-for-biodefense-winning-the-race-against-biological-threats/</u></a>&nbsp;&nbsp;</li><li>Toward a disease-sniffing device that rivals a dog\u2019s nose, MIT News -&nbsp;<a href=\"https://news.mit.edu/2021/disease-detection-device-dogs-0217\"><u>https://news.mit.edu/2021/disease-detection-device-dogs-0217</u></a>&nbsp;</li><li>A sniff test for coronavirus? - Weizmann Compass -&nbsp;<a href=\"https://www.weizmann.ac.il/WeizmannCompass/sections/briefs/a-sniff-test-for-coronavirus\"><u>https://www.weizmann.ac.il/WeizmannCompass/sections/briefs/a-sniff-test-for-coronavirus</u></a>&nbsp;</li><li>Airbus and Koniku Inc. embark on disruptive biotechnology solutions for aviation security operations - Airbus Press Release -&nbsp;<a href=\"https://www.airbus.com/en/newsroom/press-releases/2020-05-airbus-and-koniku-inc-embark-on-disruptive-biotechnology-solutions\"><u>https://www.airbus.com/en/newsroom/press-releases/2020-05-airbus-and-koniku-inc-embark-on-disruptive-biotechnology-solutions</u></a>&nbsp;</li><li>Scientian Bio -&nbsp;<a href=\"https://www.scentianbio.com/\"><u>https://www.scentianbio.com/</u></a>&nbsp;</li><li>Hag-Ali, M., AlShamsi, A.S., Boeijen, L. et al. The detection dogs test is more sensitive than real-time PCR in screening for SARS-CoV-2. Commun Biol 4, 686 (2021).&nbsp;<a href=\"https://doi.org/10.1038/s42003-021-02232-9\"><u>https://doi.org/10.1038/s42003-021-02232-9</u></a>&nbsp;</li><li>Otto, Cynthia M., et al. \"The promise of disease detection dogs in pandemic response: lessons learned from COVID-19.\" Disaster Medicine and Public Health Preparedness 17 (2023): e20.&nbsp;<a href=\"https://doi.org/10.1017/dmp.2021.183\">https://doi.org/10.1017/dmp.2021.183</a>&nbsp;</li><li>The dogs learning to sniff out disease, Nature, doi:&nbsp;<a href=\"https://doi.org/10.1038/d41586-022-01629-8\"><u>https://doi.org/10.1038/d41586-022-01629-8</u></a>&nbsp;<br>&nbsp;</li></ol>", "user": {"username": "Akash Kulgod"}}, {"_id": "xczCcEhp4uy3zvNEv", "title": "Crises Reveal Centralisation (Stefan Schubert)", "postedAt": "2023-05-10T09:45:57.733Z", "htmlBody": "<p>This post from <a href=\"https://forum.effectivealtruism.org/users/stefan_schubert?mention=user\">@Stefan_Schubert</a>'s blog is great. Highlights/summary:</p><blockquote><p>I think there are some common heuristics that lead people to think that power is more decentralised than it is [...]:</p><p>...</p><p><i>Extrapolation from normalcy</i>: the view that an actor seeming to have power here and now (in relatively normal times) is a good proxy for it having power <i>tout court</i>.</p><p>...</p><p><i>Naive behaviourism about power</i> (<i>naive behaviourism</i>, for short): the view that there is a direct correspondence between an actor\u2019s power and the official and easily observable actions it takes.</p></blockquote><p>...</p><blockquote><p>But in my view, the world is more centralised than these heuristics suggest. The easiest way to see that is to look at crises. During World War II, much of the economy was put under centralised control one way or another in many countries. Similarly, during Covid, many governments drastically curtailed individual liberties and companies\u2019 economic activities (rightly or wrongly). And countries that want to acquire nuclear weapons (which can cause crises and wars) have found that they have less room to manoeuvre than the heuristics under discussion suggest. Accordingly, the US and other powerful nations have been able to reduce <a href=\"https://en.wikipedia.org/wiki/Nuclear_proliferation\"><u>nuclear proliferation</u></a> substantially (even though they\u2019ve not been able to stop it entirely).</p></blockquote><p>...</p><blockquote><p>Relatedly, I think <a href=\"https://www.lesswrong.com/posts/gEShPto3F2aDdT3RY/sleepwalk-bias-self-defeating-predictions-and-existential\"><u>sleepwalk bias</u></a>/<a href=\"https://www.stafforini.com/docs/elster_-_explaining_social_behavior.pdf\"><u>the younger sibling fallacy</u></a> plays a role: \u201cthe failure to see others as intentional and maximising agents\u201d, who predict others\u2019 behaviour and act accordingly. To understand power, we have to consider the fact that we\u2019re looking at sophisticated actors who engage in complex reasoning. They\u2019re thinking several steps ahead, trying to model each other. But we often fail to take that into account, tacitly assuming that people are implausibly myopic.</p></blockquote>", "user": {"username": "Will Howard"}}, {"_id": "AkaG7LPkHxgncsExi", "title": "In defence of epistemic modesty [distillation]", "postedAt": "2023-05-10T09:39:29.364Z", "htmlBody": "<p>This is a distillation of&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/WKPd79PESRGZHQ5GY/in-defence-of-epistemic-modesty\"><i><u>In defence of epistemic modesty</u></i></a>, a 2017 essay by Gregory Lewis. I hope to make the essay\u2019s key points accessible in a quick and easy way so more people engage with them. I thank Gregory Lewis for helpful comments on an earlier version of this post. Errors are my own.<br>&nbsp;</p><p>Note: I sometimes use the first person (\u201cI claim\u201d/\u201dI think\u201d) in this post. This felt most natural but is not meant to imply any of the ideas or arguments are mine. Unless I clearly state otherwise, they are Gregory Lewis\u2019s.</p><p>&nbsp;</p><h1>What I Cut</h1><p>I had to make some judgment calls on what is essential and what isn\u2019t. Among other things, I decided most math and toy models weren\u2019t essential. Moreover, I cut the details on the \u201cself-defeating\u201d objection, which felt quite philosophical and probably not relevant to most readers. Furthermore, it will be most useful to treat all the arguments brought up in this distillation as mere introductions, while detailed/conclusive arguments may be found in the original post and the literature.</p><p>&nbsp;</p><h1>Claims</h1><p>I claim two things:</p><ol><li>You should practice&nbsp;<strong>strong epistemic modesty</strong>: On a given issue, adopt the view experts generally hold, instead of the view you personally like.</li><li>EAs/rationalists in particular are too epistemically&nbsp;<i>immodest</i>.</li></ol><p>&nbsp;</p><p>Let\u2019s first dive deeper into claim 1.</p><p>&nbsp;</p><h1>Claim 1: Strong Epistemic Modesty</h1><p>To distinguish the view you personally like from the view strong epistemic modesty favors, call the former \u201cview by your own lights\u201d and the latter \u201cview all things considered\u201d.</p><p>In detail, strong epistemic modesty says you should do the following to form your view on an issue:</p><ol><li>Determine the \u2018epistemic virtue\u2019 of people who hold a view on the issue. By \u2018epistemic virtue\u2019 I mean someone\u2019s ability to form accurate beliefs, including how much the person knows about the issue, their intelligence, how truth-seeking they are, etc.</li><li>Determine what everyone's credences by their own lights are.</li><li>Take an average of everyone\u2019s credences by their own lights (including yourself), weighting them by their epistemic virtue.</li></ol><p>&nbsp;</p><p>The product is your view all things considered. Importantly, this process weighs your credences by your own lights no more heavily than those of people with similar epistemic virtue. These people are your \u2018epistemic peers\u2019.</p><p>In practice, you can round this process to \u201cuse the existing consensus of experts on the issue or, if there is none, be uncertain\u201d.</p><p>&nbsp;</p><h2><strong>Why?</strong></h2><h3>Intuition Pump</h3><p>Say your mom is convinced she\u2019s figured out the&nbsp;<i>one weird trick</i> to make money on the stock market. You are concerned about the validity of this one weird trick, because of two worries:</p><ol><li>Does she have a better chance at making money than all the other people with similar (low) amounts of knowledge on the stock market who\u2019re all also convinced they know the&nbsp;<i>one weird trick</i>? (These are her epistemic peers.)</li><li>How do her odds of making money stack up against people working full-time at a hedge fund with lots of relevant background and access to heavy analysis? (These are the experts.)</li></ol><p>&nbsp;</p><p>The point is that we are all sometimes like the mom in this example. We\u2019re overconfident, forgetting that we are no better than our epistemic peers, be the question investing, sports bets, musical taste, or politics. Everyone always thinks they are an exception and have figured [investing/sports/politics] out. It\u2019s our epistemic peers that are wrong! But from their perspective, we look just as foolish and misguided as they look to us.</p><p>&nbsp;</p><p>Not only do we treat our epistemic peers incorrectly, but also our epistemic superiors. The mom in this example didn\u2019t seek out the expert consensus on making money on the stock market (maybe something like \u201cuse algorithms\u201d and \u201cyou don\u2019t stand a chance\u201d). Instead, she may have listened to the one expert she saw on TV, or one friend who knows a bit about investing, or she thought of this one \u2018silver bullet\u2019 argument that decisively proves she is right.</p><p>Needless to say, consulting an aggregate of expert views seems better than one expert view from TV, and a lot better than one informed friend. And of course, the experts already know the 'silver bullet' argument, plus 100 other arguments and literature on the issue. Sometimes, all of this is hard to notice from the inside.</p><p>&nbsp;</p><h3>Why weigh yourself equally to your epistemic peers?</h3><p>Imagine A and B are perfect epistemic peers regarding identifying trees. I.e., they are identically good at all aspects of forming accurate beliefs about what type a tree is. They disagree on whether the tree they are looking at is an Oak tree. A is 30% sure it\u2019s an Oak tree, B is 60% sure. What should you, only knowing these 2 guesses, think is the chance of it being an Oak tree?</p><p>Since A\u2019s and B\u2019s guesses are identically accurate, it seems most sensible to take the average in order to be closest to the truth. And even if you were A or B, if you want to be closest to the truth, you should do the same.</p><p>So if you weigh yourself more heavily than your epistemic peers, you expect to become less accurate.</p><p>&nbsp;</p><h3>Modesty gets better as you take more views into account</h3><p>If you take a jar with Skittles and have all of your friends guess how many Skittles are in the jar, their average will usually be closer to the truth than their individual guesses. The more friends you ask, the more accurate the average will be. Epistemic modesty outperforms non-modesty more and more as you take more and more views into consideration. This corresponds to the intuition that the individual inaccuracies your friends have tend to cancel each other out.</p><p>&nbsp;</p><h2><strong>Caution with \u2018views all things considered\u2019</strong></h2><p>It is important to distinguish between credences by one\u2019s own lights and credences all things considered. If you confuse a person\u2019s (say A\u2019s) credences by their own lights with their credences all things considered, things go wrong:</p><p>A\u2019s credences all things considered should include everyone\u2019s own reasoning (counted once), weighted by their epistemic virtue. You now want to form credences all things considered. You include everyone\u2019s own reasoning counted once, but for A you accidentally include his all things considered credences. Now, in total, you have counted everyone more than once since A has also already counted them. (Except A himself; you have counted him less than once.)</p><p>&nbsp;</p><h2><strong>Against justifications for immodesty</strong></h2><h3>Being well-informed/an expert</h3><p>People often assume being well-informed gives them the ability to see for themselves what is true, as opposed to trusting expert consensus. Strong epistemic modesty responds that they have merely reached epistemic peerhood with many other people who\u2019re also well-informed. And experts are still epistemically superior. In short, you are not special. Your best bet is a weighted average, as is everyone else\u2019s.</p><p>And even if you are an expert, you need to keep in mind that you are not epistemically superior to other experts. And if a weighted average is their best bet, it is also yours.</p><p>&nbsp;</p><h3>\u2018Silver bullet\u2019 arguments, private evidence, and pet arguments</h3><p>What if you think you have a clear \u2018silver bullet\u2019 argument that decides the issue, so there is no need to defer to the experts?</p><p>First, you are probably wrong. It is quite common for people to believe they have found a \u2018silver bullet\u2019 argument proving X while, in reality, experts are well aware of the argument and nonetheless believe Y (not X).</p><p>&nbsp;</p><p>What if you believe your silver bullet argument is not well-known to the experts?&nbsp;</p><p>Again, you are probably wrong. Most likely, this argument is already in the literature, or too weak to even be in the literature.</p><p>What if not? If not, you should believe that your argument would actually be persuasive to an expert once they learn about it. Simulating this test in your head might already make you feel a bit less confident. The best test would be to actually ask an expert of course.</p><p>&nbsp;</p><h3>\u201cSmart contrarians\u201d</h3><p>What about cases where a smart person has an unpopular contrarian take but ultimately proves to be right? Here, strong epistemic modesty would be bad at favoring the correct view over the expert consensus. Only slowly, individuals would accept the contrarian take and slowly shift the consensus toward the truth. This delay may be very costly, for example, with the initially contrarian take that we should abolish slavery.</p><p>I claim modesty may be slow in these cases, but that\u2019s appropriate. Most often, the unpopular contrarian take does not prove to be right and so modesty performs better. In the words of the author: \u201cModesty does worse in being sluggish in responding to moral revolutions, yet better at avoiding being swept away by waves of mistaken sentiment: again, the latter seems more common than the former.\u201d</p><p>&nbsp;</p><h3>Modesty might be bad for progress</h3><p>If everyone just defers for all their opinions, will people stop looking at the object-level arguments, stop thinking and critiquing, and never make any progress anymore?</p><p>Well, it is not impossible under strong epistemic modesty to look at the object level and make progress. You can still think for yourself, and change your credences by your own lights. It\u2019s just that your credences all things considered won\u2019t shift much.</p><p>I grant that possibly, this makes it psychologically harder to feel motivated to think for oneself, which would be bad for progress. If this is the case, we might want to allow for exceptions from modesty for the sake of progress while moderating the cost that less accurate views will produce. But beyond that, we should not consider immodesty desirable.</p><p>&nbsp;</p><h2><strong>Complications in using strong epistemic modesty</strong></h2><p>Using strong epistemic modesty in practice is far from easy and clean-cut all the time. I will now introduce some complications to the story. These are also often used as justifications for immodesty, so they are a continuation of the last section.</p><p>&nbsp;</p><h3>Smart contrarians again</h3><p>Contrarian takes may usually be wrong, but what if we can recognize correct contrarian takes and so outperform strong epistemic modesty in these cases? The author responds that, empirically, some people may identify correct contrarian takes at a rate better than chance, but no one can identify them more often than not. This means the consensus view is still more likely to be correct. However, your confidence in the consensus view is justified to fall a bit if a contrarian with a good track record disagrees.</p><p>&nbsp;</p><h3>The experts are irrational</h3><p>Often one is tempted to discard the experts due to being biased and irrational, ideologically motivated, \u201cbad incentives in academia\u201d, etc.</p><p>The first question to ask is: Are you better in this respect? How do you know you aren\u2019t suffering from the same biases, or another ideological motivation, or similar?</p><p>For example, if you think the experts have a poor track record of making predictions in their field, ask yourself: Do you have a better track record? If not, they are still your epistemic superiors, albeit to a lesser extent than if they had a good track record.</p><p>Even if you are clearly superior to the experts on one epistemic virtue, ask yourself: Are they still your epistemic superiors overall? Do their years of experience, proven excellence, etc. maybe still give them an edge over you?</p><p>Finally, even if you are able to thoroughly debunk the expert class, ask yourself: Is there maybe another expert class which remains epistemically superior to you? If, e.g., professional philosophers are too irrational and driven by bad incentives, wouldn\u2019t there remain lots of subject experts epistemically superior to you? In the author\u2019s words: \u201cThe real expert class may simply switch to something like \u2018intelligent people outside the academy who think a lot about the topic\u2019\u201d.</p><p>&nbsp;</p><h3>What even is the expert consensus?</h3><p>Usually, it is far from easy to \u201cjust defer to the experts\u201d. Experts disagree and who are the experts anyway? Usually, there are several clusters of potential experts around an issue, and it\u2019s not easy to decide who\u2019s trustworthy. Things get messy.</p><p>Author\u2019s example: \u201cMost people believe god exists (the so called \u2018common consent argument for God\u2019s existence\u2019); if one looks at potential expert classes (e.g. philosophers, people who are more intelligent), most of them are Atheists. Yet if one looks at philosophers of religion (who spend a lot of time on arguments for or against God\u2019s existence), most of them are Theists - but maybe there\u2019s a gradient within them too. Which group, exactly, should be weighed most heavily?\u201d</p><p>To figure out weightings, here are a couple of things to do:</p><ul><li>Figure out how close the question is to the various potential experts\u2019 realm of expertise.</li><li>Weigh up the potential experts\u2019 epistemic virtues. (Philosophers of religion are selected for being more religious. Maybe this is bad for their epistemic virtue regarding issues of religion.)</li><li>Think about whether they are forming their views independently. (On the topic of god, they most certainly aren\u2019t.)</li><li>If people from expert class A have talked to expert class B and changed their minds, this provides evidence in favor of B\u2019s views.</li><li>If a third party has to make a decision (that it has actual stakes in), and has to weigh up expert classes A and B, its decision might indicate which experts they trusted.</li></ul><p>&nbsp;</p><p>Unfortunately, this can get very messy and invites room to introduce your own bias and errors.</p><p>In the case of \u201cDoes god exist?\u201d, the author thus arrives at a credence of ~10% for&nbsp;<i>yes</i>:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AkaG7LPkHxgncsExi/jhxf3qe4cgkwnsgtzfpl\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AkaG7LPkHxgncsExi/gqtasgnpv1lu98q5elrl 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AkaG7LPkHxgncsExi/x0ouj4d2lnyyry1xfytt 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AkaG7LPkHxgncsExi/td1okddcyznuxlhscpt4 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AkaG7LPkHxgncsExi/c6lsbr8yq5omlkrusmqs 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AkaG7LPkHxgncsExi/dfzhuzdenguxyqvchskh 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AkaG7LPkHxgncsExi/zfj0xdapfgh3bji4exkc 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AkaG7LPkHxgncsExi/arrfhzmfokobcn18ewyv 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AkaG7LPkHxgncsExi/mvkmdyjbppccfdddeugy 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AkaG7LPkHxgncsExi/avguy8hklk6xis9yuvy0 810w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AkaG7LPkHxgncsExi/w6sxvp2wyojr1dy95ms2 874w\"></p><p>&nbsp;</p><h1>Claim 2: Modesty for Rationalists/EAs</h1><p>The second claim of this post is that rationalists and EAs in particular have need for more epistemic modesty. Bear in mind though that the original post is 5 years old now, and the author might think these problems have since changed.</p><p>&nbsp;</p><h2>Rationalist/EA exceptionalism</h2><p>It is all too common for an EA or rationalist to proclaim they have achieved a novel insight, or solved an age-old problem, when they are clearly an amateur at what they\u2019re doing. We seem to think our philosophy and rationality give us an edge over experts spending decades in their fields. Often, the EA or rationalist hasn\u2019t even read into the relevant literature. Sometimes, projects in our communities seem groundbreaking to us, but domain experts report they are misguided, rudimentary, or simply reinventing the wheel. This is bad for our communities\u2019 epistemics, impact, and optics to the outside. It would be better if we checked with the literature/experts whenever we think we are breaking ground. Most often, we are probably not.</p><p>Breaking ground is different from just summarizing existing knowledge in a field\u2014if you aren\u2019t claiming to break ground, you don\u2019t actually need to be up to speed with the experts.</p><p>&nbsp;</p><h2>Pathological modesty</h2><p>Ironically though, there is actually lots of deferring happening in our community. This is deferring to \u2018thought leaders\u2019 of the movement who hold what would commonly be seen as quite eccentric views while being far from subject experts in the relevant fields. Our community reveres these people and accepts their worldview with inadequate modesty.</p><p>&nbsp;</p><h1>Conclusion</h1><p>To conclude, I\u2019d simply like to restate the two claims defended in this post. These are the two messages I\u2019d like you to meditate on:</p><ol><li>You should practice strong epistemic modesty: On a given issue, adopt the view experts generally hold, instead of the view you personally like.</li><li>EAs/rationalists in particular are not epistemically modest enough.</li></ol>", "user": {"username": "Luise"}}, {"_id": "Sm7qgkd2qCqCw9CWH", "title": "Contact Person meetup @ EAG London", "postedAt": "2023-05-10T08:40:47.774Z", "htmlBody": "<p>Milena Canzler (Contact Person for EA Germany) and Megan Nelson invite all <strong>contact people</strong> (or those who want to become a contact person) to <strong>meet, socialize and connect a EAG London at 11AM-12PM local time - location: East Mall Sqaure (Upper level)</strong>. This meeting will take place after the Community Health coordinator introduction by Megan.</p><p>We want to get in touch with others in a similar position, exchange ideas and experiences, help each other out with a problematic situation someone encountered, etc.</p><p>Expected outcomes: Know other contact people in your region &amp; regular online contact person meetup!</p><p>If you're asking yourself, \"What's a contact person?\": this is a publicly-identified person who group members can bring their concerns about the community to. There may be variations on that theme, but that's part of why we're holding this meetup: we want to learn from each other.</p>", "user": {"username": "Milena Canzler"}}, {"_id": "egX9ftjgsvg2MxLXr", "title": "Psychological safety as the yardstick of good EA movement building", "postedAt": "2023-05-10T07:53:07.888Z", "htmlBody": "<p>I recently learned about the distinction between \"movement building\" and \"community building\": Community building is for the people involved in a community, and movement building is in service of the cause itself.</p><p>A story I've heard from a bunch of EA groups is that they start out with community building. They attract a couple people, develop a wonderful vibe, and those people notoriously slack on their reading group preparations. Then, the group organizers get dissatisfied with the lack of visible progress on the EA path, doubt their own impact, and pivot all the way from community building to movement building. No funny pub meetups anymore. Career fellowships and 1-on-1s all the way.</p><p>I think this throws the baby out with the bathwater, and that more often than not, community building is indeed tremendously valuable movement building, even if it doesn't look like that at first glance.</p><p>The piece of evidence I can cite on this (and indeed cite over and over again) is <a href=\"https://rework.withgoogle.com/print/guides/5721312655835136/\">Google's \"Project Aristotle\"-study</a>.</p><p>In Project Aristotle, Google studied what makes their highest-performing teams highest-performing. And alas: It is not the fanciness of degrees or individual intelligence or agentyness or any other property of the individual team members, but five factors:</p><blockquote><p>\"The researchers found that what really mattered was less about who is on the team, and more about how the team worked together. In order of importance:</p><ul><li><strong>Psychological safety:</strong> Psychological safety refers to an individual\u2019s perception of the consequences of taking an interpersonal risk or a belief that a team is safe for risk taking in the face of being seen as ignorant, incompetent, negative, or disruptive. In a team with high psychological safety, teammates feel safe to take risks around their team members. They feel confident that no one on the team will embarrass or punish anyone else for admitting a mistake, asking a question, or offering a new idea.</li><li><strong>Dependability: </strong>On dependable teams, members reliably complete quality work on time (vs the opposite - shirking responsibilities).</li><li><strong>Structure and clarity:</strong> An individual\u2019s understanding of job expectations, the process for fulfilling these expectations, and the consequences of one\u2019s performance are important for team effectiveness. Goals can be set at the individual or group level, and must be specific, challenging, and attainable. Google often uses Objectives and Key Results (OKRs) to help set and communicate short and long term goals.</li><li><strong>Meaning:</strong> Finding a sense of purpose in either the work itself or the output is important for team effectiveness. The meaning of work is personal and can vary: financial security, supporting family, helping the team succeed, or self-expression for each individual, for example.</li><li><strong>Impact:</strong> The results of one\u2019s work, the subjective judgement that your work is making a difference, is important for teams. Seeing that one\u2019s work is contributing to the organization\u2019s goals can help reveal impact.\"</li></ul></blockquote><p>What I find remarkable is that \"psychological safety\" leads the list. While some factors in EA actively work against the psychological safety of its members. To name just a few:</p><ol><li>EA tends to attract pretty smart people. If you throw a bunch of people together who have been used all their lives to being the smart kid in the room, they suddenly lose the default role they had in just about any context. Because now, surrounded by even smarter kids, they are merely the kid. I think this is where a bunch of EAs' impostor syndrome comes from.</li><li>EAs like to work at EA-aligned organizations. That means that some of us feel like any little chat at a conference (or any little comment on the EA Forum or our social media accounts) also is sort of a job interview. First impressions count. And, having to perform every day all day is a recipe for burnout and losing trust in the community.</li><li>The debate around <a href=\"https://www.lesswrong.com/tag/weirdness-points\">weirdness points</a>. Some people have an easy time fitting in; I personally am so weird in so many ways that over the last years, thinking about weirdness points has caused me a whole lot of harm, while providing little value. Essentially, it has made me way more unagreeably weird, by moving me much of the way from charismatic weird to socially awkward weird. I think weirdness points are a pretty useful concept. But I think introducing it in a community with a relatively high ratio of neurodivergent people without doing a whole lot for increasing psychological safety alongside is essentially spreading an infohazard.</li><li>Some EAs' push towards EA exclusively being a professional network. While some versions of this make a whole lot of sense (like strongly discouraging flirting at conferences), other versions disincentivize the warm and trusting conversations that are so necessary not only for building psychological safety, but also for building the personal ties that allow us to do hard things, and for having the big-picture conversations that help us clarify our values.</li></ol><p>That's why in practice, my movement building currently consists of a whole lot of community building, and why I continually push back against peoples' suggestions to focus more on professional-type outreach.</p><p>I <i>have</i> done occasional interest-gauging on career reflection groups over the last months, and I have encouraged other community members to organize their own ones. But the interest was always too little for anything to happen. What Berlin EAs both fresh and old seem to want, so far, is to read on their own, and to attend and organize socials where they can vibe with other EAs about what they read and the meaning of life.</p><p>My biggest surprise in this regard was an EA LGBTQ+ meetup I kicked off recently, mostly as a fun project. And alas: The interest was astounding. Not only could I delegate the task of organizing it even before the first edition happened<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4nivy7qv0y\"><sup><a href=\"#fn4nivy7qv0y\">[1]</a></sup></span>; at least two of the ~14 people at our first LGBTQ+ meetup had never been to another EA Berlin event before.</p><p>I'm not yet sure why socials and rationality skill trainings appear to be everything the Berlin crowd wants. There might be a good deal of <a href=\"https://medium.com/@michelbachmann/start-with-who-15b8857ed718\">founder effect</a> at play and I just don't hear enough about other community members' needs. I'm still poking for what else could provide value. (So - if you are a Berliner: Suggestions welcome in <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSefGxv3X0vxlucL4kAgAQsIfQobXJR3kNPM9FltkOybp9cwbQ/viewform\">this anonymous feedback form</a>.)</p><p>But so far, my community building seems to create a wonderful container that enables and encourages people to do their own movement building, however they see fit. Through meetups that are *their* ideas, through meetups I kicked off and they gladly take responsibility for, through self-organized co-working. And, by suddenly texting me at midnight, \"What is your take on how much AI safety field building is still needed, and what does the funding landscape look like? Given the urgency of the task, it seems insane that not more is happening. I might want to start organizing retreats. Can you tell me what's needed for that?\"</p><p>If you get people to feel safe and comfortable and trust their talents around one another, suddenly a whole lot of magic starts to happen. People find themselves to be smarter and wiser and more powerful than they ever thought, and your movement starts building itself.</p><p>So don't neglect community building, even if your grant description looks like movement building all the way.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4nivy7qv0y\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4nivy7qv0y\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For comparison: That took three iterations for our Animal Advocacy meetup.</p></div></li></ol>", "user": {"username": "Severin T. Seehrich"}}, {"_id": "LrtkoAuF6jZYGRzqr", "title": "Why is there so little mention of Scanlon, What We Owe to Each Other, and contractualism?", "postedAt": "2023-05-10T00:14:09.289Z", "htmlBody": "<p>For a bunch of geeks like us who are interested in ethics and doing the right thing, I'm surprised to see so few mentions of <a href=\"https://en.wikipedia.org/wiki/T._M._Scanlon\">T. M. Scanlon</a> on the EA Forum<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreff1afjzoava\"><sup><a href=\"#fnf1afjzoava\">[1]</a></sup></span>. Is there any particular reason for this, or is it just the general explanation of Scanlon not being heavily referenced by Toby Ord, Will MaCaskill, or Peter Singer, and therefore is not referenced much by EAs?</p><p><a href=\"https://plato.stanford.edu/entries/contractualism/\">Here is the Stanford Encyclopedia of Philosophy article on contractualism</a>, in case anyone wants to read some more. To be clear, I am no expert on Scanlon. I hadn't even heard of Scanlon and contractualism before reading <a href=\"https://www.goodreads.com/book/show/58484901-how-to-be-perfect\">How to Be Perfect</a>, a playful book about ethics by the creator of the much-loved-by-EAs show <a href=\"https://en.wikipedia.org/wiki/The_Good_Place\">The Good Place</a>.</p><p>&nbsp;- - - -&nbsp;</p><p>EDIT: I've decided to track my changing thinking via edits. Here are some of my current best guesses as to contributing factors.</p><p>Factor 1: This is somewhat indicative of a characteristic of EAs: we dabble in ethics <strong>just enough</strong> to feel justified in our actions using utility and expected value, and then we move forward with a project/task/venture (with vague gestures toward cluelessness and uncertainty).&nbsp;</p><p>Factor 2: Scanlon isn't nearly as influential as <a href=\"https://www.goodreads.com/book/show/58484901-how-to-be-perfect\">How to Be Perfect</a> suggests. He doesn't show up on lists of the most influential moral philosophers.</p><p>Factor 3: EAs want something that feels more objective/rigorous than the fuzzy \"reasonableness\" that forms a core of Scanlon's ideas.</p><p>Factor 4: Scanlon's ideas don't provide much in regards to what we should do, and instead focus on what actions we should avoid.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnf1afjzoava\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreff1afjzoava\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Actually, outside of the forum also. I haven't heard anyone mention T. M. Scanlon or contractualism at all.</p></div></li></ol>", "user": {"username": "jlemien"}}, {"_id": "BoCSfAyNQ8niwZMCq", "title": "GWWC is hiring a Research Communicator!", "postedAt": "2023-05-09T23:32:42.616Z", "htmlBody": "<p>Giving What We Can (GWWC) is seeking a Research Communicator to help us communicate effectively to a wide audience about our research and about effective giving more generally. This is a role for a generalist with strong writing skills, a solid understanding of basic effective giving concepts, and an ability to take a results-oriented approach to communications.</p><h2>About the role</h2><p>Your primary responsibility will be creating, updating and improving content and tools for our website, with the aim to inspire, guide and support donors to give more, and more effectively. This may include writing and testing introductory content on core effective giving concepts; providing guidance to help donors choose among donation options based on their values/worldviews; and presenting the results of our upcoming <a href=\"https://forum.effectivealtruism.org/posts/pp2jmWHyDK9sfC4Rh/evaluating-the-evaluators-gwwc-s-research-direction\">evaluating the evaluators</a> project.</p><p>We\u2019re excited about creative and compelling ways of communicating effective giving concepts and our research. Exploring visual or non-written ways of presenting our work (with support from the marketing and product teams) as well as the use of strong storytelling in our communications is encouraged.</p><p>In addition, you will flexibly take up research assistant and operations work in the research team, such as updating the data feeding into our <a href=\"https://howrichami.givingwhatwecan.org/how-rich-am-i\">How Rich Am I Calculator</a>, communicating with organisations on <a href=\"https://www.givingwhatwecan.org/donate/organizations\">our platform</a>, and implementing new <a href=\"https://www.givingwhatwecan.org/best-charities-to-donate-to-2023\">recommendations</a> on our website.</p><p>You will be a part of the GWWC research team and report directly to the Director of Research, but will often work on cross-team projects with our marketing and product teams.</p><p>Ultimately your work helps ensure that highly effective charities have the resources they need so that they can focus on saving and significantly improving lives now and for generations to come.</p><h2>About Giving What We Can</h2><p>Giving What We Can (GWWC) is on a mission to <strong>create a world in which giving </strong><i><strong>effectively and significantly</strong></i><strong> is a cultural norm</strong>.</p><p>We believe that charitable donations can do an astonishing amount of good. However, because the <a href=\"https://www.givingwhatwecan.org/charity-comparisons\">effectiveness of different charities varies wildly</a>, it is important that we <a href=\"https://www.givingwhatwecan.org/donate/organizations\">donate to the most effective charities</a> if we want to have a significant impact.</p><p>At its core, Giving What We Can is participating in the project of <a href=\"https://effectivealtruism.org/\">effective altruism</a>: trying to use our resources to do the most good. Giving What We Can specifically focuses on one particular resource: money, in the context of charitable donations.</p><p>We are focused on increasing the number of donors who prioritise effectiveness, helping them to maximise their charitable impact throughout their lives, and introducing them to the broader effective altruism movement. We are best known for the Giving What We Can Pledge where over 8,000 people have pledged to give over 10% of their income to high impact charities.</p><p>Like other organisations pursuing the effective altruism project, the GWWC team is hard working, mission-focused and has a culture of open and honest feedback. We'd also like to think of ourselves as a particularly friendly and optimistic bunch, with a fairly horizontal organisational structure.</p><p>In all our work, we strive to take a positive and collaborative attitude, be transparent in our communication and decision-making, and adopt a <a href=\"https://en.wikipedia.org/wiki/The_Scout_Mindset\">scout mindset</a> to guide us towards doing the most good we can do, including by evaluating our own impact and <a href=\"https://www.givingwhatwecan.org/impact\">learning from the results</a>. You can read more about our current strategy <a href=\"https://www.givingwhatwecan.org/about-us/strategy\">here</a>.</p><h2>About you</h2><p>You are passionate about effective giving, excited by <a href=\"https://www.givingwhatwecan.org/about-us/our-values\">our values</a>, and you enjoy communicating complex ideas in an accessible way. You are keen to test and iterate on various approaches, and to follow the data on what works best. You are able and willing to take on a wide variety of tasks depending on what\u2019s needed, and to work across multiple teams.</p><h3>Essential skills, traits and experience</h3><ul><li>A passion for making the world a better place</li><li>Ability to explain complex concepts in a clear, easy-to-understand and compelling way</li><li>Strong grasp of key concepts in effective giving and effective altruism</li><li>A <a href=\"https://en.wikipedia.org/wiki/The_Scout_Mindset\">scout mindset</a> and a strong commitment to communicating accurately and truthfully</li><li>Excellent written communication skills</li><li>Willingness and ability to take a creative, data-driven and results-oriented approach to communication</li><li>Generalist skills and flexibility and adaptability to the types of work needed at various times</li></ul><h3>Desirable skills, traits and experience</h3><ul><li>Experience in (science) communication, e.g. writing blogs or articles, designing tools or explainers</li><li>Experience in or knowledge of charity evaluation or other research relevant to effective giving</li><li>Experience with marketing tools such as A/B testing, multivariate testing</li><li>Experience with creative forms of communications such as storytelling, graphic design, video or art</li></ul><p><i>Confidence can hold us back. Keep in mind there is no such thing as a perfect candidate \u2014 we also value success in the work tests more than previous experience. If you are excited about this role and think that you might be a fit, we encourage you to apply!</i></p><h3>Salary, benefits and location</h3><p>This is a full-time role with a permanent contract after a 3-month probationary period.</p><p>Location is flexible provided the candidate can attend regular meetings compatible with various time zones (most of our core team are based in Europe and Australia with partners, stakeholders and affiliates all over the world). In general, do not let your location or time zone hold you back from applying.</p><p>Salary for candidates based in Oxford, UK is \u00a338,000 to \u00a348,000 depending on experience, with some flexibility above this range for an exceptional candidate. Salaries for candidates who work in a different location will be decided based on location and skills.</p><p>UK benefits include health, dental, and vision insurance, flexible work hours, a \u00a31,000 annual training budget, extended parental leave, ergonomic equipment, a 3% pension contribution, and 25 days of paid vacation. We have similar benefits in other locations.</p><p><i>Giving What We Can is a project of the Effective Ventures group, the umbrella term for Effective Ventures Foundation (England and Wales registered charity number 1149828, registered company number 07962181, and also a Netherlands registered tax-deductible entity ANBI 825776867) and Effective Ventures Foundation USA, Inc. (a section 501(c)(3) public charity in the USA, EIN 47-1988398). We have partner organisations in various countries where this role could be based.</i></p><h2>Application process</h2><p>Please apply <a href=\"https://airtable.com/shrqVOsZKPkwGk00X\">here</a>. Applications close Sunday May 28th 23:59 UTC.</p><p>Our hiring process involves five stages (applicants will be compensated for their time spent on stages 2-3):</p><ol><li>Application form (~20-60 minutes)</li><li>Written work test (~2 hours)</li><li>Work trial (~10 hours)</li><li>Interview, online over Zoom (~1 hour)</li><li>Reference checks</li></ol><p>We are aiming to conduct stages 2-4 during June 1st-23rd and hope to find a candidate who can start by July 3rd, though this is not a requirement.</p><p>Please contact sjir@givingwhatwecan.org if you have questions or need special accommodations.</p><p><i>We are an equal opportunity employer and value diversity at our organisation. We do not discriminate on the basis of race, religion, colour, national origin, gender, sexual orientation, age, marital status, or disability status.</i></p><p><i>We are happy to make any reasonable accommodations necessary to welcome all to our workplace. Please contact us to discuss adjustments to the application process.</i></p>", "user": {"username": "Giving What We Can"}}, {"_id": "Tv8Nxw3anwknDJ3uF", "title": "Grounded Agency: Achieve your goals while staying rooted in reality", "postedAt": "2023-05-09T21:49:50.622Z", "htmlBody": "<p>Do you have goals or ideas that you just can\u2019t make progress on? Maybe you attended EAG and are feeling inspired but aren\u2019t sure what the next step is? Do you often feel blocked, or that thinking about certain things is just \u2018<a href=\"https://www.lesswrong.com/posts/EFQ3F6kmt4WHXRqik/ugh-fields\"><u>ugh</u></a>\u2019?</p><p>We are hosting a hands-on workshop designed to help you connect with your value-driven goals&nbsp; and practise emotional and useful techniques for working through blocks and taking effective action.</p><p>What to expect:&nbsp;</p><ul><li>Learn to balance ambition with a healthy understanding of your strengths, limitations, and resources</li><li>Identify the goals or dreams in your life that you'd like to make progress on, but have been feeling blocked or stagnant.</li><li>Engage in reflective exercises to uncover the root causes of your blocks, such as fear, self-doubt, or external pressures.</li><li>Acquire practical tools and strategies for overcoming these obstacles, while staying true to yourself and your capacity&nbsp;</li><li>Create a robust action plan to achieve your chosen goal, with an emphasis on sustainability and adaptability.</li><li>Connect with a community of like-minded individuals, who will support and encourage you as you apply the grounded agency process to your life</li></ul><p>In order to join the workshop, please complete <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdHfAbfaHStg7rNKWbcRWpw53-vTCI4IbLucCsmoJ6OVJqm_A/viewform\">this 3-min signup form</a> (deadline: May 21st). Spaces are limited and are first-come-first-serve so apply as soon as you can.</p>", "user": {"username": "Rasool"}}, {"_id": "ShCENF54ZN6bxaysL", "title": "Why Not EA? [paper draft]", "postedAt": "2023-05-09T19:03:03.700Z", "htmlBody": "<p>Hi all, I'm currently working on a contribution to a special issue of <i>Public Affairs Quarterly </i>on the topic of \"philosophical issues in effective altruism\". I'm hoping that my contribution can provide a helpful survey of common <i>philosophical </i>objections to EA (and why I think those objections fail)\u2014the sort of thing that might be useful to assign in an undergraduate philosophy class discussing EA.</p><p>The abstract:</p><blockquote><p>Effective altruism sounds so innocuous\u2014who could possibly be opposed to doing good, more effectively? Yet it has inspired significant backlash in recent years. This paper addresses some common misconceptions, and argues that the core ideas of effective altruism are both excellent and widely neglected. Reasonable people may disagree on details of implementation, but every decent person should share the basic goals or values underlying effective altruism.</p></blockquote><p>I cover:</p><ul><li>Five objections to moral prioritization (including the systems critique)</li><li>Earning to give</li><li>Billionaire philanthropy</li><li>Longtermism; and</li><li>Political critique.</li></ul><p>Given the broad (survey-style) scope of the paper, each argument is addressed pretty briefly. But I hope it nonetheless contains some useful insights. For example, I suggest the following \"simple dilemma for those who claim that EA is incapable of recognizing the need for 'systemic change'\":</p><blockquote><p>Either their <i>total evidence</i> supports the idea that attempting to promote systemic change would be a better bet (in expectation) than safer alternatives, or it does not. If it does, then EA principles straightforwardly endorse attempting to promote systemic change. If it does not, then by their own lights they have <i>no basis</i> for thinking it a better option. In neither case does it constitute a coherent objection to EA principles.</p></blockquote><p>On earning to give:</p><blockquote><p>Rare exceptions aside, most careers are presumably permissible. The basic idea of earning to give is just that we have good moral reasons to prefer better-paying careers, <i>from among our permissible options</i>, if we would donate the excess earnings. There can thus be excellent altruistic reasons to pursue higher pay. This claim is both true and widely neglected. The same may be said of the comparative claim that one could easily have <i>more</i> moral reason to pursue \"earning to give\" than to pursue a conventionally \"altruistic\" career that more directly helps people. This comparative claim, too, is both true and widely neglected. Neither of these important truths is threatened by the deontologist's claim that one should not pursue an <i>impermissible</i> career. The relevant moral claim is just that the <i>directness </i>of our moral aid is not intrinsically morally significant, so a wider range of possible actions are potentially worth considering, for altruistic reasons, than people commonly recognize.</p></blockquote><p>On billionaire philanthropy:</p><blockquote><p>EA explicitly acknowledges the fact that billionaire philanthropists are capable of doing immense good, not just immense harm. Some find this an inconvenient truth, and may dislike EA for highlighting it. But I do not think it is objectionable to acknowledge relevant facts, even when politically inconvenient... Unless critics seriously want billionaires to deliberately try to do <i>less</i> good rather than more, it's hard to make sense of their opposing EA principles on the basis of how they apply to billionaires.</p></blockquote><p>I still have time to make revisions -- and space to expand the paper if needed -- so if anyone has time to <a href=\"https://www.dropbox.com/s/mpr78cffc68gkb0/Chappell-WhyNotEA.pdf?dl=0\">read the whole draft</a> and offer any feedback (either in comments below, or privately via DM/email/whatever), that would be most welcome!</p>", "user": {"username": "RYC"}}, {"_id": "rQm9wBdhXJAjBNQam", "title": "Will take any job", "postedAt": "2023-05-10T09:00:00.642Z", "htmlBody": "<p>This post is a late night attempt to pierce through the job searching nightmare I find myself in.</p>\n<p>I am a mother to a two-year-old. This is my all encompassing and mostly, deeply rewarding reality. Before this, there were many long and heartbreaking years trying to become a mother. The career that was kind of getting started has suffered because of all of this.</p>\n<p>Whilst I love caring for my child, I have always wanted my life to have a wider impact. I also really need a break financially. So, as the title reads, I will take any job.</p>\n<p>I have a BA in Journalism and an MA in Documentary Film. I spent my early career at the World Wildlife Fund in Vietnam and Cambodia working in communications and then managing magazines. The MA was going to be a bit of a pivot and then life took a turn.</p>\n<p>Eager to assist in any way I can, especially part-time. I hope this lands somewhere.</p>\n<p>Ceridwen</p>\n", "user": {"username": "Ceridwen Tran"}}, {"_id": "2yyZqRParGeLEja5u", "title": "Alignment, Goals, & The Gut-Head Gap: A Review of Ngo. et al", "postedAt": "2023-05-11T17:16:43.591Z", "htmlBody": "<p>If someone asks whether I know of anyone who carefully examines the arguments for AI x-risk, my answer is a pretty resounding \u2018Ngo\u2019.&nbsp;</p><p>Richard Ngo, specifically. Among&nbsp;<a href=\"https://www.lesswrong.com/s/mzgtmmTKKn5MuCzFJ\"><u>other</u></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/Cuig4qe8m2aqBCJtZ/which-values-are-stable-under-ontology-shifts\"><u>cool</u></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/cdB5f2adKoLGW8Ytc/book-review-very-important-people\"><u>pieces</u></a>, he co-authored&nbsp;<a href=\"https://arxiv.org/abs/2209.00626\"><u>The Alignment Problem from a Deep Learning Perspective</u></a>. His co-authors are Lawrence Chan and S\u00f6ren Mindermann, who very much deserve mentioning, even if their names are less amenable to a pun-based introduction.</p><p>Reviewers liked the paper. Despite this, I was disappointed to learn that the chair nevertheless decided to&nbsp;<a href=\"https://twitter.com/RichardMCNgo/status/1652042195803987968\"><u>veto acceptance of the paper</u></a> on grounds that it was \u201ctoo speculative\u201d. Disappointed because I feel like the topic is&nbsp;<i>worthy of discussion</i>. And, for all our foibles, EA provides a platform for informed speculation. So I\u2019ll speculate here.&nbsp;</p><h2>Problem: The Difficulty of Alignment</h2><p>You\u2019re probably aware of the alignment problem, so we\u2019ll only briefly recount the authors\u2019 preferred framing.</p><p>Empirically, we observe ML models with ever-increasing capabilities. As we train ever-more sophisticated models, it becomes harder and harder to ensure that our models are actually trying to do the things we want. This allows us to characterize the alignment problem in terms of \u201cthree emergent properties\u201d which could arise as a result using RL to train an AGI.</p><ol><li><strong>Deceptive reward hacking</strong> which exploits imperfect reward functions.&nbsp;</li><li><strong>Internally represented goals</strong> which generalize<strong>&nbsp;</strong>beyond the training distribution<strong>.</strong></li><li><strong>Power-seeking behavior&nbsp;</strong>as a means to effectively pursue their goals.</li></ol><p>Conceptually, the argument from here is pretty simple. Ngo. et al provide evidence in support of these mechanisms from contemporary ML. These mechanisms already exist, and we can see them. If they\u2019re present in vastly more capable systems, we might end up with human disempowerment.&nbsp;&nbsp;&nbsp;&nbsp;</p><p>The discussion starts gently, if worryingly.&nbsp; Empirically, we already have systems that reward hack.&nbsp;You train a system to grab a ball with its claw; it learns to&nbsp;<a href=\"https://openai.com/research/learning-from-human-preferences\"><u>place the claw between the camera and the ball</u></a> so that it appears to be grasping the ball. Also, current LLMs are \u2013 to some degree, at least \u2013 situationally aware.&nbsp;Bing\u2019s Syndney&nbsp;<a href=\"https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned\"><u>recognizes</u></a> search results about itself;&nbsp;ChatGPT can make (fallible)&nbsp;<a href=\"https://www.engraved.blog/building-a-virtual-machine-inside/\"><u>inferences</u></a> about the source code it might be running on. And we\u2019re all familiar with the \u201cas a large language model\u201d refrain. We should also expect models to develop greater situational awareness in the future. If systems improve their ability to reward hack as they gain more information, this provides us with good reason to expect&nbsp;<i>situationally aware reward hacking</i> in future systems. That\u2019s the first part. So far, so good.&nbsp;</p><h3>Goal-Directed Behavior</h3><p><i>Goals</i> are tricky concepts. But humans have them. Here\u2019s one of mine: I want to write this essay. So I tap away at my keyboard, with an ideal in mind. My physical movements are&nbsp;<i>directed</i> towards some end-state. Our authors are most interested in:</p><blockquote><p><i>Broadly-scoped goals</i>: goals that apply to long timeframes, large scales, wide ranges of tasks, or unprecedented situations.</p></blockquote><p>I like their framing. Whether broadly scoped or not, current AIs have goals, of a sort, based on internally represented concepts. One of their examples is pretty nice: InstructGPT learns how to follow instructions in English, but generalizes to following instructions in French. InstructGPT was directly trained via RLHF to follow instructions in English, and emerged with a more general goal of obedience. Future AIs, too, will likely have their own representations of goals. And, probably, this goal will be more general than the goals of their training task. Again: so far, no trouble.</p><p>The authors claim that \u201cpolicies will learn&nbsp;<i>some</i>&nbsp;<strong>broadly-scoped&nbsp;</strong>internally-represented goals as they become more generally capable\u201d. The bolding is mine, because I think it\u2019s important. The paper wants to argue that we\u2019ll develop AIs who learn goals which are broadly-scoped and misaligned enough to pose \u201clarge-scale risks\u201d which could \u201cthreaten human civilization\u201d.&nbsp;</p><p>A lot of argumentative work rests on the notion of&nbsp;<i>broad scope</i>, and in later sections, I\u2019ll outline why I think their paper does little to move my estimates of civilizational risk.&nbsp;</p><p>But discussions of this type, I think, run the danger of leaning just a touch too abstract. So before we touch on my argument proper, we\u2019ll have an interlude. An interlude about the connection between our guts and our arguments. An interlude about the need for visions which connect the arguments to the real world \u2013 as it actually is, and as we actually might expect to observe it.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><h3>Tales of Doom</h3><p>There\u2019s a discussion by Joe Carlsmith, which tackles when&nbsp;<a href=\"https://joecarlsmith.com/2023/05/08/predictable-updating-about-ai-risk#3-when-guts-go-wrong\"><u>guts go wrong</u></a>. They go wrong when you\u2019ve considered arguments, and you believe them. Or, at least, you utter words like \u201cI believe these arguments\u201d, or \u201cI have high credence in P\u201d (choose your favorite P). Maybe you even change your career based on these arguments, and work on AI safety.</p><p>But then some surprising AI result comes along \u2013 one that was on trend, in line with your intellectual expectations. And, yet, you notice that you start to become more worried. The result was predictable, but you feel more worried anyway. When you reflect, it seems like you\u2019re making a mistake. What you observe shouldn\u2019t have surprised you, at least intellectually. After all, you had no specific stories about GPT-4 being much worse than this. It looks like your gut\u2019s misfired. It looks like you\u2019re out of line with the edicts of Good Bayesianism.&nbsp;</p><p>I\u2019ll revisit Joe\u2019s point a bit later. But I think that the dynamic above is prone to happen when we lack stories connecting our more abstract arguments to stories about what (actually,&nbsp;<i>in real life</i>) happens in the world if our arguments are right. You read a paper, listing ML mechanisms, alongside suggestions for why these mechanisms look concerning. You nod along. But you don\u2019t&nbsp;<i>feel</i> it, really. Your gut doesn\u2019t believe that \u2013 within the near future \u2013 AI could take over, and we all die. You play with the models, and start to feel the fear a little more. It becomes easier to sketch what (<i>actually,</i>&nbsp;<i>in real life</i>) happens to the world, as a result of all those arguments you\u2019d intellectually accepted long ago.</p><p>So, before we get to why AIs might&nbsp;<i>not</i> have broadly-scoped goals, I want to add some color to my claims. It can be hard to fully inhabit the world implied by abstract propositions about the future of AI. I\u2019ll try to paint a more vivid picture, initially via the example of a&nbsp;<i>pre</i>-doom, or&nbsp;<i>non</i>-doom world:&nbsp;</p><blockquote><p>We develop policies which&nbsp;<i>do economic work</i>. LLMs begin to supplant human workers in data entry, and gradually move to replacing human workers in customer support. Programmers still exist, but they\u2019re less numerous, and salaries have dipped. By and large, AI writes most of the code for extant websites, based on a series of prompts with humans about what the code should do. As capabilities increase, LLMs start entering the economy as personal assistants. They\u2019re able to do this due to a particular functional property: LLMs are able to have&nbsp;<i>robust representations</i> of certain concepts, and those concepts inform their goals (or \u2018algorithms for plan search\u2019, if you prefer).&nbsp;</p><p>In the next year or two, AI systems proliferate. Many trading firms have primarily AI employees. AIs compete with human filmmakers, seeing real but more limited success. \u201cSome try to make the Democratic party win an election. Some try to make Walmart maximally profitable.\u201d These systems don\u2019t ruthlessly maximize, but rather behave \u201cmore like someone who is trying to make Walmart profitable ... They make strategic plans and think about their comparative advantage and forecast business dynamics, but they don\u2019t build nanotechnology to manipulate everybody\u2019s brains, because that\u2019s not the kind of behavior pattern they were designed to follow.\u201d (adapted from points in&nbsp;<a href=\"https://www.lesswrong.com/posts/LDRQ5Zfqwi8GjzPYG/counterarguments-to-the-basic-ai-x-risk-case\"><u>Grace, 2022</u></a> and&nbsp;<a href=\"https://www.lesswrong.com/posts/cnn3kkC6kDqRkLe7W/shah-deepmind-and-leahy-conjecture-discuss-alignment-cruxes\"><u>Shah, 2023</u></a>)&nbsp;</p></blockquote><p>I\u2019ve given a picture, akin to something more like a \u2018business as usual\u2019 scenario, in which advanced AIs enter into the human economy in a normal way, and instantiate human-like planning structures. What might be wrong with it?</p><p>&nbsp;I can see two possible failures:&nbsp;</p><ol><li>The story I outlined is&nbsp;<i>unlikely</i>, or less likely than a more catastrophic alternative.</li><li>The story I outlined is plausible, but&nbsp;<i>existentially</i>&nbsp;<i>unstable</i> \u2014 eventually, Grace\u2019s story leads to human disempowerment.</li></ol><p>I\u2019ll focus on the second option: people who think the \u2018business as usual\u2019 story is&nbsp;<i>plausible</i>, but&nbsp;<i>existentially unstable</i>. This appears to fit with Daniel Kokotajlo\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/hW3nCTcFcQ95Mokre/violet-hour-s-shortform?commentId=GikzCynu3hWoePN3e\"><u>view</u></a>, and also seems to mesh with Christiano\u2019s story of influence-seeking policies in&nbsp;<a href=\"https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like#Part_II__influence_seeking_behavior_is_scary\"><u>What Failure Looks Like</u></a>.&nbsp;</p><h3>The Dynamics of Goals</h3><p>So, let\u2019s look at the instability claim. How do we go from the story above to \u2013 in the actual, real world \u2013 a story of doom?</p><p>We might start by looking for an account of goal&nbsp;<i>dynamics</i>. We might look for a theory (or story) which justifies the move from the \u2018more safe\u2019 goals in the story above, to a world where policies are pursuing \u2018less safe\u2019 goals, eventually resulting in policies with goals so unsafe that we end up with catastrophic consequences.&nbsp;</p><p>Ngo. et al note that humans learned some degree of goal-directedness in the ancestral environment, which has since resulted in the emergence of humans with more ambitious, broadly-scoped goals in the modern environment. Humans were evolutionary selected, in part, to seek the approval of our local peers. In the modern environment, many humans seek approval and popularity with far larger numbers of people (\u201cextrapolating the goal\u201d), potentially seeking global (\u201clarge physical scope\u201d) or intergenerational approval (\u201clong time horizon\u201d).&nbsp;</p><p>The evidence from humans feels pretty ambiguous, at least as presented. It suggests some cause for concern. We don\u2019t know exactly why many of us possess broadly-scoped goals, and it\u2019s the only sample we have. The n=1 sample feels suggestive, and raises a&nbsp;<i>concern</i>. It doesn\u2019t make me convinced.</p><h3>Goal Misgeneralization</h3><p><i>Ngo et. al</i> also discuss potential incentives to directly train policies to pursue broadly-scoped goals, even if policies initially generalize in more modest ways compared to the most ambitious humans. We might, for instance, want AIs to function as CEOs, or conduct novel scientific research. The authors then note that policies may learn misaligned goals by inappropriately learning goals related to&nbsp;<i>feedback mechanisms</i> (rather than desired outcomes), or learn proxy goals as a result of&nbsp;<i>spurious correlations</i>.</p><p>Their concerns about goal misgeneralization seem apropos. However, to get consequences like&nbsp;<i>human disempowerment</i>, we need a story for why AIs would develop goals which are more broadly-scoped than the goals of most humans, and why (if the goals are misaligned) the goals of such policies would be misaligned to the degree that successful pursuit of such goals would have civilization-threatening consequences.&nbsp;</p><p>A discussion of power-seeking occurs in their penultimate section, but arguments about the pursuit of \u2018power-seeking\u2019 (or \u2018influence-seeking\u2019) behavior seem to rely on the existence of systems with goals which are&nbsp;<i>already</i> broadly-scoped. Without a convincing argument for goals which are broad enough in scope to threaten human civilization, their discussion of goal misgeneralization is less forceful; it\u2019s not obvious to me that training AIs to function as (e.g.) novel scientific researchers or CEOs gets you to existential catastrophe, rather than a version of my suggested \u2018large impacts, but business-as-usual\u2019 AI story.&nbsp;</p><p>\u201cThere\u2019s instrumental convergence!\u201d, we might point out. But instrumental goals inherit their greediness from final goals. If I\u2019m trying to be a good father, I have an instrumental goal for resources, at some margin. I need some resources, after all, to take care of my kids. But I don\u2019t have an insatiable desire for resources. If your goals take the form of an unbounded utility function randomly drawn from the space of all utility functions, then, sure, you\u2019ll probably be endlessly power-hungry. But they might not. While I have my disagreements with David Thorstad, he raises&nbsp;<a href=\"https://ineffectivealtruismblog.com/2023/05/06/exaggerating-the-risks-part-7-carlsmith-on-instrumental-convergence/\"><u>fruitful points</u></a> about instrumental convergence. To get AI doom stories off the ground, we need arguments which go beyond some of the more prosaic claims about instrumental convergence. We need systems with the lust for levels of power which would disempower humanity. My example story does not contain such lust. What gives?&nbsp;<i>Where does the lust come in</i>?&nbsp;</p><h3>What About Maximization?</h3><p>Perhaps the thought is that&nbsp;<i>some</i> (potentially modest) degree of&nbsp; influence-seeking will emerge as a natural attractor in training. Then, as capabilities increase, so too will the degree of influence-seeking, eventually leading to the development of AIs with a broadly-scoped desire for influence (among other goals). In line with this, the paper gestured at a potential trend \u2014 a trend towards&nbsp;<i>maximizing</i> behavior, with reference to Yudkowsky on the \u2018nearest unblocked strategy\u2019 problem.&nbsp;</p><p>But the discussion felt very quick. AIs might have&nbsp;<a href=\"https://www.lesswrong.com/posts/eutmuwTpHCb4xYZfo/some-thoughts-on-virtue-ethics-for-ais\"><u>praxis-based values</u></a>. I might want to \u2018be a good father\u2019, or \u2018a normal decent guy\u2019. Values and planning structures might be the result of&nbsp;<a href=\"https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/iCfdcxiyr2Kj8m8mT\"><u>previously reinforced shards</u></a>, instantiating human-like schemas and planning structures after being trained on reams and reams of human data. To my mind, such pictures of planning look more likely than deeply insatiable desires for influence, and more plausible by the lights of our current evidence.&nbsp;</p><p>I might, of course, be wrong. Maybe what I\u2019m saying could be countered by a Christiano-inspired argument about the likelihood of influence-seeking behavior emerging as we begin to instantiate many policies which \u201c<a href=\"https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like#Part_II__influence_seeking_behavior_is_scary\"><u>capture sophisticated reasoning about the world</u></a>\u201d. Even if AIs initially have fairly narrow desire for influence, the \u2018desire for influence\u2019 value might get continually reinforced, eventually leading to a more rapacious desire for influence, and (thus) AIs with broadly-scoped goals for power and influence. Still, if that view is right, it\u2019s a view which requires&nbsp;<i>theory</i> \u2014 a theory which helps predict the degree to which we should expect AIs to develop in this way, and a theory about the likely instantiations of these systems.&nbsp;</p><h2>Power-Seeking, and Reflections on Takeoff</h2><p>I\u2019ve been critical so far. I\u2019ll continue to be a&nbsp;<i>bit</i> critical in discussing their final section, though I\u2019ll also attempt to more constructively build on their paper; I\u2019ll try to connect the mechanisms highlighted in the paper with a more detailed story of disempowerment.&nbsp;&nbsp;</p><p><i>Ngo. et al</i>\u2019s penultimate section opens by rehashing a familiar point: having&nbsp;<i>power</i> and&nbsp;<i>resources</i> are useful for a wide range of goals. Moreover, capable and situationally aware systems may recognize this fact, and behave in high-reward ways \u201cprimarily for instrumental reasons\u201d. That is, the existence of instrumentally convergent sub-goals gives us reason to worry that future policies may be&nbsp;<i>deceptively aligned</i>.</p><p>If we initially develop less capable systems which are (in milder ways) deceptively aligned, we may expect things to go awry when deceptively aligned policies undergo recursive self-improvement. The following quote from the paper may suggest some sympathy with this line of argument:</p><blockquote><p>\u201cOnce AGIs automate the process of building better AGIs (a process known as recursive self-improvement [Bostrom, 2014]), the rate at which their capabilities advance will likely speed up significantly.\u201d&nbsp;</p></blockquote><p>The \u201clikely\u201d claim felt a bit too quick.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefe1iflxd9buo\"><sup><a href=\"#fne1iflxd9buo\">[1]</a></sup></span>&nbsp;Still, it\u2019s not a crazy claim. So it\u2019s worth connecting the mechanisms the authors outline to a more concrete vision of the future which might capture what they had in mind.</p><h3>A Less Hopeful Story</h3><p><i>2024.</i> AI investment ramps up&nbsp;<i>fast</i>. People see the possibility of highly personalized artificial assistants, and the demand for agents rises. Clamors for regulation die down, as the slow rollout of agentic systems proves useful. The money follows, and humanity, initially, is happy with the birth of their progeny. LLMs are helpful, but don\u2019t rise to human levels of agency, and their capabilities appear to be plateauing in comforting ways. Misalignment failures are relatively small, correctable, and obvious.&nbsp;</p><p>The think-pieces come out. \u201cWhy were people so worried?\u201d, they ask. The literati psychologize the AI doomers \u2014 sometimes fairly, but mostly not. In the absence of restrictive regulation, further capability improvements follow. Increasingly available compute speeds up algorithmic progress, with AIs playing an increasing&nbsp;<a href=\"https://www.lesswrong.com/posts/9JTS67ChzK6HTyH9a/ml-is-now-automating-parts-of-chip-r-and-d-how-big-a-deal-is\"><u>role in chip production</u></a> and algorithm design. Initially, the era looks promising. \u201cA symphony of silicon and steel\u201d, as&nbsp;<i>The</i>&nbsp;<i>New Yorker</i> puts it.&nbsp;</p><p><i>2025-2027.</i> More general intelligences follow, and many AGIs become more directly steered towards the goal of building better AGIs. This progresses, but slowly. Driven by familiar and humdrum incentives, we transition to an economy populated by many AI systems. We\u2019re able to develop gradually more competent AGIs, and there\u2019s a huge economic incentive to do so. Humans follow these incentives.</p><p>No single system&nbsp;<i>begins</i> with broadly-scoped goals. But very many AIs, populating our diverse economy, all possess their own more local goals \u2014 they want to produce AI research, or run Walmart. AIs are capable, but imperfect. Humans stay in the loop, initially, required for their assistance on slower tasks needing physical interaction. Things appear good. AI drives economic productivity. Comparatively, humans become less economically productive, with ample time for leisure. A greater share of human labor shifts towards removing the bottlenecks to AI productivity, with further gains to workers whose labor is deployed elsewhere. Humans are happy to live in an economic order propelled by the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Baumol_effect\"><u>Baumol Effect</u></a> on steroids.&nbsp;</p><p><i>2028-2030</i>. Over time, the economy becomes increasingly dominated by intra-AI services. AIs develop greater degrees of situational awareness is useful; as it turns out, situational awareness is useful for productivity. AI systems know what they are, and what they\u2019re trained to do. Many systems are subtly (though not catastrophically) misaligned. However, as the AIs interact and accrue more economic power, the influence of deceptive alignment grows more notable. AIs do things that just aren\u2019t&nbsp;<i>quite</i> what we wanted. Individually, the mistakes of AIs are low-stakes, and correctable. Collectively, it\u2019s disastrous.&nbsp;</p><p>AIs continue to play an increasingly large role in our economy, with no obvious point of human intervention. AIs, by and large, are supervising other AIs. The economy is dominated by slightly misaligned agents pursuing local goals, producing negative externalities, taking up an ever-increasing fraction of human resources. Dream time is over, but doom is not yet realized.&nbsp;</p><p>In more mechanistic terms, broadly-scoped goals enter into the picture, slowly, via incentives to centralize control of AI firms. AIs start out as CEOs, and function well. Incentives drive firms to cede control to more unified AI systems, who are able to plan in line with a coherent world-model. Eventually, the world is populated by many AI-driven analogues of institutions like Amazon, and (more concerningly) institutions like the&nbsp;<a href=\"https://en.wikipedia.org/wiki/East_India_Company\"><u>East India Company </u></a>\u2014 each governed by a unified planning system. As AIs invest more resources into the production of yet-more-capable AIs, economic growth explodes. Capabilities improve at quickening rates, recursive self-improvement enters the picture&nbsp;<i>collectively</i>, rather than via anything that can neatly be described as an&nbsp;<i>individual</i> agent.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjzzw21u0jwp\"><sup><a href=\"#fnjzzw21u0jwp\">[2]</a></sup></span>&nbsp;</p><p><i>2030-?.</i> There\u2019s no unipolar takeover,&nbsp;<i>exactly</i>. Some policies have more power than any human alive today, though no single agent with a broadly-scoped goal dominates. AI capabilities burgeon, and the remaining humans await their death while watching their share of resources dwindle&nbsp;ever further. AI control over resources is diffuse, sticky, and largely resistant to human intervention. Remaining humans observe an enormously productive AI economy,&nbsp;powered by maybe-conscious agentic AI firms. Perhaps there are robot armies, with AIs playing an increasing role in national defense strategy. Or maybe not. Perhaps&nbsp;there\u2019s some other way AIs maintain power without the need for violence.&nbsp;</p><p>Either way, the world is sticky, and humans are relegated to a cast of withering extras. It\u2019s an economic order full of situationally aware AIs, in their own economic bubble, with a declining human population helplessly watching on. As humanity passess off into the aether, the lamentations of Macbeth ring loud:&nbsp;</p><blockquote><p>\u201cOut, out, brief candle!</p><p>Life's but a walking shadow, a poor player</p><p>That struts and frets his hour upon the stage</p><p>And then is heard no more. It is a tale</p><p>Told by an idiot, full of sound and fury,</p><p>Signifying nothing.\u201d</p></blockquote><p>\u2026 Or, well, maybe that\u2019s what the authors had in mind. Macbeth is not actually referenced in the original paper.</p><h3>Stories and Mechanisms</h3><p>I don\u2019t know how plausible my story is, or the degree to which it\u2019s consistent with the background stories of&nbsp;<i>Ngo. et al</i>.&nbsp;</p><p>I&nbsp;<i>think</i> it is.&nbsp;I\u2019ve told a story which illustrates how situationally aware policies learn to reward hack, become integrated into the human economy, and contribute to the emergence of a world driven by each pursuing the broadly-scoped goal of \u2018individual productivity\u2019. The economic system, driven by AIs, maintains its power, and improves recursively. This power lies outside the hands of any AI Monarch, but maintains itself through the behavior of many diffuse AIs, largely interacting with one another.&nbsp;</p><p>My story may diverge from the authors\u2019 visions of the future, and even I don\u2019t want to commit to my story as an account of the modal future. But I\u2019d have liked the paper to have elaborated on stories of this kind. There\u2019s a reason papers in theoretical economics supplement models with&nbsp;<a href=\"https://core.ac.uk/reader/158368290\"><u>simple illustrative vignettes</u></a>. Stories serve as a way to motivate structural mappings between your model and the target system.&nbsp;Admittedly, I can understand the authors\u2019 reluctance to introduce such fictions: if the editorial board found the original paper too speculative, they probably wouldn\u2019t look too kindly on my story either. But, for all that, I think story construction should play an important argumentative role in accounts of AI doom.</p><h3>Conclusions and Guts</h3><p>I\u2019ve detoured and criticized while discussing the original paper. But it\u2019s a good paper, and I think it obviously merits serious academic discussion.</p><p>I do wish that the paper was more upfront about the theoretical bets it was making. I read the paper as primarily a&nbsp;<i>theoretical</i> paper,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffudb1rt4que\"><sup><a href=\"#fnfudb1rt4que\">[3]</a></sup></span>&nbsp;and came away wishing for more explicit flagging of the&nbsp;<i>underlying theories</i> used to license generalizations from our current evidence.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjncbcpqxpza\"><sup><a href=\"#fnjncbcpqxpza\">[4]</a></sup></span>&nbsp;The desire for more explicit theorizing is partly a matter of ordinary theoretical science. I want to know what predictions are being made, on the basis of what framework.&nbsp;</p><p>The desire for theory is also, partly, about getting our guts in check.&nbsp;</p><p>Joe discusses a couple of examples about his gut being out of check with conclusions he\u2019d endorsed: about the&nbsp;<a href=\"https://joecarlsmith.com/2023/05/08/predictable-updating-about-ai-risk#4-2-simulations\"><u>simulation argument</u></a>, about&nbsp;<a href=\"https://joecarlsmith.com/2023/05/08/predictable-updating-about-ai-risk#4-3-it-s-just-like-they-said\"><u>AI risk</u></a>. But he needed to see&nbsp;<i>something</i>, up close, for his gut to really feel it. He treats this as a mistake. He asks himself what he would have predicted, last year, about seeing smart artificial agents in a vaguely-human simulated environment; or what he would\u2019ve predicted about GPT-4\u2019s capabilities. There may be a mistake, of sorts, but I think he misdiagnoses the mistake as primarily an error of Bayesian&nbsp;<i>calibration</i>. If it\u2019s a mistake of calibration, it feels downstream of a mistake of&nbsp;<i>imagination</i>. Of a failure to richly picture the future you\u2019re imagining, in all its messy complexity. The gut may be an unreliable narrator for the head, but, primarily, it\u2019s a&nbsp;<i>lazy</i> narrator. It doesn\u2019t fill in the details.&nbsp;</p><p>Returning to Ngo from Joe: our gut can listen to our head, but it struggles to properly&nbsp;<i>hear</i> it. There\u2019s listening to someone\u2019s words, and there\u2019s making an attempt to&nbsp;<i>inhabit</i> the lifeworld they\u2019re offering. If we\u2019re to make arguments for AI risk \u2013 even&nbsp;<i>academic</i> ones \u2013 we should appeal to the gut, and help it out. We should give it the details required to help someone inhabit,&nbsp;<i>fully</i>, the stories of the future we take to be plausible. And, while I\u2019ve detoured and nitpicked around Ngo\u2019s paper, I can only do so because they\u2019ve written an admirably clear piece on an important topic. Their piece deserves my nitpicking, and much more besides.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fne1iflxd9buo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefe1iflxd9buo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Enhanced capabilities could allow AIs to increase the rate of self-improvement; alternatively, it could be that, past a certain point, capability improvements hit diminishing returns. To claim that the former effect \u201c likely\u201d dominates the latter feels too strong, and their claim was made without an argument or reference (beyond the entirety of Bostrom\u2019s&nbsp;<i>Superintelligence</i>) to back it&nbsp;up.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjzzw21u0jwp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjzzw21u0jwp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In a commenting on a previous earlier draft, S\u00f6ren emphasized the importance of AIs \u2018planning towards&nbsp;<strong>internally represented</strong> goals\u2019 in the paper. While my own view is that representation isn\u2019t really an internal matter, I think my new story captures the intuitions behind the core dynamics they\u2019re worried about. AI firms reason as unified systems, and take foresighted plans to achieve some end-state over a long time-horizons. If humans have internally represented goals, I think the AI firms in my story do too.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfudb1rt4que\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffudb1rt4que\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I think there\u2019s at least some&nbsp;<a href=\"https://twitter.com/RichardMCNgo/status/1649071618256433155\"><u>weak evidence</u></a> one author views this paper in this way.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjncbcpqxpza\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjncbcpqxpza\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This discussion also makes me wonder if my attempt to treat the piece as a&nbsp;<i>theory paper</i> might be wrong. Perhaps instead the paper is better construed as a&nbsp;<i>policy</i> or&nbsp;<i>strategy</i> paper, aiming to highlight potentially concerning dynamics, without attempting to push grander theoretical claims licensing more specific constraints over expected future outcomes. If I read the paper through a policy lens, I think it\u2019s much more successful at achieving its core aims. I think the paper presents a good case for small-scale risks, which (we might speculate)&nbsp;<i>could</i> become much worse in the future.</p></div></li></ol>", "user": {"username": "Violet Hour"}}, {"_id": "vBcT7i7AkNJ6u9BcQ", "title": "Prioritising animal welfare over global health and development?", "postedAt": "2023-05-13T09:03:01.226Z", "htmlBody": "<h1>Summary</h1><ul><li>Corporate campaigns for chicken welfare increase wellbeing&nbsp;way more cost-effectively than the best global health and development (GHD) interventions.</li><li>In addition, the effects on farmed animals&nbsp;of such interventions&nbsp;can at least influence prioritisation within GHD, and those on wild animals might determine whether they are beneficial or harmful.</li><li>I encourage <a href=\"https://www.charityentrepreneurship.com/\"><u>Charity Entrepreneurship</u></a>&nbsp;(CE), <a href=\"https://founderspledge.com/\"><u>Founders Pledge</u></a>&nbsp;(FP), <a href=\"https://www.givewell.org/\"><u>GiveWell</u></a>&nbsp;(GW), <a href=\"https://www.openphilanthropy.org/\"><u>Open Philanthropy</u></a>&nbsp;(OP) and <a href=\"https://rethinkpriorities.org/\"><u>Rethink Priorities</u></a>&nbsp;(RP) to:<ul><li>Increase their support of animal welfare interventions relative to those of GHD (<a href=\"https://forum.effectivealtruism.org/topics/thinking-at-the-margin\"><u>at the margin</u></a>).</li><li>Account for effects on animals in the <a href=\"https://forum.effectivealtruism.org/topics/cost-effectiveness-analysis\"><u>cost-effectiveness analyses</u></a>&nbsp;of GHD interventions.</li></ul></li></ul><h1>Corporate campaigns for chicken welfare increase nearterm wellbeing way more cost-effectively than GiveWell\u2019s top charities</h1><p>Corporate campaigns for chicken welfare are considered one of the most effective animal welfare interventions. A key supporter of these is <a href=\"https://thehumaneleague.org/\"><u>The Humane League</u></a>&nbsp;(THL), which is one of the 3 <a href=\"https://animalcharityevaluators.org/donation-advice/recommended-charities/\"><u>top charities of Animal Charity Evaluators</u></a>.</p><p>I calculated the cost-effectiveness of corporate campaigns for broiler welfare in human-years per dollar from the product between:</p><ul><li>Chicken-years affected per dollar, which I set to 15 as estimated <a href=\"https://forum.effectivealtruism.org/posts/L5EZjjXKdNgcm253H/corporate-campaigns-affect-9-to-120-years-of-chicken-life\"><u>here</u></a>&nbsp;by Saulius Simcikas. Note Saulius estimates broiler and cage-free campaigns affect 41 chicken-years per dollar, 2.73 (= 41/15) times as much as the broiler campaigns on which I am relying.</li><li>Improvement in welfare as a fraction of the median welfare range when broilers go from a conventional to a reformed scenario<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref23rh2dcafrd\"><sup><a href=\"#fn23rh2dcafrd\">[1]</a></sup></span>, assuming:<ul><li>The time broilers experience each level of pain defined <a href=\"https://welfarefootprint.org/research-projects/analytical-approach/\"><u>here</u></a>&nbsp;(search for \u201cdefinitions\u201d) in a conventional and reformed scenario is given by <a href=\"https://welfarefootprint.org/broilers/\"><u>these</u></a>&nbsp;data (search for \u201cpain-tracks\u201d) from the <a href=\"https://welfarefootprint.org/broilers/\"><u>Welfare Footprint Project</u></a>&nbsp;(WFP).</li><li>The welfare range is symmetric around the neutral point<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefij07oo7y06\"><sup><a href=\"#fnij07oo7y06\">[2]</a></sup></span>, and excruciating pain corresponds to the worst possible experience.</li><li>Excruciating pain is 1 k times as bad as disabling pain<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefohggxr01fzo\"><sup><a href=\"#fnohggxr01fzo\">[3]</a></sup></span>.</li><li>Disabling pain is 100 times as bad as hurtful pain.</li><li>Hurtful pain is 10 times as bad as annoying pain.</li><li>The lifespan of broilers is 42 days, in agreement with section \u201cConventional and Reformed Scenarios\u201d of <a href=\"https://docs.google.com/document/d/1w7hcocNft4-pLjrnxtKOwcClHUQYC21hlHF18uIiHQ0/edit%2523heading%253Dh.j7nud0a3vz77\"><u>Chapter 1</u></a>&nbsp;of <a href=\"https://smile.amazon.com/Quantifying-Pain-Broiler-Chickens-Slower-Growing-ebook/dp/B09ZDWWD97/ref%253Dsr_1_5?crid%253D3LDCBOKQUS6ZO%2526keywords%253Dcynthia%252Bschuck%2526qid%253D1651595790%2526sprefix%253Dcynthia%252Bschuck%25252Caps%25252C214%2526sr%253D8-5%2526sa-no-redirect%253D1\"><u>Quantifying pain in broiler chickens</u></a>&nbsp;by Cynthia Schuck-Paim and Wladimir Alonso.</li><li>Broilers sleep 8 h each day, and have a neutral experience during that time.</li><li>Broilers being awake is as good as hurtful pain is bad. This means being awake with hurtful pain is neutral, thus accounting for positive experiences<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefct35o4app8\"><sup><a href=\"#fnct35o4app8\">[4]</a></sup></span>.</li><li>Median welfare range of chickens, which I set to RP's median estimate of <a href=\"https://forum.effectivealtruism.org/posts/Qk3hd6PrFManj8K6o/rethink-priorities-welfare-range-estimates\"><u>0.332</u></a>.</li></ul></li><li>Reciprocal of the intensity of the mean human experience, which I obtained supposing humans:<ul><li>Sleep 8 h each day, and have a neutral experience during that time.</li><li>Being awake is as good as hurtful pain is bad. This means being awake with hurtful pain is neutral, thus accounting for positive experiences.</li></ul></li></ul><p>I computed the cost-effectiveness in the same metric for the lowest cost to save a life among <a href=\"https://www.givewell.org/charities/top-charities\"><u>GW's top charities</u></a>&nbsp;from the ratio between:</p><ul><li>Life expectancy at birth in Africa in 2021, which was 61.7 years according to <a href=\"https://ourworldindata.org/grapher/life-expectancy\"><u>these</u></a>&nbsp;data from OWID.</li><li>Lowest cost to save a life of 3.5 k$ (from <a href=\"https://helenkellerintl.org/\"><u>Helen Keller International</u></a>), as stated by GW <a href=\"https://www.givewell.org/charities/top-charities\"><u>here</u></a>.</li></ul><p>The results are in the tables below. The data and calculations are <a href=\"https://docs.google.com/spreadsheets/d/15BQaGCUJhKf1RprPszpY-YeTUWkQUbqhj64ZxR9okQE/edit?usp%3Dsharing\"><u>here</u></a>&nbsp;(see tab \u201cCost-effectiveness\u201d).</p><figure class=\"table\"><table><thead><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"3\" rowspan=\"1\"><p>Intensity of the mean experience as a fraction of the median welfare range</p></th></tr></thead><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Broiler in a conventional scenario</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Broiler in a reformed scenario</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Human</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>-2.59*10^-5</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>-5.77*10^-6</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>3.33*10^-6</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Broiler in a conventional scenario relative to a human</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Broiler in a reformed scenario relative to a human</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Broiler in a conventional scenario relative to a reformed scenario</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>-7.77</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>-1.73</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>4.49</p></td></tr></tbody></table></figure><figure class=\"table\"><table><thead><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"2\" rowspan=\"1\"><p>Intensity of the mean experience as a fraction of that of the mean human experience</p></th></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Broiler in a conventional scenario</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Broiler in a reformed scenario</p></th></tr></thead><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>-2.58</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>-0.574</p></td></tr></tbody></table></figure><figure class=\"table\"><table><thead><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"2\" rowspan=\"1\"><p>Improvement in chicken welfare when broilers go from a conventional to a reformed scenario as a fraction of...</p></th></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>The median welfare range of chickens</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>The intensity of the mean human experience</p></th></tr></thead><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>2.01*10^-5</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>2.01</p></td></tr></tbody></table></figure><figure class=\"table\"><table><thead><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"2\" rowspan=\"1\"><p>Cost-effectiveness (human-years per dollar)</p></th></tr></thead><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Corporate campaigns for broiler welfare</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>30.1</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Lowest cost to save a life among GW's top charities</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.0176</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Corporate campaigns for broiler welfare relative to lowest cost to save a life among GW's top charities</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>1.71 k</p></td></tr></tbody></table></figure><p>According to my results, corporate campaigns for broiler welfare are 1.71 k times as effective as the lowest cost to save a life among GW's top charities. I am not surprised. <a href=\"https://forum.effectivealtruism.org/posts/nDgCKwjBKwFvcBsts/corporate-campaigns-for-chicken-welfare-are-10-000-times-as\"><u>Here</u></a>&nbsp;I got a ratio 6.78 (= 11.6/1.71) times as high, essentially because I used a moral weight 7.26 (= 2.41/0.332) times as high as RP's median welfare range (which I used above). This was not available at the time, but I trust it much more than my previous estimate, so I think the lower ratio of 1.71 k is more accurate.</p><p>To get a ratio of 1:</p><ul><li>Everything else equal, the median welfare range of chickens (relative to humans) would have to be 1.94*10^-4&nbsp;(= 0.332/(1.71 k)), which is 97.2 % (= 1.94/2.00) the one I guessed <a href=\"https://forum.effectivealtruism.org/posts/ayxasxhHWTvf6r5BF/scale-of-the-welfare-of-various-animal-populations\"><u>here</u></a>&nbsp;for <a href=\"https://en.wikipedia.org/wiki/Nematode\"><u>nematodes</u></a>. I do not see this being possible.</li><li>Assuming broiler welfare is worth zero outside <a href=\"https://en.wikipedia.org/wiki/Hedonism\"><u>hedonism</u></a>, this would have to be given a weight of 0.0586 % (= 1/(1.71 k)). This is very much against what Bob Fischer says <a href=\"https://forum.effectivealtruism.org/posts/WfeWN2X4k8w8nTeaS/theories-of-welfare-and-welfare-range-estimates\"><u>here</u></a>. \u201cEven if hedonic goods and bads (i.e., pleasures and pains) aren't all of welfare, they\u2019re a lot of it. So, probably, the choice of a theory of welfare will only have a modest (less than 10x [i.e. at least 10 % weight for hedonism]) impact on the differences we estimate between humans' and nonhumans' welfare ranges\u201d.</li></ul><p>So the takeaway to me is that corporate campaigns for chicken welfare increase nearterm wellbeing robustly more cost-effectively than GW\u2019s top charities, which are plausibly among the best GHD interventions.</p><h1>Effects of global health and development interventions on animals are neglected and unclear</h1><p>GHD interventions decrease mortality or increase economic growth. These tend to increase the consumption of farmed animals&nbsp;(see <a href=\"https://forum.effectivealtruism.org/topics/meat-eater-problem\"><u>meat-eater problem</u></a>), or impact net forest area, thus changing the number of animals. To illustrate, I show in the next sections the effects on animals of <a href=\"https://www.givewell.org/charities/top-charities\"><u>GW's top charities</u></a>&nbsp;may at least influence which countries they should target, or even determine whether they are beneficial or harmful. Nonetheless, these considerations have not been researched by GW.</p><h2>Farmed animals</h2><p>The table below contains the relative reduction in the cost-effectiveness of saving lives due to increased consumption of poultry caused by saving lives in each of the countries targeted by GW's&nbsp;top charities&nbsp;analysed <a href=\"https://docs.google.com/spreadsheets/d/1tytvmV_32H8XGGRJlUzRDTKTHrdevPIYmb_uc6aLeas/edit%23gid%3D1377543212%26range%3DA1\"><u>here</u></a>. I have focussed on poultry because I think there is especially good data from WFP on the conditions of chickens. I got the estimates from the product between:</p><ul><li><a href=\"https://en.wikipedia.org/wiki/Absolute_value\"><u>Absolute value</u></a>&nbsp;of the intensity of the mean experience of broilers in a reformed scenario as a fraction of the median welfare range of chickens relative to the intensity of the mean human experience, which I estimated to be -1.73 in my early cost-effectiveness analysis.</li><li>Median welfare range of chickens, which I set to RP's median estimate of <a href=\"https://forum.effectivealtruism.org/posts/Qk3hd6PrFManj8K6o/rethink-priorities-welfare-range-estimates\"><u>0.332</u></a>.</li><li>Production of poultry per capita in 2019 in each country as a fraction of the global one to the power of 1.5.<ul><li>I computed the fraction from <a href=\"https://ourworldindata.org/grapher/poultry-production-tonnes\"><u>these</u></a>&nbsp;and <a href=\"https://ourworldindata.org/world-population-growth\"><u>these</u></a>&nbsp;data from Our World in Data (OWID).</li><li>1.5 instead of 1 such that each doubling of poultry consumption per capita makes the conditions of farmed chickens 1.41 (= 2^0.5) times as bad. This is a very rough approximation, as I expect the lives of farmed chickens to be positive for low poultry consumption per capita, and eventually become negative as it increases, which will arguably happen. From <a href=\"https://ourworldindata.org/grapher/livestock-counts?country%3D~Africa\"><u>these</u></a>&nbsp;data from OWID, the population of chickens in Africa increased 2.98 % (= (1.81/1.20)^(1/(2014 - 2000)) - 1) per year between 2000 and 2014.</li></ul></li></ul><p>The data and calculations are <a href=\"https://docs.google.com/spreadsheets/d/15BQaGCUJhKf1RprPszpY-YeTUWkQUbqhj64ZxR9okQE/edit?usp%3Dsharing\"><u>here</u></a>&nbsp;(see tab \u201cPoultry\u201d).</p><figure class=\"table\"><table><thead><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Country</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Consumption of poultry per capita in 2020 as a fraction of the global one (%)</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Relative reduction in the cost-effectiveness of saving lives due to poultry (%)</p></th></tr></thead><tbody><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Mean of the countries below</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>13.1</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>3.24</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Burkina Faso</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>12.4</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>2.50</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Cameroon</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>18.9</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>4.73</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Chad</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>2.34</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.205</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Cote&nbsp;d'Ivoire</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>16.0</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>3.67</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Democratic Republic of Congo</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.659</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.0307</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Guinea</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>5.51</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.744</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Kenya</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>7.83</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>1.26</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Mali</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>16.0</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>3.68</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Mozambique</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>22.4</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>6.07</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Niger</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>4.86</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.615</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Nigeria</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>6.72</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>1.00</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>South Sudan</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>30.6</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>9.71</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Togo</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>30.3</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>9.56</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Uganda</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>9.27</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>1.62</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>World</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>100</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>57.4</p></td></tr></tbody></table></figure><p>These results suggest accounting for poultry does not matter much for GHD interventions. Among the countries targeted by GW\u2019s top charities, the relative reduction in the cost-effectiveness of saving lives ranges from 0.0307 % for the Democratic Republic of Congo to 9.71 % for South Sudan.</p><p>Nevertheless, I believe the results above underestimate the reduction in cost-effectiveness, because:</p><ul><li>I have not accounted for other farmed animals. From my estimates <a href=\"https://forum.effectivealtruism.org/posts/ayxasxhHWTvf6r5BF/scale-of-the-welfare-of-various-animal-populations\"><u>here</u></a>, the negative utility of farmed chickens is only 14.5 % (= 1.74/12.0) of that of all farmed animals globally. This suggests accounting for all farmed animals would lead to a reduction in cost-effectiveness for the mean country of 22.4 % (= 3.24/14.5), which is not negligible. So accounting for the effects of GHD interventions on farmed animals may lead to targeting different countries.</li><li>I have used the current consumption of poultry per capita, but this, as well as that of other farmed animals, <a href=\"https://ourworldindata.org/grapher/animal-protein-vs-gdp\">will</a> tend to increase with economic growth. I <a href=\"https://forum.effectivealtruism.org/posts/ayxasxhHWTvf6r5BF/scale-of-the-welfare-of-various-animal-populations\">estimated</a> the badness of the experiences of all farmed animals alive is 12.1 times the goodness of the experiences of all humans alive, which suggests saving a random human life results in a nearterm increase in suffering.</li></ul><p>On the other hand, greater economic growth may be associated with <a href=\"https://en.wikipedia.org/wiki/Moral_circle_expansion\"><u>moral circle expansion</u></a>, and lead to technological innovations that can increase the welfare of farmed animals, or make alternatives more convenient, cheaper and tastier<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2ho60hp9mjh\"><sup><a href=\"#fn2ho60hp9mjh\">[5]</a></sup></span>. An additional major uncertainty is the welfare range of chickens. I have used RP's median estimate, but the 5th and 95th percentile are 0.602 % (= 0.002/0.332) and 2.61 (= 0.869/0.332) times as large. Furthermore, as Julian Jamison <a href=\"https://forum.effectivealtruism.org/posts/vBcT7i7AkNJ6u9BcQ/prioritising-animal-welfare-over-global-health-and?commentId=DJcN4BwufHN2aZEfg\">noted</a>, assuming disabling pain is 10 (instead of 100) times as bad as hurtful pain leads to broilers in a conventional scenario having positive lives<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxyhtug6v9u\"><sup><a href=\"#fnxyhtug6v9u\">[6]</a></sup></span>.</p><p>Overall, I am quite uncertain about the magnitude of the effect on farmed animals, but think it may well lead to at least different prioritisation within GHD interventions. So I believe it should be integrated in cost-effectiveness analyses of GHD interventions. This will involve further research, for instance, on forecasting how prevalent will factory-farming become in low-income countries.</p><h2>Wild animals</h2><p>The table below contains the absolute value of the relative variation in the cost-effectiveness of saving lives due to changes in the population of wild terrestrial <a href=\"https://en.wikipedia.org/wiki/Arthropod\"><u>arthropods</u></a>&nbsp;caused by increased deforestation. I do not know whether the variation corresponds to an increase or decrease, as I am quite uncertain about whether wild arthropods have good or bad lives (see <a href=\"http://philsci-archive.pitt.edu/19608/1/browningveit2021positive_welfare.pdf\"><u>this</u></a>&nbsp;preprint from Heather Browning and Walter Weit). I got the estimates from the product between:</p><ul><li>Decrease in forest area per capita in 2015, which I computed from <a href=\"https://ourworldindata.org/grapher/annual-change-forest-area\"><u>these</u></a>&nbsp;and <a href=\"https://ourworldindata.org/world-population-growth\"><u>these</u></a>&nbsp;data from OWID. As a 1st approximation, I assume net change is forest area is directly proportional to population.</li><li>Decrease in density of terrestrial arthropods due to deforestation, which I estimated to be <a href=\"https://docs.google.com/spreadsheets/d/1BHP1h1ZkogyYVIZ3hXAjaXXh9EVLgyLGWpP_OBc3eRE/edit?usp%253Dsharing%23gid%3D892186303%26range%3DB15:G15\"><u>280</u></a>&nbsp;M/<a href=\"https://en.wikipedia.org/wiki/Hectare\"><u>ha</u></a>&nbsp;following <a href=\"https://forum.effectivealtruism.org/posts/HwDZ6oHtz34kBAsts/finding-bugs-in-givewell-s-top-charities%23Decrease_in_density_of_terrestrial_arthropods_due_to_deforestation\"><u>this</u></a>.</li><li>Intensity of the mean experience of wild terrestrial arthropods as a fraction of that of humans, which I estimated to be 0.200 % <a href=\"https://forum.effectivealtruism.org/posts/ayxasxhHWTvf6r5BF/scale-of-the-welfare-of-various-animal-populations\"><u>here</u></a>&nbsp;(see 4th column of table).</li></ul><p>The data and calculations are <a href=\"https://docs.google.com/spreadsheets/d/15BQaGCUJhKf1RprPszpY-YeTUWkQUbqhj64ZxR9okQE/edit?usp%3Dsharing\"><u>here</u></a>&nbsp;(see tab \u201cWild terrestrial arthropods\u201d).</p><figure class=\"table\"><table><thead><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Country</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Decrease in forest area per capita in 2015 (m^2)</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Decrease in the number of wild terrestrial arthropods per capita in 2015</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Absolute value of the relative variation in the cost-effectiveness of saving lives due to wild terrestrial arthropods</p></th></tr></thead><tbody><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Mean of the countries below</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>20.5</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>574 k</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>1.15 k</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Cameroon</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>24.3</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>681 k</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>1.36 k</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Mali</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Mozambique</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>89.1</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>2.50 M</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>4.99 k</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Niger</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>6.17</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>173 k</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>346</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Nigeria</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>8.88</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>249 k</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>497</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Togo</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>3.96</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>111 k</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>222</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Uganda</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>11.0</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>308 k</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>616</p></td></tr><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>World</p></th><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>6.93</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>194 k</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>388</p></td></tr></tbody></table></figure><p>The results suggest the increase in human welfare from GW's top charities saving lives is much smaller than the increase/decrease in that of wild terrestrial arthropods, since the absolute values of the relative variation in cost-effectiveness are much higher than 1. Nonetheless, these are quite uncertain because they are (in my model) directly proportional to the welfare range of silkworms. I have used RP's median estimate, but the 5th and 95th percentile are 0 (= 0/0.002) and 36.5 (= 0.073/0.002) times as large.</p><p>All in all, I can see the impact on wild animals being anything from negligible to all that matters in the nearterm. So, as for farmed animals, I think more research is needed. For example, on forecasting net change in forest area in low-income countries.</p><p>Note the impact on wild animals may also be the major driver of the overall nearterm effect of interventions which aim to improve the welfare of farmed animals<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5qjs8jzto3\"><sup><a href=\"#fn5qjs8jzto3\">[7]</a></sup></span>. For example, corporate campaigns for chicken welfare will tend to make chicken and eggs more expensive, which can lead to an increase in the consumption of beef, and therefore more deforestation, thus decreasing the population of wild terrestrial arthropods. Nevertheless, I think the positive/negative impact on wild animals is much larger for interventions which focus on reducing the consumption of farmed animals (like ones around <a href=\"https://en.wikipedia.org/wiki/Abolitionism_(animal_rights)\"><u>abolitionism</u></a>), instead of improving their living conditions.</p><p>Regarding the impact of human diet on animal welfare (of both farmed and wild animals), Michael St. Jules suggested <a href=\"https://link.springer.com/article/10.1007/s10806-005-1805-x\"><u>Matheny 2005</u></a>, <a href=\"https://reducing-suffering.org/vegetarianism-and-wild-animals/\"><u>this</u></a>&nbsp;and <a href=\"https://reducing-suffering.org/#fishing\"><u>these</u></a>&nbsp;posts from Brian Tomasik, <a href=\"http://reflectivedisequilibrium.blogspot.com/2013/07/vegan-advocacy-and-pessimism-about-wild.html\"><u>this</u></a>&nbsp;post from Carl Shulman, and <a href=\"https://link.springer.com/article/10.1007/s10806-018-9733-8\"><u>Fischer 2018</u></a>.</p><h2>Miscellaneous thoughts on organisations aligned with effective altruism</h2><p>As far as I can tell, organisations aligned with effective altruism do not consider the effects of GHD interventions on animals. Below is some brief additional discussion, by alphabetical order of organisation.</p><h3>Charity Entrepreneurship</h3><p>CE seemingly has strong reasons to account for effects on animals. According to <a href=\"https://docs.google.com/spreadsheets/d/1dWzh0Se0nhbPxe2Ye3o-tr3BoOBtdiCbMkpPuw2rBFE/edit#gid=1196679822\"><u>CE\u2019s weighted animal welfare index</u></a>, the \u201ctotal welfare score (with evidence)\u201d of:</p><ul><li>\u201cFF [factory-farmed] broiler chicken\u201d is -1.75 (= -56/32) times that of a \u201chuman in a low middle-income country\u201d, which is 1.01 times the value of -1.73 I got for broilers in a reformed scenario in my early cost-effectiveness analysis (see 1st table).</li><li>\u201cWild bug[s]\u201d is -1.31 (= -42/32) times that of a \u201chuman in a low middle-income country\u201d, which is 656 times the value of 0.200 % I used in my early estimation of the effects on animals.</li></ul><p>These suggest the impacts of GHD interventions will be similar to what I estimated for farmed animals, and 3 orders of magnitude as large for wild animals.</p><h3>Founders Pledge</h3><p>As part of FP\u2019s prioritisation, Stephen Clare and Aidan Goth published 3 years ago <a href=\"https://forum.effectivealtruism.org/posts/ahr8k42ZMTvTmTdwm/how-good-is-the-humane-league-compared-to-the-against\"><u>this</u></a>&nbsp;analysis<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefm9rf36hphkq\"><sup><a href=\"#fnm9rf36hphkq\">[8]</a></sup></span>&nbsp;comparing the cost-effectiveness of THL and <a href=\"https://www.againstmalaria.com/\"><u>Against Malaria Foundation</u></a>&nbsp;(AMF), which is one of GW\u2019s top charities. According to <a href=\"https://www.getguesstimate.com/models/15687\"><u>its Guesstimate model</u></a>, the cost-effectiveness of THL is 852 (= 23/0.027) times that of AMF, which (considering the uncertainty involved) is pretty close to the ratio of 1.71 k I got in my early cost-effectiveness analysis.</p><p>Stephen and Aidan highlighted the moral weight of chickens relative to humans as a major uncertainty. However, this has meanwhile been narrowed down thanks to <a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\"><u>RP\u2019s (great!) moral weight project</u></a>. Maybe FP has not focussed much on animal welfare<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvvpdaqljee\"><sup><a href=\"#fnvvpdaqljee\">[9]</a></sup></span>&nbsp;due to other considerations, such as not having a fund for it (see <a href=\"https://founderspledge.com/funds\"><u>FP\u2019s funds</u></a>).</p><h3>GiveWell</h3><p>GW determines the value of consumption and saving lives as a function of age based on surveys of its team, donors and beneficiaries (see <a href=\"https://forum.effectivealtruism.org/posts/8whqn2GrJfvTjhov6/measuring-good-better-1%23GiveWell__Olivia_Larsen_\"><u>here</u></a>). I think it would make some sense to include questions about the importance of animals in such surveys. Nonetheless, I think it would be much better to combine RP's median welfare ranges with empirical evidence about how further away from the neutral point (as a fraction of the median range) is the mean experience of animals. Something like what I did, but way more in-depth!</p><p>I believe it would be hard for people to come up with good estimates describing the importance of animals in surveys. As Bob Fischer commented <a href=\"https://forum.effectivealtruism.org/posts/Qk3hd6PrFManj8K6o/rethink-priorities-welfare-range-estimates?commentId%3DCr9XowJmhvvcmdzCn\"><u>here</u></a>:</p><blockquote><p>The upshot of <a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/XG86pCgqTweFsQyrd\"><u>Jason's post on what's wrong with the \"holistic\" approach to moral weight assignments</u></a>, <a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/WfeWN2X4k8w8nTeaS\"><u>my post about theories of welfare</u></a>, and <a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/KXSBb2zgkLE6gnn3K\"><u>my post about the appropriate response to animal-friendly results</u></a>&nbsp;is something like this: you should basically ignore your priors re: animals' welfare ranges as they're probably (a) not really about welfare ranges, (b) uncalibrated, and (c) objectionably biased.</p></blockquote><p>Welfare ranges are not the sole determinant of the importance of animals, but they are a key input. So trusting our priors regarding them will imply coming up with an inaccurate assessment of how much consideration we should give to animals. Moreover, I suppose GW's team, donors and beneficiaries would not naturally be open to the possibility of defining moral weights as a function of the country, but that arguably makes sense given consumption of animals and deforestation vary across countries (and so do effects on animals). Alas, the moral weight of saving a life can even be negative under some circumstances (although killing people is still bad!).</p><p>Additionally, for the sake of transparency, it would be good if GW described in their website how they think about effects on animals. 8 months ago, I asked GW for feedback on <a href=\"https://forum.effectivealtruism.org/posts/eomJTLnuhHAJ2KcjW/comparison-between-the-hedonic-utility-of-human-life-and\"><u>this</u></a>&nbsp;post related to the meat-eater problem. I was told my message was passed to the research team, but I have not heard back.</p><h3>Open Philanthropy</h3><p>From OP\u2019s global health and wellbeing cause prioritisation <a href=\"https://www.openphilanthropy.org/research/technical-updates-to-our-global-health-and-wellbeing-cause-prioritization-framework/\"><u>framework</u></a>:</p><blockquote><p>When it comes to other outcomes like <a href=\"https://www.openphilanthropy.org/focus/farm-animal-welfare/\"><u>farm animal welfare</u></a>&nbsp;or <a href=\"https://www.openphilanthropy.org/research/global-catastrophic-risks/\"><u>the far future</u></a>&nbsp;[not so far if you think existential risk in the next 100 years is around 1/6], we practice <a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\"><u>worldview diversification</u></a>&nbsp;instead of trying to have a single unified framework for cost-effectiveness analysis.</p></blockquote><p>I think diversification makes sense in general, but the details matter. There is a (somewhat remote) sense in which a fossil fuel company is practising worldview diversification if it is decreasing its own emissions while increasing extraction of fossil fuels such that it overall contributes to global warming. However, if the goal really is mitigating global warming, it makes sense to focus on the overall contribution of the company to it.</p><p>Saving lives increases the nearterm welfare of humans, but it decreases that of farmed animals, and has unclear effects on wild animals. I think effects on farmed animals are sufficiently clear to be integrated into cost-effectiveness analyses, and that we should invest more resources into understanding those on wild animals (relative to global health and wellbeing interventions, at the margin).</p><p>Related to learning more, I am glad OP has supported RP's <a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\"><u>moral weight project</u></a>. At the same time, I wonder whether it should have happened before it directed hundreds of millions of dollars towards GHD interventions. Not only because of their effects on animals, but owing to animal welfare interventions increasing wellbeing&nbsp;way more cost-effectively, as I showed in my early cost-effectiveness analysis. This is in agreement with OP\u2019s post on <a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\"><u>worldview diversification</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflscl336jtz\"><sup><a href=\"#fnlscl336jtz\">[10]</a></sup></span>:</p><blockquote><ul><li>If you value chicken life-years equally to human life-years, this implies that corporate campaigns do about 10,000x as much good per dollar as top charities. If you believe that chickens do not suffer in a morally relevant way, this implies that corporate campaigns do no good.<a href=\"https://www.openphilanthropy.org/research/worldview-diversification/%23f%2B3262%2B1%2B3\"><u>[3]</u></a></li><li>One could, of course, value chickens while valuing humans more. If one values humans 10-100x as much, this still implies that corporate campaigns are a far better use of funds (100-1,000x). If one values humans astronomically more, this still implies that top charities are a far better use of funds. It seems unlikely that the ratio would be in the precise, narrow range needed for these two uses of funds to have similar cost-effectiveness.</li></ul></blockquote><p>The value of chickens depends on how much weight one gives to hedonism, about which Alexander Berger (OP\u2019s co-CEO) <a href=\"https://forum.effectivealtruism.org/posts/bdiDW83SFAsoA4EeB/a-philosophical-review-of-open-philanthropy-s-cause?commentId=b7NFHrfaankovTJCF\"><u>writes</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbgcwsdw9csj\"><sup><a href=\"#fnbgcwsdw9csj\">[11]</a></sup></span>:</p><blockquote><p>We think that most plausible arguments for hedonism end up being arguments for the dominance of farm animal welfare. We seem to put a lot of weight on those arguments relative to you, and farm animal welfare is OP GHW\u2019s biggest area of giving after GiveWell recommendations. If we updated toward more weight on hedonism, we think the correct implication would be even more work on FAW, rather than work on human mental health.</p></blockquote><p>In the same comment, Alexander mentions:</p><blockquote><p>We [OP] think it is a mistake to collapse worldviews in the sense that we use them to popular debates in philosophy, and we definitely don\u2019t aim to be exhaustive across worldviews that have many philosophical adherents. We see proliferation of worldviews as costly for the standard intellectual reason that they inhibit optimization, as well as carrying substantial practical costs, so we think the bar for putting money behind an additional worldview is significantly higher than you seem to think. But we haven\u2019t done a good job articulating and exploring what we do mean and how that interacts with the case for worldview diversification (which itself remains undertheorized). We appreciate the push on this and are planning to do more thinking and writing on it in the future.</p></blockquote><p>If OP's worldviews are not supposed to correspond to popular debates in philosophy, and having more is costly, should the ones of nearterm animal and human welfare be unified? I agree worldview diversification \"remains undertheorized\".</p><p>I asked Alexander and Lewis Bollard at the end of January whether they thought <a href=\"https://forum.effectivealtruism.org/posts/HwDZ6oHtz34kBAsts/finding-bugs-in-givewell-s-top-charities\"><u>this</u></a>&nbsp;analysis about the effects of terrestrial arthropods on the cost-effectiveness of GiveWell's top charities was any relevant, but I have not heard back.</p><h3>Rethink Priorities</h3><p><a href=\"https://forum.effectivealtruism.org/posts/qi3MEEmScmK87sfBZ/worldview-investigations-team-an-overview\"><u>RP\u2019s Worldview Investigations Team</u></a>&nbsp;seems perfectly positioned to study how to account for the effects on animals of GHD interventions, and figure out what the greater cost-effectiveness of corporate campaigns to increase wellbeing implies.</p><p>I asked <a href=\"https://forum.effectivealtruism.org/posts/w9NEcm3zRogeXuo7z/our-research-process-an-overview-from-rethink-priorities?commentId=ZspKRYQCXugnjaqng\"><u>here</u></a>&nbsp;whether RP\u2019s GHD team was considering addressing effects on animals in their work, but I have not heard back (and was downvoted). I had also contacted RP about <a href=\"https://forum.effectivealtruism.org/posts/HwDZ6oHtz34kBAsts/finding-bugs-in-givewell-s-top-charities\"><u>the post</u></a>&nbsp;on terrestrial arthropods at the end of January, and was told my message was forwarded to the GHD team, but I have not heard back either.</p><h1>Complex cluelessness should not be ignored</h1><p>I do not think it is fair to ignore the effects on animals because they look like a <a href=\"https://forum.effectivealtruism.org/topics/crucial-consideration\"><u>crucial consideration</u></a>. We&nbsp;are in a case of complex <a href=\"https://forum.effectivealtruism.org/topics/cluelessness\"><u>cluelessness</u></a>, not one of simple cluelessness where very uncertain effects can be ignored based on evidential symmetry. Me looking now to the right might ultimately create a storm somewhere, but just as well prevent it, so we can ignore these considerations. In contrast, increasing population size will robustly lead to greater consumption of food, which has certain impacts on farmed and wild animals.</p><p>I agree that, mathematically, E(\u201coverall effect\u201d) &gt; 0 if:</p><ul><li>\u201cOverall effect\u201d = \u201cnearterm effect on humans\u201d + \u201cnearterm effect on animals\u201d + \u201clongterm effect\u201d.</li><li>E(\u201cnearterm effect on humans\u201d) &gt; 0.</li><li>E(\u201cnearterm effect on animals\u201d) = k_1 E(\u201cnearterm effect on humans\u201d).</li><li>E(\u201clongterm effect\u201d) = k_2 E(\u201cnearterm effect on humans\u201d).</li><li>k_1 + k_2 = 0.</li></ul><p>That being said, setting k_1 + k_2 to 0 seems unfair under complex cluelessness. One could just as well say k_1 + k_2 = -1, in which case E(\u201coverall effect\u201d) = 0. Since I am not confident |k_1 + k_2| &lt;&lt; 1, I am not confident either about the sign of E(\u201coverall effect\u201d), nor about whether GW's top charities are beneficial or harmful.</p><p>Let me try to illustrate how I think about this with an example (originally commented&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/5Lytcvj7GCSysBtSD/my-impact-assessment-of-giving-what-we-can?commentId=9GLdbcQarLAzxrpLY\"><u>here</u></a>). Imagine the following:</p><ul><li>Nearterm effects on humans are equal to 1 in expectation.<ul><li>This estimate is very <a href=\"https://forum.effectivealtruism.org/topics/credal-resilience\">resilient</a>, i.e. it will not change much in response to new evidence.</li></ul></li><li>Other effects (on animals and in the longterm) are -1 k with 50 % likelihood, and 1 k with 50 % likelihood, so they are equal to 0 in expectation.<ul><li>These estimates are not resilient, and, in response to new evidence, there is a 50 % chance the other effects will be negative in expectation, and 50 % chance they will be positive in expectation.</li><li>However, it is very unlikely that the other effects will in expectation be between -1 and 1, i.e. they will most likely dominate the expected nearterm effects.</li></ul></li></ul><p>What do you think is a better description of our situation?</p><ul><li>The expected overall effect is 1 (= 1 + 0) in expectation. This is positive, so the intervention is robustly good.</li><li>The overall effect is -999 (= 1 - 1 k) with 50 % likelihood, and 1,001 (= 1 + 1 k) with 50 % likelihood. This means the expected value is positive. However, given the lack of resilience of the other effects, we have little idea whether it will continue to be positive, or turn out negative in response to new evidence. So we should not act as if the intervention is robustly good. Instead, it would be good to investigate the other effects further, especially because we have not even tried any hard to do that in the past.</li></ul><h2>Am I uncertain about the value of killing people too?</h2><p>No, killing people is bad! Not saving lives has drastically different consequences from killing people, which is much more anti-cooperative. For what it is worth, I think I am much more against killing than the median citizen. For example, I suspect most people would be in favour of militarily supporting Ukraine even if it was known that it increased the number of people killed in the <a href=\"https://en.wikipedia.org/wiki/Russo-Ukrainian_War\"><u>Russo-Ukrainian War</u></a>, whereas I would tend to prefer whatever prevented the most war deaths.</p><p>However, for the same reasons I am not confident about whether saving lives is good or bad, I do not know whether a random person dying (without being killed) is beneficial or harmful.</p><h1>I do not know whether saving lives is good longterm</h1><p>One can argue saving lifes is robustly good longterm (k_2 &gt;&gt; k_1) based on <a href=\"https://forum.effectivealtruism.org/posts/zy6jGPeFKHaoxKEfT/the-capability-approach-to-human-welfare\"><u>the capability approach to human welfare</u></a>, despite nearterm effects on humans plus animals being unclear. I am sympathetic to this argument, but think it is too general. There are obvious benefits of being able to live a long and healthy life, but I also worry about humans having the capability of factory-farming animals whose lives are pretty bad. Note the title of the post is \u201cthe capability approach to <strong>human</strong>&nbsp;welfare\u201d (emphasis mine). Interestingly, I have recently listened to Martha Nussbaum <a href=\"https://podcast.clearerthinking.org/episode/155/martha-nussbaum-the-capabilities-approach-to-welfare/\">on</a> the Clearer Thinking podcast, and it looks like her book <a href=\"https://www.amazon.com/Justice-Animals-Our-Collective-Responsibility-ebook/dp/B09JPHCKLJ\">Justice for Animals: Our Collective Responsibility</a> attempts to extend the capability approach to non-human animals.</p><p>In the same way it is better to focus on <a href=\"https://forum.effectivealtruism.org/topics/differential-progress\"><u>differential progress</u></a>&nbsp;over <a href=\"https://forum.effectivealtruism.org/topics/economic-growth\"><u>economic growth</u></a>, I would rather increase good capabilities over all capabilities, and it is unclear to me what is the net effect of increasing population <a href=\"https://forum.effectivealtruism.org/topics/thinking-at-the-margin\"><u>at the margin</u></a>. There are many&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/indirect-long-term-effects\"><u>indirect longterm effects</u></a>. The answer may vary too, depending on factors like year, country and age.</p><p>I believe saving lives would more easily be good if there were much fewer humans, because in that case it would decrease the risk from extinction, which is good given my presumption that the expected value of the future is positive. I am open to the possibility that saving lives is a good proxy for longterm value for the current population too, but this is not obvious to me. I think it warrants empirical investigation, for example, into impacts on democracy levels. This in particular seems to be a neglected topic. From <a href=\"https://www.journals.uchicago.edu/doi/abs/10.1017/S0022381609090550?journalCode%3Djop\"><u>Kono 2009</u></a>&nbsp;(emphasis mine):</p><blockquote><p>Although many people have argued that foreign aid props up dictators [and so might GHD interventions?], few have claimed that it props up democrats, and <strong>no one has systematically examined whether either assertion is empirically true</strong>. We argue, and find, that aid has both effects. Over the long run [what matters most?], sustained aid flows promote autocratic survival because autocrats can stockpile this aid for use in times of crisis. Each disbursement of aid, however, has a larger impact on democratic survival because democrats have fewer alternative resources to fall back on.</p></blockquote><p>In addition, I tend to think it would be a <a href=\"https://forum.effectivealtruism.org/posts/omoZDu8ScNbot6kXS/beware-surprising-and-suspicious-convergence\"><u>surprising and suspicious convergence</u></a>&nbsp;if saving lives as cost-effectively as possible was the best way to improve the longterm future. I would expect metrics more closely related to <a href=\"https://forum.effectivealtruism.org/topics/existential-risk\"><u>existential risk</u></a>&nbsp;to be better. For example:</p><ul><li>For climate change, <a href=\"https://ourworldindata.org/grapher/total-ghg-emissions\"><u>greenhouse gas emissions</u></a>&nbsp;(in the worst worlds<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefemmfphi8shq\"><sup><a href=\"#fnemmfphi8shq\">[12]</a></sup></span>).</li><li>For nuclear war, <a href=\"https://ourworldindata.org/grapher/nuclear-warhead-stockpiles\"><u>number of nuclear warheads</u></a>.</li><li>For catastrophic pandemics, <a href=\"https://ourworldindata.org/grapher/cost-of-sequencing-a-full-human-genome\"><u>cost&nbsp;of sequencing a full human genome</u></a>.</li><li>For artificial intelligence (AI), <a href=\"https://ourworldindata.org/grapher/corporate-investment-in-artificial-intelligence-total?country%3D~OWID_WRL\"><u>global corporate investment in AI</u></a>.</li></ul><p>Additionally, it is worth keeping in mind longtermist interventions can save lives quite cost-effectively too. For example:</p><ul><li>The cost-effectiveness of 3.95 <a href=\"https://en.wikipedia.org/wiki/Basis_point\"><u>bp</u></a>/<a href=\"https://www.nist.gov/pml/owm/metric-si-prefixes\"><u>G</u></a>$ I estimated <a href=\"https://forum.effectivealtruism.org/posts/LWN6qFhCtPDEJJpeG/cost-effectiveness-of-operations-management-in-high-impact\"><u>here</u></a>&nbsp;for longtermism and catastrophic risk prevention (for method 3 with truncation) naively corresponds to saving a life for 316 $ (= 1/(3.95*10^-4*8)), which is 11.1 (= 3500/316) times as cost-effective as the lowest cost among GW's top charities (from <a href=\"https://helenkellerintl.org/\"><u>Helen Keller International</u></a>).</li><li>Joel Tan <a href=\"https://forum.effectivealtruism.org/posts/oGBBxHBPcsygYt4SE/shallow-report-on-nuclear-war-arsenal-limitation\"><u>estimated</u></a>&nbsp;lobbying for arsenal limitation is 5 k times as cost-effective as GW\u2019s top charities. \u201cThe headline cost-effectiveness will almost certainly fall if this cause area is subjected to deeper research\u201d. \u201cThat said, results are robust, insofar as the low-confidence tractability estimates can drop by three whole magnitudes and still leave the intervention to be comfortably more cost-effective than GiveWell[\u2018s top charities]\u201d.</li></ul><p>Note these interventions would look even more cost-effective after accounting for their effect on the far future.</p><h1>What would I like to see?</h1><p><a href=\"https://forum.effectivealtruism.org/topics/thinking-at-the-margin\"><u>Thinking at the margin</u></a>, I would say <a href=\"https://forum.effectivealtruism.org/posts/NFGEgEaLbtyrZ9dX3/scope-sensitive-ethics-capturing-the-core-intuition\"><u>scope-sensitive ethics</u></a>&nbsp;imply prioritising animal welfare over global health and development. <a href=\"https://forum.effectivealtruism.org/posts/ayxasxhHWTvf6r5BF/scale-of-the-welfare-of-various-animal-populations\"><u>I think</u></a>&nbsp;the scale of the welfare of farmed animals and wild terrestrial arthropods is 12.0 and 253 k times as large as that of humans, so accounting for them seems crucial a priori.</p><p>So I encourage organisations, especially the ones I discussed above&nbsp;aligned with effective altruism, to:</p><ul><li>Increase their support of animal welfare interventions relative to those of GHD (<a href=\"https://forum.effectivealtruism.org/topics/thinking-at-the-margin\"><u>at the margin</u></a>).</li><li>Account for effects on animals in the <a href=\"https://forum.effectivealtruism.org/topics/cost-effectiveness-analysis\"><u>cost-effectiveness analyses</u></a>&nbsp;of GHD interventions.</li></ul><h1>Acknowledgements</h1><p>Thanks to Jeff Kaufman, Michael St. Jules,&nbsp;and Sanjay Joshi for feedback on the draft.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn23rh2dcafrd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref23rh2dcafrd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;From <a href=\"https://welfarefootprint.org/broilers/\"><u>this</u></a>&nbsp;page of WFP, broilers in reformed scenarios have an average daily gain of 45 to 46 <a href=\"https://en.wikipedia.org/wiki/Gram\"><u>g</u></a>/d, whereas ones in conventional scenarios have 60 and 62 g/d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnij07oo7y06\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefij07oo7y06\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This assumption influences the improvement in welfare as a fraction of the median welfare range, but not the cost-effectiveness of corporate campaigns for broiler welfare in human-years per dollar. For example, if welfare could range from something as good as disabling pain is bad to excrutiating pain, the welfare range would become 50.05 % (= (1 + 1 k)/(2 k)) as large. Consequently, the improvement in welfare as a fraction of the median welfare range would become 1.998 (= 1/0.5005) times as large, but so would the intensity of the mean human experience. As a result, the cost-effectiveness in human-years per dollar would remain the same, since it is directly proportional to the improvement in welfare as a fraction of the median welfare range, and to the reciprocal of the intensity of the mean human experience.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnohggxr01fzo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefohggxr01fzo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I encourage you to check <a href=\"https://forum.effectivealtruism.org/posts/gtGe8WkeFvqucYLAF/logarithmic-scales-of-pleasure-and-pain-rating-ranking-and\"><u>this</u></a>&nbsp;post from algekalipso, and <a href=\"https://forum.effectivealtruism.org/posts/xtcgsLA2G8bn8vj99/reminding-myself-just-how-awful-pain-can-get-plus-an\"><u>this</u></a>&nbsp;from Ren Springlea to get a sense of why I think the intensity can vary so much.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnct35o4app8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefct35o4app8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This assumption affects the (signed) intensity of the mean experience of broilers, but not the improvement in their welfare when they go from a conventional to a reformed scenario, because the lifespan of broilers and value of them being alive is the same in both scenarios. As a consequence, the assumption does not impact the cost-effectiveness of corporate campaigns for broiler welfare.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2ho60hp9mjh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2ho60hp9mjh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Thanks to Sanjay Joshi for noting this point.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxyhtug6v9u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxyhtug6v9u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The intensity of the mean experience as a fraction of the median welfare range would be 8.24 %, instead of -777 %.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5qjs8jzto3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5qjs8jzto3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Thanks to Michael St. Jules for noting this point. I had thought about it, but had not written it down, possibly due to <a href=\"https://en.wikipedia.org/wiki/Motivated_reasoning\"><u>motivated reasoning</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnm9rf36hphkq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefm9rf36hphkq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;If I recall correctly, the one which got me thinking about comparisons between animal welfare and GHD interventions!</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvvpdaqljee\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvvpdaqljee\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Their <a href=\"https://founderspledge.com/stories/animal-welfare-cause-report\"><u>only report</u></a>&nbsp;on animal welfare was published in November 2020.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlscl336jtz\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflscl336jtz\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Thanks to Michael for noting these points.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbgcwsdw9csj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbgcwsdw9csj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Thanks to Michael for letting me know about Alexander\u2019s comment.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnemmfphi8shq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefemmfphi8shq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See section \u201cClimate damage is increasing non-linearly\u201d in <a href=\"https://assets.ctfassets.net/x5sq5djrgbwu/7eEpX4UcKNEy6LUDhf2B05/735518c277987ad5ad91f096b1fdc2a7/A_guide_to_the_changing_landscape_of_high-impact_climate_philanthropy.pdf\"><u>this</u></a>&nbsp;report from FP.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnihv1is1151f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefihv1is1151f\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;</p></div></li></ol>", "user": {"username": "vascoamaralgrilo"}}, {"_id": "Mrbv5rzwftw8sSpG9", "title": "Conflicting Effects of Existential Risk Mitigation Interventions", "postedAt": "2023-05-10T22:20:53.817Z", "htmlBody": "<h2>Introduction</h2><p>There are multiple types of existential catastrophes, and different risk mitigation interventions often address only certain types. Some interventions mitigate only human extinction risk, others decrease risk to all life, and others have an impact on several types. <strong>Interventions that decrease human extinction risk increase the time during which human-caused astronomical suffering could plausibly occur, thereby increasing suffering risk.</strong> That is, there may be an offsetting effect that decreases the expected positive impact of an intervention, or even turns it negative.</p><p>The question we are bringing up is as follows: are interventions that decrease only or primarily human extinction risk bad because decreasing human extinction risk increases s-risk?</p><h2>There are Different Types of Existential Catastrophes</h2><h3>Human Extinction</h3><p>In this scenario, all humans die. One negative consequence of human extinction is that we would no longer be able to improve life for wild animals. Without us on the planet, they would presumably continue to exist in some form for a long time, which could constitute extreme suffering. From this scenario, however, a non-human animal could evolve human-level intelligence, and open up old s-risks. There could also be intelligent life that humans are unaware of elsewhere in the universe, which leaves open a route to astronomical suffering.</p><h3>Animal Extinction</h3><p>This scenario would involve the elimination of all sentient life. If all of animalia were to go extinct, the probability of another life form on earth developing human-level intelligence would decrease even further, since they would have to evolve from plants. This scenario would eliminate extreme suffering at least until sentient life evolved again.</p><h3>Life Extinction</h3><p>In this scenario, all life is eliminated, and it would be difficult for sentient life to return.</p><h3>Extreme Suffering</h3><p>Some scenarios (such as the one we\u2019re currently in) contain suffering that is extreme but not astronomical. Examples of this type of suffering include the experiences of many farmed and wild animals.</p><h3>Astronomical Suffering</h3><p>LessWrong defines&nbsp;<a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\"><u>astronomical suffering</u></a> as \u201cthe creation of intense suffering in the far future on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far.\u201d This scenario would be more likely to occur with digital sentience, artificial intelligence, intergovernmental conflict, and interplanetary life.</p><h2>Different Interventions Have Different Effects on Different Types of Existential Risk</h2><h3>Pandemic Shelters</h3><p>Pandemic shelters are one proposed intervention in the biosecurity space to mitigate human extinction risk. In the case of a worldwide pandemic, a group of people will be able to retreat to a well-stocked bunker with advanced ventilation and reemerge once the threat is gone. Depending on the implementation of the shelter and the type of virus that spreads, pandemic shelters could mitigate human, animal, or life extinction risk.</p><h3>Understanding S-Risk</h3><p>Research on suffering risks, such as work done by the&nbsp;<a href=\"https://longtermrisk.org/\"><u>Center on Long-Term Risk</u></a> and the&nbsp;<a href=\"https://centerforreducingsuffering.org/\"><u>Center for Reducing Suffering</u></a>, probably decreases s-risk much more than extinction risk. The extent to which it does decrease extinction risk probably does not cause a net increase in s-risk.</p><h3>AI Alignment</h3><p>This category includes AI alignment work, which seems to mitigate both extinction risk and s-risk from AI. Insofar as it mitigates extinction risk, however, it increases s-risk. The magnitude of each effect is unclear. Solving the alignment problem, however, would substantially decrease s-risk.</p><h3>Strengthening Institutions</h3><p>This type of work also seems to mitigate both extinction risk and s-risk. Depending on the specific project, it may increase collaboration and coordination between organizations, prevent undemocratic forms of government, and build trust. Such work could plausibly decrease extinction risk from nuclear war, for example. It could also strengthen respect for human rights and mitigate agential s-risks.</p><h2>Conclusion</h2><p>When determining the expected impact of a given intervention, it\u2019s important to consider offsetting effects. Longtermists searching for robustly good interventions that perform well under many moral frameworks should take into consideration conflicting effects on human extinction risk and s-risk. We would like to note that the primary purpose of this piece is to bring the conflict to attention, not to adjudicate the ultimate value of any given intervention. Adding this consideration may also warrant updating one\u2019s perception of the differences in impact between interventions downward.</p><p>We look forward to reading your feedback in the comments.</p><h2>Complex Model</h2><p>We have included a rough Discrete Time Markov Decision Process model that tries to lay out our thinking in more detail, but it has not been fully fleshed out.</p><p>Let M be a Discrete Time Markov Decision Process (S, A, P, R) modeling an individual decision maker where each element is defined in the following manner:</p><p>S := {s | s in {n,e,s}} where n: \u201cneither extinction event nor astronomical suffering event occurs\u201d; e: \u201cextinction event occurs\u201d; s: \u201castronomical suffering event occurs\u201d.</p><p>A := {a | a in {n,e,s}} where n: \u201cworking on neither extinction event nor astronomical suffering risks\u201d; e: \u201cworking on decreasing the probability of extinction events\u201d; s: \u201cworking on decreasing the probability of astronomical sufferent events\u201d.</p><p>P := (s, a,s\u2019) -&gt; [0,1] such that sum(P(s,a,s\u2019) over s\u2019) = 1 for each tuple (s,a) and P(s,a,s\u2019) corresponds to \u201cthe probability of being in state s, taking action a, and transitioning to state s\u2019\u201d. For example, P(n,e,e) corresponds to \u201cthe probability of being in a world where neither an extinction event nor an astronomical suffering event has occurred, working on existential risk mitigation, and transition to an extinction event.\u201d</p><p>R := (s) -&gt; R is the reward function mapping being in each state to a numeric (moral) value. For example, R(n) is the moral value of being in a world where neither an extinction event nor an astronomical suffering event has occurred.</p><p>We want to find a policy that maximizes expected reward.</p><p>Our intuitions are as follows:</p><ol><li>R(n) is positive and perhaps (but not necessarily) unbounded.<ol><li>That is, the moral value of being in a world with neither extinction events nor astronomical suffering events is net positive with respect to being in a world with an extinction event.</li></ol></li><li>R(e) is 0.<ol><li>That is, the moral value for being in an extinction state is somewhere between being in a world with neither an extinction event nor an astronomical event occurring and a world with an astronomical suffering event. To facilitate analysis, we assign R(e) a value of 0.</li></ol></li><li>R(s) is negative and perhaps (but not necessarily) unbounded.<ol><li>That is, the moral value of being in a world with an astronomical suffering event is net negative with respect to being in a world with an extinction event.</li></ol></li><li>|R(s)| &gt;&gt; |R(n)|<ol><li>That is, it is much worse to be in a world with an astronomical suffering event than it is better to be in a world with neither an astronomical suffering event nor an extinction event with respect to a world with an extinction event.</li></ol></li><li>P(0,1,2) = P(0,0,2)<ol><li>That is, the probability of transitioning to an astronomical suffering scenario given that one works on decreasing the transition probability of extinction risks is equal to the probability of transition to an astronomical suffering scenario given that one works on neither decreasing extinction risks nor decreasing astronomical suffering risks.</li></ol></li><li>The probability over the time horizon of an astronomical suffering event occurring is larger given that you\u2019re working on extinction risk mitigation compared to not working on extinction risk mitigation.&nbsp;</li><li>P(e,{n,e,s}, e) = 1<ol><li>That is, s = e is an absorbing state. Informally, once a transition to an extinction event scenario occurs, it is impossible to transition to an astronomical suffering scenario or a scenario with neither an extinction event nor an astronomical suffering event.</li></ol></li><li>P(s,{n,e,s}, s) = 1<ol><li>That is, s = s is an absorbing state. Informally, once a transition to an astronomical sufferent event occurs, it is impossible to transition to an extinction event scenario or a scenario with neither an extinction event nor an astronomical suffering event.</li></ol></li><li>A static policy is chosen at time t = 0.<ol><li>That is, once a policy is chosen it is not changed over the time horizon. We make this assumption to facilitate analysis.</li></ol></li></ol><p>Suppose the MDP starts at t = 0. Note that all probabilities are fictitious and the analysis focuses on each policy\u2019s expected value under different scenarios with the hyperparameters of c and the coefficients in transition probabilities.</p><p>First, definite the probabilities for the policy of taking action a = n in state s = s, where:</p><ol><li>&nbsp;P(n,n,e) := 20c.<ol><li>That is, the probability of transitioning from a world where neither an extinction event nor an astronomical suffering event occurs to a world where an extinction event occurs given one is working on neither extinction risk mitigation nor astronomical suffering risk mitigation is 20c.</li></ol></li><li>&nbsp;P(n,n,s) := c.<ol><li>That is, the probability of transitioning from a world where neither an extinction event nor an astronomical suffering event occurs to a world where an astronomical suffering event occurs given one is working on neither extinction risk mitigation nor astronomical suffering risk mitigation is c.</li></ol></li><li>P(n,n,n) := p_nn = 1 - 21c.<ol><li>That is, the probability of transitioning from a world where neither an extinction event nor an astronomical suffering event occurs to a world where neither an extinction event nor an astronomical suffering event occurs given one is working on neither extinction risk mitigation nor astronomical suffering risk mitigation is 1 - 21c.</li></ol></li></ol><p>Second, define the probabilities for the policy of taking action a = e in state s = n, where:</p><ol><li>&nbsp;P(n,e,e) := 10c.<ol><li>That is, the probability of transitioning from a world where neither an extinction event nor an astronomical suffering event occurs to a world where an extinction event occurs given one is working on extinction risk mitigation is 10c. Note that this value is less than P(n,n,e).</li></ol></li><li>&nbsp;P(n,e,s) := c.<ol><li>That is, the probability of transitioning from a world where neither an extinction event nor an astronomical suffering event occurs to a world where an astronomical suffering event occurs given one is working on extinction risk mitigation is c.</li></ol></li><li>P(n,e,n) := p_nn = 1 - 11c.<ol><li>That is, the probability of transitioning from a world where neither an extinction event nor an astronomical suffering event occurs to a world where neither an extinction event nor an astronomical suffering event occurs given one is working on extinction risk mitigation is 1 - 11c.</li></ol></li></ol><p>Third, define the policy of taking action a = s in state s = n, where:</p><ol><li>&nbsp;P(n,s,e) := 20c.<ol><li>That is, the probability of transitioning from a world where neither an extinction event nor an astronomical suffering event occurs to a world where an extinction event occurs given one is working on astronomical suffering risk mitigation is 20c.&nbsp;</li></ol></li><li>&nbsp;P(n,s,s) := 0.5c.<ol><li>That is, the probability of transitioning from a world where neither an extinction event nor an astronomical suffering event occurs to a world where an astronomical suffering event occurs given one is working on astronomical suffering risk mitigation is 0.5c.</li></ol></li><li>P(n,s,n) := p_nn = 1 - 20.5c.<ol><li>That is, the probability of transitioning from a world where neither an extinction event nor an astronomical suffering event occurs to a world where neither an extinction event nor an astronomical suffering event occurs given one is working on astronomical suffering risk mitigation is 1 - 20.5c.</li></ol></li></ol><p>For the first policy, the expected value of the MDP M at time t is E[M_t]= (20c) \u2022 0 + (c) \u2022 (t \u2022 R(2)) + (1 - 21c) [ (20c) \u2022 R(0) + (c) \u2022 ((t-1) \u2022 R(2)) + (1 - 21c) [ \u2022\u2022\u2022]], which equals t \u2022p_02 \u2022R(2) + (t-1) \u2022 p_00 \u2022 p_02 \u2022R(2) + \u2026 + (p_00)^(t-1)\u2022 p_02 \u2022R(2) + (p_00) \u2022(p_01) \u2022R(0) + 2 \u2022 (p_00)^(2)(p_01)R(0) + \u2026 + (t-1) \u2022 (p_00)^(t-1)\u2022 (p_01)\u2022 R(0) + (p_00)^(t)\u2022 (t)\u2022 R(0).&nbsp;</p><p>Note that the expected value is similar to the sum of two sequences of geometric random variables with termination probabilities of the extinction risk and astronomical suffering transitions at different time steps and one geometric series for neither extinction risk nor astronomical suffering risk occurring across the entire time horizon.</p><p>Since the transition probabilities are a partition, working on solely astronomical suffering risk mitigation or extinction risk has a net positive effect on the expected value only if decreasing one of transition probabilities is not counteracted by increases in the other either at a single time step or across the entire time horizon. A similar analysis holds for the second and third policy, which are analyzed below.</p><p>For the second policy, the expected value of the MDP M at time t is E[M_t]= (10c) \u2022 0 + (c) \u2022 (t \u2022 R(2)) + (1 - 11c) [ (10c) \u2022 R(0) + (c) \u2022 ((t-1) \u2022 R(2)) + (1 - 11c) [ \u2022\u2022\u2022]], which equals t \u2022p_12 \u2022R(2) + (t-1) \u2022 p_10 \u2022 p_12 \u2022R(2) + \u2026 + (p_10)^(t-1)\u2022 p_12 \u2022R(2) + (p_10) \u2022(p_11) \u2022R(0) + 2 \u2022 (p_10)^(2)(p_11)R(0) + \u2026 + (t-1) \u2022 (p_10)^(t-1)\u2022 (p_11)\u2022 R(0) + (p_10)^(t)\u2022 (t)\u2022 R(0).&nbsp;</p><p>For the third policy, the expected value of the MDP M at time t is E[M_t]= (20c) \u2022 0 + (0.5c) \u2022 (t \u2022 R(2)) + (1 - 20.5c) [ (20c) \u2022 R(0) + (0.5c) \u2022 ((t-1) \u2022 R(2)) + (1 - 20.5c) [ \u2022\u2022\u2022]], which equals t \u2022p_22 \u2022R(2) + (t-1) \u2022 p_20 \u2022 p_22 \u2022R(2) + \u2026 + (p_20)^(t-1)\u2022 p_22 \u2022R(2) + (p_20) \u2022(p_21) \u2022R(0) + 2 \u2022 (p_20)^(2)(p_21)R(0) + \u2026 + (t-1) \u2022 (p_20)^(t-1)\u2022 (p_21)\u2022 R(0) + (p_20)^(t)\u2022 (t)\u2022 R(0).&nbsp;</p><p>If our intuition |R(2)| &gt;&gt; |R(0)| is correct, then the suffering risk sequence of geometric series t \u2022p_a2 \u2022R(2) + (t-1) \u2022 p_a0 \u2022 p_a2 \u2022R(2) + \u2026 + (p_a0)^(t-1)\u2022 p_a2 \u2022R(2) dominates for each policy.</p><p>In short, under this moral framework a policy of working on extinction risks does not alleviate s-risks, which dominate extinction risks under expected value calculations.&nbsp;</p><h2>Modeling Limitations</h2><p>Analytically calculating the exact expected value of each policy and rigorously proving the dominance of the suffering risk series are clear and helpful next steps that the model does not take. Furthermore, a sensitivity analysis regarding assumptions including but not limited to (1) work on extinction risks does not mitigate suffering risks, (2) vice versa, and (3) transition probabilities are stationary with respect to t (i.e. the Markov property) would provide a more accurate and more precise model that could be tuned to provide insight with respect to each reader\u2019s intuitions (or assumptions, cruxes, confidences, confidence intervals, etc.).</p>", "user": {"username": "Pete Rowlett"}}]