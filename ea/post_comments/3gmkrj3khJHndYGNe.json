[{"_id": "tubSF3tgah7Hbg5gS", "postedAt": "2022-09-29T01:28:11.370Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Rethink Priorities has a sizable AI Governance and Strategy team, currently with 8 members. I think this should qualify under Non-technical AI safety research.</p>\n", "parentCommentId": null, "user": {"username": "Peter_Hurford"}}, {"_id": "zumcDQNawvs4pXdJ5", "postedAt": "2022-09-29T04:22:08.378Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Thanks for posting, seems good to know these things! I think some of the numbers for non-technical research should be substantially lower--enough that  <strong>an estimate of ~55 non-technical safety researchers seems more accurate</strong>:</p>\n<ul>\n<li>CSET isn't focused on AI safety; maybe you could count a few of their researchers (rather than 10).</li>\n<li>I think SERI and BERI have 0 full-time non-technical research staff (rather than 10 and 5).</li>\n<li>As far as I'm aware, the Leverhulme Centre for the Future of Intelligence + CSER only have at most a few non-technical researchers in total focused on AI safety (rather than 10 &amp; 5). Same for FLI (rather than 5).</li>\n<li>I hear Epoch has ~3 FTEs (rather than 10).</li>\n<li>GoodAI's <a href=\"https://www.goodai.com/goodai-research-roadmap-2021-2022/\">research roadmap</a> makes no mention of public/corporate policy or governance, so I'd guess they have at most a few non-technical safety-focused researchers (rather than 10).</li>\n</ul>\n<p>If I didn't mess up my math, all that should shift our estimate from 93 to ~42. Adding in 8 from Rethink (going by Peter's comment) and 5 (?) from OpenPhil, we get ~55.</p>\n", "parentCommentId": null, "user": {"username": "Mauricio"}}, {"_id": "gEHMi8y7mwapFAErR", "postedAt": "2022-09-29T08:01:20.286Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Thanks for the information! Your estimate seems more accurate than mine.</p>\n<p>In the case of Epoch, I would count every part-time employee as roughly half a full-time employee to avoid underestimating their productivity.</p>\n", "parentCommentId": "zumcDQNawvs4pXdJ5", "user": {"username": "Stephen McAleese"}}, {"_id": "8BDj4XQ954KDBscf8", "postedAt": "2022-09-29T22:42:25.375Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Edits based on feedback from LessWrong and the EA Forum:</p><p>EDITS:<br>- Added new 'Definitions' section to introduction to explain definitions such as 'AI safety', 'researcher' and the difference between technical and non-technical research.</p><p>UPDATED ESTIMATES &nbsp;(lower bound, estimate, upper bound):</p><p>TECHNICAL<br>- CHAI: 10-30-60 -&gt; 5-25-50<br>- FHI: 10-10-40 -&gt; 5-10-30<br>- MIRI: 10-15-30 -&gt; 5-10-20</p><p>NON-TECHNICAL<br>- CSER: 5-5-10 -&gt; 2-5-15<br>- Delete BERI from the list of non-technical research organizations<br>- Delete SERI from the list of non-technical research organizations<br>- Levelhume Centre: 5-10-70 (Low confidence) -&gt; 2-5-15 (Medium confidence)<br>- FLI: 5-5-20 -&gt; 3-5-15<br>- Add OpenPhil: 2-5-15<br>- Epoch: 5-10-15 -&gt; 2-4-10<br>- Add 'Other': 5-10-50</p>", "parentCommentId": null, "user": {"username": "Stephen McAleese"}}, {"_id": "kufm8raQfqtsmxiTy", "postedAt": "2022-09-29T22:55:07.637Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>I re-estimated counts for many of the non-technical organizations and here are my conclusions:</p><ul><li>I didn't change the CSET estimate (10) because there seems to be a core group of about 5 researchers there and many others (20-30). Their productivity also seems to be high: I counted over 20 publications so far this year though it seems like only about half of them are related to AI governance (<a href=\"https://cset.georgetown.edu/publications/?fwp_content_type=analysis%2Cdata-brief%2Cformal-response#publications\">list of publications</a>).</li><li>I deleted BERI and SERI from the list because they don't seem to have any full-time researchers.</li><li>Epoch: &nbsp;decreased estimate from 10 to 4.</li><li>Good AI seems to be more technical than non-technical (todo).</li></ul>", "parentCommentId": "zumcDQNawvs4pXdJ5", "user": {"username": "Stephen McAleese"}}, {"_id": "tm7yjtvrtFLTNhBZN", "postedAt": "2022-09-29T23:31:16.457Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>More minor suggestions:</p><ul><li>OpenAI non-technical: there are more than 5.</li><li>AI Impacts non-technical: there are exactly 5.</li><li>I &nbsp;would have said Epoch is at least 5 FTEs (disagreeing with Mauricio).</li><li>Better estimating the number of independent technical researchers seems pretty important and tractable.</li></ul><p>(Edit: also many people on the CHAI website don't actually do AI safety research, but definitely more than 5 do.)</p>", "parentCommentId": "8BDj4XQ954KDBscf8", "user": {"username": "zsp"}}, {"_id": "MqfEsE4oLxiscDf8D", "postedAt": "2022-09-30T02:35:36.814Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Thanks for the updates!</p>\n<p>I have it on good word that CSET has well under 10 safety-focused researchers, but fair enough if you don't want to take an internet stranger's word for things.</p>\n<p>I'd encourage you to also re-estimate the counts for CSER, Leverhulme, and the Future of Life Institute.</p>\n<ul>\n<li>CSER's <a href=\"https://www.cser.ac.uk/research/risks-from-artificial-intelligence/\">list of team members related to AI</a> lists many affiliates, advisors, and co-founders but only ~3 research staff.</li>\n<li>The Future of Life Institute seems more focused on policy and field-building than on research; they don't even have a research section on <a href=\"https://futureoflife.org/\">their website</a>. Their <a href=\"https://futureoflife.org/team/\">team page</a> lists ~2 people as researchers.</li>\n<li>Of the 5 people listed in Leverhulme's <a href=\"http://lcfi.ac.uk/projects/ai-futures-and-responsibility/governance-ethics-and-responsible-innovation/\">relevant page</a>, one of them was already counted for CSER, and another one doesn't seem safety-focused.</li>\n</ul>\n<p>I also think the number of \"Other\" is more like 4.</p>\n", "parentCommentId": "kufm8raQfqtsmxiTy", "user": {"username": "Mauricio"}}, {"_id": "hSephwHj524o69jtN", "postedAt": "2022-09-30T03:24:09.641Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>I think Alignment Forum double-counts researchers as most of them are not independent, especially if you count MATS separately (which I think had about 6 mentors and 31 fellows this summer). Looking at the top posts this year:<br><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/14c3a6c96ec8ee4a38741d08e49b5edab6bdeee343c01420.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/14c3a6c96ec8ee4a38741d08e49b5edab6bdeee343c01420.png/w_154 154w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/14c3a6c96ec8ee4a38741d08e49b5edab6bdeee343c01420.png/w_234 234w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/14c3a6c96ec8ee4a38741d08e49b5edab6bdeee343c01420.png/w_314 314w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/14c3a6c96ec8ee4a38741d08e49b5edab6bdeee343c01420.png/w_394 394w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/14c3a6c96ec8ee4a38741d08e49b5edab6bdeee343c01420.png/w_474 474w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/14c3a6c96ec8ee4a38741d08e49b5edab6bdeee343c01420.png/w_554 554w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/14c3a6c96ec8ee4a38741d08e49b5edab6bdeee343c01420.png/w_634 634w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/14c3a6c96ec8ee4a38741d08e49b5edab6bdeee343c01420.png/w_714 714w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/14c3a6c96ec8ee4a38741d08e49b5edab6bdeee343c01420.png/w_794 794w\"></p><p>Paul Christiano works at ARC. Yudkowsky works at MIRI. janus is from Conjecture, Kraknova is at DeepMind, I don't know about nostalgebraist, Nanda is independent, Larsen works at MATS, Ajeya is at OpenPhil. So 4-5 of the top authors are double-counted.</p>", "parentCommentId": null, "user": {"username": "tkwa"}}, {"_id": "vkqbaRrPWMGTTGQv9", "postedAt": "2022-09-30T08:32:52.740Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Thanks for making this. I expect that after you make edits based on comments and such this will be the most up to date and accurate public look at this question (the current size of the field). I look forward to linking people to it!</p>\n", "parentCommentId": null, "user": {"username": "Aaron_Scher"}}, {"_id": "R4tvphCtKirgxAubi", "postedAt": "2022-09-30T19:52:41.458Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>I re-estimated the number of researchers in these organizations and the edits are shown in the 'EDITS' comment below.</p><p>Copied from the EDITS comment:</p><p>- CSER: 5-5-10 -&gt; 2-5-15<br>- FLI: 5-5-20 -&gt; 3-5-15<br>- Levelhume Centre: 5-10-70 (Low confidence) -&gt; 2-5-15 (Medium confidence)</p><p>My counts for CSER:</p><p>- full-time researchers: 3<br>- research affiliates: 4</p><p>FLI: counted 5 people working on AI policy and governance.</p><p>&nbsp;Levelhume Centre:</p><p>- 7 senior research fellows<br>- 14 research fellows</p><p>Many of them work at other organizations. I think 5 is a good conservative estimate.</p><p>New footnote for the 'Other' row in the non-technical list of researchers (estimate is 10):</p><p>\"There are about 45 research profile on Google Scholar with the 'AI governance' tag. I counted about 8 researchers who weren't at the other organizations listed.\"<br>&nbsp;</p>", "parentCommentId": "MqfEsE4oLxiscDf8D", "user": {"username": "Stephen McAleese"}}, {"_id": "tCduy2TDqezgq7CpY", "postedAt": "2022-09-30T20:07:41.392Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p><em>[Edit: I think the following no longer makes sense because the comment it's responding to was edited to add explanations, or maybe I had just missed those explanations in my first reading. See my other response instead.]</em></p>\n<p>Thanks for this. I don't see how the new estimates incorporate the above information. (The medians for CSER, Leverhulme, and FLI seem to still be at 5 each.)</p>\n<p>(Sorry for being a stickler here--I think it's important that readers get accurate info on how many people are working on these problems.)</p>\n", "parentCommentId": "R4tvphCtKirgxAubi", "user": {"username": "Mauricio"}}, {"_id": "xmvKyzqDhB4vYsbjc", "postedAt": "2022-09-30T20:20:22.945Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Edit: added Rethink Priorities to the list of non-technical organizations.</p>", "parentCommentId": "tubSF3tgah7Hbg5gS", "user": {"username": "Stephen McAleese"}}, {"_id": "tyQyWND45NYJY3He6", "postedAt": "2022-09-30T21:06:55.402Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>New estimates:</p><p>CSER: 2-5-10 -&gt; 2-3-7<br>FLI: 5-5-20 -&gt; 3-4-6<br>Levelhume: &nbsp;2-5-15 -&gt; 3-4-10</p>", "parentCommentId": "tCduy2TDqezgq7CpY", "user": {"username": "Stephen McAleese"}}, {"_id": "DX8H7qsoeX5grmrTx", "postedAt": "2022-09-30T22:02:16.492Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Edit: updated OpenAI from 5 to 10.</p><p>From their website, AI Impacts currently has 2 researchers and 2 support staff (the current total estimate is 3).</p><p>The current estimate for Epoch is 4 which is similar to most estimates here.</p><p>I'm trying to come up with a more accurate estimate for independent researchers and 'Other' researchers.</p>", "parentCommentId": "tm7yjtvrtFLTNhBZN", "user": {"username": "Stephen McAleese"}}, {"_id": "3xKhhsGF9rfawjQjM", "postedAt": "2022-10-02T17:09:25.761Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>More edits:<br>- DeepMind: 5 -&gt; 10.<br>- OpenAI: 5 -&gt; 10.<br>- Moved GoodAI from the non-technical to technical table.<br>- Added technical research organization: Algorithmic Alignment Group (MIT): 4-7.<br>- Merged 'other' and 'independent researchers' into one group named 'other' with new manually created (accurate) estimate.</p>", "parentCommentId": null, "user": {"username": "Stephen McAleese"}}, {"_id": "FXbPwtkJvpDw2a7QT", "postedAt": "2022-10-02T17:12:36.622Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>I went through all the authors from the Alignment Forum from the past ~6 months, manually researched each person and came up with a new estimate named &nbsp;'Other' of about 80 people which includes independent researchers, other people in academia and people in programs such as SERI MATS.</p>", "parentCommentId": "hSephwHj524o69jtN", "user": {"username": "Stephen McAleese"}}, {"_id": "Muenfxr4jYpmcko9t", "postedAt": "2022-10-02T17:19:21.650Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>\"Price's Law says that half of the contributions in a field come from the square root of the number of contributors. In other words, productivity increases linearly as the number of contributors increases exponentially.\" This is incorrect; the square root of an exponential is still an exponential... it's just exp(0.5 * x) rather than exp(x).</p>", "parentCommentId": null, "user": {"username": "David M. Perlman"}}, {"_id": "R8uRYq8YgmJn5TCcx", "postedAt": "2022-10-02T17:41:07.300Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Thanks, I think you're right. I'll have to edit that section.</p>", "parentCommentId": "Muenfxr4jYpmcko9t", "user": {"username": "Stephen McAleese"}}, {"_id": "k3neEMChLdofe4tnB", "postedAt": "2022-10-03T12:48:27.416Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/8ErtxW7FRPGMtDqJy/the-academic-contribution-to-ai-safety-seems-large\">Here's </a>a relevant set of estimates from a couple of years ago, which has a <a href=\"https://www.getguesstimate.com/models/16387\">guesstimate model</a> you might enjoy. Your numbers seem to be roughly consistent with theirs. They were trying to make a broader argument that \"1. EA safety is small, even relative to a single academic subfield. 2. There is overlap between capabilities and short-term safety work. 3. There is overlap between short-term safety work and long-term safety work. 4. So AI safety is less neglected than the opening quotes imply. 5. Also, on present trends, there\u2019s a good chance that academia will do more safety over time, eventually dwarfing the contribution of EA.\"</p>", "parentCommentId": null, "user": {"username": "NthOrderVices"}}, {"_id": "vBJkHMkhjWHiGXavQ", "postedAt": "2022-10-03T15:04:26.021Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Thanks this is helpful!</p>\n<p>Just a heads up my latest estimate is here in footnote 15:\n<a href=\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism#fn-15\">https://www.effectivealtruism.org/articles/introduction-to-effective-altruism#fn-15</a></p>\n<p>I went for 300 technical researchers though say the estimate seems more likely to be too high than too low, so seems like we're pretty close.</p>\n<p>(My old Twitter thread was off the top of head, and missing the last year of growth.)</p>\n<p>Glad to see more thorough work on this question :)</p>\n", "parentCommentId": null, "user": {"username": "Benjamin_Todd"}}, {"_id": "YbDDbJnjJYdBdAyAJ", "postedAt": "2022-10-05T21:11:05.874Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>The whole section on Price's Law has been replaced with a section on Lotka's Law.</p>", "parentCommentId": "Muenfxr4jYpmcko9t", "user": {"username": "Stephen McAleese"}}, {"_id": "RTdMDzL75j8cnCHig", "postedAt": "2022-10-21T22:20:37.096Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>This question would have been way easier if just I estimated the number of AI safety researchers in my city (1?) instead of the whole world.</p>", "parentCommentId": null, "user": {"username": "Stephen McAleese"}}, {"_id": "H98No8ibkvrgdiBnr", "postedAt": "2022-11-18T03:20:15.793Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>I last checked the AI Watch database a few weeks ago and it seemed very bad. E.g. missing Mark Xu, John Wentworth, Rebecca Gorman, Vivek Hebbar, and Quintin Pope, many of whom are MATS mentors! Also missing Conjecture, ARC, Apart Research, and GovAI as far as I can tell.<br><br>Given these flaws, it's strange &nbsp;that the AI Watch database is part of the most visible AI safety intro material. The <a href=\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism#fn-15\">official EA intro</a> (Ben Todd's comment suggests he wrote this?) says there are ~300 people working on AI safety; it cites a few sources including AI Watch. The <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\">80k problem profile</a> also estimates 300; as far as I can tell the only source is AI Watch.</p>", "parentCommentId": null, "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "WGMcz7gnidurbSqCz", "postedAt": "2022-11-19T05:18:59.320Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<blockquote><p><a href=\"https://web.archive.org/web/20220728170925/https://aiwatch.issarice.com/\">AI Watch</a> attempted a headcount of AI Safety researchers, which found 160 notable researchers who have worked on AI Safety.</p></blockquote><p>Where did you find the \"160 notable researchers\" part?</p>", "parentCommentId": "vBJkHMkhjWHiGXavQ", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "ATxGhreiYWiAwLSEP", "postedAt": "2022-11-19T05:20:54.036Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>nostalgebraist is <a href=\"https://nostalgebraist.tumblr.com/post/698045223162593280/taking-a-break-from-ai-discourse\">taking a break from \u201cai discourse\u201d</a></p>", "parentCommentId": "hSephwHj524o69jtN", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "wPB3DsyTmyfAZNCq5", "postedAt": "2022-12-10T07:28:24.813Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>What about the &nbsp;<a href=\"https://www.alignmentforum.org/posts/H5iGhDhQBtoDpCBZ2/announcing-the-alignment-of-complex-systems-research-group\">Alignment of Complex Systems Research Group</a> at Charles University and the <a href=\"https://www.cs.cmu.edu/~focal/index.html\">Foundations of Cooperative AI Lab</a> at CMU?&nbsp;</p>", "parentCommentId": null, "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "ixaZ5pqjG5L22q2tG", "postedAt": "2023-02-18T19:31:37.042Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>I added them to the list of technical research organizations. Sorry for the delay.</p>", "parentCommentId": "wPB3DsyTmyfAZNCq5", "user": {"username": "Stephen McAleese"}}, {"_id": "6kCXDYTfQSkMwho4h", "postedAt": "2023-08-02T20:58:24.670Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>According to Price's <a href=\"https://mondaymornings.madisoncres.com/productivity-and-prices-law-1\">Law</a>, the square root of the number of contributors contributes half of the progress. If there are 400 people working on AI safety full-time then it's quite possible that just 20 highly productive researchers are making half the contributions to AI safety research. I expect this power law to apply to both the quantity and the quality of research.</p>", "parentCommentId": null, "user": {"username": "Stephen McAleese"}}, {"_id": "iH6ZCxfJdkWvxHKx5", "postedAt": "2023-09-29T10:19:51.370Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Do you have any sense of how many may be working predominately on near-term risks vs. long term risks? I'd be interested to know the latter number, and have reason to believe it would be lower than this estimate (i.e. because I highly doubt OpenAI has 10 people working fully on the long term risks instead of the near term stuff that they generally seem more interested in)</p>", "parentCommentId": null, "user": {"username": "tswizzle96"}}, {"_id": "GfargSKJusafbtntd", "postedAt": "2023-09-30T08:45:26.878Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>At OpenAI, I'm pretty sure there are far more people working on near-term problems that long-term risks. Though the Superalignment team now has over 20 people from what I've heard.</p>\n", "parentCommentId": "iH6ZCxfJdkWvxHKx5", "user": {"username": "Stephen McAleese"}}, {"_id": "rHJTJ3D2rCwaWzBwr", "postedAt": "2023-09-30T10:09:32.816Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Oh really? I thought it was far smaller, like in the range of 5-10.&nbsp;</p>", "parentCommentId": "GfargSKJusafbtntd", "user": {"username": "tswizzle96"}}, {"_id": "ZXuD5iZexAvvYsHWt", "postedAt": "2023-09-30T10:37:08.636Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>The Superalignment team currently has about 20 people according to <a href=\"https://www.alignmentforum.org/posts/bsNXqHgiDA6dAKNun/axrp-episode-24-superalignment-with-jan-leike#Superalignment_team_logistics_\">Jan Leike</a>. Previously I think the scalable alignment team was much smaller and probably only 5-10 people.</p>", "parentCommentId": "rHJTJ3D2rCwaWzBwr", "user": {"username": "Stephen McAleese"}}, {"_id": "eCgir7oz3Cjydi7pR", "postedAt": "2023-09-30T11:21:53.006Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Good update, thanks for sourcing!&nbsp;</p>", "parentCommentId": "ZXuD5iZexAvvYsHWt", "user": {"username": "tswizzle96"}}, {"_id": "9EsGZLz8Z2QnBuPpk", "postedAt": "2023-09-30T11:36:46.611Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Do you relatedly have any read on the current number of AI Saftey research papers?</p>", "parentCommentId": null, "user": {"username": "tswizzle96"}}, {"_id": "dPJ55GKKBJBEmbmcJ", "postedAt": "2023-10-01T09:21:06.174Z", "postId": "3gmkrj3khJHndYGNe", "htmlBody": "<p>Good question. I haven't done much research on this but a paper named <a href=\"https://arxiv.org/pdf/2206.02841.pdf\">Understanding AI alignment research: A Systematic Analysis</a> found that the rate of new Alignment Forum and arXiv preprints grew from less than 20 per year in 2017 to over 400 per year in 2022. However, the number of Alignment Forum posts has grown much faster than the number of arXiv preprints.&nbsp;</p>", "parentCommentId": "9EsGZLz8Z2QnBuPpk", "user": {"username": "Stephen McAleese"}}]