[{"_id": "nAcooMxt7taX8koGd", "postedAt": "2017-03-11T19:28:11.015Z", "postId": "6F6ix64PKEmMuDWJL", "htmlBody": "<p>I think one aim here is to stop people from conflating other things with cause impartiality, which does seem like an unhelpful thing that people occasionally consciously or subconsciously do.</p>\n", "parentCommentId": "QXxWZ7R3n6eyviona", "user": {"username": "RyanCarey"}}, {"_id": "teGvLufmD8DnQzZzu", "postedAt": "2017-03-15T01:18:31.536Z", "postId": "6F6ix64PKEmMuDWJL", "htmlBody": "<p>The discussion of <a href=\"http://lesswrong.com/lw/o9h/further_discussion_of_cfars_focus_on_ai_safety/\">CFAR's pivot to focusing on existential risk</a> seemed to use &quot;cause-neutral&quot; to mean something like &quot;cause-general&quot;.</p>\n<p>Confusingly, the way &quot;cause-neutral&quot; was used there directly contradicts its use here: there, it meant <em>avoiding</em> cause-impartially favoring a specific cause based on its apparent expected value, in favor of a cause-partial commitment to pet causes like rationality and EA capacity-building. (Admittedly, at the organizational level it often makes sense to codify some &quot;pet causes&quot; even if in principle the individuals in that organization are trying to maximize global welfare impartially.)</p>\n", "parentCommentId": "QXxWZ7R3n6eyviona", "user": {"username": "RobBensinger"}}]