[{"_id": "ifnhppnRxggbFt4sW", "title": "Expert trap (Part 2 of 3) \u2013 how hindsight, hierarchy, and confirmation biases break conductivity and accuracy of knowledge", "postedAt": "2023-06-09T22:53:45.158Z", "htmlBody": "<p><i>Crossposted from&nbsp;</i><a href=\"https://sysiak.substack.com\"><i>Pawel\u2019s blog</i></a></p><p><i>This essay has three parts. </i><a href=\"https://forum.effectivealtruism.org/posts/sz7sYj3XFENNecLDa/expert-trap-part-1-of-3-how-hindsight-hierarchy-and\"><i>In part one</i></a><i>, I include notes on epistemic status, I give a summary of the topic. But mainly I describe what is the expert trap.</i></p><p><i>Part two is about context. In \u201cWhy is expert trap happening?\u201d I dive deeper explaining biases and dynamics behind it. Then in \u201cExpert trap in the wild\u201d I try to point out where it appears in reality.</i></p><p><a href=\"https://forum.effectivealtruism.org/posts/QJdCgwhaQm3c4hnFK/expert-trap-ways-out-part-3-of-3-how-hindsight-hierarchy-and\"><i>Part three</i></a><i> is about \u201cWays out\u201d. I list my main ideas of how to counteract the expert trap. I end with conclusions and with a short Q&amp;A.</i></p><p><i>How to read it? In this part, there may be a lot of knowledge that you already are familiar with. All chapters make sense on their own. Feel free to treat this article like a Q&amp;A page and skip parts of it.</i></p><h1>Why is expert trap happening?</h1><p>I think there are three main causes of the expert and they are: forgetting, hierarchy bias, and my-side bias.</p><h3>Forgetting</h3><p>First, forgetting. Keep in mind I am not quoting any relevant research here. This is my hypothesis. The core mechanism is the following. When we learn something we forget how was it not to understand it. We are in a constant process of forming new memories and forgetting old ones. Some are prioritized over others. In context of learning, as soon as we learn something we tend to deprioritize reasons why it was hard to understand it. It may be costly to keep the score of it because we need to keep a second, seemingly more important thread \u2013 how to understand the problem we just learned about. And these two are different. It is as if we unplug certain cables connecting old contexts and plug them into new ones. Maybe they seem redundant to our brain as the direction is to keep ascending further.</p><p>Also, if my-side bias is in play (on that later) we will deprioritize memories that don\u2019t strengthen our self-image. Sate of confusion, not being able to understand things feel unpleasant like a shoe that is too tight. So we may deprioritize these memories and prioritize ones that are leaving us with positive states: Everything fits correctly. I am smart.</p><h3>Hierarchy bias</h3><p>I see the expert trap as largely caused by <a href=\"https://sysiak.com/cognitive-biases/hierarchy-bias\">hierarchy bias</a>. In <i><strong>Elephant in the brain</strong></i> Robin Hanson and Kevin Simler explain this distortion. The main thesis of the book is that we are deeply hierarchical creatures and at the same time don\u2019t view ourselves this way.</p><p><i>Elephant in the Brain</i> made a large impression on my thinking. Suddenly a lot of convoluted reasoning paths straightened up. The consequences of it, however, were quite hard to digest. Quite a lot of them are ugly to imagine. No wonder this mechanism is hidden from us. One idea that helped me was to realize that hierarchies, which often feel to me strongly negative, can be viewed in a positive light. They enable collaboration, motivate people to take action, and are also magnets and filters for the right communities to form.</p><p>The authors explain the main thesis of the book with some uncertainty. It makes sense to be skeptical here. It is a large reinterpretation of how we usually assume human motivations work. It\u2019s not that hierarchies are completely overlooked, but Hanson and Simler assert a significantly broader influence they have on human motivations. I wrestled with these ideas for a bit and stumbled upon a take that was revelatory to me. I think one can be more certain of a hypothesis when evidence for it comes from different knowledge areas. Frans de Wall explains something similar but from the perspective of a primatologist:</p><blockquote><p>In primatology, when we talk about primates, almost everything is viewed through a perspective of a hierarchy in the group. In social sciences, when we talk about humans, hierarchy is rarely ever mentioned</p></blockquote><p><i>Elephant in the Brain</i> also argues that the extent our brains are driven by hierarchies is inherently hidden from us. If we were to be aware that we are primarily motivated by ascending hierarchies, we would potentially be a lot worse at it. It\u2019s a lot easier to deceive other people if you yourself don\u2019t know you\u2019re attempting to deceive them.</p><p>I think finding from Elephant in the brain also applies to learning. One of the main motivations behind the drive to acquire knowledge is a need to impress others and ascend in hierarchies. Robin Hanson sees three main functions of academia. Alongside preserving and teaching, \u201cAcademia functions to create and confer prestige to associated researchers, students, firms, cities, and nations.\u201d <a href=\"https://www.overcomingbias.com/2022/10/more-academic-prestige-futures.html\">Link</a></p><p>This may help explain our drive to use jargon. Sometimes more complex vocabulary is a shortcut to create a more specific definition. However, it may more often be used because people want to be viewed as more knowledgeable. Again this is most likely a subconscious force. So people who use jargon unnecessarily are not only confusing others but also themselves.</p><h3>My-side bias</h3><p>The third dynamic causing the expert trap is <a href=\"https://sysiak.com/cognitive-biases/my-side-bias\">my-side bias</a> (largely overlapping with the definition of \u2019motivated reasoning\u2019). We have a fundamentally distorted view of who we are. Our ego distorts facts, manufactures impressions and memories to perceive ourselves in the most positive light. This dynamic is described at length by Daniel Kahneman, Daniel Gilbert, Julia Galef among others. I wrote a more extensive description of <a href=\"https://sysiak.com/cognitive-biases/my-side-bias\">my-side bias</a>, but here is the summary. There are two main sub-mechanisms of it: the illusory-superiority and ego-shield</p><p>First, illusory superiority. Our ego distorts facts, manufactures impressions and memories to create the best possible self-image.</p><p>The majority of people think they are above average. In one study, 96 percent of cancer patients claimed to be in better health than the average cancer patient.</p><p>We select positive and filter out negative information about ourselves. In one study researchers showed that if somebody praises a person \u2013 they will look for evidence of how competent the source is. If somebody criticizes them \u2013 they will look for evidence of how incompetent the source is.</p><p>We evaluate things more positively once they become our own. \u201cConsumers evaluate kitchen appliances more positively after they buy them, job seekers evaluate jobs more positively after they accept them, and high school students evaluate colleges more positively after they get into them. Racetrack gamblers evaluate their horses more positively when they are leaving the betting window than when they are approaching it, and voters evaluate their candidates more positively when they are exiting the voting booth than when they are entering it.\"</p><p>We look for positive explanations of things we are already doing. If we are eating ice cream we will think it\u2019s not as bad as if we weren\u2019t eating it.</p><p>We evaluate actions higher when we realize they were done by us. We will find more mistakes in our own work if we were tricked to think it wasn\u2019t done by us.</p><p>My-side bias also works as ego-shield, something like the immune system for our psychology. When experiences make us feel sufficiently unhappy this system will kick in and we will shift blame or manufacture facts in order to create more positive versions of memories. \u201cAble-bodied people are willing to pay far more to avoid becoming disabled than disabled people are willing to pay to become able-bodied again because able-bodied people underestimate how happy disabled people are.\u201d examples from <strong>Stumbling on Happiness</strong>.</p><p>I believe <a href=\"https://sysiak.com/cognitive-biases/confirmation-bias\">confirmation bias</a> is a sub-effect of my-side bias. It\u2019s something very similar to illusory superiority described above but in relation to opinions we hold. From the ocean of available data, we filter for information that strengthens views we already have.</p><p>In one study researchers juxtaposed two groups. One was pro-capital punishment and another was against it. Researchers fabricated two texts supporting each group's ideas in an equal way. Both groups read both texts. Groups came out of this exercise more polarized, believing stronger than they believed before. <a href=\"https://www.researchgate.net/publication/232555483_Biased_Assimilation_and_Attitude_Polarization_The_Effects_of_Prior_Theories_on_Subsequently_Considered_Evidence\">Link</a></p><p>If we do something, we\u2019re less critical of it and interpret it as a job better done. In one study participants were tasked with finding errors in simple reasoning exercises. Participants first did the exercises themselves. When given the chance to do revisions, they didn\u2019t make many corrections. They were a lot more effective at spotting their own mistakes when researchers told them they were looking at somebody else's work.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec0f0ed7-8e5f-47b2-98c1-7b481408273d_960x720.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec0f0ed7-8e5f-47b2-98c1-7b481408273d_960x720.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec0f0ed7-8e5f-47b2-98c1-7b481408273d_960x720.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec0f0ed7-8e5f-47b2-98c1-7b481408273d_960x720.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec0f0ed7-8e5f-47b2-98c1-7b481408273d_960x720.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec0f0ed7-8e5f-47b2-98c1-7b481408273d_960x720.png 1456w\"></a></p><h1>Expert trap in the wild</h1><h3>Replication crisis</h3><p>It has been found that many classical scientific studies that form the foundation of modern science are difficult or impossible to reproduce. This has been particularly widely discussed in the fields of psychology and medicine. In 2017 in response to this crisis, eight hundred scientists and mathematicians signed a paper to \"<a href=\"https://www.nature.com/articles/s41562-017-0189-z\">Redefine statistical significance</a>\". It proposes that in \"fields where the threshold for defining statistical significance for new discoveries is P &lt; 0.05, we propose a change to P &lt; 0.005\". Daniel Kahneman, who spoke widely on this subject, sees my-side and confirmation biases as the main drivers. To put it simply \u2014 scientists may be subconsciously finding ways to prove theories that will make them better scientists, making them more highly acclaimed in their field.</p><h3>Experts are often worse at predicting reality</h3><p>According to research by Philip Tetlock, laid out in the book <i>Expert Political Judgment,</i> experts are often worse at predicting reality than informed non-experts:</p><blockquote><p>\"Studies have found that deep expertise in a subject does not positively correlate with accuracy in judgment. As part of his research on forecasting, professor Phillip Tetlock conducted a study with 284 political experts, that generated over 80,000 informed (where the estimate matched the area of expertise of the individual) and uninformed predictions over the course of twenty years. Surprisingly, Tetlock discovered that specialists are less reliable than non-experts, even within their specific area of study. In fact, the study concludes that after a certain point, deepening one's knowledge about a specific topic is affected by the law of diminishing returns and can hinder the ability to accurately predict a certain outcome. The results of the study can be attributed to the fact that subject matter experts are more likely to suffer from confirmation bias and are more likely to feel the pressure associated with reputational damage, both of which can affect their ability to produce accurate predictions.\u201d <a href=\"https://magur.no/essays/a-playbook-for-expressive-products/\">Link</a></p></blockquote><h3>Education system: learning sounds not concepts</h3><p>I believe large parts of educational systems are riddled with the expert trap. Most students experience school as boring. I think this is caused by circulating knowledge that has low conductivity and is, in large parts, illusory. When something is deeply understood, knowledge works across distant areas, and resulting synthesis connects and untangles things. If knowledge is delivered this way it feels energizing, enlightening, and revelatory. Later I will describe Richard Feynman\u2019s approach to learning. His incredible lecture <a href=\"https://youtu.be/P1ww1IXRfTA\">\u201cFun to Imagine\u201d</a> unifies chemistry, math, and physics into one interconnected field.</p><p>Eliezer Yudkowsky in his essay <a href=\"https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password\">\u201dGuessing the Teacher\u2019s Password\u201d</a> points to the failed dynamics of education. Largely how we are habituated to learn is through memorization and guessing teachers\u2019 answers.</p><blockquote><p>There is an instinctive tendency to think that if a physicist says \u201clight is made of waves,\u201d and the teacher says \u201cWhat is light made of?\u201d and the student says \u201cWaves!\u201d, then the student has made a true statement. That\u2019s only fair, right? We accept \u201cwaves\u201d as a correct answer from the physicist; wouldn\u2019t it be unfair to reject it from the student?</p><p>Suppose the teacher asks you why the far side of a metal plate feels warmer than the side next to the radiator. If you say \u201cI don\u2019t know,\u201d you have&nbsp;<i>no</i>&nbsp;chance of getting a gold star\u2014it won\u2019t even count as class participation. But, during the current semester, this teacher has used the phrases \u201cbecause of heat convection,\u201d \u201cbecause of heat conduction,\u201d and \u201cbecause of radiant heat.\u201d One of these is probably what the teacher wants. You say, \u201cEh, maybe because of heat conduction?\u201d This is not a hypothesis&nbsp;<i>about</i>&nbsp;the metal plate. This is not even a proper belief. It is an attempt to&nbsp;<i>guess the teacher\u2019s password.</i></p><p>\u201cWhat remains is not a belief, but a verbal behavior.\u201d</p></blockquote><p>Real learning, Eliezer points out, is about being aware of the difference between an explanation and a password. Learning is about finding knowledge that is in close contact with how we anticipate it should show up in reality. If that hypothesis is true what differences do I expect to see in reality? If that hypothesis is true what I shouldn\u2019t expect to encounter?</p><blockquote><p>Maybe, if we drill students that&nbsp;<i>words don\u2019t count, only anticipation-controllers,</i>&nbsp;the student will&nbsp;<i>not</i>&nbsp;get stuck on \u201cHeat conduction? No? Maybe heat convection? That\u2019s not it either?\u201d</p></blockquote><p>&nbsp;</p><p>Read <a href=\"https://forum.effectivealtruism.org/posts/QJdCgwhaQm3c4hnFK/expert-trap-ways-out-part-3-of-3-how-hindsight-hierarchy-and\">Expert trap: Ways out (Part 3 of 3)</a></p>", "user": {"username": "pawsys"}}, {"_id": "K85qGvjqnJbznNgiY", "title": "Strawmen, steelmen, and mithrilmen: getting the principle of charity right", "postedAt": "2023-06-09T13:02:14.448Z", "htmlBody": "<p>Something you hear a lot in discussions is that it's important not to <i>strawman </i>arguments: to assume they are much weaker than they are. This is uncharitable.</p><p>Instead, the suggestion is you should <i>steelman </i>arguments: consider the strongest version of it, even if that's not what the person has said, and then evaluate that. Steelmanning is thought of as getting the principle of charity just right.</p><p>However, I suspect there should be a third category, call it <i>mithrilman: </i>where arguments are treated as much stronger than they are, and you accept them even though you don't understand the reasons. For the non-nerds out there, mithril is Tolkien's fictional super-strong metal in Lord of the Rings.&nbsp;</p><p>Whilst strawmanning is being too <i>un</i>charitable, mithrilmanning is being <i>too </i>charitable. You don't want to do either. Goldilocks lies inbetwixt the two.&nbsp;</p><p>I see mithrilmanning quite a lot among effective altruists. Usually, it goes something like this. People are discussing a view or argument they've heard person X make. The individuals are sitting around, brows furrowed, and struggling to find a good steelman of the argument: they can't work out what plausible reasons that person could have for their conclusion. After a while, even though they can't find a suitable steelman, someone says \"Well, X <i>does </i>seem really smart, so...\". Everyone nods. The conversation moves on.</p><p>What's happened is that someone has suggested the group should defer, even though they can't follow the reasoning or provide it themselves. This seems to happen much more often when person X is important (not least because you don't want to risk looking stupid).&nbsp;</p><p>I think there can be good cases where one should defer, but I'm worried I see too much of this. We should give people the benefit of the doubt - assume they are smart, thoughtful, etc. rather than fools - but we should still doubt. To err is human. We all make mistakes. We make progress by pointing those out.&nbsp;</p><p>So, if you think someone is really smart, but you can't make sense of what they are thinking, at least hesitate on deferring to them. If possible, ask them to explain. It seems too charitable to assume they are right, not charitable enough to assume they are wrong. In assuming that they can give you a sensible answer, you are treating them with appropriate charity.&nbsp;</p><p>I don't think I need to say why strawmanning is bad. The danger of mithrilmanning is you end up with too much deference, an information cascade and ultimately false beliefs. People end up believing what X says, even though no one really understands why.&nbsp;</p><p>So, if you find yourself overhearing, or even saying yourself, \"well, they do seem really smart...\" consider adding \"um, are we mithrilmanning this? We don't want to defer uncritically.\"</p>", "user": {"username": "MichaelPlant"}}, {"_id": "J4cLuxvAwnKNQxwxj", "title": "How does AI progress affect other EA cause areas?", "postedAt": "2023-06-09T12:43:05.741Z", "htmlBody": "<p>The EA community has spent a lot of time thinking about transformative AI. In particular, there is a lot of research on x-risks from transformative AI, and on how transformative AI development will unfold. However, advances in AI have many other consequences which appear crucial for guiding strategic decisionmaking in areas besides AI risk, and I haven't seen/found much material about these implications.</p><p>Here is one example of why this matters. In the upcoming decades, AI advancements will likely cause substantial changes to what the world looks like. The more the world changes, the less likely it is that research done earlier still applies to that context. The degree to which research is affected by this will depend on the type of research, but I expect the average effect to be relatively large. Therefore, we should discount the value of research in proportion to the expected loss in generalizability over time.</p><p>Another way in which AI could influence the value of research is by being able to entirely automate it. If such AI is quick enough, and able to decide what types of research should be done, then there's no role for humans to play in doing research anymore. Thus, from that point onwards, human capital ceases to be useful for research. Furthermore, such AI could redo research that was done until that point, so (to a first approximation) the impact of research done beforehand would cease when AI has these capabilities. Similarly to the previous consideration, it implies that we should discount the value of research (and career capital) over time by the probability of such development occurring.</p><p>I suspect that there are many other ways in which AI might affect our prioritization. For example, it could lower the value of poverty reduction interventions (due to accelerated growth), or increase the value of interventions that allow us to influence decisionmaking/societal values. It should also change the relative value of influencing certain key actors, based on how powerful we expect them to become as AI advances.</p><p>I'd really appreciate any thoughts on these considerations or links to relevant material!</p>", "user": {"username": "LuisMota"}}, {"_id": "sqYtEGMqYWTR4YXZC", "title": "\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u3068\u306f\u4f55\u304b", "postedAt": "2023-06-09T14:59:59.631Z", "htmlBody": "<p><i>This is a Japanese translation of \"</i><a href=\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism\"><i>What is effective altruism?</i></a><i>\"</i></p><h1>\u306f\u3058\u3081\u306b</h1><p>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\uff08effective altruism\uff09\u306f\u3001\u4ed6\u8005\u3092\u52a9\u3051\u308b\u305f\u3081\u306e\u6700\u826f\u306e\u65b9\u6cd5\u3092\u767a\u898b\u3057\u3001\u5b9f\u8df5\u3059\u308b\u3053\u3068\u3092\u76ee\u6a19\u306b\u63b2\u3052\u305f\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u3059\u3002</p><p>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306f\u3001\u3053\u306e\u4e16\u754c\u306e\u6700\u3082\u5207\u8feb\u3057\u305f\u554f\u984c\u3068\u305d\u306e\u6700\u826f\u306e\u89e3\u6c7a\u7b56\u306e\u7279\u5b9a\u3092\u76ee\u6307\u3059<strong>\u7814\u7a76\u9818\u57df</strong>\u3067\u3042\u308b\u3068\u3068\u3082\u306b\u3001\u305d\u3046\u3057\u305f\u767a\u898b\u3092\u6d3b\u7528\u3057\u3066\u3008\u3088\u3044\u3053\u3068\u3009\u3092\u884c\u3046\u3053\u3068\u3092\u76ee\u6307\u3059<strong>\u5b9f\u8df5\u30b3\u30df\u30e5\u30cb\u30c6\u30a3</strong>\u3067\u3082\u3042\u308a\u307e\u3059<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0ll7h79wcif\"><sup><a href=\"#fn0ll7h79wcif\">[1]</a></sup></span>\u3002</p><p>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306e\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304c\u91cd\u8981\u306a\u306e\u306f\u3001\u3008\u3088\u3044\u3053\u3068\u3009\u3092\u884c\u304a\u3046\u3068\u3044\u3046\u591a\u304f\u306e\u8a66\u307f\u304c\u5931\u6557\u306b\u7d42\u308f\u308b\u4e00\u65b9\u3067\u3001\u4e00\u90e8\u306e\u8a66\u307f\u306f\u9014\u8f4d\u3082\u306a\u304f\u52b9\u679c\u7684\u3067\u3042\u308b\u3001\u3068\u3044\u3046\u3053\u3068\u304c\u3042\u308b\u304b\u3089\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u540c\u3058\u91cf\u306e\u8cc7\u6e90\u3092\u4e0e\u3048\u3089\u308c\u305f\u3068\u304d\u306b\u3001\u4e00\u90e8\u306e\u6148\u5584\u56e3\u4f53\u306f\u4ed6\u306e\u56e3\u4f53\u3088\u308a\u3082100\u500d\u3001\u3055\u3089\u306b\u306f1000\u500d\u3082\u306e\u6570\u306e\u4eba\u3073\u3068\u3092\u52a9\u3051\u308b\u3053\u3068\u3059\u3089\u3042\u308a\u307e\u3059\u3002</p><p>\u3053\u308c\u306f\u3064\u307e\u308a\u3001\u652f\u63f4\u306e\u305f\u3081\u306e\u6700\u826f\u306e\u65b9\u6cd5\u306b\u3064\u3044\u3066\u614e\u91cd\u306b\u8003\u3048\u308b\u3053\u3068\u3067\u3001\u3053\u306e\u4e16\u754c\u306e\u5927\u554f\u984c\u306b\u5bfe\u3057\u3066\u3001\u305a\u3063\u3068\u591a\u304f\u306e\u3053\u3068\u3092\u6210\u3057\u9042\u3052\u3089\u308c\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002</p><p>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306f\u30aa\u30c3\u30af\u30b9\u30d5\u30a9\u30fc\u30c9\u5927\u5b66\u306e\u5b66\u8005\u304c\u5b9a\u5f0f\u5316\u3057\u305f\u3082\u306e\u3067\u3059\u304c\u3001\u73fe\u5728\u3067\u306f\u4e16\u754c\u4e2d\u306b\u666e\u53ca\u3057\u300170\u4ee5\u4e0a\u306e\u56fd\u3005\u306e\u3001\u4f55\u4e07\u4eba\u3082\u306e\u4eba\u3073\u3068\u304c\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u3092\u53d6\u308a\u5165\u308c\u3066\u3044\u307e\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefi1cu6phuiwa\"><sup><a href=\"#fni1cu6phuiwa\">[2]</a></sup></span></p><p>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306b\u89e6\u767a\u3055\u308c\u305f\u4eba\u3073\u3068\u306f\u3001\u30de\u30e9\u30ea\u30a2\u3092\u9632\u3050\u305f\u3081\u306e2\u5104\u5e33\u3082\u306e\u868a\u5e33\u306e\u914d\u5e03\u306b\u8cc7\u91d1\u3092\u63d0\u4f9b\u3057\u305f\u308a\u3001AI\u306e\u672a\u6765\u306b\u95a2\u3059\u308b\u5b66\u8853\u7814\u7a76\u3084\u6b21\u306e\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u3092\u9632\u3050\u305f\u3081\u306e\u653f\u7b56\u30ad\u30e3\u30f3\u30da\u30fc\u30f3\u3092\u884c\u3046\u306a\u3069\u3001\u69d8\u3005\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u53d6\u308a\u7d44\u3093\u3067\u304d\u307e\u3057\u305f\u3002</p><p>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u8005\u3092\u675f\u306d\u3066\u3044\u308b\u306e\u306f\u3001\u4e16\u754c\u306e\u8af8\u554f\u984c\u306b\u5bfe\u3059\u308b\u7279\u5b9a\u306e\u89e3\u6c7a\u7b56\u3067\u306f\u306a\u304f\u3001\u8003\u3048\u65b9\u3067\u3059\u3002\u3059\u306a\u308f\u3061\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u8005\u306f\u3001\u4e00\u5b9a\u91cf\u306e\u52aa\u529b\u3067\u6841\u5916\u308c\u306e\u52b9\u679c\u3092\u751f\u307f\u51fa\u3059\u3088\u3046\u306a\u3001<u>\u6841\u5916\u308c\u306b</u>\u3008\u3088\u3044\u3009\u652f\u63f4\u65b9\u6cd5\u3092\u898b\u3064\u3051\u51fa\u305d\u3046\u3068\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u3067\u306f\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u8005\u306e\u3053\u308c\u307e\u3067\u306e\u53d6\u308a\u7d44\u307f\u3092\u3044\u304f\u3064\u304b\u7d39\u4ecb\u3057\u3001\u305d\u306e\u3042\u3068\u3067\u3001\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u8005\u305f\u3061\u3092\u675f\u306d\u308b\u8af8\u4fa1\u5024\u306b\u3064\u3044\u3066\u8ff0\u3079\u307e\u3059\u3002</p><h1>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306e\u5b9f\u8df5\u4f8b\u306f\uff1f</h1><h2><strong>\u6b21\u306e\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u3092\u9632\u3050</strong></h2><p><strong>\u3053\u306e\u554f\u984c\u306b\u53d6\u308a\u7d44\u3080\u7406\u7531</strong></p><p>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306b\u643a\u308f\u308b\u4eba\u3073\u3068\u306f<a href=\"https://forum.effectivealtruism.org/topics/itn-framework\"><u>\u5178\u578b\u7684\u306b\u306f</u></a>\u3001\u898f\u6a21\u304c\u5927\u304d\u304f\u3001\u53d6\u308a\u7d44\u307f\u3084\u3059\u304f\u3001\u4e0d\u5f53\u306b\u7121\u8996\u3055\u308c\u3066\u304d\u305f\u554f\u984c\u3092\u540c\u5b9a\u3057\u3088\u3046\u3068\u3057\u307e\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefekd7h7nqi1u\"><sup><a href=\"#fnekd7h7nqi1u\">[3]</a></sup></span>&nbsp;\u76ee\u6a19\u306f\u3001\u73fe\u6bb5\u968e\u3067\u306a\u3055\u308c\u3066\u3044\u308b\u8a66\u307f\u306e\u6700\u5927\u306e\u76f2\u70b9\u3092\u898b\u3064\u3051\u51fa\u3057\u3001\u8ffd\u52a0\u306e\u4eba\u6750\u304c\u6700\u5927\u306e\u30a4\u30f3\u30d1\u30af\u30c8\u3092\u3082\u3066\u308b\u9818\u57df\u3092\u767a\u898b\u3059\u308b\u3053\u3068\u306b\u3042\u308a\u307e\u3059\u3002\u3053\u3046\u3057\u305f\u57fa\u6e96\u3092\u6e80\u305f\u3059\u3088\u3046\u306b\u601d\u308f\u308c\u308b\u8ab2\u984c\u306e\u3072\u3068\u3064\u304c\u3001\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u3092\u9632\u3050\u3053\u3068\u3067\u3059\u3002<br><br>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306e\u7814\u7a76\u8005\u306f<a href=\"https://www.openphilanthropy.org/research/biosecurity/\"><u>2014\u5e74\u6642\u70b9\u3067\u65e2\u306b</u></a>\u3008\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306e\u767a\u751f\u3068\u5e38\u306b\u96a3\u308a\u5408\u308f\u305b\u3060\u3063\u305f\u3053\u308c\u307e\u3067\u306e\u6b74\u53f2\u3092\u8e0f\u307e\u3048\u308b\u3068\u3001\u5927\u898f\u6a21\u306a\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u304c\u6211\u3005\u306e\u751f\u304d\u3066\u3044\u308b\u3046\u3061\u306b\u8d77\u3053\u308b\u898b\u8fbc\u307f\u306f\u9ad8\u3044\u3009\u3068\u8ad6\u3058\u3066\u3044\u307e\u3057\u305f\u3002</p><p>\u3057\u304b\u3057\u6b21\u306e\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u3092\u9632\u3050\u305f\u3081\u306b\u6295\u5165\u3055\u308c\u305f\u8cc7\u91d1\u306e\u984d\u306f\u3001\u4ed6\u306e\u4e16\u754c\u7684\u554f\u984c\u3068\u6bd4\u3079\u3066\u3068\u3066\u3082\u5c11\u306a\u304b\u3063\u305f\u3067\u3059\u3057\u3001\u305d\u308c\u306f\u4eca\u3082\u5909\u308f\u308a\u307e\u305b\u3093\u3002\u4f8b\u3048\u3070\u3001\u904e\u53bb10\u5e74\u9593\u3001\u7c73\u56fd\u306f\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u3092\u9632\u3050\u305f\u3081\u306b\u5e74\u9593\u304a\u3088\u305d80\u5104\u30c9\u30eb\u3092\u6295\u5165\u3057\u3066\u3044\u308b\u306e\u306b\u5bfe\u3057\u3066\u3001\u30c6\u30ed\u5bfe\u7b56\u306b\u8cbb\u3084\u3057\u305f\u984d\u306f\u5e74\u9593\u7d042,800\u5104\u30c9\u30eb\u3067\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref11tgvd21b6dg\"><sup><a href=\"#fn11tgvd21b6dg\">[4]</a></sup></span></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sqYtEGMqYWTR4YXZC/bisnxnkzx1lyegfsx2dv\"><br><br>\u30c6\u30ed\u3092\u9632\u3050\u306e\u306f\u78ba\u304b\u306b\u91cd\u8981\u3067\u3059\u3002\u3057\u304b\u3057\u554f\u984c\u306e\u898f\u6a21\u306f\u6bd4\u8f03\u7684\u5c0f\u3055\u3044\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u6b7b\u8005\u6570\u3060\u3051\u306b\u7126\u70b9\u3092\u5f53\u3066\u308c\u3070\u3001\u904e\u53bb50\u5e74\u9593\u3067\u30c6\u30ed\u306b\u3088\u3063\u3066\u6bba\u5bb3\u3055\u308c\u305f\u4eba\u306e\u6570\u306f\u7d0450\u4e07\u4eba\u3067\u3059\u3002\u3057\u304b\u3057COVID-19\u3060\u3051\u30672,100\u4e07\u4ee5\u4e0a\u3082\u306e\u4eba\u3073\u3068\u306e\u547d\u304c\u596a\u308f\u308c\u3066\u3044\u307e\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefb8jr023kwyr\"><sup><a href=\"#fnb8jr023kwyr\">[5]</a></sup></span>\u3042\u308b\u3044\u306f\u3001HIV/AIDS\u306b\u3088\u3063\u30664,000\u4e07\u4eba\u3082\u306e\u547d\u304c\u596a\u308f\u308c\u305f\u3053\u3068\u3082\u8003\u616e\u3059\u3079\u304d\u3067\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdld8ohip7um\"><sup><a href=\"#fndld8ohip7um\">[6]</a></sup></span></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sqYtEGMqYWTR4YXZC/ckttqccqjv3ebelcx2al\"></p><p>\u52a0\u3048\u3066\u3001\u5c06\u6765\u306e\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306fCOVID-19\u3088\u308a\u3082\u305a\u3063\u3068\u6df1\u523b\u306a\u3082\u306e\u306b\u5bb9\u6613\u306b\u306a\u308a\u3048\u307e\u3059\u3002\u3059\u306a\u308f\u3061\u3001\u30aa\u30df\u30af\u30ed\u30f3\u682a\u3088\u308a\u3082\u611f\u67d3\u529b\u304c\u5f37\u304f\u3001\u304b\u3064\u5929\u7136\u75d8\u3068\u540c\u3058\u304f\u3089\u3044\u81f4\u547d\u7684\u306a\u75c5\u6c17\u304c\u767a\u751f\u3059\u308b\u53ef\u80fd\u6027\u3092\u6392\u9664\u3059\u308b\u3082\u306e\u306f\u4f55\u3082\u3042\u308a\u307e\u305b\u3093\u3002\uff08\u6bd4\u8f03\u306b\u3064\u3044\u3066\u3088\u308a\u8a73\u3057\u304f\u306f\u811a\u6ce84\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002\uff09</p><p>\u3072\u3068\u305f\u3073\u898f\u6a21\u304c\u5927\u304d\u304f\u3001\u304b\u3064\u898b\u904e\u3054\u3055\u308c\u3066\u3044\u308b\u554f\u984c\u304c\u7279\u5b9a\u3055\u308c\u308b\u306a\u3089\u3001\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306e\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306f\u305d\u306e\u554f\u984c\u306e\u89e3\u6c7a\u306b\u5927\u304d\u304f\u5bc4\u4e0e\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u3001\u304b\u3064\u305d\u306e\u554f\u984c\u306b\u73fe\u5728\u53d6\u308a\u7d44\u3093\u3067\u3044\u308b\u4ed6\u306e\u4eba\u3073\u3068\u306b\u306f\u898b\u904e\u3054\u3055\u308c\u3066\u3044\u308b\u89e3\u6c7a\u7b56\u3092\u63a2\u3057\u51fa\u305d\u3046\u3068\u3059\u308b\u308f\u3051\u3067\u3059\u304c\u3001\u3053\u308c\u304c\u6b21\u306e\u8a71\u984c\u306b\u3064\u306a\u304c\u308a\u307e\u3059\u3002</p><p><strong>\u3053\u308c\u307e\u3067\u306e\u53d6\u308a\u7d44\u307f</strong></p><p>2016\u5e74\u3001\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306b\u89e6\u767a\u3055\u308c\u3066\u3067\u304d\u305f\u8ca1\u56e3\u3067\u3042\u308b Open Philanthropy \u306f<a href=\"https://www.centerforhealthsecurity.org/\"><u>\u30b8\u30e7\u30f3\u30ba\u30fb\u30db\u30d7\u30ad\u30f3\u30ba\u5927\u5b66\u5065\u5eb7\u5b89\u5168\u4fdd\u969c\u30bb\u30f3\u30bf\u30fc\uff08Johns Hopkins Center for Health Security\uff09</u></a>\u306e\u6700\u5927\u306e\u8cc7\u91d1\u63d0\u4f9b\u8005\u3068\u306a\u308a\u307e\u3057\u305f\u3002\u3053\u306e\u30bb\u30f3\u30bf\u30fc\u306f\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306b\u5bfe\u5fdc\u3059\u308b\u3088\u308a\u826f\u3044\u653f\u7b56\u3092\u6848\u51fa\u3059\u308b\u305f\u3081\u306e\u7814\u7a76\u3092\u884c\u3046\u6570\u5c11\u306a\u3044\u56e3\u4f53\u306e\u3072\u3068\u3064\u3067\u3042\u308a\u3001COVID-19\u3078\u306e\u5bfe\u5fdc\u3067\u3082\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u305f\u56e3\u4f53\u3067\u3057\u305f\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4zb2u9abduk\"><sup><a href=\"#fn4zb2u9abduk\">[7]</a></sup></span></p><p>COVID-19 \u306e\u767a\u751f\u6642\u3001\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306e\u30e1\u30f3\u30d0\u30fc\u305f\u3061\u306f\u3001<a href=\"https://www.1daysooner.org/\"><u>1DaySooner</u></a>\u3068\u3044\u3046\u975e\u55b6\u5229\u56e3\u4f53\u3092\u8a2d\u7acb\u3057\u3001\u30d2\u30c8\u30c1\u30e3\u30ec\u30f3\u30b8\u8a66\u9a13\uff08human challenge trials\uff09\u3092\u652f\u6301\u3057\u307e\u3057\u305f\u3002\u3053\u306e\u7a2e\u306e\u30ef\u30af\u30c1\u30f3\u8a66\u9a13\u3067\u306f\u3001\u5065\u5eb7\u306a\u30dc\u30e9\u30f3\u30c6\u30a3\u30a2\u3092\u610f\u56f3\u7684\u306b\u75c5\u6c17\u306b\u611f\u67d3\u3055\u305b\u308b\u3053\u3068\u3067\u3001\u307b\u3068\u3093\u3069\u5373\u5ea7\u306b\u30ef\u30af\u30c1\u30f3\u8a66\u9a13\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u30021DaySooner\u306f\u3001\u3053\u306e\u4ecb\u5165\u7b56\u3092\u652f\u6301\u3059\u308b\u6570\u5c11\u306a\u3044\u56e3\u4f53\u306e\u4e00\u3064\u3068\u3057\u3066\u30013\u4e07\u4eba\u4ee5\u4e0a\u306e\u30dc\u30e9\u30f3\u30c6\u30a3\u30a2\u3068\u5951\u7d04\u3057<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefj3c26vw0o\"><sup><a href=\"#fnj3c26vw0o\">[8]</a></sup></span>\u3001\u4e16\u754c\u521d\u306eCOVID-19\u306b\u95a2\u3059\u308b\u30d2\u30c8\u30c1\u30e3\u30ec\u30f3\u30b8\u8a66\u9a13\u3092\u958b\u59cb\u3059\u308b\u4e0a\u3067\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3057\u305f\u3002\u3053\u306e\u30e2\u30c7\u30eb\u306f\u3001\u79c1\u305f\u3061\u304c\u6b21\u306e\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306b\u76f4\u9762\u3057\u305f\u3068\u304d\u306b\u3082\u518d\u5229\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p><p>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306e\u30e1\u30f3\u30d0\u30fc\u306f\u3001\u6b21\u306e\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u3092\u9632\u3050\u305f\u3081\u306b\u7acb\u6848\u3055\u308c\u305f\u4f55\u5341\u5104\u30c9\u30eb\u898f\u6a21\u306e\u653f\u7b56\u63d0\u6848&nbsp;<a href=\"https://biodefensecommission.org/reports/the-apollo-program-for-biodefense-winning-the-race-against-biological-threats/\">Apollo Programme for Biodefense</a> \u306e\u4f5c\u6210\u306b\u4e00\u5f79\u8cb7\u3063\u305f\u3053\u3068\u3082\u3042\u308a\u307e\u3059\u3002</p><h2><strong>\u8ca7\u56f0\u56fd\u306b\u57fa\u790e\u7684\u306a\u533b\u7642\u54c1\u3092\u63d0\u4f9b\u3059\u308b</strong></h2><p><strong>\u3053\u306e\u554f\u984c\u306b\u53d6\u308a\u7d44\u3080\u7406\u7531</strong></p><p>\u300c\u6148\u5584\uff08charity\uff09\u306f\u8eab\u5185\u304b\u3089<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgstf8gmntfl\"><sup><a href=\"#fngstf8gmntfl\">[9]</a></sup></span>\u300d\u3068\u8a00\u308f\u308c\u308b\u306e\u304c\u5e38\u3067\u3059\u304c\u3001\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u3067\u306f\u3001\u6148\u5584\u306f\u6211\u3005\u304c\u6700\u5927\u306e\u652f\u63f4\u3092\u884c\u3048\u308b\u3068\u3053\u308d\u304b\u3089\u59cb\u307e\u308a\u307e\u3059\u3002\u305d\u3057\u3066\u3053\u308c\u306f\u305f\u3044\u3066\u3044\u3001\u73fe\u884c\u306e\u30b7\u30b9\u30c6\u30e0\u306e\u306a\u304b\u3067\u3082\u3063\u3068\u3082\u898b\u904e\u3054\u3055\u308c\u3066\u3044\u308b\u4eba\u3073\u3068\u306b\u6ce8\u610f\u3092\u5411\u3051\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u304c\u3001\u305d\u308c\u306f\u3057\u3070\u3057\u3070\u3001\u81ea\u5206\u305f\u3061\u304b\u3089\u305a\u3063\u3068\u96e2\u308c\u305f\u3068\u3053\u308d\u306b\u3044\u308b\u4eba\u3073\u3068\u3067\u3082\u3042\u308a\u307e\u3059\u3002</p><p>7\u5104\u4eba\u4ee5\u4e0a\u306e\u4eba\u3073\u3068\u306e\u751f\u6d3b\u8cbb\u306f\u4e00\u65e51.90\u30c9\u30eb\u4ee5\u4e0b\u3067\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzr0ggtg4o6g\"><sup><a href=\"#fnzr0ggtg4o6g\">[10]</a></sup></span></p><p>\u305d\u308c\u3068\u6bd4\u3079\u3066\u3001\u8ca7\u56f0\u30e9\u30a4\u30f3\u8fd1\u304f\u3067\u751f\u6d3b\u3059\u308b\u7c73\u56fd\u4eba\u306e\u751f\u6d3b\u8cbb\u306f\u305d\u306e20\u500d\u3067\u3001\u7c73\u56fd\u306e\u5e73\u5747\u7684\u306a\u5927\u5352\u8005\u306e\u751f\u6d3b\u8cbb\u306f\u7d04107\u500d\u3067\u3059\u3002\u3053\u306e\u305f\u3081\u3001\u7c73\u56fd\u306e\u5e73\u5747\u7684\u306a\u5927\u5352\u8005\u306e\u53ce\u5165\u306f\u4e16\u754c\u5168\u4f53\u3067\u898b\u308b\u3068\u4e0a\u4f4d1.3%\u306b\u5165\u308a\u307e\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqaii6goa04\"><sup><a href=\"#fnqaii6goa04\">[11]</a></sup></span>\uff08\u3053\u3046\u3057\u305f\u6570\u5b57\u306f\u3001\u540c\u984d\u306e\u304a\u91d1\u3067\u3082\u8ca7\u3057\u3044\u56fd\u3067\u306f\u3088\u308a\u591a\u304f\u306e\u3082\u306e\u3092\u8cb7\u3048\u308b\u3068\u3044\u3046\u4e8b\u5b9f\u3092\u8e0f\u307e\u3048\u3066\u8abf\u6574\u6e08\u307f\u3067\u3059\u3002\uff09</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sqYtEGMqYWTR4YXZC/xlwandgqihymzbshbnzq\"></p><p>\u4e16\u754c\u306e\u4e0d\u5e73\u7b49\u306f\u9014\u8f4d\u3082\u3042\u308a\u307e\u305b\u3093\u3002\u3053\u306e\u305f\u3081\u3001\u4e16\u754c\u306e\u6975\u8ca7\u306e\u4eba\u3073\u3068\u306b\u8cc7\u6e90\u3092\u79fb\u8b72\u3059\u308b\u3053\u3068\u3067\u83ab\u5927\u306a\u91cf\u306e\u3008\u3088\u3044\u3053\u3068\u3009\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u7c73\u56fd\u3084\u82f1\u56fd\u306a\u3069\u306e\u8c4a\u304b\u306a\u56fd\u306e\u653f\u5e9c\u306f\u5178\u578b\u7684\u306b\u306f\u3001\u4e00\u4eba\u306e\u547d\u3092\u6551\u3046\u305f\u3081\u306b100\u4e07\u30c9\u30eb\u3092\u559c\u3093\u3067\u51fa\u8cbb\u3057\u307e\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqy3oixrrmf\"><sup><a href=\"#fnqy3oixrrmf\">[12]</a></sup></span>&nbsp;\u305d\u3046\u3059\u308b\u4fa1\u5024\u306f\u5341\u5206\u306b\u3042\u308a\u307e\u3059\u304c\u3001\u4e16\u754c\u306e\u6700\u3082\u8ca7\u3057\u3044\u56fd\u3005\u3067\u306f\u4e00\u4eba\u306e\u547d\u3092\u6551\u3046\u306e\u306b\u5fc5\u8981\u306a\u8cbb\u7528\u306f\u305d\u308c\u3088\u308a\u3082\u305a\u3063\u3068\u4f4e\u3044\u306e\u3067\u3059\u3002</p><p><a href=\"https://www.givewell.org/\">GiveWell</a> \u306f\u3001\u30a8\u30d3\u30c7\u30f3\u30b9\u306b\u3088\u308a\u6700\u3082\u3088\u304f\u652f\u6301\u3055\u308c\u3001\u8cbb\u7528\u5bfe\u52b9\u679c\u304c\u6700\u3082\u9ad8\u3044\u5065\u5eb7\u30fb\u958b\u767a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u898b\u3064\u3051\u51fa\u3059\u305f\u3081\u306b\u8e0f\u307f\u8fbc\u3093\u3060\u7814\u7a76\u3092\u884c\u3046\u7d44\u7e54\u3067\u3059\u3002GiveWell \u306f\u3001<a href=\"https://www.givewell.org/international/technical/criteria/impact/failure-stories\"><u>\u591a\u304f\u306e\u63f4\u52a9\u306f\u3046\u307e\u304f\u6a5f\u80fd\u3057\u3066\u3044\u306a\u3044</u></a>\u4e00\u65b9\u3001\u6bba\u866b\u5264\u51e6\u7406\u3055\u308c\u305f\u868a\u5e33\u3092\u63d0\u4f9b\u3059\u308b\u3068\u3044\u3063\u305f\u4e00\u90e8\u306e\u63f4\u52a9\u306f\u3001\u5e73\u5747\u7d045,500\u30c9\u30eb\u307b\u3069\u3067\u3072\u3068\u308a\u306e\u5b50\u3069\u3082\u306e\u547d\u3092\u6551\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u3053\u3068\u3092\u767a\u898b\u3057\u307e\u3057\u305f\u3002\u3053\u308c\u306f\u3014\u7c73\u56fd\u3084\u30a4\u30ae\u30ea\u30b9\u3067\u3072\u3068\u308a\u306e\u547d\u3092\u6551\u3046\u306e\u306b\u5fc5\u8981\u306a\u8cbb\u7528\u3068\u6bd4\u3079\u3066\u3015180\u5206\u306e1\u306e\u8cbb\u7528\u3067\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpps7e5ymyr\"><sup><a href=\"#fnpps7e5ymyr\">[13]</a></sup></span></p><p><br>\u3053\u3046\u3057\u305f\u57fa\u672c\u7684\u306a\u533b\u7642\u7684\u4ecb\u5165\u306f\u3068\u3066\u3082\u5b89\u4fa1\u3067\u52b9\u679c\u7684\u3067\u3042\u308b\u305f\u3081\u306b\u3001\u63f4\u52a9\u306b\u5bfe\u3059\u308b<a href=\"https://blog.givewell.org/2015/11/06/the-lack-of-controversy-over-well-targ\"><u>\u3082\u3063\u3068\u3082\u5f37\u529b\u306a\u61d0\u7591\u4e3b\u7fa9\u8005\u305f\u3061\u3067\u3055\u3048\u3001\u305d\u3046\u3057\u305f\u63f4\u52a9\u306e\u4fa1\u5024\u3092\u8a8d\u3081\u3066\u3044\u307e\u3059</u></a>\u3002</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sqYtEGMqYWTR4YXZC/tvfhus9hgtfax6mkigwm\"></p><p><strong>\u3053\u308c\u307e\u3067\u306e\u53d6\u308a\u7d44\u307f</strong></p><p><strong>110,000\u4eba\u4ee5\u4e0a\u306e\u500b\u4eba\u5bc4\u4ed8\u8005</strong>\u304c GiveWell \u306e\u7814\u7a76\u3092\u5229\u7528\u3057\u3066\u3001 GiveWell \u304c\u63a8\u85a6\u3057\u305f\u6148\u5584\u56e3\u4f53\u306b10\u5104\u30c9\u30eb\u4ee5\u4e0a\u306e\u5bc4\u4ed8\u3092\u884c\u3044\u3001&nbsp;<a href=\"https://www.givewell.org/charities/amf\">Against Malaria Foundation</a> \u306e\u3088\u3046\u306a\u7d44\u7e54\u3092\u652f\u63f4\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u56e3\u4f53\u306f\u3053\u308c\u307e\u3067\u306b\u30012\u5104\u5e33\u306b\u306e\u307c\u308b\u6bba\u866b\u5264\u51e6\u7406\u3055\u308c\u305f\u868a\u5e33\u3092\u914d\u5e03\u3057\u3066\u304d\u307e\u3057\u305f\u3002\u3053\u3046\u3057\u305f\u52aa\u529b\u306f\u5408\u7b97\u3059\u308b\u3068\u3001<strong>159,000\u4eba\u306e\u547d</strong>\u3092\u6551\u3063\u3066\u304d\u305f\u3068\u898b\u7a4d\u3082\u3089\u308c\u307e\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5gygdbxwk9y\"><sup><a href=\"#fn5gygdbxwk9y\">[14]</a></sup></span></p><p>\u7121\u511f\u306e\u6148\u5584\u6d3b\u52d5\u3060\u3051\u3067\u306f\u306a\u304f\u3001\u30d3\u30b8\u30cd\u30b9\u3092\u901a\u3057\u3066\u4e16\u754c\u306e\u6700\u3082\u8ca7\u3057\u3044\u4eba\u3073\u3068\u3092\u652f\u63f4\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u3002<a href=\"https://www.wave.com/\">Wave</a> \u306f\u3001\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306e\u30e1\u30f3\u30d0\u30fc\u304c\u5275\u8a2d\u3057\u305f\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u30fb\u30ab\u30f3\u30d1\u30cb\u30fc\u3067\u3001\u30a2\u30d5\u30ea\u30ab\u306e\u56fd\u3005\u306b\u65e2\u5b58\u306e\u30b5\u30fc\u30d3\u30b9\u3088\u308a\u8fc5\u901f\u304b\u3064\u4f55\u500d\u3082\u5b89\u4fa1\u306b\u9001\u91d1\u3059\u308b\u3053\u3068\u3092\u53ef\u80fd\u306b\u3057\u307e\u3059\u3002\u7279\u306b\u3001\u79fb\u6c11\u304c\u6545\u90f7\u306e\u5bb6\u65cf\u306b\u9001\u91d1\u3059\u308b\u306e\u306b\u4fbf\u5229\u306a\u306e\u3067\u3001\u30b1\u30cb\u30e4\u3084\u30a6\u30ac\u30f3\u30c0\u3001\u30bb\u30cd\u30ac\u30eb\u3068\u3044\u3063\u305f\u56fd\u3005\u306e80\u4e07\u4eba\u4ee5\u4e0a\u306e\u4eba\u3073\u3068\u306b\u4f7f\u308f\u308c\u3066\u304d\u307e\u3057\u305f\u3002\u30bb\u30cd\u30ac\u30eb\u3060\u3051\u3067\u3082\u3001Wave \u306e\u5229\u7528\u8005\u306f\u4f55\u5104\u30c9\u30eb\u5206\u3082\u306e\u9001\u91d1\u624b\u6570\u6599\u3092\u7bc0\u7d04\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\u3002\u3053\u306e\u984d\u306f\u30bb\u30cd\u30ac\u30eb\u306eGDP\u306e\u7d041%\u306b\u3042\u305f\u308a\u307e\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8iq8e9godmv\"><sup><a href=\"#fn8iq8e9godmv\">[15]</a></sup></span></p><h2><strong>AI\u30a2\u30e9\u30a4\u30e1\u30f3\u30c8\u3068\u3044\u3046\u7814\u7a76\u5206\u91ce\u306e\u78ba\u7acb\u3092\u652f\u63f4\u3059\u308b</strong></h2><p><i><strong>\u3053\u306e\u554f\u984c\u306b\u53d6\u308a\u7d44\u3080\u7406\u7531</strong></i></p><p>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306b\u643a\u308f\u308b\u4eba\u3073\u3068\u306f\u3001\u76f4\u89b3\u306b\u53cd\u3057\u3001\u66d6\u6627\u3067\u3001\u5927\u8888\u88df\u306b\u8a87\u5f35\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u306b\u898b\u3048\u308b\u554f\u984c\u306b\u7126\u70b9\u3092\u5f53\u3066\u308b\u3053\u3068\u304c\u591a\u3005\u3042\u308a\u307e\u3059\u3002\u3057\u304b\u3057\u305d\u308c\u306f\u306a\u305c\u304b\u3068\u8a00\u3048\u3070\u3001\uff08\u305d\u306e\u4ed6\u306e\u90e8\u5206\u304c\u7b49\u3057\u3051\u308c\u3070\uff09\u4ed6\u306e\u4eba\u3073\u3068\u306b\u3088\u3063\u3066\u7121\u8996\u3055\u308c\u3066\u3044\u308b\u554f\u984c\u306b\u53d6\u308a\u7d44\u3080\u65b9\u304c\u3088\u308a\u30a4\u30f3\u30d1\u30af\u30c8\u304c\u5927\u304d\u304f\u3001\u304b\u3064\u305d\u306e\u7a2e\u306e\u554f\u984c\u306f\uff08\u307b\u3068\u3093\u3069\u305d\u306e\u5b9a\u7fa9\u306b\u3088\u3063\u3066\uff09\u5947\u7570\u306a\u554f\u984c\u306b\u6620\u308b\u3060\u308d\u3046\u304b\u3089\u3067\u3059\u3002\u3072\u3068\u3064\u306e\u4f8b\u306fAI\u30a2\u30e9\u30a4\u30e1\u30f3\u30c8\u554f\u984c\uff08AI alignment problem\uff09\u3067\u3059\u3002</p><p>\u4eba\u5de5\u77e5\u80fd\uff08AI\uff09\u306f\u6025\u901f\u306b\u767a\u5c55\u3057\u3066\u3044\u307e\u3059\u3002\u5148\u7aef\u7684\u306aAI\u30b7\u30b9\u30c6\u30e0\u306f\u3044\u307e\u3084\u3001\u4e00\u5b9a\u306e\u9650\u754c\u306f\u3042\u308c\u3069\u4f1a\u8a71\u306b\u5f93\u4e8b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u3001\u5927\u5b66\u30ec\u30d9\u30eb\u306e\u6570\u5b66\u306e\u554f\u984c\u3092\u89e3\u304d\u3001\u30b8\u30e7\u30fc\u30af\u3092\u8aac\u660e\u3057\u3001\u30c6\u30ad\u30b9\u30c8\u306b\u57fa\u3065\u3044\u3066\u9014\u8f4d\u3082\u306a\u304f\u30ea\u30a2\u30eb\u306a\u7d75\u3092\u63cf\u304d\u3001\u57fa\u672c\u7684\u306a\u30d7\u30ed\u30b0\u30e9\u30e0\u3082\u66f8\u3044\u3066\u3057\u307e\u3044\u307e\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwtblnk8b5r\"><sup><a href=\"#fnwtblnk8b5r\">[16]</a></sup></span>\u3053\u306e\u3046\u3061\u306e\u3069\u308c\u3092\u3068\u3063\u3066\u3082\u3001\u305f\u3063\u305f10\u5e74\u524d\u3067\u3055\u3048\u53ef\u80fd\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002</p><p>\u5148\u7aef\u7684\u306aAI\u7814\u7a76\u6240\u306e\u6700\u7d42\u7684\u306a\u76ee\u6a19\u306f\u3001\u3042\u3089\u3086\u308b\u8ab2\u984c\u306b\u95a2\u3057\u3066\u4eba\u9593\u3068\u540c\u3058\u304f\u3089\u3044\u304b\u305d\u308c\u4ee5\u4e0a\u306b\u512a\u308c\u305fAI\u3092\u958b\u767a\u3059\u308b\u3053\u3068\u3067\u3059\u3002\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306e\u5c06\u6765\u3092\u4e88\u6e2c\u3059\u308b\u306e\u306f\u6975\u3081\u3066\u56f0\u96e3\u3067\u3059\u304c\u3001\u305d\u306e\u76ee\u6a19\u304c\u4eca\u4e16\u7d00\u4e2d\u306b\u9054\u6210\u3055\u308c\u308b\u78ba\u7387\u306e\u65b9\u304c\u3014\u8d77\u3053\u3089\u306a\u3044\u78ba\u7387\u3088\u308a\u3015\u9ad8\u3044\u3068<a href=\"https://www.cold-takes.com/where-ai-forecasting-stands-today/\"><u>\u69d8\u3005\u306a\u8b70\u8ad6\u3084\u5c02\u9580\u5bb6\u3078\u306e\u30a2\u30f3\u30b1\u30fc\u30c8\u8abf\u67fb</u></a>\u304c\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u307e\u305f<a href=\"https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/\"><u>\u6a19\u6e96\u7684\u306a\u7d4c\u6e08\u30e2\u30c7\u30eb</u></a>\u306b\u5f93\u3048\u3070\u3001\u3072\u3068\u305f\u3073\u6c4e\u7528AI\u304c\u4eba\u9593\u306e\u30ec\u30d9\u30eb\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u884c\u3048\u308b\u3088\u3046\u306b\u306a\u308c\u3070\u3001\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306e\u767a\u5c55\u306f\u5287\u7684\u306b\u52a0\u901f\u3059\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002</p><p>\u305d\u3053\u304b\u3089\u306f\u304a\u305d\u3089\u304f1800\u5e74\u4ee3\u306e\u7523\u696d\u9769\u547d\u3068\u4f3c\u305f\u3001\u3042\u308b\u3044\u306f\u305d\u308c\u3088\u308a\u3082\u5927\u304d\u306a\u610f\u7fa9\u3092\u3082\u3063\u305f\u5de8\u5927\u306a\u5909\u9769\u304c\u3082\u305f\u3089\u3055\u308c\u308b\u3067\u3057\u3087\u3046\u3002\u3046\u307e\u304f\u3044\u3051\u3070\u3001\u3053\u306e\u5909\u9769\u306f\u5168\u3066\u306e\u4eba\u3073\u3068\u306b\u8c4a\u304b\u3055\u3068\u7e41\u6804\u3092\u3082\u305f\u3089\u3059\u3053\u3068\u306b\u306a\u308b\u3067\u3057\u3087\u3046\u3057\u3001\u4e0b\u624b\u3092\u3059\u308c\u3070\u3001\u3054\u304f\u308f\u305a\u304b\u306a\u30a8\u30ea\u30fc\u30c8\u305f\u3061\u306e\u624b\u306b\u6a29\u529b\u3092\u6975\u7aef\u306b\u96c6\u4e2d\u3055\u305b\u308b\u3053\u3068\u306b\u306a\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002</p><p>\u6700\u60aa\u306e\u5834\u5408\u3001\u79c1\u305f\u3061\u306fAI\u30b7\u30b9\u30c6\u30e0\u305d\u306e\u3082\u306e\u306b\u5bfe\u3059\u308b<a href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\"><u>\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u3092\u5931\u3046</u></a>\u3053\u3068\u306b\u3082\u306a\u308a\u304b\u306d\u307e\u305b\u3093\u3002\u81ea\u5206\u305f\u3061\u3088\u308a\u3082\u306f\u308b\u304b\u306b\u512a\u308c\u305f\u80fd\u529b\u3092\u5099\u3048\u305f\u5b58\u5728\u3092\u5236\u5fa1\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u304f\u306a\u308b\u3053\u3068\u3067\u3001\u30c1\u30f3\u30d1\u30f3\u30b8\u30fc\u304c\u305d\u306e\u672a\u6765\u3092\u5236\u5fa1\u3059\u308b\u3053\u3068\u304c\u307b\u3068\u3093\u3069\u3067\u304d\u306a\u3044\u306e\u3068\u540c\u3058\u3088\u3046\u306b\u3001\u79c1\u305f\u3061\u3082\u81ea\u5206\u305f\u3061\u306e\u672a\u6765\u3092\u307b\u3068\u3093\u3069\u5236\u5fa1\u3067\u304d\u306a\u304f\u306a\u308b\u3068\u3044\u3046\u3053\u3068\u306b\u3082\u306a\u308a\u304b\u306d\u307e\u305b\u3093\u3002</p><p>\u3064\u307e\u308a\u3053\u306e\u554f\u984c\u306f\u73fe\u5728\u4e16\u4ee3\u3060\u3051\u3067\u306f\u306a\u304f\u3001\u5c06\u6765\u306e\u5168\u4e16\u4ee3\u306b\u6e21\u3063\u3066\u5287\u7684\u306a\u30a4\u30f3\u30d1\u30af\u30c8\u3092\u3082\u3064\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u554f\u984c\u304c\u300c<a href=\"https://en.wikipedia.org/wiki/Longtermism\"><u>\u9577\u671f\u4e3b\u7fa9\u8005\uff08longtermist\uff09</u></a>\u300d\u306e\u89b3\u70b9\u304b\u3089\u306f\u7279\u5225\u5207\u8feb\u3057\u305f\u554f\u984c\u3067\u3042\u308b\u306e\u306f\u3053\u306e\u305f\u3081\u3067\u3059\u3002\u9577\u671f\u4e3b\u7fa9 (longtermism) \u3068\u306f\u3001\u9577\u671f\u7684\u306a\u8996\u70b9\u304b\u3089\u672a\u6765\u306e\u72b6\u614b\u3092\u6539\u5584\u3059\u308b\u3053\u3068\u306f\u3001\u3053\u306e\u6642\u4ee3\u306b\u304a\u3051\u308b\u91cd\u8981\u306a\u9053\u5fb3\u7684\u306a\u512a\u5148\u4e8b\u9805\u306e\u3072\u3068\u3064\u3067\u3042\u308b\u3068\u4e3b\u5f35\u3059\u308b\u3001\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306b\u304a\u3051\u308b\u601d\u8003\u6f6e\u6d41\u306e\u3072\u3068\u3064\u3067\u3059\u3002</p><p>AI\u30b7\u30b9\u30c6\u30e0\u304c\u3001\u305d\u306e\u80fd\u529b\u306e\u70b9\u3067\u4eba\u9593\u3068\u540c\u7b49\u306e\uff08\u306a\u3044\u3057\u3001\u305d\u308c\u3088\u308a\u512a\u308c\u305f\uff09\u3082\u306e\u3068\u306a\u3063\u3066\u3082\u3001\u4eba\u9593\u306b\u3068\u3063\u3066\u306e\u4fa1\u5024\u3092\u4fc3\u9032\u3057\u7d9a\u3051\u308b\u3053\u3068\u3092\u3069\u3046\u4fdd\u8a3c\u3059\u308b\u306e\u304b\u3068\u3044\u3046\u554f\u984c\u306f\u3001AI\u30a2\u30e9\u30a4\u30e1\u30f3\u30c8\u554f\u984c\u3068\u547c\u3070\u308c\u3001\u3053\u306e\u554f\u984c\u306e\u89e3\u6c7a\u306b\u306f\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u767a\u5c55\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002</p><p>\u305d\u306e\u6f5c\u5728\u7684\u306a\u6b74\u53f2\u7684\u91cd\u8981\u6027\u306b\u3082\u304b\u304b\u308f\u3089\u305a\u3001\u3053\u306eAI\u30a2\u30e9\u30a4\u30e1\u30f3\u30c8\u554f\u984c\u306b\u53d6\u308a\u7d44\u3093\u3067\u3044\u308b\u7814\u7a76\u8005\u306f\u6570\u767e\u4eba\u306b\u7559\u307e\u308a\u307e\u3059\u304c\u3001\u305d\u308c\u306b\u5bfe\u3057\u3066\u3001AI\u30b7\u30b9\u30c6\u30e0\u3092\u3088\u308a\u5f37\u529b\u306a\u3082\u306e\u306b\u3057\u3088\u3046\u3068\u52aa\u529b\u3059\u308b\u7814\u7a76\u8005\u306f\u4f55\u4e07\u4eba\u3082\u3044\u307e\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxly1kl8lu5a\"><sup><a href=\"#fnxly1kl8lu5a\">[17]</a></sup></span></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sqYtEGMqYWTR4YXZC/gy8spo8q7ihcoidn0evi\"></p><p>\u6570\u6bb5\u843d\u3067\u3053\u306e\u554f\u984c\u304c\u91cd\u8981\u3067\u3042\u308b\u7406\u7531\u3092\u8981\u7d04\u3059\u308b\u306e\u306f\u96e3\u3057\u3044\u306e\u3067\u3001\u3082\u3063\u3068\u8a73\u3057\u304f\u77e5\u308a\u305f\u3044\u3068\u601d\u308f\u308c\u305f\u65b9\u306b\u306f\u3001<a href=\"https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment\"><u>\u3053\u306e\u8a18\u4e8b</u></a>\u3084<a href=\"https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/\"><u>\u3053\u306e\u8a18\u4e8b</u></a>\u3001\u305d\u3057\u3066<a href=\"https://www.cold-takes.com/most-important-century/\"><u>\u3053\u306e\u8a18\u4e8b</u></a>\u304b\u3089\u8aad\u307f\u59cb\u3081\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002</p><p><strong>\u3053\u308c\u307e\u3067\u306e\u53d6\u308a\u7d44\u307f</strong></p><p>\u512a\u5148\u8ab2\u984c\u306e\u3072\u3068\u3064\u306f\u5358\u7d14\u306b\u3001\u3053\u306e\u554f\u984c\u306e\u8a8d\u77e5\u5ea6\u3092\u9ad8\u3081\u308b\u3053\u3068\u3067\u3059\u30022014\u5e74\u3001AI\u30a2\u30e9\u30a4\u30e1\u30f3\u30c8\u306e\u91cd\u8981\u6027\u3092\u8ad6\u3058\u305f\u8457\u4f5c\u300e\u30b9\u30fc\u30d1\u30fc\u30a4\u30f3\u30c6\u30ea\u30b8\u30a7\u30f3\u30b9 : \u8d85\u7d76AI\u3068\u4eba\u985e\u306e\u547d\u904b \u300f\uff08 \u30cb\u30c3\u30af\u30fb\u30dc\u30b9\u30c8\u30ed\u30e0\u8457\u3001 \u5009\u9aa8\u5f70\u8a33\u3001\u65e5\u672c\u7d4c\u6e08\u65b0\u805e\u51fa\u7248\u793e\u30012017\u5e74\u3001\u539f\u984c\uff1a<a href=\"https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies\"><u>Superintelligence: Paths, Dangers, Strategies</u></a>\uff09\u304c\u51fa\u7248\u3055\u308c\u3001<a href=\"https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies#Reception\"><u>New York Times \u306e\u30d9\u30b9\u30c8\u30bb\u30e9\u30fc\u306b\u306a\u308a\u307e\u3057\u305f</u></a>\u3002</p><p>\u307e\u305f\u5225\u306e\u512a\u5148\u8ab2\u984c\u306f\u3001AI\u30a2\u30e9\u30a4\u30e1\u30f3\u30c8\u554f\u984c\u306b\u7126\u70b9\u3092\u5f53\u3066\u305f\u7814\u7a76\u5206\u91ce\u3092\u78ba\u7acb\u3059\u308b\u3053\u3068\u3067\u3059\u3002\u4f8b\u3048\u3070AI\u306e\u30d1\u30a4\u30aa\u30cb\u30a2\u3067\u3042\u308b\u30b9\u30c1\u30e5\u30a2\u30fc\u30c8\u30fb\u30e9\u30c3\u30bb\u30eb\uff08Stuart Russell\uff09\u3084\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306b\u5f71\u97ff\u3092\u53d7\u3051\u305f\u305d\u306e\u4ed6\u306e\u4eba\u3073\u3068\u306f&nbsp;<a href=\"https://humancompatible.ai/\"><u>\u30ab\u30ea\u30d5\u30a9\u30eb\u30cb\u30a2\u5927\u5b66\u30d0\u30fc\u30af\u30ec\u30fc\u6821\u3067\u4eba\u9593\u5171\u4f9d\u5b58\u578bAI\u30bb\u30f3\u30bf\u30fc\uff08 The Center for Human-Compatible AI \uff09</u></a>\u3092\u5275\u8a2d\u3057\u307e\u3057\u305f\u3002\u3053\u306e\u7814\u7a76\u6a5f\u95a2\u306fAI\u306e\u958b\u767a\u306b\u3001\u4eba\u9593\u306b\u3068\u3063\u3066\u306e\u4fa1\u5024\u3092\u4fc3\u9032\u3059\u308b\u50cd\u304d\u3092\u4e2d\u5fc3\u306b\u304a\u3044\u305f\u65b0\u3057\u3044\u30d1\u30e9\u30c0\u30a4\u30e0\u3092\u3082\u305f\u3089\u305d\u3046\u3068\u3057\u3066\u3044\u307e\u3059\u3002</p><p>\u305d\u306e\u4ed6\u306e\u4eba\u3073\u3068\u306f\u3001<a href=\"https://www.deepmind.com/safety-and-ethics\"><u>DeepMind</u></a> \u3084&nbsp;<a href=\"https://openai.com/alignment/\"><u>OpenAI</u></a> \u306a\u3069\u306eAI\u7814\u7a76\u306e\u91cd\u8981\u306a\u62e0\u70b9\u3067AI\u30a2\u30e9\u30a4\u30e1\u30f3\u30c8\u306b\u7126\u70b9\u3092\u5f53\u3066\u305f\u30c1\u30fc\u30e0\u306e\u767a\u8db3\u3092\u652f\u63f4\u3057\u3001<a href=\"https://arxiv.org/abs/1606.06565\"><u>Concrete Problems in AI Safety</u></a> \u3092\u306f\u3058\u3081\u3068\u3059\u308b\u6210\u679c\u7269\u306e\u4e2d\u3067\u3001AI\u30a2\u30e9\u30a4\u30e1\u30f3\u30c8\u306e\u305f\u3081\u306e\u7814\u7a76\u30a2\u30b8\u30a7\u30f3\u30c0\u3092\u7acb\u6848\u3057\u3066\u3044\u307e\u3059\u3002</p><h2><strong>\u5de5\u5834\u5f0f\u755c\u7523\u3092\u7d42\u308f\u3089\u305b\u308b</strong></h2><p><strong>\u3053\u306e\u554f\u984c\u306b\u53d6\u308a\u7d44\u3080\u7406\u7531</strong></p><p>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306b\u643a\u308f\u308b\u4eba\u3073\u3068\u306f\u95a2\u5fc3\u306e\u7bc4\u56f2\u3092\u2500\u2500\u96e2\u308c\u305f\u56fd\u306b\u4f4f\u3080\u4eba\u3073\u3068\u3084\u5c06\u6765\u4e16\u4ee3\u306b\u3060\u3051\u3067\u306f\u306a\u304f\u2500\u2500\u4eba\u9593\u4ee5\u5916\u306e\u52d5\u7269\u306b\u307e\u3067\u5e83\u3052\u3088\u3046\u3068\u3057\u3066\u3044\u307e\u3059\u3002</p><p>\u7c73\u56fd\u3067\u306f\u6bce\u5e74100\u5104\u8fd1\u3044\u52d5\u7269\u305f\u3061\u304c\u5de5\u5834\u5f0f\u755c\u7523\u8fb2\u5834\u306e\u4e2d\u3067\u2500\u2500\u3057\u3070\u3057\u3070\u305d\u306e\u4e00\u751f\u306e\u307b\u3068\u3093\u3069\u3092\u5411\u304d\u3092\u5909\u3048\u308b\u3053\u3068\u3082\u3067\u304d\u306c\u307e\u307e\u3001\u3082\u3057\u304f\u306f\u9ebb\u9154\u306a\u3057\u306b\u53bb\u52e2\u3055\u308c\u305f\u72b6\u614b\u3067\u2500\u2500\u751f\u304d\u3001\u6b7b\u3093\u3067\u3044\u307e\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz15l4lns2\"><sup><a href=\"#fnz15l4lns2\">[18]</a></sup></span></p><p>\u591a\u304f\u306e\u4eba\u3073\u3068\u306f\u3001\u52d5\u7269\u3092\u4e0d\u5fc5\u8981\u306b\u82e6\u3057\u307e\u305b\u308b\u3079\u304d\u3067\u306f\u306a\u3044\u3053\u3068\u306b\u540c\u610f\u3057\u307e\u3059\u304c\u3001\u305d\u306e\u6ce8\u610f\u306e\u5927\u534a\u304c\u30da\u30c3\u30c8\u30fb\u30b7\u30a7\u30eb\u30bf\u30fc\u306b\u5411\u3051\u3089\u308c\u3066\u3044\u307e\u3059\u3002\u7c73\u56fd\u3067\u306f\u30da\u30c3\u30c8\u30fb\u30b7\u30a7\u30eb\u30bf\u30fc\u306e1,400\u500d\u306e\u6570\u306e\u52d5\u7269\u304c\u5de5\u5834\u5f0f\u755c\u7523\u8fb2\u5834\u306e\u4e2d\u3067\u751f\u304d\u3066\u3044\u307e\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrql4jxg5vba\"><sup><a href=\"#fnrql4jxg5vba\">[19]</a></sup></span></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sqYtEGMqYWTR4YXZC/stj8sivvsgkftv6ugfxm\"></p><p>\u305d\u308c\u306b\u3082\u304b\u304b\u308f\u3089\u305a\u7c73\u56fd\u3067\u306f\u3001\u5de5\u5834\u5f0f\u755c\u7523\u3092\u7d42\u308f\u3089\u305b\u3088\u3046\u3068\u3059\u308b\u30a2\u30c9\u30dc\u30ab\u30b7\u30fc\u56e3\u4f53\u306f\u5e74\u95939,700\u4e07\u30c9\u30eb\u3057\u304b\u53d7\u3051\u53d6\u3063\u3066\u3044\u307e\u305b\u3093\u3002\u4ed6\u65b9\u3001\u30da\u30c3\u30c8\u30fb\u30b7\u30a7\u30eb\u30bf\u30fc\u306f\u5e74\u9593\u7d0450\u5104\u30c9\u30eb\u3092\u53d7\u3051\u53d6\u3063\u3066\u3044\u307e\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdbp14yt1ovu\"><sup><a href=\"#fndbp14yt1ovu\">[20]</a></sup></span></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sqYtEGMqYWTR4YXZC/yvkfyjf99eztlwsw1rux\"></p><p><strong>\u3053\u308c\u307e\u3067\u306e\u53d6\u308a\u7d44\u307f</strong></p><p>\u3072\u3068\u3064\u306e\u6226\u7565\u306f\u30a2\u30c9\u30dc\u30ab\u30b7\u30fc\u3067\u3059\u3002<a href=\"https://openwingalliance.org/impact\"><u>Open Wing Alliance</u></a> \u306f\u3001\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306b\u89e6\u767a\u3055\u308c\u305f\u8cc7\u91d1\u63d0\u4f9b\u8005\u304b\u3089\u5927\u3005\u7684\u306a\u8cc7\u91d1\u63d0\u4f9b\u3092\u53d7\u3051\u3001\u30b1\u30fc\u30b8\u3067\u98fc\u308f\u308c\u305f\u9d8f\u306e\u5375\u3092\u8cb7\u308f\u306a\u3044\u3053\u3068\u306b\u5927\u4f01\u696d\u304c\u30b3\u30df\u30c3\u30c8\u3059\u308b\u3088\u3046\u4fc3\u3059\u30ad\u30e3\u30f3\u30da\u30fc\u30f3\u3092\u5c55\u958b\u3057\u307e\u3057\u305f\u3002\u3053\u3093\u306b\u3061\u307e\u3067\u306b\u3001Open Wing Alliance \u306f2,200\u3092\u8d85\u3048\u308b\u30b3\u30df\u30c3\u30c8\u30e1\u30f3\u30c8\u3092\u52dd\u3061\u53d6\u308a\u307e\u3057\u305f\u3002\u305d\u306e\u7d50\u679c\u30011\u5104\u7fbd\u306e\u9d8f\u304c\u30b1\u30fc\u30b8\u304b\u3089\u9003\u308c\u308b\u3053\u3068\u304c\u3067\u304d\u305f\u306e\u3067\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa6nbqyw2h0b\"><sup><a href=\"#fna6nbqyw2h0b\">[21]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvy24kdv5mph\"><sup><a href=\"#fnvy24kdv5mph\">[22]</a></sup></span></p><p>\u3082\u3046\u3072\u3068\u3064\u306e\u6226\u7565\u306f\u3001\u4ee3\u66ff\u30d7\u30ed\u30c6\u30a4\u30f3\u3092\u958b\u767a\u3059\u308b\u3053\u3068\u3067\u3059\u3002\u4ee3\u66ff\u30d7\u30ed\u30c6\u30a4\u30f3\u304c\u5de5\u5834\u5f0f\u8fb2\u5834\u3067\u80b2\u3066\u3089\u308c\u305f\u8089\u3088\u308a\u3082\u5b89\u4fa1\u3067\u7f8e\u5473\u3057\u304f\u306a\u308b\u306a\u3089\u3001\u8089\u306e\u9700\u8981\u3092\u306a\u304f\u3057\u3001\u5de5\u5834\u5f0f\u755c\u7523\u3092\u7d42\u308f\u3089\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002<a href=\"https://gfi.org/\"><u>Good Food Institute</u></a> \u306f\u3001\u4e2d\u56fd\u306e Dao Foods \u3084\u7c73\u56fd\u306e Good Catch \u306e\u3088\u3046\u306a\u4f01\u696d\u306e\u8a2d\u7acb\u3092\u652f\u63f4\u3057\u3001\uff08\u4e16\u754c\u6700\u5927\u306e\u98df\u8089\u4f01\u696dJBS\u306a\u3069\u306e\uff09\u5de8\u5927\u4f01\u696d\u304c\u3053\u306e\u7523\u696d\u306b\u53c2\u5165\u3059\u308b\u3088\u3046\u4fc3\u3057\u3001\u4f55\u5343\u4e07\u30c9\u30eb\u3082\u306e\u653f\u5e9c\u306e\u8ca1\u653f\u652f\u63f4\u3092\u78ba\u4fdd\u3059\u308b\u3053\u3068\u3067\u3001\u3053\u306e\u7523\u696d\u306e\u767a\u5c55\u306b\u5f3e\u307f\u3092\u3064\u3051\u308b\u305f\u3081\u306e\u52aa\u529b\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6nvfidr2h5\"><sup><a href=\"#fn6nvfidr2h5\">[23]</a></sup></span></p><p>Open Philanthropy \u306f&nbsp;<a href=\"https://en.wikipedia.org/wiki/Impossible_Foods\"><u>Impossible Foods</u></a> \u3078\u306e<a href=\"https://www.openphilanthropy.org/grants/impossible-foods-rd-investment/#:~:text=working%20to%20overcome.-,Background,Impossible%20Burger%2C%20in%20July%202016.\"><u>\u521d\u671f\u6295\u8cc7\u5bb6</u></a>\u3067\u3057\u305f\u3002Impossible Foods \u306f\u3001\u8089\u3068\u304b\u306a\u308a\u8fd1\u3044\u5473\u3092\u3057\u305f\u5b8c\u5168\u30f4\u30a3\u30fc\u30ac\u30f3\u30fb\u30d0\u30fc\u30ac\u30fc\u3067\u3042\u308a\u3001\u30d0\u30fc\u30ac\u30fc\u30fb\u30ad\u30f3\u30b0\u3067\u3082\u8ca9\u58f2\u3055\u308c\u3066\u3044\u308b\u3001\u30a4\u30f3\u30dd\u30c3\u30b7\u30d6\u30eb\u30fb\u30d0\u30fc\u30ac\u30fc\uff08Impossible Burger\uff09\u3092\u958b\u767a\u3057\u305f\u4f01\u696d\u3067\u3059\u3002</p><h2><strong>\u610f\u601d\u6c7a\u5b9a\u3092\u6539\u5584\u3059\u308b</strong></h2><p><strong>\u3053\u306e\u554f\u984c\u306b\u53d6\u308a\u7d44\u3080\u7406\u7531</strong></p><p>\u3008\u3088\u3044\u3053\u3068\u3009\u3092\u3057\u305f\u3044\u3068\u671b\u3080\u4eba\u3073\u3068\u306f\u3057\u3070\u3057\u3070\u3001\u69d8\u3005\u306a\u554f\u984c\u306b<u>\u76f4\u63a5</u>\u53d6\u308a\u7d44\u3080\u3053\u3068\u3092\u597d\u307f\u307e\u3059\u3002\u3068\u3044\u3046\u306e\u3082\u3001\u81ea\u5206\u306e\u884c\u70ba\u306e\u76ee\u306b\u898b\u3048\u308b\u6210\u679c\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u306f\u3001\u3088\u308a\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u3092\u9ad8\u3081\u308b\u304b\u3089\u3067\u3059\u3002\u3057\u304b\u3057\u91cd\u8981\u306a\u306e\u306f\u4e16\u754c\u304c\u3088\u308a\u3088\u3044\u3082\u306e\u306b\u306a\u308b\u3053\u3068\u3067\u3042\u3063\u3066\u3001\u305d\u308c\u304c\u81ea\u5206\u306e\u624b\u306b\u3088\u308b\u3082\u306e\u3067\u3042\u308b\u3053\u3068\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u3057\u305f\u304c\u3063\u3066\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u3092\u5fdc\u7528\u3059\u308b\u4eba\u3073\u3068\u306f\u4ed6\u8005\u3092\u30a8\u30f3\u30d1\u30ef\u30fc\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u9593\u63a5\u7684\u306b\u5f79\u7acb\u3068\u3046\u3068\u3059\u308b\u3053\u3068\u3082\u591a\u3005\u3042\u308a\u307e\u3059\u3002</p><p>\u305d\u306e\u3072\u3068\u3064\u306e\u4f8b\u306f\u610f\u601d\u6c7a\u5b9a\u904e\u7a0b\u3092\u6539\u5584\u3059\u308b\u3053\u3068\u306b\u3042\u308a\u307e\u3059\u3002\u3059\u306a\u308f\u3061\u3001\u9375\u3068\u306a\u308b\u30a2\u30af\u30bf\u30fc\u2500\u2500\u653f\u6cbb\u5bb6\u3084\u6c11\u9593\u30bb\u30af\u30bf\u30fc\uff0f\u7b2c\u4e09\u30bb\u30af\u30bf\u30fc\u306e\u6307\u5c0e\u8005\u3001\u3042\u308b\u3044\u306f\u52a9\u6210\u56e3\u4f53\u306e\u52a9\u6210\u6c7a\u5b9a\u8005\u306a\u3069\u2500\u2500\u304c\u6982\u3057\u3066\u610f\u601d\u6c7a\u5b9a\u306b\u512a\u308c\u3066\u3044\u308c\u3070\u3044\u308b\u307b\u3069\u3001\u5c06\u6765\u306e\u5730\u7403\u898f\u6a21\u306e\u554f\u984c\u304c\u4f55\u3067\u3042\u308c\u3001\u305d\u306e\u3044\u305a\u308c\u306b\u5bfe\u51e6\u3059\u308b\u3046\u3048\u3067\u3082\u3001\u3088\u308a\u3088\u3044\u614b\u52e2\u3092\u793e\u4f1a\u306f\u6574\u3048\u308b\u3053\u3068\u306b\u306a\u308b\u3067\u3057\u3087\u3046\u3002</p><p>\u3057\u305f\u304c\u3063\u3066\u3001\u91cd\u8981\u306a\u30a2\u30af\u30bf\u30fc\u306e\u610f\u601d\u6c7a\u5b9a\u3092\u5411\u4e0a\u3059\u308b\u305f\u3081\u306e\u65b9\u6cd5\u3067\u3001\u3053\u308c\u307e\u3067\u898b\u904e\u3054\u3055\u308c\u3066\u304d\u305f\u65b0\u305f\u306a\u65b9\u6cd5\u3092\u898b\u3064\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306a\u3089\u3001\u5927\u304d\u306a\u30a4\u30f3\u30d1\u30af\u30c8\u3092\u3082\u3064\u305f\u3081\u306e\u9053\u304c\u5207\u308a\u958b\u3051\u308b\u3067\u3057\u3087\u3046\u3002\u305d\u3057\u3066\u3053\u308c\u3092\u9054\u6210\u3059\u308b\u3053\u3068\u306e\u3067\u304d\u308b\u3088\u3046\u306a\u3001\u898b\u8fbc\u307f\u3042\u308b\u63d0\u6848\u304c\u3044\u304f\u3064\u304b\u3042\u308b\u3088\u3046\u306b\u601d\u308f\u308c\u307e\u3059\u3002</p><p><strong>\u3053\u308c\u307e\u3067\u306e\u53d6\u308a\u7d44\u307f</strong></p><p>\u591a\u304f\u306e\u5730\u7403\u898f\u6a21\u306e\u554f\u984c\u306f\u3001\u4fe1\u983c\u3067\u304d\u308b\u60c5\u5831\u304c\u306a\u3051\u308c\u3070\u60aa\u5316\u3057\u307e\u3059\u3002<a href=\"https://www.metaculus.com/questions/\">Metaculus</a> \u306f\u3001\u91cd\u8981\u306a\u554f\u984c\uff08\u4f8b\u3048\u3070\u30ed\u30b7\u30a2\u304c\u30a6\u30af\u30e9\u30a4\u30ca\u306b\u4fb5\u653b\u3059\u308b\u898b\u8fbc\u307f\u306a\u3069\uff09\u3092\u7279\u5b9a\u3057\u3001\u4f55\u767e\u4eba\u3082\u306e\u4e88\u6e2c\u58eb\uff08forecaster\uff09\u306e\u4e88\u6e2c\u3092\u3068\u308a\u307e\u3068\u3081\u3001\u4e88\u6e2c\u58eb\u305f\u3061\u306e\u904e\u53bb\u306e\u7cbe\u5ea6\u3068\u7167\u3089\u3057\u3066\u305d\u3046\u3057\u305f\u4e88\u60f3\u3092\u91cd\u307f\u3065\u3051\u3059\u308b\u672a\u6765\u4e88\u6e2c\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u30fb\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u3067\u3059\u3002 Metaculus \u306f2022\u5e74\u306e1\u6708\u4e2d\u65ec\u6642\u70b9\u3067\u3001\u30ed\u30b7\u30a2\u304c\u30a6\u30af\u30e9\u30a4\u30ca\u306b\u4fb5\u653b\u3059\u308b\u78ba\u7387\u309247%\u3060\u3068\u898b\u7a4d\u3082\u308a\u30012\u670824\u65e5\u306e\u4fb5\u653b\u76f4\u524d \u2500\u2500\u591a\u304f\u306e\u8a55\u8ad6\u5bb6\u3001\u8a18\u8005\u3001\u5c02\u9580\u5bb6\u304c\u4fb5\u653b\u306f\u7d76\u5bfe\u306b\u306a\u3044\u3068\u8a00\u3063\u3066\u3044\u305f\u3068\u304d\u2500\u2500\u306b\u306f80%\u306e\u78ba\u7387\u3092\u4e0e\u3048\u3066\u3044\u307e\u3057\u305f\u3002<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefp6hw0pvxlcb\"><sup><a href=\"#fnp6hw0pvxlcb\">[24]</a></sup></span></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sqYtEGMqYWTR4YXZC/faxn4yiyhwazr7cxlept\"></p><p>\u30aa\u30c3\u30af\u30b9\u30d5\u30a9\u30fc\u30c9\u5927\u5b66\u306e&nbsp;<a href=\"https://globalprioritiesinstitute.org/\">Global Priorities Institute</a> \u306f\u54f2\u5b66\u3068\u7d4c\u6e08\u5b66\u306e\u4ea4\u5dee\u9818\u57df\u3067\u3001\u91cd\u8981\u306a\u610f\u601d\u6c7a\u5b9a\u8005\u306f\u3069\u3046\u3057\u305f\u3089\u4e16\u754c\u3067\u6700\u3082\u5207\u8feb\u3057\u305f\u554f\u984c\u3092\u7279\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u304b\u306b\u95a2\u3059\u308b\u57fa\u790e\u7814\u7a76\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002Global Priorities Institute \u306f<a href=\"https://globalprioritiesinstitute.org/research-agenda/\"><u>\u7814\u7a76\u30a2\u30b8\u30a7\u30f3\u30c0</u></a>\u3092\u4f5c\u6210\u3057\u3001<a href=\"https://globalprioritiesinstitute.org/papers/\"><u>\u4f55\u5341\u672c\u3082\u306e\u8ad6\u6587</u></a>\u3092\u51fa\u7248\u3057\u3001\u30cf\u30fc\u30d0\u30fc\u30c9\u5927\u5b66\u3084\u30cb\u30e5\u30fc\u30e8\u30fc\u30af\u5927\u5b66\u3001\u30c6\u30ad\u30b5\u30b9\u5927\u5b66\u30aa\u30fc\u30b9\u30c6\u30a3\u30f3\u6821\u3001\u30a4\u30a7\u30fc\u30eb\u5927\u5b66\u3001\u30d7\u30ea\u30f3\u30b9\u30c8\u30f3\u5927\u5b66\u3092\u306f\u3058\u3081\u3068\u3059\u308b\u69d8\u3005\u306a\u5927\u5b66\u3067\u95a2\u9023\u3059\u308b\u7814\u7a76\u304c\u958b\u59cb\u3055\u308c\u308b\u304d\u3063\u304b\u3051\u3068\u3082\u306a\u308b\u3053\u3068\u3067\u3001\u4e16\u754c\u7684\u512a\u5148\u8ab2\u984c\u7814\u7a76\uff08global priorities research\uff09\u3068\u3044\u3046\u65b0\u3057\u3044\u5b66\u8853\u5206\u91ce\u306e\u78ba\u7acb\u306b\u5bc4\u4e0e\u3057\u3066\u3044\u307e\u3059\u3002</p><h1>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u3092\u675f\u306d\u308b\u4fa1\u5024\u306f\u4f55\u304b</h1><p>\u4ee5\u4e0a\u306e\u69d8\u3005\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306f\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u3092\u5b9a\u7fa9\u3059\u308b\u3082\u306e\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3057\u3001\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u304c\u7126\u70b9\u3092\u5f53\u3066\u3066\u53d6\u308a\u7d44\u3080\u5bfe\u8c61\u306f\u5bb9\u6613\u306b\u5909\u308f\u308a\u307e\u3059\u3002\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u3092\u5b9a\u7fa9\u3059\u308b\u306e\u306f\u3001\u4ed6\u8005\u3092\u52a9\u3051\u308b\u305f\u3081\u306e\u6700\u826f\u306e\u65b9\u6cd5\u3092\u898b\u3064\u3051\u51fa\u305d\u3046\u3068\u3059\u308b\u55b6\u307f\u306e\u6839\u5e95\u306b\u3042\u308b\u3001\u4ee5\u4e0b\u306e\u8af8\u4fa1\u5024\u3067\u3059\u3002</p><ol><li><strong>\u3082\u306e\u3054\u3068\u306b\u306f\u512a\u5148\u9806\u4f4d\u3092\u3064\u3051\u3089\u308c\u308b\uff08Prioritization\uff09</strong>\uff1a\u3008\u3088\u3044\u3053\u3068\u3009\u3092\u3059\u308b\u3053\u3068\u306b\u3064\u3044\u3066\u306e\u79c1\u305f\u3061\u306e<a href=\"https://en.wikipedia.org/wiki/Scope_neglect\"><u>\u76f4\u89b3\uff08intuition\uff09</u></a>\u306f\u3001\u901a\u5e38\u306f\u7d50\u679c\u306e\u898f\u6a21\u3092\u8003\u616e\u306b\u5165\u308c\u307e\u305b\u3093\u3002\u4f8b\u3048\u3070\u3001100\u4eba\u3092\u52a9\u3051\u308b\u3053\u3068\u304c\u30011000\u4eba\u3092\u52a9\u3051\u308b\u306e\u3068\u540c\u3058\u304f\u3089\u3044\u306e\u6e80\u8db3\u611f\u3092\u3082\u305f\u3089\u3059\u3053\u3068\u304c\u591a\u3005\u3042\u308a\u307e\u3059\u3002\u3057\u304b\u3057\u3008\u3088\u3044\u3053\u3068\u3009\u3092\u3059\u308b\u4e00\u90e8\u306e\u65b9\u6cd5\u306f\u4ed6\u306e\u65b9\u6cd5\u3068\u6bd4\u3079\u3066\u3001\u5287\u7684\u306b\u3088\u308a\u591a\u304f\u306e\u3053\u3068\u3092\u9054\u6210\u3059\u308b\u4ee5\u4e0a\u3001\u69d8\u3005\u306a\u884c\u70ba\u304c\u3069\u308c\u307b\u3069\u306e\u52a9\u3051\u306b\u306a\u308b\u306e\u304b\u3092\u5927\u307e\u304b\u306b\u8a08\u308b\u305f\u3081\u306e\u3001\u6570\u5024\u5316\u306e\u8a66\u307f\u306f\u5fc5\u9808\u3067\u3059\u3002\u76ee\u6a19\u306f\u5358\u306b\u4f55\u3089\u304b\u306e\u9055\u3044\u3092\u3082\u305f\u3089\u3059\u305f\u3081\u306b\u50cd\u304f\u3053\u3068\u3067\u306f\u306a\u304f\u3001\u4ed6\u8005\u3092\u52a9\u3051\u308b\u305f\u3081\u306e<u>\u6700\u826f\u306e</u>\u65b9\u6cd5\u3092\u898b\u3064\u3051\u51fa\u3059\u3053\u3068\u306b\u3042\u308a\u307e\u3059\u3002</li><li><strong>\u4e0d\u504f\u7684\u5229\u4ed6\u4e3b\u7fa9\uff08Impartial altruism\uff09</strong>\uff1a\u79c1\u305f\u3061\u306f\u3001\u5168\u3066\u306e\u4eba\u3073\u3068\u7b49\u3057\u304f\u91cd\u8981\u3067\u3042\u308b\u3068\u8003\u3048\u3066\u3044\u307e\u3059<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzu5s20p9d6n\"><sup><a href=\"#fnzu5s20p9d6n\">[25]</a></sup></span>\u3002\u3082\u3061\u308d\u3093\u81ea\u5206\u81ea\u8eab\u306e\u5bb6\u65cf\u3084\u53cb\u4eba\u3001\u81ea\u5206\u306e\u4eba\u751f\u306b\u3001\u7279\u5225\u306a\u95a2\u5fc3\u3092\u3082\u3064\u306e\u306f\u3082\u3063\u3068\u3082\u306a\u3053\u3068\u3067\u3059\u3002\u3057\u304b\u3057\u53ef\u80fd\u306a\u9650\u308a\u3067\u6700\u5927\u306e\u3008\u3088\u3044\u3053\u3068\u3009\u3092\u884c\u304a\u3046\u3068\u3059\u308b\u5834\u5408\u3001\u79c1\u305f\u3061\u306f\u3001\u751f\u304d\u3066\u3044\u308b\u5834\u6240\u3084\u6642\u4ee3\u306b\u304b\u304b\u308f\u3089\u305a\u3001\u5168\u54e1\u306e\u5229\u5bb3\u306b\u7b49\u3057\u3044\u91cd\u307f\u3092\u4e0e\u3048\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u3066\u3044\u307e\u3059\u3002\u3064\u307e\u308a\u79c1\u305f\u3061\u306f\u6700\u3082\u7121\u8996\u3055\u308c\u3066\u304d\u305f\u96c6\u56e3\u306b\u7126\u70b9\u3092\u5f53\u3066\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u305d\u308c\u306f\u5927\u62b5\u306e\u5834\u5408\u3001\u81ea\u5206\u81ea\u8eab\u306e\u5229\u76ca\u3092\u78ba\u4fdd\u3059\u308b\u529b\u3092\u5341\u5206\u306b\u3082\u305f\u306a\u3044\u5b58\u5728\u306b\u7126\u70b9\u3092\u5f53\u3066\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002</li><li><strong>\u958b\u304b\u308c\u305f\u771f\u7406\u306e\u63a2\u6c42\uff08Open truthseeking\uff09</strong>\uff1a\u3042\u308b\u7279\u5b9a\u306e\u8ab2\u984c\u3084\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u3001\u30a2\u30d7\u30ed\u30fc\u30c1\u3078\u306e\u30b3\u30df\u30c3\u30c8\u30e1\u30f3\u30c8\u3092\u51fa\u767a\u70b9\u306b\u53d6\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u652f\u63f4\u306e\u305f\u3081\u306e\u69d8\u3005\u306a\u3084\u308a\u65b9\u3092\u691c\u8a0e\u3057\u3001\u305d\u306e\u306a\u304b\u3067\u6700\u826f\u306e\u3082\u306e\u3092\u898b\u3064\u3051\u51fa\u305d\u3046\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002\u305d\u306e\u305f\u3081\u306b\u306f\u3001\u81ea\u5206\u81ea\u8eab\u306e\u4fe1\u5ff5\u306b\u3064\u3044\u3066\u719f\u8003\u3057\u3001\u305d\u308c\u3092\u9867\u307f\u308b\u3053\u3068\u306b\u76f8\u5f53\u306e\u6642\u9593\u3092\u8cbb\u3084\u3057\u3001\u65b0\u305f\u306a\u30a8\u30d3\u30c7\u30f3\u30b9\u3084\u8b70\u8ad6\u306b\u5bfe\u3057\u3066\u958b\u304b\u308c\u305f\u5fc3\u3068\u95a2\u5fc3\u3092\u5e38\u306b\u3082\u3061\u3001\u81ea\u5206\u306e\u610f\u898b\u3092\u6839\u5e95\u304b\u3089\u5909\u3048\u3066\u3057\u307e\u3048\u308b\u7528\u610f\u304c\u3042\u308b\u306e\u3067\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002</li><li><strong>\u5354\u50cd\u3059\u308b\u7cbe\u795e\uff08Collaborative spirit\uff09</strong>\uff1a\u5354\u50cd\u3059\u308b\u3053\u3068\u3067\u3088\u308a\u591a\u304f\u306e\u3053\u3068\u3092\u9054\u6210\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u3057\u304b\u3057\u5354\u50cd\u3092\u52b9\u679c\u7684\u306b\u9054\u6210\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u9ad8\u5ea6\u306e\u8aa0\u5b9f\u3055\u3084\u89aa\u3057\u307f\u3084\u3059\u3055\u3001\u305d\u3057\u3066\u96c6\u56e3\u306e\u89b3\u70b9\u306b\u7acb\u3063\u3066\u7269\u4e8b\u3092\u8003\u3048\u308b\u80fd\u529b\u304c\u5fc5\u8981\u3067\u3059\u3002\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u3068\u3044\u3046\u3053\u3068\u3067\u554f\u984c\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f\u300c\u76ee\u7684\u304c\u624b\u6bb5\u3092\u6b63\u5f53\u5316\u3059\u308b\u300d\u3068\u3044\u3046\u985e\u306e\u7406\u7531\u3065\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u3008\u3088\u308a\u3088\u3044\u3009\u4e16\u754c\u306b\u5411\u304b\u3063\u3066\u91ce\u5fc3\u7684\u306b\u50cd\u304d\u304b\u3051\u306a\u304c\u3089\u3001\u826f\u304d\u5e02\u6c11\u3067\u3042\u308b\u3053\u3068\u304c\u554f\u984c\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u3059\u3002</li></ol><p>\u4ee5\u4e0a\u306e\u4fa1\u5024\u3092\u5171\u6709\u3057\u3001\u4ed6\u8005\u3092\u52a9\u3051\u308b\u305f\u3081\u306e\u3088\u308a\u3088\u3044\u65b9\u6cd5\u3092\u63a2\u3057\u3066\u3044\u308b\u4eba\u306a\u3089\u8ab0\u3057\u3082\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306b\u4e0e\u3057\u3066\u3044\u308b\u308f\u3051\u3067\u3059\u3002\u3069\u308c\u304f\u3089\u3044\u306e\u6642\u9593\u3068\u8cc7\u91d1\u3092\u8cbb\u3084\u3057\u305f\u3044\u306e\u304b\u3001\u3069\u306e\u8ab2\u984c\u3092\u9078\u3093\u3067\u53d6\u308a\u7d44\u3080\u306e\u304b\u306f\u95a2\u4fc2\u304c\u3042\u308a\u307e\u305b\u3093\u3002</p><p>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u3092\u79d1\u5b66\u7684\u624b\u6cd5\u3068\u5bfe\u6bd4\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\u79d1\u5b66\u306f\u3001\u305f\u3068\u3048\u7d50\u679c\u304c\u76f4\u89b3\u306b\u53cd\u3057\u3066\u3044\u3066\u3082\u3001\u3042\u308b\u3044\u306f\u4f1d\u7d71\u306b\u6b6f\u5411\u304b\u3046\u3082\u306e\u3067\u3042\u3063\u3066\u3082\u3001\u771f\u7406\u306e\u63a2\u6c42\u306b\u30a8\u30d3\u30c7\u30f3\u30b9\u3068\u63a8\u8ad6\u3092\u7528\u3044\u307e\u3059\u3002\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306f\u3008\u3088\u3044\u3053\u3068\u3009\u3092\u884c\u3046\u6700\u5584\u306e\u65b9\u6cd5\u3092\u63a2\u6c42\u3059\u308b\u305f\u3081\u306b\u30a8\u30d3\u30c7\u30f3\u30b9\u3068\u63a8\u8ad6\u3092\u7528\u3044\u308b\u306e\u3067\u3059\u3002</p><p><br>\u79d1\u5b66\u7684\u624b\u6cd5\u306f\u5358\u7d14\u306a\u8003\u3048\uff08\u4f8b\u3048\u3070\u81ea\u5206\u306e\u4fe1\u5ff5\u3092\u691c\u8a3c\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u3068\u3044\u3046\u3088\u3046\u306a\uff09\u306b\u57fa\u3065\u304f\u3082\u306e\u3067\u3042\u308b\u3082\u306e\u306e\u3001\u4e16\u754c\u306b\u3064\u3044\u3066\u306e\u3014\u5e38\u8b58\u3068\u306f\u3015\u6839\u672c\u7684\u306b\u7570\u306a\u308b\u63cf\u50cf\uff08\u4f8b\u3048\u3070\u91cf\u5b50\u529b\u5b66\uff09\u306b\u884c\u304d\u7740\u304d\u307e\u3059\u3002\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u3082\u540c\u69d8\u306b\u3001\u5358\u7d14\u306a\u8003\u3048\u2500\u2500\u79c1\u305f\u3061\u306f\u4eba\u3073\u3068\u3092\u7b49\u3057\u304f\u6271\u3046\u3079\u304d\u3067\u3042\u308a\u3001\u307e\u305f\u3088\u308a\u5c11\u306a\u3044\u4eba\u3073\u3068\u3088\u308a\u3082\u3088\u308a\u591a\u304f\u306e\u4eba\u3073\u3068\u306e\u52a9\u3051\u306b\u306a\u308b\u65b9\u304c\u3088\u308a\u3088\u3044\u306e\u3060\u3068\u3044\u3046\u8003\u3048\u2500\u2500\u3092\u571f\u53f0\u306b\u636e\u3048\u3066\u3001\u3008\u3088\u3044\u3053\u3068\u3009\u3092\u3059\u308b\u3053\u3068\u306b\u3064\u3044\u3066\u306e\u578b\u7834\u308a\u3067\u3001\u767a\u5c55\u3057\u7d9a\u3051\u308b\u63cf\u50cf\u306b\u884c\u304d\u7740\u304f\u306e\u3067\u3059\u3002</p><h1>\u884c\u52d5\u306e\u8d77\u3053\u3057\u65b9</h1><p>\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306b\u95a2\u5fc3\u3092\u3082\u3064\u4eba\u3073\u3068\u306f\u307b\u3068\u3093\u3069\u306e\u5834\u5408\u3001\u6b21\u306e\u69d8\u3005\u306a\u4ed5\u65b9\u3067\u305d\u306e\u8003\u3048\u65b9\u3092\u81ea\u5206\u306e\u751f\u6d3b\u306b\u53d6\u308a\u5165\u308c\u3066\u3044\u307e\u3059\u3002</p><ul><li>\u5207\u8feb\u3057\u305f\u554f\u984c\u306b\u53d6\u308a\u7d44\u3080\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306a\u30ad\u30e3\u30ea\u30a2\u3092\u9078\u3093\u3060\u308a\u3001<a href=\"https://80000hours.org/\">80,000 Hours</a> \u304b\u3089\u306e\u52a9\u8a00\u3092\u5229\u7528\u3059\u308b\u306a\u3069\u3057\u3066\u3001\u624b\u6301\u3061\u306e\u30b9\u30ad\u30eb\u3092\u4f7f\u3063\u3066\u305d\u3046\u3057\u305f\u554f\u984c\u306b\u8ca2\u732e\u3059\u308b\u65b9\u6cd5\u3092\u898b\u3064\u3051\u308b\u3002</li><li><a href=\"https://www.givewell.org/\">GiveWell</a> \u3084&nbsp;<a href=\"https://www.givingwhatwecan.org/best-charities-to-donate-to-2022\">Giving What We Can</a> \u306a\u3069\u306e\u8abf\u67fb\u3092\u5229\u7528\u3059\u308b\u306a\u3069\u3057\u3066\u3001\u614e\u91cd\u306b\u9078\u3070\u308c\u305f\u6148\u5584\u56e3\u4f53\u306b\u5bc4\u4ed8\u3092\u884c\u3046\u3002</li><li>\u5207\u8feb\u3057\u305f\u8af8\u554f\u984c\u3078\u306e\u53d6\u308a\u7d44\u307f\u3092\u652f\u63f4\u3059\u308b<a href=\"https://80000hours.org/career-reviews/founder-impactful-organisations/#lists-of-ideas\"><u>\u65b0\u305f\u306a\u7d44\u7e54\u3092\u8a2d\u7acb\u3059\u308b</u></a>\u3002</li><li>\u5207\u8feb\u3057\u305f\u554f\u984c\u306b\u53d6\u308a\u7d44\u3080\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306e\u5f62\u6210\u3092\u652f\u63f4\u3059\u308b\u3002</li></ul><p><u>\u3053\u308c\u3088\u308a\u9577\u3044\u3001</u><a href=\"https://forum.effectivealtruism.org/topics/take-action\"><u>\u884c\u52d5\u30ea\u30b9\u30c8</u></a>\u3082\u3054\u89a7\u304f\u3060\u3055\u3044\u3002</p><p>\u4e0a\u8a18\u306e\u30ea\u30b9\u30c8\u306f\u5305\u62ec\u7684\u306a\u3082\u306e\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u3008\u3088\u3044\u3053\u3068\u3009\u3092\u3059\u308b\u3053\u3068\u306b\u3069\u308c\u304f\u3089\u3044\u6ce8\u529b\u3057\u305f\u3044\u306e\u304b\u306b\u304b\u304b\u308f\u3089\u305a\u3001\u307e\u305f\u3042\u306a\u305f\u306e\u4eba\u751f\u306e\u3069\u306e\u5074\u9762\u3067\u305d\u3046\u3057\u305f\u3044\u306e\u3067\u3042\u308c\u3001\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u3092\u53d6\u308a\u5165\u308c\u308b\u3053\u3068\u306f\u53ef\u80fd\u3067\u3059\u3002\u91cd\u8981\u306a\u306e\u306f\u3001\u6210\u3057\u305f\u3044\u3068\u601d\u3046\u8ca2\u732e\u306e\u5be1\u591a\u3067\u306f\u306a\u304f\u3001\u3042\u306a\u305f\u306e\u305d\u306e\u52aa\u529b\u304c\u4e0a\u8a184\u3064\u306e\u4fa1\u5024\u306b\u5c0e\u304b\u308c\u3066\u3044\u308b\u3053\u3068\u3068\u3001\u81ea\u5206\u306e\u52aa\u529b\u3092\u53ef\u80fd\u306a\u9650\u308a\u52b9\u679c\u7684\u306a\u3082\u306e\u306b\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u3053\u3068\u3067\u3059\u3002</p><p>\u305d\u306e\u305f\u3081\u306b\u306f\u3001\u898b\u904e\u3054\u3055\u308c\u3066\u3044\u308b\u5730\u7403\u898f\u6a21\u306e\u5927\u554f\u984c\u3068\u6700\u3082\u52b9\u679c\u7684\u306a\u89e3\u6c7a\u7b56\u3001\u305d\u3057\u3066\u3001\u305d\u3046\u3057\u305f\u89e3\u6c7a\u7b56\u306b\u5bc4\u4e0e\u3059\u308b\u305f\u3081\u306e\u65b9\u6cd5\u3092\u898b\u3064\u3051\u51fa\u3059\u52aa\u529b\u304c\u5fc5\u8981\u3068\u306a\u308b\u3067\u3057\u3087\u3046\u3002\u305f\u3060\u3057\u3001\u8cbb\u3084\u305d\u3046\u3068\u3059\u308b\u6642\u9593\u3084\u304a\u91d1\u306e\u91cf\u306f\u95a2\u4fc2\u304c\u3042\u308a\u307e\u305b\u3093\u3002</p><p>\u4ee5\u4e0a\u3092\u884c\u3044\u3001\u614e\u91cd\u306b\u8003\u3048\u629c\u304f\u3053\u3068\u3067\u3001\u3042\u306a\u305f\u304c\u8cbb\u3084\u3057\u305f\u3044\u3068\u601d\u3046\u6642\u9593\u3084\u304a\u91d1\u3092\u4f7f\u3063\u3066\u9065\u304b\u306b\u5927\u304d\u306a\u30a4\u30f3\u30d1\u30af\u30c8\u3092\u4e0e\u3048\u3089\u308c\u308b\u3053\u3068\u306b\u6c17\u3065\u304f\u3053\u3068\u304c\u3067\u304d\u308b\u3067\u3057\u3087\u3046\u3002\u3042\u306a\u305f\u306e\u30ad\u30e3\u30ea\u30a2\u5168\u4f53\u3092\u901a\u3058\u3066\u3001\u4f55\u767e\u3082\u306e\u4eba\u3073\u3068\u306e\u547d\u3092\u6551\u3046\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u3002\u305d\u3057\u3066\u3001\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u5185\u306e\u4eba\u3073\u3068\u3068\u529b\u3092\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u3053\u3093\u306b\u3061\u306e\u6587\u660e\u304c\u76f4\u9762\u3059\u308b\u6700\u3082\u91cd\u8981\u306a\u554f\u984c\u306b\u5bfe\u51e6\u3059\u308b\u8a66\u307f\u306e\u4e00\u7ffc\u3092\u62c5\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3059\u3002</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0ll7h79wcif\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0ll7h79wcif\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\uff3b\u8a33\u6ce8\uff3d\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u3092\u5352\u5f15\u3059\u308b\u54f2\u5b66\u8005\u3067\u3042\u308a\u3001Center for Effective Altruism \u306b\u3088\u308b\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306e\u5b9a\u7fa9\u306e\u4f5c\u6210\u3082\u4e3b\u5c0e\u3057\u305f William MacAskill \u306f<a href=\"https://static1.squarespace.com/static/5506078de4b02d88372eee4e/t/5f36ae8fd76ee3582c25475d/1597419152486/The_Definition_of_Effective_Altruism.pdf\"><u>\u305d\u306e\u8ad6\u6587 \u201cThe Definition of Effective Altruism\u201d</u></a>\u306e\u4e2d\u3067\u3001\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306b\u304a\u3051\u308b\u300c\u3088\u3044\u3053\u3068\u300d\u306f\u300c\u4e0d\u504f\u7684\u539a\u751f\u4e3b\u7fa9\u7684\u300d\uff08impartial welfarist\uff09\u89b3\u70b9\u304b\u3089\u66ab\u5b9a\u7684\u306b\u7406\u89e3\u3055\u308c\u308b\u3068\u8ff0\u3079\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u6b21\u306e\u3053\u3068\u3092\u610f\u5473\u3057\u3066\u3044\u307e\u3059\u3002<br><br>\u300c\u66ab\u5b9a\u7684\u306a\u4eee\u8aac\u306a\u3044\u3057\u7b2c\u4e00\u6b21\u8fd1\u4f3c\u3068\u3057\u3066\u3001\u3088\u3044\u3053\u3068\u3092\u884c\u3046\u3053\u3068\u3014doing good\u3015\u306f\u3001\u8ab0\u3057\u3082\u306e\u798f\u5229\u3014everyone\u2019s wellbeing\u3015\u3092\u7b49\u3057\u304f\u6570\u3048\u3064\u3064\u3001\u798f\u5229\u3092\u4fc3\u9032\u3059\u308b\u3053\u3068\u3067\u3042\u308b\u3002\u3088\u308a\u6b63\u78ba\u306b\u306f\u3001\u6709\u9650\u306e\u6570\u306e\u540c\u3058\u500b\u4f53\u3092\u3001\u307e\u305f\u305d\u308c\u306e\u307f\u3092\u542b\u3080\u3075\u305f\u3064\u306e\u4e16\u754cA\u3068B\u306b\u3064\u3044\u3066\u3001A\u304b\u3089B\u3078\u306e1\u5bfe1\u5199\u50cf\u304c\u5b58\u5728\u3057\u3001A\u306b\u304a\u3051\u308b\u3069\u306e\u500b\u4f53\u3082B\u306b\u304a\u3051\u308b\u305d\u306e\u5bfe\u5fdc\u7269\u3068\u540c\u3058\u798f\u5229\u3092\u3082\u3064\u306a\u3089\u3001A\u3068B\u306f\u7b49\u3057\u304f\u3088\u3044\u3002\u300d\uff08accessed 12 Sept. 2022. p. 14\uff09<br><br>\u502b\u7406\u5b66\u3067\u306f\u3001\u798f\u5229\u3092\u542b\u3080\u3088\u3046\u306a \u201cthe good\u201d\u306f\u300c\u5584\u3055\u300d\u3068\u8a33\u3059\u306e\u304c\u4e00\u822c\u7684\u3067\u3059\u304c\u3001\u8852\u5b66\u7684\u306b\u805e\u3053\u3048\u308b\u304d\u3089\u3044\u304c\u3042\u308a\u3001\u73fe\u5728\u306e\u30c6\u30ad\u30b9\u30c8\u306e\u5165\u9580\u7684\u306a\u6027\u683c\u3082\u8003\u616e\u3057\u3066\u3001\u4ee5\u4e0b\u3067\u3053\u306e\u985e\u306e\u8a9e\u306f\u3088\u308a\u67d4\u3089\u304b\u304f\u3001\u3072\u3089\u304c\u306a\u3067\u300c\u3088\u3055\u300d\u3084\u300c\u3088\u3044\u3053\u3068\u300d\u3068\u8a33\u3059\u3053\u3068\u306b\u3057\u307e\u3057\u305f\u3002\u8a33\u8a9e\u306e\u9078\u629e\u306b\u3064\u3044\u3066\u306f\u3001\u9ad9\u6a4b\u793c \u3001\u798f\u5cf6\u5f26\u4e21\u6c0f\u306b\u52a9\u8a00\u3092\u9802\u304d\u307e\u3057\u305f\u3002\u3053\u3053\u306b\u8a18\u3057\u3066\u611f\u8b1d\u7533\u3057\u4e0a\u3052\u307e\u3059\u3002\u307e\u305f\u3001\u3053\u3053\u3067\u3082\u4ee5\u4e0b\u3067\u3082\u3001\u89d2\u62ec\u5f27\uff08\u3008 \u3009\uff09\u306f\u8aad\u307f\u3084\u3059\u3055\u306e\u305f\u3081\u306b\u8a33\u8005\u304c\u88dc\u3063\u305f\u3082\u306e\u3067\u3042\u308a\u3001\u4e80\u7532\u62ec\u5f27\uff08\u3014 \u3015\uff09\u306f\u8a33\u8005\u306e\u88dc\u8db3\u3092\u8868\u3057\u307e\u3059\u3002</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fni1cu6phuiwa\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefi1cu6phuiwa\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;<a href=\"https://forum.effectivealtruism.org/community\"><u>Effective Altruism Forum</u></a>\u3067\u306f\u3001\u5404\u5730\u57df\u306eEA\u56e3\u4f53\u304c\u4e16\u754c\u4e2d\u306b\u5206\u5e03\u3057\u3066\u3044\u308b\u306e\u3092\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u3053\u308c\u3092\u898b\u308b\u3068\u300170\u4ee5\u4e0a\u306e\u56fd\u306bEA\u56e3\u4f53\u304c\u3042\u308b\u306e\u304c\u5206\u304b\u308a\u307e\u3059\u3002</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnekd7h7nqi1u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefekd7h7nqi1u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u3042\u308b\u554f\u984c\u304c\u898b\u904e\u3054\u3055\u308c\u3066\u3044\u308b\u5ea6\u5408\u3044\u304c\u5c11\u306a\u3051\u308c\u3070\u5c11\u306a\u3044\u307b\u3069\u3001\u3088\u308a\u591a\u304f\u306e\u6700\u5584\u306e\u8a66\u307f\u304c\u65e2\u306b\u306a\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u306b\u306a\u308b\u305f\u3081\u3001\u8ffd\u52a0\u306e\u4eba\u6750\u304c\u30a4\u30f3\u30d1\u30af\u30c8\u3092\u4e0e\u3048\u308b\u306e\u306f\u3088\u308a\u96e3\u3057\u304f\u306a\u308b\u3067\u3057\u3087\u3046\u3002</p><p>\u5b9f\u969b\u3001\u3042\u308b\u554f\u984c\u3078\u306e\u8cc7\u6e90\u306e\u6295\u5165\u306b\u5bfe\u3059\u308b\u30ea\u30bf\u30fc\u30f3\u304c\u6982\u306d\u5bfe\u6570\u95a2\u6570\u306b\u5f93\u3046<a href=\"https://web.archive.org/web/20220728174703/https://www.fhi.ox.ac.uk/law-of-logarithmic-returns/\"><u>\u3068\u8003\u3048\u308b\u3088\u3044\u7406\u7531</u></a>\u304c\u3042\u308a\u307e\u3059\u3002\u30ea\u30bf\u30fc\u30f3\u304c\u5bfe\u6570\u95a2\u6570\u306b\u5f93\u3046\u3068\u306f\u3001\u3053\u308c\u307e\u3067\u306b10\u500d\u306e\u8cc7\u6e90\u304c\u4e00\u65b9\u306e\u8ab2\u984c\u306b\u6295\u5165\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306b\u306f\u3001\u8ffd\u52a0\u306e\u8cc7\u6e90\u306f\u3014\u305d\u306e10\u5206\u306e1\u306e\u8cc7\u6e90\u3057\u304b\u6295\u5165\u3055\u308c\u3066\u3044\u306a\u3044\u8ab2\u984c\u306b\u6295\u5165\u3055\u308c\u308b\u5834\u5408\u3068\u6bd4\u3079\u3066\u3015\u7d0410\u5206\u306e1\u7a0b\u5ea6\u306e\u9032\u6357\u3057\u304b\u751f\u307e\u306a\u3044\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002</p><p>\u3082\u3057\u3075\u305f\u3064\u306e\u554f\u984c\u304c\u7b49\u3057\u304f\u91cd\u8981\u3067\u3042\u308a\u3001\u304b\u3064\u4e00\u65b9\u304c\u3088\u308a\u898b\u904e\u3054\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u3014\u305d\u306e\u305f\u3081\u4e00\u65b9\u306e\u8ab2\u984c\u306b\u306f\u4ed6\u65b9\u306e\u8ab2\u984c\u306e10\u5206\u306e1\u306e\u8cc7\u6e90\u3057\u304b\u6295\u5165\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u3015\u3001\u3042\u308b\u4eba\u7269\u304c\u3014\u305d\u306e\u898b\u904e\u3054\u3055\u308c\u3066\u3044\u308b\u65b9\u306e\u8ab2\u984c\u306b\u53d6\u308a\u7d44\u3080\u3079\u304f\u3015\u8ffd\u52a0\u3055\u308c\u308b\u306a\u3089\u3001\u305d\u306e\u3072\u3068\u306f\u3014\u4ed6\u65b9\u306e\u8ab2\u984c\u306b\u53d6\u308a\u7d44\u3080\u3088\u308a\u3082\u301510\u500d\u306e\u30a4\u30f3\u30d1\u30af\u30c8\u3092\u3082\u3064\u3053\u3068\u306b\u306a\u308b\u3067\u3057\u3087\u3046\u3002</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn11tgvd21b6dg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref11tgvd21b6dg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>2010\u5e74\u304b\u30892019\u5e74\u307e\u3067\u306e\u9593\u306b\u7c73\u56fd\u9023\u90a6\u653f\u5e9c\u304c\u5065\u5eb7\u5b89\u5168\u4fdd\u969c\u306e\u305f\u3081\u306b\u63d0\u4f9b\u3057\u305f\u8cc7\u91d1\u306f1,410\u5104\u30c9\u30eb\u3068\u63a8\u5b9a\u3055\u308c\u307e\u3059\u3002\u3053\u306e\u3046\u3061\u306e55%\u304c\u304a\u305d\u3089\u304f\u306f\u5c06\u6765\u306e\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u3092\u9632\u3050\u65b9\u7b56\u306b\u8cbb\u3084\u3055\u308c\u305f\u3068\u79c1\u305f\u3061\u306f\u8003\u3048\u3066\u3044\u307e\u3059\u3002\u4f8b\u3048\u30704%\u306f\u9032\u884c\u4e2d\u306e\u30a8\u30dc\u30e9\u30a6\u30a3\u30eb\u30b9\u306e\u6d41\u884c\u306b\u5bfe\u51e6\u3059\u308b\u305f\u3081\u306b\u8cbb\u3084\u3055\u308c\u3001\u3053\u308c\u306f\u4ed6\u306e\u6f5c\u5728\u7684\u306a\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u3092\u9632\u3050\u305f\u3081\u306e\u30a4\u30f3\u30d5\u30e9\u3092\u6574\u5099\u3059\u308b\u3053\u3068\u306b\u3082\u3064\u306a\u304c\u308a\u307e\u3057\u305f\u3002\u3057\u304b\u3057\u3001\u5f53\u8a72\u306e\u8cc7\u91d1\u306e\u3046\u306117%\u306f\u5c06\u6765\u306e\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306e\u767a\u751f\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u306a\u3044\u3088\u3046\u306a\u4ed5\u65b9\u3067\u3001\u5316\u5b66\u7684\u8105\u5a01\u3084\u6838\u653e\u5c04\u7dda\u306e\u8105\u5a01\u306b\u5bfe\u51e6\u3059\u308b\u305f\u3081\u306b\u8cbb\u3084\u3055\u308c\u307e\u3057\u305f\u3002</p><p>1,410\u5104 \u00d7 0.55 = 790\u5104</p><p>2010\u5e74\u304b\u30892019\u5e74\u307e\u3067\u306e10\u5e74\u9593\u306b\u3064\u3044\u3066\u5e74\u63db\u7b97\u3059\u308b\u306a\u3089\u3001\u5e74\u959380\u5104\u30c9\u30eb\u306b\u306a\u308b\u3002</p><p>Federal funding for health security in FY2019 Watson, Crystal, et al., Health security 16.5 (2018): pages 281-303. Archived link, accessed 5 March 2020.</p><p>Open Philanthropy \u306f\u3001\u30b3\u30ed\u30ca\u30fb\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u4ee5\u524d\u304b\u3089\u3053\u306e\u554f\u984c\u306b\u53d6\u308a\u7d44\u3093\u3067\u3044\u305f\u4ed6\u306e\u57fa\u91d1\u3084\u6148\u5584\u5bb6\u3082\u7279\u5b9a\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u308c\u3089\u3092\u5408\u308f\u305b\u3066\u3082\u305d\u306e\u8cc7\u91d1\u7dcf\u984d\u306f1\u5104\u30c9\u30eb\u306b\u6e80\u305f\u306a\u3044\u3068\u601d\u308f\u308c\u307e\u3059\u3002</p><p>Costs of War \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u7406\u4e8b\u3067\u3042\u308b\u30af\u30ed\u30fc\u30d5\u30a9\u30fc\u30c9\u306f\u30012001\u5e74\u304b\u30892022\u5e74\u307e\u3067\u306e\u7c73\u56fd\u306e\u30c6\u30ed\u5bfe\u7b56\u8cbb\u30925\u51468\u5343\u5104\u30c9\u30eb\u3068\u7b97\u51fa\u3057\u3066\u3044\u307e\u3059\u3002</p><p>5.8\u5146\u30c9\u30eb/20\u5e74\uff1d\u5e74\u95932,900\u5104\u30c9\u30eb</p><p>United States budgetary costs of Post-9/11 wars Crawford, Neta C., Watson Institute for International &amp; Public Affairs, Brown University, 2021. Archived link, accessed 26 July 2022.\u21a9</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnb8jr023kwyr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefb8jr023kwyr\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://web.archive.org/web/20220811132237/https://www.start.umd.edu/gtd/search/Results.aspx?chart=fatalities&amp;casualties_type=&amp;casualties_max=\">Global Terrorism Database 2020</a>\uff08Accessed 11 August 2022.\uff09\u306b\u3088\u308b\u3068\u30011970\u5e74\u304b\u30892020\u5e74\u307e\u3067\u306e\u30c6\u30ed\u306b\u3088\u308b\u6b7b\u8005\u6570\u306f\u7d04456,000\u4eba\u3067\u3059\u3002</p><p>\u305f\u3060\u3057 Our World in Data \u306f\u6b21\u306e\u3088\u3046\u306b\u6307\u6458\u3057\u3066\u3044\u307e\u3059\u3002\u300cGlobal Terrorism Database \u306f\u30c6\u30ed\u653b\u6483\u306b\u95a2\u3059\u308b\u5165\u624b\u53ef\u80fd\u306a\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u306e\u4e2d\u3067\u6700\u3082\u5305\u62ec\u7684\u3067\u3042\u308a\u3001\u8fd1\u5e74\u306e\u30c7\u30fc\u30bf\u306f\u5b8c\u5168\u3067\u3059\u3002\u3057\u304b\u3057\u6211\u3005\u306e\u5206\u6790\u306b\u57fa\u3065\u304f\u3068\u3001\u3088\u308a\u9577\u671f\u306b\u308f\u305f\u308b\u30c7\u30fc\u30bf\u306f\uff08\u7c73\u56fd\u3068\u6b27\u5dde\u3092\u9664\u3051\u3070\uff09\u4e0d\u5b8c\u5168\u3060\u3068\u4e88\u60f3\u3055\u308c\u307e\u3059\u3002\u3057\u305f\u304c\u3063\u3066\u30c6\u30ed\u6d3b\u52d5\u306e\u767a\u751f\u983b\u5ea6\u306b\u95a2\u3059\u308b\u5730\u7403\u898f\u6a21\u3067\u306e\u9577\u671f\u7684\u306a\u50be\u5411\u3092\u63a8\u8ad6\u3059\u308b\u306e\u306b\u3053\u306e\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u306f\u63a8\u5968\u3057\u307e\u305b\u3093\u3002\u300d</p><p>\u3053\u308c\u304c\u610f\u5473\u3057\u3066\u3044\u308b\u306e\u306f\u3001\u4e0a\u8a18\u306e\u30bd\u30fc\u30b9\u306f\u3001\u30c6\u30ed\u6d3b\u52d5\u306b\u3088\u308b\u78ba\u8a8d\u3055\u308c\u305f\u6b7b\u8005\u6570\u306b\u95a2\u3057\u3066\u6570\u3048\u6f0f\u308c\u304c\u3042\u308a\u305d\u3046\u3060\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u3057\u304b\u30571970\u5e74\u304b\u3089\u306e\u6b7b\u8005\u6570\u304c\u3001\u30c6\u30ed\u306b\u3088\u308b\u6b7b\u8005\u6570\u304c\u6700\u591a\u3092\u8a18\u9332\u3057\u305f\u5341\u5e74\u9593\uff082010-2020\uff09\u3068\u540c\u3058\u6c34\u6e96\u3060\u3063\u305f\u3068\u4eee\u5b9a\u3057\u305f\u5834\u5408\u3067\u3055\u3048\u3001\u7dcf\u6b7b\u8005\u6570\u306f120\u4e07\u4eba\u306b\u904e\u304e\u305a\u3001\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306b\u3088\u308b\u6b7b\u4ea1\u8005\u6570\u3088\u308a\u306f\u308b\u304b\u306b\u5c11\u306a\u3044\u6570\u306b\u306a\u308a\u307e\u3059\u3002</p><p>COVID-19\u306b\u3088\u308b\u6b7b\u8005\u6570\uff1a<br>\u30a8\u30b3\u30ce\u30df\u30b9\u30c8\u8a8c\uff08The Economist\uff09\u306f\u3001COVID-19\u306b\u3088\u308b\u7d2f\u7a4d\u8d85\u904e\u6b7b\u8005\u6570\u306f\u30012022\u5e746\u6708\u6642\u70b9\u30672,147\u4e07\u4eba\u3068\u63a8\u5b9a\u3057\u3066\u304a\u308a\u3001\u304b\u3064\u3053\u306e\u6570\u306f\u4eca\u3082\u5897\u3048\u7d9a\u3051\u3066\u3044\u307e\u3059\u3002</p><p>\u3053\u306e\u30c7\u30fc\u30bf\u3068\u63a8\u5b9a\u306b\u4f7f\u308f\u308c\u305f\u30e2\u30c7\u30eb\u306f<a href=\"https://web.archive.org/web/20220728171227/https://ourworldindata.org/grapher/excess-deaths-cumulative-economist-single-entity?country=~OWID_WRL\">Our World in Data</a> (archived page, retrieved 28 July 2022)\u3067\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p><p>\u79c1\u305f\u3061\u306f\u3053\u306e\u30c7\u30fc\u30bf\u304c\u3001COVID-19\u306b\u3088\u308b\u7dcf\u6b7b\u8005\u6570\u306b\u95a2\u3059\u308b\u73fe\u6bb5\u968e\u3067\u6700\u3082\u8cea\u306e\u9ad8\u3044\u63a8\u5b9a\u3060\u3068\u8003\u3048\u3066\u3044\u307e\u3059\u3002<u>\u78ba\u8a8d\u3055\u308c\u305f</u>\u6b7b\u8005\u6570\u306f600\u4e07\u7a0b\u5ea6\u3068\u524d\u8ff0\u306e\u6570\u3088\u308a\u3082\u4f4e\u3044\u3067\u3059\u304c\u3001\u3053\u306e\u6570\u306f\u9593\u63a5\u7684\u306b\u5f15\u304d\u8d77\u3053\u3055\u308c\u305f\u6b7b\u3084\u5831\u544a\u3055\u308c\u3066\u3044\u306a\u3044\u6b7b\u8005\u306e\u6570\u3092\u542b\u3081\u3066\u3044\u307e\u305b\u3093\u3002\u30a8\u30b3\u30ce\u30df\u30b9\u30c8\u8a8c\u306e\u624b\u6cd5\u306f\u3001\u671f\u5225\u5e73\u5747\u6b7b\u8005\u6570\u3068\u8d85\u904e\u6b7b\u8005\u6570\u3092\u6bd4\u8f03\u3057\u3001\u5408\u8a08\u3067\u4f55\u4eba\u304c\u8ffd\u52a0\u3067\u6b7b\u4ea1\u3057\u305f\u304b\u3092\u63a8\u5b9a\u3059\u308b\u3068\u3068\u3082\u306b\u3001\u904e\u5c11\u5831\u544a\u5206\u3092\u8abf\u6574\u3057\u305f\u3082\u306e\u3067\u3059\u3002</p><p>\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u3068\u30c6\u30ed\u306b\u3088\u308b\u6b7b\u8005\u6570\u306f\u3044\u305a\u308c\u3082\u88fe\u306e\u91cd\u3044\u5206\u5e03\uff08\u30d8\u30d3\u30fc\u30c6\u30a4\u30eb\uff09\u306b\u306a\u308b\u305f\u3081\u3001\u904e\u53bb\u306e\u6b7b\u4ea1\u7387\u306f\u901a\u5e38\u3001\u30ea\u30b9\u30af\u306e\u5927\u304d\u3055\u3092\u63a7\u3048\u3081\u306b\u898b\u7a4d\u3082\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002</p><p>\u4f8b\u3048\u3070\u3001\u30c6\u30ed\u30ea\u30b9\u30c8\u304c\u5927\u90fd\u5e02\u3067\u6838\u5175\u5668\u3092\u4f5c\u52d5\u3055\u305b\u308b\u3053\u3068\u3082\u3042\u308a\u3048\u307e\u3059\u304c\u3001\u305d\u306e\u5834\u5408\u3001100\u4e07\u4eba\u4ee5\u4e0a\u304c\u6b7b\u4ea1\u3059\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u3053\u308c\u306f\u3001\u904e\u53bb50\u5e74\u9593\u306b\u306f\u8d77\u3053\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u304c\u3001\u3082\u3057\u8d77\u3053\u3063\u3066\u3044\u305f\u3089\u3001\u6b7b\u8005\u6570\u306e\u5897\u52a0\u306e\u4e3b\u306a\u8981\u56e0\u306b\u306a\u3063\u3066\u3044\u305f\u3067\u3057\u3087\u3046\u3002\u540c\u69d8\u306b\u904e\u53bb50\u5e74\u9593\u306bCOVID-19\u3084HIV/AIDS\u3088\u308a\u3082\u305a\u3063\u3068\u6df1\u523b\u306a\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u3082\u8d77\u3053\u308a\u3048\u305f\u306e\u3067\u3059\u3002</p><p>\u305d\u3046\u3059\u308b\u3068\u3001\u91cd\u8981\u306a\u554f\u984c\u306f\u3001\u6b74\u53f2\u7684\u306a\u8a18\u9332\u306f\u30c6\u30ed\u6d3b\u52d5\u3068\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306e\u3069\u3061\u3089\u306e\u30ea\u30b9\u30af\u3092\u3088\u308a\u4f4e\u304f\u898b\u7a4d\u3082\u3063\u3066\u3044\u308b\u304b\uff08\u3059\u306a\u308f\u3061\u3001\u30c6\u30ed\u6d3b\u52d5\u306b\u3088\u308b\u6b7b\u8005\u6570\u306f\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306b\u3088\u308b\u6b7b\u8005\u6570\u3088\u308a\u3082\u3001\u3088\u308a\u88fe\u306e\u91cd\u3044\u5206\u5e03\u306b\u306a\u308b\u304b\u3069\u3046\u304b\uff09\u3068\u3044\u3046\u70b9\u306b\u306a\u308a\u307e\u3059\u3002</p><p>\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306e\u6700\u60aa\u306e\u30b7\u30ca\u30ea\u30aa\u306f\u30c6\u30ed\u6d3b\u52d5\u306e\u6700\u60aa\u306e\u30b7\u30ca\u30ea\u30aa\u3088\u308a\u3082\u6df1\u523b\u3060\u3068\u8003\u3048\u308b\u306e\u304c\u3082\u3063\u3068\u3082\u3089\u3057\u3044\u3088\u3046\u306b\u601d\u308f\u308c\u307e\u3059\u3002COVID-19\u3088\u308a\u3082\u611f\u67d3\u529b\u304c\u9ad8\u304f\u3001\u6b7b\u4ea1\u7387\u304c10-50%\u3001\u307e\u305f\u306f\u305d\u308c\u4ee5\u4e0a\u3067\u3042\u308b\u3088\u3046\u306a\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u304c\u751f\u3058\u308b\u53ef\u80fd\u6027\u3092\u6392\u9664\u3059\u308b\u3082\u306e\u306f\u4f55\u3082\u3042\u308a\u307e\u305b\u3093\u3002\u307e\u305f\u3001\u6b74\u53f2\u7684\u306a\u8a18\u9332\u3092\u307f\u308c\u3070\u3001\u30c6\u30ed\u6d3b\u52d5\u3068\u6bd4\u3079\u3066\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306e\u65b9\u304c\u3088\u308a\u591a\u304f\u306e\u3014\u6df1\u523b\u306a\u4e8b\u614b\u306e\u767a\u751f\u3092\u9593\u4e00\u9aea\u3067\u9003\u308c\u305f\u3015\u30cb\u30a2\u30df\u30b9\u4e8b\u4f8b\u304c\u3042\u308b\u3088\u3046\u306b\u3082\u601d\u3048\u307e\u3059\u3002</p><p>\u305d\u3046\u3067\u3042\u308b\u306a\u3089\u3070\u3001\u30b5\u30f3\u30d7\u30eb\u5185\u306e\u88fe\u306e\u90e8\u5206\u306b\u5f53\u305f\u308b\u51fa\u6765\u4e8b\u3092\u9003\u3057\u3066\u3057\u307e\u3046\u3068\u3044\u3046\u554f\u984c\u306f\u30c6\u30ed\u6d3b\u52d5\u3088\u308a\u3082\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306e\u65b9\u304c\u6df1\u523b\u3067\u3057\u3087\u3046\u3002\u5b9f\u969b\u3001\u30c6\u30ed\u306b\u3088\u3063\u306610\u5104\u4eba\u4ee5\u4e0a\u306e\u4eba\u3073\u3068\u3092\u6bba\u5bb3\u3057\u3088\u3046\u3068\u601d\u3048\u3070\u3001\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u3092\u5f15\u304d\u8d77\u3053\u3059\u306e\u304c\u304a\u305d\u3089\u304f\u4e00\u756a\u6709\u529b\u306a\u65b9\u6cd5\u306b\u306a\u308b\u3067\u3057\u3087\u3046\u3002</p><p>\u30c6\u30ed\u6d3b\u52d5\u306e\u9632\u6b62\u304c\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306e\u9632\u6b62\u3088\u308a\u3082\u6700\u5927\u3067100\u500d\u306e\u8cc7\u91d1\u3092\u53d7\u3051\u53d6\u308b\u4e00\u65b9\u3067\u3001\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306f\u6b74\u53f2\u7684\u306b\u30c6\u30ed\u6d3b\u52d5\u306e10-100\u500d\u306e\u6b7b\u3092\u3082\u305f\u3089\u3057\u3066\u304d\u305f\u3068\u3057\u305f\u3089\u3001\u73fe\u5728\u306e\u8cc7\u6e90\u306e\u914d\u5206\u3092\u3088\u308a\u30d0\u30e9\u30f3\u30b9\u306e\u53d6\u308c\u305f\u3082\u306e\u306b\u3059\u308b\u305f\u3081\u306b\u3001\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u3092\u512a\u5148\u3059\u308b\u65b9\u5411\u3067\u5927\u898f\u6a21\u306a\u4fee\u6b63\u304c\u5fc5\u8981\u306b\u306a\u308b\u3067\u3057\u3087\u3046\u3002</p><p>\u4ee5\u4e0a\u306e\u5206\u6790\u306f\u3001\u91cd\u8981\u3067\u3042\u308a\u306a\u304c\u3089\u6bd4\u8f03\u7684\u8a08\u6e2c\u306e\u5bb9\u6613\u306a\u5c3a\u5ea6\u3067\u3042\u308b\u305f\u3081\u3001\u6b7b\u8005\u6570\u306e\u307f\u3092\u8003\u616e\u306b\u5165\u308c\u3066\u3044\u307e\u3057\u305f\u3002\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u3068\u30c6\u30ed\u306b\u3088\u308b\u6b7b\u8005\u306f\u91cd\u8981\u306a\u9593\u63a5\u7684\u30b3\u30b9\u30c8\u3082\u751f\u307f\u3060\u3057\u307e\u3059\u3057\u3001\u5b8c\u5168\u306a\u6bd4\u8f03\u3092\u884c\u304a\u3046\u3068\u601d\u3048\u3070\u3001\u305d\u308c\u305e\u308c\u306e\u3014\u9593\u63a5\u7684\u30b3\u30b9\u30c8\u306e\u3015\u898f\u6a21\u3092\u6bd4\u8f03\u3059\u308b\u3053\u3068\u306b\u306a\u308b\u3067\u3057\u3087\u3046\u3002</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndld8ohip7um\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdld8ohip7um\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u300c\u30a8\u30d4\u30c7\u30df\u30c3\u30af\u306e\u958b\u59cb\u4ee5\u6765\u30014,010\u4e07[3,360\u4e07-4,860\u4e07]\u4eba\u304cAIDS\u95a2\u9023\u306e\u75be\u75c5\u306b\u3088\u308a\u6b7b\u4ea1\u3057\u307e\u3057\u305f\u3002\u300d<br><a href=\"https://www.unaids.org/en/resources/fact-sheet\"><u>Global HIV &amp; AIDS statistics \u2014 Fact sheet</u></a> UNAIDS, 2022.&nbsp;<a href=\"https://web.archive.org/web/20220811142820/https://www.unaids.org/en/resources/fact-sheet\">Archived link</a>, accessed 11 August 2022.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4zb2u9abduk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4zb2u9abduk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Open Philanthropy \u306f\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306e\u8003\u3048\u65b9\u306b\u57fa\u3065\u304d\u8a2d\u7acb\u3055\u308c\u305f\u8ca1\u56e3\u3067\u3059\u3002Open Philanthropy \u304c\u6700\u521d\u306b\u30b8\u30e7\u30f3\u30ba\u30fb\u30db\u30d7\u30ad\u30f3\u30ba\u5927\u5b66\u5065\u5eb7\u5b89\u5168\u4fdd\u969c\u30bb\u30f3\u30bf\u30fc\uff08CHS\uff09\u306b\u8cc7\u91d1\u63d0\u4f9b\u3092\u3057\u305f\u306e\u306f<a href=\"https://web.archive.org/web/20220728171005/https://www.openphilanthropy.org/grants/johns-hopkins-center-for-health-security-emerging-leaders-in-biosecurity-initiative/\"><u>2016\u5e74\u3067\u3057\u305f</u></a>\u3002\u305d\u306e\u5f8c\u3082<a href=\"https://web.archive.org/web/20220728171246/https://www.openphilanthropy.org/grants/johns-hopkins-center-for-health-security-biosecurity-global-health-security-and-global-catastrophic-risks-2017/\"><u>2017\u5e74\u306b\u306f1600\u4e07\u30c9\u30eb\u306e</u></a>\u3001<a href=\"https://web.archive.org/web/20220728171354/https://www.openphilanthropy.org/grants/johns-hopkins-center-for-health-security-biosecurity-global-health-security-and-global-catastrophic-risks-2019/\"><u>2019\u5e74\u306b\u306f1950\u4e07\u30c9\u30eb\u306e</u></a>\u8ffd\u52a0\u306e\u8cc7\u91d1\u63d0\u4f9b\u3092\u884c\u3044\u3001\u305d\u306e\u4ed6\u306b\u3082\u5e7e\u5ea6\u304b\u5927\u898f\u6a21\u306a\u52a9\u6210\u91d1\u3092\u63d0\u4f9b\u3057\u3066\u304d\u307e\u3057\u305f\u3002</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnj3c26vw0o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefj3c26vw0o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>2022\u5e747\u67087\u65e5\u306e\u6642\u70b9\u3067\u3001\u30dc\u30e9\u30f3\u30c6\u30a3\u30a2\u306e\u6570\u306f38,659\u4eba\u3067\u3059\u3002<a href=\"https://web.archive.org/web/20220707165739/https://www.1daysooner.org/\">1Day Sooner</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngstf8gmntfl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgstf8gmntfl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;[\u8a33\u6ce8] \u65b0\u7d04\u8056\u66f8\u30c6\u30e2\u30c6\u30d8\u306e\u7b2c\u4e00\u306e\u624b\u7d19\u7b2c5\u7ae0\u7b2c8\u7bc0\u306b\u7531\u6765\u3059\u308b\u3068\u3055\u308c\u308b\u8afa\u3002</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzr0ggtg4o6g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzr0ggtg4o6g\">^</a></strong></sup></span><div class=\"footnote-content\"><p>COVID-19\u4ee5\u524d\u3001\u4e00\u65e51.90\u30c9\u30eb\u4ee5\u4e0b\u3067\u751f\u6d3b\u3059\u308b\u4eba\u3073\u3068\u306e\u6570\u306f2017\u5e74\u306b\u306f6\u51048900\u4e07\u4eba\u306b\u6e1b\u3063\u3066\u3044\u307e\u3057\u305f\u3002\u3057\u304b\u3057\u73fe\u5728\u3001\u6975\u5ea6\u306e\u8ca7\u56f0\u306b\u3042\u308b\u4eba\u306e\u6570\u306f\u30011998\u5e74\u4ee5\u6765\u521d\u3081\u3066\u306e\u5897\u52a0\u306b\u5411\u304b\u3046\u3068\u63a8\u5b9a\u3055\u308c\u3066\u304a\u308a\u3001\u73fe\u5728\u4e00\u65e51.90\u30c9\u30eb\u4ee5\u4e0b\u3067\u751f\u6d3b\u3057\u3066\u3044\u308b\u3068\u63a8\u5b9a\u3055\u308c\u3066\u3044\u308b\u4eba\u3073\u3068\u306e\u6570\u306f7\u51043100\u4e07\u4eba\u3067\u3059\u3002</p><p><a href=\"https://unstats.un.org/sdgs/report/2021/goal-01/\">UN SDG 1 - End poverty in all its forms</a>. UN Statistics, 2022.&nbsp;Archived link, accessed 26 July 2022.</p><p>\u3053\u3046\u3057\u305f\u63a8\u5b9a\u306f\u3001\u540c\u984d\u306e\u304a\u91d1\u3067\u3082\u8ca7\u3057\u3044\u56fd\u3067\u306f\u3088\u308a\u591a\u304f\u306e\u3082\u306e\u3092\u8cb7\u3048\u308b\u3068\u3044\u3046\u4e8b\u5b9f\uff08\u8cfc\u8cb7\u529b\u5e73\u4fa1\uff09\u3092\u8e0f\u307e\u3048\u3066\u4fee\u6b63\u6e08\u307f\u3067\u3059\u3002\u305d\u306e\u63a8\u5b9a\u4f5c\u696d\u306f\u8907\u96d1\u3055\u3092\u6975\u3081\u307e\u3059\u304c\u3001\u6240\u5f97\u306b\u95a2\u3057\u3066\u6700\u4f4e\u751f\u6d3b\u6c34\u6e96\u3059\u308c\u3059\u308c\u3067\u751f\u6d3b\u3057\u3066\u3044\u308b\u4eba\u3073\u3068\u304c\u4f55\u5104\u4eba\u3082\u3044\u308b\u3053\u3068\u306f\u660e\u3089\u304b\u3067\u3059\u3002\u3002\u3082\u3063\u3068\u8a73\u3057\u304f\u77e5\u308a\u305f\u3044\u5834\u5408\u306f\u3001 \u201c<a href=\"https://web.archive.org/web/20220728171308/https://80000hours.org/2017/04/how-accurately-does-anyone-know-the-global-distribution-of-income/\"><u>how accurately does anyone know the global distribution of income?</u></a>\u201d&nbsp;\u3092\u8aad\u3093\u3067\u307f\u3066\u304f\u3060\u3055\u3044\u3002</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqaii6goa04\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqaii6goa04\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u7c73\u56fd\u306e1\u4eba\u5f53\u305f\u308a\u306e\u8ca7\u56f0\u30e9\u30a4\u30f3\u306f\u5e74\u53ce13,590\u30c9\u30eb\u3067\u3059\u3002</p><p>13,590 / 365 = \u4e00\u65e5\u5f53\u305f\u308a37.23\u30c9\u30eb</p><p>\u3053\u308c\u306f\u8cfc\u8cb7\u529b\u5e73\u4fa1\u3092\u8003\u616e\u306b\u5165\u308c\u3066\u3082\u3001\u56fd\u969b\u7684\u306a\u8ca7\u56f0\u30e9\u30a4\u30f3\u3067\u3042\u308b1.90\u30c9\u30eb\u306e20\u500d\u3067\u3059\u3002</p><p><a href=\"https://web.archive.org/web/20220728171531/https://www.census.gov/data/tables/2022/demo/acs-2019.html\"><u>2019\u5e74\u306e\u7c73\u56fd\u52e2\u8abf\u67fb</u></a>\u306b\u3088\u308b\u3068\u3001\u5927\u5352\u307e\u305f\u306f\u305d\u308c\u4ee5\u4e0a\u306e\u5b66\u6b74\u3092\u3082\u306425\u304b\u308965\u6b73\u306e\u30d5\u30eb\u30bf\u30a4\u30e0\u52b4\u50cd\u8005\u306e\u5e74\u53ce\u306e\u4e2d\u592e\u5024\u306f\u5e74\u959374,000\u30c9\u30eb\u3067\u3057\u305f\u3002</p><p>74,000\u30c9\u30eb / 365 = \u4e00\u65e5\u5f53\u305f\u308a202.7\u30c9\u30eb<br>202\u30c9\u30eb / 1.9 = 107\u500d</p><p><a href=\"https://smartasset.com/taxes/income-taxes#PgAT7oh2Ae\">SmartAsset</a> \u306b\u3088\u308b\u3068\u300174,000\u30c9\u30eb\u306e\u53ce\u5165\u304c\u3042\u308a\u3001\u30cb\u30e5\u30fc\u30e8\u30fc\u30af\u306b\u4f4f\u3080\u5358\u8eab\u4e16\u5e2f\u306f53,000\u30c9\u30eb\u306e\u624b\u53d6\u308a\u53ce\u5165\u3092\u53d7\u3051\u53d6\u308a\u307e\u3059\u3002<br>Giving What We Can \u306e<a href=\"https://web.archive.org/web/20220728165745/https://howrichami.givingwhatwecan.org/how-rich-am-i?income=53000&amp;countryCode=USA&amp;household%5Badults%5D=1&amp;household%5Bchildren%5D=0\"><u>\u8a08\u7b97\u6a5f</u></a>\u306b\u3088\u308b\u3068\u300153,000\u30c9\u30eb\u306e\u624b\u53d6\u308a\u53ce\u5165\u304c\u3042\u308c\u3070\u3001\u3042\u306a\u305f\u306f\u4e16\u754c\u306e\u4e0a\u4f4d1.3%\u306b\u5165\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqy3oixrrmf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqy3oixrrmf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u82f1\u56fd\u56fd\u7acb\u533b\u7642\u6280\u8853\u8a55\u4fa1\u6a5f\u69cb\uff08UK\u2019s National Institute for Health and Care Excellence\uff09\u306f\u3001\u4ecb\u5165\u7b56\u304c\u4fe1\u983c\u306e\u304a\u3051\u308b\u3082\u306e\u3067\u3042\u308b\u5834\u5408\u306b\u306f\u3001\u8cea\u8abf\u6574\u751f\u5b58\u5e74\u6570\uff08<a href=\"https://en.wikipedia.org/wiki/Quality-adjusted_life_year\">quality-adjusted life year</a>, QALY\uff091\u5e74\u6bce\u306b30,000\u30dd\u30f3\u30c9\u307e\u3067\u51fa\u8cbb\u3059\u308b\u3053\u3068\u3092\u63a8\u5968\u3057\u3066\u3044\u307e\u3059\u3002</p><p>\u300cQALY\u304c\u4e00\u5e74\u5ef6\u3073\u308b\u6bce\u306b30,000\u30dd\u30f3\u30c9\u3068\u3044\u3046\u6700\u3082\u59a5\u5f53\u306a\u5897\u5206\u8cbb\u7528\u52b9\u679c\u6bd4\uff08ICER\uff09\u3092\u8d85\u904e\u3059\u308b\u5834\u5408\u306b\u306f\u3001\u8aee\u554f\u6a5f\u95a2\u306f\u3001\u554f\u984c\u306e\u4ecb\u5165\u7b56\u304c\u56fd\u6c11\u5065\u5eb7\u30b5\u30fc\u30d3\u30b9\uff08NHS\uff09\u306e\u30ea\u30bd\u30fc\u30b9\u306e\u52b9\u679c\u7684\u306a\u5229\u7528\u65b9\u6cd5\u3067\u3042\u308b\u3053\u3068\u3092\u652f\u6301\u3059\u308b\u3001\u3088\u308a\u5f37\u3044\u8b70\u8ad6\u3092\u63d0\u793a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3060\u308d\u3046\u300d\uff08&nbsp;<a href=\"https://www.nice.org.uk/process/pmg4/chapter/incorporating-health-economics\">Methods for the development of NICE public health guidance</a>. UK National Institute for Health and Care Excellence, September 2012.&nbsp;<a href=\"https://web.archive.org/web/20220728173611/https://www.nice.org.uk/process/pmg4/chapter/incorporating-health-economics\">Archived link</a>, accessed July 28, 2022.\uff09<br><br>\u30b0\u30ed\u30fc\u30d0\u30eb\u30d8\u30eb\u30b9\u306e\u5206\u91ce\u3067\u306f\u3072\u3068\u308a\u306e\u547d\u3092\u6551\u3046\u3053\u3068\u306f30\u5e74\u5206\u306eQALY\u3068\u7b49\u4fa1\u3067\u3042\u308b\u3068\u3088\u304f\u8a00\u308f\u308c\u307e\u3059\u3002\u51fa\u5178:&nbsp;<a href=\"https://web.archive.org/web/20220728171610/https://openknowledge.worldbank.org/bitstream/handle/10986/7039/364010PAPER0Gl101OFFICIAL0USE0ONLY1.pdf?sequence=1\">World Bank (Box 1.1)</a><br><br>\u3053\u308c\u306f\u3064\u307e\u308a\u3001\u3072\u3068\u308a\u306e\u547d\u3092\u6551\u3046\u8cbb\u7528\u306f\u300130 \u00d7 30,000\u30dd\u30f3\u30c9 = 900,000\u30dd\u30f3\u30c9 = 110\u4e07\u30c9\u30eb\u3067\u3042\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002<br><br>\u7c73\u56fd\u3067\u306f\u3001\u8907\u6570\u306e\u30b0\u30ed\u30fc\u30d0\u30eb\u306b\u5c55\u958b\u3059\u308b\u6a5f\u95a2\u304c\u300c<a href=\"https://en.wikipedia.org/wiki/Value_of_life\"><u>\u547d\u306e\u4fa1\u5024</u></a>\u300d\u3092\u7b97\u5b9a\u3057\u3001\u3053\u306e\u6570\u5024\u3092\u7570\u306a\u308b\u51fa\u8cc7\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u9593\u3067\u512a\u5148\u9806\u4f4d\u3092\u3064\u3051\u308b\u305f\u3081\u306b\u5229\u7528\u3057\u3066\u3044\u307e\u3059\u3002\u30a2\u30e1\u30ea\u30ab\u5408\u8846\u56fd\u9023\u90a6\u7dca\u6025\u4e8b\u614b\u7ba1\u7406\u5e81\uff08Federal Emergency Management Agency\uff09\u306f2020\u5e74\u3001\u547d\u306e\u4fa1\u5024\u3092<a href=\"https://web.archive.org/web/20220516225829/https://www.fema.gov/sites/default/files/2020-08/fema_bca_toolkit_release-notes-july-2020.pdf\"><u>750\u4e07\u30c9\u30eb</u></a>\u3068\u898b\u7a4d\u3082\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u306e\u898b\u7a4d\u3082\u308a\u306f\u6587\u8108\u306b\u3088\u308a\u5897\u6e1b\u3057\u3066\u304d\u307e\u3057\u305f\u3002\u4f8b\u3048\u3070\u3001\u30a2\u30e1\u30ea\u30ab\u5408\u8846\u56fd\u904b\u8f38\u7701\u306f2014\u5e74\u306b\u3001\u547d\u306e\u4fa1\u5024\u306f<a href=\"https://web.archive.org/web/20220728173630/https://www.transportation.gov/sites/dot.gov/files/docs/VSL_Guidance_2014.pdf\"><u>520\u4e07\u30c9\u30eb\u304b\u30891300\u4e07\u30c9\u30eb\u306e\u3042\u3044\u3060</u></a>\u3060\u3068\u63a8\u5b9a\u3057\u307e\u3057\u305f\u3002</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpps7e5ymyr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpps7e5ymyr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u3072\u3068\u308a\u306e\u547d\u3092\u6551\u3046\u306e\u306b\u5fc5\u8981\u306a\u8cbb\u7528\u306b\u95a2\u3059\u308b GiveWell \u306e\u898b\u7a4d\u3082\u308a\u306f\uff08GiveWell \u306e\u7814\u7a76\u3084\u3001\u3069\u306e\u3088\u3046\u306a\u624b\u6bb5\u304c\u5229\u7528\u53ef\u80fd\u3067\u3042\u308b\u304b\u306b\u3088\u3063\u3066\uff09\u6642\u9593\u3092\u901a\u3058\u3066\u5909\u5316\u3057\u307e\u3059\u304c\u3001\u901a\u5e38\u306f2,500\u30c9\u30eb\u304b\u30897,500\u30c9\u30eb\u306e\u9593\u306b\u53ce\u307e\u308a\u307e\u3059\u30022021\u5e74\u306b\u306fGiveWell\u306f\u3001\u6bba\u866b\u5264\u51e6\u7406\u3055\u308c\u305f\u868a\u5e33\u3092\u914d\u5e03\u3059\u308b\u306e\u306b\u4f7f\u308f\u308c\u308b\u306a\u3089\u3001\u671f\u5f85\u5024\u3068\u3057\u30665,500\u30c9\u30eb\u3067\u3072\u3068\u308a\u306e\u547d\u3092\u6551\u3046\u3060\u308d\u3046\u3068\u898b\u7a4d\u3082\u308a\u307e\u3057\u305f\u3002</p><p>GiveWell\u306e\u6700\u65b0\u306e\u63a8\u5b9a\u5024\u3092\u77e5\u308a\u305f\u3051\u308c\u3070\u3001\u8cbb\u7528\u5bfe\u52b9\u679c\u306e\u5b8c\u5168\u306a\u5206\u6790\u3092\u8f09\u305b\u305fGiveWell\u306e\u5831\u544a\u66f8\u3092\u898b\u3066\u304f\u3060\u3055\u3044\uff1a<a href=\"https://www.givewell.org/impact-estimates\">How We Produce Impact Estimates</a>, GiveWell, July 2022.&nbsp;<a href=\"https://web.archive.org/web/20220728173731/https://www.givewell.org/impact-estimates\">Archived link</a>, accessed 28 July 2022.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5gygdbxwk9y\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5gygdbxwk9y\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u300c110,000\u4ee5\u4e0a\u306e\u5bc4\u4ed8\u8005\u304c\u305d\u306e\u5bc4\u4ed8\u5148\u3092\u6c7a\u3081\u308b\u306e\u306b GiveWell \u3092\u983c\u3063\u3066\u304d\u307e\u3057\u305f\u3002\u5f0a\u56e3\u4f53\u306e\u63a8\u85a6\u3059\u308b\u7d44\u7e54\u3078\u306e\u5bc4\u4ed8\u306e\u7dcf\u984d\u306f10\u5104\u30c9\u30eb\u4ee5\u4e0a\u306b\u9054\u3057\u307e\u3059\u3002\u3053\u3046\u3057\u305f\u5bc4\u4ed8\u306f150,000\u4eba\u4ee5\u4e0a\u3082\u306e\u547d\u3092\u6551\u3044\u3001\u4e16\u754c\u4e2d\u306e\u8ca7\u56f0\u5c64\u306b1\u51047500\u4e07\u30c9\u30eb\u4ee5\u4e0a\u306e\u73fe\u91d1\u652f\u63f4\u3092\u884c\u3063\u3066\u304d\u307e\u3057\u305f\u3002\u300d</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8iq8e9godmv\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8iq8e9godmv\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u300cWave \u304c\u30bb\u30cd\u30ac\u30eb\u3067\u4e8b\u696d\u3092\u59cb\u3081\u305f\u6642\u70b9\u3067\u3001\u65e2\u5b58\u306e\u643a\u5e2f\u9001\u91d1\u30b7\u30b9\u30c6\u30e0\u306e\u4e2d\u3067\u6700\u5927\u306e\u3082\u306e\u3092\u5229\u7528\u3059\u308b\u5834\u5408\u3001Wave\u3092\u7d4c\u7531\u3057\u306a\u3044\u9001\u91d1\u8cbb\u7528\u306f\u3014Wave \u3092\u5229\u7528\u3057\u305f\u5834\u5408\u3068\u6bd4\u3079\u3066\u3015\u5e73\u5747\u30673-5\u500d\u304b\u304b\u308a\u307e\u3057\u305f\u3002\u6708\u6bce\u306e\u30a2\u30af\u30c6\u30a3\u30d6\u30fb\u30e6\u30fc\u30b6\u30fc\u6570\u3067\u3042\u308b\u4f55\u767e\u4e07\u3068\u3044\u3046\u6570\u3092\u639b\u3051\u5408\u308f\u305b\u308b\u3068\u3001\u6bce\u5e742\u5104\u4e07\u30c9\u30eb\u4ee5\u4e0a\u306e\u7bc0\u7d04\u306b\u306a\u308a\u307e\u3059\u304c....\u3014\u3053\u308c\u306f\u3015\u30bb\u30cd\u30ac\u30eb\u306eGPD\u306e\u7d041%\u3067\u3059\u3002\u300d<br><a href=\"https://www.wave.com/en/blog/world/\">Working at Wave is an extremely effective way to improve the world</a>. Ben Kuhn, July 8 2021.&nbsp;<a href=\"https://web.archive.org/web/20220726160441/https://www.wave.com/en/blog/world/\">Archived link</a>, accessed 26 July 2022.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwtblnk8b5r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwtblnk8b5r\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u4f1a\u8a71\uff1a\u300c\u6700\u3082\u6210\u529f\u3057\u305f\u30a8\u30f3\u30c9\u30fb\u30c8\u30a5\u30fb\u30a8\u30f3\u30c9\u306e\u8a13\u7df4\u3092\u7d4c\u305f\u5f53\u793e\u306eMeena\u30e2\u30c7\u30eb\u306f....SSA[\u5206\u5225\u5ea6\u3068\u5177\u4f53\u5ea6\u306e\u5e73\u5747\u5024\uff08Sensibleness and Specificity Average\uff09]\u30b9\u30b3\u30a2\u306772%\u3092\u9054\u6210\u3057\u307e\u3057\u305f\u3002....\u79c1\u305f\u3061\u306e\u9054\u6210\u3057\u305f72%\u306eSSA\u30b9\u30b3\u30a2\u306f\u3001\u5e73\u5747\u7684\u306a\u4eba\u304c\u9054\u6210\u3059\u308b86%\u306eSSA\u3068\u304b\u3089\u305d\u308c\u307b\u3069\u9060\u304f\u306a\u3044\u3068\u3053\u308d\u307e\u3067\u6765\u3066\u3044\u307e\u3059\u3002\u300d &nbsp;Towards a Conversational Agent that Can Chat About ... Anything. Adiwardana et. al., Google, 28 January 2020. Archived link, accessed 28 July 2022.&nbsp;</p><p>\u6570\u5b66\uff1a\u6dfb\u4ed8\u3057\u305f\u8a18\u4e8b\u306e\u30b0\u30e9\u30d5\u306f\u3001Google \u306e Minerva \u304c\u300c\u5927\u5b66\u53d7\u9a13\u30ec\u30d9\u30eb\u306e\u554f\u984c\u300d\u306e50%\u4ee5\u4e0a\u306b\u6b63\u3057\u304f\u7b54\u3048\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u4ed6\u306e\u6700\u65b0\u5f0f\u30e2\u30c7\u30eb\u306f10%\u4ee5\u4e0b\u306e\u6b63\u7b54\u7387\u3067\u3057\u305f\u3002</p><p>Minerva: Solving Quantitative Reasoning Problems with Language Models. Dyer et. al, Google, 30 June 2022. Archived link, accessed 28 July 2022.&nbsp;</p><p>\u30b8\u30e7\u30fc\u30af\uff1aGoogle\u306eAI\u3001PaLM\u306f\u3001\u30a4\u30f3\u30bf\u30fc\u30cd\u30c3\u30c8\u4e0a\u306e\u3069\u3053\u306b\u3082\u898b\u3064\u304b\u3089\u306a\u3044\u30b8\u30e7\u30fc\u30af\u3082\u542b\u3081\u3066\u3001\u4e00\u5ea6\u3082\u898b\u305f\u3053\u3068\u306e\u306a\u3044\u30b8\u30e7\u30fc\u30af\u306b\u8aac\u660e\u3092\u4e0e\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001</p><p>\u300c\u30b8\u30e7\u30fc\u30af\uff1aGoogle\u306f\u81ea\u793e\u306eTPU\u30c1\u30fc\u30e0\u306b\u53e3\u306e\u9054\u8005\u306a\u30af\u30b8\u30e9\u3092\u96c7\u3063\u305f\u3089\u3057\u3044\u3088\u3002\u305d\u306e\u30af\u30b8\u30e9\u3068\u304d\u305f\u3089\u3001\u3075\u305f\u3064\u306e\u3008\u30dd\u30c3\u30c9\uff08pods\uff09\u3009\u306e\u3042\u3044\u3060\u3067\u3069\u3046\u3084\u3063\u305f\u3089\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3\u304c\u3067\u304d\u308b\u304b\u3092\u6559\u3048\u305f\u3093\u3060\u3063\u3066\u3055\uff01</p><p>\u8aac\u660e\uff1aTPU\u306fGoogle\u304c\u6df1\u5c64\u5b66\u7fd2\u306b\u4f7f\u7528\u3057\u3066\u3044\u308b\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fb\u30c1\u30c3\u30d7\u306e\u4e00\u7a2e\u3067\u3059\u3002\u300c\u30dd\u30c3\u30c9\uff08pod\uff09\u300d\u3068\u3044\u3046\u8a00\u8449\u306f\u3001TPU\u306e\u4e00\u7a2e\u3092\u6307\u3059\u3068\u540c\u6642\u306b\u3001\u30af\u30b8\u30e9\u306e\u7fa4\u308c\u306e\u3053\u3068\u3082\u6307\u3057\u307e\u3059\u3002\u3053\u306e\u30b8\u30e7\u30fc\u30af\u306e\u30dd\u30a4\u30f3\u30c8\u306f\u3001\u30af\u30b8\u30e9\u306f\u5b9f\u969b\u306b\u306f\u3075\u305f\u3064\u306e\u30af\u30b8\u30e9\u306e\u96c6\u56e3\u306e\u3042\u3044\u3060\u3067\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u3068\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3060\u3051\u3067\u3042\u308b\u306b\u3082\u95a2\u308f\u3089\u305a\u3001\u30b8\u30e7\u30fc\u30af\u306e\u8a71\u3057\u624b\u304c\u3001\u30af\u30b8\u30e9\u304c\u4e8c\u7a2e\u985e\u306eTPU\u306e\u3042\u3044\u3060\u3067\u3082\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u3068\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u304b\u306e\u3088\u3046\u306a\u8a71\u3057\u3076\u308a\u3092\u3057\u3066\u3044\u308b\u70b9\u306b\u3042\u308a\u307e\u3059\u3002\u300d</p><p><a href=\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\"><u>Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance</u></a>. Narang et. al., Google, 4 April 2022.&nbsp;<a href=\"https://web.archive.org/web/20220728174127/https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\"><u>Archived link</u></a>.</p><p>\u7d75\u753b\uff1aOpenAI\u306eDAll-E 2\u304c\u63cf\u3044\u305f\u7d75\u306e\u4f8b\u306f<a href=\"https://web.archive.org/web/20220728174215/https://openai.com/dall-e-2/\"><u>\u3053\u3061\u3089</u></a>\u3067\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p><p>\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\uff1aCodeGen\u306f\u3001Salesforce\u306eAI\u30c4\u30fc\u30eb\u3067\u3042\u308a\u3001\u4eba\u9593\u306e\u6307\u793a\u3092\u30b3\u30fc\u30c9\u306b\u5909\u63db\u3059\u308b\u3082\u306e\u3067\u3059\u304c\u3001CodeGen\u306b\u95a2\u3059\u308b Salesforce\u306e\u7814\u7a76\u8ad6\u6587\u201c<a href=\"https://web.archive.org/web/20220404070940/http://arxiv.org/pdf/2203.13474.pdf\"><u>A Conversational Paradigm for Program Synthesis</u></a>\u201d\u306e\u7b2c3.1\u7bc0\u3067\u306f\u3001CodeGen \u304cHumanEval\u30b9\u30b3\u30a2\u306775\uff05\u3092\u9054\u6210\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u6982\u7565\u7684\u306b\u5831\u544a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u3053\u3068\u304c\u610f\u5473\u3057\u3066\u3044\u308b\u306e\u306f\u3001CodeGen\u306f\u3001\u4eba\u9593\u306e\u81ea\u7136\u306a\u8a00\u8a9e\u3067\u8a18\u8ff0\u3055\u308c\u305f<a href=\"https://web.archive.org/web/20211011051701/https://paperswithcode.com/dataset/humaneval\"><u>HumanEval\u306e\u30bb\u30c3\u30c8</u></a>\u5185\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8ab2\u984c\u3067\u3042\u308c\u3070\u3001\u305d\u306e\u3046\u306175%\u3092\u89e3\u6c7a\u3067\u304d\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxly1kl8lu5a\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxly1kl8lu5a\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u3042\u308b\u30c8\u30d4\u30c3\u30af\u306b\u53d6\u308a\u7d44\u3093\u3067\u3044\u308b\u7814\u7a76\u8005\u306e\u6570\u3092\u63a8\u5b9a\u3059\u308b\u306e\u306f\u56f0\u96e3\u3067\u3059\u3002\u306a\u305c\u306a\u3089\u3001\u305d\u306e\u30c8\u30d4\u30c3\u30af\u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u304c\u305d\u3082\u305d\u3082\u96e3\u3057\u304f\u3001\u591a\u304f\u306e\u7814\u7a76\u8005\u304c\u8907\u6570\u306e\u30c8\u30d4\u30c3\u30af\u306b\u53d6\u308a\u7d44\u3093\u3067\u304a\u308a\u3001\u304b\u3064\u300c\u7814\u7a76\u8005\u3067\u3042\u308b\u300d\u3068\u8a00\u3048\u308b\u305f\u3081\u306e\u57fa\u6e96\u3092\u7279\u5b9a\u3059\u308b\u306e\u304c\u96e3\u3057\u3044\u304b\u3089\u3067\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u3053\u3053\u3067\u306e\u6570\u306f\u305d\u306e1/3\u304b\u30893\u500d\u7a0b\u5ea6\u306e\u5e45\u3092\u6301\u3063\u305f\u898b\u7a4d\u3082\u308a\u3060\u3068\u7406\u89e3\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3057\u3001\u3053\u306e\u554f\u984c\u306e\u89e3\u91c8\u306b\u5fdc\u3058\u3066\u4e00\u6841\u9055\u3046\u3053\u3068\u3082\u3042\u308a\u3048\u307e\u3059\u3002</p><p>2020\u5e74\u306b\u306f\u300187,000\u4eba\u304carXiv\u4e0a\u306bAI\u95a2\u9023\u306e\u7814\u7a76\u3092\u516c\u958b\u3057\u307e\u3057\u305f\u3002Element AI \u306e 2020 Global AI Talent Report \u306f\u3001\u30bd\u30fc\u30b7\u30e3\u30eb\u30fb\u30e1\u30c7\u30a3\u30a2\u4e0a\u3067AI\u306e\u7814\u7a76\u3084\u8a2d\u8a08\u306b\u53d6\u308a\u7d44\u3093\u3067\u3044\u308b\u3068\u8868\u793a\u3057\u3066\u3044\u308b\u4eba\u306e\u6570\u306f155,000\u4eba\u306b\u306e\u307c\u308b\u305f\u3081\u3001\u4e16\u754c\u4e2d\u3067AI\u306e\u958b\u767a\u306b\u53d6\u308a\u7d44\u3093\u3067\u3044\u308b\u4eba\u306e\u6570\u306f\u3001arXiv\u306b\u7814\u7a76\u3092\u516c\u958b\u3057\u305f\u4eba\u6570\u3088\u308a\u3082\u591a\u3044\u3068\u63a8\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\u3057\u304b\u3057AI\u306e\u8a2d\u8a08\u306b\u53d6\u308a\u7d44\u3093\u3067\u3044\u308b\u4eba\u306e\u4e00\u90e8\u306fAI\u306b\u65b0\u3057\u3044\u9032\u5c55\u3092\u3082\u305f\u3089\u3059\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\u306b\u53d6\u308a\u7d44\u3093\u3067\u3044\u308b\u308f\u3051\u3067\u306f\u306a\u3044\u3068\u4e88\u60f3\u3055\u308c\u307e\u3059\u3002\u79c1\u305f\u3061\u306e\u63a8\u5b9a\u3067\u306f\u3001\u3088\u308a\u5c11\u306a\u3044\u898b\u7a4d\u3082\u308a\u3068\u3057\u306687,000\u4eba\u3092\u512a\u5148\u3057\u3001\u304b\u3064\u305d\u306e\u6570\u3092\u5927\u96d1\u628a\u306b\u534a\u5206\u306b\u5272\u3063\u3066\u300140,000\u4eba\u3068\u3044\u3046\u898b\u7a4d\u3082\u308a\u3092\u51fa\u3057\u3066\u3044\u307e\u3059\u3002</p><p>2021\u5e74\u3001&nbsp;<a href=\"https://web.archive.org/web/20220728174413/https://forum.effectivealtruism.org/posts/8ErtxW7FRPGMtDqJy/the-academic-contribution-to-ai-safety-seems-large\">Gavin Leech</a> \u306f270\u4eba\u304b\u3089830\u4eba\u5206\u306e\u30d5\u30eb\u30bf\u30a4\u30e0\u52e4\u52d9\u306b\u76f8\u5f53\u3059\u308b\u4eba\u3073\u3068\u304cAI\u306e\u5b89\u5168\u6027\u306e\u554f\u984c\u306b\u53d6\u308a\u7d44\u3093\u3067\u3044\u308b\u3068\u898b\u7a4d\u3082\u3063\u3066\u3044\u307e\u3059\u3002\u3057\u304b\u3057\u3053\u306e\u63a8\u5b9a\u7bc4\u56f2\u306e\u4e0a\u9650\u306f\u3001\u4f55\u304cAI\u30a2\u30e9\u30a4\u30e1\u30f3\u30c8\u7814\u7a76\u3068\u307f\u306a\u3055\u308c\u308b\u306e\u304b\u306b\u95a2\u3059\u308b\u5e83\u3059\u304e\u308b\u7406\u89e3\u306b\u57fa\u3065\u3044\u3066\u304a\u308a\u3001\u304b\u3064\u3053\u306e\u5408\u8a08\u306e\u5927\u90e8\u5206\u306f\u3001\u81ea\u5206\u306e\u6642\u9593\u306e\u307b\u3093\u306e\u308f\u305a\u304b\u306a\u6642\u9593\u3057\u304bAI\u306e\u5b89\u5168\u6027\u306e\u554f\u984c\u306e\u7814\u7a76\u306b\u8cbb\u3084\u3057\u3066\u3044\u306a\u3044\u591a\u6570\u306e\u7814\u7a76\u8005\u306e\u6642\u9593\u3092\u52a0\u7b97\u3059\u308b\u3053\u3068\u3067\u51fa\u3066\u304d\u305f\u3082\u306e\u3067\u3059\u3002\u3057\u304b\u3057\u79c1\u305f\u3061\u306e\u76ee\u6a19\u306f\u3001AI\u306e\u5b89\u5168\u6027\u306e\u554f\u984c\u306b<u>\u7126\u70b9\u3092\u5f53\u3066\u3066\u53d6\u308a\u7d44\u3093\u3067\u3044\u308b</u>\u7814\u7a76\u8005\u306e\u6570\u3092\u6570\u5024\u5316\u3059\u308b\u3053\u3068\u3067\u3057\u305f\u3002</p><p><a href=\"https://web.archive.org/web/20220728170925/https://aiwatch.issarice.com/\">AI Watch</a>&nbsp;\u306fAI Safety\u306e\u7814\u7a76\u8005\u306e\u982d\u6570\u3092\u7279\u5b9a\u3057\u3088\u3046\u3068\u8a66\u307f\u3001AI Safety\u306b\u53d6\u308a\u7d44\u3093\u3067\u304d\u305f\u7279\u7b46\u3059\u3079\u304d\u7814\u7a76\u8005\u3092160\u540d\u7279\u5b9a\u3057\u307e\u3057\u305f\u3002\u3053\u3053\u306b\u306f\u3001\u904e\u53bb\u4e00\u5e74\u9593\u306e\u9593\u306b AI Safety \u306b\u95a2\u3059\u308b\u7814\u7a76\u3092\u767a\u8868\u3057\u3066\u3044\u306a\u3044\u4eba\u3073\u3068\u3082\u591a\u304f\u542b\u307e\u308c\u307e\u3059\u304c\u3001 \u4e0a\u8a18\u306e87,000\u4eba\u3068\u3044\u3046\u898b\u7a4d\u3082\u308a\u306b\u95a2\u3057\u3066\u3044\u3048\u3070\u3001\u305d\u306e\u3059\u3079\u3066\u304c\u904e\u53bb\u4e00\u5e74\u9593\u306b\u7814\u7a76\u3092\u767a\u8868\u3057\u3066\u3044\u307e\u3059\u3002\u4ed6\u65b9\u3067\u3001\u300c\u7279\u7b46\u3059\u3079\u304d\uff08notable\uff09\u300d\u7814\u7a76\u8005\u3068\u307f\u306a\u3055\u308c\u308b\u305f\u3081\u306e\u57fa\u6e96\u306f\u3001 arXiv\u306b\u7814\u7a76\u3092\u516c\u958b\u3059\u308b\u3088\u308a\u3082\u9ad8\u304f\u306a\u308b\u3067\u3057\u3087\u3046\u3002&nbsp;</p><p>\u79c1\u305f\u3061\u306e\u6700\u7d42\u7684\u306a\u898b\u7a4d\u3082\u308a\u306f\u3001 AI Safety\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u53d6\u308a\u7d44\u3093\u3067\u3044\u308b\u7814\u7a76\u8005\u306f300\u4eba\u3067\u3042\u308b\u3068\u3044\u3046\u3082\u306e\u3067\u3059\u3002</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz15l4lns2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz15l4lns2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>2018\u5e74\u3001\u7c73\u56fd\u3067\u306f\u300195\u51046\u5343\u4e07\u306e\u755c\u7523\u52d5\u7269\u304c\u98df\u8089\u7528\u306b\u5c60\u6bba\u3055\u308c\u307e\u3057\u305f\u3002\u305d\u308c\u4ee5\u964d\u3001\u3053\u306e\u6570\u306f\u5897\u3048\u3066\u3044\u308b\u78ba\u7387\u304c\u9ad8\u3044\u3067\u3059\u3002\u5c60\u6bba\u3055\u308c\u305f\u755c\u7523\u52d5\u7269\u306e\u5185\u8a33\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002\u9d8f91\u51046000\u4e07\u7fbd\u3001\u4e03\u9762\u9ce52\u51043700\u4e07\u7fbd\u3001\u8c5a1\u51042500\u4e07\u982d\u3001\u725b3400\u4e07\u982d\u3001\u7f8a200\u4e07\u982d\u3002\u51fa\u5178\uff1avisualization at&nbsp;<a href=\"https://ourworldindata.org/meat-production#number-of-animals-slaughtered\">Our World in Data</a>, using data from the&nbsp;<a href=\"https://web.archive.org/web/20220811152257/https://www.fao.org/faostat/en/\">UN Food and Agriculture Organization</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrql4jxg5vba\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrql4jxg5vba\">^</a></strong></sup></span><div class=\"footnote-content\"><p>2021\u5e74\u3001\u7d04650\u4e07\u306e\u52d5\u7269\u304c\u7c73\u56fd\u306e\u30a2\u30cb\u30de\u30eb\u30fb\u30b7\u30a7\u30eb\u30bf\u30fc\u306b\u9001\u3089\u308c\u307e\u3057\u305f\u3002\u3053\u306e\u6570\u306f2011\u5e74\u306b\u306f720\u4e07\u3067\u3057\u305f\u3002\u4e00\u5b9a\u306e\u5272\u5408\u3067\u6570\u304c\u6e1b\u5c11\u3057\u3066\u3044\u308b\u3068\u60f3\u5b9a\u3059\u308b\u306a\u3089\u3001\u3053\u306e\u3053\u3068\u304c\u610f\u5473\u3059\u308b\u306e\u306f\u30012018\u5e74\u306b\u306f\u7d04670\u4e07\u306e\u52d5\u7269\u305f\u3061\u304c\u30a2\u30cb\u30de\u30eb\u30fb\u30b7\u30a7\u30eb\u30bf\u30fc\u306b\u9001\u3089\u308c\u3066\u3044\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002</p><p>\u5de5\u5834\u5f0f\u755c\u7523\u8fb2\u5834\u306b\u306f\u300195.6\u5104/670\u4e07 =\u3014\u30b7\u30a7\u30eb\u30bf\u30fc\u3068\u6bd4\u3079\u3066\u30151,427\u500d\u306e\u6570\u306e\u52d5\u7269\u304c\u53ce\u5bb9\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p><p><a href=\"https://www.aspca.org/helping-people-pets/shelter-intake-and-surrender/pet-statistics\">Pet Statistics</a>. American Society for the Prevention of Cruelty to Animals, 2021.&nbsp;<a href=\"https://web.archive.org/web/20210805170626/https://www.aspca.org/helping-people-pets/shelter-intake-and-surrender/pet-statistics\">Archived link</a>, accessed August 2021.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndbp14yt1ovu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdbp14yt1ovu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u30a2\u30cb\u30de\u30eb\u30fb\u30b7\u30a7\u30eb\u30bf\u30fc\u306e\u652f\u51fa\uff1a</p><p>Andrew Rowan \u306f2018\u5e74\u6642\u70b9\u3067\u7c73\u56fd\u306e\u30a2\u30cb\u30de\u30eb\u30fb\u30b7\u30a7\u30eb\u30bf\u30fc\u56e3\u4f53\u4e0a\u4f4d 3,000\u56e3\u4f53\u3078\u306e\u652f\u63f4\u91d1\u306f50\u5104\u30c9\u30eb\u306b\u306e\u307c\u308b\u3068\u7b97\u51fa\u3057\u3001\u6b21\u306e\u8ad6\u6587\u306b\u307e\u3068\u3081\u3066\u3044\u307e\u3059\u3002\u201c<a href=\"https://www.researchgate.net/publication/335325531_Cat_Demographics_Impact_on_Wildlife_in_the_USA_the_UK_Australia_and_New_Zealand_Facts_and_Values\">Cat Demographics &amp; Impact on Wildlife in the USA, the UK, Australia and New Zealand: Facts and Values</a>\u201d Rowan et. al. (2020), Journal of Applied Animal Ethics Research, pages 7\u201337.</p><p>Andrew Rowan \u306f\u79c1\u305f\u3061\u3068\u306e\u3084\u308a\u3068\u308a\u306e\u306a\u304b\u3067\u3001\u3053\u306e\u8a08\u7b97\u306e\u5143\u3068\u306a\u3063\u305f\u30c7\u30fc\u30bf\u3092\u793a\u3057\u3001\u305d\u306e\u6b63\u3057\u3055\u3092\u88cf\u3065\u3051\u3066\u3044\u307e\u3059\u3002</p><p>\u755c\u7523\u52d5\u7269\u30a2\u30c9\u30dc\u30ab\u30b7\u30fc\u3078\u306e\u8cc7\u91d1\u63d0\u4f9b\uff1a</p><p>\u3053\u3053\u306b\u516c\u958b\u3055\u308c\u305f Open Philanthrophy \u306e\u7814\u7a76\u306b\u3088\u308c\u3070\u3001\u755c\u7523\u52d5\u7269\u306b\u95a2\u3059\u308b\u30a2\u30c9\u30dc\u30ab\u30b7\u30fc\u56e3\u4f53\u3092\u5bfe\u8c61\u3068\u3059\u308b2018\u5e74\u306e\u52a9\u6210\u91d1\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002</p><p>1) \u65e2\u5b58\u306e\u7c73\u56fd\u306e\u56e3\u4f53\uff08PETA, PCRM, HSUS, ALDF, ASPCA\uff09\u306b3,230\u4e07\u30c9\u30eb<br>2) \u7c73\u56fd\u306b\u62e0\u70b9\u3092\u304a\u304f\u65b0\u305f\u306a\u56e3\u4f53\u306e\u3046\u3061\u3067\u4e3b\u8981\u306a\u3082\u306e\uff08CIWF, WAP, RSPCA, HSI\uff09\u306b3,260\u4e07\u30c9\u30eb<br>3) \u4ed6\u306e\u7c73\u56fd\u306e\u56e3\u4f53\u306b3,220\u4e07\u30c9\u30eb</p><p>3,230 + 3,260 + 3,220 = 9,710\u4e07\u30c9\u30eb</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna6nbqyw2h0b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa6nbqyw2h0b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>[\u8a33\u6ce8] \u6ce819\u306b\u3082\u3042\u308b\u901a\u308a\u3001\u3053\u306e\u6570\u306f\u3001\u30b1\u30fc\u30b8\u30d5\u30ea\u30fc\u30fb\u30b3\u30df\u30c3\u30c8\u30e1\u30f3\u30c8\u3092\u5ba3\u8a00\u3057\u305f\u4f01\u696d\u304c\u5b9f\u969b\u306b\u30b3\u30df\u30c3\u30c8\u30e1\u30f3\u30c8\u3092\u5b9f\u884c\u3057\u305f\u3068\u4eee\u5b9a\u3057\u305f\u5834\u5408\u306e\u6570\u3067\u3042\u308a\u3001\u3053\u306e\u8a18\u4e8b\u3092\u7ffb\u8a33\u3057\u3066\u3044\u308b2022\u5e74\u6642\u70b9\u3067\u306f\u3001\u4f01\u696d\u304c\u5b9f\u969b\u306b\u30b1\u30fc\u30b8\u30d5\u30ea\u30fc\u30fb\u30b3\u30df\u30c3\u30c8\u30e1\u30f3\u30c8\u3092\u5b9f\u73fe\u3059\u308b\u306e\u304b\u3069\u3046\u304b\u304c\u755c\u7523\u52d5\u7269\u306b\u95a2\u308f\u308b\u30a2\u30c9\u30dc\u30ab\u30b7\u30fc\u56e3\u4f53\u306b\u5171\u901a\u306e\u8ab2\u984c\u3068\u3057\u3066\u8a8d\u8b58\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvy24kdv5mph\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvy24kdv5mph\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://web.archive.org/web/20220728174554/https://www.ams.usda.gov/sites/default/files/media/Egg%20Markets%20Overview.pdf\">USDA Egg Markets Overview</a> \u306e\u5831\u544a\u306b\u3088\u308c\u3070\u30012022\u5e745\u6708\u6642\u70b9\u3067\u7c73\u56fd\u3060\u3051\u3067\u30821\u5104650\u4e07\u306e\u96cc\u9d8f\u305f\u3061\u304c\u30b1\u30fc\u30b8\u30d5\u30ea\u30fc\u306e\u53ce\u5bb9\u65b9\u5f0f\u3067\u98fc\u80b2\u3055\u308c\u3066\u3044\u307e\u3059\u3002<a href=\"https://web.archive.org/web/20160823231743/https://www.ams.usda.gov/sites/default/files/media/Egg%20Markets%20Overview.pdf\">\u3053\u306e\u6570\u306f2016\u5e74\u306b\u306f1,700\u4e07</a>\u3067\u3057\u305f\u3002Open Wing Alliance \u306e\u53d6\u308a\u7d44\u307f\u306e\u7d50\u679c\u3001\u6b27\u5dde\u3067\u306f\u3055\u3089\u306b1\u5104\u7fbd\u306e\u96cc\u9d8f\u305f\u3061\u304c\u30b1\u30fc\u30b8\u304b\u3089\u89e3\u653e\u3055\u308c\u305f\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u304c\u3001\u3053\u306e\u6570\u3092 Open Wing Alliance \u3060\u3051\u306e\u6210\u679c\u306b\u5e30\u3059\u308b\u306e\u306f\u3014\u7c73\u56fd\u3068\u6bd4\u3079\u3066\u3015\u3088\u308a\u96e3\u3057\u3044\u3068\u601d\u308f\u308c\u307e\u3059\u3002</p><p>\u52a0\u3048\u3066\u30a2\u30c9\u30dc\u30ab\u30b7\u30fc\u56e3\u4f53\u304c\u3068\u308a\u3064\u3051\u305f\u4f01\u696d\u306e\u30b1\u30fc\u30b8\u30d5\u30ea\u30fc\u5ba3\u8a00\u306f\u3001\u3082\u3057\u305d\u308c\u304c\u5b9f\u884c\u3055\u308c\u308b\u306a\u3089\u3001\u3069\u306e\u6642\u70b9\u3092\u3068\u3063\u3066\u3082\u751f\u304d\u3066\u3044\u308b5\u5104\u7fbd\u4ee5\u4e0a\u306e\u96cc\u9d8f\u3092\u6551\u3046\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6nvfidr2h5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6nvfidr2h5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Good Food Institute\u3068\u306e\u4ea4\u6e09\u5f8c\u3001\u7c73\u56fd\u653f\u5e9c\u306f\u30bf\u30d5\u30c4\u5927\u5b66\u3067\u306e\u7d30\u80de\u8fb2\u696d\u7814\u7a76\u6240\u306e\u5275\u8a2d\u306b1\u5343\u4e07\u30c9\u30eb\u306e\u52a9\u6210\u3092\u884c\u3046\u3068\u767a\u8868\u3057\u307e\u3057\u305f\u3002\u30a4\u30ae\u30ea\u30b9\u306e\u72ec\u7acb\u6a5f\u95a2\u3001National Food Strategy \u306f\u30011\u51042500\u4e07\u30dd\u30f3\u30c9\u3092\u4ee3\u66ff\u30d7\u30ed\u30c6\u30a4\u30f3\u306e\u7814\u7a76\u3068\u958b\u767a\u306b\u6295\u8cc7\u3059\u308b\u3088\u3046\u63a8\u5968\u3057\u307e\u3057\u305f\u3002<br>\u51fa\u5178\uff1a<a href=\"https://web.archive.org/web/20220728174648/https://gfi.org/wp-content/uploads/2022/03/GFI_Year-in-Review_2021_v2.pdf\">GFI Year in Review 2021</a> (page 3)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnp6hw0pvxlcb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefp6hw0pvxlcb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u69d8\u3005\u306a\u4e88\u6e2c\u306e\u30bf\u30a4\u30e0\u30e9\u30a4\u30f3\u5168\u4f53\u306f&nbsp;<a href=\"https://web.archive.org/web/20220228132434/https://www.metaculus.com/questions/8898/russian-invasion-of-ukraine-before-2023/\">Metaculus</a> \u4e0a\u3067\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzu5s20p9d6n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzu5s20p9d6n\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;[\u8a33\u6ce8] \u3053\u3053\u3067\u8457\u8005\u306f\u300c\u4eba\u3073\u3068\uff08people\uff09\u300d\u3068\u3044\u3046\u8a00\u8449\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u300c\u5de5\u5834\u5f0f\u755c\u7523\u3092\u7d42\u308f\u3089\u305b\u308b\u300d\u3068\u3044\u3046\u9805\u76ee\u304c\u7acb\u3066\u3089\u308c\u3066\u3044\u305f\u3088\u3046\u306b\u3001\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306f\u6c7a\u3057\u3066\u3001\u4eba\u9593\u3060\u3051\u306e\u5229\u5bb3\u3092\u8003\u616e\u306b\u5165\u308c\u308b\u3082\u306e\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u78ba\u304b\u306b\u9053\u5fb3\u7684\u306a\u914d\u616e\u306b\u50be\u659c\u3092\u3064\u3051\u308b\u3053\u3068\u3082\u8003\u3048\u3089\u308c\u3001\u305d\u306e\u610f\u5473\u3067\u771f\u306b\u300c\u4e0d\u504f\u7684\u300d\u306a\u306e\u306f\u4eba\u9593\u306b\u5bfe\u3057\u3066\u3060\u3051\u3060\u3068\u52b9\u679c\u7684\u5229\u4ed6\u4e3b\u7fa9\u306e\u67a0\u5185\u3067\u8003\u3048\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u304c\u3001\u4ed6\u65b9\u3001\u8a33\u6ce8\u306e1\u3067\u53c2\u7167\u3057\u305f MacAskill \u306f\u300c\u4eba\u3073\u3068\uff08people\uff09\u300d\u3067\u306f\u306a\u304f\u300c\u8ab0\u3057\u3082\u306e\u798f\u5229\uff08everyone\u2019s wellbeing\uff09\u300d\u3068\u3044\u3046\u8a00\u3044\u65b9\u3092\u3057\u3066\u3044\u305f\u3053\u3068\u3082\u3001\u3053\u3053\u3067\u6307\u6458\u3057\u3066\u304a\u304d\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002</p></div></li></ol>", "user": {"username": "EA Japan"}}, {"_id": "beG8QPhPc6AvxJrjt", "title": "Have your say on the Australian Government's AI Policy [Brisbane]", "postedAt": "2023-06-09T00:15:06.714Z", "htmlBody": "<p>The Australian Government is asking what it can do the mitigate any potential risks from AI [see <a href=\"https://consult.industry.gov.au/supporting-responsible-ai\">consultation request</a>].</p><p>The Australian EA AI Governance group and <a href=\"https://www.goodancestors.org.au/policy\">Good Ancestors Policy</a> are preparing a submission to help mitigate the worst outcomes from AI [e.g., see <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\">80,000 hours cause profile</a>].</p><p>At this event, we will assume most people are aware of those risks, and will instead be focusing on what government levers might help [e.g., <a href=\"https://www.openphilanthropy.org/research/12-tentative-ideas-for-us-ai-policy/\">which of these apply in Australia</a>, or what might be similar alternatives]. For those members of the community who want to make a submission to the consultation, we will provide advice, support, and feedback.</p><p>Given AI Governance may be an impossible task, we'll also go for <a href=\"https://www.grilld.com.au/meatfreemondays\">2-for-1 Impossible Burgers afterwards</a>.</p><p>Location: The Atrium, The University of Queensland, 88 Creek St, Brisbane<br>Room: 500D-512 Collaborative Room</p>", "user": {"username": "mnoetel"}}, {"_id": "sNqzGZjv4pRJjjhZs", "title": "Wild Animal Welfare Scenarios for AI Doom", "postedAt": "2023-06-08T19:41:37.599Z", "htmlBody": "<p><i>Epistemic Status: Uhhhhhh.</i></p><h2>Intro</h2><p>I think longtermists sometimes fail to grapple with animal welfare, and even when we do, the arguments leave something to be desired in the way of nuance. For example, I have encountered claims like \"there will be no animals in the future!\" or \"moral circle expansion solves!\" which strike me as shallow dismissals of an important question.</p><p>In this post, I'm interested in identifying some empirical questions about animal welfare that bear on how bad AI doom would be. I focus on wild animals because I think their welfare probably controls the question of how good the world is at present.&nbsp;</p><h2>Variables</h2><p>For our model, suppose doom is a discrete event that could occur at a fixed time in the future and would eliminate all humans. All events up to doom are considered sunk; we are concerned with what happens post-doom. Here are some variables to consider:</p><p><strong>WAW | no humans:</strong> If wild animals survive AI doom, do they lead good lives, on net? To first order, this is True iff wild animals lead good lives now, which is a fraught ethical problem. Wild animals are subject to predation, disease, starvation, injury, and disease, and many die before reaching adulthood. On the other hand, much of an animals life consists of some state of preference-satisfaction, which might outweigh the preceding sources of pain. (Of course, sharing the world with AGI introduces complications for wild animals that are not captured by their current welfare levels.)</p><p><strong>\u0394 WAW | humans: </strong>Suppose humans do survive (no doom). Do they improve or frustrate wild animal welfare compared to the baseline with no humans? In the optimistic camp, we find arguments like moral circle expansion, which holds that humans will benefit wild animals through interventions like planting soy worms for the birds or whatever. In the pessimistic camp lives the conventional litany of ways that humans seem to hurt wild animals (deforestation, hunting, pollution, etc).</p><p><strong>WAW | humans: </strong>Accounting for the effects of sharing a doomless world with humans, do wild animals lead good lives? In other words, is the above delta large enough to flip the sign of animal welfare from negative to positive or vice versa? For example, maybe wild animals would suffer in a world without humans, but humans stand to make positive contributions to wild animals such that they would lead net-good lives.</p><p><strong>Will doom kill animals? </strong>For simplicity, if True, doom spells the end for every wild animal. If False, there will be roughly no longterm change in the wild animal population due to doom. Note that animal doom would probably involve pain but I'm ignoring this because it's unclear if doom-death is more painful than a counterfactual death, and even if it is, this greater marginal pain would be negligible compared to the long run impact of, e.g., averted suffering. Some doom scenarios seem to imply wild animals' extinction (AGI needs those sweet, sweet atoms), but I think it's often unclear, which reflects the poor state of Animal x Longtermist discourse. Is the elimination of wild animals a convergent instrumental subgoal in its own right? Or would animal extinctions occur incidentally in pursuit of human extinction?</p><h2>Outputs</h2><p>From these four inputs, we are interested in whether doom is good or bad for wild animals:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/scxgmyv2wbckdewap0zd\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/lwkxzd7whxhwu4ilojnb 250w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/xasmyyhcvuueh98i9eor 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/sw10uhh6xpr8e91pcwhx 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/dqwn1rlolansbyjph1st 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/tva8k7yp252ef5nyzcxo 1250w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/lslbmldibthhi7bqz7ik 1500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/syetk3cpbornzk4iqe7x 1750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/t8pkytdeoxbkyboxoev0 2000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/dtasfhkdwqecbdycuh9w 2250w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/dda6acecj0glnfjnz4es 2456w\"></figure><p>If doom would kill animals, the effect of doom is the opposite of WAW | humans (would animals be better off disappearing along with humans?). Otherwise, the effect of doom is the opposite of \u0394 WAW | humans (would animals be better off without humans?).</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/ek4nxwfiztnbxakcnbbu\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/hlsketpqil8itvjwa3ih 260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/cm9rozmgxaqvh9fwpupc 520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/ybhh9rv6goyxueyobz6p 780w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/b2ldc3gbrtg7l0mj0wmo 1040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/ytztzkmqnwa6bxpoa2dk 1300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/cgixcpncpsq6gjwdzij9 1560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/trmizmjnm8c1bubk1zg7 1820w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/ysueth6gaqqmpppwmh4g 2080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/yzxbkfxqrhjq8612cnwq 2340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sNqzGZjv4pRJjjhZs/dobkkktfb1dk2wnvgqcu 2516w\"></figure><p>Observe that there are an equal number of scenarios for which doom is good as bad. I would be interested in hearing probability distributions over these cases in the comments, especially ones that make more crisp my understanding of whether doom would eliminate wild animals.&nbsp;</p><p>I think it's worth explicating some of these scenarios. The others are left as an exercise to the reader ;)</p><h2>Scenarios</h2><p><strong>Shepherdless Sheep</strong> (row 2, 8, 10): Wild animals would have benefited from humans, but instead they are adrift in the world (leading good or bad lives, depending on the case). We should avoid doom so we have a chance to populate the planet with better-off wild animals.</p><p><strong>Animal Liberation</strong> (row 4, 6): In the presence of humans, wild animals would be worse off, but in the absence of humans, they are quite content. Doom is good because it frees animals from humans.</p><p><strong>Misery Putter-Outer</strong> (row 5, 9, 11, 12): Humans either make things worse or don't help enough. Doom presents a merciful end to wild animal suffering.</p><h2>Closing</h2><p>I'm not trying to be contrarian; once you account for the impact on humans, Doom is probably, like, bad. But I do wish people attended more carefully to other moral patients in their arguments about AI doom.&nbsp;</p><p>I think the easy way to avoid these debates is to outweigh wild animals with digital minds. I'm receptive to this move, but I would like to see some estimates comparing numbers of wild animals vs potential for digital sentience. I also think this argument works best if alignment research will afford some tractability on the quantity of digital minds or quality of digital experiences, so I would be interested to hear about work in this direction.&nbsp;</p>", "user": {"username": "Rocket"}}, {"_id": "nSXoHLiWdbZkKwj8F", "title": "How to DIY a Funder's Circle", "postedAt": "2023-06-08T19:23:47.114Z", "htmlBody": "<p>I talked to Will Troy who works with Charity Entrepreneurship to support the <a href=\"https://forum.effectivealtruism.org/posts/jEHcbrsumxditRhtG/updates-from-the-mental-health-funder-s-circle\">Mental Health Funder\u2019s Circle</a>. They got together to help promising mental health charities survive the \u201cvalley of death\u201d between being first created and being proven to work, and are just completing their second round of funding charities.</p>\n<p>\u201cIt\u2019s definitely possible to organise a donor\u2019s circle no matter what your level of giving,\u201d Will says. And funder\u2019s circles are trending in the charity world, even outside of EA - check out this <a href=\"https://guerrillafoundation.org/donor-engagement/join-our-funders-circle/\">funder\u2019s circle for activists</a>. So how could you create your own funder\u2019s circle with your friends?</p>\n<p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yGcDMhmBmvjPFiHpX/ikhsr7f7jtlb2enbfejt\" alt=\"\"></p>\n<h1>1. Get a group together</h1>\n<p>Gather a group of people who are happy to donate and willing to put in some research into different charities.</p>\n<p>The mental health funder\u2019s circle expects a minimum donation of $50,000. Having some kind of minimum donation, whether it\u2019s $50 or $5000, can help you to avoid \u201ctourists\u201d who are just checking it out for a short while - you want people with a bit of skin in the game. That said, it\u2019s certainly possible to have a circle where people just share information, with no commitment to donate on that topic.</p>\n<p>How many people? \u201cMore than three and less than ten,\u201d Will suggests. You should be able to easily have a conversation about important topics and all have your voices heard.</p>\n<h1>2. Find your niche</h1>\n<p>The smaller the amount your group is planning to donate, the more useful it can be to niche down. For example, if some of the funders were based in the Philippines and had experience working on or donating to mitigating the negative effects of climate change on low income population over the next fifty years, or if funders from India wanted to find local organisations focused on reducing extinction risks, or if funders from the midwestern United States wanted to look at local approaches to improving the wellbeing of farmed animals, they\u2019d likely still find plenty of opportunities to choose from.</p>\n<p>This might sound like the opposite of the typical EA approach of doing the most good, but Will says it has some benefits. Defining a cause area, philosophical beliefs and/or geographic region you\u2019re looking to donate to can help you to avoid analysis paralysis and avoid trying to replicate the work of a much larger organisation like GiveWell or the Open Philanthropy project. And once you\u2019ve found the best organisations within a small niche, you can compare them to more traditional EA charities - it\u2019s possible you could identify a great charity that then gets picked up by GiveWell, Animal Charity Evaluators, or a large charitable foundation.</p>\n<h1>3. Meet monthly</h1>\n<p>Will suggests meeting at least monthly in order to keep a sense of momentum. That could be sharing information on different charities you could potentially donate to, or due diligence you\u2019ve been doing on a charity that looks great, or reporting back on the donations you\u2019ve made - anything that helps you to find great places to donate and encourages you to keep researching and donating.</p>\n<p>The mental health funder\u2019s circle gives out grants twice a year. That gives roughly six sessions to meet, agree on priorities, start sharing ideas of different charities, contact the best charities, do your due diligence, and make your donations. The time will go faster than you think!</p>\n<p>Are you thinking of starting a funder\u2019s circle? Tweet me <a href=\"https://twitter.com/EAheadlines\">@EAheadlines</a> or comment below to find your co-funders, and message Will (<a href=\"https://forum.effectivealtruism.org/users/wtroy\">wtroy</a>) on the EA Forum for more tips. He says this space is moving so fast, the advice may have changed between the interview and publication, so definitely reach out!</p>\n<p>Read more posts like this: <a href=\"http://ealifestyles.substack.com\">ealifestyles.substack.com</a></p>\n", "user": {"username": "Khorton"}}, {"_id": "zdTLKhxDjbAfP58Ky", "title": "[Applications Open] CEA\u2019s University Group Accelerator Program (UGAP)", "postedAt": "2023-06-08T23:28:38.284Z", "htmlBody": "<p>Are you a student at a university without a university group? Or, does your university group need reviving? Consider applying to UGAP!&nbsp;</p><p>The <a href=\"https://centreforeffectivealtruism.notion.site/centreforeffectivealtruism/University-Group-Accelerator-Program-6df8c8fccf8b4ffbb6488d9dfa275282\">University Group Accelerator Program (UGAP)</a> is a program run by the Centre For Effective Altruism that aims to take a university group from a couple of interested organizers to a mid-sized new EA group. It offers stipends for main organizers, regular meetings with an experienced mentor, trainings, and useful resources to run your first intro seminar (or fellowship) program.&nbsp;</p><p><strong>Applications for next semester are open!</strong></p><ul><li><a href=\"https://www.notion.so/centreforeffectivealtruism/University-Group-Accelerator-Program-6df8c8fccf8b4ffbb6488d9dfa275282\"><strong>Learn more and apply here</strong></a><strong> by 11:59pm PT, on July 2nd</strong></li></ul><p><i>We will have some limited capacity to provide mentorship to organizers from non-new university groups via our </i><a href=\"https://www.notion.so/centreforeffectivealtruism/Organizer-Support-Program-OSP-0aa5dc55d97e444da347d8d227db93d4?pvs=4\"><i>organizer support program</i></a><i>. Organizers from those groups can apply using the same form.</i></p><p>We are also looking for additional experienced organizers to serve as mentors and hiring senior mentors. &nbsp;<a href=\"https://www.notion.so/centreforeffectivealtruism/Apply-to-Mentor-University-EA-Groups-b5b005ce6be24d0b825fc1ab5dcd7c3d\">You can find out more about and express interest in becoming a mentor here.&nbsp;</a></p><p>We have supported over 200 organizers via the program and have had groups from every populated continent participate. We are excited to keep growing!</p>", "user": {"username": "jessica_mccurdy"}}, {"_id": "DAH6h9rMhiHSj5EcE", "title": "X-risk Agnosticism", "postedAt": "2023-06-08T15:02:27.857Z", "htmlBody": "<p><strong>Tl;dr: </strong><i>Epistemic modesty recommends distributing credence across a diverse range of models or perspectives, some of which rate x-risk very highly. Applying standard methods of prudent decision-making under uncertainty, even x-risk agnostics should take x-risk seriously in practice.</i></p><p>&nbsp;</p><p>While I generally assume that the odds of a global catastrophe (\u201cx-risk\u201d) eventuating this century are <i>pretty low</i>, I\u2019m highly uncertain<i>. </i>I expect many others are in a similar epistemic position. But I also think that this broadly \u201cagnostic\u201d, non-committal position yields some pretty clear and striking practical implications: namely, that we should <i>take x-risk seriously</i>, and want to see a non-trivial (though also, of course, non-maximal) amount of resources dedicated to its further investigation and possible mitigation.</p><h3>Small yet non-trivial risks of disaster are worth mitigating</h3><p>If I lived near a nuclear power plant, I would hope that it had safety engineers and protocols in place to reduce the risk of a nuclear accident. For any given nuclear reactor, I trust that it is <i>very</i> unlikely (p&lt;0.001) to suffer a catastrophic failure during my lifetime. But a nuclear disaster would be sufficiently bad that it can still be worth safety investments to reduce these (already low) risks.</p><p>Even if I lived near a nuclear power plant, its risk of catastrophic failure would not be my biggest fear. I\u2019d be much more worried about dying in a car accident, for example. That, too, is a small risk that\u2019s well worth mitigating (e.g., via seatbelts and modern car safety mechanisms). And, perhaps roughly on a par with the risk of dying in a car accident, we might all die in some horrific global catastrophe\u2014such as nuclear war, a bioengineered pandemic, or misaligned AI shenanigans.</p><p>I don\u2019t think that any of the latter possibilities are outright <i>likely</i>. But unless you can rule out the risk of disaster to an <i>extreme</i> confidence level (p&lt;0.000001, perhaps), then it\u2019s clearly worth investing some societal resources into investigating and reducing global catastrophic risks. The <i>stakes</i> are much higher than an isolated nuclear reactor, after all; and if you\u2019re broadly agnostic about the precise probabilities, then it also seems like the <i>odds</i> are plausibly much higher. (That might sound self-contradictory: didn\u2019t we just assume agnosticism about the odds? But read on\u2026)</p><h3>Higher-order uncertainty</h3><p>There are lots of different ways that one might try to go about estimating x-risk probabilities. For example, you might opt for a highly optimistic prior based on induction over human history: we\u2019ve <i>never</i> wiped ourselves out before! Or you might take the Fermi paradox to imply that there\u2019s a \u201c<a href=\"https://en.wikipedia.org/wiki/Great_Filter\">great filter</a>\u201d likely to wipe out any intelligent species before it\u2019s capable of spanning the galaxy. Or you might defer to subject-area experts for each candidate risk. Or you might put aside these \u201coutside view\u201d approaches and instead just try to assess the arguments for and against specific candidate risks directly on their merits, to reach your own \u201cinside view\u201d of the matter. And so on.</p><p>As a thoroughgoing x-risk agnostic, I\u2019m highly uncertain about which of these approaches is best. So it seems most reasonable to distribute my credence between them, rather than confidently going \u201call-in\u201d on any one narrow approach (or confidently <i>ruling out</i> any particular approach, for that matter). But there are some reasonable-seeming models on which x-risk is disturbingly high (many AI experts are highly concerned about the risks from misaligned AI, for example; it also seems, commonsensically, like the base rates aren\u2019t great for past hominids superseded by more intelligent competitor species!). If I can\u2019t confidently rule out those gloomier approaches as <i>definitely </i>mistaken, then it seems I must give them some non-trivial weight.</p><p>As a result, assigning anything in the general vicinity of 0.1% - 5% risk of a global existential catastrophe this century seems broadly reasonable and epistemically modest to me. (Going below 0.1%, or above 5%, feels more extreme to me\u2014though I confess these are all basically made-up numbers, which I\u2019m not in a position to defend in any robust way.) I personally lean towards ~1% risk. Which is very \u201clow\u201d by ordinary standards for assessing probabilities (e.g., if we were talking about \u201cchance of rain\u201d), but distressingly high when specifically talking about the risk of <i>literally everyone dying </i>(or otherwise suffering the permanent loss of humanity\u2019s potential).</p><p>This appeal to higher-order or <strong>\u201cmodel\u201d uncertainty</strong> also suggests that the basic case for taking x-risk seriously isn\u2019t so <i>fragile</i> or sensitive to particular objections or counterarguments as you might expect. (I\u2019ll discuss an example in the next section.) Given the broad range of possibilities and competing arguments over which I\u2019m distributing my credence here, it\u2019s hard to imagine a \u201cuniversal acid\u201d objection that would confidently apply across <i>all</i> of them. I could easily see myself being persuaded to reduce my estimate of p(doom) from 1% to 0.1%, for example; but I can\u2019t easily imagine what could convince me that x-risk as a whole is nothing to worry about. Total complacency strikes me as an extraordinarily extreme position.</p><h3>\u201cTime of Perils\u201d Agnosticism</h3><p>One of the most interesting objections to specifically <i>longtermist</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6fjo3ento0p\"><sup><a href=\"#fn6fjo3ento0p\">[1]</a></sup></span>&nbsp;concern about x-risk is the argument that humanity effectively has <i>no astronomical potential</i>: given a constant, non-trivial risk-per-century of extinction, we\u2019re basically <i>guaranteed</i> to wipe ourselves out sooner rather than later anyway. Brian Weatherson has <a href=\"https://twitter.com/bweatherson/status/1557035220855099392\">tweeted an argument along these lines</a>, and David Thorstad has <a href=\"https://ineffectivealtruismblog.com/category/my-papers/existential-risk-pessimism/\">a whole series of blog posts</a> (based on a paper) developing the argument at length.</p><p>I grant that that\u2019s a possibility. But I also think it\u2019s far from a <i>certainty</i>. So we also need to distribute our credence over possibilities in which per-century risks are variable\u2014and, in particular, scenarios in which we are currently in a <i>uniquely dangerous</i> \u201ctime of perils\u201d, after which (if we survive at all) x-risk can be reduced to sustainably low levels.</p><p>Why give any credence to the \u201ctime of perils\u201d hypothesis? It seems to me that there are at least two reasonably plausible ways that the far future (several centuries hence) could be <i>much</i> safer than the near future (assuming that near-future risks are dangerously high):</p><p>(1) Humanity might be so widely dispersed across the stars that different (self-perpetuating) pockets are effectively inaccessible to each other. A disaster in one stellar region simply won\u2019t have time to reach the others before they\u2019ve already succeeded in spreading humanity further.</p><p>(2) Safely-aligned Transformative AI might asymmetrically improve our safety capabilities relative to our destructive ones. For example, a global \u201cbenevolent dictator\u201d AI might use universal surveillance to identify and deal with potential threats far more effectively than today\u2019s human-powered law enforcement. (If this is to be a non-dystopian future, we would of course want a broad array of liberal protections to ensure that this power is not abused.)</p><p>Both possibilities are speculative, of course, and I don\u2019t necessarily claim that either is the <i>most</i> likely outcome. (If I had to guess, I\u2019d probably say the <i>most</i> likely outcome is that per-century objective x-risks are <i>so</i> low that none ultimately eventuate even without these specific protections; but again, <i>that\u2019s just one model</i>, and I wouldn\u2019t want to place all my eggs in that basket!) But neither strikes me as outrageously unlikely<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefut1udla6fw\"><sup><a href=\"#fnut1udla6fw\">[2]</a></sup></span>\u2014the one thing we know about the future is that <a href=\"https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/\">it will be weird</a>\u2014so I think they each warrant non-trivial credence, and either could support the view that current x-risk mitigation efforts have very high expected value.</p><h3>Conclusion</h3><p>My claims here are pretty non-committal. I\u2019m no expert on x-risk, and haven\u2019t looked super closely into the issues. (I haven\u2019t even read <a href=\"https://theprecipice.com/\"><i>The Precipice</i></a> yet!) So for all I\u2019ve said, it may well be totally reasonably for those better-informed about the first-order issues to have a more extreme credence (in either direction) than what I take \u201cmoderate agnosticism\u201d to call for.</p><p>Still, for those who share my more limited epistemic position, moderate agnosticism seems pretty reasonable! And I think it\u2019s interesting that this agnosticism, when combined with a prudent approach to decision-making under uncertainty, seems to strongly support <i>taking x-risk seriously</i> over dismissive complacency.</p><p>That\u2019s not to defend fanatical views on which 99.9% of global resources should be put towards that goal\u2014on the contrary, I think that commonsense rejection of fanaticism reflects our intuitive sense that it, too, doesn\u2019t sufficiently respect normative and model uncertainty. (We\u2019re <a href=\"https://www.openphilanthropy.org/research/worldview-diversification/#when-and-how-should-one-practice-worldview-diversification\">rightly wary</a> of putting all our eggs in one basket.) But 0% (or even 0.001%) is surely imprudently low. X-risk agnostics should support further research to improve our collective ability to both <i>identify</i> and (where possible) <i>mitigate</i> potential existential risks.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6fjo3ento0p\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6fjo3ento0p\">^</a></strong></sup></span><div class=\"footnote-content\"><p>But cf. <a href=\"https://forum.effectivealtruism.org/posts/DiGL5FuLgWActPBsf/how-much-should-governments-pay-to-prevent-catastrophes\">Elliott Thornley &amp; Carl Shulman\u2019s argument</a> that x-risk mitigation passes cost-benefit analysis even when merely considering the interests of already-existing individuals.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnut1udla6fw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefut1udla6fw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>David Thorstad <a href=\"https://ineffectivealtruismblog.com/2022/12/07/existential-risk-pessimism-and-the-time-of-perils-part-4-space/\">seems to assume</a> that interstellar colonization could not possibly happen within the next two millennia. This strikes me as a massive failure to properly account for model uncertainty. I can\u2019t imagine being so confident about our technological limitations even a few <i>centuries</i> from now, let alone <i>millennia</i>. He also <a href=\"https://ineffectivealtruismblog.com/2023/04/01/existential-risk-pessimism-and-the-time-of-perils-part-9-objections-and-replies/\">holds</a> the suggestion that superintelligent AI might radically improve safety to be \u201cgag-inducingly counterintuitive\u201d, which again just seems a failure of imagination. You don\u2019t have to find it the <i>most</i> likely possibility in order to appreciate the possibility as <i>worth including in your range of models</i>.</p></div></li></ol>", "user": {"username": "RYC"}}, {"_id": "m8p4ZYqZZvN96w5q8", "title": "Profit for Good- an FAQ Responsive to EA Feedback", "postedAt": "2023-06-08T13:52:41.312Z", "htmlBody": "<p>I would like to share an FAQ regarding Profit for Good &nbsp;produced by the Consumer Power Initiative's Director of Research Blake Hannagan, which he created responsively to questions/redteaming of primarily Effective Altruists. Profit for Good consists of companies with popular charities in the 90%+ shareholder position, meaning profits primarily benefit charities (Examples: Newman's Own, Humanitix, Patagonia)We believe such companies can produce goods and services of similar quality at similar price, enabling consumers, employees other economic actors to benefit charities without making any sacrifice, unlike donations. This positive discrimination we call Charity Choice, and offers the ability for philanthropists to leverage their charitable dollars through a world which would rather the money from their purchases went to end malaria, for instance, rather than enrich further wealthy investors.</p><p>I am going to be doing a <a href=\"https://www.tedxtaftavenue.com/\">TEDx talk</a> &nbsp;on Profit for Good in late July and am interest in feedback on my draft talk (PM me). <a href=\"https://consumerpowerinitiative.org/donation\">Donations are also most welcome to our 501c3</a>- significant costs are associated regarding travel expenses for the talk, rebranding and development of our website, and preparing to establish an incubator/accelerator/funder for Profit for Good companies. Said costs are primarily now being borne by myself, with difficulty.</p><p>Here is a list of <a href=\"https://docs.google.com/document/d/136oCdRIDfQ3nyF0fud8JD_ErhVD2eGEJ?rtpof=true&amp;usp=drive_fs\">reading materials on Profit for Good</a>, with a good starting place being this <a href=\"https://forum.effectivealtruism.org/posts/WMiGwDoqEyswaE6hN/making-trillions-for-effective-charities-through-the\">essay on the forum</a>. Our <a href=\"https://consumerpowerinitiative.org/\">website</a></p><p>&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<strong>FAQ:</strong><br><br><strong><u>1. What if the charity needs the money now? It isn\u2019t very useful for a charity to be entitled to the profits of a company if they need funding immediately, right?</u></strong></p><p><br>&nbsp;</p><p>Even though the charity would not possess cash, they would possess assets against which they can borrow. If shares were purchased for $X, then the charity should fairly safely be able to borrow between 0.5*P*$X and 0.7*P*$X depending on the specific context where P is a positive discrimination multiplier which should be greater than 1. They can then pay back the loan with the dividends they receive later because they own the stock. Additionally, the charity could benefit from some positive discrimination from their lender and receive lower interest rates as a way for the lender to cultivate a positive public image. Ultimately, they may not be able to access $X immediately, but they would likely be able to access a large portion of $X soon. There do exist some optimistic scenarios where charities could actually access more than $X relatively quickly. If you would like more information on these scenarios, please contact Blake Hannagan by emailing blake@consumerpowerinitiative.org.</p><p><br>&nbsp;</p><p><strong><u>2. Wouldn\u2019t a Profit for Good (PFG) company have trouble competing in the market because they can\u2019t reinvest their profits back into their company?</u></strong></p><p><br>&nbsp;</p><p>No! This is one of the great things about the Profit for Good (PFG) framework; the PFG company still operates as a for-profit company the same way any other for-profit company operates. They are still able to reinvest some, or all, of their profits back into the company. The only difference is who gets the dividends/profits companies do pay out. The charities get their dividends the same way any other investor would get their dividends.</p><p><br>&nbsp;</p><p><strong><u>3. I\u2019ve seen companies donate proceeds to charity before; how is PFG different?</u></strong></p><p><br>&nbsp;</p><p>Oftentimes companies donate proceeds or profits to charities as a marketing tactic to reach new customers or as a way to gain positive publicity. However, in these situations, donations represent an additional cost to the company. In contrast, PFG companies do not pay any additional costs to make charitable donations; rather their profit, which would usually be paid to ordinary shareholders, are instead paid to shareholders which happen to be charities.</p><p><br>&nbsp;</p><p><strong><u>4. How will consumers know if a purchase is from a PFG company?</u></strong></p><p><br>&nbsp;</p><p>This is an important component of the PFG framework; if consumers cannot easily identify a PFG company\u2019s product, then it will be challenging/cumbersome for them to positively discriminate in favor of PFG companies. Our plan is to display the CPI logo (displayed at the top of this page) on each product produced by PFG companies as our stamp of certification and to signal to consumers the profit from buying this product will benefit charities.</p><p><br>&nbsp;</p><p><strong><u>5. Aren\u2019t the charities who own the stock in the PFG companies, in actuality, also in charge of running the company? If so, isn\u2019t it reasonable to think they may not be very good at that and hence the company would actually perform worse and decrease in value?</u></strong></p><p><br>&nbsp;</p><p>This is a topic CPI would like to research further. We have identified some studies which suggest corporations primarily owned by charities may benefit from some advantages with regard to corporate governance. Additionally, for this and other reasons, we currently believe it would be preferable for a charitable foundation to own the stock instead of the charities themselves so the charitable foundation(s) can help the charities navigate certain financial obstacles and conduct the necessary research to elect strong board members.</p><p><br>&nbsp;</p><p><strong><u>6. What\u2019s motivating PFG companies to perform well? If they no longer have a profit motive, won\u2019t they lack the incentives to perform optimally? What would Milton Friedman and other free-market economists say about the PFG framework?</u></strong></p><p><br>&nbsp;</p><p>PFG companies are still for-profit companies. This is a point worth emphasizing. They are still seeking to maximize profits for their shareholders the same way any other company would. They also still have the ability to reinvest a portion of their profits the same way any other company would. None of this has changed. The only thing which has changed is who the shareholders are - they are now charities or charitable foundations. These shareholders still want the company to succeed, perform exceptionally well, and maximize profits so the shareholders can access large funds.</p><p><br>&nbsp;</p><p>Regarding Milton Friedman, Friedman was a very strong supporter of charitable foundations. In fact, he felt charitable foundations were so great at meeting the needs of everyday people there was little to no need to develop a government-run social safety net. Additionally, Friedman thought companies competing in the \u201cfree market\u201d were well-incentivized to maximize their profits and that, to some extent, was a benefit to society all by itself (think of, or search for, his famous pencil example if you need clarification here). The PFG framework still works within the framework of the \u201cfree market\u201d and therefore still enjoys many, if not all, of its benefits. However, we theorize PFG companies, capitalizing on consumer\u2019s good will, enjoy a competitive advantage in the marketplace and their profits are better allocated to make the world a better place than they would if they were allocated to the average investor (in expectation).</p><p><br>&nbsp;</p><p><strong><u>7. Why not just earn to give? Shouldn't people just make money starting a company and then donate their money?</u></strong></p><p>&nbsp;</p><p>Earning to give is actually more of a complementary idea to the PFG framework than a substitute. Profit for Good companies are still for-profit companies and therefore are still able to, and ideally should, pay market wages. Therefore, if Microsoft were a PFG company (they are pretty close to being one as it is), and someone makes a lot of money working as a \u201cinsert fancy job title here\u201d, they can still donate money from their personal income to effective charities and the company\u2019s profits will also be donated to (hopefully effective) charities.</p><p>&nbsp;</p><p>We at CPI believe starting a company and donating the profits to charities alone is inherently ineffective because that company would fail to capitalize on the good will their customers feel towards charities. Additionally, we believe founding a PFG company would increase your probability of success because you would be able to capitalize on consumer good will, benefit from business services at lower prices (because of positive discrimination), and take advantage of the environment CPI is trying to develop where consumers are aware of the PFG framework in general, the logo of CPI, and may even seek out PFG companies from which to purchase products.</p><p>&nbsp;</p><p><strong><u>8. Won't companies without private shareholders lack necessary signals they get from the buying and selling of shares? Won't singular ownership otherwise adversely affect corporate governance?</u></strong></p><p>&nbsp;</p><p>There may be certain situations where it is advantageous for only 90% (just a put a number here) of the shares to be held by charities or charitable foundations for price discovery and/or corporate governance reasons. In situations like this, it may also be beneficial for non-charity shareholders to gain disproportionately large voting rights so the charities and/or charitable foundations are effectively not (or at least far less) responsible for corporate governance but may still have veto power in case other investors are advocating for questionable or unethical practices or other exceptional circumstances. This is an area the CPI research team would like to explore more in the future. In particular, we would like to identify when different strategies would be most appropriate and what the effects of different strategies would be.</p><p>&nbsp;</p><p><strong><u>9. Consumers are very set in their habits and loyal to their brands: what makes you think they would shift purchasing for a factor that doesn't benefit them? Might consumers think they are implicitly paying someone to donate?</u></strong></p><p>&nbsp;</p><p>Our claim is not that all consumers would shift their purchasing habits to purchase from PFG companies. Rather, our claim is that&nbsp;<i>some consumers</i> will shift their purchasing habits because they are more or less indifferent between the different brands. Additionally, maybe consumers are more likely to try a product if it is produced by a PFG company. The PFG company does not need to monopolize their market for their market share to increase; the PFG framework merely relies on an increase in sales. In fact, if one percent of consumers shift their purchasing habits, that could have an exceptionally large impact on the value of the PFG company, depending on the market and their initial market share. Additionally, for some brands, donating all of their profits to charities may make their product a status symbol. For instance, Patagonia could market that wearing their products displays to the world you care about the environment and have made a choice to support organizations who share your values.</p><p>&nbsp;</p><p>Regarding whether consumers believe they are implicitly paying someone to donate, it is important for our marketing and outreach teams to clarify only the profit which would have been paid out to other shareholders anyway. This is not an extra cost on the business consumers have to pay for. This may be challenging to communicate, but as the PFG framework develops and spreads, we hope the public will develop a clearer understanding.</p>", "user": {"username": "Brad West"}}, {"_id": "Y2xbKLjEmL6dCd2Z6", "title": "UK government to host first global summit on AI Safety", "postedAt": "2023-06-08T13:24:15.542Z", "htmlBody": "<p>An exciting development in this space, it may be worth people thinking about what to organise around this summit. I've copied the press release in full below.</p><p>----------------------------------------------------------------</p><ul><li>Summit will bring together key countries, leading tech companies and researchers to agree safety measures to evaluate and monitor the most significant risks from AI</li><li>PM and President Biden will take a coordinated approach to the opportunities and challenges of emerging tech when they meet at the White House today</li><li>Global companies are expanding their AI work in the UK, as PM confirms new university scholarships to further UK-US tech leadership</li></ul><p>As the world grapples with the challenges and opportunities presented by the rapid advancement of Artificial Intelligence, the UK will host the first major global summit on AI safety, the Prime Minister has announced today (Wednesday 7 June).</p><p>Breakthroughs from AI continue to improve our lives \u2013 from enabling paralysed people to walk to discovering superbug-killing antibiotics. But the development of AI is extraordinarily fast moving and this pace of change requires agile leadership. That is why the UK is taking action, because we have a global duty to ensure this technology is developed and adopted safely and responsibly.</p><p>Last week dozens of leading experts warned about the potential for AI to endanger humanity in similar ways to pandemics or nuclear weapons.</p><p>In Washington DC today, the Prime Minister will stress the importance of likeminded allies and companies working to develop an international framework to ensure the safe and reliable development and use of AI.</p><p>The summit, which will be hosted in the UK this autumn, will consider the risks of AI, including frontier systems, and discuss how they can be mitigated through internationally coordinated action. It will also provide a platform for countries to work together on further developing a shared approach to mitigate these risks.</p><p>In recent weeks the Prime Minister has discussed this issue with a number of businesspeople and world leaders. This includes all members of the G7 who were united in their ambition to take a shared approach to this issue at the Hiroshima Summit last month.</p><p>In May the PM also met the CEOs of the three most advanced frontier AI labs, OpenAI, DeepMind and Anthropic in Downing Street and the Secretary of State for Science, Innovation and Technology also hosted a roundtable with senior AI leaders. The work at the AI safety summit will build on recent discussions at the G7, OECD and Global Partnership on AI.</p><p>In July the Foreign Secretary will also convene the first ever briefing of the UN Security Council on the opportunities and risks of Artificial Intelligence for international peace and security.</p><p>The UK is well-placed to convene discussions on the future of AI. The UK is a world-leader in AI \u2013 ranking third behind the US and China. Our AI sector already contributes \u00a33.7 billion to the UK economy and employs 50,000 people across the country.</p><p>Our departure from the EU also allows us to act more quickly and agilely in response to this rapidly changing market. The UK was one of the first leading nations to set out a blueprint for the safe and responsible development of AI, which will be adaptive to the speed of advances in this technology. And the UK has launched an expert taskforce to help build and adopt the next generation of safe AI, backed by \u00a3100 million of funding, alongside a commitment to spend \u00a3900 million developing compute capacity, including an exascale supercomputer in the UK.</p><h1>The Prime Minister said:</h1><blockquote><p>AI has an incredible potential to transform our lives for the better. But we need to make sure it is developed and used in a way that is safe and secure.</p><p>Time and time again throughout history we have invented paradigm-shifting new technologies and we have harnessed them for the good of humanity. That is what we must do again.</p><p>No one country can do this alone. This is going to take a global effort. But with our vast expertise and commitment to an open, democratic international system, the UK will stand together with our allies to lead the way.</p></blockquote><p>Last month, OpenAI and Anthropic opened offices in London, with OpenAI appointing UK firm Faculty as their technical integration partner and announcing the expansion of Google Deepmind under the leadership of Demis Hassabis headquartered in King\u2019s Cross.</p><h1>Demis Hassabis, CEO &amp; Co-Founder, Google DeepMind said:</h1><blockquote><p>AI brings incredible opportunities but also challenges for the world, and international cooperation is essential for ensuring this technology is developed safely and responsibly for the benefit of everyone.</p><p>The Global Summit on AI Safety will play a critical role in bringing together government, industry, academia and civil society, and we\u2019re looking forward to working closely with the UK Government to help make these efforts a success.</p></blockquote><h1>Dario Amodei, CEO and Co-Founder of Anthropic said:</h1><blockquote><p>It\u2019s deeply important we make AI safe. There is an enormous amount of work that still needs to be done. So we commend the Prime Minister for bringing the world together to find answers and have smart conversations.</p></blockquote><p>Recognising the strength of the UK\u2019s AI expertise, US tech giant Palantir has also today announced it will make the UK its new European HQ for AI development. Palantir, which already employs more than 800 people in the UK, has provided many of the world\u2019s most critical enterprises and institutions with foundational architecture for data processing.</p><p>Alexander C. Karp, Co-founder and Chief Executive Officer of Palantir Technologies Inc. and chairman of The Palantir Foundation for Defense Policy &amp; International Affairs said:</p><blockquote><p>The ability of institutions to effectively capture the recent advances of artificial intelligence, and in particular large language models, will determine which organizations succeed and ultimately survive over the longer term.</p><p>We are proud to extend our partnership with the United Kingdom, where we employ nearly a quarter of our global workforce. London is a magnet for the best software engineering talent in the world, and it is the natural choice as the hub for our European efforts to develop the most effective and ethical artificial intelligence software solutions available.</p></blockquote><p>Today the Prime Minister will meet President Biden in the White House for wide ranging discussions on the UK-US relationship, in particular how we can work together to strengthen our economies and cement our joint leadership in the technologies of the future.</p><p>The UK and US are two of the only three countries in the world to have a tech industry valued at more than $1 trillion. This is thanks, in part, to the strength of our universities and research institutions \u2013 between us, our countries are home to 7 of the world\u2019s top 10 research universities.</p><p>The Prime Minister will also announce an increase in the number of scholarships the UK Government funds for students undertaking post-graduate study and research at UK and US universities, enhancing our shared expertise in STEM subjects.</p><p>Under the scholarship uplift announced today, the number of Marshall scholarships will increase by 25%, to 50 places a year. The Marshall scheme was established 70 years ago to give high potential Americans the opportunity to study in the UK for two years. Alumni of the programme include two serving Supreme Court Justices, founders of companies including Dolby Labs and LinkedIn, and one Nobel Laureate.</p><p>The UK will also fund five new Fulbright scholarships a year \u2013 up from the 25 currently funded. The Fulbright programme is predominantly funded by the United States to sponsor international students to study in the US and vice versa. Since the programme launched in 1948, around 15,000 British nationals have studied in the US on Fulbright scholarships.</p><p>These new scholarships will focus on STEM-related subjects, boosting the UK and US\u2019 shared expertise in the technologies of the future.</p><h1>Hugh Milward, Vice-President, External Affairs Microsoft UK said:</h1><blockquote><p>The opportunity AI presents us could fundamentally help solve some of society\u2019s greatest problems. But it\u2019s going to require the kind of multi-lateral agreement the Prime Minister is proposing to help create confidence and address the challenges AI also presents.</p></blockquote><h1>Dr Marc Warner, CEO of Faculty, said:</h1><blockquote><p>The potential for this technology is breathtaking but we need to make sure that it\u2019s rolled out in a human first and safe way.</p><p>This will require technological leadership and the ability to foster international collaboration; both of which the UK is perfectly placed to provide.</p></blockquote>", "user": {"username": "DavidNash"}}, {"_id": "izLnDHpDiPidoXCm3", "title": "if you're reading this it's too late (a new theory on what is causing the Great Stagnation)", "postedAt": "2023-06-08T11:54:24.885Z", "htmlBody": "<figure class=\"image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa55b1a18-dc99-46ad-88ff-f9ebb6c2939d_640x640.jpeg\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa55b1a18-dc99-46ad-88ff-f9ebb6c2939d_640x640.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa55b1a18-dc99-46ad-88ff-f9ebb6c2939d_640x640.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa55b1a18-dc99-46ad-88ff-f9ebb6c2939d_640x640.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa55b1a18-dc99-46ad-88ff-f9ebb6c2939d_640x640.jpeg 1456w\"><figcaption>&nbsp;</figcaption></figure><p><i>A sword is against the internet, against those who live online, and against its officials and wise men. A sword is against its false prophets, and they will become fools. A drought is upon its waters, and they will be dried up. For it is a place of graven images, and the people go mad over idols. So the desert creatures and hyenas will live there and ostriches will dwell there. The bots will chatter at its threshold, and dead links will litter the river bed.</i></p><hr><p>One consistent theme of this blog has been the role of the internet and digital technology in what I see as a subtle but pervasive loss of creativity across the arts, science, and technology (manifesting as the so-called Great Stagnation in the latter domains). Here, I want to pull together some of my writing on this subject from various essays and share some recent research/sources that have led me to a new hypothesis for what\u2019s going on here.</p><p>~~~~~</p><p>From \u201c<a href=\"https://www.secretorum.life/p/on-stories-and-histories-q-and-a\"><u>On Stories and Histories: Q&amp;A</u></a>\u201d:</p><p>The crushing excess of new content is especially problematic in this regard. We have less familiarity with the deep history of art and culture than ever before, and this has caused us to lose a kind of instinct for Greatness, an intuition for what makes things stand the test of time. At the same time, most of us also have too much knowledge of our chosen art or discipline and this makes it difficult for us to do the unprecedented, path-breaking work that revolutionizes a field. In the arts, the danger is that everything you make will feel derivative, a pale imitation of yesteryear\u2019s avant garde. In the sciences, an extensive knowledge of the literature will bias you towards conducting incremental gap-filling research instead of the truly innovative research which creates new gaps in the literature. Richard Hamming has a little riff on this in his classic speech, \u201c<a href=\"https://www.cs.virginia.edu/~robins/YouAndYourResearch.html\"><u>You and Your Research</u></a>\u201d:</p><blockquote><p>\u201cThere was a fellow at Bell Labs, a very, very, smart guy. He was always in the library; he read everything. If you wanted references, you went to him and he gave you all kinds of references. But in the middle of forming these theories, I formed a proposition: there would be no effect named after him in the long run. He is now retired from Bell Labs and is an Adjunct Professor. He was very valuable; I'm not questioning that. He wrote some very good Physical Review articles; but there\u2019s no effect named after him because he read too much.</p><p>If you want to think new thoughts that are different, then do what a lot of creative people do\u2014get the problem reasonably clear and then refuse to look at any answers until you\u2019ve thought the problem through carefully how you would do it, how you could slightly change the problem to be the correct one.\u201d</p></blockquote><p>~~~~~</p><p>From \u201c<a href=\"https://www.secretorum.life/p/exploring-the-landscape-of-scientific\"><u>Exploring the Landscape of Scientific Minds (Let My People Go)</u></a>\u201d:</p><p>The internet is usually seen as an unequivocal good for the advancement of science, but I suspect that we do not fully appreciate the psychological costs. I can think of at least three reasons why the internet might have negative effects on creativity and diversity in scientific thinking (note: the first reason has been omitted because it was redundant with the previous excerpt).</p><p>(2) It is too easy to know if your ideas are considered fringe and unusual, or have already been studied and \u201cproved\u201d. This makes people less likely to do the kind of thinking that can overturn conventional wisdom or show something to be true which was highly unlikely to be so, resulting in a sort of global chilling effect on intellectual risk-taking.</p><p>(3) The internet has also had the effect of homogenizing cultures across the world. German, Russia, American, and Chinese cultures are much more similar now than they were 50 years ago (and were much more similar 50 years ago than they were 100 years ago); accordingly, German, Russia, American, and Chinese science (in terms of their organization, goals, norms, values, etc.) are much more similar as well.</p><p>I wonder if the overall effect of the internet has been to create a cognitive environment which is better at producing minds that do&nbsp;\u201cnormal science\u201d (working towards fixed goals within a given paradigm, \u201cpuzzle-solving\u201d as Thomas Kuhn called it) and worse at producing the kinds of minds that do revolutionary, paradigm-shifting science.</p><p>~~~~~</p><p>This last conjecture was developed further in my essay \u201c<a href=\"https://www.secretorum.life/p/exegesis\"><u>Exegesis</u></a>\u201d.</p><blockquote><p>In science the Apollonian tends to develop established lines to perfection, while the Dionysian rather relies on intuition and is more likely to open new, unexpected alleys for research\u2026The future of mankind depends on the progress of science, and the progress of science depends on the support it can find. Support mostly takes the form of grants, and the present methods of distributing grants unduly favor the Apollonian. Applying for a grant begins with writing a project. The Apollonian clearly sees the future lines of his research and has no difficulty writing a clear project. Not so the Dionysian, who knows only the direction in which he wants to go out into the unknown; he has no idea what he is going to find there and how he is going to find it. <i>Defining the unknown or writing down the subconscious is a contradiction in absurdum</i>.</p><p>\u2014 <a href=\"http://stunlaw.blogspot.com/2012/03/dionysians-and-apollonians-letter-from.html\"><u>Letter from Albert Szent-Gy\u00f6rgyi to </u><i><u>Science</u></i></a> (1972)</p></blockquote><p>The <a href=\"https://en.wikipedia.org/wiki/Apollonian_and_Dionysian\"><u>ancient dichotomy</u></a> of the Apollonian (order, logic, restraint, harmony) and the Dionysian (chaos, emotion, intuition, orgiastic revelry) provides the psychological bedrock upon which the higher-level dichotomy of climbing and wandering is built. This points us towards a crucial, perhaps the crucial, difference between our two modes of travel: climbing is about the quantifiable (the logic can be traced, the calculations can be checked, the data can be analyzed) while wandering is about the unquantifiable\u2014ideas, intangible and ineffable, their value, causes, and consequences only knowable in hindsight, if at all (more on this later).</p><p>[\u2026]</p><p>Computers and the internet have been rocket fuel for legibility, easily allowing us to track nearly every aspect of the scientific enterprise.</p><p>I would argue that this intense \u201cmetrification\u201d is responsible, either directly or indirectly, for much of what\u2019s wrong with science<strong>.</strong> Much ink has been spilled on the nature of these issues so I won\u2019t belabor the discussion, but suffice it to say that many of the metrics used to evaluate scientists and their research serve to incentivize conservatism and easily-quantified productivity over risk-taking and the development of new ideas (which, as previously noted, are by their very nature unquantifiable) (See Matt Clancy\u2019s \u201c<a href=\"https://mattsclancy.substack.com/p/conservatism-in-science\"><u>Conservatism in Science</u></a>\u201d). In essence we have created a system where scientists are disincentivized from doing anything that won\u2019t help them publish more research or bring in more grant money: performing replications, sharing data or ideas, mentoring undergraduates, teaching, etc.</p><p>Some of these problems with metrics are due to the unsophisticated use of simple metrics and it should be possible to alleviate these problems with better metrics and greater awareness of their strengths and weaknesses. That being said, there are also some more fundamental issues that arise from the intrinsic nature of metrics and the incentive structures they create. It is easy to say that we need more complex metrics, but greater complexity comes with its own costs (e.g. more data might be needed to develop the new metric, more bureaucracy needed to collect that data, less intuitive metrics are less likely to be understood and correctly applied). Metrics, whether complex or simple, always suffer from the curse of Goodhart\u2019s Law, \u201cwhen a measure becomes a target, it ceases to be a good measure\u201d (see the <a href=\"https://rogersbacon.substack.com/p/eponymous-laws-part-i-laws-of-the\"><u>Eponymous Laws</u></a> series). For example, citation counts may have been a good proxy for a scientist\u2019s influence when we first started using them, but now that scientists are aware of their importance they have become a less effective measure because scientists are incentivized to game the citation counts however they can (publishing in hot research areas, rushing flashy results to publication without careful replication, developing superfluous new terms or metrics in hopes that they will catch on, self-citing). We might also mention here that citation norms themselves represent an implicit epistemological orientation that predisposes one towards incremental research that clearly builds off previous work (i.e. climbing) at the expense of truly novel ideas that might not have clear antecedents (I could provide a citation for this point, but I refuse to do so as an act of civil disobedience). Lastly, it is fair to wonder about the psychological cost of excessive metrification. I\u2019m no expert here, but the awareness that your work is constantly being measured on a variety of dimensions probably isn\u2019t conducive to creativity, job satisfaction, or mental health.</p><p>It might be helpful to make explicit something that has been lurking in the background of this section: the role of the internet in the scientific enterprise. The internet is, I think, usually seen as an unequivocal boon for science as it allows greater access to information and rapid communication between scientists. My contrarianism runs deep, but not quite deep enough to deny that the internet has been, on balance, a good thing for science. But there is a balance, and I don\u2019t think we have fully grappled with the costs of this mass shift in scientific thinking and doing. To put it in the terminology of this essay: the internet has supercharged our ability to climb but subtly (or not so subtly) harmed our ability to wander. In his song \u201c<a href=\"https://www.youtube.com/watch?v=k1BneeJTDcU\"><u>Welcome to the Internet</u></a>\u201d, comedian-troubadour Bo Burnham sings, \u201cCould I interest you in everything all of the time?\u201d; I\u2019m not the first to say it nor will I be the last, but there is something incredibly unnatural about this everything-everywhere-all-the-time state of affairs and it seems more than likely to me that it is warping our creativity and cultural dynamics in a problematic manner. Derek Thompson speculates on this theme in his recent article \u201c<a href=\"https://www.theatlantic.com/ideas/archive/2021/12/america-innovation-film-science-business/620858/\"><u>American is Running on Fumes</u></a>\u201d:</p><blockquote><p>The world is one big panopticon, and we don\u2019t fully understand the implications of building a planetary marketplace of attention in which everything we do has an audience. Our work, our opinions, our milestones, and our subtle preferences are routinely submitted for public approval online. Maybe this makes culture more imitative. If you want to produce popular things, and you can easily tell from the internet what\u2019s already popular, you\u2019re simply more likely to produce more of that thing. This mimetic pressure is part of human nature. But perhaps the internet supercharges this trait and, in the process, makes people more hesitant about sharing ideas that aren\u2019t already demonstrably pre-approved, which reduces novelty across many domains.</p></blockquote><p>~~~~~~</p><p>A more poetic take on this theme in \u201c<a href=\"https://www.secretorum.life/p/nothing#footnote-3-78248104\"><u>Nothing</u></a>.\u201d:</p><p>\u201cThe internet is hell, a fallen realm in which souls are threshed and all that is Good, Beautiful, and True is optimized out of existence.</p><p>The past is denied its usual slip into nothingness, instead becoming trapped in the ever-growing machine-readable databases that provide food for the ravenous algorithms which predict and control our actions with ever-increasing power and precision. Ambiguity and idiosyncrasy will be the first to go, replaced by perfect dichotomy and uniformity; all numbers besides 1 and 0 will cease to exist; grey areas become mythical places like Atlantis or <a href=\"https://en.wikipedia.org/wiki/Hyperborea\"><u>Hyperborea</u></a>. Soon thereafter, the eclipse will be total\u2014<i>The Future</i> as a programmed event, a synthetic remix of the past.</p><p><i>Rejoice! Tonight, we feast on copypasta for evermore!</i></p><figure class=\"image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7aae660-5ab7-4106-a5bb-8122b9b2e94f_596x465.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7aae660-5ab7-4106-a5bb-8122b9b2e94f_596x465.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7aae660-5ab7-4106-a5bb-8122b9b2e94f_596x465.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7aae660-5ab7-4106-a5bb-8122b9b2e94f_596x465.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7aae660-5ab7-4106-a5bb-8122b9b2e94f_596x465.png 1456w\"><figcaption>&nbsp;</figcaption></figure><p>Time itself now enslaved, all possibility of novelty is extinguished. This third rock hanging in endless void become a haunted merry-go-round, a zombie theatre of eternal recurrence.\"</p><figure class=\"image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F08c9e388-dc8d-43f3-8bf7-424017aa2f56_810x1080.jpeg\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F08c9e388-dc8d-43f3-8bf7-424017aa2f56_810x1080.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F08c9e388-dc8d-43f3-8bf7-424017aa2f56_810x1080.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F08c9e388-dc8d-43f3-8bf7-424017aa2f56_810x1080.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F08c9e388-dc8d-43f3-8bf7-424017aa2f56_810x1080.jpeg 1456w\"><figcaption>&nbsp;</figcaption></figure><hr><p>A few distinct but interrelated hypotheses for why the internet/computers stifles creativity were put forth in the preceding excerpts.</p><p>Too much information/knowledge is bad for your imagination. It\u2019s almost impossible for us to now come at any problem with fresh eyes (\u201can extensive knowledge of the literature will bias you towards conducting incremental gap-filling research instead of the truly innovative research which creates new gaps in the literature\u201d).</p><p>Internet-induced cultural homogenization</p><p>The internet makes it too easy for the entire world to criticize radical ideas. People won\u2019t try crazy shit if the threat of criticism and ridicule is too salient. Similarly, \u201cIt is too easy to know if your ideas are considered fringe and unusual, or have already been studied and \u201cproved\u201d. This makes people less likely to do the kind of thinking that can overturn conventional wisdom\u201d.</p><p>The internet increases mimetic pressure and leads to more groupthink (see the Derek Thompson quote above).</p><p>Excessive metrification/hyper-legibility is bad for creativity.</p><p>Something about algorithms disfavoring novelty and making us more predictable or whatever I was going on about in that excerpt.</p><p>A recent study provides another hypothesis (and yes I\u2019m just going to take the result at face value and run with it).</p><p><a href=\"https://psycnet.apa.org/record/2023-16136-001\"><u>Listening prompts intuition while reading boosts analytic thought</u></a></p><blockquote><p>It is widely assumed that thinking is independent of language modality because an argument is either logically valid or invalid regardless of whether we read or hear it. This is taken for granted in areas such as psychology, medicine, and the law. Contrary to this assumption, we demonstrate that thinking from spoken information leads to more intuitive performance compared with thinking from written information. Consequently, we propose that people think more intuitively in the spoken modality and more analytically in the written modality. This effect was robust in five experiments (<i>N</i> = 1,243), across a wide range of thinking tasks, from simple trivia questions to complex syllogisms, and it generalized across two different languages, English and Chinese. We show that this effect is consistent with neuroscientific findings and propose that modality dependence could result from how language modalities emerge in development and are used over time. This finding sheds new light on the way language influences thought and has important implications for research that relies on linguistic materials and for domains where thinking and reasoning are central such as law, medicine, and business.</p></blockquote><p>I\u2019ll focus on science and technology but the story should be similar in the arts and other creative fields. It goes something like this: back in the day, the scientific literature was small enough that you could keep up to date without reading all that much and getting most of your information through in-person discussions with other scientists. This mode of research life gave scientists something like an optimal balance between analytical and intuitive thinking, enabling them to excel at both hill-climbing towards \u201cknown unknowns\u201d and wandering off in search of new \u201cunknown unknowns\u201d. I like the way one anonymous commenter on <a href=\"https://marginalrevolution.com/marginalrevolution/2022/11/listening-speaks-to-our-intuition-while-reading-promotes-analytic-thought.html\"><u>Marginal Revolution</u></a> summarizes the difference between analytical and intuitive thinking:</p><blockquote><p>\u201cIntuitive\u201d I\u2019d think includes that just-out-of-reach place where creativity and knowledge\u2014known things\u2014blur. I guess \u201cpossibilities\u201d is the word.</p><p>\u201cAnalytic\u201d should be more about known things, or at least a great deal of rigor about unknown things. \u201cSolutions\u201d maybe is the word here.</p></blockquote><p>Back to the story: this all started to change during the post-WWII science boom as it became virtually impossible to keep up with the cutting edge unless you were doing massive amounts of reading and writing.</p><figure class=\"image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2cd8b41-7c2b-4e57-a6d8-21b8bee4dc10_1344x960.webp\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2cd8b41-7c2b-4e57-a6d8-21b8bee4dc10_1344x960.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2cd8b41-7c2b-4e57-a6d8-21b8bee4dc10_1344x960.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2cd8b41-7c2b-4e57-a6d8-21b8bee4dc10_1344x960.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2cd8b41-7c2b-4e57-a6d8-21b8bee4dc10_1344x960.webp 1456w\"><figcaption>&nbsp;</figcaption></figure><p>There also were too many scientists now\u2014you could no longer get all of the experts in one field in a room and have them hash out some problem. Similar changes occurred in science education\u2014individual attention from professors and small discussion-based classes were replaced by lecture halls and textbooks. The overall effect was that the entire scientific community shifted towards more analytical modes of thinking/researching, making us really good at incremental progress and worse at revolutionary, paradigm-shifting progress. Some 50 years after this mass psychological shift we are seeing a slowdown in science and technology (the Great Stagnation) because the existing paradigms are starting to become exhausted. The End.</p><p>A recent essay by <a href=\"https://twitter.com/eric_is_weird\"><u>Eric Gilliam</u></a> at his blog FreakTakes, \u201c<a href=\"https://freaktakes.substack.com/p/math-and-physics-divorce-poetry-and\"><u>Math and Physics\u2019 Divorce, Poetry, and the Scientific Slowdown</u></a>\u201d, offers some qualitative evidence for my thesis (he uses it in service of a related but ultimately different thesis).</p><blockquote><p>Many World War II-era physicists were not only practicing researchers in the small and bustling field of early-1900s physics, but continued on as researchers well into the 1970s. As there was an explosion of publications in their field, the researcher accounts I\u2019ve come across do contain complaints from the old-guard researchers.</p></blockquote><p>The old guard did, \u201cgrumble at the changing state of publications and how smaller findings were published more often\u201d, but their primary complaints were about something else:</p><blockquote><p>The growing conference sizes made it much more difficult to keep up with adjacent fields and scientific meetings. Seminars began to cater to narrower and narrower sub-branches of work rather than broad ones. These were the places that many researchers leveraged to actually keep up to date on new work and problems in their fields as well as others.</p></blockquote><p>Richard Feynman, \u201cwho never religiously kept up with the literature\u201d and was an imaginative and intuitive researcher if there ever was one, pointed to the growing conferences sizes as a significant problem during a 1973 interview:</p><blockquote><p><strong>Feynman:</strong>&nbsp;No, they\u2019ve gotten too big. For example, they have parallel sessions which they never had before. Parallel sessions means that more than one thing is going on at a time, in fact, usually three, sometimes four. And when I went to the meeting in Chicago, I was only there two days before I broke my kneecap, but I had a great deal of trouble making up my mind which of the parallel sessions I was going to miss. Sometimes I\u2019d miss them both, but sometimes there were two things I would be interested in at the same time. These guys who organize this imagine that each guy is a specialist and only interested in one lousy corner of the field. It\u2019s impossible really to go \u2014 so it\u2019s just as if you went to half the meeting. Therefore half is not much better than nothing. You might as well stay home and read the reports.</p></blockquote><p>Warren Weaver\u2014physicist, director of the division of natural sciences at the Rockefeller Foundation (1932\u201355), and president of the American Association for the Advancement of Science (AAAS)\u2014lodged a similar complain in the 70s:</p><blockquote><p>This vast organization (the AAAS), with the largest membership of any general scientific society in the world, embraces all fields of pure and applied science. In the earlier days each branch of science conducted intensive sessions at which its own specialized research reports were given, and there was also some mild attempt to organize interdisciplinary sessions of general interest. Then the meetings got so large that they almost collapsed under their own weight. One group after another, first the chemists, then the physicists, the mathematicians, the biologists found it necessary to hold other and separate meetings. Attendance fell and the function of serving as a communication center between the various branches of science became less effective.</p></blockquote><p>Also of interest in Gilliam\u2019s article is this quote about the shift in mathematical thinking that began to take hold in the post-war era:</p><blockquote><p>As physics became messy and muddy, mathematics turned austere and rigid. After the war, mathematicians started to reconsider the foundations of their discipline and turned inside, driven by a great need for rigor and abstraction. The more intuitive and approachable style of the \u201cold-school\u201d mathematicians like von Neumann and Weyl, who eagerly embraced new developments in physics like general relativity and quantum mechanics, was replaced by the much more austere approach of the next generation. (Robbert Dijkgraaf)</p></blockquote><p>Taken together, all of these examples point to the same story: reading is a helluva drug and we\u2019re all addicted. The rapid growth of STEM fields following WWII precipitated a shift towards more reading and less face-to-face discussion which in turn precipitated a shift away from more intuitive styles of research and towards more analytical approaches.</p><p><a href=\"https://twitter.com/a_m_mastroianni\"><u>Adam Mastroianni</u></a> raises a good point: the bigger issue here might be psychological homogeneity\u2014having some scientists that read \u201ctoo much\u201d is probably a good thing, but there needs to be other avenues to scientific success which don\u2019t require you to read quite as much and we seem to have lost that.</p><p>There is another aspect of this problem that I\u2019ll just mention in passing as I\u2019m currently writing another piece which explores the idea in detail: given the incredible amount of time that we spend staring at things which are inches from our face, is it surprising that we\u2019ve become \u201csmall-minded\u201d and are lacking in \u201cvision\u201d and foresight?</p><p>It\u2019s also worth considering how scientific writing has changed over time, something I wrote about at length in \u201c<a href=\"https://newscience.substack.com/p/1f4d1a46-f0de-4e71-894f-26ce7e066b9c\"><u>Research Papers Used to Have Style. What Happened?</u></a>\u201d. To make a long story short: scientists used to write much longer pieces and infused their writing with much more personal style and aesthetic value than they do today. The explosion in scientific literature put pressure on scientists to write with greater concision and clarity and led to almost universal standardization of article formats.</p><figure class=\"image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F772b0205-c57c-4bbe-8f88-93110435da81_640x292.jpeg\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F772b0205-c57c-4bbe-8f88-93110435da81_640x292.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F772b0205-c57c-4bbe-8f88-93110435da81_640x292.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F772b0205-c57c-4bbe-8f88-93110435da81_640x292.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F772b0205-c57c-4bbe-8f88-93110435da81_640x292.jpeg 1456w\"><figcaption><strong>Figure 1.</strong> The percentage of articles in IMRAD (Introduction, Methods, Results, and Discussion) format in four major medical journals. From \u201c<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC442179/\"><u>The introduction, methods, results, and discussion (IMRAD) structure: a fifty-year survey</u></a>\u201d (2004).</figcaption></figure><p>This draining of nearly all style/aesthetic value from scientific writing was really bad for our collective creativity:</p><blockquote><p>Some scientists, for example, may choose to add a dash of humor to their writing. Aside from making the reading experience more enjoyable (not something that should be underestimated given how much scientists read), humor often juxtaposes ideas or introduces a novel way of looking at something commonplace, two things that are wonderful to do when searching for new ideas. And what can be said about humor can be said of other aesthetic qualities as well; infusion of beauty (either in content or language) or emotion may lead an author to develop a new metaphor or consider some minor aspect of a phenomenon in more detail, either of which could provide the seed of a new idea or observation.</p><p>[\u2026]</p><p>The \u201clife of science\u201d (i.e. scientific creativity) demands a balance between the instrumental and the aesthetic in the same way that evolution requires a balance between selection and mutation. Selection improves the average fitness of a population, but drains it of diversity and limits its potential for future adaptation. Mutation increases diversity, but lowers the average fitness of the population.</p><p>Like mutation, aesthetics are diversifying and generative, but generally harmful to clarity and concision. And like selection, restrictions on style and format may improve the average quality of our writing, but at the cost of creative potential.</p></blockquote><p>I\u2019ll leave you with the conclusion of what is (judging by pageviews and controversy generated) still my popular post, \u201c<a href=\"https://www.secretorum.life/p/fuck-your-miracle-year\"><u>Fuck Your Miracle Year</u></a>\u201d.</p><blockquote><p>Do you really want to have an <i>annus mirabilis</i> (miracle year)?&nbsp;</p><p>Delete the draft of that blog post you were writing. The post sucks and no one was going to read it anyways.</p><p>Stop gorging yourself on the internet and its endless buffet of information. Stop masturbating to progress studies porn. Stop reading my blog. Stop masturbating to sex porn while you\u2019re at it.</p><p>Use your imagination, just like they did back in Einstein\u2019s day.&nbsp;</p><p>Fantasize.&nbsp;</p><p>Think.&nbsp;</p></blockquote>", "user": {"username": "rogersbacon1"}}, {"_id": "ciiSYAgXKNpr2fK6Q", "title": "What are your biggest challenges with online fundraising?", "postedAt": "2023-06-08T13:55:12.916Z", "htmlBody": "<p>I'm working on an app that enables charities and their donors to raise money online using customizable collectionboxes. I'd be very grateful if you could share what challenges you face when fundraising online and what are your most effective fundraising methods.</p>", "user": {"username": "Pawe\u0142 Biegun"}}, {"_id": "MFA9jC2daSMnhNryv", "title": "Notes on how I want to handle criticism", "postedAt": "2023-06-08T11:47:32.306Z", "htmlBody": "<p><i>Tl;dr/</i><a href=\"https://forum.effectivealtruism.org/posts/bbtvDJtb6YwwWtJm7/epistemic-status-an-explainer-and-some-thoughts\"><i>epistemic status</i></a><i>: quick post mostly comprised of bullet points.&nbsp;</i></p><p>I have a pretty public-facing role, so I get a fair amount of feedback (both negative and positive) on my work from different people. A while back, I wrote a doc for myself on how I want to respond to this kind of negative feedback (and related stuff). Then I shared the doc with some folks I know and work with to get input on it,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2jlmt2bu3ra\"><sup><a href=\"#fn2jlmt2bu3ra\">[1]</a></sup></span>&nbsp;then never shared it further. There\u2019s been more related&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/fMCnMCMSEjanhAwpM/probably-tell-your-friends-when-they-make-big-mistakes\"><u>discussion</u></a> recently, so I thought I\u2019d share it now.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref66so7ki5n9t\"><sup><a href=\"#fn66so7ki5n9t\">[2]</a></sup></span>&nbsp;I haven\u2019t really updated it except to add the introductory notes (before the first actual section), some links, and a picture.&nbsp;</p><p>Sections:</p><ol><li><a href=\"https://forum.effectivealtruism.org/posts/MFA9jC2daSMnhNryv/notes-on-how-i-want-to-handle-criticism#Notes_on_criticism\">Notes on criticism</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/MFA9jC2daSMnhNryv/notes-on-how-i-want-to-handle-criticism#How_I_want_to_handle_criticism\">How I want to handle criticism</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/MFA9jC2daSMnhNryv/notes-on-how-i-want-to-handle-criticism#What_I_want_to_do_to_shift_towards_handling_criticism_better\">What I want to do to shift towards handling criticism better</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/MFA9jC2daSMnhNryv/notes-on-how-i-want-to-handle-criticism#Bonus__notes_on_how_to_share_criticism_in_ways_that_make_this_process_easier_for_the_one_being_criticized\">Bonus: notes on how to share criticism in ways that make this process easier for the one being criticized</a></li></ol><p>I use the word \u201ccriticism\u201d here for a pretty vague/broad class of things that includes things like \u201cnegative feedback\u201d and \u201cpeople sharing that they think I\u2019m wrong in some important way.\u201d (Other things are also arguably \u201ccriticism,\u201d like people pointing out specific minor errors, but those are easier for me to handle, so I wasn\u2019t really focusing on them when I wrote this.)</p><p>Please <strong>don't interpret this as a request to stop sharing feedback!</strong> Feedback has often helped me improve my work and grow. (I do think there are better and worse ways of sharing feedback, though \u2014 see more below.)</p><h1>Notes on criticism</h1><p><i>These were kind of used like lemmas for the original doc \u2014 claims to help arrive at my ~conclusions</i></p><ul><li>What\u2019s the point of (engaging with) criticism?<ul><li>To improve what I\u2019m working on now</li><li>To learn something I might be able to use in the future</li><li>[Something about supporting communal norms around engaging with good-faith criticism.]</li></ul></li><li>Sometimes criticism isn\u2019t really criticism and shouldn\u2019t be (it could be reframed as a suggestion)</li><li>Criticism can be wrong</li><li>(Fear of) criticism can discourage action in bad ways</li></ul><h1>How I want to handle criticism</h1><ol><li><a href=\"https://www.lesswrong.com/tag/steelmanning\"><u>Steelman</u></a> it before updating on it, <a href=\"https://www.lesswrong.com/posts/MdZyLnLHuaHrCskjy/itt-passing-and-civility-are-good-charity-is-bad%23Things_other_people_have_said\">or</a> try to understand the other person\u2019s point of view if I first want to respond to it.</li><li>Consider it rationally, and avoid over-updating on it. (Criticism can be wrong!)</li><li>Actually listen to it, and&nbsp;<a href=\"https://www.benkuhn.net/abyss/\"><u>stare into the abyss</u></a> if the criticism is potentially scary.</li><li>Welcome it and be grateful for it, and avoid&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/oRx3LeqFdxN2JTANJ/epistemic-legibility\"><u>pre-emptively guarding against it by being vague</u></a>.</li><li>Acknowledge explicitly (to myself and ideally to the person criticizing me) where I was wrong, if I was.</li><li>Avoid feeling like I need to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/HX9ZDGwwSxAab46N9/you-don-t-need-to-justify-everything\"><u>justify everything</u></a> to everyone, especially when this is taking time or energy.</li><li>Avoid letting&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LwRSnvnaFL9eE4yKz/invisible-impact-loss-and-why-we-can-be-too-error-averse#1__Fear_of_criticism_2_\"><u>fear of criticism stop me from doing things I endorse doing</u></a>.</li><li>Interpret it in a productive way \u2014 what should I learn, how should I change my behavior (if at all) \u2014 and avoid immediately judging myself, worrying that it means that I\u2019m terrible in some deep important way. (Related:&nbsp;<a href=\"https://www.lesswrong.com/posts/EEv9JeuY5xfuDDSgF/flinching-away-from-truth-is-often-about-protecting-the\"><u>\"Flinching away from truth\u201d is often about *protecting* the epistemology</u></a>.)</li><li>Relatedly, avoid over-interpreting: avoid thinking that whoever is passing this on must think that I\u2019m terrible in many other ways.</li><li>If it\u2019s very aggressive, get some help dealing with it.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6c5lxwdc5i\"><sup><a href=\"#fn6c5lxwdc5i\">[3]</a></sup></span></li><li>Allow myself to engage with it in healthy ways, e.g. don\u2019t go head-first into the criticism out of a misguided impulse towards ~epistemic bravado.&nbsp;<ol><li>Sometimes I\u2019m emotionally overwhelmed. It\u2019s ok to listen to and process the criticism later. If someone is giving me feedback, I can ask them to write it down and send it a little later. If I have an email, I can snooze it until next week. A method that a friend proposed but that I\u2019ve never tried is asking a friend to read the criticism and to contextualize it for me.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft26hxj4x8r\"><sup><a href=\"#fnt26hxj4x8r\">[4]</a></sup></span>&nbsp;(I assume the process is something like \u201cthis is pretty chill, don\u2019t worry about it,\u201d or \u201chey, this is a criticism of your work on X. It\u2019s pretty harsh. It sounds pretty reasonable, but even if it\u2019s all true, it doesn\u2019t mean you\u2019re bad or anything like that.\u201d)</li></ol></li></ol><p>Things I want to avoid:&nbsp;</p><ol><li><a href=\"https://en.wikipedia.org/wiki/Straw_man\"><u>Strawmanning</u></a> it, or responding to it in its weak (<a href=\"https://forum.effectivealtruism.org/posts/gtivkdZKjFRcvfLQC/butterfly-ideas\"><u>butterfly</u></a>) form and crushing it</li><li>Getting grumpy or defensive</li><li>Over-deferring to people whose opinions I particularly respect (or, potentially worse, to those whose assessment of me matters to me)<ol><li>Or allowing for information cascades. It seems important to try to understand if the criticism is independent or not.</li></ol></li><li>Lumping it in with other kinds of criticism that I\u2019ve heard in the past, which weren\u2019t compelling, and dismiss it all together</li><li>Judging myself quite harshly</li><li>Letting it become an&nbsp;<a href=\"https://medium.com/@robertwiblin/ugh-fields-or-why-you-can-t-even-bear-to-think-about-that-task-5941837dac62\"><u>ugh field</u></a> because I feel bad or guilty about it</li></ol><h1>What I want to do to shift towards handling criticism better</h1><ul><li>List how I want to handle criticism, get input on the list, adapt it, and read it carefully again + iterate [this!]</li><li>Practice<ul><li>Do stuff</li><li>Put my opinions and work out in public</li><li>Get criticism/feedback</li><li>Respond to it and update on it</li><li>Reflect on how I responded to it</li><li>Repeat</li></ul></li><li>Tell myself that I want to be the kind of person who handles criticism well<ul><li>I think that this is a hack that might actually work</li></ul></li><li>Remind myself of what actually matters<ul><li>E.g. random people\u2019s broad opinions of me doesn\u2019t matter that much. My small projects don\u2019t matter that much. My worth isn\u2019t determined by how well I perform in various areas. I want to improve. (Related:&nbsp;<a href=\"https://waitbutwhy.com/2014/06/taming-mammoth-let-peoples-opinions-run-life.html\"><u>Why You Should Stop Caring What Other People Think (Taming the Mammoth) - Wait But Why</u></a>.)</li></ul></li></ul><h1>Bonus: notes on how to share criticism in ways that make this process easier for the one being criticized</h1><ol><li>When possible, reframe things as suggestions or ideas rather than criticism.</li><li>Avoid judgemental language.</li><li>Criticize a product or a way of doing something, not the person.</li><li>Just be kind.</li><li>Explicitly flag the boundaries of the criticism \u2014 say, \u201cI think this part of the project is performing poorly, but these other parts seem useful.\u201d Express agreement or appreciation for something alongside the criticism.&nbsp;</li><li>Flag your uncertainty or how polished the criticism is, and whether you\u2019re deferring to anyone or whether you might be biased in some way.</li><li>If you think the person might be getting lots of criticism, consider offering to share the criticism and offering to share it later (if it\u2019s not immediately action-relevant).</li><li>Share it privately, unless you think it\u2019s important for there to be common knowledge for some reason.</li><li>Remember that even if it doesn\u2019t seem like the person has taken your criticism into account, they might be mulling on it and might e.g. update next time they notice something you pointed out (which they\u2019re now on the lookout for).&nbsp;</li></ol><p>More here (and in other places!):&nbsp;<a href=\"https://forum.effectivealtruism.org/s/dg852CXinRkieekxZ/p/CkikpvdkkLLJHhLXL\"><u>Supportive scepticism in practice</u></a>. I'd be grateful for comments (or messages) that share other resources on this topic that people appreciate. (I might write more on this in the near future.)</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFA9jC2daSMnhNryv/pwu4vt7rm9brjsmp1xvq\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFA9jC2daSMnhNryv/bj3wb5xt0twism1obq6e 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFA9jC2daSMnhNryv/ehnsx3sp4l3obmpjaug2 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFA9jC2daSMnhNryv/wfvhad50owsk1fc5kihp 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFA9jC2daSMnhNryv/jko0qnbcagozhuu71scj 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFA9jC2daSMnhNryv/zeibdthyxai6wq2ejnln 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFA9jC2daSMnhNryv/ffgaidbf98nruisry30g 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFA9jC2daSMnhNryv/zim5set7zfsvaw7fukp0 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFA9jC2daSMnhNryv/widwavvukhywjwd8akek 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFA9jC2daSMnhNryv/r7q2hslwkwuzalkilbi5 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFA9jC2daSMnhNryv/lucqc06amoimy5sob05x 1479w\"><figcaption>Credit: Midjourney</figcaption></figure><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2jlmt2bu3ra\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2jlmt2bu3ra\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Really grateful for the comments people shared!&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn66so7ki5n9t\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref66so7ki5n9t\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I wasn't sure about whether I should share it as a Community post or as a shortform (or at all), and I'd welcome thoughts on this.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6c5lxwdc5i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6c5lxwdc5i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>E.g. you can <a href=\"https://forum.effectivealtruism.org/posts/CAC8zn292C9T5aopw/community-health-and-special-projects-updates-and-contacting-1\">reach out to the Community Health team at CEA</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt26hxj4x8r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft26hxj4x8r\">^</a></strong></sup></span><div class=\"footnote-content\"><p>When I wrote first this and shared it, someone offered&nbsp;to do this for me. &lt;3</p></div></li></ol>", "user": {"username": "Lizka"}}, {"_id": "5Gb6YufcsrpX9oJLv", "title": "An Exercise to Build Intuitions on AGI Risk", "postedAt": "2023-06-08T11:20:31.154Z", "htmlBody": "<p><i>Epistemic status: confident that the underlying idea is useful; less confident about the details, though they're straightforward enough that I expect they're mostly in the right direction.</i></p><p><strong>TLDR:&nbsp;</strong>This post describes a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Pre-mortem\"><u>pre-mortem</u></a>-like exercise that I find useful for thinking about AGI risk.&nbsp;<strong>It is the only way I know of to train big-picture intuitions about what solution attempts are more or less promising and what the hard parts of the problem are.</strong> The (simple) idea is to iterate between constructing safety proposals ('builder step') and looking for critical flaws in a proposal ('breaker step').</p><h1>Introduction</h1><p>The way that scientists-in-training usually develop&nbsp;<a href=\"https://colah.github.io/notes/taste/\"><u>research taste</u></a> is to smash their heads against reality until they have good intuitions about things like which methods tend to work, how to interpret experimental results, or when to trust their proof of a theorem. This important feedback loop is mostly absent in AGI safety research, since we study a technology that does not exist yet (AGI). As a result, it is hard to develop a good understanding of which avenues of research are most promising and what the hard bits of the problem even are.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv93efl14sj\"><sup><a href=\"#fnv93efl14sj\">[1]</a></sup></span></p><p>The best way I know of to approximate that feedback loop is an iterative exercise with two steps: 1) propose a solution to AGI safety, and 2) look for flaws in the proposal. The idea is simple, but most people don\u2019t do it explicitly or don\u2019t do it often enough.</p><p>Multiple rounds of this exercise tend to bring up details about one\u2019s assumptions and predictions that would otherwise stay implicit or unnoticed. Writing down specific flaws of a specific proposal helps ground more general concepts like instrumental convergence or claims like \u2018corrigibility is unnatural\u2019. And after some time, the patterns in the flaws (the \u2018hard bits\u2019) become visible on their own.</p><p>I ran an earlier version of this exercise as a workshop (an important component is to discuss your ideas with others, so a workshop format is convenient).&nbsp;<a href=\"https://docs.google.com/presentation/d/1oczkSdC_GLlTgLaAD78b6m6FGeDCMlWJPIjcarXMhXw/edit#slide=id.g22000a1fd0e_0_224\"><u>Here are the slides</u></a>.</p><h1>The exercise</h1><p>The exercise consists of two phases:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftdpwutpgyf\"><sup><a href=\"#fntdpwutpgyf\">[2]</a></sup></span>&nbsp;a&nbsp;<strong>builder phase</strong> in which you write down a best guess / proposal for how we might avoid existential risk from AGI, and a&nbsp;<strong>breaker phase</strong> in which you dig into the details until you understand how the proposal fails.</p><p>Importantly, in the context of this exercise the only thing that counts is your own&nbsp;<a href=\"https://www.lesswrong.com/tag/inside-outside-view\"><u>inside view</u></a>, that is your own understanding of the technical or political feasibility of the proposal. You might have thoughts like \u201cThere\u2019s smart people who have thought about this much longer than I have, and they think X; why should I disagree?\u201d. Put that aside for now; the point is to develop your own views, and that works best when you don\u2019t think too much about other people\u2019s views except to inform your own thoughts.</p><h2>Builder phase</h2><p>Write down the<i> proposal</i>: a plausible story for how we might avoid human extinction or disempowerment due to AGI.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreff9y00qwi1eh\"><sup><a href=\"#fnf9y00qwi1eh\">[3]</a></sup></span>&nbsp;It doesn\u2019t need to be very detailed yet; that comes in the breaker phase.</p><ul><li>The proposal will look very different depending on the assumptions you\u2019re starting with.</li><li>If you\u2019re relatively optimistic about AGI risk, the proposal might look very much like things continuing to go on the current trajectory. Write down how you expect the future to go in very broad terms: if we build AGI, how do we know it\u2019ll behave like we want it to? If we don\u2019t build AGI, how come?</li><li>If you\u2019re more pessimistic: what\u2019s the most plausible way in which we could move things from their current trajectory? Is it plausible AI labs can coordinate to not build AGI? Or is there a technical solution that seems promising?</li><li>The builder phase is complete when you have a proposal (can be as short as a paragraph) that seems to you like it stands a decent chance of avoiding disaster. The exact probabilities here depend on your overall levels of pessimism; if you\u2019re relatively pessimistic, then a proposal with a 5-10% chance of working is great; if you\u2019re more optimistic, then you can probably find a proposal that you assign a higher chance of working.</li></ul><p>The proposal does not need to be purely technical; e.g. governance approaches are fair game.</p><p><strong>Example builder phase (oracle AI):&nbsp;</strong>Instead of building an \u201cagent AI\u201d that acts in the world, we could build a system that just tries to make good predictions (an \u201coracle\u201d). An oracle would be very useful and economically valuable while avoiding existential risk from AGI, because an oracle has no agency and thus no reason to act against us.</p><p><strong>If you get stuck, i.e. you can\u2019t come up with an AGI safety proposal:</strong><br>(Don\u2019t worry, this is a common problem).</p><p>\u2192 Write down in broad outlines what you expect to happen if we develop AGI. If that inevitably ends badly, start with the breaker phase: describe a failure scenario, then try to find a fix.</p><p>\u2192 Talk to an AGI optimist, if you can find one. If they have an idea that doesn\u2019t seem to you like it has obvious flaws, start with that. Alternatively, look for written proposals like the&nbsp;<a href=\"https://aligned.substack.com/p/alignment-optimism\"><u>OpenAI alignment plans</u></a>.</p><h2>Breaker phase</h2><p>Make the proposal detailed and concrete. Try to find flaws. Adopt a&nbsp;<a href=\"https://www.schneier.com/blog/archives/2008/03/the_security_mi_1.html\"><u>security mindset</u></a> /&nbsp;<a href=\"https://arbital.com/p/AI_safety_mindset/\"><u>AI safety mindset</u></a>.</p><ul><li>Take the part of the proposal that seems intuitively weakest to you (or that you feel most uncertain about) and make it as detailed as possible. You don\u2019t need to accurately predict the future here; just generate any plausible detailed scenario (e.g. about what training methods are used, or what actions the government takes, etc) and see if the proposal works in that scenario.</li><li>If you find you need to stretch and twist the details of the scenario to make the proposal work, that\u2019s a good sign you\u2019ve found a flaw.</li><li>Don\u2019t just generate a list of flaws; in particular don\u2019t list flaws that you think are far-fetched or unlikely. Instead, try to find 1-2 important flaws that actually break the proposal as you\u2019ve currently written it down.</li><li>Keep in mind&nbsp;<a href=\"https://www.schneier.com/blog/archives/2011/04/schneiers_law.html\"><u>Schneier\u2019s law</u></a>:&nbsp;<i>Anyone can create a security system that he or she can\u2019t break. It\u2019s not even hard. What is hard is creating an algorithm that no one else can break.</i> Share the proposal with others and see if they can find flaws.</li><li>The breaker phase is complete when you find yourself a lot more pessimistic about the proposal than when you started (at least about the exact proposal you stated in the builder phase; it\u2019s fine if you\u2019re still overall optimistic about the general idea).</li></ul><p><strong>Example breaker phase (oracle AI):&nbsp;</strong>Let\u2019s say we go ahead and build an oracle AGI. What exactly are we planning to do with this oracle? If the runner-up AI lab builds an agentic AGI 6 months later, their AGI might cause a catastrophe even if we\u2019re careful. It\u2019s not enough for the idea to be safe; it needs to be useful for alignment somehow, or otherwise help us prevent disaster from a competitor AGI. The current proposal doesn\u2019t say anything about how to do that, which is a critical flaw.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdoqrprarn2a\"><sup><a href=\"#fndoqrprarn2a\">[4]</a></sup></span></p><ul><li>(For another approach to finding flaws in the oracle AI proposal, see&nbsp;<a href=\"https://gwern.net/tool-ai\"><u>this article by Gwern on \u2018Why Tool AIs Want to Be Agent AIs\u2019</u></a>)</li></ul><p><strong>If you get stuck, i.e. it seems like the proposal works:</strong></p><p>\u2192 Consider different kinds of ways the proposal might fail. A useful resource here is&nbsp;<a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\"><u>this very appropriately titled essay</u></a>.</p><p>\u2192 Write up your proposal and get others to critique it.</p><p>&nbsp;</p><h2>Iterate</h2><p>If the proposal seemed promising to start with, it\u2019s plausible that a single serious flaw will not be enough to wreck it beyond repair. If you can see a way to adapt the proposal to fix the flaw, go to step 1 and repeat.</p><p><strong>Example fix (Oracle AI):&nbsp;</strong>So we need to adapt the proposal to make sure we can do something useful with the Oracle AI that prevents a less careful competitor lab from causing a disaster. Maybe an oracle can help us by evaluating our plans to convince other companies to not build AGI?</p><p>\u2192&nbsp;<strong>Adapted proposal&nbsp; (Oracle AI 2):</strong> Instead of building an \u201cagent AI\u201d that acts in the world, we could build a system that just tries to make good predictions (an \u201coracle\u201d). An oracle would be very useful and economically valuable while avoiding existential risk from AGI, because an oracle has no agency and thus no reason to act against us. We train the oracle to be good at answering questions such as \u201cwill research program X have catastrophic consequences?\u201d and at evaluating the consequences of actions such as \u201ctalk to person X to convince them they should stop research program X\u201d. The oracle will warn us if another lab gets close to deploying a dangerous AGI, and if so it can tell us how to convince them to stop.<br>&nbsp;</p><p><strong>If you get stuck, i.e. it seems like the proposal is unfixable.</strong></p><p>\u2192 Talk to others about your idea, in particular if you know people who are optimistic about ideas similar to the proposal you\u2019re working with. Send them your notes and ask for opinions.</p><p>\u2192 If that fails: congratulations, you have completed the exercise! Start again from scratch with a new idea :)</p><h1>Details</h1><ul><li>As you iterate through steps 1-2, the proposal will typically accumulate more detail to avoid the flaws you uncover during breaker steps. This is not by itself bad\u2015it\u2019s a sign of progress\u2015but at some point it becomes unrealistic. Once you notice that your proposal starts with \u201cOk, so here\u2019s the 37 hoops that the AGI safety research program needs to jump through in order to stand a chance at all\u201d, it\u2019s probably time to scrap the proposal and start anew.</li><li>In order to have justified confidence in a belief, you need to set things up such that in worlds in which the belief is wrong, you are very likely to end up disbelieving it. In particular:<ul><li>In order to end up correctly pessimistic about AGI risk, you need to&nbsp;<i>really try</i> to find solutions (Builder Phase).</li><li>In order to end up correctly optimistic about a safety proposal, you need to&nbsp;<i>really try</i> to find flaws (Breaker Phase).</li><li>In both cases, an important part of really trying is to share your thoughts with others and to take their feedback seriously.</li></ul></li><li>If you\u2019re doing the exercise right then you will change your mind, or at the very least fill in a lot of missing details in your understanding. If you\u2019re not actively stuck (that is, you\u2019re moving through the steps 1-2 and iterating), but you\u2019re not changing your mind or learning something then you are possibly making one of these mistakes:<ul><li>1) You don\u2019t actually believe in the proposal to begin with, so when you find a flaw you are neither surprised nor did you learn something new.</li><li>2) You don\u2019t really buy the flaws that you find during your breaker phase. For example, it\u2019s easy to fall into the trap of listing many weak or implausible-seeming flaws.</li><li>It\u2019s fine to generally meta-level expect that you\u2019ll find flaws in most proposals you come up with, even if you don\u2019t know the specifics ahead of time. But on the&nbsp;<a href=\"https://www.lesswrong.com/tag/object-level-and-meta-level\"><u>object level</u></a> you should try to write down proposals you actually believe in, i.e. you think have a &gt;50% chance of working (again, this is ignoring considerations like \u201clots of people who have thought about this more than I think the problem is hard, so this proposal is unlikely to work\u201d).</li><li>Even if you end up finding a fix for the proposal, you\u2019ll have changed your mind about the original idea or learned something about which details hold up and which don\u2019t.</li></ul></li><li>A friend told me a story about their research advisor, who likes to say that (paraphrasing)&nbsp;<i>you should think of it as \u201cthe hypothesis\u201d, not \u201cmy hypothesis\u201d; don\u2019t get too attached to ideas</i>. In this post, I\u2019ve taken care to write \u201cthe proposal\u201d rather than \u201cyour proposal\u201d; I recommend you think about it in the same way.</li><li>In the spirit of <a href=\"https://www.cold-takes.com/learning-by-writing/\">Learning by Writing</a>, write down the outputs of this exercise.</li></ul><h1>Resources</h1><h2>Writing on AGI safety</h2><p>If you decide to do this exercise, you\u2019ll probably (depending on how much you\u2019ve already read) find it useful to read other people\u2019s thoughts on the topic. I\u2019ve compiled some resources that you might find useful to read through for inspiration at various points in this exercise. The list is very incomplete - it\u2019s just what I could come up with from the top of my head.</p><p><br>Breakers (criticisms of AGI Safety proposals &amp; arguments for why safety is harder than one might otherwise think):</p><ul><li><a href=\"https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\"><u>AGI Ruin: A List of Lethalities - AI Alignment Forum</u></a><ul><li>And this response&nbsp;<a href=\"https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer\"><u>Where I agree and disagree with Eliezer - AI Alignment Forum</u></a></li></ul></li><li><a href=\"https://www.alignmentforum.org/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment\"><u>On how various plans miss the hard bits of the alignment challenge</u></a></li><li><a href=\"https://www.alignmentforum.org/s/v55BhXbpJuaExkpcD\"><u>This entire thread</u></a></li></ul><p><br>Builders (solution proposals):</p><ul><li><a href=\"https://aligned.substack.com/p/alignment-optimism\"><u>Jan Leike: Why I\u2019m optimistic about our alignment approach</u></a></li><li><a href=\"https://openai.com/blog/our-approach-to-alignment-research\"><u>OpenAI: Our approach to alignment research</u></a></li></ul><p><br>Lists / collections of posts and papers:</p><ul><li><a href=\"https://arbital.greaterwrong.com/explore/ai_alignment/\"><u>https://arbital.greaterwrong.com/explore/ai_alignment/</u></a></li><li><a href=\"https://intelligence.org/category/analysis/\"><u>https://intelligence.org/category/analysis/</u></a></li><li><a href=\"https://www.alignmentforum.org/library\"><u>The Library - AI Alignment Forum</u></a></li><li><a href=\"https://www.alignmentforum.org/s/n945eovrA3oDueqtq\"><u>https://www.alignmentforum.org/s/n945eovrA3oDueqtq</u></a></li><li><a href=\"https://www.alignmentforum.org/allPosts?sortedBy=topAdjusted&amp;timeframe=allTime\"><u>https://www.alignmentforum.org/allPosts?sortedBy=topAdjusted&amp;timeframe=allTime</u></a></li><li><a href=\"https://www.alignmentforum.org/s/v55BhXbpJuaExkpcD\"><u>2022 MIRI Alignment Discussion</u></a></li><li><a href=\"https://www.agisafetyfundamentals.com/ai-alignment-curriculum\"><u>https://www.agisafetyfundamentals.com/ai-alignment-curriculum</u></a></li><li><a href=\"https://www.agisafetyfundamentals.com/ai-governance-curriculum\"><u>https://www.agisafetyfundamentals.com/ai-governance-curriculum</u></a></li></ul><h2>Other writing on how to learn about / work in AGI safety</h2><p>After I wrote this post I noticed that there\u2019s already&nbsp;<a href=\"https://www.alignmentforum.org/posts/yLTpo828duFQqPJfy/builder-breaker-for-deconfusion\"><u>a post by Abram Demski</u></a> that describes basically the same exercise, and later people pointed out to me that John Wentworth runs&nbsp;<a href=\"https://www.lesswrong.com/posts/nvP28s5oydv8RjF9E/mats-models#Game_Tree_of_Alignment\"><u>a similar exercise that is briefly described here</u></a>. Both of those seem worth reading if you want more perspectives on the builder/breaker exercise, as is&nbsp;<a href=\"https://www.alignmentforum.org/posts/EF5M6CmKRd6qZk27Z/my-research-methodology\"><u>Paul Christiano\u2019s post on his research methodology</u></a>.</p><p><br>Neel Nanda has&nbsp;<a href=\"https://www.neelnanda.io/blog/47-inside-views\"><u>a good post on forming your own views in AGI safety</u></a>.</p><p><br>The&nbsp;<a href=\"https://www.alignmentforum.org/posts/PqMT9zGrNsGJNfiFR/alignment-research-field-guide\"><u>MIRI alignment research field guide</u></a> covers some useful basics for doing research and discussion groups with others.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv93efl14sj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv93efl14sj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Of course, AGI safety researchers do build research experience in adjacent fields like deep learning and maths, but there are intuitions and ways of thinking specific to AGI safety that one doesn\u2019t typically inherit from other fields.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntdpwutpgyf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftdpwutpgyf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I adopt the terms \u201cbuilder / breaker\u201d from the&nbsp;<a href=\"https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge\"><u>ELK report</u></a>, though I may not be using the terms in exactly the same way.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnf9y00qwi1eh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreff9y00qwi1eh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If helpful, you can choose a more concrete disaster scenario, such as \u201can autonomous human-level AGI breaks containment\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndoqrprarn2a\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdoqrprarn2a\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'm somewhat dissatisfied with this example because the flaw is obvious enough that there's no need to go into much concrete detail. Usually you'd do more of that, e.g. if the plan is to use the oracle or 'tool-AI' to prevent a dangerous AGI from being built, how exactly might that work?</p></div></li></ol>", "user": {"username": "Lauro Langosco"}}, {"_id": "KBMSJj63nZfsji2wS", "title": "Beware popular discussions of AI \"sentience\"", "postedAt": "2023-06-08T08:57:48.665Z", "htmlBody": "<p><i>Summary:&nbsp;</i>I apply some philosophy of mind to some common mistakes people make about (possible) AI minds. In our current situation (where we are very ignorant of the relationships between the following things), take care to talk about problem-solving ability, goal-orientedness, phenomenal consciousness, self-consciousness, and moral patiency separately, and to watch out for others eliding these concepts under the single word \u201csentience\u201d.<br><br>&nbsp;</p><p><i>Epistemic status</i>: Written quickly. But I did write my PhD on the philosophy of consciousness, so I have subject-matter expertise to draw on in the judgments I express here.&nbsp;</p><p><br>&nbsp;</p><p><i>Thanks to my Arb colleague Gavin Leech for heavily revising this in a way that greatly improved style/readability</i>.&nbsp;</p><p><br>&nbsp;</p><p>&nbsp;</p><hr><p>One thing I see a lot in the media is discussions about when AI will become \u201csentient\u201d. Unfortunately, these discussions are often quite misleading:</p><ol><li>If you read them casually, you\u2019d come away confused about what the word \u201csentient\u201d means.&nbsp;</li><li>You could also come away with the impression that being sentient is closely connected to being very smart and capable. It\u2019s unclear how true this is: I find it plausible (maybe 60% likely) that a non-sentient AI could still be very smart and agentic. I\u2019m also unsure whether training a very smart AI gets you a conscious one by default (maybe 70% that you\u2019d get one by default, and 30% that you wouldn\u2019t). And I think it is very likely (&gt;90%) that a not-very-capable AI could be sentient.&nbsp;</li></ol><p><br>&nbsp;</p><p><i><strong>Confusion about \u201csentient\u201d&nbsp;</strong></i></p><p><br>&nbsp;</p><p>Here\u2019s a definition of sentience (found by googling \u2018are lobsters sentient\u2019) which matches what I thought was common knowledge:<br><br>Sentience is the ability to have feelings, such as pain, distress, or comfort.<br><br>However, popular articles about AI, and the putative experts quoted within them, often use \u201csentience\u201d in very different ways:</p><p>a) \u2018Lemoine says that before he went to the press, he tried to work with Google to begin tackling this question \u2013 he proposed various experiments that he wanted to run. He thinks sentience is predicated on the ability to be a \u201cself-reflective storyteller\u201d, therefore he argues a crocodile is conscious but not sentient because it doesn\u2019t have \u201cthe part of you that thinks about thinking about you thinking about you\u201d.\u2019<br>&nbsp;</p><p>b) The AI community has historically been swift to dismiss claims (including Lemoine\u2019s) that suggest AI is&nbsp;<i>self-aware&nbsp;</i>[my italics], arguing it can distract from interrogating bigger issues like ethics and bias.\u2019&nbsp;</p><p><br>c) \u2018Moral outsourcing, she says, applies the logic of&nbsp;<i>sentience&nbsp;</i>[my italics] and choice to AI, allowing technologists to effectively reallocate responsibility for the products they build onto the products themselves\u2026\u2019</p><p><br>The first two passages here give you the impression \u201csentience\u201d means \u2018having a self-concept\u2019 or \u2018being able to identify your own sensations\u2019. But this is not obviously the same thing as being able to undergo pains, pleasures and other sensations (phenomenal consciousness). The idea that chickens, for example, have pleasures and pains and other sensations seems at first glance to be compatible with them lacking the capacity to think about themselves or their own mental states. In the third passage meanwhile, it\u2019s not entirely clear what \u201csentience\u201d is being used to mean (moral agency and responsibility?), but it\u2019s not \u2018feels pleasures, pains and other sensations\u2019.&nbsp;</p><p>\u201cSentience\u201d as the ability to have pleasures, pains and other sensations is (I claim) the standard usage. But even if I\u2019m wrong, the important thing is for people to make clear what they mean when they say \u201csentient\u201d, to avoid confusion.&nbsp;</p><p>Philosophers of mind usually use the technical term \u201cphenomenal consciousness\u201d to pick out sentience in the \u201ccan experience pleasures, pains and other sensations\u2019 sense. Lots of information processing in the brain is not conscious in this sense. For example, during vision, the information encoded in the retinal image goes through heavy preprocessing you are not aware of, and which isn\u2019t conscious.&nbsp;</p><p>In fairness,&nbsp;<a href=\"https://plato.stanford.edu/entries/self-consciousness/#SelfConsCons\"><u>one tradition</u></a> in philosophy of consciousness holds that the capacity to be aware of your own current experience is a necessary condition of being sentient at all,<br>so-called \u2018higher-order\u2019 theories of phenomenal consciousness and sentience. On these theories, what makes a process phenomenally conscious is that the subject is&nbsp;<i>aware</i> that they are having an experience which contains that information. So, for example, what makes a visual representation of red in the brain phenomenally conscious, is that you\u2019re&nbsp;<i>aware</i> you\u2019re having an experience of red. On this view, only agents who are self-aware can have conscious experiences. So this view implies that only agents who can be self-aware can be sentient in the \u2018experiences sensations, such as pleasure and pain\u2019 sense.&nbsp;&nbsp;</p><p>But whilst this view has some academic endorsement, it is anything like a consensus view. So we shouldn\u2019t talk as if being sentient required self-awareness, without clarifying that this is a disputed matter.</p><p><br><br><strong>Sentience as intelligence</strong></p><p><br>&nbsp;</p><p>Reading newspaper articles on AI, you could also easily come away with the impression that being sentient is closely connected to being smart and capable. In fact, it\u2019s unclear how true this is: I find it plausible (maybe 60% likely) that a non-sentient AI could still be a very smart and agentic. I\u2019m also unsure whether training a very smart AI gets you a conscious one by default (maybe 70% that you\u2019d get one by default, and 30% that you wouldn\u2019t). And I think it is very likely (&gt;90%) that a not-very-capable AI could in principle be sentient.&nbsp;</p><p>Last year, the New York Times published&nbsp;<a href=\"https://www.nytimes.com/2022/08/05/technology/ai-sentient-google.html\"><u>an article</u></a> arguing that current AIs are not sentient and speculating about why some people in tech (allegedly) mistakenly think that they are. However, the discussion in the article ranges beyond sentience itself, to the idea that AI researchers have a long history of overhyping the<i> capabilities</i> of AIs:<br><br><br>\u2018The pioneers of the field aimed to recreate human intelligence by any technological means necessary, and they were confident this would not take very long. Some said a machine would beat the world chess champion and discover its own mathematical theorem within the next decade. That did not happen, either.</p><p>The research produced some notable technologies, but they were nowhere close to reproducing human intelligence.\u2019&nbsp;</p><p><br>The article goes on to discuss worries about AI x-risk, in a context where that arguably at least implies this is part of the same overhyping of AI capabilities:<br><br><br>\u2018In the early 2000s, members of a sprawling online community \u2014 now called Rationalists or Effective Altruists \u2014 began exploring the possibility that artificial intelligence would one day destroy the world. Soon, they pushed this long-term philosophy into academia and industry.</p><p>Inside today\u2019s leading A.I. labs, stills and posters from classic science fiction films hang on the conference room walls. As researchers chase these tropes, they use the same aspirational language used by Dr. Rosenblatt and the other pioneers.</p><p>Even the names of these labs look into the future: Google Brain, DeepMind, SingularityNET. The truth is that most technology labeled \u201cartificial intelligence\u201d mimics the human brain in only small ways \u2014 if at all. Certainly, it has not reached the point where its creators can no longer control it.\u2019<br><br><br>Whilst the article itself does not actually make this claim, reading it could easily give you the impression that there is a tight link between an AI being sentient, and it displaying impressive intellectual capabilities, agency, or the ability to takeover the world.&nbsp;</p><p>In fact however, it\u2019s not obvious why there couldn\u2019t be a highly capable \u2013 even overall superhuman \u2013&nbsp; AI that was not sentient at all. And it\u2019s very likely that even some fairly unimpressive AIs that are a lot less capable than human beings could nonetheless be sentient.&nbsp;<br><br>&nbsp;</p><p><br><i><strong>Why it\u2019s plausible that a non-sentient AI could still be very smart and agentic</strong></i></p><p><br>&nbsp;</p><p>To be smart, an agent just has to be good at taking in information, and coming up with correct answers to questions. To be capable, an agent just has to be both good at coming up with plans (a special case of answering questions correctly) and able to execute those plans.</p><p>We know that information processing can be done unconsciously: plenty of unconscious processing goes on in the brain. So if you want to argue that performing some particular cognitive task requires sentience, you need to explain why unconscious information processing, despite being possible, can\u2019t accomplish that particular task. Likewise, if you want to argue that training an AI to be good at a particular task will&nbsp;<i>probably</i> produce a conscious AI, you need an argument for why it's easier to find a conscious than unconscious way of performing that task.&nbsp;&nbsp;</p><p>It would be nice if we could a) look at what \u201cthe science\u201d says about when information processing is conscious, and then b) check for particularly impressive intellectual tasks whether they could possibly/plausibly be performed without doing the sort of processing the theory says is conscious. If we could do this, we could figure out whether an AI could plausibly do the cognitive tasks needed for something big \u2013 automating science, or perform all current white-collar labour, or attempt to take over the world \u2013 without being conscious.&nbsp;</p><p>Unfortunately, we can\u2019t really do that as \u2013 steel yourself \u2013 people in cognitive science and philosophy don\u2019t agree on what makes some information-processing conscious rather than unconscious. A&nbsp;<a href=\"https://tsc2023-taormina.it/ConsciousnessChallenge.htm\"><u>huge variety</u></a> of academic theories of consciousness get taken seriously by some decent number of philosophers and cognitive scientists. And those theories often imply very different things about which sort or amount of processing is conscious.&nbsp;</p><p>(This is not the only problem: it\u2019s also likely to be a very hard question what sort of processing you need to do to be able to accomplish very broad tasks like \u2018automate science\u2019, or indeed, much more specific tasks. We don\u2019t actually understand everything about how human vision works, for example, let alone all the&nbsp;<a href=\"https://adamkdean.co.uk/posts/gpt-unicorn-a-daily-exploration-of-gpt-4s-image-generation-capabilities\"><u>alternative ways</u></a> tasks currently performed by our visual system&nbsp;<i>could</i> be accomplished.)</p><p>What I can say is: it's not clear to me that any theory of consciousness I know of implies that a very smart AI, capable of doing the vast majority of reasoning human beings perform, would<i> have&nbsp;</i>to be conscious.</p><h3><br><br>Global Workspace theories</h3><p><br>&nbsp;</p><p>According to these theories, consciousness works like this:&nbsp;<br>A mind has lots and lots of different subsystems which process information: for example, they work out what you\u2019re seeing based on features of the retinal image, or they help you choose what action to perform next. But they tend to do their task&nbsp;<i>without checking</i> what\u2019s going on in the other subsystems. Sometimes, however, those subsystems broadcast the result of their information-processing to a \u201cglobal workspace. And all the subsystems do pay attention to information that reaches the global workspace, and make use of it in their own reasoning. When information is broadcast to the global workspace, there\u2019s a corresponding conscious experience. But no conscious experience occurs when the subsystems process information internally.</p><p>Clearly, some kind of argument would be needed before we can conclude that an AI as intelligent as a human, or which is capable of automating science, or which is superintelligent, would have to have&nbsp;<i>this sort</i> of separate subsystems-plus-global workspace architecture.</p><p><i>Maybe</i> it\u2019ll turn out that training AIs to do complicated tasks just naturally results in them being organized in a global workspace plus subsystems way. But even if global workspace theorists turn out to be right that this is how the human mind is organized, its emergence in AI trained to do complex tasks is not guaranteed.</p><p><br>&nbsp;</p><h3>Higher-order theories&nbsp;</h3><p><br>&nbsp;</p><p>Explained above. On higher-order theories, for information-processing to give rise to conscious experience, the subject needs to be in some sense aware of the processing. In other words, the reason, for example, that your visual experience of red is conscious, and not unconscious like much processing of visual sensory input, is that the experiences cause you to realize that it\u2019s an experience of red.&nbsp;</p><p>Clearly, an AI could perform a lot of complicated and impressive thinking without needing to be aware of any particular processing within itself and classify them as conscious experiences of a certain kind. So the mere fact that an AI is smart, and impressive, and super-human in many domains is not evidence that it counts as conscious by the lights of higher-order theories. On the other hand, it is somewhat plausible that in order to be able to \u201ctakeover the world\u201d an AI would need to be self-aware&nbsp;<i>in some sense</i>. It\u2019s hard to engage in sophisticated long-run planning about the results of your actions, if you don\u2019t understand basic facts about what and where you are.&nbsp;</p><p>But \u2018having accurate beliefs about yourself\u2019 and \u2018being able to recognize some internal information-processing episodes as they occur\u2019 are not the same thing. So in order to get from \u2018AI must be self-aware to be a takeover risk\u2019 to \u2018AI must meet the higher-order theory criteria for being conscious to be a takeover risk\u2019, we\u2019d need some kind of argument that you can\u2019t have the former without the latter. This looks non-trivial to me.&nbsp;</p><p>These are not by any means the&nbsp;<i>only&nbsp;</i>theories of consciousness that are taken seriously in cognitive science. But I strongly doubt that many of the other theories taken seriously make it completely obvious that a human-level, or existentially dangerous, or super-human AI would&nbsp;<i>have&nbsp;</i>to be conscious, and it\u2019s not clear to me how many even suggest that it would be more likely to be conscious than not.&nbsp;</p><p><br><br><i><strong>Relatively unintelligent AIs could (in principle) be sentient</strong></i></p><p><br>&nbsp;</p><p>Many people seem to think that building a conscious AI would be super-hard, and something that would probably only happen if the AI was able to do things we find intuitively intellectually impressive, or human-like. I am skeptical: many animals, including for example, chickens are widely perceived as conscious/sentient (that\u2019s chiefly why people object to factory farming them!), and yet are not particularly intellectually impressive or human-like. If chickens can manage this, why not AIs?&nbsp;</p><p>(I don\u2019t have a strong sense of which current academic theories of consciousness are or are not compatible with chickens being conscious, so it\u2019s possible some might rule it out.)</p><p>It might well be possible to construct a conscious AI right now. We could almost certainly construct an AI with a global workspace architecture if we really wanted to now, and global workspace theory implies that such an AI would be conscious. I am currently unsure whether we could construct an AI that counts as conscious by the standards of the best higher-order theories. (This seems a very complex question to me that I would like to see serious philosophical work on.)&nbsp; But \u201cunsure\u201d is a weak argument for \u201cit\u2019s very unlikely we can\u201d.&nbsp;</p><p>More generally, we currently don\u2019t really<i>&nbsp;</i>know what consciousness/sentience is, and probably no one knows every possible way of constructing AIs. So current knowledge doesn\u2019t seem to rule out, or even provide much direct evidence against, the idea that there are sentient AIs in the set of constructible AIs.&nbsp;</p><p>Meanwhile, the fact that it\u2019s at least plausible that relatively unsophisticated animals, like chickens, are sentient, suggests we shouldn\u2019t place a particularly low prior on the idea that it is currently within our capacity to construct a sentient AI, just because we clear do not yet have the ability to construct AIs as generally intelligent as humans. People should avoid saying things which imply that AI consciousness would be a very difficult and super-impressive achievement that we are currently far from reaching.&nbsp;</p><p><br><br><br>&nbsp;</p>", "user": {"username": "Dr. David Mathers"}}, {"_id": "ct3zLpD5FMwBwYCZ7", "title": "EA Strategy Fortnight (June 12-24)", "postedAt": "2023-06-07T23:07:41.142Z", "htmlBody": "<p>Tl;dr: I\u2019m kicking off a push for public discussions about EA strategy that will be happening June 12-24. You\u2019ll see new posts under&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/ea-strategy-fortnight\"><u>this tag</u></a>, and you can find details about people who\u2019ve committed to participating and more below.&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/bafw9z8yvefp8vudpulf\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/pqrogdbgwhd0ryac3eae 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/ndbejzwurpkhz8ap14nu 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/gwxuqrbmnr4qj94ct1bk 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/y59wdwvuyce6zmwik3pw 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/yi6aibpmp0feghof2a4t 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/p3wawc7kanraqdcis5xv 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/p4oi16qxj6gcarqwki9c 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/rgqzti1xederrka9bh1f 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/x6k4gb4lhnnz2gdf68nu 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/oyhoqpouvb40sb1jvjxr 1600w\"></p><h2>Motivation and what this is(n\u2019t)</h2><p>I feel (and, from conversations in person and seeing discussions on the Forum, think that I am not alone in feeling) like there\u2019s been a dearth of public discussion about EA strategy recently, particularly from people in leadership positions at EA organizations.&nbsp;</p><p>To help address this, I\u2019m setting up an \u201cEA strategy fortnight\u201d \u2014 two weeks where we\u2019ll put in extra energy to make those discussions happen. A set of folks have already volunteered to post thoughts about major strategic EA questions, like how centralized EA should be or current priorities for GH&amp;W EA.</p><p>This event and these posts are generally intended to start discussion, rather than give the final word on any given subject. I expect that people participating in this event will also often disagree with each other, and participation in this shouldn\u2019t imply an endorsement of anything or anyone in particular.</p><p>I see this mostly as an experiment into whether having a simple \u201cevent\u201d can cause people to publish more stuff. Please don't interpret any of these posts as something like an official consensus statement.</p><h2>Some people have already agreed to participate</h2><p>I reached out to people through a combination of a) thinking of people who had shared private strategy documents with me before that still had not been published b) contacting leaders of EA organizations, and c) soliciting suggestions from others. About half of the people I contacted agreed to participate. I think you should view this as a convenience sample, heavily skewed towards the people who find writing Forum posts to be low cost. Also note that I contacted some of these people specifically because I disagree with them; no endorsement of these ideas is implied.&nbsp;</p><p>People who\u2019ve already agreed to post stuff during this fortnight [in random order]:</p><ol><li><a href=\"https://forum.effectivealtruism.org/users/habryka\"><u>Habryka</u></a> - How EAs and Rationalists turn crazy</li><li><a href=\"https://forum.effectivealtruism.org/users/maxdalton\"><u>MaxDalton</u></a> - In Praise of Praise</li><li><a href=\"https://forum.effectivealtruism.org/users/michaela\"><u>MichaelA</u></a> - Interim updates on the RP AI Governance &amp; Strategy team</li><li><a href=\"https://forum.effectivealtruism.org/users/william_macaskill\"><u>William_MacAskill</u></a> - Decision-making in EA</li><li><a href=\"https://forum.effectivealtruism.org/users/ardenlk\"><u>Ardenlk</u></a> - On reallocating resources from EA per se to specific fields</li><li><a href=\"https://forum.effectivealtruism.org/users/oagr\"><u>Ozzie Gooen</u></a> - Centralize Organizations, Decentralize Power</li><li><a href=\"https://forum.effectivealtruism.org/users/julia_wise\"><u>Julia_Wise</u></a> - EA reform project updates</li><li><a href=\"https://forum.effectivealtruism.org/users/shakeel-hashim-1\"><u>Shakeel Hashim</u></a> - EA Communications Updates</li><li><a href=\"https://forum.effectivealtruism.org/users/jakub-stencel\"><u>Jakub Stencel</u></a> -&nbsp;EA\u2019s success no one cares about</li><li><a href=\"https://forum.effectivealtruism.org/users/lincolnq\"><u>lincolnq</u></a> - Why Altruists Can't Have Nice Things</li><li><a href=\"https://forum.effectivealtruism.org/users/ben_west\"><u>Ben_West</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/users/2ndrichter\"><u>2ndRichter</u></a> - FTX\u2019s impacts on EA brand and engagement with CEA projects</li><li><a href=\"https://forum.effectivealtruism.org/users/jeffsebo\"><u>jeffsebo</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/users/sofia_fogel\"><u>Sofia_Fogel</u></a> - EA and the nature and value of digital minds</li><li>Anonymous \u2013 Diseconomies of scale in community building</li><li><a href=\"https://forum.effectivealtruism.org/users/lukefreeman\"><u>Luke Freeman</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/users/sjir-hoeijmakers\"><u>Sjir Hoeijmakers</u></a> - Role of effective giving within E</li><li><a href=\"https://forum.effectivealtruism.org/users/kuhanj\"><u>kuhanj</u></a> - Reflections on AI Safety vs. EA groups at universities</li><li><a href=\"https://forum.effectivealtruism.org/users/joey\"><u>Joey</u></a> - The community wide advantages of having a transparent scope</li><li><a href=\"https://forum.effectivealtruism.org/users/jamessnowden\"><u>JamesSnowden</u></a> - Current priorities for Open Philanthropy's Effective Altruism, Global Health and Wellbeing program</li><li><a href=\"https://forum.effectivealtruism.org/users/nicole_ross\"><u>Nicole_Ross</u></a> - Crisis bootcamp: lessons learned and implications for EA</li><li><a href=\"https://forum.effectivealtruism.org/users/oldman\"><u>Rob Gledhill</u></a> -&nbsp;AIS vs EA groups for city and national groups</li><li><a href=\"https://forum.effectivealtruism.org/users/vaidehi_agarwalla\"><u>Vaidehi Agarwalla</u></a> - The influence of core actors on the trajectory and shape of the EA movement</li><li><a href=\"https://forum.effectivealtruism.org/users/renan-araujo\"><u>Renan Araujo</u></a> - Thoughts about AI safety field-building in LMICs</li><li><a href=\"https://forum.effectivealtruism.org/users/chanamessinger\">ChanaMessinger</a> - Reducing the social miasma of trust</li><li><a href=\"https://forum.effectivealtruism.org/users/particlemania\">particlemania</a> - Being Part of Systems</li><li><a href=\"https://forum.effectivealtruism.org/users/jordan-pieters\">jwpieters</a> - Thoughts on EA community building</li><li><a href=\"https://forum.effectivealtruism.org/users/michaelplant\">MichaelPlant</a> - The Hub and Spoke Model of Effective Altruism</li><li><a href=\"https://forum.effectivealtruism.org/users/quadratic-reciprocity\">Quadratic Reciprocity</a> - Best guesses for how public discourse and interest in AI existential risk over the past few months should update EA's priorities</li><li><a href=\"https://forum.effectivealtruism.org/users/olliebase\">OllieBase</a> - Longtermism</li><li><a href=\"https://forum.effectivealtruism.org/users/peter_wildeford\">Peter Wildeford</a> and <a href=\"https://forum.effectivealtruism.org/users/marcus_a_davis\">Marcus_A_Davis </a>- Past and future of Rethink Priorities</li></ol><h2>If you would like to participate</h2><ol><li>If you are able to pre-commit to writing a post: comment below and I will add you to this list.</li><li>If not: you can publish a post normally, and then tag your post with&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/ea-strategy-fortnight\"><u>this tag</u></a>.</li><li>And include the following at the bottom of your post:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk2h52v4o2x8\"><sup><a href=\"#fnk2h52v4o2x8\">[1]</a></sup></span></li></ol><blockquote><p>This post is part of <a href=\"https://forum.effectivealtruism.org/posts/ct3zLpD5FMwBwYCZ7/ea-strategy-fortnight-june-12-24\">EA Strategy Fortnight</a>. You can see other Strategy Fortnight posts <a href=\"https://forum.effectivealtruism.org/topics/ea-strategy-fortnight\">here</a>.</p></blockquote><h2>How to follow posts from this event</h2><p>Posts will be tagged with&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/ea-strategy-fortnight\"><u>this tag</u></a>. As there is no formal posting schedule, you might want to subscribe to the tag to be notified when new posts get made.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/aybkn7k3js3zznrfa8v2\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/hhblbjr9oybgtupys3ty 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/hpbpeuqrdflktxmt4bpz 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/qacpylhlghmxu4vjfuer 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/xwk4otrakr75xuek02rc 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/e7tgtx3zhmg3l3mgqqby 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/l01wwncqtmr5g26znygz 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/qcfvjknushkl9y1szhhi 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/rolomfu3odg8khigxdoi 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/b9cetgf6jnb6dgydqssk 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ct3zLpD5FMwBwYCZ7/t4gcfw7xplg5m5j28fca 985w\"></p><p>If you want to start reading now, the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/building-effective-altruism\"><u>Building Effective Altruism</u></a> tag has a bunch of already-published posts on this subject.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk2h52v4o2x8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk2h52v4o2x8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Thanks to <a href=\"https://forum.effectivealtruism.org/users/vaidehi_agarwalla?mention=user\">@Vaidehi Agarwalla</a> for suggesting people do this</p></div></li></ol>", "user": {"username": "Ben_West"}}, {"_id": "ewvd4mxLCWM8MKi8h", "title": "GiveDirectly Unveils Challenges in Cash Transfer Program, Pledges Solutions to Support Impoverished Communities in the Democratic Republic of Congo: My Two Cents", "postedAt": "2023-06-07T22:22:35.034Z", "htmlBody": "<p>In a<a href=\"https://www.givedirectly.org/drc-case-2023/?utm_source=Sailthru&amp;utm_medium=email&amp;utm_campaign=Future%20Perfect%206/7/23&amp;utm_term=Future%20Perfect\"> recent revelation</a>, GiveDirectly, a prominent nonprofit organization facilitating direct cash transfers to impoverished households, has encountered a case of fraudulent activities within its cash transfer program in the Democratic Republic of Congo (D.R.C.). Although this development is unsettling, GiveDirectly remains dedicated to addressing the issue and implementing solutions to safeguard the well-being of the affected communities.</p><p>The scale of the fraud is deeply concerning, as it directly impacts individuals living in extreme poverty. With over 60 percent of Congolese families trapped in dire circumstances, exacerbated by ongoing security threats such as militia violence in the eastern region, GiveDirectly's program plays a crucial role in supporting these vulnerable populations. This regrettable event necessitates immediate attention to protect the well-being of those impacted.</p><p>GiveDirectly's cash transfer program operates through mobile money technology, enabling funds to be sent and received via a simple SIM card. In the process of ensuring funds reach the intended recipients, the organization encountered a significant breach. Certain staff members took advantage of a special exception granted in remote regions, where proximity to mobile money agents is limited. Unfortunately, instead of delivering the registered SIM cards to the recipients, these staff members replaced them with different SIM cards and kept the originals for themselves. Regrettably, these deceitful actions went unnoticed, leading to cash transfers being initiated to the stolen SIM cards.</p><p>Shockingly, many of these cards were in the possession of dishonest GiveDirectly staff members. With the assistance of external individuals, including mobile money agents and former staff, these complicit parties diverted the funds intended for the recipients. This fraudulent activity continued through subsequent transfers until the organization finally identified the scheme and put an end to the illicit payments.</p><p>This intricate scheme undermined GiveDirectly's fraud prevention system at various levels. Fraud checks were bypassed through the involvement of local staff members embedded within multiple layers of the organization. They colluded to suppress evidence of the fraud, including complaints from families who never received the promised funds. Additionally, they conspired with third-party mobile money agents to transfer funds from the stolen SIM cards, further complicating the investigation.</p><p>Upon discovering signs of fraud in a different program in nearby Ituri, D.R.C., GiveDirectly's internal audit team took immediate action in January 2023. The prompt detection of fraudulent activities in this program led to an immediate halt of all activities to prevent any financial losses. Soon after, ongoing payments in the South Kivu program were also suspended upon detecting indications of similar fraudulent practices. Working closely with the local prosecutor's office, GiveDirectly obtained payment records from the telecom provider, providing concrete evidence that money had been redirected from stolen SIM cards to other accounts.</p><p>In response to this disheartening situation, GiveDirectly has taken significant measures to rectify the issue. A comprehensive turnover of staff within the country office in the D.R.C. has been conducted, and certain staff members and external individuals have been referred to local authorities for further investigation and potential prosecution. Although a fraction of the lost funds have been recovered, GiveDirectly acknowledges that the majority may remain unrecoverable. Nonetheless, the organization is committed to allocating additional resources to compensate the defrauded recipients and fulfill their promise of support.</p><p>GiveDirectly remains dedicated to rectifying the situation and is actively working to regain trust in its operations. Transparency, accountability, and the welfare of the impoverished communities they serve are at the forefront of their efforts. GiveDirectly aims to prevent such incidents from recurring as they strive to provide much-needed support to vulnerable communities. By implementing robust safeguards, learning from the experience, and working hand in hand with local authorities, GiveDirectly is determined to rebuild confidence and continue its vital work in fighting poverty in the Democratic Republic of Congo.</p><p>&nbsp;</p><p><strong>My Two Cents</strong></p><p>GiveDirectly's response to the fraudulent activities within their cash transfer program in the Democratic Republic of Congo is commendable. By publicly disclosing this incident and promptly addressing the problem, they have showcased their dedication to the well-being of the impacted communities. This transparency sets an example for other charitable organizations, encouraging them to prioritize accountability and proactive measures to protect their operations.</p><p>Going forward, GiveDirectly can consider the following approaches as part of their solutions:</p><p>1. Enhanced Staff Vetting: Strengthen the screening process for potential staff members, ensuring thorough background checks and verification of their credentials. This includes validating previous employment, conducting reference checks, and verifying the authenticity of qualifications and certifications.</p><p>2. Internal Controls and Oversight: Enhance the existing fraud prevention system by incorporating additional checks and balances, such as independent verification of recipient information, random audits of cash transfers, and cross-referencing payment records with telecom providers. Implement multi-step approval processes to reduce the risk of employees taking advantage of loopholes in existing policies. Through proactive measures like these, GiveDirectly can ensure the protection of its vulnerable beneficiaries against fraud.</p><p>3. Enhanced Staff Training: Provide comprehensive training to all staff members involved in the cash transfer process, focusing on ethics, fraud prevention, and the importance of transparency. This training should also raise awareness about the potential consequences of fraudulent actions and their impact on vulnerable communities. Regular refresher courses can reinforce the importance of integrity and honesty.</p><p>4. Whistleblower Protection: Establish a secure and confidential reporting system that encourages employees, beneficiaries, and external individuals to report any suspected fraudulent activities without fear of retaliation. Implement whistleblower protection policies and procedures to encourage early detection of fraud and prevent its escalation.</p><p>5. Independent Audits and External Oversight: Engage independent audit firms to conduct regular audits of financial transactions, program implementation, and internal controls. This external oversight can provide an impartial perspective and an additional layer of protection against fraud.</p><p>6. Strengthening Collaboration with Local Authorities: Forge stronger partnerships with local law enforcement agencies, prosecutors, and telecom providers to exchange information and collaborate on investigations. This collaboration can facilitate quicker response times and improve the chances of apprehending individuals involved in fraudulent activities.</p><p>7. Technological Solutions: Explore the application of cutting-edge technologies, like biometric identification, two-factor authentication, and blockchain-based systems, to strengthen the security and traceability of cash transfers. Introduce automated transaction monitoring, real-time alert systems, and predictive analytics to more effectively identify irregularities. By harnessing data-driven tools and partnering with experienced third-party providers, GiveDirectly can detect potentially fraudulent activities at an early phase and promptly address any potential harm.</p><p>8. Conduct Regular Risk Assessments: Regularly assess the organization's exposure to fraud risks and adjust internal controls and policies accordingly. This proactive measure aids in identifying and addressing potential vulnerabilities before they can be exploited.</p><p>9. Community Engagement and Beneficiary Feedback: Establish regular channels for engaging with beneficiaries, seeking their feedback, and addressing their concerns. Actively involving the community in the monitoring and evaluation process can help detect fraudulent activities and ensure transparency.</p><p>By implementing these strategies, GiveDirectly can enhance its operations, discourage fraudulent behavior, and restore trust not only within the Democratic Republic of Congo but also among stakeholders worldwide. Their dedication to openness, responsibility, and ongoing enhancement will facilitate more efficient and meaningful assistance to marginalized communities, establishing a benchmark for the entire humanitarian industry. Furthermore, through collaboration and knowledge exchange with other organizations, GiveDirectly can contribute to a sector that prioritizes the well-being of communities. This collective effort fosters innovation and resilience, leading to effective solutions for vulnerable populations.</p>", "user": {"username": "Vee"}}, {"_id": "yCx3kCReJtucpdd33", "title": "The current alignment plan, and how we might improve it | EAG Bay Area 23", "postedAt": "2023-06-07T21:03:10.946Z", "htmlBody": "<p>I gave a talk at EAG SF where I tried to describe my current favorite plan for how to align transformatively powerful AI, and how I thought that this plan informs current research prioritization. I think this talk does a reasonable job of representing my current opinions, and it was a great exercise for me to write it.</p><p>I have one major regret about this talk, which is that my attitude towards the risks associated with following the proposed plan now seems way too blas\u00e9 to me. I think that if labs deploy transformative AI in the next ten years after following the kind of plan I describe, there's something like 10-20% chance that this leads to AI takeover.</p><p>Obviously, this is a massive amount of risk. It seems totally unacceptable to me for labs to unilaterally impose this kind of risk on the world, from both the longtermist perspective and also from any common-sense perspective. The&nbsp;<a href=\"https://www.planned-obsolescence.org/the-costs-of-caution/\"><u>potential benefits of AGI development are massive</u></a>, but not massive enough that it\u2019s worth accepting massive risk to make the advent of AGI happen a decade sooner.</p><p>I wish I'd prefixed the talk by saying something like: \"This is not the plan that humanity deserves. In almost all cases where AI developers find themselves in the kind of scenario I described here, I think they should put a lot of their effort into looking for alternative options to deploying their dangerous models with just the kinds of safety interventions I was describing here. From my perspective, it only makes sense to follow this kind of plan if the developers are already in exceptionally dire circumstances which were already a massive failure of societal coordination.\"</p><p>So if I think that this plan is objectively unacceptable, why did I describe it?</p><ul><li>Substantially, it's just that I had made the unforced error of losing sight of how objectively terrible a 10-20% chance of AI takeover is.<ul><li>This is partially because my estimate of x-risk from AI used to be way higher, and so 10-20% intuitively feels pretty chill and low to me, even though obviously it's still objectively awful. I spend a lot of time arguing with people whose P(AI takeover) is way higher than mine, and so I kind of naturally fall into the role of trying to argue that a plan is more likely to succeed than you might have thought.</li><li>I realized my error here mostly by talking to Ryan Greenblatt and Beth Barnes. In particular, Beth is focused on developing safety standards that AI labs follow when developing and deploying their AI systems; plans of the type that I described in this talk are probably not able to be robust enough that they would meet the safety standards that Beth would like to have in place.</li></ul></li><li>I think of my job as an alignment researcher as trying to do technical research such that AI developers are able to make the best of whatever empirical situation they find themselves in; from this perspective, it\u2019s someone else\u2019s job to reduce the probability that we end up in a situation where the alignment techniques are under a lot of strain. And so the fact that this situation is objectively unacceptable is often not really on my mind.</li></ul><p>Since this talk, I\u2019ve also felt increasingly hopeful that it will be possible to intervene such that labs are much more careful than I was imagining them being in this talk, and I think it\u2019s plausible that people who are concerned about AI takeover should be aiming for coordination such that we\u2019d have an overall 1% chance of AI takeover rather than just shooting for getting to ~10%. The main levers here are:</p><ul><li><strong>Go slower at the end. A lot of the risk of developing really powerful AI comes from deploying it in high-leverage situations before you\u2019ve had enough time to really understand its properties. I would feel substantially safer if instead of labs&nbsp;</strong><a href=\"https://www.planned-obsolescence.org/continuous-doesnt-mean-slow/\"><strong><u>engaging in explosive economic growth over one year</u></strong></a><strong>, they did this automation of R&amp;D over the course of a decade.</strong></li><li><strong>Have much better arguments for the safety of your AIs</strong>, rather than relying on the coarse empirical evaluations I describe in this talk. This could come by various techniques, e.g.&nbsp;<a href=\"https://www.cold-takes.com/high-level-hopes-for-ai-alignment/#digital-neuroscience\"><u>digital neuroscience</u></a> techniques, or development of techniques where we have theoretical arguments for why they will scale to arbitrarily powerful systems (e.g. the work that ARC does).</li></ul><p>So when reading this, please think of this as a plan designed for fairly dire circumstances, rather than an example of things going \u201cwell\u201d.</p><p>That said, even though the plan I described here seems objectively quite unfortunate, I still think it\u2019s likely that something even riskier will happen, and I think it\u2019s worth putting effort into ensuring that AI developers follow a plan at least as good at this one, rather than entirely focusing on trying to get AI developers to have access to a better plan than this.</p><p>I've recently been spending a bunch of my time (with Ryan Greenblatt and a bit of Nate Thomas) trying to come up with a much more fleshed-out proposal for how we'd recommend aligning really powerful models if we had to do so right now. I'm hoping that in the next month or two we've written something up which is a much better version of this talk. We'll see if that ends up happening.</p><hr><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=YTlrPeikoyw\"><div><iframe src=\"https://www.youtube.com/embed/YTlrPeikoyw\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>You can see the slides for this talk <a href=\"https://docs.google.com/presentation/d/1cJ5layIBRXByjbeHAiXbAvRKRkGqL0gz47nck1QaerQ/edit#slide=id.p\">here</a></p><hr><p>Here's the full transcript with slides:</p><p>&nbsp;</p><p>Good morning, everybody. Well, I would like to talk about what I think we would try to do, ideally, if AI happened very soon, and then talk about how I prioritize alignment research based on this kind of consideration. It's going to be a good time.&nbsp;</p><h1><strong>Introduction</strong></h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/xahkid5shjkfbultjahe\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/sv5wrvxmurctstpmjsrr 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ncpxw0glpoquwtl4ipvm 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/zacnckytzu6zhaalubzu 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/o3gxjruru7vlpitcempo 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/natqtauyrdktbmzgalh6 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/tdgv5fyqgsiq1de623vi 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/pywunuxgyfy6chvm8wbc 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/io9vt7pza2nciukvxq7m 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/jlul8lra8gr8bijorvfb 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/eptvydyor2n9bo25oarl 1600w\"></p><p>Just to summarize before I get into it, what is going to happen in this talk. We're going to have two parts. In the first part, I'm going to be talking about the situation that I'm mostly thinking about when I'm thinking about alignment research and what we need from alignment techniques in this situation. I'm going to say what kind of things I think we, just in practice, would use if we tried to do a pretty good job of it pretty soon. And then how I think those might fail.&nbsp;</p><p>In the second half of this talk, I'm going to talk about how I use thinking about the nearcast to assess how much I like different technical alignment research directions.&nbsp;</p><p>You should submit clarifying questions and complaints to Swapcard as we go, and I will take them as I want to.&nbsp;</p><p>All right, acknowledgements. I'm cribbing heavily from Holden Karnofsky's&nbsp;<a href=\"https://www.alignmentforum.org/posts/vZzg8NS7wBtqcwhoJ/nearcast-based-deployment-problem-analysis\"><u>\u2018deployment problem\u2019 analysis</u></a>, and some of his other writings. He doesn't agree necessarily with stuff that I say, and I disagree with him on some of the stuff he says, so ... That's an acknowledgement, but not a mutual endorsement.&nbsp;</p><p>Thanks to Ajeya [Cotra] and Ryan [Greenblatt] for a bunch of conversations here.&nbsp;</p><p>Oli Habryka had a complaint that I really appreciated about the title of this talk: &nbsp;he complained that it made it sound more like the thing I was proposing is consensus about what we should do for AI alignment, than I think it in fact is. So just to be clear about the extent to which I think this talk is (a) normative versus descriptive and (b) consensus. The things I'm going to claim about what would happen, and what should happen are, I think, not very controversial among people who do empirical AI alignment research because they care about AI takeover at large labs. I think they're a lot more controversial among Alignment Forum users. I think for a lot of this, my best guess is that a lot of the things I think we should do about AI alignment when AGI is near will be quite obvious. And so I'm kind of going to blend between talking prescriptively and descriptively, and hopefully this doesn't induce confusion. Great, let's go.</p><h1><strong>The nearcast</strong></h1><p>Okay, so the nearcast.&nbsp;</p><h2><strong>The situation I\u2019m imagining</strong></h2><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/sxn4zgrnscxh2q2mnqum\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/tqun9oi6hjswzjwmkzex 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/orlsfjg3zy9iwrebicah 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/mlxmblgtldfpoqrwoau2 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/egpvktalsf2u2t3jgfqk 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/vbibm1aigwnsyi1b8avg 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/gpynzxqvt6mz5ql247oj 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/pl3jdxpaeqc1q0svumdx 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/cn4i3wfezgggghlsvvcn 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/kbs62ykdgbm0sjgqhkbq 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/psbtbmwg926ri2opparr 1600w\"></p><p>Here's the situation that I spend my nights worrying about, the situation which I am trying to make go better. I'm interested in the case where there's some company \u2013 Ajeya suggested using the name Magma for this hypothetical company \u2013 that is close to being able to build and deploy AI that if they deployed it, would be a massive deal, would be transformative AI, or TAI as we're going to call it. And they are in a situation, where if they don't deploy their transformative AI, if they don't use it somehow, there is a large risk of a catastrophe caused by AI systems deployed by other actors.&nbsp;</p><p>And so there's this natural spectrum of plans, where one type of plan is: don't use your AIs at all. And you're unhappy with those plans, because if you try one of the plans where you just don't use AI at all, you think you're going to get killed by someone else.&nbsp;</p><p>And then on the other [end of the] spectrum, if you just use AI as much as you possibly could, try to use AI to its fullest potential, in whatever way will cause the AI to be most capable, you're worried that the AI will successfully execute a coup.&nbsp;</p><p>And so there's kind of this trade-off between&nbsp;<strong>inaction risk</strong>, where you don't do anything, and then something really bad happens because of someone else, and&nbsp;<strong>accident risk,</strong> where you used AIs in ways that weren't actually safe.&nbsp;</p><p>One of the big themes of this talk is going to be thinking of plans in terms of spectrums, and thinking of the trade-offs between different places you might end up. I already said the thing about prescriptiveness.&nbsp;</p><p>Maybe one more specific thing to say here about this situation: I think of my job as causing a particular one- or two-year interval to go well. From my perspective, there's going to be this one-year or two-year interval where AI, at the start of those two years, is not very important. And at the end of those two years, it's complete crazy town, and the economy is doubling every month or something.&nbsp;</p><p>From my perspective, my job is not to worry about bad stuff that happens before that two-year period, because I don't think it's that important. I don't think AI is going to mess up the world that drastically before this particular crucial period, where AI becomes extremely powerful. And afterwards, I just think the situation's out of my hands. At the point where the world economy is doubling every month or something, and you know the robots are building all these insane space factories or whatever, I'm just like, \u201cI can't help anymore\u201d. At this point, you know, the AIs are going to be making so many decisions so quickly that I have no chance of keeping up with it. The only chance I have of my values being preserved is if I've already ensured that there are AIs who are taking care of me, like an old person becoming senile or something and slowly losing mental capacities, and having to make sure they have caretakers. In that case, the person's getting weaker, but in this case, it's more like the rest of the world gets crazier.&nbsp;</p><p>So I'm like, my job is just this two-year period. When you mention a problem to me, I'm like, \"Did it happen in that two-year period?\" And if it didn't, I'm like, \"That's someone else's problem.\" And this is, to some extent, me just choosing a thing to focus on. I think there are some concerns that I have about AI before the transformative time. But empirically, that's just not the situation I focus on.&nbsp;</p><h2><strong>What the AI might help us with</strong></h2><p><strong><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/v0lail3hrur1zxe397xi\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/wnaivfh8j0zdnjjt93dx 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/x8nnupqxb1lfwabxgm8z 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/z06dquz8zbrxpmkvcptp 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/akf2y5zrs4qehylax9g7 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/h9gol0hftyzhkzs4aenx 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/w06rxnbqoceblcdvrtrk 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/uqkhlvjq7guxmlh3ujqa 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/fhjawi68piecpdpc33e9 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/gohwj4c6ap1rmr0ie0ho 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/fytw2wau8gulg6o3jiv5 1570w\"></strong></p><p>Why might this company Magma be in a situation where they really think that they need to deploy some AI? So here are some things that they might need to do, that they might not be able to do without getting the AI to help them.&nbsp;</p><p>So one option here is alignment. One obvious thing you might need to do with really powerful AIs is get much better at aligning yet more powerful AIs. I think one possibility here is more like&nbsp;<strong>alignment research</strong>, where you're like\u2026 you know, currently I try to do alignment research. But unfortunately, I can only do so many things in a day. If I had AI assistants who were very competent at helping me with research, my research would proceed a lot faster. And so, alignment research is one kind of thing you might wanna do.</p><p>The other thing that you might wanna do is more like [actually doing alignment], or&nbsp;<strong>alignment contracting</strong>, or something. A lot of work which currently goes into aligning systems is not research, it's more like \u2018reviewing a bunch of things the AI said, and seeing if any of them were bad\u2019, and I think this is not well-described as research. And I think that you probably also want your AIs doing things like reviewing a bunch of plans produced by other AI systems, and seeing whether anything in them seems sketchy or bad.&nbsp;</p><p>Some other things you can imagine your AI helping you with: you can imagine it&nbsp;<strong>helping enforce agreements on how AI is developed</strong> by proposing those to governments or arguing for them, or building technology such that it's easier to enforce them. For example, if you really wanna make it the case that no one is allowed to develop models that are bigger than a certain size without passing some set of alignment evals, one difficulty here is how technically do you ensure that no one has enough compute to train those models? You can imagine wanting your AI to help you with this.&nbsp;</p><p>Another thing you might want powerful AIs to help you with is&nbsp;<strong>making it harder for unaligned systems to wreak havoc</strong>. So the simplest example here is: currently, there are definitely a bunch of computer security problems. Everyone's browsers, everyone's operating systems have a bunch of computer security problems. If an unaligned AI showed up right now, they could pwn everyone, and cause a lot of problems by stealing lots of people's money. It would be much more difficult for an unaligned AI to cause trouble if computer security were much better, and so one thing you might be tempted to do with an aligned AI or with AIs that you could use for whatever purposes you wanted is to try to harden everyone's computer security, such that when other unaligned AIs appear, they have a harder time causing trouble, and cause fewer problems.&nbsp;</p><p><strong>Eating the low-hanging fruit</strong> is kind of another example here. Online poker is a classic way of making money if you're kind of smart and know how to use a computer. I mean, this one has actually mostly already been done by bots. But suppose your threat model was \u2018someone's going to build an unaligned AI, and then it's going to play a bunch of online poker, and make a bunch of money\u2019. One thing we can do\u2026 this example is kind of lame. One thing we could do is use our aligned AIs to play a lot of online poker first,&nbsp; and already eat the low-hanging fruit. It's like the unaligned AI shows up, and it's like \"Oh man, how am I going to make all this money, so that I can buy computers to run more copies of myself, so that I can make paperclips? Oh maybe I should try playing online poker. Oh no, there's already this super competitive market in AIs playing online Poker.\"&nbsp;</p><p>Maybe a better example is other kinds of online assistant things that are more like productive activities in the economy, like proofreading, or writing SEO spam or whatever. I'm just like: if our aligned AIs are already crushing this, making it a super competitive market to do AI-generated SEO spam, the unaligned AIs will have a lot more trouble making money to buy computers given that the prices have already been knocked low. Another thing you can imagine doing is just using powerful systems to help you&nbsp;<strong>detect whether other people are building aligned AIs</strong>. It's just a thing you expect would be easier if you had a bunch of AIs do a bunch of cognitive labor. So the claim here is basically, we might be in a situation, where we have these AIs, if they were aligned we would be able to use them to do stuff like this, and then we would no longer be very worried about X-risk from unaligned AIs, but we aren't totally sure that our AIs are aligned, and we aren't totally sure how to align our AIs, and so we have this trade-off between not doing the stuff that we need to do in order to not get killed by unaligned AIs, and getting killed by our own AIs.</p><h2><strong>The v0 plan</strong></h2><p><strong><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/hpqco6mv6ypr85ht99ly\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/xccon8ysdphlitnx5xuj 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/nk4dbtln32ivyze9d44o 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/xeoanjz9ssphma8jqtnf 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/bzkabgwrsjbepypr4ewi 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/tkeibxqhy7aftlotmwnb 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ztqvf7grd2mzpxgaswks 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/uecm5shecov0cikrxlk5 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/zhux85qnxtydmzphzu8q 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/q3zgbqdqn9ah4neahhxp 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/oblk6ov9to7if66thu6b 1570w\"></strong></p><p>So, let's talk about the v0 version of the plan. In machine learning, as I'm going to assume you know, the basic shtick is: we look for models which do well according to some function evaluating their output. The most obvious way that we would try to build really powerful AGI right now is, you probably train a model on a mixture of tasks. You probably take some task, where it's very cheap to train the model on this task. When you train the model on this task, it learns lots of stuff.&nbsp;</p><p>The classic example we use right now is pre-training on the internet. No one actually really wants an AI that is really good at guessing what the next piece of some text from the internet was. But the internet does have a lot of text on it, and so it's extremely cheap to train it. Per dollar, one of the best ways to make a very capable system that knows a lot of stuff about the world right now seems to be training it on this next token prediction task on the internet.&nbsp;</p><p>But it's almost never going to be the case that the cheapest task to train your model on to make it smart is the same as a task that you care about. So you're almost always going to wanna train your model on a mixture of tasks. One of which is cheap, to produce capabilities with, and the other one of which is what you actually care about doing.&nbsp;</p><p>Right now the way that this plays out is: we pre-train our language models on auto-regressively predicting the internet text and then we fine-tune them based on the extent to which they match human demonstrations of being helpful, or how much humans like the outputs they have. So you know, for instance ChatGPT or Bing's new chatbot, which is named Sydney apparently, were both trained on the whole internet, at which point it is not super useful for very much stuff. And then you give it a bunch of examples of how to be a helpful chatbot, ask it to try to be a helpful chatbot a bunch of times, pay your humans to upvote or downvote whether that was in fact a good answer to the human's question. And then you get this model, that's actually pretty capable.&nbsp;</p><p>It might be the case in general that the task that you used to make your model be smart, and the task that you cared about would be the same task, but from my perspective, that would be a kind of weird coincidence. I just don't really see why it would be the case that these tasks would be the same.&nbsp;</p><p>So, this here is entirely descriptive. I'm just like: look, if AGI was getting really powerful next year, and people were like \"How do we make as much money as possible?\" \u2013 which is a question people love asking \u2013 they would probably use a plan something like this.&nbsp;</p><h2><strong>The basic problems with the v0 plan</strong></h2><p><strong><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/d4k57zqcmcl6ap4tudsj\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/v8g1y92mvxlgybvrod56 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/huepgg95ctjxxph5rkhw 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/bpb3pdhhgz2z9lkz6oqh 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/aktw2nf8kefrgptksuxw 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/yl9vwydqu7hrscvlp2vz 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/c69mnzhbs9uup01hma6d 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/zvj6dt0clj9yspl9y5qf 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/v1a1jstyy9h4ujptyz4h 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/iez02fheiry52lu59gdj 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/bmqldtbfdq2tzll4qfjf 1570w\"></strong></p><p>Here are my problems with this plan. Problems with this plan have to be of the form\u2026 The hypothesis is, the model looks good at the task we care about, but nevertheless something really bad happens. So it has to be the case that the model was looking good according to our evaluations of how well it was doing at that task, but it still led to AI takeover. It's still engaged in doing an AI coup. And it feels to me like there's basically two possibilities for why things went wrong.&nbsp;</p><p>Firstly, perhaps&nbsp;<strong>when we were evaluating it, we were evaluating it poorly</strong>. The answers that it was suggesting - we thought that they were fine answers, but actually they were answers that were manipulating us. Therefore the problem was our evaluation was too weak. The full definition here I would use is: you have a problem if your model knows things about the outcomes of its own actions, that you don't know when you're evaluating it, because if there's a difference of knowledge there, the model is incentivized to do things that manipulate you in sad ways.&nbsp;</p><p>So the other possibility is, maybe even though during training we were very good at evaluating how good the model\u2019s outputs were,&nbsp;<strong>it did something catastrophic in deployment, but not during training</strong>. Holden Karnofsky calls this the&nbsp;<a href=\"https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/\"><u>King Lear problem</u></a>.&nbsp;</p><p>I hadn't actually known anything about the plot of King Lear before this blog post of his, perhaps because I'm, you know, uncultured. But the story of King Lear apparently is in the start of the story, King Lear is this king, and he has these daughters. He is trying to decide which daughter to trust his kingdom to, and two of them are really obsequious. They act as if they'll be really great at being in charge of the kingdom. And this other [daughter] is more honest. The King selects the daughters who are being really obsequious and acting as if they really love him. Then they actually have played him for a fool, and when they have the power, they do stuff he doesn't like.&nbsp;</p><p>The basic point of the story is, suppose you want to assess someone to answer the question, \u201cWhat will they do post-assessment, when you've already entrusted them with lots of power?\u201d It's just structurally quite difficult to answer that kind of question because they have this amazing strategy available to them of \u2018behave really aligned and then do something else when they actually have all the power\u2019. Okay, so that was the v0 plan. I think people would pretty plausibly try this v0 plan. I think they would pretty plausibly notice these problems. We maybe already have noticed some of these problems. The catastrophe one we don't notice super much, because the models aren't smart enough yet to think of doing that, I think. But you know, maybe soon.&nbsp;</p><h2><strong>v1 Plan</strong></h2><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/tzz4bjknfwaesrz2cywm\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/xnp8veuv7gjizrmud3gi 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/h3sadn0zrdfq2b5crwig 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/eyzoj5t7pava1wid9ji4 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/qwkkna1iplxjjttvplrl 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/mpyjfgvecnvkrmirfd6k 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/k777kntyjtv7safwwriu 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/flbvtlg2lh8bgc7b0xls 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/cblqo1m9d3idqrximvby 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/xk1xksm78yevxjlsjv4t 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/dxwrku1oswszq4j55s5q 1600w\"></p><p>So I'm going to talk about the v1 plan, which is the basic improvement on the previous plan. So this is mostly super simple stuff.</p><p>Problem 1: maybe we were evaluating poorly. Proposed solution:&nbsp;<strong>evaluate better</strong>. How might we evaluate better? Well, we're going to have a bunch of AIs around. Maybe we should use those AIs to help us evaluate better. This is a very obvious idea. I think that if no alignment people had pointed out this idea, I think it would've been obvious to approximately everyone. It's like, if you were asking me to evaluate a bunch of AI outputs, it wouldn't even occur to me to not use Google when evaluating whether its answers were good and correct and stuff, because I just think of Googling facts to see if they're true as a basic action that I always take.&nbsp;</p><p>Similarly, my guess is that in the future when people are evaluating how good their AI\u2019s answers are, they're going to be so used to using ChatGPT or whatever to help them answer questions, that of course they're going to use the AIs they currently have to try and do a better job of evaluating the AIs they're currently training. So a bunch of stuff has been done in this space.&nbsp;</p><p>You might be like, \"Great! Is there a theorem, that as long as you have access to the AI you're training while you're training it, everything goes fine?\" And the answer is no. Basically for this kind of plan to work in general, you have to have some guarantees that everything grounds out in a particular way that maybe doesn't ground out in reality. My position here is: I don't know whether the basic question is like, \u201cdo you end up knowing everything you needed to know about the outcomes of the model's actions, given that you're also asking the model questions while you're doing the evaluation?\u201d And my best guess right now is you definitely don't know everything, but you also probably do know a lot of stuff. Is this problem bad enough that you get killed? I'm not sure.&nbsp;</p><p>So to address the like King Lear problems, or the model occasionally doing terrible catastrophes, the most obvious thing from my perspective to try to do is, as well as training the model on a bunch of natural inputs, you&nbsp;<strong>try really hard to construct inputs where the model really thinks that it has a good opportunity to take a catastrophic action</strong>. Basically, for literally the King Lear problem, one thing you can imagine him doing is being like, \"Great, daughters, it's the day where I've transferred the power to you. Congratulations, I'm so excited to see what you do.\" And then he waits a week, during which they don't actually have the power. He's just watching to see what stuff they do, believing they have the power. And then if he's unhappy with the stuff they do, he's like, \"Great, I have assessed that you are bad daughters for being kings.\u201d&nbsp;</p><p>Roughly speaking, I think the same thing is possible with AIs, though it's actually way easier for us, because we control all of their input. They're just programs on our computer, so it's very easy for us to try running, having ... Oh man, there is a certain famous piece of fan fiction where someone asks questions to someone in a whole bunch of different ways, iteratively learning how to get better and better at asking, persuading them to do something, and eventually they persuade them, because of the fact that they were able to use trial and error to ask the questions a whole bunch of times, and I'm just like, we get to do the same thing with our AIs. We get to iterate really hard on trying to come up with inputs where they really think that they can get away with doing a coup right now. And I think that actually probably makes it a lot harder for the AIs to stage coups when they have opportunities to.&nbsp;</p><p>Okay, so again we could ask the theoretical question of: in theory, who wins the game, where I am trying to construct fake situations, where you have the opportunity to do a coup, and you are trying to detect whether a situation is a fake coup opportunity or a real coup opportunity? And unfortunately, because of P /= MP reasons, you win: it's easier to detect whether a situation has been faked, than to construct fake situations.&nbsp;</p><p>And so again we find ourselves in the situation where the alignment problem\u2026 the situation seems like empirically maybe we're just fine, like maybe we win the game of trying to construct realistic coup opportunities, but we don't have theoretical guarantees, and so this rests on empirical contingencies, which is a little regrettable in my opinion.&nbsp;</p><p>Some other stuff we're going to do: super simple stuff.&nbsp;<strong>When you are asking your AI to make decisions that are particularly cruxy</strong>, or the kinds of decisions where bad answers are more plausibly going to lead to catastrophes,&nbsp;<strong>you should consider checking more carefully</strong>. If you were like, \u201cwell it would save me a million dollars to have the AI be in charge of the launch ordering, and it would save me a million dollars to have the AI be in charge of the nuclear launch codes\u201d, I'm just like: maybe you should try putting the AI in charge of the former, and not the latter, on the margin, because it seems like that means there's fewer opportunities to have really bad things happen.&nbsp;</p><p>We should try a bunch of really obvious stuff that you would try if you wanted some humans to not do a coup. I think one kind of underrated strategy along these lines is&nbsp;<strong>whistle-blowing</strong>. I think we should probably, as well as training our model on a bunch of opportunities to start a coup, we should train our model on a bunch of faked situations where an AI coup is in progress: like the AI is like seven steps into taking actions that will lead to an AI coup, or it sees that another instance of itself has done something that might lead to a takeover.&nbsp;</p><p>So one example here is a classic concern I have about my AI is, you know, Magma, or whatever company is doing this, is probably going to be using its AI to write a bunch of software, and do a bunch of stuff in its data centers where it's running the AI, and something which seems really scary is: because the AI is writing so much code, you might have thought that it would be really easy for the AI to insert back doors into that code, insert security vulnerabilities into the code that it is writing, such that it can get out onto the open internet in ways that you didn't think it would be able to do, and you might initially be like \"How could we possibly stop the AI from inserting security vulnerabilities into the code that's being run on this data center?\"&nbsp;</p><p>One thing I would like to do is train it a whole lot on cases where you&nbsp;<strong>give it some code, which is AI-written, that has had subtle security vulnerabilities inserted, and then you train it to point those security vulnerabilities out to you</strong>. It seems quite hard for the AI to coordinate with itself to never point out cases that it notices where it has been in the process of starting a coup. So AI whistle-blowing: you just train the AI to, when it notices that it has been doing takeover-y activities, it sends you an email about them, and I think this might make it pretty hard for it to really get away with it.</p><p>[Buck answers some audience questions]</p><p>Some people asked about why I said two-year-periods as the thing that was my job. I'm just like, \u201cEh, this is just some empirical claim\u201d. I'm just like: I bet that the problems are over roughly that time scale, it's just a guess about takeoff speeds or something. My guess is that the AIs don't have that much takeover capability, two years before the singularity, and they do afterwards, but I'm not super-strongly wedded to this.&nbsp;</p><h2><strong>v1 Plan Prognosis</strong></h2><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/k3i6umwi422pvm8kazla\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/c9qsmkkvbf8ylupy76ib 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/maqsggpyrk4hor2bmoaw 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/zxax2impytqtgx4m2qhq 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ggigw5nkachaehy8xbtk 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/co7trqhscvl1widqr8jp 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/izn1li6xx274m9vac1qm 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/gqdgprfzmbtd51zyo3ze 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/jgbl4i4imswmf9st4i6f 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/y2jniqnbfugwuk0vk0dq 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/sinivkh7ggjx2l6apnds 1600w\"></p><p>Okay, moving forward, my prognosis on this plan, as I basically said, it's kind of like we're summoning unreliable eldritch spirits to harness their labor enough to solve some problem, but we don't want them to stage a coup.&nbsp;</p><p>The version of this that I've been thinking about recently is: we need like a trillion person-years of labor in order to solve our problems, and we have this ability to just summon these eldritch spirits, and a trillion person-years is a lot, you know, like that would require\u2026 you know, there's currently like six billion humans who do work in the world, maybe seven billion humans\u2026 no, there's more like six billion humans who do work, so if we wanted to do a trillion years of work in a year, we'd have to have 12 times the current workforce basically. That's a lot of intellectual labor going on, it just feels that by mass or something, it feels like they should be able to stage a coup, but we also have all these structural advantages, where we get to choose the organizational structure that they're being put in, we get to put them in all these insane training situations, all these insane evaluations.&nbsp;</p><p>I think one of the big questions here is how well these systems coordinate with each other. Currently, I'm actually feeling more optimistic after the last few months. I've been looking at Bing Chat. Bing Chat is kind of helpful (or Sydney as it apparently identifies). So Sydney or Bing Chat, seems like it would be quite helpful. It also just does not seem like it has\u2026 like, that it can keep it together long enough to do a long con, and it really seems to me like the core question here is: can your systems keep it together for long enough to pull a long con, before you have extracted enough labor from them, that you've solved all of your problems.&nbsp;</p><p>And I'm just like, well, Sydney is like a lot weaker than these systems are going to get, and so maybe the situation is going to be like: imagine a version of Sydney, who is a bunch more powerful, pretty plausibly still can't keep it together, and then we have like three months to do, you know, an enormous amount of intellectual labor, sped up enormously by this unreliable assistant. And I'm just like, how fine is this situation?&nbsp;</p><p>Yeah, I don't know. It seems kind of unlikely to me that Sydney coordinates with itself really well. Yeah. And&nbsp;<strong>we don't really have a good argument for why they wouldn't stage a coup, but also it seems like we can probably make their lives pretty hard.</strong></p><p>A controversial take of mine compared to a lot of other alignment people: a lot of people seem to think that the deal is, you wanna make sure that you don't deploy AIs that want to kill you. From my perspective, I don't know if we're going to get that.&nbsp;<strong>I think we're plausibly fine, even if we deploy AIs that kind of wanna kill you, as long as they empirically fail to do so</strong>.</p><p>I mean, if you just look at the world, there's a lot of unpopular people who run countries. Vladimir Putin would lose a physical fight against the population of Russia, right? He maintains power through a complicated structure where a variety of people are individually incentivized, and tested, and it's just hard for anyone to stage a coup.&nbsp;</p><p>There's this book,&nbsp;<a href=\"https://www.goodreads.com/en/book/show/11612989\"><u>The Dictator's Handbook</u></a>, which is about how to prevent coups, and it's all about, you need to have some special forces, you need to separate out the different branches of the military, and you normally wanna make it so your army isn't allowed to enter your capital, and you have to make everyone suspicious of each other and constantly have it so that people in senior leadership roles are being approached by people who are implying that they should start a coup with them, and then you kill everyone who does.&nbsp;</p><p>And I'm just like: empirically many people get by even when leading large nations full of people who kind of hate them, and it's not clear that our situation is way worse than that situation, at least for the time period\u2026like we just have to last in a situation like that for long enough that we can do the tasks that we thought would make the situation better. In my opinion, EA is underweighting how hard this [AI] coup-staging seems to be. I feel like their models would suggest that governments would look really different.&nbsp;<strong>My overall P-doom at this point is like 25%</strong>, so it fluctuates wildly from day to day.&nbsp;</p><p>A point that I wanna make here is: this situation does not seem clearly fine, though it also seems maybe okay. I understand this to be a somewhat controversial opinion compared to a bunch of other alignment people. Yep, that's where I'm at.&nbsp;</p><h2><strong>Far mode vs near mode</strong></h2><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/h5l0tdzp8ibm5dq3joue\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/yfhb4repb248swafwncf 130w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/yin2tfkjtf80m8cgntvy 260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/gvo7eipfopzc6pcyp93b 390w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/kkiugisxob8jcd1kidha 520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ug3ky9ko5gvqfzl1bush 650w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/fx1i5wpujrqrvculucyl 780w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/bbrvbwgv3wlxcooyt4nl 910w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/t2jvkos8c7iz5vmn4gxn 1040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/dzcsbmplxjuhr6p6pvtz 1170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/jag3iqirwxptvi6vonlz 1297w\"></p><p><br>Okay, here's a thing from Twitter the other day from Agnes Callard, the person who talks to Robin Hanson a lot. A problem I have with EAs who talk about alignment\u2026 I'm sure some of you guys understand this picture: the question is \u201cHow do you pack 17 squares into a square?\u201d and the answer is this kind of ugly thing on the right. From my perspective, a lot of people seem to think about AI alignment, and like what you do with your AI systems, and it's kind of upsettingly farmered, where they aren't seriously being like, \"Okay, but how would the AIs kill you?\u201d&nbsp;</p><p>Like they're going to insert these back doors, but you are going to be trying really hard to get AIs to look for back doors\u2026&nbsp; The whole situation seems complicated. A lot of the interventions we can take feel like they're on like spectra, as opposed to \u2018you do this or you don't do this\u2019. I recommend thinking about the situation with a bunch of squares that are all askew. I think I personally regret thinking about it more like the thing on the left in the past.&nbsp;</p><h2><strong>One frame on Magma\u2019s doom minimization problem</strong></h2><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/crfhzeygasviiqezg8jf\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/fjat1elnkkrqdmfzintx 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ez4h8xt2fdqhbhm76lk2 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/shijifaojtdqi6yw3fio 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/jdbhye0ks6rgcxpbi8xg 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/qw9zwvzyvtemqugqtbhz 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/twek64lnzfdndcdfmmta 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/odaykxza30oekivfho2m 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/znbyoyh02vjnc9eezfel 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/rgycrvp9ijk52qyr55n4 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/fnhp9zkjmdw6xjjxrlej 1536w\"></p><p><br>Okay, so one frame on Magma's doom minimization problem is, they're&nbsp;<strong>choosing where to spend their slack</strong>. Magma has some resources, and by assumption, Magma is able to operate somewhat less efficiently, because of the fact that they have alignment concerns. They aren't in this cutthroat race where if they don't use their AIs as capably and as powerfully as possible as soon as possible, they're just immediately going to lose. We assume that they have some surplus that they're able to spend, and the question that they'll be asking themselves, and that we want to make easier for them, is how to make these trade-offs. So I wanna name a bunch of trade-offs that they\u2026 a bunch of options they have.&nbsp;</p><p>So one option: they could&nbsp;<strong>do less in the world</strong>. We've already talked about this one: they could just do less with their AIs, they could&nbsp;<strong>take spare money, and spend it on something totally unrelated to AI</strong>, like paying humans to do computer security research, or to do hardening: this is in the action space.&nbsp;</p><p>They could, instead of training models that are as smart as they would train them if they had no slack\u2026 instead of training models that are the optimal level of intelligence to have them be capable, you could&nbsp;<strong>use dumber models</strong>, even though this means that it's more expensive to use your models because you have to spend more on inference, and they're just less capable.&nbsp;</p><p>You could&nbsp;<strong>use ML techniques, which are worse, but safer</strong> for some reason. You could&nbsp;<strong>spend some of your extra money doing more alignment stuff</strong>; you could spend money on more alignment evaluations of your models, you could spend more on technical alignment research, you could wield the intelligence of your models less efficiently.&nbsp;</p><p>What are my examples here? So by \u2018wielding the intelligence of your models\u2019, I mean doing a worse job of eliciting the latent capabilities of your models. So outcome-based training is where you train your models based on how much you think its actions look good to you. And so suppose you wanna have an AI that gives good answers to questions. One kind of thing you could do is pay humans to write a bunch of answers to questions, and train a model to imitate them. Another thing you could do is train a model to generate a bunch of answers to questions, and then train it based on how much you liked those answers to the questions, and the second kind of training is going to incentivize your model to lie to you in ways that humans wouldn't have lied to you, in a way that the first type doesn't. If you're just imitating humans, your models are not going to have any reason to lie to you because all they've been selected to do is do a good job of imitating the humans, whereas if they're being selected based on whether they answer questions in ways humans approve of, then they are going to, in cases where they can get a better evaluation by saying things that are false, they will be incentivized to do that.&nbsp;</p><p>In general, it's going to be more powerful to train your models in these outcome-based ways, because when you train models in outcome-based ways, this allows you to elicit knowledge the models have that humans don't have, but also it seems scarier. We can make trade-offs like that.&nbsp;</p><p>The next thing I wanna say [is] you can require your models to provide more legible reasoning for doing stuff. You can also&nbsp;<strong>choose tasks carefully</strong>. Instead of deploying your AIs on the things which are the most direct path to solving your problems, you can err on the side of having them do tasks where oversight is easier, or there aren't opportunities for catastrophic actions.</p><p>So for instance, the nuclear war example is an example of this. Maybe you should just not give your AI the nuclear weapon codes, even if it would be kind of convenient to do so, because that just seems like a particularly catastrophe-prone action.&nbsp;</p><h1><strong>Technical alignment research prioritization</strong></h1><p>So given that picture, how do we do good alignment research? Sorry, not how do&nbsp;<i>we</i>, how do&nbsp;<i>I</i> do the alignment research that I empirically do? I'm not sure this strategy is good, but I wanna tell you about how I use the previous reasoning about the alignment situation, to think about what research I should personally do.</p><p>So here's the question that I ask myself. Assume that a lab ends up in the situation described earlier and they're being as smart as possible about handling the alignment problems. How can I produce helpful changes to their alignment plan by doing technical research now?&nbsp;</p><h2><strong>My restricted-scope question about technical alignment research prioritization</strong></h2><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/fhd5diw0vkyuft4kbw2f\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/o3mtyhv1xgonurc29v2h 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/qkvrhjosyl0vt4krruaz 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/nyu6pt3ej9vtldotec5d 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/qbs9zptcg1rplytievqi 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/wun5lv8sc7hgqxnvpmm1 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/kqckgdisi0pjemt6q6m1 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/o4xiwben7n02hezkbbl1 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/xcuwe2u26bn9hqvkajqk 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ijvdndqn91e99be8dxpf 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/tqfemkebc4phtdxzrwdh 1600w\"></p><p><br>&nbsp;</p><p>Let's put that in a picture of a table. We made four assumptions/restrictions of scope. Firstly, we're assuming&nbsp;<strong>a lab ends up in that situation</strong>. The other possibility is that that situation never happens, or some more important thing happens instead.&nbsp;</p><p>We're assuming that&nbsp;<strong>they're being smart about handling alignment problems</strong>. The other possibility is the lab doesn't care about alignment problems because they're silly, in which case perhaps I should be spending my time ensuring the labs do more reasonable things. I assumed&nbsp;<strong>the alignment plan looks similar to what I described</strong> in the nearcast section. The other possibility is it looks very different. And another assumption was&nbsp;<strong>I should try to improve the situation by doing technical research now instead of something else</strong>. This is some mixture of comparative advantage considerations for myself, and thinking that things are actually more important than other things, it's a little hard to say which are which.&nbsp;</p><p>The ones of these that seem like most obviously a bad choice to me\u2026or inasmuch as I'm making a mistake by doing these restrictions, my best guesses as to where I'm making a mistake are, I feel good about the first one, I feel not good about the second one, it seems really plausible that like ... Eliezer [Yudkowsky] has this line about \u2018<a href=\"https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy\"><u>death with dignity</u></a>\u2019, we can talk about \u2018high dignity\u2019 versus \u2018low dignity\u2019 failures. The \u2018high dignity\u2019 failure is: we tried so hard to develop our amazing alignment techniques, and the lab implemented all the amazing alignment techniques and the AI still did a coup, and the \u2018low dignity\u2019 failures are like: we didn't try very hard to develop alignment techniques, and the people who were in the labs completely YOLO-d everything and then we got killed.&nbsp;</p><p>And so these are both, in my mind, plausible stories for how the AI takeover happens, so it seems very plausible to me that I'm making a terrible mistake, by assuming that the labs are going to do reasonable things, and trying to make it so that if they try to do reasonable things, it goes well for them.&nbsp;</p><p>I'm assuming the alignment plan is similar to what I described: I feel pretty good about this. This is another controversial take. I think that the basic idea of just \u2018try to understand whether your AI is doing bad stuff, try to do a really good job of evaluating it, especially in cases where you're going to be very sad if it's actually doing a bad job, and also give it lots of inputs, such that if it was lying in wait hoping to kill you later you would notice, and train against those\u2019... I think that's just a pretty reasonable plan, and I don't feel very bad at all about assuming that.&nbsp;</p><p>And then like, should I be trying to contribute to this situation by doing technical research? I don't know, maybe.&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/txeyo7rixnzysuuvlhfk\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/wgdjszldkcgtewq8wpcp 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/gnmkiukxwzrjfmggblam 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/eraem0rzlfrdie6k4tik 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ozkbaygblskybcyxeder 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/c6oy3nwo0b07inzwiwcc 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/hxltrp88kjfsvzgao528 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/oc2vadeeprjp4avl12ni 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ct2jeblke0snv37pmeo7 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/xfitd5febfbdmrkxgtum 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/jvtrlpkst1auszcyv6j0 1600w\"></p><p>So the framework I'm going to talk about here is not that different, it doesn't lead to very different things from other people. Re bullet point 2, I just personally like thinking about this stuff. It's really not obvious whether in fact my research is better, because I try to ask myself the question like, \u201cis this technique that we're trying to research actually better than stuff which is just going to be in the plan by default?\u201d</p><p>I think if you look at the research that I did last year, it is not substantially better than what it would've been if I spent no time thinking about ... Maybe it's like 2x better. I don't think it's wildly, wildly better, than if I had spent almost no time asking myself what the situation with AGI was going to be like, and just stopped after like one sentence of reasoning.&nbsp;</p><p>If I had said to myself \"Surely it would be nice to know what was happening inside the neural nets\", and then I just tried to do good science on top of that, I think that probably would've done about as well in terms of my actual research output, so I'm definitely making a controversial bet by saying \"You should try to think carefully about what's going to happen with alignment, when you're prioritizing your research.\" I'm going to keep making this bet for the moment, but it does seem like plausibly a mistake.&nbsp;</p><h2><strong>Classes of technical research</strong></h2><p>Okay. So I want to talk about some different classes of technical research, broken down by how you assess a completed research project. And each of these is a different answer to the following question: Why do you believe that this research project is in the direction of producing helpful changes to the plan?</p><p>We're going to have five classes of technical research, okay?&nbsp;</p><h3><strong>Ready to go</strong></h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/fzblbvktkvhmnocr8fke\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/t6dkfglktaxsqq58xk4n 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/eauj50grudqlma5v8dto 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/sjeprwdvbhh3bj8nw5sz 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/bc1aje5ouac9jtgw1izr 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ys0iovbinkmrqmt0x2z5 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ommblthxlpsw88s9lhnj 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/g98jbfkzncs2ay4rcv5d 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/eivs2hzktivv4otoflq8 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/adjfohd5qkyemiantmyn 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/juahuhfjbrkdc13utsug 1600w\"></p><p>Number (1), \u2018Ready to go\u2019: why do you believe that this research project is in the direction of producing helpful deltas to the plan? Well, the technique we developed here has now been demonstrated to work, and it's just obviously applicable. So now that I've written this paper, if we had to align an AGI right now, we would just definitely try using this technique that I just developed. So obviously it's relevant.</p><p>This is the \u2018ready to go\u2019 category. I'm like: the reason that you research [this] is that you just think it's ready to go to be used to align some AGIs. Examples of things along these lines: the critiques research at OpenAI where they train AIs to point out problems with the outputs of other AIs \u2013 that just seems like an obvious kind of thing you wanna be able to do for alignment. That's category one.&nbsp;</p><h3><strong>Worst-case techniques<img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/kknanuqoaum9fmw4qgng\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/c6rrb49c0qyfmuo2mnzw 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/mqbnuhbs4rqu6wklbuik 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/xbpujmgpnu4unoyzk8fj 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ichidfbkmlbztzk0tvpt 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/xu6dqapuhhtpxx3i9qtv 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/w4pazwou45dltwfuss2n 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/rzizgvejynatvqilro9l 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/vnqaik3pbts9jgjmkqoq 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/x0amlchlcuaiidaveqv8 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/drnn5xg8h9wbuiwmowh9 1600w\"></strong></h3><p>Category (2), \u2018worst-case techniques\u2019. Why do you believe this is in the direction of producing helpful deltas to the plan? Because if we completed development of this technique, it would work regardless of empirical contingencies. So this is a very different argument for why you think it's going to be relevant: the argument is because we just literally didn't rely on any factual predictions about the world that might or might not be true. We just relied on \u201cwe're using machine learning\u201d or something.&nbsp;</p><p>In my opinion, the majority of value that is produced in the worst-case techniques department is from ARC theory. Yep. That's my Clause (2).&nbsp;</p><h3><strong>Gain of function</strong></h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/tcgseydvnhli8cgjme0n\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/hupptjdjzs6bqtfhkle5 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/sul6bnuz7lfotghcjynr 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/fjj89byaj1iehjmsoh8h 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/omg8pobfd3sahydtuolv 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/aa4oetak45dm6nhusgy5 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/pgpyqa60ledygnbul8xp 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ng5y3tkfr6pfymnsx1il 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/fpxpanyvd65aqygqyju0 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/pscbqebqwvtv8pnvslpx 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/fngzpubdvxlyaxwipdto 1323w\"></p><p>Clause (3), gain of function research. So gain of function research is: you try to make empirical demonstrations of things that are as close as possible to the reason why we're scared about AI takeover eventually. Like you try to develop situations in which your AIs do scary misaligned stuff. Currently the people who are doing stuff most closely related to this are the ARC Evals people, though Ethan Perez also does some stuff on this.</p><p>The reason why this will be super helpful is, if we just had empirical demonstrations of our models doing things that feel very closely analogous to the reason we're worried they might stage a coup later\u2026 I mean it just seems really obviously helpful for a variety of reasons. We'd answer a bunch of questions about what types of failure modes seem most likely. The one that really brings joy to my heart is: we'd be able to do empirical research where we get to evaluate our research based on whether it fixed something which was a close demonstration of an actual alignment problem in an actual model, as opposed to what we do right now, which is having to give worse analogies.</p><p>So I'm like, it would be really great, if we had empirical demonstrations of all these failure modes. Unlike the other classes, this gain of function research stuff does not cash out in a technique. I sometimes use this metaphor of training wheels, like we wanna know if it's physically possible to ride a bike, and currently we've just got this dude laying on the ground, and you've got a bike, and you put the man on the bike, and he just topples over, and you're like, \"Well, the first thing I'm going to do is I'm going to stick on some training wheels that are very close to the ground\", and then you put the dude on the bike, and tell him to pedal, and he pedals forward a little bit or whatever with the training wheels, and you're like, \"Well, we haven't established that it's physically possible for this dude to ride a bike, but we have done one of the intermediate steps: if he couldn't do this, he definitely can't ride a bike.\"&nbsp;</p><p>And so I think of this as, you have a sequence of training wheel distances down: there's a sequence of different things that all have to happen for your models to successfully stage an AI coup.&nbsp; They have to want to, they have to be able to, they have to be able to hold it together long enough to not just tell you that they're in the middle of staging a coup. And I'm just like, currently we haven't seen many demonstrations of many of these things, though increasingly many in the last few months \u2013 thanks Sydney.&nbsp;</p><p>And I'm just really excited for, at every point in the future, having demonstrated the failures with the tallest training wheels that we can make happen at that moment. That great, that's gain of function research.</p><h3><strong>Low-hanging fruit</strong></h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/fojz8rsahwwhacbwzxay\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/yd58wdryqb8ywsm7znts 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/zv0oivfhq6fkdktkh2uv 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/iouzn1yzrz4luakkefqm 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ufnpe7cgpgoctamltlaz 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/a5p0dzeqxnz3jnnjw81f 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/um6eifscdf4upoviu0ag 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/bpbty0hzfocdrsyi3xrj 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/gzcpeckivjrxgquipbmu 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/hrrxtymvnytdlqaqxuqh 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/lvirxsiscbp9ojuq5lvh 1600w\"></p><p>Number (4): low-hanging fruit. Why do you believe that this research project is in the direction of producing helpful deltas to the plan? Well, I don't have a really great story, but it seems kind of relevant, and is cheap and easy. From my perspective, this is what the ML academics just do all the time. They're just like, what's some research that's kind of easy, and has keywords in common, with the problem that I eventually want to solve?&nbsp;</p><p>I'm like, no shame in that: I regret not being more persuaded by the benefits of doing stuff that's easy, over the last few years. But the key difference between this and the other ones is like, you just really emphasize doing stuff that seems easy. So an example of this is in Collin Burns' \"<a href=\"https://arxiv.org/abs/2212.03827\"><u>Discovering Latent Knowledge</u></a>\" paper, he has this thing where he is looking for a direction in activation space, and from my perspective, that technique\u2026 like there's no particular principled reason to think that this would work when other techniques wouldn't, but who knows, it's some stuff which is kind of different to other stuff. Seems good to try out. It only took like one grad student a few months or something, so it seems like it's worth the money.&nbsp;</p><h3><strong>Big if true</strong></h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/dvbvd56wuytvj0ilhhq5\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/cjar2h4snm8scawzstn7 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/jirkubx2srgqbtxaq6og 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/cr6m18g1g1hqkpcpaanf 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/j8id2yqxiubzyi0dw1pc 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/cfcv3847emihfeze58so 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/pognsf9g7twp5zdblph7 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/je3b1uejqtenyfy0tkj3 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/mvof9zcqbfn0duzx5agy 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ugptt3zv4kjkn79ydhgc 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/tmdqf6bxrsohaiwtw9z9 1600w\"></p><p>Okay, classes of technical research, 5 out of 5: big if true. In this case, the answer is: it sort of seems like it ought to be possible to do this, and it would be super great if it worked. So this is how I think about almost all of the research on the internals and models, which is, for example, interpretability and some other things.&nbsp;</p><p>Basically we're just like: interpretability, it doesn't work in the worst case, it doesn't help us right now, like we couldn't use it for anything if we had to align an AGI tomorrow \u2013 maybe a slightly controversial hot take. It's not helping us develop better threat models and it's not easy, it's hard, and expensive, but if it worked, man, that would be sweet.&nbsp;</p><p>Just intuitively, it would be so good if you could edit and read the internal states of your models, and perhaps we're so excited by that sweet, sweet possibility, that it's worth doing this research, even though it has all those other downsides. So that's basically how I think about model internals research, like interpretability.</p><p>So by \u2018model internals research\u2019, I mean anything where you take advantage of the fact that your model is just on your computer, and you can read any of its internal states, you can do interventions, you can just edit or whatever you want with the internal states of the model while it's running.&nbsp;</p><p>Currently there isn't very much useful that we do in practice, using this capacity that you would think in theory would be super helpful, but maybe one day there will be something useful. I often think of this as \u2018model neuroscience\u2019, it's like if you wanna understand a human brain, one thing you can do is cut up the skull, and peek at it or something\u2026</p><p>You might think that neuroscience would be really helpful to prevent lying. One thing you might notice about this is that this does not seem to be true of people who wanna prevent coups in practice right now. My understanding is that people who currently wanna prevent coups in fact get very little value out of neuroscience, they don't even have truth serum.&nbsp;</p><p>This is an example where some people wanna prevent a coup, and you might be like, \"Well, why would this situation be different with AIs, than it is with humans?\" And it's like, well, human neuroscience is way more of a pain than AI neuroscience, because the things are just literally already on your computers, and you\u2019d really think you would be able to do something with it.&nbsp;</p><p>So unfortunately, the thing which I crave is an example of taking this \u2018big if true\u2019 kind of research, and converting it into something which is ready to go. Like converting it into a technique, such that if we wanted to build AGI next year, we would be able to use it, and it would contribute. I would be thrilled if that happened. Unfortunately it has never happened, but maybe this year, you never know.&nbsp;</p><h2><strong>What the current portfolio should be</strong></h2><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/zufdokgfnj3mk8emagvk\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/e6rxrpausa9od78zkmtl 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/jxfgl2ezl8sgdznhewvw 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/acqxwfrkffhsk2hv0suq 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/mdvmvox7er2giebfy5ci 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/w1owonqybnldmgp6wcvf 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/azbosra2mqbe4eygy0nv 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/i8x7xhnkjjtoe7mdhcm0 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/wen9zhgcl4riauz0deux 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/wrtfb1goj4q9ljbfb2c9 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/ctwwydh9aebzbm8a7auk 1600w\"></p><p>If I got to pick the current portfolio of alignment research, here's what I'd pick. We've split it up by gain of function versus anything else, because the gain of function isn't directly leading to new alignment technical research. Those are my numbers.</p><h2><strong>How will this change between here and AGI?</strong></h2><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/elbtzf5wjmsoaqveacfr\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/hvqrmi5ujezln7tr2pdg 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/o6lidmjuwblowgy7scks 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/kngd8ocnjbmsnzlougjk 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/dvm4vptuojqv9fo6xp2z 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/iyrkwbtfekazeyzv8mal 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/sk28dcqsdv8blexyrnr9 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/frlgcqlohwv2mmhbivbo 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/urjbousb6m5vtcbz81pn 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/zrxs0wxmqdnkwbnyh3vv 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/yCx3kCReJtucpdd33/hvrzowl1byhak6apdtd8 1600w\"></p><p>How do we think this will change between here and AGI? My best guess as to how this changes between here and AGI is basically: eventually the gain of function people will succeed, and then we'll have all these great empirical alignment failures, at which point the low-hanging fruit stuff is going to look way better. Currently when the ML academics write these random papers about random unprincipled techniques, I look at it and I'm like, \"Well, okay, I can kind of see why you think that would be helpful\", but actually I have no evidence that it is. The demonstration that this is relevant is super sketch. But if we had actual demonstrations of alignment failures, I think that just making a big bag of tricks, where you just try unprincipled things, and see which ones work empirically would look a lot more promising to me.</p><p>So I'm very excited for the gain of function stuff, mostly because it enables the low-hanging fruit stuff. Also probably we're going to run out of time to do worst case-y stuff, and big if true stuff. So that's going to be exciting.&nbsp;</p><p>Let me think about how I wanna conclude this. The way that I personally engage with prioritizing alignment research in both my own research, and when thinking about how I would advise other people to do their research, is by thinking about what I think would happen, if we had to align an AGI right now, and then thinking about what we could do now that would produce a helpful delta to that plan. And I've laid out a number of different possibilities for what that might look like. And again, I wanna say, I think thinking through all this stuff for me has not clearly panned out. Frankly I'm very proud of my alignment takes, but the \u2018take to value production pipeline\u2019 has not yet been proven, and I think it's like very plausible that in a year I'm going to be like: damn, I was just wasting time thinking in advance about how we're going to align this AGI. We should just have been way more empirical, we should just been like, let's just do some vaguely reasonable looking stuff, and then got into it later. That's my talk.&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "Buck"}}, {"_id": "MwfMqx7EoPjsqtdYK", "title": "Understanding how hard alignment is may be the most important research direction right now", "postedAt": "2023-06-07T19:05:15.904Z", "htmlBody": "<p>[Epistemic status: some fairly rough thoughts on alignment strategy. I am moderately confident (60%) that these claims are largely correct.]</p><hr><p>As of today, most alignment research is directed at solving the problem of aligning AI systems through either:</p><ul><li>Using AI to supervise and evaluate superhuman systems (scalable oversight).</li><li>Trying to understand what neural nets are doing internally (mechanistic interpretability).</li><li>Fundamental research on alignment theory.</li></ul><p>Ultimately, most research is trying to answer the question \u201chow do we align AI systems?\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsd70vwql84\"><sup><a href=\"#fnsd70vwql84\">[1]</a></sup></span>. In the short term however, I claim that a potentially more important research direction is work to answer the question \u201chow hard is (technical) alignment?\u201d.&nbsp;</p><p>In what follows, I will take estimating the following quantity as a suitable proxy<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref50stquc6tdp\"><sup><a href=\"#fn50stquc6tdp\">[2]</a></sup></span>&nbsp;for \u201chow hard is alignment?\u201d, and will refer to this quantity as the <i>alignment tax</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefprokhm5j3up\"><sup><a href=\"#fnprokhm5j3up\">[3]</a></sup></span>:</p><blockquote><p>\u201cnumber of years it would take humanity to build aligned AGI in a world where we do not build AGI until we are &gt;95% confident it is safe\u201d / \u201cnumber of years it would take humanity to build AGI in the world as it exists today\u201d</p></blockquote><h3>Claim 1: there is currently significant uncertainty over the difficulty of the alignment problem, and much of that uncertainty is Knightian</h3><p>Between the AI capabilities and alignment communities, (implied) estimates for this quantity differ significantly:</p><ul><li>A subset of the ML community (albeit a decreasing subset) largely dismisses alignment as a concern or believe it to be essentially trivial, corresponding to a tax of roughly 1.</li><li>Frontier labs (such as Deepmind, OpenAI) appear to be firmly in the \u201creal but tractable\u201d range, which I\u2019d map to a tax somewhere around 1.1-1.5.</li><li>Many researchers at institutions such as MIRI appear to place significant credence on a tax &gt;=2, with a fat tail.</li></ul><p>This uncertainty largely stems from different researchers having different models through which they view the alignment problem, with fairly little direct empirical support for any specific model. I interpret Eliezer\u2019s model on the difficulty of alignment as stemming from a mixture of heuristics on the failure of gradient descent to learn policies where alignment capabilities generalise reliably out of distribution along with heuristics regarding the extent to which progress across many domains and at many levels is discontinuous, with these heuristics heavily influenced by the example of natural selection. I see the latter point re discontinuity as being one of the main disagreements between him and e.g. Paul Christiano.&nbsp;</p><p>The key point is that we do not currently know which world we are in. A priori, it is not reasonable to rule out an alignment tax of &gt;=2. Moreover, discussion on alignment has historically <a href=\"https://twitter.com/janleike/status/1659293309704228864\"><u>neglected</u></a> the degree to which we are uncertain, with frontier labs (an exception being <a href=\"https://www.anthropic.com/index/core-views-on-ai-safety\"><u>Anthropic</u></a>) proposing <a href=\"https://openai.com/blog/governance-of-superintelligence\"><u>approaches</u></a> to AGI development that largely ignore the possibility we are in worlds where alignment is hard (I extend this criticism to both the subset of the ML community that predicts alignment being trivial, and also to doomers like Eliezer who\u2019s confidence seems unjustifiable under <a href=\"https://www.lesswrong.com/posts/tG9BLyBEiLeRJZvX6/communicating-effectively-under-knightian-norms\"><u>reasonable Knightian norms</u></a>).</p><h3>Claim 2: reducing this Knightian uncertainty may be significantly easier than solving alignment, even in worlds where alignment is hard</h3><p>Previous efforts to identify cruxes or testable predictions between models that predict alignment being hard vs alignment being tractable have <a href=\"https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty\"><u>not</u></a> <a href=\"https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer\"><u>been</u></a> particularly successful.</p><p>Despite this, a priori I expect that attempts to reduce this uncertainty will have a reasonable chance of success. In particular, in worlds where alignment is hard I only see the difficulty of alignment as very weakly correlated with identifying that alignment is hard. While many of the heuristics that predict alignment being easy or hard necessarily reason about minds that are at least as intelligent as humans (and these intuitions are informed by analogies to human evolution), my prior is that with work to sufficiently flesh out and formalise these heuristics it will be possible to make testable predictions about specific sub-human systems.</p><p>Moreover, in worlds where alignment is extremely hard I expect that it will be easier to find stronger formalisms of such heuristics<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyfbs8x9cco\"><sup><a href=\"#fnyfbs8x9cco\">[4]</a></sup></span>, and as such it will either be possible to prove stronger results in fundamental alignment theory, or easier to demonstrate specific failure modes empirically.&nbsp;</p><p>Importantly, this claim is distinct from the claim that it will be easy to reduce uncertainty through empirical evidence on whether a <i>given </i>alignment solution is sufficient. I find it plausible that for a given alignment solution (e.g. more advanced versions of RLHF) failure will only appear close to the subhuman/superhuman boundary \u2013 indeed, this is a large component of why alignment may be hard \u2013 because alignment techniques will likely use human input (such as human feedback) which will strongly incentivise good behaviour at the subhuman level. A priori, the underlying reasons for misalignment \u2013 <a href=\"https://arxiv.org/abs/1906.01820\"><u>failure to generalise out of distribution</u></a>, <a href=\"https://www.lesswrong.com/tag/sharp-left-turn\"><u>sharp left turns/ phase changes</u></a>, deception etc. \u2013 are failure modes that may appear in toy subhuman systems. It is by understanding and observing whether these proposed failure modes are real that we may be able to gather empirical evidence on the difficulty of alignment.</p><h3>Claim 3: research in this direction may look different to existing research</h3><p>Most research today is not directly answering this question, although much research today does make progress in this direction:</p><ul><li>Interpretability and scalable oversight may make it easier to identify specific failure modes empirically, which in turn may inform our view of the difficulty of alignment.</li><li>Progress on alignment theory may be useful for formalising specific intuitions.</li><li>Approaches such as ARC evals may also empirically demonstrate failure modes.</li></ul><p>However, I fundamentally see existing approaches as not working to directly attack the question of how hard alignment is (which isn\u2019t surprising, since they are not trying to answer this question).</p><p>At a high level, I see research in this direction as looking like a mixture of:</p><ul><li>Formalising heuristics that predict different difficulties of the alignment problem so testable, empirical predictions can be made. Ultimately, the goal would be to identify as many cruxes as possible between models that make differing predictions regarding the difficulty of alignment.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvh7n9r1z0i\"><sup><a href=\"#fnvh7n9r1z0i\">[5]</a></sup></span></li><li>Demonstrating specific failure modes, such as trying to <a href=\"https://www.lesswrong.com/posts/QvvFRDG6SG3xZ8ELz/challenge-construct-a-gradient-hacker\"><u>construct gradient hackers</u></a>; or demonstrating sub-human systems that display increasingly egregious forms of mesa-optimisation (e.g. deceptive alignment); or demonstrating sharp left turns in toy models. More generally, I see this work as largely attempting to increase the evidence base that informs heuristics regarding the difficulty of alignment to more than just natural selection.</li></ul><h3>Claim 4: resolving this question has significant strategic implications</h3><p>Progress on this question has two strategic implications:</p><p>1. It informs what humanity should do:</p><blockquote><p>In worlds where alignment is easy and there is common knowledge that we have a high (&gt;95%) chance of succeeding, there is a strong argument to continue current attempts to build AGI given the baseline probability of other x-risks over the next century.</p><p>In worlds where we know with high confidence that alignment is hard, existing approaches are not just unlikely to succeed \u2013 instead they are suicidal. Knowing that alignment is intractable over the next ~50 years would completely shift the set of feasible governance options \u2013 in these worlds, despite the difficulty of global coordination to stop the development of AGI, it would be the only remaining option. We would be able to eliminate investing resources/political capital into approaches that only stall<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefenni12klskj\"><sup><a href=\"#fnenni12klskj\">[6]</a></sup></span>&nbsp;AGI development from the set of policy options under consideration, and instead know that we need to aim for actions that halt AGI development long term \u2013 actions that are too risky to aim for today given uncertainty over whether they are necessary.</p></blockquote><p>&nbsp;2. It may provide legible evidence that alignment is hard:</p><blockquote><p>In worlds where alignment is hard, the only solution is to coordinate.&nbsp;</p><p>This is unlikely to be easy, and a major obstacle to succeeding is likely to be being able to produce legible evidence that ensures major players \u2013 frontier labs, governments and regulators \u2013 both know, and have common knowledge (common knowledge plausibly being important to prevent race dynamics), that alignment is hard. Optimising to demonstrate convincing evidence of the difficulty of alignment is not the same as optimising to actually understand the difficulty of alignment, however I expect them to be sufficiently correlated that progress on the latter will be extremely important for the former.</p></blockquote><h3>Claim 5: research on this question is among the most important research directions right now</h3><p>In most worlds where alignment is not hard, I expect us to have a reasonable probability of avoiding catastrophe due to misalignment (although I am less optimistic regarding misuse etc.). I expect in these worlds, through a mixture of public/ government pressure, warning shots and concern of those working in labs, that alignment research will be sufficiently incentivised such that labs do extensive work on aligning subhuman systems. Absent failure modes that would make alignment hard (such as sharp left turns, deception in inner alignment etc.) I\u2019m much more optimistic that the work we will do on subhuman systems will generalise to superhuman systems. In these worlds, knowing that alignment is not hard is still valuable. Knowing this would allow us to better allocate resources between different alignment agendas, proceed with confidence that such alignment techniques are likely to work, and allow us to focus policy attention on the problem of safely evaluating and deploying frontier models.</p><p>In worlds where alignment is hard, the default outcome I expect is failure. I expect us to train models that only <i>appear </i>aligned, in particular through explicitly optimising against legible failure<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy2d0ehtop2\"><sup><a href=\"#fny2d0ehtop2\">[7]</a></sup></span>. Explicitly, if we are in a world where alignment is hard and timelines are short, the top priority isn\u2019t to solve alignment \u2013 <strong>we don\u2019t have time</strong>! It\u2019s to discover as quickly as possible that we are in this world, and to convince everyone else that we are.&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsd70vwql84\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsd70vwql84\">^</a></strong></sup></span><div class=\"footnote-content\"><p>With some exceptions, such as evals which try to answer \u201chow do we determine whether a model is safe?\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn50stquc6tdp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref50stquc6tdp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I don\u2019t consider the specific operationalization of the question particularly important.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnprokhm5j3up\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefprokhm5j3up\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The alignment tax is also sometimes used to refer to the performance cost of aligning a system.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyfbs8x9cco\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyfbs8x9cco\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In worlds where almost all attempts to build aligned AGI converge on specific failure modes, I expect it will be easier to prove this or demonstrate this empirically, compared to worlds in which failure is highly dependent on the alignment technique used.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvh7n9r1z0i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvh7n9r1z0i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Examples of such research would include things such as progress on <a href=\"https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents\"><u>selection theorems</u></a>; or hypothesising/ proving conditions under which sharp left turns are likely to happen. The aim here isn\u2019t to be completely rigorous, but rather sufficiently formal to make predictions.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnenni12klskj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefenni12klskj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Such policies may still be instrumentally useful, such as by widening the Overton window and building political support for more aggressive action later on.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fny2d0ehtop2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefy2d0ehtop2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For example by training models until our interpretability tools cannot detect visible misalignment - and then losing control following deployment.</p></div></li></ol>", "user": {"username": "Aron"}}, {"_id": "weJZjku3HiNgQC4ER", "title": "A note of caution about recent AI risk coverage", "postedAt": "2023-06-07T17:05:01.839Z", "htmlBody": "<p><i>Epistemic status: some thoughts I wanted to get out quickly</i></p><p>A lot of fantastic work has been done by people in the AI existential risk research community and related communities over the last several months in raising awareness about risks from advanced AI. However, I have some cause for unease that I\u2019d like to share.</p><p>These efforts may have been too successful too soon.</p><p>Or, more specifically, this level of outreach success this far ahead of the development of AI capable of posing existential risk may have fallout. We should consider steps to mitigate this.&nbsp;</p><p><strong>(1)</strong>&nbsp; <strong>Timelines</strong></p><p>I know that there are well-informed people in the AI and existential risk communities who believe AI capable of posing existential risk may be developed within 10 years. I certainly can\u2019t rule this out, and even a small chance of this is worth working to prevent or mitigate to the extent possible, given the possible consequences. My own timelines are longer, although my intuitions don\u2019t have a rigorous model underpinning them (my intuitions line up similarly to the 15-40 year timelines mentioned in <a href=\"https://epochai.org/blog/a-compute-based-framework-for-thinking-about-the-future-of-ai\">this recent blog post</a> by Matthew Barnett from Epoch).</p><p>Right now the nature of media communications means that the message is coming across with a lot of urgency. From speaking to lay colleagues, impressions often seem to be of short timelines (and some folks e.g. Geoff Hinton have explicitly said 5-20 years, sometimes with uncertainty caveats and sometimes without).</p><p>It may be that those with short (&lt;10 years) timelines are right. And even if they\u2019re not, and we\u2019ve got decades before this technology poses an existential threat, many of the attendant challenges \u2013 alignment, governance, distribution of benefits \u2013 will need that additional time to be addressed. And I think it\u2019s entirely plausible that the current level of buy-in will be needed in order to initiate the steps needed to avoid the worst outcomes, e.g. recruiting expertise and resources to alignment, development and commitment to robust regulation, even coming to agreements not to pursue certain technological developments beyond a certain point.</p><p>&nbsp;However, if short timelines do not transpire, I believe there\u2019s a need to consider a scenario I think is reasonably likely.</p><p><strong>(2)</strong>&nbsp; <strong>Crying wolf</strong></p><p>I propose that it is most likely we are in a world where timelines are &gt;10 years, perhaps &gt;20 or 30 years. Right now this issue has a lot of the most prominent AI scientists and CEOs signed up, and political leaders worldwide committing to examining the issue seriously (<a href=\"https://twitter.com/RishiSunak/status/1663838958558539776;\">examples</a> <a href=\"https://theelders.org/news/elders-urge-global-co-operation-manage-risks-and-share-benefits-ai;\">from</a> <a href=\"https://&nbsp;https://twitter.com/tedlieu/status/1664430739717255168\">last</a> week). What happens then in the &gt;10 year-timeline world?</p><p>&nbsp;The extinction-level outcomes that the public is hearing, and that these experts are raising and policymakers making costly reputational investments in, don\u2019t transpire. &nbsp;What does happen is all the benefits of near-term AI that have been talked about, plus all the near-term harms that are being predominantly raised by the AI ethics/FAccT communities. Perhaps these harms include somewhat more extreme versions than what is currently talked about, but nowhere near catastrophic. Suddenly the year is 2028, and that whole 2023 furore is starting to look a bit silly. Remember when everyone agreed AI was going to make us all extinct? Yeah, like Limits to Growth all over again. Except that we\u2019re not safe. In reality, in this scenario, we\u2019re just entering the period in which risk is most acute, and in which gaining or maintaining the support of leaders across society for coordinated action is most important. And it\u2019s possibly even harder to convince them, because people remember how silly lots of people looked the last time.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkcq5okqj3r\"><sup><a href=\"#fnkcq5okqj3r\">[1]</a></sup></span><strong>&nbsp;&nbsp;</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgk9hef3oa5\"><sup><a href=\"#fngk9hef3oa5\">[2]</a></sup></span></p><p><strong>(3)</strong>&nbsp; <strong>How to navigate this scenario (in advance).&nbsp;</strong></p><p>Suggestions:</p><ul><li>Have our messaging make clear that we don\u2019t know when extinction-potential AI will be developed, and it\u2019s quite likely that it will be over a decade, perhaps much longer. But it needs to be discussed now, because<ul><li>we can\u2019t rule out that it will be developed sooner;</li><li>there are choices to be made now that will have longer-term consequences;</li><li>the challenges need a lot more dedicated time and effort than they\u2019ve been getting.<br><br>Uncertainty is difficult to communicate in media, but it\u2019s important to try.</li></ul></li><li><a href=\"https://twitter.com/yonashav/status/1664505416846376960\">Don\u2019t be triumphal</a> over winning the public debate now; it may well be \u2018lost\u2019 again in 5 years</li><li>Don\u2019t unnecessarily antagonise the AI ethics/FaCCT folk&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmxdrl9nlph\"><sup><a href=\"#fnmxdrl9nlph\">[3]</a></sup></span>&nbsp;because they\u2019re quite likely to look like the ones who were right in 5 years (and because it\u2019s just unhelpful).</li><li>Build bridges where possible with the AI ethics/FaCCT folk on a range of issues and interventions that seem set to overlap in that time; work together where possible. Lots of people from those communities are making proposals that are relevant and overlapping with challenges associated with the path to transformative AI. This includes external evaluation; licensing and liability; oversight of powerful tech companies developing frontier AI; international bodies for governing powerful AI, and much more. E.g. see <a href=\"https://ainowinstitute.org/publication/gpai-is-high-risk-should-not-be-excluded-from-eu-ai-act\">this </a>and <a href=\"https://www.wired.com/story/ai-desperately-needs-global-oversight/\">this</a>, as well as <a href=\"https://www.safe.ai/post/three-policy-proposals-for-ai-safety\">CAIS's recent blog post.</a></li><li>Don\u2019t get fooled into thinking everyone now agrees. A lot more senior names are now signing onto statements and speaking up, and this is making it easier for previously silent-but-concerned researchers to speak up. However I think a majority of AI researchers probably still don\u2019t agree this is a serious, imminent concern (<a href=\"https://twitter.com/ylecun/status/1664011158989275138\">Yann LeCun\u2019s silent majority</a> is probably still real), and this disconnect in perceptions may result in significant pushback to come.</li><li>Think carefully about the potential political fallout if and when this becomes an embarrassing thing for the politicians who have spoken up, and how to manage this.&nbsp;</li></ul><p>To sum: I\u2019m not saying it was wrong to push for this level of broad awareness and consensus-building; I think it may well turn out to be necessary this early in order to navigate the challenges on the path to transformative AI, even if we still have decades until that point (and we may not). But there\u2019s the potential for a serious downside/backlash that this community, and everyone who shares our concern about existential risk from AI, should be thinking carefully about, in terms of positioning for effectiveness on slightly longer timelines.</p><p><br><i>Thank you to Shakeel Hashim, Shahar Avin, Haydn Belfield and Ben Garfinkel for feedback on a previous draft of this post.</i><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkcq5okqj3r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkcq5okqj3r\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Pushing against this, it seems likely that AI will have continued advancing as a technology, leading to ever-greater scientific and societal impacts. This may maintain or increase the salience of the idea that AI could pose extremely significant risks.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngk9hef3oa5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgk9hef3oa5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A \u2018softer\u2019 version of this scenario is that some policy happens now, but then quietly drops off / gets dismantled over time, as political attention shifts elsewhere</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmxdrl9nlph\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmxdrl9nlph\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I don\u2019t know how much this is happening in practice (there\u2019s just so much online discourse right now it\u2019s hard to track), but I have seen it remarked on several times e.g. <a href=\"https://twitter.com/timhwang/status/1664335308307963904\">here&nbsp;</a></p></div></li></ol>", "user": {"username": "Sean_o_h"}}, {"_id": "9T7qmJcRpgvr3bjGZ", "title": "Seeking important GH or IDEV working papers to evaluate", "postedAt": "2023-06-07T19:29:30.994Z", "htmlBody": "<p>I am a Global Health and Development editor at <a href=\"http://unjournal.org/\"><u>The Unjournal</u></a>. In brief, our mission is to organize and fund public, journal-independent evaluation and rating of working papers. Right now we are focusing on quantitative work informing global priorities, and my area focuses on global health and international development.&nbsp;</p><p><strong>I am writing because I want people to send me working papers (pre-publication) that are decision-relevant to organizations working in global health and international development.</strong>&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuzbzxfme5g\"><sup><a href=\"#fnuzbzxfme5g\">[1]</a></sup></span>&nbsp;Anyone can submit paper suggestions to&nbsp;<a href=\"mailto:rbriggs@uoguelph.ca\"><u>rbriggs@uoguelph.ca. &nbsp;</u></a>If you prefer, authors can submit work through our quick <a href=\"https://airtable.com/shrwlxes5AeasnkfC\">submission form HERE</a>, and others \u200bcan suggest work <a href=\"https://airtable.com/shrdHHI0zK7rkJCP3\">using this form</a>.</p><p>I think two especially useful cases are:</p><ol><li>Authors sending their own papers so that they can get high quality feedback before submission. This feedback will be public, but we are open to discussing delaying or embargoing feedback on a case-by-case basis.</li><li>Grant-making staff in orgs like GiveWell or OpenPhil sending in papers that are relevant to their work so that they can read expert reviews before they use the results to inform decisions.</li></ol><p>The rest of this post explains in more detail: what The Unjournal is, our review process, why we\u2019re doing this, what we\u2019re looking for in a paper, and who can submit.</p><h1>The Unjournal</h1><p><a href=\"https://unjournal.org\"><u>The Unjournal</u></a> (evaluations are <a href=\"https://unjournal.pubpub.org\">here</a>) is the brainchild of&nbsp;<a href=\"https://www.davidreinstein.org/\"><u>David Reinstein</u></a>. Its mission is to offer clear, public pre-publication expert review of working papers. There are two motivations behind this project. The first is that researchers can do better work if they are able to more easily get high quality feedback on their work. The second is that the entire research ecosystem can benefit from these reviews being public. We pay expert reviewers for their time and we give awards for especially high quality research that we evaluate.</p><p>The initial focus of The Unjournal is on quantitative work that informs global priorities, especially in economics, policy, and social science.&nbsp;<a href=\"https://www.ryancbriggs.net/\"><u>I\u2019m</u></a> an editor for global health and international development.</p><h1>The process</h1><p>While our process runs similarly to an academic journal,&nbsp;<strong>we do not publish articles.</strong> Instead, we link to working papers and publish reviews (which we call 'evaluations') and author responses.</p><p>The process starts with editors screening lists of working papers created from submissions or based on our knowledge of the research area. Once a working paper is selected, we contact the authors to ask if they would like to be involved in the process. Author involvement allows authors to converse with reviews and/or respond to reviews. It is not mandatory, but we prefer if authors are involved.</p><p>We then aim to find two reviewers per paper. Reviewers are experts in the research area and are paid for their reviews. Reviewers have the choice to remain anonymous or not. They read the work and fill out standardized review forms.</p><p>We then publish the reviews of the work on our webpage (see <a href=\"https://unnjournal.pubpub.org\">here</a>) alongside brief comments from the editor and responses from the authors if they so wish.</p><h1>Why do this?</h1><p><i>If the journal system already exists, then why have something that looks kind of like a journal and offers sort of similar evaluations?</i></p><p>We think The Unjournal adds value to the existing journal system in a number of important ways. For example:</p><ol><li>Unlike most academic journals, we have no bias against null results. This means that we\u2019re just as likely to review important work that \u201cfails\u201d to find an effect as important work that rejects the null.</li><li>We\u2019re transparent. We publish reviews online so that anyone can see not only the working paper but also what expert reviewers thought of it. Most social science journals do not do this.</li><li>We\u2019re experimental. Journals move very slowly, but we\u2019re willing to try new things. For example, we\u2019re exploring ways to have work with &nbsp; \u201cliving\u201d research projects in dynamic formats. Evaluations can &nbsp;can be updated as the work progresses, and as we learn more about a research question.</li><li>We\u2019re fast. It can take over 5 years to publish a paper in economics. We aim to have published reviews of working papers in a period of months.</li><li>We collect standardized metrics across all papers. We aim to synthesize and analyze these in useful ways, as well as making this quantitative data accessible to meta-science researchers, meta-analysts, and tool-builders.&nbsp;</li></ol><h1>Ideal papers and who can submit</h1><p>Ideal papers for me are those that are likely to be decision-relevant to the work of organizations in the international development or global health space. Papers should be quantitative and pre-publication. Anyone can submit a paper.</p><p>The easiest way to submit a paper is simply to email me about it&nbsp;at <a href=\"mailto:rbriggs@uoguelph.ca\"><u>rbriggs@uoguelph.ca</u></a></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuzbzxfme5g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuzbzxfme5g\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Different disciplines have different names for these. We're interested in working papers, pre-prints, papers hosted on places like arxiv, <strong>but not work that is published in academic journals</strong> or under review. We're aiming to come into the process before submission to, or publication in, an academic journal.</p></div></li></ol>", "user": {"username": "ryancbriggs"}}, {"_id": "pM8zNWHgMTp7u7iGe", "title": "The Three M's: Measurement, Multiplication, Maximization", "postedAt": "2023-06-07T15:50:49.337Z", "htmlBody": "<p>Meet your new EA elevator pitch.&nbsp;</p><p>I have provided a massive win for EA marketing, and if Big EA doesn't retroactively fund me for it I'm gonna quit donating and try to get my kidney back.&nbsp;</p><p>When people ask me about EA, or many other aspects of my worldview and behavior, I'm gonna say \"I'm all about the three M's\". To me, the three M's are the most resilient and useful load-bearing elements of our worldview. I expect discussions based on this elevator pitch to add more value faster to the average passersby than anything else in 80k or CEA introductory literature, and it's not even close.</p><p>Pardon my \"we/our\", my more heterodox friends.&nbsp;</p><h1>Measurement</h1><blockquote><p><i>If you know almost nothing, almost anything will tell you something.</i> - <a href=\"https://hubbardresearch.com/publications/how-to-measure-anything-book/\">Douglas Hubbard</a></p></blockquote><p>By default, we accept the risks (or even costs) of <a href=\"https://en.wikipedia.org/wiki/McNamara_fallacy\">McNamara fallacies</a> because we are <i>overall</i> uncompelled by the alternatives. In physical sciences, measuring instruments come with datasheets that bound the expected error, and may even suggest a probability distribution that you can expect your errors to form. We would <strong>prefer</strong> it if this aspect of datasheets was established or possible for every quantity we care about. When we elicit numbers from the world, we do not believe that we're compressing what matters <i>well</i>, but we aim to subject our expected errors to appropriate rigor.&nbsp;</p><h1>Multiplication</h1><blockquote><p><i>That really sums up our product</i> - Dustin Moskovitz</p></blockquote><p>We believe that multiplication is a morally relevant and justified operation. At the end of the day, we do not think that multiplication is subject to the <a href=\"https://en.wikipedia.org/wiki/Fallacy_of_composition\">composition fallacy</a>. We estimate the probability of a bednet preventing a counterfactually fatal case of malaria, and we look up the price of a bednet, and it is <strong>only by the joys of multiplication </strong>that we provide an estimate for the cost of saving a life.&nbsp;</p><p>This one's a little cheeky, because expected value theory relies just as much on addition as it does on multiplication.&nbsp;</p><h1>Maximization</h1><blockquote><p><i>How can less be more? It's impossible. More is more.</i> - <a href=\"https://youtu.be/QHZ48AE3TOI\">Yngwie Malmsteen</a></p></blockquote><p>By default, we prefer views that admit some notion of optimization, and are not <i>overall</i> deeply compelled by disciplines or traditions that emphasize failure modes or blindspots of optimization. We are vigilant about <a href=\"https://arxiv.org/abs/1803.04585\">goodhart</a> and we may stumble into the <a href=\"https://www.lesswrong.com/s/HXkpm9b8o964jbQ89\">low-slack</a> mistake class from time to time. But at the end of the day, we do not think these considerations are quite damning enough to support an all-things-considered argument against maximization.&nbsp;</p>", "user": {"username": "quinn"}}, {"_id": "wx9GgKGWqksvMWjg2", "title": "Successif: helping mid-career and senior professionals have impactful careers", "postedAt": "2023-06-07T15:16:49.309Z", "htmlBody": "<h3>We\u2019re <a href=\"https://www.successif.org/\">Successif</a>, we\u2019ve been around for a year, and we\u2019re excited to finally formally introduce ourselves to the&nbsp; EA community. This post will cover who we are, what we have been doing, some lessons learnt in the past year, and how you can contribute to our mission!</h3><p><u>Executive summary:</u></p><p>Successif, previously known as EA Pathfinder and created over a year ago, is an organization designed to support mid-career and senior professionals transition into high-impact work. We adopt a holistic approach focused on cause prioritization, personal fit, and aptitude building. We offer collective workshops, one-on-one mentoring, women leadership workshops, peer support groups, and match-making.</p><p>In this post, we are sharing the most common bottlenecks mid-career professionals face, what we learnt about giving people permission, and issues specific to women.&nbsp;</p><p>We are also inviting the community to contribute through referring candidates to our programs, sharing thoughts on our strategy and activities, providing voluntary help, and contributing funds to our new AI program. In a follow-up post, we will present that program.&nbsp;</p><h1><strong>1.&nbsp;</strong>Context on Successif</h1><p>Successif was created in May of 2022 as a result of the belief that there were mid-career and senior professionals aligned with EA values who did not work on EA causes due to lack of information, misconceptions or blockers, and that much expected value would be gained if the community captured these people earlier, especially those who could work on longtermist causes.&nbsp;<br>&nbsp;</p><p>Further investigation revealed a notable gap: a lack of resources and programs tailored to support mid-career and senior professionals in addressing the world\u2019s most urgent problems. For instance, 80,000 Hours\u2019 materials often cater mostly to young individuals, leaving an important segment underserved. To fill this gap, EA Pathfinder\u2014the predecessor to Successif\u2014was launched in 2022. Since then, we have rebranded and refined our strategy accordingly.</p><p>Successif\u2019s core mission is to create a world in which professionals motivated by evidence and reason can find high impact work. Focusing on mid-career and senior professionals leverages unique expertise, skills, and experience to address pressing global issues and fill certain skills gaps in the high-impact talent pipeline.&nbsp;</p><p>We recognize that our audience has specific needs, blockers, and concerns when it comes to transitioning to high-impact work, especially when the work is in a different field. We provide coaching, upskilling through training programs, networking opportunities, and matches with relevant opportunities. More fundamentally, we take a human-centered approach, helping mentees navigate the uncertainties that accompany career transitions. We help address feelings of discouragement from uncertain outcomes and overcome psychological barriers preventing them from being their most impactful professional self.</p><p>We work in six core areas: Technical AI Safety and AI Governance, Biorisk Prevention, Nuclear Arms Risk Prevention, Climate Change, Animal Welfare, and Global Health and Development. We have launched a specific track for AI jobs, which we will cover in a subsequent post.</p><h1><strong>2.</strong>&nbsp; What we do</h1><h2><strong>A.&nbsp; </strong>Our approach</h2><p>Our approach to career advising is based on the ikigai model and on the aptitudes framework.&nbsp;</p><h3><u>Ikigai Model</u></h3><p>\u201cIkigai (\u751f\u304d\u7532\u6590, lit. 'a reason for being'): Japanese concept referring to something that gives a person a sense of purpose, a reason for living.\u201d When it comes to career advising, the Ikigai model offers valuable insights and guidance for career advising. It empowers us to effectively help our advisees discover what is at the intersection of what they love, what they\u2019re good at, what they can be paid for, and what the world needs.&nbsp;</p><p><br>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/wx9GgKGWqksvMWjg2/ctm0vxv9y9gmz5smjbdn\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/wx9GgKGWqksvMWjg2/y7hnnhsewxb3jk8rdt4p 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/wx9GgKGWqksvMWjg2/xeu3xxucm9qlujxjqr1g 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/wx9GgKGWqksvMWjg2/phqp6htr9ecppdmust0p 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/wx9GgKGWqksvMWjg2/qrkkx8pllbpks7xwe24f 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/wx9GgKGWqksvMWjg2/e0zbbbz95pymjlelac4g 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/wx9GgKGWqksvMWjg2/ecqb33q8f9q0jgoa5x1h 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/wx9GgKGWqksvMWjg2/zxxhsry23fqkzwclrlos 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/wx9GgKGWqksvMWjg2/awld8oaau8cx8x624s3q 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/wx9GgKGWqksvMWjg2/r5yijkolkxcps92rouen 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/wx9GgKGWqksvMWjg2/t23pfn6yebjwwouayhmm 1600w\"></p><p><br>&nbsp;</p><p>We observe that by examining the intersecting elements of passion, mission, vocation, and profession, we can build positive and encouraging relationships with our advisees. This approach allows us to holistically consider the key personal and professional factors that are unique to every individual. It prompts individuals to reflect on their interests, values, and talents, helping them identify their true calling and purpose in their professional lives.</p><p>The component of what the world needs enables us to guide advisees through cause prioritization exercises and empowers them to learn more about different cause areas.&nbsp;<br>&nbsp;</p><h3><u>Aptitudes Framework</u></h3><p>Our approach to career advising is strongly inspired by Holden Karnowsky\u2019s&nbsp;<a href=\"https://80000hours.org/podcast/episodes/holden-karnofsky-building-aptitudes-kicking-ass/\"><u>aptitudes framework</u></a>, which differs from what usually prevails in the EA community. Our advising activities are guided by the following \u201captitudes framework\u201d principles:&nbsp;</p><ol><li>\u201c&nbsp;<strong>Whatever you\u2019re doing, you\u2019re doing it so much better if you\u2019re better at it&nbsp;</strong>\u201d. We help our advisees figure out what their greatest aptitudes are, as well as what they may become extremely good at in the future. We have found that it is particularly important when working with mid-career and senior professionals. Their extensive experience has often allowed them to develop a particular set of aptitudes which, combined with their knowledge, makes them uniquely placed to help mitigate AI existential risks. For instance, an experienced project manager may have developed a remarkable aptitude to effectively maintain people's focus and progress, which in turn helps amplify the impact of a key AI organization (e.g., an AI evaluation organization).</li><li>Unless there is a direct path to impact with a higher counterfactual,&nbsp;<strong>individuals should focus on building on their aptitudes</strong>, rather than focusing on obtaining a specific role in a specific cause area. As underlined by Holden Karnowsky, if someone has or develops a specific set of aptitudes, the latter may be transferable in whichever cause area they decide to pursue, allowing the individual to amplify their impact. For Successif, this may translate in helping an advisee pursue a nonlinear AI career path. For instance, we may not help place an advisee in a top AI auditing job right away, but would rather focus on placing them in a non-X risk auditing job so that they build their aptitudes and career capital. This, in turn, will make them a particularly strong candidate for obtaining an impactful position when the opportunity arises, and enable them to be particularly effective in the field of AI auditing.</li><li><strong>Fit is crucial.&nbsp;</strong>One has to ensure that they\u2019re a good fit for a certain aptitude. It is critical for individuals to focus on identifying the areas where they can flourish. They should prioritize asking themselves this question early on, and one of our focuses is to help advisees navigate this often complicated question. Useful indicators may be receiving positive or negative feedback from peers, whether one\u2019s enjoying oneself at a certain position, and if they find a given kind of work sustainable. Even though we don\u2019t blindly advise people to do whatever they want to do, we encourage them to not dive head first into a role solely because they have calculated that it has a high impact and because they\u2019re given the opportunity. Of course, we generally agree that if one has the chance to pursue the exact thing they believe is right for them in the long-run, they should go for it. Nonetheless, we also think that aptitude building and accumulation is important for enabling one to be particularly successful, inspirational, and effective once they get a highly impactful position.&nbsp;</li></ol><h2><strong>B.&nbsp; </strong>Our activities<br>&nbsp;</h2><p><u>Collective workshops:</u> These workshops address some of the most common blockers we have identified for mid-career people. They also foster a dynamic and interconnected environment where advisees have the opportunity to connect with fellow advisees. Through interactive activities, participants engage in meaningful discussions, sharing experiences, and building a supportive network. They also leave with concrete tools and strategies to navigate the changes successfully.&nbsp;<br>&nbsp;</p><p><u>One-on-one mentoring</u>: Advisees are matched with one of our mentors based on their needs and backgrounds. Throughout the entire career transition process, mentors engage in one-on-one calls with their advisees, providing invaluable support and guidance. Our sessions draw inspiration from a combination of advising and coaching techniques, tailored to meet each individual's unique requirements. This includes offering targeted information specific to their field, such as exploring how a physical engineer can contribute to AI alignment, or how a medical research fellow can help mitigate biosecurity x-risks. We also assist in the investigation and decision-making process, helping advisees determine the most impactful path among various options. Furthermore, we address psychological barriers, using coaching techniques and offering support to overcome challenges that may arise during the transition.</p><p>&nbsp;</p><p><u>Peer support group</u>: We have found that it is particularly important for mid-career and senior professionals to be part of a community and receive peer support when seeking to reorient towards a high-impact role (although we believe this is applicable for any individual at any career stage). We cannot emphasize enough how crucial it is to benefit from a nurturing environment where individuals connect with like-minded professionals who share similar goals and aspirations, and who face similar hardships. Thus, peer support within the Successif community offers encouragement, motivation, and a sense of belonging during the challenging phases of transitioning, which in turn boosts confidence and resilience.&nbsp;<br>&nbsp;</p><p><u>Women leadership workshops</u>: Women face particular challenges in the job market. Qualified female candidates will often pursue roles that underutilize their talents and years of experience relative to male counterparts with similar accomplishments and expertise. This tendency towards underreach rather than overreach ultimately affects both the women and their total impact. Women leadership workshops aim to help them address the unique challenges and personal blockers that they face.&nbsp;<br>&nbsp;</p><p><u>AI-specific track</u>: With recent developments in AI and shortened timelines for risk actualization, we determined our impact would be amplified if we developed additional programs specifically for those interested in transitioning into AI Safety and AI Governance. We will present our AI track in a subsequent post.&nbsp;</p><p>&nbsp;</p><h1><strong>3.</strong>&nbsp; What we\u2019ve learned across cause areas</h1><p>Our career advising program, reinforced by continuous research and interviews, has allowed us to gain thorough, insightful, and sometimes surprising knowledge about (A) blockers and bottlenecks experiences by mid-career and senior professionals seeking to transition, (B) how the gender gap comes into play when doing career advising, and (C) that giving permission to people to do what they actually want to do (that is impactful) is crucial.&nbsp;</p><h2><strong>A.</strong>&nbsp; Blockers and Bottlenecks</h2><p>In our work to date, we have identified a variety of psychological blockers and bottlenecks experienced by mid-career and senior professionals considering a career transition towards an impactful role. As we identify these, we find appropriate interventions and outside programs to work through these blockers. These interventions have worked with surprising efficacy and have provided us with the confidence to scale our work.<br>&nbsp;</p><p>Some of the main blockers and bottlenecks we\u2019ve identified and that we address through our program are:</p><h3>Fear of\u2026</h3><ul><li>change after being in the same field or job for years</li><li>salary cut</li><li>geographic displacement (especially if they have a family)</li><li>losing autonomy in job if transitioning to a more junior role</li></ul><h3>Lack of\u2026</h3><ul><li>awareness of what is possible (e.g., person in AI-adjacent field without machine learning background is unaware they could be useful in technical alignment or governance)</li><li>confidence in one\u2019s own capabilities&nbsp;</li><li>credentials in field being considered</li><li>suitable jobs</li><li>network connections</li><li>industry knowledge and context</li><li>competence / suitability</li><li>job security in rapidly evolving fields</li><li>recognition and acceptance of one's competence or accomplishments (i.e. imposter syndrome)</li></ul><h3>Not knowing how to\u2026</h3><ul><li>utilize niche skills</li><li>navigate the job market again</li><li>maximize impact</li><li>sell oneself for what they\u2019re worth (often a gender-specific issue)</li><li>apply only for jobs at the appropriate level (often a gender-specific issue)</li><li>find a safe space and community for support during the challenging phases of transitioning&nbsp;<br>&nbsp;</li></ul><p>Our advisors have acquired much useful information through their advising interventions. We have created archetypes of the advisees with appropriate interventions for different types. We are also continuously doing research on our hypotheses to make sure we are most impactful and perpetually revisit our advising strategy.&nbsp;</p><p>&nbsp;</p><h2><strong>B.</strong>&nbsp; Gender gap</h2><p>Our observations across most fields are constant: women feel unsure of themselves by existing in an environment which directly or indirectly perpetuates gender stereotypes. This is especially true in the field of AI, where the vast majority of professionals are men. A lot of the women that we accompany face additional barriers and challenges as they navigate their career, and are often grappling with a sense of insecurity and self-doubt. This in turn often blocks them from having the highest impact in their professional life.&nbsp;</p><p>In analyzing career progression and job changes, we have found that women frequently sell themselves short. Qualified female candidates will often pursue roles that underutilize their talents and years of experience relative to male counterparts with similar accomplishments and expertise. This tendency towards underreach rather than overreach ultimately affects both the women and their total impact.</p><p>This tendency is often accompanied by a pronounced \u201cimposter syndrome\u201d, where women doubt their abilities and exhibit lower levels of self-confidence in self-promotion. For instance, they often diminish their own accomplishments during interviews. We take this gender gap seriously, and actively seek to empower women to recognize their competence, to aim higher, and to apply for more suitable and impactful positions. For instance, in response to some of our female advisees\u2019 concerns about lacking qualifications, we reassure them that if they genuinely lack the necessary qualifications, they will not be selected for the role, and we encourage and accompany them in applying for the role anyways.&nbsp;</p><p>To address these challenges, we have started offering women-only workshops. Our preliminary evaluation indicates that the workshop has helped women communicate their accomplishments better to prospective employers.&nbsp;</p><p>&nbsp;</p><h2><strong>C.</strong>&nbsp; Giving permission</h2><p>Often, much of our advising time is spent giving people permission to do things they wish to do. By doing so, we enable them to take agency in pursuing a path they would see fit for them in the long term. This approach is a core element of the aptitudes framework, discussed in section 3 (A).&nbsp;</p><p>Transactional analysis, a branch of psychology elaborated by Eric Berne that studies the instructions and injunctions people subconsciously receive from their early childhood on, is particularly useful for understanding the importance of \u201cgiving permission.\u201d For instance, a child who is constantly shushed and told to calm down will hear \u201cdo not exist.\u201d That will make for an adult who always apologizes when entering a room. Other injuctions include \u201cdon\u2019t express emotions,\u201d \u201cdon\u2019t succeed,\u201d \u201cdon\u2019t fail,\u201d etc. Unfortunately, we keep receiving injunctions throughout our adult life: from a jealous romantic partner, colleagues\u2019 peer-pressure, or a friend\u2019s judgment.&nbsp;</p><p>The opposite of giving people injunctions is to give them permission. We have observed that this simple intervention is extremely empowering for our advisees. Thus, in our role, we give people permission to leave a corporate job to go into a cause area that is dear to them, permission to turn down a job that some collaborators are guilting them into accepting, permission to contact someone they admire and view as unreachable, permission to be happy or do what they truly want. A surprisingly high proportion of individuals just need someone to tell them \u201cyes.\u201d This is the most fulfilling and easiest intervention that we can take to unblock people on a path to impact. The next step is to get them to give themselves permission.</p><h1><strong>4.</strong>&nbsp; What you can do to help</h1><h2><strong>A.</strong>&nbsp; Refer candidates to our programs</h2><h3>Encourage people you think meet these criteria to&nbsp;<a href=\"https://www.successif.org/apply\"><u>apply for our services</u></a>:</h3><ul><li>5+ years of professional experience</li><li>Competent and promising</li><li>Proto-aligned</li><li>Proto-understanding of existential risks</li></ul><h2><strong>B.</strong>&nbsp; Share your thoughts on our strategy and activities</h2><ul><li>If you have thoughts on how we can better screen participants,&nbsp;<a href=\"https://forms.gle/ZWB2BWG9cBFNQ5eH6\"><u>let us know here</u></a>.</li><li>If you have thoughts on how we can create a strong community for our participants to stay engaged with others in their fields in the long-run,&nbsp;<a href=\"https://forms.gle/GDCf7XkkoMb4UBDs5\"><u>let us know here</u></a>.</li><li>If you know any mid-career professionals who have transitioned from another field into high-impact AI jobs, or if you are such a professional yourself, please&nbsp;<a href=\"https://forms.gle/9RTygFMwNVhUHa1r8\"><u>let us know here</u></a>. We're interested in incorporating these experiences into our case studies.&nbsp;</li><li>If you are able to volunteer your services for podcast editing, please&nbsp;<a href=\"https://forms.gle/JahLe5mbbtioiMHT7\"><u>let us know here</u></a>.&nbsp;</li><li>If you can help us organize the High Impact Talent Ecosystem, a community to promote information sharing and collaboration between meta-EA and career-focused organizations,&nbsp;<a href=\"https://forms.gle/cEkbExYjbHpTo4Zp8\"><u>volunteer here</u></a>.<br>&nbsp;</li></ul><h2><strong>C.</strong>&nbsp; Fund Our AI track</h2><p>We are actively seeking funding to accelerate the growth of our AI safety initiatives. Timelines for advanced AI progress are shortening, creating urgency. Simultaneously, recent media coverage and shifting perceptions have generated new momentum, political enthusiasm, and opened windows of opportunity. We believe that it\u2019s crucial to seize them.&nbsp; If AI safety or existential risks in general are your philanthropic priority, we invite your support via donation.<br>&nbsp;</p><h1><strong>5.</strong>&nbsp; How we can help you</h1><p><u>Match-making</u>: If you are a high-impact organization and have a potential project sitting on a desk that you never hired for because it would require niche skills, we can help you find a good person to take it on!&nbsp;</p><p><u>Research</u>: If you have an impactful research project idea, let us know! Our trainees could undertake it as their capstone project.&nbsp;</p><p><u>Head hunting</u>: If you have an open role that is difficult to hire for, we can help!&nbsp;</p><p><u>Training programs</u>: Let us know if our training modules could be useful to you or your organization.&nbsp;</p><h3>How Can We Support You Best?</h3><p>Do you think that our program could be beneficial to you or someone you know, but wish to discuss it further with us? Please reach out to us at&nbsp;<a href=\"mailto:contact@successif.org\"><u>contact@successif.org</u></a>.&nbsp;</p>", "user": {"username": "ClaireB"}}, {"_id": "aDFR6c3Qd6cqrQu7c", "title": "Could AI accelerate economic growth?", "postedAt": "2023-06-07T19:07:26.318Z", "htmlBody": "<p> <i> Note: This post was crossposted from <a href=\"https://www.planned-obsolescence.org/could-ai-accelerate-economic-growth/\">Planned Obsolescence</a> by the Forum team, with the author's permission. The author may not see or respond to comments on this post.</i></p>\n<table><tbody><tr><td><i>Most new technologies don\u2019t accelerate the pace of economic growth. But advanced AI might do this by massively increasing the research effort going into developing new technologies.</i></td></tr></tbody></table>\n <p>What will happen to economic growth once AI has made us all obsolete? Economists are often skeptical of big effects.</p>\n<p>One reason they give is that new technologies typically don\u2019t accelerate economic growth. Instead, they typically cause a one-time gain in economic output, and then growth continues at its normal rate.</p>\n<p>For example, <a href=\"http://www.bcaplan.com/?ref=planned-obsolescence.org\">Bryan Caplan</a>, an econ prof at GMU, <a href=\"https://twitter.com/bryan_caplan/status/1641841635427221508?ref=planned-obsolescence.org\">recently</a> <a href=\"https://twitter.com/bryan_caplan/status/1641840379556253697?ref=planned-obsolescence.org\">tweeted</a>:</p>\n<blockquote>\n<p>Tech moved 10x faster than I expected in the last year\u2026</p>\n<p>Economic effects will be modest &amp; gradual. Even electricity took decades to make a huge difference\u2026</p>\n<p>What I doubt is that any one new tech will raise growth by even 1 percentage-point per year.</p>\n</blockquote>\n<p>This dynamic has played out over the past 50 years. We developed computers and the internet, but economic growth didn\u2019t speed up, and if anything <a href=\"https://en.wikipedia.org/wiki/The_Great_Stagnation?ref=planned-obsolescence.org\">it got <em>slower</em></a>.</p>\n<p>I think this is the right way to understand the economic impact of current AI. GPT-4 will raise productivity in many sectors as it is gradually adopted across the economy, but it won\u2019t permanently accelerate economic growth by itself.</p>\n<p>But this doesn\u2019t mean it\u2019s impossible to ever accelerate economic growth. In fact, <a href=\"https://www.openphilanthropy.org/research/modeling-the-human-trajectory/?ref=planned-obsolescence.org\">economic growth has become much faster</a> over the past 2000 years. Over the last few decades, the global economy has grown at about ~3% per year. But around 1800 it grew more slowly, at about ~1% per year. And earlier in time growth was slower still, below 0.1% per year if you go back far enough.</p>\n<p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/aDFR6c3Qd6cqrQu7c/lr3wmofuq6xpybbqr4qk\" alt=\"po-gwp\" loading=\"lazy\"> </p><figcaption>From David Roodman\u2019s <a href=\"https://www.openphilanthropy.org/research/modeling-the-human-trajectory/?ref=planned-obsolescence.org\">Modeling the Human Trajectory</a>. The solid line is historical economic growth from 10,000 BC to 2019. If the rate of growth were constant, this graph should look like a straight line. But it curves upwards, showing economic growth accelerating over time. </figcaption><p></p>\n<p>So, if new technologies don\u2019t ever really accelerate economic growth, why is growth so much higher today than it was 2000 years ago? The consensus view in economics is that modern growth is so fast because we put <strong>continual effort into innovation</strong><sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>. We deliberately invest in R&amp;D to invent new technologies, we put effort into making our manufacturing processes more efficient, we design supply chains to efficiently distribute new technologies quickly across the economy, and so on.</p>\n<p>The world is collectively putting more effort into innovation now compared to 2000 years ago for two main reasons: we have more people overall, and a larger fraction of those people are working on developing new technologies.</p>\n<ul>\n<li>On the first point: The global population is about thirty times larger than it was 2000 years ago, so there are more people who can potentially come up with ideas for new technologies, and more people who can work to make them a reality.</li>\n<li>On the second point: a larger fraction of the population today specializes in R&amp;D for new technologies.\n<ul>\n<li>One big reason for this is better education \u2014 2000 years ago almost nobody received an academic education; today, mass education means that a larger fraction of the population has the background skills needed to contribute to technology R&amp;D.<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup></li>\n<li>Another major reason is better institutions for encouraging and enabling innovation. For example, in the past you had to be independently wealthy to be an inventor, because you had to fund your own research. But today, investment markets and government grants will often finance promising ideas, so you can try to invent new technologies even if you couldn\u2019t fund all the research yourself.</li>\n</ul>\n</li>\n</ul>\n<p>Today around ~20 million people work in R&amp;D worldwide.<sup class=\"footnote-ref\"><a href=\"#fn3\" id=\"fnref3\">[3]</a></sup> Two thousand years ago the effort going into research was much smaller, I\u2019d guess by a factor of 1000.<sup class=\"footnote-ref\"><a href=\"#fn4\" id=\"fnref4\">[4]</a></sup> Economic growth is much faster today than it was 2000 years ago not because of any single new technology, but because we are putting in so much more effort into generating a steady stream of new innovations.</p>\n<p>I think future AI might massively increase the world\u2019s innovation efforts again, and thereby accelerate economic growth. Rather than being \u201cjust one more technology\u201d, it might massively increase the pace at which humanity develops new technologies.</p>\n<p>How much might future AI increase the world\u2019s innovation efforts?</p>\n<p>As I discussed in a previous <a href=\"https://www.planned-obsolescence.org/continuous-doesnt-mean-slow/\">post</a>, once we develop AI that is \u201cexpert human level\u201d at AI research, it might not be long before we have AI that is way beyond human experts in all domains. That is, AI that is way better than the best humans at thinking of new ideas, designing experiments to test those ideas, building new technologies, running organizations, and navigating bureaucracies.<sup class=\"footnote-ref\"><a href=\"#fn5\" id=\"fnref5\">[5]</a></sup></p>\n<p>What\u2019s more, because it takes so many more computer chips to train powerful AI than to run it, once we\u2019ve trained these superhuman AIs we would potentially have enough computation to run them on <em>billions</em> of tasks in parallel.<sup class=\"footnote-ref\"><a href=\"#fn6\" id=\"fnref6\">[6]</a></sup> There could be massive research organizations where AIs manage other AIs to conduct millions of research projects in parallel. And these AIs could innovate tirelessly day and night.<sup class=\"footnote-ref\"><a href=\"#fn7\" id=\"fnref7\">[7]</a></sup></p>\n<p>As well as having superhuman intelligence, these AIs could think much more quickly than humans. ChatGPT Turbo can <a href=\"https://fabienroger.github.io/trackoai/?ref=planned-obsolescence.org\">already write ~800 words per minute</a>, whereas humans typically write about 40 words per minute. So AI can already write ~20X faster than humans. In just one day, each AI could potentially think as many thoughts as a human thinks in a month.<sup class=\"footnote-ref\"><a href=\"#fn8\" id=\"fnref8\">[8]</a></sup></p>\n<p>My best guess is that AI this powerful would increase the world\u2019s innovative efforts by more than 100X. Perhaps, like before, this would <a href=\"https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/?ref=planned-obsolescence.org\">significantly accelerate economic growth</a>.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>In <a href=\"https://en.wikipedia.org/wiki/Solow%E2%80%93Swan_model?ref=planned-obsolescence.org\">standard neoclassical models</a> growth is ultimately driven by better technology, which the model assumes improves exponentially. <a href=\"https://en.wikipedia.org/wiki/Jones_model?ref=planned-obsolescence.org\">Semi-endogenous models</a> go further in modeling technological progress as resulting from targeted R&amp;D efforts. <a href=\"https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/?ref=planned-obsolescence.org#7211-frankel-1962\">Other models</a> take a different approach and represent technological progress as driven by \u201clearning by doing\u201d. In all these models, growth is ultimately driven by innovation.<br>\nThere are some growth models where growth is ultimately driven by capital accumulation rather than technological progress. But these aren\u2019t particularly popular and they must deny that there are diminishing returns to capital on the current margin. Interpreted at face value, they imply that developed countries became richer over the past 100 years solely by producing more of the goods that already existed 100 years ago, rather than by developing higher quality goods and new technologies.<br>\nI do think standard growth models miss part of the picture, which is that growth in GDP/capita has been in part driven by one-time changes like <a href=\"http://klenow.com/HHJK.pdf?ref=planned-obsolescence.org\">increased workforce participation by women</a>. <a href=\"#fnref1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>Better education also means that the people doing R&amp;D are more skilled on average. <a href=\"#fnref2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn3\" class=\"footnote-item\"><p><a href=\"https://www.nsf.gov/statistics/2018/nsb20181/report/sections/science-and-engineering-labor-force/global-s-e-labor-force?ref=planned-obsolescence.org\">OECD</a> data from 2015 estimates that the global workforce in science and engineering is 7 million, though this omits India and Brazil. A <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/1758-5899.13182?ref=planned-obsolescence.org\">recent paper</a> estimates ~20 million full-time equivalents do R&amp;D worldwide. <a href=\"#fnref3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn4\" class=\"footnote-item\"><p>Why a factor of 1000? Population was ~30X lower, and various data sources suggest research concentration in 1800 was 30X lower than today (see the subsection \u201cData on the research concentration in 1800\u201d in <a href=\"https://www.openphilanthropy.org/research/social-returns-to-productivity-growth/?ref=planned-obsolescence.org#appendix-f-back-of-the-envelope-calculations-of-value-of-rd-in-1800\">this report</a>). Combining those factors, the number of researchers 2000 years ago was lower than today by a factor of 30*30 = ~1000X.<br>\nIn fact, the fraction of people doing research 2000 years ago was likely lower than in 1800, suggesting an even bigger difference than 1000X. On the other hand, research effort 2000 years ago may have come more from many people making small innovations in their personal workflows than from full time researchers. <a href=\"#fnref4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn5\" class=\"footnote-item\"><p>There are some innovative activities that disembodied AI couldn\u2019t automate, because they require interacting with the physical world. This could significantly limit AI\u2019s effect on economic growth. On the other hand, AI might design robots that can do all the physical tasks that humans can do. Then AIs could control these robots remotely and perform all the tasks involved in innovation. <a href=\"#fnref5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn6\" class=\"footnote-item\"><p>Of course, we don\u2019t know how much compute superhuman AI would take to train or to run. To guess at this, I estimated how many tasks GPT-8 could perform in parallel using only the computer chips needed to train it.<br>\nIn a <a href=\"https://www.planned-obsolescence.org/continuous-doesnt-mean-slow/#fn2\">previous post</a> I estimated that GPT-4 could perform 300,000 tasks in parallel with the compute used to train it. Compared to GPT-4, I assumed that GPT-8 would have 10,000 times as many parameters and need 100 million times as much computing power to train (in line with the Chinchilla scaling law). This implies that GPT-8 could perform 10,000X (= 100 million/10,000) as many tasks in parallel. I.e. 3 billion (=300,000 * 10,000) tasks. See <a href=\"https://docs.google.com/spreadsheets/d/1qzq4dYkSAccuRnIsXNkIzJ4Vd6SDxrsXSMCrx9hrWiw/edit?ref=planned-obsolescence.org#gid=511355734\">calc</a>. <a href=\"#fnref6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn7\" class=\"footnote-item\"><p>What\u2019s more, the size of this AI workforce could grow rapidly over time. AI could work to increase the number of AIs and how smart they are by designing better AI algorithms, designing better AI chips, and investing more money to build more AI chips. Already AI algorithms are becoming about <a href=\"https://epochai.org/blog/revisiting-algorithmic-progress?ref=planned-obsolescence.org\">twice as efficient each year</a>, AI chips are becoming twice as efficient every ~2-3 years, and investments in AI are growing quickly. If this pace of improvement keeps up, the size of the AI workforce would more than double every year! This fast-growing workforce could innovate quickly despite ideas becoming harder to find. <a href=\"#fnref7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn8\" class=\"footnote-item\"><p>A previous footnote argued that if we took the computer chips that were used to train superhuman AI and used them to run copies of the superhuman AI, we could run 3 billion copies in parallel. They could work tirelessly day and night, rather than the ~8 hours/day from human workers, which increases the size of the effective AI workforce to 9 billion.<br>\nIf 2 million out of these 9 billion AIs work on innovation, that is already a 100X increase on the current size of the R&amp;D workforce (~20 million \u2013 see previous footnote). But there are two reasons the increase in innovative effort will be bigger than this. Firstly, each superhuman AI is much more smart and productive than the best R&amp;D workers today. This is a massive effect, bigger than turning every scientist alive today into a top performer in their field. Lastly, there\u2019s a large gain in productivity from the AIs being able to think faster. Rather than having 2 billion AIs thinking at human speed, we could have 100 million AIs thinking at 20X human speed. <a href=\"#fnref8\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li></ol></section>", "user": {"username": "Tom_Davidson"}}, {"_id": "KRSthwicCTRw9Ayzg", "title": "Large epistemological concerns I should maybe have about EA a priori", "postedAt": "2023-06-07T14:11:51.288Z", "htmlBody": "<h2>Summary</h2><p>I have become more truthseeking and epistemically modest in recent months and feel I have to re-evaluate my 'EA-flavored' beliefs, including:</p><ol><li>My particular takes about particular cause areas (chiefly alignment). Often, these feel immodest and/or copied from specific high-status people.</li><li>Trust in the \u201cEA viewpoint\u201d on empirical issues (e.g., on AI risk). People tend to believe in <i>stories</i> about things that are too big for them to understand and I don't know if EA is just <i>one plausible story</i> out of these.</li><li>Are these large empirical questions too hard for us to make reasonable guesses? Are we deluding ourselves in thinking we are better than most other ideologies that have been mostly wrong throughout history?</li><li>Can I assume 'EA-flavored' takes on moral philosophy, such as utilitarianism-flavored stuff, or should I be more 'morally centrist'?</li><li>Can I, as a smart and truthseeking person, do better than just deferring on, say, \u201cMight AI lead to extinction?\u201d Even though there are smarter &amp; more epistemically virtuous people who I could defer to?</li><li>Should I hold very moderate views on everything?</li><li>Can EA, as a \u201csmart and truthseeking\u201d movement, assume its opinions are more accurate than other expert groups\u2019?</li></ol><p>&nbsp;</p><figure class=\"image image_resized\" style=\"width:43.01%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/KRSthwicCTRw9Ayzg/metrskfkg45v7btky6c8\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/KRSthwicCTRw9Ayzg/ibag9ja84xjbtq7f92ze 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/KRSthwicCTRw9Ayzg/qsumdkhlxma2z39qbsuz 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/KRSthwicCTRw9Ayzg/sonssxidb9u8lmvifcny 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/KRSthwicCTRw9Ayzg/inc9anm4t7mpxnslygmh 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/KRSthwicCTRw9Ayzg/dl9js0zmh8bnkexupskm 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/KRSthwicCTRw9Ayzg/qgwwjoulismlwooxnzpr 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/KRSthwicCTRw9Ayzg/ndpxtdf9augqkhbokwka 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/KRSthwicCTRw9Ayzg/aodp8btasqg4uyyfhhlv 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/KRSthwicCTRw9Ayzg/blu62mqpxdbmpfwkzxxq 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/KRSthwicCTRw9Ayzg/efjqk7slozlvupfs4mvr 1024w\"></figure><p>&nbsp;</p><h2>Note</h2><p>I originally wrote this as a private doc, but I thought maybe it's valuable to publish. I've only minimally edited it.</p><p>Also, I now think the epistemological concerns listed below aren't super clearly carved and have a lot of overlap. The list was never meant to be a perfect carving, just to motion at the shape of my overall concerns, but even so, I'd write it differently if I was writing it today.</p><p>&nbsp;</p><h2>Motivation</h2><p>For some time now, I\u2019ve wanted nothing more than to finish university and just work on EA projects I love. I\u2019m about to finish my third year of university and could do just that. A likely thing I would work on is alignment field-building, e.g., helping to run the&nbsp;<a href=\"https://www.serimats.org/\"><u>SERI MATS program</u></a> again. (In this doc, will use alignment field-building as the representative of all the community building/operations-y projects I\u2019d like to work on, for simplicity.)</p><p>However, in recent months, I have become more careful about how I form opinions. I am more truthseeking and more epistemically modest (but also more hopeful that I can do more than blind deferral in complex domains). I now no longer endorse the epistemics (used here broadly as \u201cways of forming beliefs\u201d) that led me to alignment field-building in the first place. For example, I think this in part looked like \u201cchasing cool, weird ideas that&nbsp;<i>feel</i> right to me\u201d and \u201cbelieving whatever high-status EAs believe\u201d.</p><p>I am now deeply unsure about many assumptions underpinning the plan to do alignment field-building. I think I need to take some months to re-evaluate these assumptions.</p><p>&nbsp;</p><h2><strong>In particular, here are the questions I feel I need to re-evaluate:</strong></h2><p>&nbsp;</p><h3>1. What should my particular takes about particular cause areas (chiefly alignment) and about community building be?</h3><p>My current takes often feel immodest and/or copied from specific high-status people. For example, my takes on which alignment agendas are good are entirely copied from a specific Berkeley bubble. My takes on the size of the \u201ccommunity building multiplier\u201d are largely based on quite immodest personal calculations, disregarding that many \u201cexperts\u201d think the multiplier is lower.</p><p>I don\u2019t know what the right amount of immodesty and copying from high-status people is, but I\u2019d like to at least try to get closer.</p><p>&nbsp;</p><h3>2. Is the \u201cEA viewpoint\u201d on empirical issues (e.g., on AI risk) correct (because we are so smart)?</h3><p>Up until recently I just assumed (a part of) EA is right about large empirical questions like \u201cHow effectively-altruistic is \u2018Systemic Change\u2019?\u201d, \u201cHow high are x-risks?\u201d and \u201cIs AI an x-risk?\u201d. (\u201cEmpirical\u201d as opposed to \u201cmoral\u201d.) First, this was maybe a na\u00efve kind of tribalistic support, later because of the \u201csuperior epistemics\u201d of EAs. The poster version of this is \u201c<i>Just believe whatever Open Phil says</i>\u201d.</p><p>Here\u2019s my concern: In general, people adopt&nbsp;<i>stories</i> they like on big questions, e.g., the capitalism-is-cancer-and-we-need-to-overhaul-the-system story or the AI-will-change-everything-tech-utopia story. People don\u2019t seek out all the cruxy information and form credences to&nbsp;<i>actually get closer to the truth.&nbsp;</i>I used to be fine just to back \u201c<i>a</i> plausible story of how things are\u201d, as I suspect many EAs are. But now I want to back&nbsp;<i>the correct</i> story of how things are.</p><p>I\u2019m wondering if the EA/Open Phil worldview is just&nbsp;<i>a&nbsp;</i>plausible story. This story probably contains a lot of truthseeking and truth on lower-level questions, such as \u201cHow effective is deworming?\u201d. But on high-level questions such as \u201cHow big a deal is AGI?\u201d, maybe it is close to impossible not just to believe in a story and instead do the hard truthseeking thing. Maybe that would be holding EA/Open Phil to an impossible standard. I simply don\u2019t know currently if EA/Open Phil epistemics are better than that and therefore I should not defer to them unreservedly.</p><p>I am even more worried about this in the context of bubbles of EAs in the bay area. I\u2019ve perceived the desire of people there to buy into big/exciting stories to be quite strong.</p><p>Maybe the EA/Open Phil story is therefore roughly as likely to be true as the stories of other smart communities, e.g. other ML experts. This seems especially damning for parts of the story where (a part of) EA is taking a minority view. A modest person would assume that EA is wrong in such cases.</p><p>&nbsp;</p><p>I haven\u2019t really heard many convincing stories that run counter EA, but I also haven\u2019t really tried. Sure, I have heard counter-arguments repeated&nbsp;<i>by EAs</i>, but I\u2019ve never sought out disagreeing communities and heard them out on their own terms. An additional problem is that others probably don\u2019t spend as much time on refuting EA as EAs spend on backing it with arguments.</p><p>For illustration, I recently read an article criticizing the way EA often deals with low-probability risks. The author claims their critical view is common in the relevant academia. I wouldn\u2019t even be surprised if this was the case. This makes my blind assumption that x-risk is a core moral priority of our time seem unjustified. I haven\u2019t even considered plausible alternative views! (I don\u2019t remember the name of the article sadly.)</p><h3><br>3. Are humans in general equipped to answer such huge empirical questions?</h3><p>Maybe questions such as \u201cHow high are x-risks?\u201d are just so hard that even our best guesses are maybe 5% likely to be right. We delude ourselves to think we understand things and build solid theories, but really we are just like all the other big ideologies that have come and gone in history. We\u2019re like communists who believe they have found the ultimate form of society, or like hippies who believe they figured out the answer to everything is love. (Or like 30s eugenicists who believe they should \u201cimprove\u201d humanity.)</p><p>Here are a bunch of related concerning considerations:</p><ul><li>Maybe there is something very big and real about the term \u201cgroupthink\u201d that we don\u2019t understand.</li><li>Maybe our epistemics don\u2019t weigh that heavily in all of this, maybe they increase our chances of being correct from 5% to 6%.</li><li>Maybe the main ingredient to finding answers to the big questions throughout history has just been guessing a lot.&nbsp;<i>Not&nbsp;</i>being smart and truthseeking.</li><li>Maybe there\u2019s a huge illusion in EA of \u201csomeone else has probably worked out these big assumptions we are making\u201d. This goes all the way up to the person at Open Phil thinking \u201cHolden has probably worked these out\u201d but actually no one has.</li><li>Maybe it\u2019s really hard to notice for people when they are&nbsp;<i>not smart enough</i> to have accurate views on something.</li><li>Maybe the base rate of correct big ideas is just very low.</li></ul><p>&nbsp;</p><p>I don\u2019t understand any of these dynamics well and I don\u2019t know if EA could be falling prey to them currently. Since these seem plausibly big, they seem worth investigating.</p><p>Again, I am even more worried about this in the context of EA bubbles in the Bay Area.<br>&nbsp;</p><h3>4. \u201cEA-flavored\u201d moral philosophy</h3><p>I\u2019ve been assuming a lot of \u201cEA-flavored\u201d takes on moral philosophy. This includes utilitarianism-flavored stuff, de-emphasizing rules/duties/justice/other moral goods, and totalist population ethics. Some of them are minority views, including among very smart subject experts. I am considering whether I should be more \u201cmorally centrist\u201d. Depending on my answer to this question, this might imply anything from spending a bit more time with my family to changing my work focus to something \u201crobustly good\u201d like clean tech.</p><p>&nbsp;</p><h2><strong>Interlude - information value</strong></h2><p>Now at the latest, I\u2019m expecting a reaction like \u201cBut how on earth are you going to make progress on questions like fundamental moral philosophy?\u201d.</p><p>First, note that I do not need to make progress on the fundamental philosophical/scientific issues as long as I can make progress on my epistemic strategies&nbsp;<i>about them.</i> E.g., I don\u2019t need to decisively prove utilitarianism is correct if I can just decide my epistemic strategy should be a bit more immodest and therefore I believe in utilitarianism somewhat immodestly.</p><p>Practically, looking at my list of assumptions to re-evaluate, I feel like I could easily change my mind on each of them in a matter of weeks or months. The main problem is that I haven\u2019t had the time to even look at them superficially and read 1 article or talk to 1 person about each of them. I think the information value in some deliberation is quite high and justifies investing some time. And better to do so sooner than later.</p><p>An objection to this may be: \u201cSure, your beliefs might change. But how can you expect your beliefs post-deliberation to be more accurate than pre-deliberation? Philosophers have been split over them for millennia and that doesn\u2019t change, no matter what you deliberate.\u201d</p><p>I would respond that this is precisely the claim of the concept called \u201cepistemics\u201d: that some ways of forming beliefs produce more accuracy than others. E.g., modest epistemics might produce more accurate beliefs than immodest epistemics. So if I have some credence that epistemics are powerful enough to do this in a domain like moral philosophy, then I\u2019m justified to think my accuracy might increase post-deliberation. (And I do have some credence in that.)</p><p>Also, I\u2019m expecting a gut reaction like \u201cI\u2019m skeptical that someone who just wants to have an impact in alignment field-building should end up having to do philosophy instead/first.\u201d I don\u2019t know much to say in response except that my reasoning seems to straightforwardly imply this. I would still be interested in whether many people have this gut reaction, please feel free to leave a comment if you do!</p><p>&nbsp;</p><h2><strong>Back to questions I need to re-evaluate</strong></h2><p>From the four assumptions listed above, it\u2019s probably evident that this is going in a very \u201cmeta\u201d epistemic direction. How much can I trust EA in forming my beliefs? How much can I trust myself, and in which situations?</p><p><strong>Here are the more \u201cmeta\u201d epistemics questions to re-evaluate:</strong></p><h3><br>5. Can I, as a smart and truthseeking person, do better than just deferring on complex empirical/moral questions?</h3><p>For example, can I do better than just deferring to the \u201clargest and smartest\u201d expert group on \u201cMight AI lead to extinction?\u201d (which seems to be EA). Can I instead look at the arguments and epistemics of EAs versus, say, opposing academics and reach a&nbsp;<i>better&nbsp;</i>conclusion? (Better in the sense of \u201cmore likely to be correct\u201d.) If so, how much and how should I do that in the details?</p><p>Just in case you are thinking \u201cclearly you&nbsp;<i>can&nbsp;</i>do better\u201d: Consider the case of a smarter, more knowledgeable person with better epistemics than me. I know such a person, and they\u2019ve even spent a lot more time thinking about \u201cMight AI lead to extinction?\u201d than me. They are probably also better than me at doing the whole weighing up different people\u2019s views thing. From this angle, it seems unlikely that I can do better than just deferring to them. (To their view \u2018all things considered\u2019, not their view \u2018by their own lights\u2019.)</p><p>Just in case you are thinking \u201cclearly you&nbsp;<i>can\u2019t&nbsp;</i>do better\u201d: This seems to contradict the way essentially everyone behaves in practice. I know no one who only ever defers to the \u201clargest and smartest\u201d expert group on everything, and doesn\u2019t presume to look at arguments or at least the epistemics of different expert groups.</p><p>&nbsp;</p><h3>6. Should I be more normal?</h3><p>If I tend to say I&nbsp;<i>can\u2019t&nbsp;</i>do better than just deferring on complex empirical/moral questions, should I hold very moderate views on everything? Should I be a third deontologist, third virtue ethicist, third consequentialist, so to speak? Should I believe climate change is the biggest issue of our time? Should I stop drinking meal shakes? (I\u2019m being mostly serious.)</p><p>(This is similar to point 4.)</p><p>&nbsp;</p><h3>7. Can EA, as a \u201csmart and truthseeking\u201d movement, assume its opinions are more accurate than other expert groups\u2019?</h3><p>We seem to often hope this is the case. E.g., we hope we are right about AI being an existential risk based on how smart and truthseeking we are. (In another sense, of course, we hope we are wrong.)</p><p>&nbsp;</p><h2><strong>More on information value</strong></h2><p>I want to reiterate here that, even though these questions seem daunting, I think I could learn something that changes my mind in a lasting way within weeks or months. For example, I could imagine finding out that almost no one supports epistemic modesty in its strongest form and becoming more immodest as a result. Or I could imagine finding out that influential EAs haven\u2019t thought about modesty much and becoming more cautious about \u201cEA beliefs\u201d as a result. I think it therefore makes sense to think about this stuff, and do so now rather than later.</p><p>&nbsp;</p><p><i>I am grateful to Isaac Dunn and Pedro Oliboni for helpful feedback on earlier versions of this post.</i></p><p><br>&nbsp;</p>", "user": {"username": "Luise"}}, {"_id": "nZSFdAdyyfeXF4Rpa", "title": "Would it make sense for EA funding to be not so much focused on top talent?", "postedAt": "2023-06-07T13:56:14.291Z", "htmlBody": "<p>Across loads of EA project, career development services and organisations in general there's a strong &nbsp;sentiment towards focusing on 'top talent'. For example in AI safety there are a few very well funded but extremely competitive programmes for graduates who want to do research in the field. Naturally their output is then limited to relatively small groups of people. An opposing trend seems to have gained traction in AI capability research as e.g. the \"We have no moat\" paper argued, where a load of output comes from the sheer mass of people working on the problem with a breadth-first approach. A corresponding opposite strategy for EA funds and career development services could be to spread the limited ressouces they &nbsp;have over a larger amount of people.</p><p>This concentration of funds on the development on a small group of top talent rather than distributing it over a wider group of people seems to me is a general sentiment quite prominent in the US economy and much less so in EU-countries like Germany, Scandinavia, the netherlands etc. I could imagine that EA origins in US/UK are a major reason for this structural focus.</p><p>Has anyone pointers to research on effectiveness comparisons between focusing on top talent vs a broader set of people, ideally in the context of EA? Or any personal thoughts/anecdotes to share on this?</p>", "user": {"username": "Franziska Fischer"}}, {"_id": "xFXeoYn872J9vr7jh", "title": "Article Summary: Current and Near-Term AI as a Potential Existential Risk Factor", "postedAt": "2023-06-07T13:53:22.201Z", "htmlBody": "<p>This post summarizes the new paper&nbsp;<a href=\"https://dl.acm.org/doi/10.1145/3514094.3534146\"><u>Current and Near-Term AI as a Potential Existential Risk Factor</u></a>, authored by Benjamin Bucknal and Shiri Dori-Hacohen. The paper diverges from the traditional focus on the potential risks of artificial general intelligence (AGI), instead examining how existing AI systems may pose an existential threat. The authors underscore the urgency of mitigating the harms from these AI systems by detailing how they might contribute to catastrophic outcomes.</p><p>Key points:</p><ol><li>Not only can current and near-term AI systems contribute to existential risk through misaligned artificial general intelligence, but they can also serve as amplifiers of other sources of existential risk (e.g., AI may help design dangerous pathogens.)</li><li>AI is reshaping power dynamics between states, corporations, and citizens in ways that can compromise political decision-making and response capabilities. This includes:<ul><li>An AI arms race between states could lead to nuclear conflict and divert resources away from mitigating existential risks.</li><li>The increasing power of multinational corporations over states could hinder effective regulation. AI technology is already outpacing ethical and legal frameworks.</li><li>The increased state surveillance through AI raises the possibility of repressive regimes.</li></ul></li><li>AI is affecting information transfer through social media and recommendation systems, potentially spreading false narratives, polarizing, eroding trust in institutions, and impeding effective collective action.</li><li>The authors provide a diagram showing causal relationships between near-term AI impacts and existential risk sources to illustrate how current AI can contribute to existential catastrophe.</li></ol><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/deMS6sbrr6GqvuxwR/cvkgjq2nvhsh9qmceyil\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/deMS6sbrr6GqvuxwR/pcldb1dvgjhbfzutit1o 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/deMS6sbrr6GqvuxwR/jdxzgpbqwouizkpbilhu 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/deMS6sbrr6GqvuxwR/lgf8dsprprskhykftmjz 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/deMS6sbrr6GqvuxwR/ypaviqmsn2nderqcp8rg 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/deMS6sbrr6GqvuxwR/cxpauiqqtt14r6mvxghr 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/deMS6sbrr6GqvuxwR/rxsxuiqriixywpoi7tca 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/deMS6sbrr6GqvuxwR/g0rptd8qoathyuetumbe 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/deMS6sbrr6GqvuxwR/ysljmnuevx5p8yviozj1 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/deMS6sbrr6GqvuxwR/kpb0rovjq8zepdu82umm 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/deMS6sbrr6GqvuxwR/etjmmkphkcqe8cmvwpxd 1600w\"></p><p>So in summary, the key takeaway is that current and near-term AI poses risks not just through potential future AGI, but also by significantly impacting society, politics and information ecosystems in ways that amplify other existential risks.</p><p>Acknowledgment: The idea for this summary and its commission came from&nbsp;<a href=\"https://forum.effectivealtruism.org/users/michaelchen\"><u>Michael Chen</u></a>, whom I would like to thank for his support.</p>", "user": {"username": "andreferretti"}}, {"_id": "PYeMoDripSZsasgi6", "title": "Rethink Priorities is hiring a Compute Governance Researcher or Research Assistant", "postedAt": "2023-06-07T13:22:44.026Z", "htmlBody": "<h2><strong>TL;DR</strong></h2><ul><li>Rethink Priorities\u2019 AI Governance &amp; Strategy team works to reduce long-term/extreme AI risks. We\u2019re seeking a Compute Governance Researcher or Research Assistant to tackle questions such as:<ul><li>how hardware security features could be used to facilitate AI governance</li><li>how&nbsp;<a href=\"https://www.csis.org/analysis/choking-chinas-access-future-ai\"><u>recent US export controls</u></a> will affect compute availability to different actors</li><li>the details of governance proposals such as ideas 2, 3, and 12 mentioned&nbsp;<a href=\"https://www.openphilanthropy.org/research/12-tentative-ideas-for-us-ai-policy/\"><u>here</u></a></li></ul></li><li>This role does not require prior governance-related experience.</li><li><strong>Deadline:</strong>&nbsp;<strong>June 12</strong>, end of day in US/EST time zone</li><li><strong>Type of Role:</strong> Permanent role, full- or-part-time (min. 20h / week)</li><li><strong>Location:&nbsp;</strong>Remote &amp; able to hire in most countries</li><li><strong>Compensation if full-time:</strong>&nbsp;$69,000 - $114,000 / year<strong>&nbsp;</strong>or equivalent in local currency (the amount is calculated using RP\u2019s salary algorithm and is dependent on prior relevant experience and corresponding title level)</li><li><strong>A brief pitch for applying:</strong><ul><li>Important compute-related policy windows for reducing AI risk are open now or likely to open soon.</li><li>This team's compute governance work is in-demand from decision-makers at various key institutions.</li><li>The team is able to simply work on what we actually think is most impactful and do so in whatever way we think is most efficient.</li><li>We actively value and work to support professional development, employee wellbeing &amp; satisfaction, and work-life balance. (Details below.)</li></ul></li></ul><h2><strong>About the Position&nbsp;</strong></h2><p>We are seeking a Compute Governance Researcher or Research Assistant (RA) to join our&nbsp;<a href=\"https://rethinkpriorities.org/team#longtermism:~:text=Artificial%20Intelligence%20(AI,Abi%20Olvera%20%E2%80%94%20Affiliate\">AI Governance and Strategy (AIGS) team</a>. This is an opportunity for technically inclined people to contribute to compute governance (<a href=\"https://forum.effectivealtruism.org/posts/BJtekdKrAufyKhBGw/ai-governance-needs-technical-work\">see also</a>), and does not require prior governance-related experience. We will determine whether to offer the successful candidate a Researcher or RA role based on the candidate\u2019s prior experience and their performance in our hiring process. (In any case, RAs can potentially get promoted to researchers later on, and Rethink Priorities puts significant emphasis on professional development, such as by allowing staff to dedicate 10% of their work time to that.)</p><p>This role is fully remote, and we are able to legally hire in most countries. We welcome applicants from all time zones, although you may be expected to attend meetings during working hours between UTC-8 and UTC+3 time zones. This role is equally open to candidates who are available for either full-time or part-time work, as long as you\u2019re available to work at least 20 hours per week.</p><h2><strong>About the Team</strong></h2><p>Our AIGS team tackles&nbsp;<a href=\"https://docs.google.com/document/d/1bkaPeijvzVyoCvd6t7IurPbWWe4MzImbVmR-sfkpt_s/edit\">a diverse set of questions</a> related to (1) what AI development and deployment scenarios may occur over the next few decades, and (2) how governments, firms, and other actors should prepare for, steer, and respond to various scenarios to reduce long-term/extreme risks. We engage closely with decision-makers (e.g., in labs, foundations, and policy communities) to increase the relevance and impact of our work.</p><p>Currently, our team is organized into four main workstreams: China-West relations, compute governance, corporate labs, and US regulation &amp; legislation. Compute governance essentially means governing access to significant, concentrated computing resources. This could be a uniquely feasible way to create guarantees that all of the most powerful AI systems are developed and deployed safely, and to thereby alleviate dangerous&nbsp;<a href=\"https://www.vox.com/future-perfect/23591534/chatgpt-artificial-intelligence-google-baidu-microsoft-openai\">race dynamics</a> and risks of both accidents and misuse. This workstream\u2019s current projects include research on how hardware security features could be used to facilitate compute (and thereby AI) governance, and how<a href=\"https://www.csis.org/analysis/choking-chinas-access-future-ai\"> recent US export controls</a> will affect compute availability to different actors. Future projects will likely include researching the details of governance proposals such as ideas 2, 3, and 12 mentioned<a href=\"https://www.openphilanthropy.org/research/12-tentative-ideas-for-us-ai-policy/\"> here</a>.</p><h2><strong>Key Responsibilities</strong></h2><p>If hired as a researcher, your responsibilities would likely include:</p><ul><li>Planning and conducting extended independent research projects</li><li>Collaborating with other team members on projects</li><li>Reviewing others\u2019 research</li></ul><p>If hired as a research assistant, your responsibilities would likely include:</p><ul><li>Supporting researchers by conducting various research tasks, including:<ul><li>Searching and writing up answers to various questions related to e.g. AI hardware and cybersecurity</li><li>Seeking out and reaching out to various experts to find answers to these questions</li></ul></li><li>Completing short (e.g., less than one month) research projects and write-ups, semi-independently</li><li>Providing support with other research-related tasks</li></ul><p>More specific examples of what you might do:</p><ul><li>Help flesh out details of compute governance proposals.&nbsp;<ul><li>For examples of what we mean by a compute governance proposal, see&nbsp;<a href=\"https://arxiv.org/pdf/2303.11341.pdf\">Shavit (2023)</a>.</li></ul></li><li>Read about relevant technical and security standards, and assess e.g., whether a product meeting a given standard could be used in a particular governance application, and what changes would make it more applicable.&nbsp;<ul><li>For example, what can and can\u2019t be done with systems that implement&nbsp;<a href=\"https://confidentialcomputing.io/wp-content/uploads/sites/10/2023/03/CCC-A-Technical-Analysis-of-Confidential-Computing-v1.3_unlocked.pdf\">confidential computing</a> support. More specifically, how could remotely attested trusted execution environments be used to verify how a given AI system was trained, or what its properties are, without revealing private or proprietary information?</li></ul></li></ul><h2><strong>What We Are Looking For</strong></h2><p><strong>The following attributes are each desired but not essential</strong>; we are open to hiring someone who currently lacks some of these attributes, and then adjusting the role around that and/or helping that person develop those attributes.&nbsp;</p><h3>Skills and Competencies</h3><ul><li>Ability to write clearly, concisely, and with&nbsp;<a href=\"https://www.openphilanthropy.org/research/reasoning-transparency/\">reasoning transparency</a></li><li>Intellectual curiosity and open-mindedness, including willingness to carefully consider ideas</li><li>Ability to find, read, critically assess, and apply research from various disciplines and on various topics</li><li>Resourcefulness and problem-solving ability&nbsp;</li><li>Attention to detail, and a commitment to maintaining high quality and accuracy in all research outputs</li><li>Good interpersonal skills and comfort with reaching out to various people outside of Rethink Priorities</li><li>Comfort with remote work (the AIGS team is fully remote, with staff in multiple time zones)</li><li>Ability to prioritize well, meet deadlines, and work productively</li></ul><h3>Knowledge and Experience</h3><p>Below is a list of topics about which knowledge would be helpful, in roughly decreasing order of priority. That said, we do not expect any applicant to have extensive knowledge of all or even most of these topics.&nbsp;</p><ul><li>Computer hardware, particularly data center hardware used for large-scale AI training and inference</li><li>Cybersecurity, especially hardware security</li><li>Distributed computing, especially as applied to large-scale AI systems</li><li>The semiconductor supply chain, especially for high-end chips</li><li>AI and machine learning</li><li>Technical, safety, and security standards and the processes by which they are set</li><li>The US government and policymaking processes, both executive and legislative</li><li>Arguments for existential risks from AI and proposed ways to reduce said risks through governance</li><li>Relevant players in the AI industry</li><li>International relations between \u2013 and technology policy in \u2013 any of the US, China, South Korea, Taiwan, Japan, and/or the Netherlands</li></ul><h2><strong>What We Offer</strong></h2><h3>Compensation</h3><ul><li>Annual salary between the following ranges for a full-time<i>&nbsp;</i>research assistant or researcher position, prorated for part-time work:&nbsp;<ul><li>USD: $69,000 - $114,000 pre-tax</li><li>GBP: \u00a354,000 - \u00a388,000 pre-tax</li><li>EUR: \u20ac61,000 - \u20ac101,000 pre-tax</li></ul></li><li>The exact salary will be based on the candidate\u2019s prior relevant experience and corresponding title level, and calculated using RP\u2019s salary algorithm. To ensure fairness, RP does not negotiate salaries.&nbsp;&nbsp;</li><li>Compensation is not restricted to the currencies listed above. Payments may be made in different currencies and payment intervals depending on the location of applicants and legal requirement</li></ul><h3>Other Benefits</h3><ul><li>Opportunity to contribute to a fast-growing, high-impact organization \u2014 our research is used by key decision-makers who influence the distribution of hundreds of millions of charitable dollars</li><li>Flexible work hours and location</li><li>Comprehensive global benefits package (while they vary by country, we make every effort to ensure that our benefits package is equitable and high-quality for all staff)&nbsp;</li><li>Generous paid time off leave, including, but not limited to:<ul><li>Unlimited vacation with a minimum of 30 days off per year (including public local holidays, vacation time, and \u201cmandated\u201d 3-weeks total mid- and end-year organization-wide breaks)</li><li>Unlimited (within reason) personal and sick leave</li><li>Parental leave - up to 6 months of parental leave during the first 2 years after a child\u2019s birth or adoption, for parents of all genders</li></ul></li><li>For more details about our benefits, please see&nbsp;<a href=\"https://drive.google.com/file/d/1iTqnTYCEUxTFsZ4FwJSta0sCwC6Qa0e7/view\">Benefit Package for Permanent Roles</a></li><li>A caring team that values respectful work relations and a healthy work-life balance</li><li>Opportunities to grow/advance your career and engage in professional development</li><li>Low administrative bureaucracy</li><li>We don\u2019t provide snacks but we could mail you a box of Oreos if you want!</li></ul><h2><strong>Additional Information&nbsp;&nbsp;</strong></h2><ul><li><strong>Extension requests:&nbsp;</strong>We will try to accommodate extension requests that are made before the deadline and are up to five (5) days. To ensure fairness to other applicants, we generally cannot accommodate extension requests that are made on or after the application deadline or are longer than 5 days, and cannot accept late submissions.</li><li><strong>Inclusivity and fairness:&nbsp;</strong>&nbsp;RP is committed to finding the best people for our team and to building an inclusive, equitable, and supportive community for you to thrive and do your best work. So please don\u2019t hesitate to apply for a role regardless of your age, gender identity/expression, political identity, personal preferences, physical abilities, veteran status, neurodiversity, or any other background. We provide reasonable accommodations and benefits, including flexible work schedules and locations, mental health coverage in medical benefits (as available), and technology budgets and professional development time that can be used to purchase assistive technology or engage in job coaching.&nbsp;</li><li><strong>Accessibility:</strong> We\u2019re committed to running an inclusive and accessible application process. We warmly invite you to reach out to careers@rethinkpriorities.org with any questions or accessibility requests such as chat box use during interviews or time extension requests for any assessments that impose a time limit.</li><li><strong>Language:&nbsp;</strong>Please submit all of your application materials in English, and note that we require professional level English proficiency.</li><li><strong>Travel:&nbsp;</strong>Travel is not a requirement for this position. A majority of our staff travel a few times per year for conferences, team and all-staff retreats, and other work-related purposes, and we prefer if staff can travel for at least one retreat per year. But this won\u2019t be taken into account in the hiring process, and we likely can and often do make accommodations such as allowing virtual participation for at least parts of retreats.</li><li><strong>Other:&nbsp;</strong><ul><li>Visit our&nbsp;<a href=\"https://rethinkpriorities.org/career-opportunities\">Career Opportunities</a> webpage if you\u2019d like to know more about our hiring process, culture, and what working at RP is like.</li><li>Please&nbsp;<strong>do not</strong> include a cover letter, photograph, or headshot of yourself, or any personal information that is not relevant to the role for which you\u2019re applying (including marital status, age, identity traits, etc.).&nbsp;</li><li>Please&nbsp;<strong>do not</strong> ask our staff members involved in the hiring process to meet with you \u2013 to ensure fairness, we try to minimize such interactions.</li></ul></li></ul><h2><strong>About Rethink Priorities</strong></h2><p>Founded in 2018,&nbsp;<a href=\"http://rethinkpriorities.org/\">Rethink Priorities</a> (RP) is a nonprofit organization that addresses global priorities\u2014important and neglected issues\u2014by researching solutions and strategies, mobilizing resources, and empowering our team and others. RP\u2019s mission is to generate the most significant possible impact for others in the present and the long-term future.&nbsp;</p><p>Our cause areas include animal welfare, global health and development, climate change, artificial intelligence, and other work to safeguard a flourishing long-term future. RP also aims to understand and support the professional communities working on these issues. Each researcher tends to focus on one particular cause area.</p><p><strong>Rethink Priorities works as all of the following:</strong></p><ul><li>A consultancy doing commissioned work in response to demands from organizations doing high-impact work&nbsp;</li><li>A research institute driven by research agendas we set according to our own priorities.</li><li>A think tank aiming to inform public policy to improve the world.</li><li>An accelerator, incubator, and base for entrepreneurial projects.</li></ul><p><strong>Some of RP\u2019s recent accomplishments include:&nbsp;</strong></p><ul><li>Publishing a nine-post sequence on&nbsp;<a href=\"https://forum.effectivealtruism.org/s/8rYkpiFhbb4HsbzFc\">understanding the diffusion of large language models</a> which presents key findings from case studies on the diffusion of eight language models that are similar to GPT-3.&nbsp;</li><li>Conducting and writing up results from&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/g4fXhiJyj6tdBhuBK/survey-on-intermediate-goals-in-ai-governance\">an expert survey on AI strategy</a>, which has informed key decision-makers and been included in reading lists for people entering this field.</li><li>Organizing a well-received summit for 35 leading members of the existential-risk-focused AI strategy and policy field.&nbsp;</li><li>Producing public and nonpublic reports&nbsp;<a href=\"https://docs.google.com/document/d/1bkaPeijvzVyoCvd6t7IurPbWWe4MzImbVmR-sfkpt_s/edit\">on various topics</a>, including&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/L8GjzvRYA9g9ox2nP/prospects-for-ai-safety-agreements-between-countries\">prospects for AI safety agreements between countries</a>.</li><li>Helping major foundations to answer their questions on climate change solutions, weather forecasting in lower- and middle-income countries, increasing access to medicine, and the effectiveness of&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xanSjg6Hq2PaGEkZP/how-effective-are-prizes-at-spurring-innovation\">prizes</a> and other interventions.&nbsp;</li><li>Comparing the capacity of&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\">different animal species</a> to experience pleasure and pain to help philanthropists decide how to allocate funding.</li><li>Investigating various&nbsp;<a href=\"https://rethinkpriorities.org/animal-welfare\">animal welfare</a>&nbsp;<a href=\"https://rethinkpriorities.org/publications/effectiveness-of-a-theory-informed-documentary-to-reduce-consumption-of-meat-and-animal-products\">interventions</a>, as well as bringing to light the neglected areas of&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/EDCwbDEhwRGZjqY6S/invertebrate-welfare-cause-profile\">invertebrate</a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/fZF9ffZD2kkpDy7jB/research-summary-brain-cell-counts-in-black-soldier-flies\">insect</a> welfare.</li><li>Publishing pieces on&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/AuhkDHEuLNxqx9rgZ/a-new-database-of-nanotechnology-strategy-resources\">nanotechnology</a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/E5vp2LCEfkrrLWozJ/potentially-great-ways-forecasting-can-improve-the-longterm\">ways to use forecasting to improve the long-term future</a>, as well as&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Na6pkfpZrfyKBhEcp/interested-in-ea-longtermist-research-careers-here-are-my\">supporting</a> those interested in these types of topics.</li><li>Launching a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/AFgvA9imsT6bww8E3/announcing-the-rethink-priorities-special-projects-program\">Special Projects Team</a> to incubate promising new initiatives, such as&nbsp;<a href=\"https://epochai.org/\">Epoch</a> (a new AI research organization) and&nbsp;<a href=\"https://condor.camp/\">Condor Camp</a> (longtermism movement-building in Brazil and Latin America).</li><li>Conducting surveys to better understand the&nbsp;<a href=\"https://rethinkpriorities.org/ea-movement-research\">Effective Altruism community</a></li></ul><p>We welcome you to review our database of published work&nbsp;<a href=\"https://rethinkpriorities.org/research\">here</a>.&nbsp;</p><p>We\u2019re supported by&nbsp;<a href=\"https://www.openphilanthropy.org/\">Open Philanthropy</a>, the&nbsp;<a href=\"https://survivalandflourishing.fund/\">Survival and Flourishing Fund</a>, and additional institutional and individual donors.&nbsp;</p><h2>Information on applying</h2><p>To apply, please respond to the prompts in&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/postings/f553d816-53ef-40e6-84bb-257d550ec52b/applications/new\"><u>the application form</u></a>.&nbsp;<strong>We ask that you spend no more than one (1) hour preparing your responses to the knowledge and experience questions.</strong></p><p><strong>Application Deadline: June 11, 2023, at the end of the day in US/Eastern (EST) time zone.&nbsp;</strong></p><p><strong>Q&amp;A Webinar:&nbsp;</strong>You can find the recording of the Q&amp;A webinar held on May 26, Friday<strong>&nbsp;</strong><a href=\"https://drive.google.com/file/d/1BRdziECKlCdZMKd0xAu0JzRPuxDAWptp/view\"><strong>here</strong></a> and the chat history&nbsp;<a href=\"https://drive.google.com/file/d/19J2m2yfiZCfr_UAaW_wvfy3P9c9FSpz-/view\"><strong>here</strong></a>.</p><p><strong>Contact:</strong> Please email careers@rethinkpriorities.org&nbsp; if you have any questions.</p><p><strong>Resume/CV:&nbsp;</strong>Feel free to upload your CV i<strong>f you want&nbsp;</strong>on the application page. But this is optional and&nbsp;<strong>will not be used in our evaluation process.&nbsp;</strong>We will use CVs only for purposes like later considering whether to refer you to other future roles within RP or at other organizations if you have consented for us to do so.</p><p><strong>We invite anyone to apply</strong> and will evaluate applications based on anonymized prompt answers, so please ensure they represent your fit for the position well. We aim to select more for revealed knowledge and skills than for experience in itself. We also want to note that significant governance/policy knowledge is&nbsp;<strong>not</strong> required.<br><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PYeMoDripSZsasgi6/lk8cjenbiajwzetm32yx\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PYeMoDripSZsasgi6/os7ugctnqs1xigbxfd8m 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PYeMoDripSZsasgi6/cdrvhri0zvgth07amcku 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PYeMoDripSZsasgi6/levzaxymunqakg1lyv0x 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PYeMoDripSZsasgi6/jrc46vex838bp1d7iu5m 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PYeMoDripSZsasgi6/eiaarfzkttsebshnj4op 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PYeMoDripSZsasgi6/vcycan6eu59zy6ezalo2 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PYeMoDripSZsasgi6/d2hr9nekhxdz74rmmraa 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PYeMoDripSZsasgi6/zuzw3jekgrxxan4dub9x 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PYeMoDripSZsasgi6/ujza81mzvihoda1fygno 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PYeMoDripSZsasgi6/wy0olplvbiuc640beyfc 1600w\"></p><p><a href=\"https://rethinkpriorities.org/\"><i>Rethink Priorities</i></a><i> is a think tank dedicated to informing decisions made by high-impact organizations and funders across various cause areas. If you are interested in RP\u2019s work, please visit our&nbsp;</i><a href=\"https://www.rethinkpriorities.org/research\"><i>research database</i></a><i> and subscribe to our&nbsp;</i><a href=\"https://www.rethinkpriorities.org/newsletter\"><i>newsletter</i></a><i>.&nbsp;</i></p>", "user": {"username": "MichaelA"}}, {"_id": "2boNQdpy785t34xnC", "title": "Mapping out collapse research", "postedAt": "2023-06-07T12:10:58.214Z", "htmlBody": "<p><i>This here is a cross post for my blog </i><a href=\"https://existentialcrunch.substack.com/\"><i>Existential Crunch</i></a><i>. It is the first official entry in a living literature review on societal collapse, which is funded by Open Phil. They are also </i><a href=\"https://www.openphilanthropy.org/focus/innovation-policy/\"><i>looking into other possible topics </i></a><i>if you are interested. The idea behind a living literature review is that it is an easily accessible resource, which gets continuously updated &nbsp;in the light of new evidence and insights.&nbsp;</i></p><p>Societal collapse has been with humanity since the agricultural revolution, but only during the enlightenment did humans gain the means to understand these cataclysms in a systematic way. This line of research continues today, and in this post, I will map out how the field of collapse research has developed over time, what different schools exist (Figure 1) and what factors they emphasize as leading to collapse. Much of this is based on Brozovi\u0107 (2023), who read and summarized an impressive amount of collapse research. We will first explore the different schools of collapse research in roughly chronological order. These schools can also be seen as representatives of how we should think about collapse in general. After introducing these major schools, we\u2019ll use them as a lens to begin investigating two important questions: what even is a collapse? And what causes it?</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04c5c702-e6cf-4baf-8e80-053291946c3e_1353x419.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04c5c702-e6cf-4baf-8e80-053291946c3e_1353x419.png\" alt=\"Schools\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04c5c702-e6cf-4baf-8e80-053291946c3e_1353x419.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04c5c702-e6cf-4baf-8e80-053291946c3e_1353x419.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04c5c702-e6cf-4baf-8e80-053291946c3e_1353x419.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04c5c702-e6cf-4baf-8e80-053291946c3e_1353x419.png 1456w\"></a></p><p>Figure 1: Schools of collapse research.</p><h3>Traditional theories of cultural evolution</h3><p><i>\u201cCultures rise through toughness and fall through their decadence\u201d</i></p><p>In this school of research, cultures are seen as organisms. They develop and adapt, and if they fail to do so, they die. Much of it is originally based on the main work by Edward Gibbon, \u201cThe Decline and Fall of the Roman Empire\u201d. In this opus, Gibbons explores what might have caused the end of the Roman Empire. According to him the main reason the Roman Empire (and thus possibly other cultures as well) ceased to exist was the loss of civic virtues in its citizens, caused by decadence.</p><p>His ideas were very influential for collapse research, and how collapse is viewed in general. For example, in 1926, Spengler\u2019s \u201cDecline of the West\u201d argued \u201cThe West\u201d peaked before World War I, and would subsequently crumble, because it had lost its ancient ways. However, today, this view on collapse is seen as discredited, as it is too simplistic, and often inscribes certain cultures as inherently more valuable than others. Still, these ideas are deeply ingrained, especially in western, conservative circles. There they are used for example to frame migrants as disruptors of culture, who will bring on the fall of civilization by corrupting enlightenment values.</p><h3>Limits to growth</h3><p><i>\u201cPeople will always try to consume all available resources\u201d</i></p><p>Limits to growth is the idea that the Earth has a fixed carrying capacity it can support in the long term. This kind of thinking has experienced several transformations over time. The first variant of this view is the concept of the Malthusian Trap, which was first developed by Thomas Malthus in 1798. The main idea of the Malthusian trap is that poor people will always have as many children as can be supported by society\u2019s resources, so that available resources act as a strict regulator of overall population size, possibly via famines and other catastrophes (see <a href=\"https://existentialcrunch.substack.com/p/escaping-the-malthusian-trap\">an earlier post here for a more detailed explanation</a>). This idea had a resurgence in the 1960s in the USA around the book \u201cPopulation Bomb\u201d by Paul Ehrlich, but quickly decreasing population growth in the 20th century has proven it untrue for post-industrial civilizations.</p><p>The idea of the Malthusian trap lives on today in the ideas of limits to growth by the Club of Rome (Meadows et al., 1974), in the notion of so-called planetary boundaries (Rockstr\u00f6m et al., 2009) and the degrowth movement. Planetary boundaries are defined as parts of the Earth system that have to stay within certain boundaries to make this planet habitable for the long term. The underlying assumption is that human civilization developed in a stable window of environmental conditions in the Holocene, and is therefore adapted to it. Every transgression of those conditions could lead to collapse, as humanity is not adapted to the environment outside of them.</p><p>How rigid those boundaries are, and how much they could contribute to a collapse is hotly debated. For example, Kareiva &amp; Carranza (2018) argue that from all planetary boundaries only climate change has a clear path to a potential collapse, while Cernev (2022) explores scenarios where also other overstepped planetary boundaries could lead to an increased chance of a global collapse . Some argue that the path from exponential growth to collapse is typical for any kind of civilization, even extraterrestrial ones (see <a href=\"https://existentialcrunch.substack.com/p/the-universal-anthropocene\">here for a deeper explanation</a>).</p><h3>Collapse by complexity</h3><p><i>\u201cSocieties get ever more complex, but at some point complexity cannot be supported\u201d</i></p><p>In the complexity view on collapse (introduced by Joseph Tainter, 1988), societies have to continuously solve problems (for example, how to provide more food for their citizens). These problems arise from a society\u2019s interaction with its environment and internal dynamics. The key assumption is that every solution found increases the complexity of the society, and thus the energy it needs to sustain itself. The trouble is, usually societies can only increase in complexity. Past solutions have to be permanently maintained, while new problems need new and additional solutions. For example, consider a sewage system. This stops people from simply dropping their feces in the streets, but you cannot \u201csolve\u201d sewage. Instead you have a very complex sewage system that you most continuously monitor, repair and extend.</p><p>Tainter also argues that problems arise if the energy return of investment declines, by which he means that you have to continuously increase the amount of energy you invest to get new energy. This happens because societies start with the most easily accessible energy and once this source of energy is used up, the next sources of energy will necessarily be more complex to access. A society collapses at the point where it lacks the energy to sustain the complexity it has accumulated (Brozovi\u0107, 2023).</p><p>This theory is seen as the most compact theory of collapse, as it can be used for every society without having to make adaptations (Brozovi\u0107, 2023).</p><h3>Structural demographic theory</h3><p><i>\u201cHistory is made of recurring patterns and we can measure them\u201d</i></p><p>If traditional theories of cultural evolution are a narrative based style of collapse research, structural demographic theory is the opposite and tries to be as quantitative as possible. The main proponent of this kind of history, Peter Turchin, uses mathematical modeling that relies on extensive data collected from history to explore societal dynamics in the past. This has resulted in some very impressive datasets about history (Turchin et al., 2023), which can be used to test specific hypotheses. One of the main findings in this field of research are called secular cycles (Turchin &amp; Nefedov, 2009).</p><p>Secular cycles describe a recurring pattern in history. It starts with a growing population that also has room to grow. The growth leads to more resources for everyone, which leads to an overall cooperative atmosphere. However, at some point, the room to grow shrinks, so that people can only get more resources if they take it from others. Population increases also depress real wages whilst also leading to an overproduction of elites relative to elite positions.This decreases trust and peace until it possibly triggers a redistributive event, which could be something like a civil war, which in turn decreases the population and the cycle starts anew.</p><p>While this approach of history creates very elegant theories, it has also been criticized as being too simplistic and having a too narrow focus on easily measurable variables (Maini, 2020).</p><h2>What even counts as collapse?</h2><p>So far we\u2019ve seen four main approaches to thinking about societal collapse. The cultural school puts the blame on the cultural decadence of a society. The limits to growth school emphasizes societies pushing beyond the environmental envelope that can sustain them. The complexity collapse school emphasizes the unsustainable increase in social complexity. And structural demographic schools emphasize a shift from positive sum growth to zero sum conflict once room to expand reaches its limits. However, are all of those approaches even talking about the same thing?</p><p>There is no definitive definition of collapse. This has become even more true as collapse research has shifted from being a part of history, to the more transdisciplinary kind of research it is today (Brozovi\u0107, 2023). Over time, collapse research has become less qualitative, and more quantitative (going from Gibbon to Turchin).</p><p>Some examples of collapse definitions:</p><p>Collapse is the fragmentation or disarticulation of a particular political apparatus and transformation is a broad concept engulfing different sorts of societal changes (Faulseit, 2016)</p><p>Collapse is a rapid, uncontrolled, unexpected and ruinous decline of something that had been going well before (Bardi, 2020)</p><p>Collapse is a drastic decrease in human population size and/or political/economic/social complexity over a considerable area for an extended time (Diamond, 2011)</p><p>All those definitions have one thing in common: They are rather vague. What counts as a drastic decrease? What is a significant loss? How would we even measure sociopolitical complexity? When reading anything about collapse, we should always keep in mind the definition of collapse the author had in mind, because drawing conclusions depends on it. Interestingly, even though the possible definitions vary, the past collapse events references are the same. In a certain way it seems that you could answer the question \u201cWhen can we classify an event as collapse?\u201d with \u201cI know it when I see it\u201d. For example, there seems to be a very broad agreement that the end of the Roman Empire can be counted as a collapse, independent of the specific definition of collapse.</p><p>Others argue against the idea of collapse as a whole. Lawler (2010) argues that, for instance, the fall of the Maya was not really a collapse, but more of a transformation to a different kind of society. However, at least to me, this seems to be mostly a semantics game. Whenever someone argues that historical event X was not a collapse, but a transformation, they typically still acknowledge that the society at hand underwent significant changes and societal complexity was lost. The disagreement is not about the event, but if it was quick, big and permanent enough to count as collapse. This ultimately boils down to a value judgment about the preferred definition of collapse by the person who is making the claim. As the definition of collapse is so vague this cannot be really resolved until there is a widely accepted definition of collapse. In this series I\u2019ll be using a variety of definitions, though focusing primarily on the kinds of events that people regard as bad.</p><h2>Frameworks for causes of collapse</h2><p>Whereas the definition of collapse remains vague, the search for causes of collapse has found somewhat more crisp results. We have come a long way from Edward Gibbon\u2019s explanation that the Romans were simply not virtuous enough to avoid collapse. In general, there is now the view in collapse research that monocausal explanations are too simplistic (Brozovi\u0107, 2023). Still, in the explanations of collapse we can see a distinction between research focussing on society\u2019s interaction with the external environment and research focussing on the internal mechanisms of a society or even viewing collapse as a concept that applies to all complex systems and not only societies. In the following I will show three frameworks which highlight the differences between these research directions.</p><h3>Environmental Framework (Example: Jared Diamond)</h3><p>Let\u2019s begin with Jared Diamond\u2019s work, one of the most well known collapse frameworks, which focuses on society\u2019s interaction with its environment. Overall there are five factors Diamond sees as the main drivers of collapse (though note Diamond believes it is possible to take actions to mitigate each of these factors - for example, a well managed societal response can help mitigate a sudden shift in the climate).</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95f7ad-364e-4c3f-98f3-09273844d175_1347x323.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95f7ad-364e-4c3f-98f3-09273844d175_1347x323.png\" alt=\"Factor Diamond\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95f7ad-364e-4c3f-98f3-09273844d175_1347x323.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95f7ad-364e-4c3f-98f3-09273844d175_1347x323.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95f7ad-364e-4c3f-98f3-09273844d175_1347x323.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95f7ad-364e-4c3f-98f3-09273844d175_1347x323.png 1456w\"></a></p><p>Figure 2: Jared Diamond\u2019s main factors of collapse</p><p>In addition to the five main factors, Diamond also identifies twelve environmental problems that are often connected to collapse. This framework has been used by others to create a conceptual model (Figure 3) (Carter, 2013). While no one has used it for further research yet, it highlights nicely the inherent complexity of trying to capture the essence of why collapse happens.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3001815b-3d42-4a5a-a49c-4740b37afe6f_793x812.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3001815b-3d42-4a5a-a49c-4740b37afe6f_793x812.png\" alt=\"Model Carter\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3001815b-3d42-4a5a-a49c-4740b37afe6f_793x812.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3001815b-3d42-4a5a-a49c-4740b37afe6f_793x812.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3001815b-3d42-4a5a-a49c-4740b37afe6f_793x812.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3001815b-3d42-4a5a-a49c-4740b37afe6f_793x812.png 1456w\"></a></p><p>Figure 3: Reproduced figure from Carter (2013) based on Diamond (2011). Red: Negative Feedback, Green: Positive Feedback, Purple: Positive/Negative Feedback, Grey boxes: Main Factors, White Boxes: Subfactors.</p><p>While Diamond\u2019s focus on the environment and humanity\u2019s interaction with it makes intuitive sense to me, it should be noted that Diamond\u2019s work has been criticized by anthropologists for being too focused on the environment (e.g. Butzer (2012)), and misrepresenting or cherry picking historical evidence (Hunt &amp; Lipo, 2012).</p><h3>Societal Framework (Example: Karl Butzer)</h3><p>In contrast to Diamond, Karl Butzer focuses on the societal response instead of the environment (Butzer, 2012). The environment has a role in potentially triggering an economic crisis, but even this is only assumed to happen after an internally generated economic decline. When a crisis is triggered the reaction depends on whether society manages to end up in positive or negative feedback loops. In either case, reconstruction is possible, but is more likely to be incomplete after a collapse. Butzer further discriminates between a collapse and complete societal devolution, though he does not really define either. It reads as if you can still recover from a collapse, while a devolution leads to a complete dissolution of state-like structures. This focus on economic growth can also be found in other works, e.g. Samo Burja\u2019s great founder theory (Burja, 2018).</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f66ce0d-a1ed-4f5c-9671-dc47375f9be4_528x812.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f66ce0d-a1ed-4f5c-9671-dc47375f9be4_528x812.png\" alt=\"Model Butzer\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f66ce0d-a1ed-4f5c-9671-dc47375f9be4_528x812.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f66ce0d-a1ed-4f5c-9671-dc47375f9be4_528x812.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f66ce0d-a1ed-4f5c-9671-dc47375f9be4_528x812.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f66ce0d-a1ed-4f5c-9671-dc47375f9be4_528x812.png 1456w\"></a></p><p>Figure 4: Karl Butzer\u2019s Framework of societal collapse.</p><h3>Meta Framework (Example: Cumming and Peterson)</h3><p>The framework of Cumming &amp; Peterson (2017) goes up one level in abstraction in comparison to the other two. While Butzer and Diamond are only interested in societal collapse, Cumming and Peterson see collapse as a more general phenomenon. The examples they look at are ranging from Bronze Age collapse to the collapse of bee hives. They come to the conclusion that the type of collapse that happens is based on the structure of the system. For example, if you have a hierarchical and individualistic structure you get the kind of collapse as Peter Turchin describes in his ideas of secular cycles. Overall, I find it more convincing, as it is a more holistic picture. It is also even more abstract than the other models proposed, which makes it more difficult to apply. However, Cumming and Peterson articulate that they see their work as the basis of an interdisciplinary study of collapse, which sounds quite promising.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29b064a6-6f9a-4976-9da5-6342cca8c26c_1188x844.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29b064a6-6f9a-4976-9da5-6342cca8c26c_1188x844.png\" alt=\"Model Meta\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29b064a6-6f9a-4976-9da5-6342cca8c26c_1188x844.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29b064a6-6f9a-4976-9da5-6342cca8c26c_1188x844.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29b064a6-6f9a-4976-9da5-6342cca8c26c_1188x844.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29b064a6-6f9a-4976-9da5-6342cca8c26c_1188x844.png 1456w\"></a></p><p>Figure 5: Types of collapse based on the structure of the system from Cumming and Peterson (2017)</p><h2>Conclusion</h2><p>My main takeaway is that this field still has a long way to go. This is troubling, because in our society today we can see signs that could be interpreted as indications of a nearing collapse. There are voices warning that our global society has become decadent (writers like <a href=\"https://www.vox.com/policy-and-politics/2020/2/28/21137971/the-decadent-society-ross-douthat-book\">Ross Douthat</a>), that we are pushing against environmental limits (for example, <a href=\"https://rebellion.global/\">Extinction Rebellion</a>), that we are having a decreasing return of investment for our energy system (for example, work by <a href=\"https://royalsocietypublishing.org/doi/10.1098/rsta.2013.0126\">David Murphy</a>) and that there has been an overproduction of elites in the last decades (writers like <a href=\"https://www.noahpinion.blog/p/the-elite-overproduction-hypothesis\">Noah Smith</a>). This means we have warning signs that fit all major viewpoints on collapse. Moreover, new technological capabilities pose novel dangers that require us to extrapolate beyond the domain of historical experience. All this means that understanding how collapse really happens is rather urgent.</p><p>While the work of Jared Diamond and others brought the field forward by sparking a variety of discussions and new research, the field of collapse research is still in the process of finding a theory that is widely accepted in the community. There is no clear definition of what collapse is. Depending on their background, people argue for a bigger focus on the environment or a bigger focus on societal factors. This also seems to lead to a lot of discussion about semantics. Researchers criticize each other\u2019s definition of collapse, even though most of the researchers seem to agree that there are clearly discontinuities in the historical record. The closest to having an actual model of collapse prediction is the field of structural demographic theory. However, their focus is mostly on internal mechanisms of society, and less so on the interaction of society with the environment. A good way forward would be to couple structural demographic theory work with a more complex environmental model. This could also be combined with the work of Cumming and Peterson, to make it more widely applicable. However, this is a long term project, which requires much more work. Still, it would be an important piece of work, as we need to understand how past collapse has worked, to be safe against the complex, intricate and possible unexpected ways our society might be in danger today.</p><h2>References</h2><p>Bardi, U. (2020). Before the Collapse: A Guide to the Other Side of Growth. Springer International Publishing. https://doi.org/10.1007/978-3-030-29038-2</p><p>Brozovi\u0107, D. (2023). Societal collapse: A literature review. Futures, 145, 103075. https://doi.org/10.1016/j.futures.2022.103075</p><p>Butzer, K. W. (2012). Collapse, environment, and society. Proceedings of the National Academy of Sciences, 109(10), 3632\u20133639. https://doi.org/10.1073/pnas.1114845109</p><p>Carter, M. J. (2013). A Sociological Model of Societal Collapse. Comparative Sociology, 12(2), 236\u2013254. https://doi.org/10.1163/15691330-12341262</p><p>Cernev, T. (2022). Global catastrophic risk and planetary boundaries: The relationship to global targets and disaster risk reduction. https://www.undrr.org/publication/global-catastrophic-risk-and-planetary-boundaries-relationship-global-targets-and</p><p>Cumming, G. S., &amp; Peterson, G. D. (2017). Unifying Research on Social\u2013Ecological Resilience and Collapse. Trends in Ecology &amp; Evolution, 32(9), 695\u2013713. https://doi.org/10.1016/j.tree.2017.06.014</p><p>Diamond, J. M. (2011). Collapse: How societies choose to fail or survive. Penguin Books.</p><p>Faulseit, R. K. (2016). Beyond Collapse: Archaeological Perspectives on Resilience, Revitalization, and Transformation in Complex Societies. SIU Press.</p><p>Burja S. (2018, September 5). Great Founder Theory. https://samoburja.com/great-founder-theory/</p><p>Hunt, T., &amp; Lipo, C. (2012). Ecological Catastrophe and Collapse: The Myth of \u201cEcocide\u201d on Rapa Nui (Easter Island) (SSRN Scholarly Paper No. 2042672). https://doi.org/10.2139/ssrn.2042672</p><p>Kareiva, P., &amp; Carranza, V. (2018). Existential risk due to ecosystem collapse: Nature strikes back. Futures, 102, 39\u201350. https://doi.org/10.1016/j.futures.2018.01.001</p><p>Lawler, A. (2010). Collapse? What Collapse? Societal Change Revisited. Science, 330(6006), 907\u2013909. https://doi.org/10.1126/science.330.6006.907</p><p>Maini, A. (2020). On Historical Dynamics by P. Turchin. Biophysical Economics and Sustainability, 5(1), 3. https://doi.org/10.1007/s41247-019-0063-x</p><p>Meadows, D. H., Club of Rome, &amp; Potomac Associates (Eds.). (1974). The limits to growth: A report for the club of rome\u2019s project on the predicament of mankind (2. ed). Universe books.</p><p>Rockstr\u00f6m, J., Steffen, W., Noone, K., Persson, \u00c5., Chapin, F. S. I., Lambin, E., Lenton, T., Scheffer, M., Folke, C., Schellnhuber, H. J., Nykvist, B., de Wit, C., Hughes, T., van der Leeuw, S., Rodhe, H., S\u00f6rlin, S., Snyder, P., Costanza, R., Svedin, U., \u2026 Foley, J. (2009). Planetary Boundaries: Exploring the Safe Operating Space for Humanity. Ecology and Society, 14(2). https://doi.org/10.5751/ES-03180-140232</p><p>Tainter, J. (1988). The Collapse of Complex Societies. Cambridge University Press.</p><p>Turchin, P., &amp; Nefedov, S. A. (2009). Secular cycles. Princeton University Press.</p><p>Turchin, P., Whitehouse, H., Larson, J., Cioni, E., Reddish, J., Hoyer, D., Savage, P. E., Covey, R. A., Baines, J., Altaweel, M., Anderson, E., Bol, P., Brandl, E., Carballo, D. M., Feinman, G., Korotayev, A., Kradin, N., Levine, J. D., Nugent, S. E., \u2026 Fran\u00e7ois, P. (2023). Explaining the rise of moralizing religions: A test of competing hypotheses using the Seshat Databank. Religion, Brain &amp; Behavior, 13(2), 167\u2013194. https://doi.org/10.1080/2153599X.2022.2065345</p>", "user": {"username": "FJehn"}}, {"_id": "yfuoCzFNdsp6CHX5z", "title": "Unveiling the Challenges and Potential of Research in Nigeria: Nurturing Talent in Resource-Limited Settings", "postedAt": "2023-06-07T11:05:04.416Z", "htmlBody": "<p><strong>Summary</strong></p><p>This essay sheds light on the challenges faced by academics working in public institutions in Nigeria, aiming to provide insight into the limited impact of research in the Nigerian context. Aiming to answer the question, why are we not as impactful as expected? It emphasizes the experiences of researchers from resource-limited situations, with a focus on my personal journey as a Nigerian researcher.</p><p>&nbsp;</p><p><strong>Background&nbsp;</strong></p><p>I graduated from Nnamdi Azikiwe University in Awka with a Ph.D. in medical microbiology. Currently, I am a lecturer at Plateau State University, Bokkos, Nigeria. In order to finish my Ph.D. study, I spent three months conducting Ph.D. benchwork at Duke University in the United States. I was fortunate to get local funding for my doctorate from the Tertiary Education Trust Fund, which enabled me to travel to Duke for my doctorate. Not everyone is that fortunate. People just very rarely receive sponsorship for post-graduate education in our Nigerian setting. Most people finance their postgraduate education on their own.&nbsp;</p><p>&nbsp;</p><p>You must pay both your school fees and your research expenses as a postgraduate student in Nigeria. I've begun supervising both undergraduate and graduate students. What I saw at Duke and what I have heard from developed universities is that university supervisors provide their students with research labs and consumables through grants They also offer assistance through post-doctoral training programs after receiving a PhD.&nbsp;</p><p>The ordinary postgraduate student here is not like that. What ground-breaking outcomes or depth of study will self-funding enable? On the other hand, few students are fortunate enough to receive collaboration for their projects either directly or through the assistance of their supervisors. In my experience as a supervisor, the university system offers me zero financial assistance for my students' projects.&nbsp;</p><p>&nbsp;</p><p><strong>What is it like to do research here?</strong></p><p>Although my experience might not be representative of all academics in Nigeria. It can be used to illustrate what it's like to conduct research in a Nigerian university and what it's like to hold a professorial position there.&nbsp; Very few scholars focus on a certain topic or element. For example. specialist in the development of bacterial cell walls. or something else. The majority of the time, you will find a lecturer's research articles dispersed throughout many areas. What causes this? As a medical microbiologist, I have published in the fields of mycology, virology, and global warming. At the beginning of my work, while pursuing my Masters degree, I isolated <a href=\"https://pubmed.ncbi.nlm.nih.gov/22380533/\">Candida africana</a> with the intention of concentrating on it and thoroughly examining its biology and pathology.&nbsp;</p><p>&nbsp;</p><p>Because there are no labs in the nation that can do this kind of research, it was not possible. I so gave up on the research part. I then proceeded on to my doctoral work, where I examined the molecular characterization of the environmental <a href=\"https://pubmed.ncbi.nlm.nih.gov/27184613/\">Cryptococcus neoformans</a>. This research was at Duke University. When I got home, I had a ton of questions, such as: What causes the genetic diversity of C. neoformans in Nigeria? What impact has rising temperatures had on this species' evolution of virulence? . Rarely will you find a lab where \"ordinary\" PCR can be performed. How are consumables purchased? Electricity is a problem. I came to the realization that I could neither pursue these inquiries in Nigeria nor the biology of this fungus. If I must continue studying this pathogen, I will be limited to studying its genotype and epidemiology (which requires cooperation with international labs).</p><p>&nbsp;</p><p>I started looking at how environmental infections are responding to <a href=\"https://journals.plos.org/plospathogens/article?id=10.1371/journal.ppat.1009503\">climate change while seeking for relevant research.</a> My hypothesis is that as a result of climate change, environmental diseases will grow more aggressive and target humans (u<a href=\"https://journals.asm.org/doi/10.1128/mbio.01397-19\">sing Candida auris as an illustration; it was the first successful pathogen to emerge from the environment as a result of climate change</a>). Again, this hasn't been simple. My lab hardly ever has continuous power for more than six hours every day. How do I &nbsp;incubate a pathogen for &nbsp;48 hours at a high temperature? This field of study is still active for me though as i keep looking for funds.&nbsp;</p><p>&nbsp;</p><p>My lab is now investigating <a href=\"https://journals.asm.org/doi/10.1128/MRA.00489-21\">phage therapy </a>(Phages are viruses that feed on bacteria). Infections caused by bacteria can be treated with it. It significantly contributes to addressing the issue of medication resistance in both people and animals. It will be quite beneficial, in my opinion. Once more, power and the capacity to sequence these phages present difficulties.&nbsp;</p><p>&nbsp;</p><p>Funding is a major issue. I attempt to submit applications to at least 4-5 different funding sources each year. Including EA funds. I have had a nearly 0% of success rate. I utilize my salary's own funds as a result. I currently make around 270 USD per month. Without receiving any compensation, I am normally expected to use this money to fund my research, publish in journals with high-impact factors (some of which can cost up to $2,000 USD), and look after my family. How much of an impact can I possibly make trouh research?&nbsp; The fact that we haven't received our salary since January 2023 only makes things worse.</p><p>&nbsp;</p><p><strong>So few positives</strong></p><p>Emergent Ventures just helped me build up a phage lab. It was similar to beginning a brand-new lab. Every piece of equipment required for the lab has to be purchased by me. Although there is not enough money, we have made some headway in buying certain equipment. We hope to get funding for solar energy to find a solution to the electrical issue, after which we can purchase supplies for post-graduate students' studies.&nbsp;</p><p>The lab's objective is to provide postgraduate students from Universities across Nigeria with a workspace they can do as much of their research as they can. in an effort to have an impact. My doctoral student is currently researching phages to combat salmonella typhi. Two phages against S. typhi have been identified, and we intend to investigate their effectiveness in a mouse model.</p><p>&nbsp;</p><p>So far, supervising undergraduate students have been quite exciting. My student reported the first genotype of <a href=\"https://bsppjournals.onlinelibrary.wiley.com/doi/full/10.5197/j.2044-0588.2019.040.020\">Phytophthora infestan</a>s from Nigeria.&nbsp;</p><p>&nbsp;</p><p><strong>Conclusion</strong></p><p>&nbsp;</p><p>Researchers from Nigeria and most LMICs are not lazy but handicapped. The frustration needed to make an impact through research is limited. I will be glad to hear about how you think this situation can be improved or if you have any word of advice that will be welcomed also. How can EA help in improving this situation? If however you are considering areas to help boost research in Nigeria and some LMICs. You can support our laboratories. I am available for discussion.&nbsp;</p><p>In my mind how can I make the most good with the prevailing situation like this that seem not to be improving? I will like to hear from experienced people and any other person that has a solution.&nbsp;</p><p>&nbsp;</p><p>Also, if you know how I can get funding, or if you will like to collaborate with me in applying for funding? Please feel free to reach out</p><p>&nbsp;</p><p>Let us engage in discussions, seek innovative solutions, and collaborate across borders to create an environment where researchers in resource-limited settings can thrive and make significant contributions to scientific knowledge and societal development. Together, we can transform the research landscape and unlock the untapped potential of researchers from Nigeria and other LMICs.</p>", "user": {"username": "emmannaemeka"}}, {"_id": "hDyqm5tK8KCoRPvQ8", "title": "[job ad] AAC is looking for a Learning and Digital Manager", "postedAt": "2023-06-07T08:31:00.411Z", "htmlBody": "<p>Super excited to let you know that we at Animal Advocacy Careers are looking for a new team member: <a href=\"https://www.animaladvocacycareers.org/learning-and-digital-manager\">Learning and Digital Manager</a>!<br><br>You will focus on helping our audiences to learn about helping animals with their careers and guiding them through available choices, some of them will hopefully result in impactful role placements for the animals!<br><br>This is a part-time, fully remote, autonomous new role, so you can shape it according to your skills and preferences.<br><br>If interested, please <strong>apply before 2nd July</strong> and/or forward to your friends/contacts if it sounds like it\u2019s up their street.</p><p>We will be hosting an optional Q&amp;A session about this role on 22nd June, Thursday, 6pm GMT. To join the Q&amp;A, <a href=\"https://us06web.zoom.us/j/89542796089?pwd=czZCN0dXWWU3b09JT2RMZ1ZCNnk1QT09\"><u>click this link</u></a> on the 22nd June 2022. (Meeting ID: 895 4279 6089 Passcode: 063444). <a href=\"https://calendar.google.com/calendar/u/0/r/eventedit/copy/bDhrcGk3OXJndTd2dDg0YTZwdHZsbDU3NTQgaW5mb0BhbmltYWxhZHZvY2FjeWNhcmVlcnMub3Jn/c29maS5iYWxkZXJzb25AZ21haWwuY29t\">Click here</a> to add the webinar to your calendar.</p><p><strong>About AAC:</strong></p><p>AAC's mission is to speed up the end of unnecessary animal suffering by placing people into roles that help animals the most. We were incubated by Charity Entrepreneurship in late 2019 and now have 5 team members. You can read more <a href=\"https://www.animaladvocacycareers.org/about\">about AAC in general here</a>, <a href=\"https://www.animaladvocacycareers.org/impact\">read more about our impact to date here </a>and <a href=\"https://www.animaladvocacycareers.org/programs\">our current programmes here</a>.</p>", "user": {"username": "sofiabalderson"}}, {"_id": "JuzgngibdtcEvQRFy", "title": "Functions of a community standard in the 2%/8% fuzzies/utilons debate", "postedAt": "2023-06-07T05:23:45.451Z", "htmlBody": "<p>This is post four in a series arguing for adjusting a <a href=\"https://forum.effectivealtruism.org/posts/kg2SafWQ5DK4eFeWn/is-the-10-giving-what-we-can-pledge-core-to-ea-s-reputation\">core idea of EA</a>: the pledge to donate 10% of one's annual income to an effective charity. This is most well-known in the form of the Giving What We Can pledge.</p><p>&nbsp;In conversation with commenters, I found that I had not sufficiently defined my proposal. Here, I will define my proposed modification unambiguously, and then discuss it in relation to the idea of <a href=\"https://en.wikipedia.org/wiki/Focal_point_(game_theory)\">Schelling points</a>, a common way of explaining why EA advocates for a 10%-income annual donation to effective charities.</p><p><strong>Defining the 2%/8% fuzzies-utilons pledge</strong></p><ul><li>The donor pledges to donate 2% of annual income to a charity of one's choice. This could be an EA charity, the opera, or any other cause the donor is passionate about.</li><li>On top of this, the donor pledges an additional 8% or 10% of annual income specifically to charities deemed to be effective. Here, one definition of \"effective\" is that the charity in question has been found via an unmotivated, in-depth and professional cost-benefit analysis to provide extremely high expected value to the beneficiaries. A variety of definitions of \"effective\" could suffice. I primarily mean \"effective\" in any of the ways that EAs mean when they refer to \"effective charities\" or \"effective giving.\"</li></ul><p>The key misunderstanding that has arisen in the past is that this calls for a <i>mandatory</i> 2% donation to <i>\"ineffective\" </i>charities, such as one's alma mater or the opera. This is <u>not</u> what I am calling for. In fact, I personally would <i>prefer</i> if EAs who already give 10% or more to EA charities continued to do so.</p><p>Instead, I am calling for EA to center conversations with non-EAs around the idea of a this 2%/8% or 2%/10% variant that I have defined here.</p><p><strong>How I think about Schelling points</strong></p><p>EAs often use the idea of Schelling points to explain why we have instituted a 10% donation to an effective charity, rather than some other amount, such as 2%, 9%, 11%, 20%, 50%, or \"until your material quality of life has degraded to the point that the dysfunction this causes you decreases your ability to donate, perhaps by threatening your performance at work.\"</p><p>For example, in my last post, mhendric explained the specific choice of a 10% donation thus:</p><blockquote><p>EA argues for a duty of beneficence and asks members to donate 10%. <strong>10% is an arbitrary [Sch]elling point</strong>. Why not 11%? Why not 12% (you are here)? But consider: why not 13%? (...) Why not 99%? These worries are a classic critique of duties of beneficence, at least since Singer released Famine, Affluence, and Morality.</p></blockquote><p>In a previous post, mhendric expanded on the rationale for a 10% donation:</p><blockquote><p>10% effective donations has brand recognition and is a nice round number, as you point out. It is used by other groups, such as religious groups, making it easy to re-funnel donations to e.g. religious communities to effective charities. This leaves 90% of your income at your disposal, part of which you may spend on fuzzy causes. It does not seem required to me to change the 10% to allow for fuzzy donations, nor do I think there's a motivation to make donations to fuzzy causes morally required.&nbsp;</p></blockquote><p>First, we are going to start with a note about jargon. In game theory, the notion of a Schelling point is used specifically to refer to a way that players coordinate in the absence of direct communication. A classic example is that two friends separated in New York City without a way to contact each other or a predetermined meeting spot would each go to Grand Central Station (a famous landmark in the city), assuming that the other would also assume that's where to look.</p><p>Here, we are using Schelling point in a broader sense that ignores the need to converge on the same solution. Of course, EAs could select their own donation levels, from 0-100% of annual income, and those choices would not necessarily impact the choices of other EAs. There is no actual need for EAs to all donate the same amount, and indeed we don't. Will MacAskill donates 50% of his income to charity, for example, and advocates that the super-rich donate 99% or more of their wealth. Undoubtedly, there are EAs who earn incomes or have obligations such that they understandably feel unable to commit to a 10% annual donation.</p><p>This means that a 10% donation standard is not a Schelling point in a game theoretic sense. It is just an arbitary standard, which is a less specific thing.</p><p>A more conventional way to refer to an arbitrary practical standard is as a \"line in the sand.\" <a href=\"https://www.nytimes.com/2015/04/05/opinion/sunday/nicholas-kristof-the-trader-who-donates-half-his-pay.html\">One sympathetic journalist asks:</a></p><blockquote><p>... where do we draw the line? If we\u2019re prepared to donate one-third of our incomes to maximize happiness, then why not two-thirds? Why not live in a tent in a park so as to be able to donate 99 percent and prevent even more cases of blindness?&nbsp;</p></blockquote><p>This journalist, Nicholas Kristof of the New York Times, is not describing Giving What We Can, but the 50% donation practice of the trader Matt Wage. Giving What We Can draws the line at 10% to answer objections such as Kristof's.</p><p>As another journalist, <a href=\"https://www.newyorker.com/news/annals-of-inquiry/sam-bankman-fried-effective-altruism-and-the-question-of-complicity\">Gideon Lewis-Kraus writing for the New Yorker</a> puts it:</p><blockquote><p>On the one hand, what makes the movement distinct is its demand for absolute moral rigor, a willingness, as they like to put it, to \u201cbite the philosophical bullet\u201d and accept that their logic might precipitate extremes of thought and even behavior\u2014to the idea, to take one example, that any dollar one spends on oneself beyond basic survival is a dollar taken away from a child who does not have enough to eat. On the other hand, effective altruists, or E.A.s, have recognized from the beginning that there are often both pragmatic and ethical reasons to defer to moral common sense. This enduring conflict\u2014between trying to be the best possible person and trying to act like a normal good person\u2014has put them in a strange position. If they lean too hard in the direction of doing the optimal good, their movement would be excessively demanding, and thus not only very small but potentially ruthless; if they lean too hard in the direction of just trying to be good people, their movement would not be anything special...</p></blockquote><p>Giving What We Can's 10% donation standard can be seen as an arbitary line in the sand, which we draw in order to create a movement that is neither too lax nor too demanding of its members.</p><p>I would argue that it's more subtle. In fact, mhendric (who I greatly respect and appreciate for their contributions to the comments on my previous posts) across multiple comments serves as an illustration of the inconsistencies in viewing the 10% standard as 'arbitrary'.</p><p>In my last post, they commented:</p><blockquote><p>10% is an arbitrary [Sch]elling point.</p></blockquote><p>Yet in an <a href=\"https://forum.effectivealtruism.org/posts/r4dFEckvT7b9w4wk9/further-defense-of-the-2-fuzzies-8-ea-causes-pledge-proposal\">earlier post</a>, they argued:</p><blockquote><p>10% effective donations has brand recognition and is a nice round number, as you point out. It is used by other groups, such as religious groups, making it easy to re-funnel donations to e.g. religious communities to effective charities.</p></blockquote><p>So the 10% figure is not exactly <i>arbitrary</i>. It is chosen for a specific set of practical reasons - having brand recognition, being a nice round number, and being used by religious groups.</p><p>Let's briefly integrate the argument so far as a set of premises. These premises will serve as the starting point for the remainder of the discussion.</p><p><strong>Premise 1: The Giving What We Can 10% pledge is best understood not a Schelling point, but as a line in the sand or conventional community standard.</strong></p><p><strong>Premise 2: The Giving What We Can 10% pledge&nbsp;is not arbitary, but selected for specific practical reasons.</strong></p><p><strong>Premise 3: It is the specific practical problem we are trying to solve with a community standard that determines its optimal structure.</strong></p><p><i>Note: if you do not agree with one or more of these premises, please let me know in the comments! I am genuinely unsure of whether this will be an important objection or not. If this is your belief, you are unlikely to find the remainder of this discussion valid.</i></p><p>Since the Giving What We Can pledge is <i>not</i> functioning as a way for EAs to coordinate in the absence of the ability to communicate, but as a practical community standard, we need to understand what problems we might be trying to solve with such a standard.</p><p>mhendric and the aforementioned journalists have already articulated some of them:</p><ul><li>Encouraging a level of giving that is effective, sustainable, and motivating.</li><li>Choosing a number that orients discussion in a productive way.</li><li>Aligning our standards with those of major existing altruistic traditions.</li><li>Maintaining consistency in the EA movement.</li></ul><p>I agree with mhendric that these are the three main problems we are trying to solve with our community pledge standard.</p><p>With both a 2%/8% fuzzies/utilons pledge and our current Giving What We Can 10% pledge, we achieve an equally <strong>sustainable</strong> level of giving, while the 2%/10% fuzzies/utilons pledge is equally <strong>effective</strong>.</p><p>So our disagreement is over the other problems.</p><p>The 2%/10% choice is slightly less <strong>sustainable</strong>, since the total donation level is higher, while the 2%/8% choice is slightly less <strong>effective</strong>, since the effective donation component is lower. These differences are real. But a simple comparison assumes that the choice of pledge does not have an effect on the total amount of donations received. In other words, two people donating $2,000 to the opera and $8,000 to GiveWell contribute more to fighting malaria than one person donating $10,000 to GiveWell. So our more important question is how the choice of standard would motivate donations.</p><p>Regarding motivation of donations, let's note that \"orienting discussion\" and \"aligning standards\" are things we care about primarily for their instrumental effects in <strong>motivating</strong> donations. Maintaining consistency is something we care about to both <strong>motivate </strong>and <strong>sustain </strong>donations. How our choice of community standard affects the inflow and outflow of committed donors is what's truly at issue.</p><p>My arguments for a 2%/8% fuzzies/utilons standard have made similar appeals to a practical benefit for sustainability and motivation.</p><p>First, my perception is that the \"10% to effective causes\" standard orients the discussion in a way that encourages many to interpret us as follows:</p><ul><li>EA wants me to donate 10% of my income</li><li>EA wants me to give 100% of my donations to charities they deem effective</li></ul><p>Nicholas Kristof's article above doesn't engage with a 10% standard - more like a 30-50% standard. But Kristof explicitly articulates the second idea - that EA wants him to make 100% of his donations to our preferred charities.</p><p>In fact, a sophisticated EA might point out that he could donate 10% to EA charities, and then extra on top of that to whatever causes he prefers (in line with my 2%/10% pledge). <i>But that is not what Kristof took away from his discussion with Matt Wage.</i></p><p>I believe it's well known that EA has a reputation among critics for being absolutist. I covered some of those criticisms in <a href=\"https://forum.effectivealtruism.org/posts/kg2SafWQ5DK4eFeWn/is-the-10-giving-what-we-can-pledge-core-to-ea-s-reputation\">my last post</a>. I believe that an \"X%-to-effective charities\" standard reliably contributes to this perception, not because it is an accurate takeaway, but because it is what outsiders understand when they hear this in the context of our discussions with them. If we had the chance to explain in more detail, we could enlighten them. <i>But we won't get that chance. </i>For example, Nicholas Kristof is unlikely to read an email from me explaining this point and then publish an updated version of his original column.</p><p>By opening the explanation with a 2%/8% fuzzies/utilons pledge, it gives the person we are talking to a chance to see that we approve of their multiple loyalties and have an explicit community standard that lets them see how they can satisfy the competing demands of their <a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/2021/06/Parliamentary-Approach-to-Moral-Uncertainty.pdf\">moral parliament.</a></p><p>In turn, this aligns with what Lewis-Kraus calls \"traditional liberal concerns.\" Of that portion of society that believes in charitable giving at all, the vast majority of it believes that there is hidden virtue in giving to the charities they currently support. Ideas of obligation, loyalty, insider knowledge, cosmopolitanism, liberalism, risk-taking, democracy, systemic change, beauty, religion, and many other values guide people's choices about charitable giving. If a 10% standard aligns with religious tithing, a 2%/8% split aligns better with traditional notions of liberalism and moderation.</p><p>I argue that EA is primarily trying to appeal not to religious donors, but to secular donors. <a href=\"https://rethinkpriorities.org/publications/eas2019-community-demographics-characteristics#:~:text=Effective%20altruists%20report%20being%20highly,then%20doctoral%20degrees%20(16%25)%20.\">As of 2019, EA was 86% agnostic/atheist/non-religious</a>. Furthermore, a 10% religious tithe is typically supposed to be made <i>to one's church</i>. EA is not a church, and aligning with a 10% tithing obligation seems unlikely to be useful, because most of those who actually consider themselves under obligation to tithe will not consider donating to EA in the first place.</p><p>Thus, I believe that appealing to the cultural norms of the communities EA is trying to reach as donors is most important. And our critics most often criticize our donation standards as preventing them from donating to secular causes, such as alma maters, political movements, the arts, and so on. Showing them that there is a way to include these interests in their giving, while still saving lives in a way that can be demonstrated with cost-benefit analysis in the manner of Effective Altruism seems to be a promising strategy to me.</p><p>This leaves the issue of maintaining consistency. Clearly, a 2%/8% fuzzies/utilons pledge maintains some of the consistency. In fact, since the 2% portion is <i>optional</i>, all those currently donating 10% to EA charities could continue doing so with no disruption at all. If the 10% community standard is all that's preventing a subset of current pledgers from redirecting 20% of their annual giving to the opera instead of GiveWell, are we really pleased that the 10% community standard is having that effect?</p><p>I think the main concern is that shifting to a 2%/8% pledge might alienate more potential donors than it attracts - that there is some motivating and sustaining power in the way we currently structure discourse primarily around the idea of a 10% pledge to effective charities. As mhendric put it:</p><blockquote><p>When I encountered EA, a pitch of \"Donate X% to the most effective ways of improving lives, then spend an additional 2% on whatever you feel like\" would have created more rather than less confusion in me.</p></blockquote><p>I have no reason to doubt mhendric is describing their experience accurately. However, I think this then becomes a messaging problem to solve in cases like theirs. And for others, the 2%/8% pledge <i>would be solving a messaging problem</i>. Figuring out the right language and context to bring it up would be key, and one benefit of the 10% standard is that we have worked some of that out already. <strong>There would be switching costs to a new standard, perhaps substantial.</strong> This is one of the true downsides to changing our messaging and community standards. These costs include changing descriptions on our various websites, figuring out new ways to describe the 2%/8% pledge, and evaluating whether it is in fact as useful as I believe it would be in driving and sustaining donations.</p><p>The question is one of costs and benefits, as always. I cannot complete the cost-benefit analysis here. We need empirical data on how a broad segment of EAs and non-EAs would react to the idea of making such a change, when presented well. My goal for now is to continue addressing the conceptual questions and critiques raised by commenters here. At some point, I hope either to have them change my mind, or to change theirs and move from the realm of discussion and into the realm of a more substantial empirical investigation.</p><p>I thank all readers for their continued engagement and thoughts.</p>", "user": {"username": "AllAmericanBreakfast"}}, {"_id": "F2xsFvDFxRiDeFyhL", "title": "Launching Lightspeed Grants (Apply by July 6th)", "postedAt": "2023-06-07T02:53:29.250Z", "htmlBody": "", "user": {"username": "Habryka"}}, {"_id": "nTrqLCrRmKpEWx96m", "title": "Cultivate an obsession with the object level", "postedAt": "2023-06-07T01:39:55.030Z", "htmlBody": "", "user": {"username": "richard_ngo"}}, {"_id": "xaeLYGQNXHA9x3rWc", "title": "Is there a case for Pronatalism as an effective cause area?", "postedAt": "2023-06-07T00:42:45.533Z", "htmlBody": "<p>I recently heard <a href=\"https://podcast.clearerthinking.org/episode/158/malcolm-simone-collins-why-are-birth-rates-plummeting-and-how-much-does-it-matter\">Simone and Malcolm Collins talk about pronatalism on Spencer Greenberg's podcast</a>. They make a pretty clear case that the coming demographic collapse is inevitable and, and further claim this will be catastrophic for humanity and for the economy. If so, you might think that pronatalism was a key pressing problem worthy of a considerable investment in resources. Do the claims hold up? And does pronatalism--increasing birthrates across the world--rank anywhere amongst other cause areas EAs deem important, and if so, where in the priority ranking could it lie?</p><p>I looked at some of the data points they've draw attention to and demographic collapse at least throughout Western Europe, the Americas, and East Asia--just about everywhere except Africa, the Middle East, and South Asia--is a pretty undeniable fact. The picture for global fertility rates <a href=\"https://www.metaculus.com/questions/12866/world-population/\">seems less clear</a>, and I wonder whether Metaculus and professional estimates of future populations have properly taken into account recent declines of fertility rates as well as the <i>continuing declining trajectory</i> of fertility rates in all parts of the world (including throughout sub-Saharan Africa).&nbsp;</p><p>In the graphs, ~2.1 is the \"replacement rate\" of fertility, and numbers below that indicate shrinking populations. The Americas, Europe, and East Asia are already below replacement, and Middle East and North Africa and South Asia may get there within 20 years.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/mytpipamlv3pgnjpquam\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/i3mu9zlqa91aqainn64o 155w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/fya2rpmdc1k6xjzy13eh 235w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/rbdtsyzr8x5gr9rsx4ya 315w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/x2urjlzqlugjztwqjib4 395w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/cgquz1yin6cecvv48wca 475w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/q2ccivs8dwgoftmx1x0t 555w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/wdmrsdiidv2iwlgoc7yi 635w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/qryd3mcucepcrge7wqzs 715w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/rzzfirdgemt23libmvpg 795w\"></p><p>The trend in Sub-Saharan Africa is in the same direction, although at present, fertility in that region still indicates an expected doubling in population from the current generation of child-bearing-aged women to the next one.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/plchzqlphpblpid9ywhi\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/rkzmvflxhyv77diyl0yr 155w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/jktf02gst2flqsdi0qaz 235w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/wwvygzgjbbngvqjysnvl 315w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/txwoillrj5mcmpsri0ex 395w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/m9niemuzmktmvdlseba3 475w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/ry4cg9we2ueqtkio3zev 555w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/ixbo9j2azaa1htmyrggv 635w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/wi4jea1yinukph33en6s 715w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xaeLYGQNXHA9x3rWc/k1uax4krmai1dt7wnhfb 795w\"></figure><p>Will it really be so catastrophic? AI is expected to automate a large number of roles. If it does, then the decline in the numbers of people available to work will not threaten economic decline as much as it would as if the economy continued to be as labor-intensive as it is today. Metaculus predicts the labour force participation in the US amongst working-age people will decline from 63% now to <a href=\"https://www.metaculus.com/questions/13979/us-labor-force-participation-rate-2035/\">58% in 2035</a> and <a href=\"https://www.metaculus.com/questions/8163/labour-force-participation-rate-in-the-2040s/\">46% for the 2040s</a>.&nbsp;</p><p>If AI is likely to decrease the demand for work--seems plausible or even likely--why exactly is demographic collapse bad or dangerous or harmful? In that case it doesn't seem like it will cause economic collapse, because we'll no longer need the labor from younger workers to sustain the economy.</p><p>One reason: on total utilitarian grounds. For contemporary humans, life is worth living, and Malthusian ideas that more humans will mean a worse planet generally seem unconvincing (though reasonable people may disagree!). So, it seems that in general, every extra birth is a net positive for the world from a total utility standpoint, and that's sufficient enough to be a pronatalist.</p><p>But perhaps it not sufficient enough to prioritize pronatalism. In an era where we are facing down truly existential risks from AI and other human technologies, it seems like those issues are much more relevant for maximizing total utility in the very long term compared to whether humanity enters 2100 with a population of 9, 10, or 11 billion seems less important for maximizing total utility over the very long term. In fact, though I am mostly unconvinced by e.g., climate-change-based Malthusian stories, considering the pace of decarbonization, it seems <i>possible</i> that other forms of overcrowding, given the social and political structures we have, <i>might</i> precipitate military conflict of the sort that raises the risk of catastrophic warfare.&nbsp;</p><p>I'm curious what you think about the idea</p><ul><li>that AI is likely to mostly alleviate the risk of economic&nbsp;collapse derived from the coming demographic collapse, or whether you think there is still a risk of economic&nbsp;disruption.</li><li>the total-utilitarian long-termist&nbsp;critique that, although raising birthrates in the short-term is net positive, it seems relatively less important for maximizing total utility over the long term, because reducing catastrophic and existential risk form most of our leverage in the 21st century over total utility in the long-term future.</li></ul><p>I can sort of anticipate a pushback that perhaps more births now will result in a larger human population right out into the long-term future; I suppose how plausible that idea is depends on whether we think there are likely to be future periods of population constraints, say, in the 22nd or 23rd century, after we have hopefully dealt with pressing AI risk but before humanity goes multi-planetary and multi-stellar.</p><hr><p>So considering all the above, how important is pronatalism? Catastrophic risk arising from demographic collapse seems unlikely, so the case has to rest on the intrinsic value of additional lives, relative to other means of bringing about additional full human lives (i.e., avoiding deaths of infants and children). We can try about a quick BOTEC.</p><p>The average person worldwide can expect to live to the age of 73. For a committed total utilitarian who doesn't accept the <a href=\"https://en.wikipedia.org/wiki/Person-affecting_view\">person-affecting view</a>, to a first approximation, bringing a new person into the world is a gain of 73 life-years, worth the same as preventing the death of one infant who would have lived the same number of years. Of course this is not quite right as the death of a person almost always has negative effects on those around them who will mourn their loss, although these are likely to be small relative to the value of the person's life itself. And most of us are probably willing to give at least a little bit of weight to the person-affecting view, where we care more about improving welfare of those who already exist than people we might choose to cause to exist. But with those caveats, a total utilitarian might value bringing a person into the world at roughly the same order of magnitude as they would preventing one death.</p><p>Famously, Givewell estimates they can save one life in Nigeria through donations to AMF for the value of around <a href=\"https://docs.google.com/spreadsheets/d/10JFJaWnFAEKmsv5XjXqGqEoMUx0eM7x3WYwu_vC7FRw/edit#gid=1364064522\">$3100</a>. For the sake of simplicity, let's assume the value of one life anywhere is roughly equal. Then, pronatalism might be competitive as a cause area if we can cause on extra birth for the value of $3,100.</p><p>The cost of raising a child in the United States is estimated to be around <a href=\"https://www.investopedia.com/articles/personal-finance/090415/cost-raising-child-america.asp\">$300,000</a>. Coincidentally, <a href=\"https://thezvi.substack.com/p/on-car-seats-as-contraception\">Zvi Mowshowitz</a> estimates around the same figure to generate one extra birth when used as a direct childraising subsidy. So, the actual raising of a child doesn't seem particularly competitive, as a cause area. However, there may be a number of interesting public policy interventions in which one extra birth could be gained for much lower than the cost of raising a child. For instance, it's been estimated that <a href=\"https://thezvi.substack.com/p/on-car-seats-as-contraception\">car seat regulations</a> that tell parents what sort of car seats they must use for their children prevent around 8,000 births a year in the United States. Changing such a policy, if we counterfactually value one extra life at $3,100, might return $25m for every year the carseat policy remains in place.</p><p>That seems competitive with global poverty interventions <i>if</i> we accept a thoroughgoing total utilitarian, non-person-affecting view of life.</p><hr><p>Does that make a great cause area? I think most of us don't really intuitively fully accept a non-person-affecting total utilitarian view. In other words, even if you think that all else being equal, a world with 100 people enjoying life is better than a world with only 99 enjoying lives equally as good, you still think it is more valuable to save the life of an existing person than to counterfactually bring one extra person into the world.</p><p>It seems to me the most interesting discussion to have on pro-natalism stands apart from the pure direct effect of bringing people into the world. Primarily, there is the impact on possible catastrophic risk (and thereby, possible knock-on effects for existential risk). But having ruled out economic collapse as a possible effect of collapsing birthrates, do we have anything else to worry about?</p>", "user": {"username": "ben.smith"}}, {"_id": "gZhrqihqSEvbtTBpi", "title": "Tim Cook was asked about extinction risks from AI ", "postedAt": "2023-06-06T18:46:01.950Z", "htmlBody": "<p>Tim Cook (CEO of Apple) was on Good Morning America. The reporter asked about his concerns on AI, including extinction.</p><hr><p>Q: You said, you want to remain deliberate and thoughtful in your approach. So what are your concerns?</p><blockquote><p>...What people are now talking about are these large language models. And, I think they have great promise. I do think that it's so important to be very deliberate and very thoughtful in the development and the deployment of these. Because they can be so powerful that you worry about things like bias, things like misinformation, <strong>maybe worse in some cases.</strong></p></blockquote><p>Q: You saw a lot of leaders in this area coming together, and they were really sounding the alarm. They went so far as to use the word <strong>extinction</strong> ... of the human race.</p><blockquote><p>Regulation is something that's needed in this space, I think guardrails are needed. And if you look down the road, that it's so powerful that companies have to employ their own ethical decisions. <strong>Regulation will have a difficult time staying even with the progress on this because its moving so quickly. And so I think it's incumbent on companies, as well, to regulate themselves.</strong></p></blockquote><p>On ChatGPT:</p><blockquote><p>Oh, of course I use it! Yeah, I'm excited about it. I think there's some unique applications for it, and <strong>you can bet that it's something we're looking at closely</strong>.</p></blockquote><p>More in the <a href=\"https://www.youtube.com/watch?v=YmOC7dK3rc0\">full video</a>, but it gets pretty off-topic after ~2:10.</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=YmOC7dK3rc0\"><div><iframe src=\"https://www.youtube.com/embed/YmOC7dK3rc0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure>", "user": {"username": "Saul"}}, {"_id": "Pfayu5Bf2apKreueD", "title": "A Playbook for AI Risk Reduction (focused on misaligned AI)", "postedAt": "2023-06-06T18:05:55.331Z", "htmlBody": "", "user": {"username": "HoldenKarnofsky"}}, {"_id": "BCwaWkMHTMjFMvedS", "title": "AISN #9: Statement on Extinction Risks, Competitive Pressures, and When Will AI Reach Human-Level?", "postedAt": "2023-06-06T15:56:49.938Z", "htmlBody": "<p>Welcome to the AI Safety Newsletter by the <a href=\"https://www.safe.ai/\">Center for AI Safety</a>. We discuss developments in AI and AI safety. No technical background required.</p><p>Subscribe <a href=\"https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916\">here</a> to receive future versions.</p><hr><h2>Top Scientists Warn of Extinction Risks from AI</h2><p>Last week, hundreds of AI scientists and notable public figures signed a <a href=\"https://www.safe.ai/statement-on-ai-risk\"><u>public statement on AI risks</u></a> written by the Center for AI Safety. The statement reads:</p><blockquote><p>\u201cMitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\u201d</p></blockquote><p><strong>The statement was signed by a broad, diverse coalition. </strong>The statement represents a historic coalition of AI experts \u2014 along with philosophers, ethicists, legal scholars, economists, physicists, political scientists, pandemic scientists, nuclear scientists, and climate scientists \u2014 establishing the risk of extinction from advanced, future AI systems as one of the world\u2019s most important problems.&nbsp;</p><p>The international community is well represented by signatories who hail from more than two dozen countries, including China, Russia, India, Pakistan, Brazil, the Philippines, South Korea, and Japan. The statement also affirms growing public sentiment: a recent poll found that <a href=\"https://www.reuters.com/technology/ai-threatens-humanitys-future-61-americans-say-reutersipsos-2023-05-17/\"><u>61 percent of Americans</u></a> believe AI threatens humanity\u2019s future.</p><p><strong>A broad variety of AI risks must be addressed.</strong> As stated in the first sentence of the signatory page, there are many \u201cimportant and urgent risks from AI.\u201d Extinction is a risk, but there are a variety of others, including misinformation, arbitrary bias, cyberattacks, and weaponization.</p><p>Different AI risks share common solutions. For example, an <a href=\"https://ainowinstitute.org/publication/gpai-is-high-risk-should-not-be-excluded-from-eu-ai-act\"><u>open letter</u></a> from the AI Now Institute argued that AI developers should be held legally liable for harms caused by their systems. This liability would apply across a range of potential harms, from <a href=\"https://githubcopilotlitigation.com/\"><u>copyright infringement</u></a> to <a href=\"https://www.theverge.com/2022/3/17/22983197/ai-new-possible-chemical-weapons-generative-models-vx\"><u>creating chemical weapons</u></a>.&nbsp;</p><p>Similarly, autonomous AI agents that act without human oversight could amplify bias and pursue goals misaligned with human values. The AI Now Institute\u2019s open letter argued that humans should be able to \u201cintercede in order to override a decision or recommendations that may lead to potential harm.\u201d</p><p>Yesterday, the Center for AI Safety wrote about support for several <a href=\"https://www.safe.ai/post/three-policy-proposals-for-ai-safety\"><u>policy proposals from the AI ethics community</u></a>, including the ones described above. Governing AI effectively will require addressing a range of different problems, and we hope to build a broad coalition for that purpose.&nbsp;</p><p><strong>Policymakers are paying attention to AI risks. </strong>Rishi Sunak, Prime Minister of the United Kingdom, retweeted the statement and <a href=\"https://twitter.com/RishiSunak?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor\"><u>responded</u></a>, \u201cThe government is looking very carefully at this.\u201d During a <a href=\"https://www.gov.uk/government/news/pm-meeting-with-leading-ceos-in-ai-24-may-2023\"><u>meeting last week</u></a>, he \u201cstressed to AI companies the importance of putting guardrails in place so development is safe and secure.\u201d</p><p>Only a few months ago, White House press secretary Karine Jean-Pierre laughed at the notion of extinction risks from AI. Now, the Biden administration is striking a different tune. Their new <a href=\"https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf\"><u>2023 National AI R&amp;D Strategic Plan</u></a> explicitly calls for additional research on the \u201cexistential risk associated with the development of artificial general intelligence.\u201d</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac06482d-b3fc-4501-94c3-b954fc31780f_1760x1570.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac06482d-b3fc-4501-94c3-b954fc31780f_1760x1570.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac06482d-b3fc-4501-94c3-b954fc31780f_1760x1570.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac06482d-b3fc-4501-94c3-b954fc31780f_1760x1570.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac06482d-b3fc-4501-94c3-b954fc31780f_1760x1570.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac06482d-b3fc-4501-94c3-b954fc31780f_1760x1570.png 1456w\"></a></p><p><i>The White House\u2019s attitude towards extinction risks from AI have rapidly changed.</i></p><p>The statement was widely covered by media outlets, including the <a href=\"https://www.nytimes.com/2023/05/30/technology/ai-threat-warning.html\"><u>New York Times</u></a>, <a href=\"https://www.cnn.com/videos/tech/2023/05/31/exp-ai-extinction-risk-dan-hendrycks-053003pseg2-cnn-business.cnn\"><u>CNN</u></a>, <a href=\"https://www.foxnews.com/tech/next-generation-arms-race-could-cause-extinction-event-akin-nuclear-war-pandemic-tech-chief\"><u>Fox News</u></a>, <a href=\"https://www.bloomberg.com/news/articles/2023-05-30/ai-leaders-warn-of-risk-of-extinction-from-the-technology\"><u>Bloomberg News</u></a>, <a href=\"https://www.bbc.com/news/uk-65746524\"><u>BBC</u></a>, and others.&nbsp;</p><h2>Competitive Pressures in AI Development</h2><p>What do climate change, traffic jams, and overfishing have in common? In the parlance of game theory, they\u2019re all collective action problems.</p><p>We\u2019d all benefit by cooperating to reduce carbon emissions, the number of cars on the road, and the depletion of natural resources. But if everyone selfishly chooses what\u2019s best for themselves, nobody cooperates and we all lose out.</p><p>AI development suffers from these problems. Some are imaginary, like the idea that we must race towards building a potentially dangerous technology. Others are unavoidable, like Darwin\u2019s laws of natural selection. Navigating these challenges will require clear-eyed thinking and proactive governance for the benefit of everyone.</p><p><strong>AI should not be an arms race. </strong>Imagine a crowd of people standing on a lake of thin ice. On a distant shore, they see riches of silver and gold. Some might be tempted to sprint for the prize, but they\u2019d risk breaking the ice and plunging themselves into the icy waters below. In this situation, even a selfish individual should tread carefully.</p><p>This situation is analogous to AI development, <a href=\"https://time.com/6283609/artificial-intelligence-race-existential-threat/\"><u>argues</u></a> Katja Grace for a new op-ed in TIME Magazine. Everyone recognizes the potential pitfalls of the technology: misinformation, autonomous weapons, even rogue AGI. The developers of AI models are not immune from these risks; if humanity suffers from recklessly developed AI, the companies who built it will suffer too. Therefore, self-interested actors should prioritize AI safety.</p><p>Sadly, not every company has followed this logic. Microsoft CEO Satya Nadella <a href=\"https://venturebeat.com/ai/the-race-starts-today-in-search-as-microsoft-reveals-new-ai-powered-bing-copilot-for-the-web/\"><u>celebrated the idea of an AI race</u></a>, even as his company allowed their chatbot Bing to <a href=\"https://time.com/6256529/bing-openai-chatgpt-danger-alignment/\"><u>threaten users</u></a> who did not follow its rules. Similarly, only days after OpenAI\u2019s alignment lead Jan Leike <a href=\"https://twitter.com/janleike/status/1636788627735736321?s=20\"><u>cautioned</u></a> about the risks of \u201cscrambling to integrate LLMs everywhere,\u201d OpenAI released <a href=\"https://openai.com/blog/chatgpt-plugins\"><u>plugins</u></a> which allow ChatGPT to send emails, make online purchases, and connect with business software. Even if their only goal is profit, more companies should prioritize improving AI safety.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f096b60-7928-4379-928f-5db2b7713e37_2400x1513.jpeg\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f096b60-7928-4379-928f-5db2b7713e37_2400x1513.jpeg\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f096b60-7928-4379-928f-5db2b7713e37_2400x1513.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f096b60-7928-4379-928f-5db2b7713e37_2400x1513.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f096b60-7928-4379-928f-5db2b7713e37_2400x1513.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f096b60-7928-4379-928f-5db2b7713e37_2400x1513.jpeg 1456w\"></a></p><p><strong>Natural selection favors AIs over humans.</strong> We tend to think of natural selection as a biological phenomenon, but it governs many competitive processes. Evolutionary biologist Richard Lewontin proposed that natural selection will govern any system where three conditions are present: 1) there is a population of individuals with different characteristics, 2) the characteristics of one generation are passed onto the next, and 3) the fittest variants propagate more successfully.&nbsp;</p><p>These three conditions are all present in the development of AI systems. Consider the algorithms that recommend posts on social media. In the early days of social media, posts might\u2019ve been shown chronologically in the order they were posted. But companies soon experimented with new kinds of algorithms that could capture user attention and keep people scrolling. Each new generation improves upon the \u201cfitness\u201d of previous algorithms, causing addictiveness to rise over time.&nbsp;</p><p>Natural selection will tend to promote AIs that disempower human beings. For example, we currently have chatbots that can help us solve problems. But AI developers are working to give these chatbots the ability to <a href=\"https://openai.com/blog/chatgpt-plugins\"><u>access the internet and online banking</u></a>, and even <a href=\"https://say-can.github.io/\"><u>control the actions of physical robots</u></a>. While society would be better off if AIs make human workers more productive, competitive pressure pushes towards AI systems that <a href=\"https://digitaleconomy.stanford.edu/news/the-turing-trap-the-promise-peril-of-human-like-artificial-intelligence/\"><u>automate human labor</u></a>. Self-preservation and power seeking behaviors would also give AIs an evolutionary advantage, even to the <a href=\"https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/\"><u>detriment of humanity</u></a>.</p><p>For more discussion of this topic, check out the <a href=\"https://arxiv.org/abs/2303.16200\"><u>paper</u></a> and <a href=\"https://time.com/6283958/darwinian-argument-for-worrying-about-ai/\"><u>TIME Magazine article</u></a> by CAIS director Dan Hendrycks.</p><h2>When Will AI Reach Human Level?</h2><p>AIs currently outperform humans in complex tasks like playing chess and taking the bar exam. Humans still hold the advantage in many domains, but OpenAI recently <a href=\"https://openai.com/blog/governance-of-superintelligence\"><u>said</u></a> \u201cit\u2019s conceivable that within the next ten years, AI systems will exceed expert skill level in most domains.\u201d&nbsp;</p><p>AI pioneer Geoffrey Hinton made a similar prediction, <a href=\"https://www.cnn.com/2023/05/01/tech/geoffrey-hinton-leaves-google-ai-fears/index.html#:~:text=%E2%80%9CThe%20idea%20that%20this%20stuff,years%20or%20even%20longer%20away.\"><u>saying</u></a>, \u201cThe idea that this stuff could actually get smarter than people \u2014 a few people believed that. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\u201d</p><p>What motivates these beliefs? Beyond the impressive performance of recent AI systems, there are a variety of key considerations. A new <a href=\"https://epochai.org/blog/a-compute-based-framework-for-thinking-about-the-future-of-ai\"><u>report</u></a> from Epoch AI digs into the arguments.&nbsp;</p><p><strong>Technological progress has rapidly accelerated before.</strong> In some areas, it\u2019s a safe bet that tomorrow will resemble today. The sun will rise, rain will fall, and many things will stay the same. But technological development is probably not one of them.&nbsp;</p><p>For the first 100,000 years that <i>homo sapiens</i> walked the earth, there was very little technological progress. People fashioned tools from stone and wood, but typically did not live in permanent settlements, and therefore did not accumulate the kinds of wealth and physical capital that we would observe in the archaeological record.&nbsp;</p><p>Agriculture permanently changed the pace of technological progress. Farmers could produce more food than they individually consumed, creating a surplus that could feed people working other occupations. Technologies developed during this time include iron tools, pottery, systems of writing, and elaborate monuments like pyramids and temples.&nbsp;</p><p>These technologies accumulated fairly slowly from about 10,000 B.C. until around two hundred years ago, when the Industrial Revolution launched us into the modern paradigm of economic growth. Electricity, running water, cars, planes, modern medicine, and more were invented in only a few generations, following tens of thousands of years of relative stagnation.&nbsp;</p><p>Is this the end of history? Have we reached a stable pace of technological progress? Economist Robin Hanson <a href=\"https://mason.gmu.edu/~rhanson/longgrow.pdf\"><u>argues against</u></a> this notion. One possible source of accelerating technological change could be intelligent machines, which are deliberately designed rather than evolved by random chance.&nbsp;</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02875a0e-01d5-4310-b62b-ffd1b10591e0_1752x1252.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02875a0e-01d5-4310-b62b-ffd1b10591e0_1752x1252.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02875a0e-01d5-4310-b62b-ffd1b10591e0_1752x1252.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02875a0e-01d5-4310-b62b-ffd1b10591e0_1752x1252.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02875a0e-01d5-4310-b62b-ffd1b10591e0_1752x1252.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02875a0e-01d5-4310-b62b-ffd1b10591e0_1752x1252.png 1456w\"></a></p><p><i>Technological progress can occasionally accelerate more quickly than was previously anticipated.</i></p><p><strong>Building AI models as big as the human brain.</strong> Modern AI systems are built using artificial neural networks, which are explicitly inspired by neural network of the human brain. This allows us to make comparisons between our AI models and the size of the human brain.&nbsp;</p><p>For example, the most advanced AI models have hundreds of millions of parameters, referring to the connections between artificial neurons. The human brain has roughly <a href=\"https://medicine.yale.edu/lab/colon_ramos/overview/#:~:text=The%20human%20brain%20consists%20of,stars%20in%20the%20milky%20way!\"><u>100 trillion synapses</u></a>, or about 1000 times more connections than our largest AI systems.&nbsp;</p><p>Similarly, you can consider the number of computational operations performed by biological and artificial neural networks. The <a href=\"https://www.openphilanthropy.org/research/new-report-on-how-much-computational-power-it-takes-to-match-the-human-brain/\"><u>human brain</u></a> uses roughly 10<sup>15</sup> operations per second, or 10<sup>24</sup> operations over the first 30 years of our lives. This is strikingly close to the amount of computation used to train our largest neural networks today. Google\u2019s largest language model, <a href=\"https://blog.heim.xyz/palm-training-cost/\"><u>PaLM</u></a>, also used 10<sup>24 </sup>operations during training.</p><p>Both of these comparisons show that AI models are quickly approaching the size and computational power of the human brain. Crossing this threshold wouldn\u2019t guarantee any new capabilities, but it is striking that AIs can now perform many of the creative cognitive tasks once exclusively performed by humans, and it raises questions about the capabilities of future AI models.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65891908-7018-4d1a-9a86-ffb7c8e6138e_499x420.jpeg\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65891908-7018-4d1a-9a86-ffb7c8e6138e_499x420.jpeg\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65891908-7018-4d1a-9a86-ffb7c8e6138e_499x420.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65891908-7018-4d1a-9a86-ffb7c8e6138e_499x420.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65891908-7018-4d1a-9a86-ffb7c8e6138e_499x420.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65891908-7018-4d1a-9a86-ffb7c8e6138e_499x420.jpeg 1456w\"></a></p><p><i>Back in 2005, futurist Ray Kurzweil (who also signed the CAIS AI extinction statement) predicted that AI systems would exceed the computational power of a single human brain during the 2020s.</i></p><p><strong>AI accelerating AI progress.</strong> Another reason to suspect that AI progress could soon hit an inflection point is that AI systems are now contributing to the development of more powerful AIs. CAIS has collected <a href=\"https://ai-improving-ai.safe.ai/\"><u>dozens of examples</u></a> of this phenomenon, but here we\u2019ll explain just a few.&nbsp;</p><p>GitHub Copilot is a language model that can write computer code. When a programmer is writing code, GitHub Copilot offers autocomplete suggestions for how to write functions and fix errors. One study found that programmers completed tasks <a href=\"https://arxiv.org/abs/2302.06590\"><u>55% faster</u></a> when using GitHub Copilot. This reduces the amount of time required to run experiments with new and improved AI systems.&nbsp;</p><p>Data is another way that AIs are being used to accelerate AI progress. A Google team trained a language model to answer general questions that required multi-step reasoning. Whenever the model was confident in its answer, they trained the model to mimic that answer in the future. By <a href=\"http://correctly/\"><u>training on its own outputs</u></a>, the model improved its ability to answer questions correctly.&nbsp;</p><p>Philosopher David Chalmers <a href=\"https://consc.net/papers/singularity.pdf\"><u>argues</u></a> that if we build a human-level AI, and that AI can build an even more capable AI, then it could self-improve and rapidly surpass human capabilities.&nbsp;</p><p>Subscribe <a href=\"https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916\">here</a> to receive future versions.</p><h2>Links</h2><ul><li>Hackers broadcast an <a href=\"https://www.politico.eu/article/fake-vladimir-putin-announces-russia-under-attack-ukraine-war/\"><u>AI-generated video of Vladimir Putin declaring full mobilization against Ukraine</u></a> on multiple TV stations in border regions of Ukraine and Russia yesterday.</li><li><a href=\"https://www.theguardian.com/technology/2023/jun/04/ai-poses-national-security-threat-warns-terror-watchdog\"><u>AI could enable terrorism</u></a>, says a British national security expert.&nbsp;</li><li>The White House will attempt to engage China on <a href=\"https://www.nytimes.com/2023/06/02/us/politics/china-arms-control-nuclear-weapons.html\"><u>nuclear arms control and AI regulation</u></a>.&nbsp;</li><li>Claims about <a href=\"https://www.foreignaffairs.com/china/illusion-chinas-ai-prowess-regulation\"><u>China\u2019s AI prowess may be exaggerated</u></a>.&nbsp;</li><li>A Chinese company is selling an AI system that identifies anybody holding a banner and <a href=\"https://ipvm.com/reports/dahua-protestor-alarms?code=lfgsdfasd\"><u>sends a picture of their face to the police</u></a>.&nbsp;</li><li>A professor at the Chinese Academy of Sciences explains <a href=\"https://long-term-ai.center/research/f/avoiding-catastrophic-risks-of-ai-for-healthy-development\"><u>why he signed the statement</u></a> on AI extinction risks.&nbsp;</li><li>AI labs should have an internal team of independent auditors that can report safety violations directly to the board, argues a <a href=\"https://arxiv.org/abs/2305.17038\"><u>new paper</u></a>.&nbsp;</li><li>Copywriters and social media marketers are among the first to <a href=\"https://www.washingtonpost.com/technology/2023/06/02/ai-taking-jobs/\"><u>lose their jobs</u></a> to ChatGPT.&nbsp;</li><li>Sam Altman discusses <a href=\"https://web.archive.org/web/20230531203946/https://humanloop.com/blog/openai-plans\"><u>OpenAI\u2019s plans after GPT-4</u></a>.&nbsp;</li><li>OpenAI is offering grants for <a href=\"https://openai.com/blog/openai-cybersecurity-grant-program\"><u>using AI to improve cyberdefense</u></a> and have provided more <a href=\"https://trust.openai.com/\"><u>public information about their own cybersecurity</u></a>. Meanwhile, hackers <a href=\"https://decrypt.co/143207/openais-cto-hacked-twitter-account-promotes-fraudulent-openai-token\"><u>stole the password</u></a> to the Twitter account of OpenAI\u2019s Chief Technology Officer.</li></ul><p>See also: <a href=\"https://www.safe.ai/\"><u>CAIS website</u></a>, <a href=\"https://twitter.com/ai_risks?lang=en\"><u>CAIS twitter</u></a>, <a href=\"https://newsletter.mlsafety.org/\"><u>A technical safety research newsletter</u></a></p><p>Subscribe <a href=\"https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916\">here</a> to receive future versions.</p>", "user": {"username": "Center for AI Safety"}}, {"_id": "3yAtGF3bCHqkSN52h", "title": "Stampy's AI Safety Info - New Distillations #3 [May 2023]", "postedAt": "2023-06-06T14:27:25.419Z", "htmlBody": "<p>Hey! This is another update from the distillers at the&nbsp;<a href=\"https://aisafety.info/\">AI Safety Info</a>&nbsp;website (and its more playful clone&nbsp;<a href=\"https://stampy.ai/\">Stampy</a>).</p><figure class=\"image image_resized\" style=\"width:61.67%\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EELddDmBknLyjwgbu/pvqp2geuw04y2yhsbwmn\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EELddDmBknLyjwgbu/ndzacadpz6hj9mvjvyrj 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EELddDmBknLyjwgbu/q52mx4jh4qit0s4pzicb 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EELddDmBknLyjwgbu/qehdfcvupomp3olilgj7 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EELddDmBknLyjwgbu/yetsnyjd0xwdwdmxvmvx 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EELddDmBknLyjwgbu/bwpfydkym7oks4qtkant 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EELddDmBknLyjwgbu/n9p4biatnnq4q5pdp9mp 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EELddDmBknLyjwgbu/nnkaixg6oajgut4kqfxp 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EELddDmBknLyjwgbu/k9fax0iojeuxhsimsspu 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EELddDmBknLyjwgbu/y4xwvi0eqpwc3foyizf5 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EELddDmBknLyjwgbu/toubwbyvml7ruduizzx1 1133w\"></figure><p>Here are a couple of the answers that we wrote up over the last month (May 2023). As always let us know in the comments if there are any questions that you guys have that you would like to see answered.&nbsp;</p><p>The list below redirects to individual links, and the collective URL above renders all of the answers in the list on one page at once.</p><p>&nbsp;</p><h2>How can I help tree</h2><p>There are many different types of individuals from all manner of different backgrounds who want to contribute to AI Safety. So we added a FAQ with advice on various different ways to help with AI safety.</p><p><a href=\"https://aisafety.info/?state=8TJV_6474_8U30_8U32_8UMA_8U2S_8U2P_8U2R_8U2Q_8U2Z_8U2Y_8U2X_8U2W_8U2V_7763_8U2O_8U2M_8U2K_8U2J_8U2I_\">Here</a> is the unified link for all the answers in the how can I help tree if you wish to navigate to aisafety.info directly. See <a href=\"https://forum.effectivealtruism.org/posts/Poq2MKtH7nb9gHygT/aisafety-info-how-can-i-help-faq\">Steven's post</a> for the full list of individual questions and details.</p><h2>Other Distillations</h2><ul><li><a href=\"https://aisafety.info?state=95LE_\">Why would a misaligned superintelligence kill everyone in the world?</a></li><li><a href=\"https://aisafety.info?state=967I_\">Aren't AI existential risk concerns just an example of Pascal's mugging?</a></li><li><a href=\"https://aisafety.info?state=85E4_\">What is Redwood Research's strategy?</a></li><li><a href=\"https://aisafety.info?state=8AER_\">What is imitation learning?</a></li><li><a href=\"https://aisafety.info?state=8AEQ_\">What is behavioral cloning?</a></li><li><a href=\"https://aisafety.info?state=6939_\">What is \"coherent extrapolated volition\"?</a></li><li><a href=\"https://aisafety.info?state=9358_\">What is compute?</a></li><li><a href=\"https://aisafety.info?state=94D9_\">What is the \"Bitter Lesson\"?</a></li><li><a href=\"https://aisafety.info?state=935A_\">What is adversarial training?</a></li><li><a href=\"https://aisafety.info?state=8FJZ_\">How is red teaming used in AI alignment?</a></li><li><a href=\"https://aisafety.info?state=6411_\">Isn't the real concern autonomous weapons?</a></li><li><a href=\"https://aisafety.info?state=6222_\">Isn\u2019t it immoral to control and impose our values on AI?</a></li><li><a href=\"https://aisafety.info?state=8NYD_\">Isn't the real concern bias?</a></li><li><a href=\"https://aisafety.info?state=6410_\">Isn't the real concern AI being misused by terrorists or other bad actors?</a></li><li><a href=\"https://aisafety.info?state=8QH5_\">Would a slowdown in AI capabilities development decrease existential risk?</a></li><li><a href=\"https://aisafety.info?state=90PK_\">What is a singleton?</a></li><li><a href=\"https://aisafety.info?state=8VOT_\">What is mindcrime?</a></li><li><a href=\"https://aisafety.info?state=8V5J_\">Are AIs conscious?</a></li><li><a href=\"https://aisafety.info?state=8517_\">What might an international treaty on the development of AGI look like?</a></li><li><a href=\"https://aisafety.info?state=8KGQ_\">What are polysemantic neurons?</a></li></ul><p>&nbsp;</p><h2>Developer Updates</h2><p>There are also a couple of cool new features that the developers of the website worked on. We now have&nbsp;<a href=\"https://aisafety.info/tags/\">a tags page</a>, which allows the reader to navigate the answers based on the tag that they are most interested in. There is also an implementation of a glossary function which auto-creates a short definition popup whenever a jargony term pops up in the answer that the reader might not be familiar with.&nbsp;<a href=\"https://aisafety.info/?state=0_6953r6201r\">Here</a>&nbsp;is a sample answer that shows how this functionality works. Try hovering over the \"agent\", \"s-risk\", \"Goodhart\" etc... links.</p><p>This is all still a work in progress, but it is exciting to share everything that the people who are all part of the Stampy project have been working on.</p><p>&nbsp;</p><p><i>Cross-posted to Lesswrong: </i><a href=\"https://www.lesswrong.com/posts/fFtkQCasMjeEpMdhp/stampy-s-ai-safety-info-new-distillations-3-may-2023\"><i>https://www.lesswrong.com/posts/fFtkQCasMjeEpMdhp/stampy-s-ai-safety-info-new-distillations-3-may-2023</i></a></p>", "user": {"username": "markov"}}, {"_id": "8nNWnDBTh8r59x2L7", "title": "Debates at EAGxNYC", "postedAt": "2023-06-06T14:17:15.260Z", "htmlBody": "<p>I'm thinking about arranging some debates at EAGxNYC in August. What questions should be debated? Who would you like to see debate? Would <em>you</em> like to debate? Is this a good idea?</p>\n", "user": {"username": "Kaleem"}}, {"_id": "qhpmjCwWtETKwo3Fd", "title": "Malaria question from an aspiring novelist", "postedAt": "2023-06-06T16:11:34.180Z", "htmlBody": "<p>Hi!&nbsp;</p><p>I am 60, an editor &amp; translator by trade, and am writing a novel, my first.</p><p>I have one of my characters (a German male born in 1915) picking up malaria while hiking in the Camarague around 1930 (maybe earlier) and then having a relapse in 1943. I need &amp; would like some expert advice, particularly on the severity &amp; duration of the 1943 relapse.&nbsp;</p><p>I thank everyone in advance for their time &amp; consideration.</p>", "user": {"username": "Andylute"}}, {"_id": "Pe69bG6uGxLzThYZp", "title": "Agentic Mess (A Failure Story)", "postedAt": "2023-06-06T13:16:18.392Z", "htmlBody": "", "user": {"username": "Karl von Wendt"}}, {"_id": "Ci2Lh5fuwBqtKSG72", "title": "[Linkpost] Given Extinction Worries, Why Don\u2019t AI Researchers Quit? Well, Several Reasons", "postedAt": "2023-06-06T07:31:53.133Z", "htmlBody": "<p>I've written a blog post for a lay audience, explaining some of the reasons that AI researchers who are concerned about extinction risk have for continuing to work on AI research, despite their worries</p><p>The apparent contradiction is causing a a lot of confusion among people who haven't followed the relevant discourse closely. In many instances, lack of clarity seems to be leading people to resort to borderline conspiratorial thinking (e.g., about the motives of signatories of the recent <a href=\"https://www.safe.ai/statement-on-ai-risk\">statement</a>), or to otherwise dismiss the worries as not totally serious.</p><p>I hope that this piece can help make common knowledge some things that aren\u2019t widely known outside of tech and science circles.</p><p>As an overview, the reasons I focus on are:</p><ol><li>Their specific research isn\u2019t actually risky</li><li>Belief that AGI is inevitable and more likely to go better if you personally are involved</li><li>Thinking AGI is far enough away that it makes sense to keep working on AI for now</li><li>Commitment to science for science sake</li><li>Belief that the benefits of AGI would outweigh even the risk of extinction</li><li>Belief that advancing AI on net reduces global catastrophic risks, via reducing other risks&nbsp;</li><li>Belief that AGI is worth it, even if it causes human extinction</li></ol><p>I'll also note that the piece isn't meant to defend the decision of researchers who continue to work on AI despite thinking it presents extinction risks, nor to criticize them for their decision, but instead to add clarity.</p><p>If you're interested in reading more, you can follow the link <a href=\"https://medium.com/@daniel_eth/given-extinction-worries-why-dont-ai-researchers-quit-well-several-reasons-a0b6027da4e7\">here</a>. And of course feel free to send the link to anyone who's confused by the current situation.</p>", "user": {"username": "Daniel_Eth"}}, {"_id": "kg2SafWQ5DK4eFeWn", "title": "Is the 10% Giving What We Can Pledge Core to EA's Reputation?", "postedAt": "2023-06-06T05:42:27.354Z", "htmlBody": "<h1>Introduction</h1><blockquote><p>I don't think the 10% norm forms a major part of EA's public perception, so I don't believe tweaking it would make any difference. - RobertJones</p><p>10% effective donations has brand recognition and is a nice round number, as you point out. -- mhendric</p></blockquote><p>These two comments both received agreement upvotes on my recent post <a href=\"https://forum.effectivealtruism.org/posts/r4dFEckvT7b9w4wk9/further-defense-of-the-2-fuzzies-8-ea-causes-pledge-proposal\"><strong>Further defense of the 2% fuzzies/8% EA causes pledge proposal</strong></a>, and although they're differently worded and not irreconcileable, they seem basically to stand in contradiction with each other. Is the 10% <a href=\"https://www.givingwhatwecan.org/pledge\">Giving What We Can pledge</a>, in which participants commit to donating 10% of their annual income to an effective charity, part of EA's brand or reputation?</p><p><strong>Tl;dr</strong></p><ul><li>The Giving What We Can 10% pledge is often featured in prominent coverage of EA</li><li>Broader notions of earning to give, giving specifically to effective charities, and donating a 10%+ fraction of one's income are even more common themes</li><li>Giving What We Can is widely mentioned within EA, sees itself as \"one of the shop fronts\" for the movement, the pledge itself gets mentioned fairly often, and it's an important driver of EA participation.</li><li>Giving What We Can is a core part of the EA movement and the pledge is core to GWWC.</li><li>Criticism of EA often focuses on specific aspects of the GWWC pledge:<ul><li>Can we really draw a line while maintaining intellectual rigor?</li><li>Is 10% the right line to draw?</li><li>Should 100% of what we donate be specifically to what EA deems to be effective charities, or is it OK to also donate to pet causes like the arts?</li><li>More broadly, is EA too focused on its own ideas - a failure of moral cosmopolitanism? Does EA need to incorporate or make some concession to the values of other cultural, moral, or political movements?</li></ul></li></ul><p>This does not resolve the question of how core the specific idea of a 10% pledge of one's income to effective charities is to the EA movement or to its reputation. But the pledge is provocative and the fact that so many EAs commit to it seems clearly to play a role in the media's interest in the movement, and both its appreciation for EA and its criticism of it. Based on the couple of hours I put into looking through this material, here is my model of the role of a 10% GWWC pledge in EA and in outward-facing reputation:</p><ul><li>The GWWC pledge is critical in giving concrete form to the ideas of earning to give, and the fact that people do it defines EA in the public eye and gives the movement serious credibility</li><li>Non-EAs who take EA seriously often focus on the specific details of the pledge, without necessarily mentioning it by name. In particular, they question whether 10% is the right amount (for everybody, or for specific income brackets), and they question whether making 100% of one's donations to charities deemed effective is the right call. Criticism of EA for being too absolutist is pretty common.</li><li>Media coverage of EA is typically either critical of the ideas and culture and its relationship with FTX and Sam Bankman Fried, or it is in a positive mode of introducing the movement in a broadly sympathetic light to readers who might not have heard of it before. Sometimes, that criticism seems to fail to understand that EA is presenting itself to them in an oversimplified manner because we know that journalists won't fit every layer of subtlety that we've considered into their limited column space. But this seems to lead some journalists to assume we haven't considered the criticisms that they raise, or that we don't have an answer for them.</li></ul><p>Clearly, a 2%/8% or 2%/10% fuzzies/utilons standard for an earning to give pledge would be a concrete way to show we've taken onboard some of these critiques. It would be an example of making allowance for people's multiple loyalties, showing that we are not absolutist, and taking action in response to valid criticism.</p><p>I think that non-EAs who hear about the 10% pledge may sometimes find its clarity compelling, but others will be alienated by the perceived absolutism. It may not be clear that you can <i>also</i> donate more, or that it's probably fine (and at the very least a huge marginal improvement) if you donate a little less to EA causes and the rest to the opera.</p><h2>Reaffirming My Position</h2><p>Overall, I conclude that <strong>the notion of earning to give 10% to effective charities is a core part of EA's brand</strong>, and that <strong>its simplicity is often a source of criticism.</strong> <strong>These are good reasons to continue arguing for a shift to a 2%/8% or 2%/10% pledge.</strong></p><h1><strong>What is the Giving What We Can pledge and how popular is it within EA?</strong></h1><p>On its <a href=\"https://www.givingwhatwecan.org/about-us\">about page</a>, GWWC says it was founded by Toby Ord and Will MacAskill. Its vision statement says:</p><blockquote><p>Giving What We Can's mission is <strong>to make giving effectively and significantly a cultural norm.</strong> We mean this quite literally: our goal isn't just to marginally increase the amount of money going to effective charities \u2014 we're aiming to make meaningful cultural change.</p></blockquote><p>They define their community primarily in terms of its donating members:</p><blockquote><p>At its heart, Giving What We Can is a community. We are a group of like-minded people who are committed to make a meaningful positive impact on others\u2019 lives <strong>by donating to highly effective charities.</strong>&nbsp;</p></blockquote><p>The wording of the GWWC pledge is:</p><blockquote><p><i>\"I recognise that I can use part of my income to do a significant amount of good. Since I can live well enough on a smaller income, I pledge that from __ until __ I shall give __ to whichever organisations can most effectively use it to improve the lives of others, now and in the years to come. I make this pledge freely, openly, and sincerely.\"</i></p></blockquote><p>GWWC says that 9,217 of its members have taken the pledge, and it's this number that they refer to in defining their size:</p><blockquote><p>We\u2019ve got over 9,217 members across the world</p></blockquote><p><a href=\"https://forum.effectivealtruism.org/posts/YSA3vRNky4oSu8hpJ/giving-what-we-can-pledge-thoughts\">As of 2017, it was experiencing 80% year-over-year growth.</a></p><p>In addition to recruiting and supporting donors in maintaining a substantial level of charitable giving, GWWC also does research to identify charities they consider effective, and run a donation platform.</p><p>Among EA forum posts, a search for \"giving what we can pledge\" returns 417 results, with 230 for \"GWWC pledge,\" 819 for \"giving what we can,\" and 2638 for \"GWWC.\" Note that GWWC puts out a monthly newsletter that has \"GWWC\" in the title, and they have a dedicated EA Forum account; this accounts for a few dozen of these entries. I briefly skimmed a few of the top posts that pop up when searching for \"GWWC,\" and they are, no surprise, typically focused at least in part on motivating charitable giving.</p><p>\"Earn to give\" gets 1515 results.</p><p>For comparison with various cause areas, \"vegetarian\" gets 399 results, while \"vegan\" gets 1338 results. \"Pandemic\" gets 1471 results, \"x-risk\" gets 1016 results, and \"ai safety\" gets 1985 results.</p><p>Among EA-related organizations, \"GiveWell\" gets 1984 results, \"GiveDirectly\" 573 results, \"EA Forum\" gets 4544 results, \"MIRI\" gets 5291 results, and \"Future of Humanity Institute\" gets 495 results. \"OpenPhil,\" \"Open Phil,\" and \"Open Philanthropy\" respectively get 1532, 2661, and 1846 results.</p><p>Giving What We Can's 10% donation pledge was prominently featured in one of Scott Alexander's most popular slatestarcodex posts, <a href=\"https://slatestarcodex.com/2014/12/19/nobody-is-perfect-everything-is-commensurable/\">Nobody is Perfect, Everything is Commensurable.</a> It gets plenty of mentions on <a href=\"https://www.lesswrong.com/search?query=giving%20what%20we%20can\">LessWrong</a>. Overall, I think it is fair to say that GWWC is identified pretty strongly with its 10% pledge, that it's among the most prominent EA organizations, and that it is enacting one of the most prominent, visible EA ideas, earning to give.</p><h1>How does the media think about the idea of earning to give and the GWWC pledge?</h1><p>The most I can offer here is an hour or two of searching the internet to find references to GWWC and see how the media portrays it in relation to Earning to Give.</p><p>The first thing I searched for was <a href=\"https://www.youtube.com/watch?v=QkB3Zq3zoR4&amp;ab_channel=TheDailyShow\">Will MacAskill's 12-minute interview on The Daily Show</a>, which is the biggest pop culture moment for EA that I know of.&nbsp;</p><p>The first time in the interview that Trevor Noah digs into the topic of charitable donations is just after the 3:30 mark, talking about the magnitude of MacAskill's giving, how it ties into issues of privilege. He specifically mentions the idea of giving away 10-20% of one's income, and MacAskill reinforces it (along with the idea of donating to effective nonprofits) at about the 10:55 mark. About 75% of the segment revolves around the idea of earning to give. Giving What We Can and the GWWC pledge are not mentioned in the interview, but Will's book is. Giving What We Can and its pledge are mentioned several times in the book.</p><p>The EA Forum has an <a href=\"https://forum.effectivealtruism.org/topics/effective-altruism-in-the-media\">Effective Altruism in the Media tag</a>. The highest-relevancy post covers <a href=\"https://forum.effectivealtruism.org/posts/hd2LeisjAmzdJdGb3/the-most-successful-ea-podcast-of-all-time-sam-harris-and\">the podcast between Sam Harris and Will MacAskill</a>. This podcast resulted in Sam joining GWWC after two episodes, driving a spike of about 600 GWWC memberships, and according to Aaron Gertler, the post's author:</p><blockquote><p>An extremely engaged community builder told me in February 2021: \"I feel like most new EAs I've met in the last year came in through Sam Harris.\"</p></blockquote><p>So just a couple years ago, an extremely well-received Sam Harris podcast was an extremely important influence in driving EA membership, and it was a big promoter specifically of the GWWC pledge.</p><p><a href=\"https://podscribe.app/feeds/http-joeroganexpjoeroganlibsynprocom-rss/episodes/d67a3d6953ec371c72569bf754b8400c\">The Joe Rogan podcast also featured MacAskill and specifically mentioned the GWWC 10% pledge</a>. You can listen to it <a href=\"https://soundcloud.com/effectivealtruism/will-macaskill-on-the-joe-rogan-experience\">here</a>.&nbsp;</p><p><a href=\"https://www.nytimes.com/2023/02/21/business/bankman-fried-altruism-jane-street.html\">NY Times critical coverage of Sam Bankman Fried and his link to EA </a>says that EA:</p><blockquote><p>holds that taking a high-paying job is worthwhile if the end goal is to give much of the income away.</p></blockquote><p>But it doesn't cover the GWWC pledge or the idea of specifically donating 10% of one's income. It does mention that Jane Street contains a lot of people who practice earning to give and are into effective altruism or similar ideas. The article links to <a href=\"https://www.nytimes.com/2015/04/05/opinion/sunday/nicholas-kristof-the-trader-who-donates-half-his-pay.html\">Nicholas Kristof's glowing coverage of Matt Wage</a>, another practitioner of earning to give and effective altruism who donates half his income, and also mentions that Peter Singer donates a third of his income. Kristof describes EA as:</p><blockquote><p>... a new movement called \u201ceffective altruism,\u201d aimed at taking a rigorous, nonsentimental approach to making the maximum difference in the world.</p></blockquote><p>Despite the very positive coverage of EA, one of Kristof's main concerns is specifically about the level of an earning-to-give commitment (% of income) and the idea that 100% of that donation should be focused specifically on effective charities:</p><blockquote><p>First, where do we draw the line? If we\u2019re prepared to donate one-third of our incomes to maximize happiness, then why not two-thirds? Why not live in a tent in a park so as to be able to donate 99 percent and prevent even more cases of blindness?</p><p>I want to take my wife to dinner without guilt; I want to be able to watch a movie without worrying that I should instead be buying a bed net. There is more to life than self-mortification, and obsessive cost-benefit calculus, it seems to me, subtracts from the zest of life.</p><p>Second, humanitarianism is noble, but so is loyalty. So are the arts, and I\u2019m uncomfortable choosing one cause and abandoning all others completely.</p><p>For my part, I donate mostly to humanitarian causes but also to my universities, in part out of loyalty to institutions that once gave me scholarships.</p></blockquote><p><a href=\"https://www.nytimes.com/2022/11/13/business/ftx-effective-altruism.html\">Another NY Times article on the FTX collapse</a> describes EA as:</p><blockquote><p>...a philosophy that advocates applying data and evidence to doing the most good for the many...</p></blockquote><p>&nbsp;</p><p>In a New Yorker article covering the collapse, the author articulates the tension at the heart of EA:</p><blockquote><p><br>On the one hand, what makes the movement distinct is its demand for absolute moral rigor, a willingness, as they like to put it, to \u201cbite the philosophical bullet\u201d and accept that their logic might precipitate extremes of thought and even behavior\u2014to the idea, to take one example, that any dollar one spends on oneself beyond basic survival is a dollar taken away from a child who does not have enough to eat. On the other hand, effective altruists, or E.A.s, have recognized from the beginning that there are often both pragmatic and ethical reasons to defer to moral common sense. This enduring conflict\u2014between trying to be the best possible person and trying to act like a normal good person\u2014has put them in a strange position. If they lean too hard in the direction of doing the optimal good, their movement would be excessively demanding, and thus not only very small but potentially ruthless; if they lean too hard in the direction of just trying to be good people, their movement would not be anything special...</p><p>The broader culture is marked by neither a widespread sensitivity to misery nor a pervasive sense of obligation to do something practical about it, and for all of its faults the culture of E.A. was. One didn\u2019t have to agree with everything they did to believe that they created a worthwhile role for themselves and acquitted themselves honorably.</p></blockquote><p>Although this doesn't mention the 10% GWWC pledge, it is digging into the dilemma that the pledge is meant to address - setting a substantial but manageable standard for what it means to do an adequate job of earning to give.</p><p>For the author, one of the problems with EA's culture that enabled Sam Bankman Friend to pull the wool over our eyes is an attitude among the leadership that he attributes to Rob Wiblin:</p><blockquote><p>In other words, <strong>it seems as though the only thing that truly counts for Wiblin is the inviolate sphere of ideas\u2014not individual traits, not social relationships, not \u201cshe said\u201d disagreements about whether it was wise to throw in one\u2019s lot with billionaire donors of murky motive, and certainly not \u201ctraditional liberal concerns.\u201d</strong> (Wiblin told me, \u201cI wasn\u2019t talking about articles that focus on personal virtue, integrity, or character. I was talking about, for example, a focus on physical appearance, individual quirks, and charisma.\u201d) <strong>Effective altruism did not create Sam Bankman-Fried, but it is precisely this sort of attitude among E.A.\u2019s leadership, a group of people that take great pride in their discriminatory acumen, that allowed them to downweight the available evidence of his ethical irregularities.</strong> This was a betrayal of the E.A. rank and file, which is, for the most part, made up of extremely decent human beings.</p></blockquote><p>Again, we find that one of the critical themes about EA is its resistance to compromise with extant moral and cultural ways of parsing issues.</p><p>Luke Freeman, the Executive Director at Giving What We Can, <a href=\"https://forum.effectivealtruism.org/posts/SjK9mzSkWQttykKu6/big-tent-effective-altruism-is-very-important-particularly\">said in 2022</a> that:</p><blockquote><p>We are aware that we are one of the \"shop fronts\" [of the Effective Altruism movement] at Giving What We Can.&nbsp;</p></blockquote><p><a href=\"https://time.com/6204627/effective-altruism-longtermism-william-macaskill-interview/\">A Times Article on MacAskill and EA </a>specifically mentions the Giving What We Can 10% pledge.</p>", "user": {"username": "AllAmericanBreakfast"}}, {"_id": "ARkbWch5RMsj6xP5p", "title": "Transformative AGI by 2043 is <1% likely", "postedAt": "2023-06-06T15:51:07.142Z", "htmlBody": "<p>(<a href=\"https://www.lesswrong.com/posts/DgzdLzDGsqoRXhCK7/transformative-agi-by-2043-is-less-than-1-likely\">Crossposted </a>to LessWrong)</p><h2>Abstract</h2><p>The linked paper is our submission to the <a href=\"https://www.openphilanthropy.org/open-philanthropy-ai-worldviews-contest/\">Open Philanthropy AI Worldviews Contest</a>. In it, we estimate the likelihood of transformative artificial general intelligence (AGI) by 2043 and find it to be &lt;1%.</p><p>Specifically, we argue:</p><ul><li><strong>The bar is high:</strong> AGI as defined by the contest\u2014something like AI that can perform nearly all valuable tasks at human cost or less\u2014which we will call transformative AGI is a much higher bar than merely massive progress in AI, or even the unambiguous attainment of expensive superhuman AGI or cheap but uneven AGI.</li><li><strong>Many steps are needed:</strong> The probability of transformative AGI by 2043 can be decomposed as the joint probability of a number of necessary steps, which we group into categories of software, hardware, and sociopolitical factors.</li><li><strong>No step is guaranteed:</strong> For each step, we estimate a probability of success by 2043, conditional on prior steps being achieved. Many steps are quite constrained by the short timeline, and our estimates range from 16% to 95%.</li><li><strong>Therefore, the odds are low:</strong> Multiplying the cascading conditional probabilities together, we estimate that transformative AGI by 2043 is <strong>0.4% likely</strong>. Reaching &gt;10% seems to require probabilities that feel unreasonably high, and even 3% seems unlikely.</li></ul><p>Thoughtfully applying the cascading conditional probability approach to this question yields lower probability values than is often supposed. This framework helps enumerate the many future scenarios where humanity makes partial but incomplete progress toward transformative AGI.</p><h2>Executive summary</h2><p>For AGI to do most human work for &lt;$25/hr by 2043, many things must happen.</p><p>We forecast cascading conditional probabilities for 10 necessary events, and find they multiply to an overall likelihood of 0.4%:</p><figure class=\"table\"><table><tbody><tr><td><h3>Event</h3></td><td><h3>Forecast</h3><p>by 2043 or TAGI,<br>conditional on<br>prior steps</p></td></tr><tr><td>We invent algorithms for transformative AGI</td><td>60%</td></tr><tr><td>We invent a way for AGIs to learn faster than humans</td><td>40%</td></tr><tr><td>AGI inference costs drop below $25/hr (per human equivalent)</td><td>16%</td></tr><tr><td>We invent and scale cheap, quality robots</td><td>60%</td></tr><tr><td>We massively scale production of chips and power</td><td>46%</td></tr><tr><td>We avoid derailment by human regulation</td><td>70%</td></tr><tr><td>We avoid derailment by AI-caused delay</td><td>90%</td></tr><tr><td>We avoid derailment from wars (e.g., China invades Taiwan)</td><td>70%</td></tr><tr><td>We avoid derailment from pandemics</td><td>90%</td></tr><tr><td>We avoid derailment from severe depressions</td><td>95%</td></tr><tr><td><strong>Joint odds</strong></td><td><strong>0.4%</strong></td></tr></tbody></table></figure><p>If you think our estimates are pessimistic, feel free to substitute your own&nbsp;<a href=\"https://www.tedsanders.com/agi-forecaster/\"><u>here</u></a>. You\u2019ll find it difficult to arrive at odds above 10%.</p><p>Of course, the difficulty is by construction. Any framework that multiplies ten probabilities together is almost fated to produce low odds.</p><p>So a good skeptic must ask: Is our framework fair?</p><p>There are two possible errors to beware of:</p><ul><li>Did we neglect possible parallel paths to transformative AGI?</li><li>Did we hew toward unconditional probabilities rather than fully conditional probabilities?</li></ul><p>We believe we are innocent of both sins.</p><p>Regarding failing to model parallel disjunctive paths:</p><ul><li>We have chosen generic steps that don\u2019t make rigid assumptions about the particular algorithms, requirements, or timelines of AGI technology</li><li>One opinionated claim we do make is that transformative AGI by 2043 will almost certainly be run on semiconductor transistors powered by electricity and built in capital-intensive fabs, and we spend many pages justifying this belief</li></ul><p>Regarding failing to really grapple with conditional probabilities:</p><ul><li>Our conditional probabilities are, in some cases, quite different from our unconditional probabilities. In particular, we assume that a world on track to transformative AGI will\u2026<ul><li>Construct semiconductor fabs and power plants at a far faster pace than today (our unconditional probability is&nbsp;<i>substantially&nbsp;</i>lower)</li><li>Have invented very cheap and efficient chips by today\u2019s standards (our unconditional probability is&nbsp;<i>substantially&nbsp;</i>lower)</li><li>Have&nbsp;<i>higher</i> risks of disruption by regulation</li><li>Have&nbsp;<i>higher&nbsp;</i>risks of disruption by war</li><li>Have&nbsp;<i>lower&nbsp;</i>risks of disruption by natural pandemic</li><li>Have&nbsp;<i>higher</i> risks of disruption by engineered pandemic</li></ul></li></ul><p>Therefore, for the reasons above\u2014namely, that transformative AGI is a very high bar (far higher than \u201cmere\u201d AGI) and many uncertain events must jointly occur\u2014we are persuaded that the likelihood of transformative AGI by 2043 is &lt;1%, a much lower number than we otherwise intuit. We nonetheless anticipate stunning advancements in AI over the next 20 years, and forecast substantially higher likelihoods of transformative AGI beyond 2043.</p><h3><strong>For details, read </strong><a href=\"https://arxiv.org/abs/2306.02519\"><strong>the full paper</strong></a><strong>.</strong></h3><h2>About the authors</h2><p>This essay is jointly authored by Ari Allyn-Feuer and Ted Sanders. Below, we share our areas of expertise and track records of forecasting. Of course, credentials are no guarantee of accuracy. We share them not to appeal to our authority (plenty of experts are wrong), but to suggest that if it sounds like we\u2019ve said something obviously wrong, it may merit a second look (or at least a compassionate understanding that not every argument can be explicitly addressed in an essay trying not to become a book).</p><h3>Ari Allyn-Feuer</h3><p><strong>Areas of expertise</strong></p><p>I am a decent expert in the complexity of biology and using computers to understand biology.</p><ul><li>I earned a Ph.D. in Bioinformatics at the University of Michigan, where I spent years using ML methods to model the relationships between the genome, epigenome, and cellular and organismal functions. At graduation I had offers to work in the AI departments of three large pharmaceutical and biotechnology companies, plus a biological software company.</li><li>I have spent the last five years as an AI Engineer, later Product Manager, now Director of AI Product, in the AI department of GSK, an industry-leading AI group which uses cutting edge methods and hardware (including Cerebras units and work with quantum computing), is connected with leading academics in AI and the epigenome, and is particularly engaged in reinforcement learning research.</li></ul><p><strong>Track record of forecasting</strong></p><p>While I don\u2019t have Ted\u2019s explicit formal credentials as a forecaster, I\u2019ve issued some pretty important public correctives of then-dominant narratives:</p><ul><li>I said in print on January 24, 2020 that due to its observed properties, the then-unnamed novel coronavirus spreading in Wuhan, China, had a significant chance of promptly going pandemic and killing tens of millions of humans. It subsequently did.</li><li>I said in print in June 2020 that it was an odds-on favorite for mRNA and adenovirus COVID-19 vaccines to prove highly effective and be deployed at scale in late 2020. They subsequently did and were.</li><li>I said in print in 2013 when the Hyperloop proposal was released that the technical approach of air bearings in overland vacuum tubes on scavenged rights of way wouldn\u2019t work. Subsequently, despite having insisted they would work and spent millions of dollars on them, every Hyperloop company abandoned all three of these elements, and development of Hyperloops has largely ceased.</li><li>I said in print in 2016 that Level 4 self-driving cars would not be commercialized or near commercialization by 2021 due to the long tail of unusual situations, when several major car companies said they would. They subsequently were not.</li><li>I used my entire net worth and borrowing capacity to buy an abandoned mansion in 2011, and sold it seven years later for five times the price.&nbsp;</li></ul><p>Luck played a role in each of these predictions, and I have also made other predictions that didn\u2019t pan out as well, but I hope my record reflects my decent calibration and genuine open-mindedness.</p><h3>Ted Sanders</h3><p><strong>Areas of expertise</strong></p><p>I am a decent expert in semiconductor technology and AI technology.</p><ul><li>I earned a PhD in Applied Physics from Stanford, where I spent years researching semiconductor physics and the potential of new technologies to beat the 60 mV/dec limit of today's silicon transistor (e.g., magnetic computing, quantum computing, photonic computing, reversible computing, negative capacitance transistors, and other ideas). These years of research inform our perspective on the likelihood of hardware progress over the next 20 years.</li><li>After graduation, I had the opportunity to work at Intel R&amp;D on next-gen computer chips, but instead, worked as a management consultant in the semiconductor industry and advised semiconductor CEOs on R&amp;D prioritization and supply chain strategy. These years of work inform our perspective on the difficulty of rapidly scaling semiconductor production.</li><li>Today, I work on AGI technology as a research engineer at OpenAI, a company aiming to develop transformative AGI. This work informs our perspective on software progress needed for AGI. (Disclaimer: nothing in this essay reflects OpenAI\u2019s beliefs or its non-public information.)</li></ul><p><strong>Track record of forecasting</strong></p><p>I have a track record of success in forecasting competitions:</p><ul><li>Top prize in SciCast technology forecasting tournament (15 out of ~10,000, ~$2,500 winnings)</li><li>Top Hypermind US NGDP forecaster in 2014 (1 out of ~1,000)</li><li>1st place Stanford CME250 AI/ML Prediction Competition (1 of 73)</li><li>2nd place \u2018Let\u2019s invent tomorrow\u2019 Private Banking prediction market (2 out of ~100)</li><li>2nd place DAGGRE Workshop competition (2 out of ~50)</li><li>3rd place LG Display Futurecasting Tournament (3 out of 100+)</li><li>4th Place SciCast conditional forecasting contest</li><li>9th place DAGGRE Geopolitical Forecasting Competition</li><li>30th place Replication Markets (~$1,000 winnings)</li><li>Winner of ~$4200 in the 2022 Hybrid Persuasion-Forecasting Tournament on existential risks (told ranking was \u201cquite well\u201d)</li></ul><p>Each finish resulted from luck alongside skill, but in aggregate I hope my record reflects my decent calibration and genuine open-mindedness.</p><h2>Discussion</h2><p>We look forward to discussing our essay with you in the comments below. The more we learn from you, the more pleased we'll be.</p><p>If you disagree with our admittedly imperfect guesses, we kindly ask that you supply your own preferred probabilities (or framework modifications). It's easier to tear down than build up, and we'd love to hear how you think this analysis can be improved.</p>", "user": {"username": "Ted Sanders"}}, {"_id": "uedpFddfLct7qtRLx", "title": "Spanish Translation of \"The Precipice\" by Toby Ord (Unofficial)", "postedAt": "2023-06-06T01:11:58.975Z", "htmlBody": "<p>Hello everyone! I'm pleased to make this announcement. Now, free from language barriers, a wider audience can delve into one of the most pivotal books of the Effective Altruism movement. Enjoy!<br><br><br>&nbsp;</p>", "user": {"username": "davidfriva"}}, {"_id": "YAcbDoKbpPvTdfwyF", "title": "What is Progress and Why I think social science is the key to the future.", "postedAt": "2023-06-06T08:51:30.527Z", "htmlBody": "<p>There exist a prolific discussion on what we need to solve the world's pressing problems. Most of these discussions, however, center around natural sciences and technology and lack the perspectives of social sciences. My essay demonstrates the significance of social science and I argue that the key to progress lies in social science.</p><p>https://anthonyfu.wixsite.com/sociality/post/general-reasoning-planning-of-social-science-based-education-first-draft</p>", "user": {"username": "Anthony Fu"}}, {"_id": "Poq2MKtH7nb9gHygT", "title": "AISafety.info \"How can I help?\" FAQ", "postedAt": "2023-06-05T22:09:58.008Z", "htmlBody": "", "user": {"username": "StevenKaas"}}, {"_id": "hChXEPPkDpiufCE4E", "title": "I made a news site based on prediction markets", "postedAt": "2023-06-05T18:33:33.222Z", "htmlBody": "<h1>Introduction</h1><h2><strong>\u201cNews through prediction markets\u201d</strong></h2><p>The Base Rate Times is a nascent news site that incorporates prediction markets prominently into its coverage.</p><p>Please see current iteration:&nbsp;<a href=\"http://www.baseratetimes.com\"><u>www.baseratetimes.com</u></a></p><p>Twitter:&nbsp;<a href=\"http://www.twitter.com/base_rate_times\"><u>www.twitter.com/base_rate_times</u></a></p><h1>What problem does it solve?</h1><h2><strong>Forecasts are underutilized by the media</strong></h2><p>Prediction markets are more accurate than pundits, yet the media has made limited use of their forecasts. This is a big problem: one of the most rigorous information sources is being omitted from public discourse!</p><p>The Base Rate Times creates prediction markets content, substituting for inferior news sources. This improves the epistemics of its audience.</p><h2><strong>Forecasts are dispersed, generally inconvenient to consume</strong></h2><p>Prediction markets are dispersed among many different platforms, fragmenting the information forecasters provide. For example, different platforms ask similar questions in different ways. Furthermore, platforms\u2019 UX is orientated towards forecasters, not information consumers. Overall, trying to use prediction markets as \u2018news replacement\u2019 is cumbersome.</p><p>There is value in aggregating and curating forecasts from various platforms. We need engaging ways of sharing prediction markets\u2019 insights. The Base Rate Times aims to make prediction markets easily digestible to the general public.</p><h1>How does it work?</h1><h2><strong>News media (emotive narrative) vs Base Rate Times (actionable odds)</strong></h2><p>For example, this is a real headline from a reputable newspaper: <i><strong>\u201cTaiwan braces for China's fury over Pelosi visit\u201d</strong>. </i>Emotive and incendiary, it does not help you form an accurate model of the situation.</p><p>By contrast, The Base Rate Times: <i><strong>\u201cChina-Taiwan conflict risk 14%, up 2x from 7% after Pelosi visit\u201d</strong>. </i>That's an actionable insight. It can inform your decision on whether to stay in Taiwan or to flee, for example.</p><h2><strong>News aggregation, summarizing prediction markets</strong></h2><p>Naturally, the probabilities in the example above come from prediction markets. The Base Rate Times presents what prediction markets are telling us about news in an engaging way.</p><p>Stories that shift market odds are highlighted. And if a seemingly important story doesn\u2019t shift market odds, that also tells you something.</p><p>On The Base Rate Times, right now you can see the latest odds on:</p><ul><li>Putin staying in power</li><li>Russian territorial gains in Ukraine</li><li>Escalation risk of NATO involvement</li><li>and more...</li></ul><p>By glancing at a few charts, you can form a more accurate model (in less time) of Russia-Ukraine than reading countless narrative-based news stories.</p><h1>Inspiration</h1><p>A key inspiration was Scott Alexander\u2019s&nbsp;<a href=\"https://astralcodexten.substack.com/p/prediction-market-faq\"><u>Prediction Market FAQ</u></a>:</p><blockquote><p><i>I recently had to read many articles on Elon Musk\u2019s takeover of Twitter, which all repeated that \u201crumors said\u201d Twitter was about to go down because of his mass firing. Meanwhile, there were several prediction markets on whether this would happen, and they were all around 40%. If some journalist had thought to check the prediction markets and cite them in their article, they could have not only provided more value (a clear percent chance instead of just \u201cthere are some rumors saying this\u201d), but also been right when everyone else was wrong.</i></p></blockquote><p>Also Scott\u2019s 'Mantic Monday' posts and Zvi\u2019s&nbsp;<a href=\"https://thezvi.substack.com/\"><u>blog</u></a>.</p><p>This&nbsp;<a href=\"https://twitter.com/ClayGraubard/status/1496699988801433602\"><u>simple chart</u></a> by @ClayGraubard was another inspiration. Wanted something like this, but for all major news stories. Couldn't find it, so making it myself. (Clay is making geopolitics videos and podcasts now,&nbsp;<a href=\"https://twitter.com/GeovaneVideo\"><u>check it out</u></a>.)</p><h1>Goals</h1><h2><strong>Like 538, but for prediction markets</strong></h2><p>The Base Rate Times is a bet that forecasts can be popularized, as opinion polls have been, and improve society\u2019s models of the world.</p><p>Goal: Longshot probability of going mainstream, e.g. like 538.</p><p>If highly successful in scaling, we\u2019d be effectively running an experiment on whether prediction can markets serve as early warning systems. For example, if there was a major newspaper consistently reporting a 1 in 3 risk of a global pandemic before COVID-19, would it have made a difference?</p><h1>Future</h1><h2><strong>Next topic the site will cover is Artificial Intelligence</strong></h2><p>Launched with coverage of Russia-Ukraine and Nuclear War (with a small sideline on US debt). The plan is to expand into a new \u2018vertical\u2019 every month, the topic being decided via&nbsp;<a href=\"https://twitter.com/base_rate_times/status/1659188337855578113\"><u>Twitter polls</u></a> (I am a man of the people). Coverage of AI will be launched in June.</p><h2><strong>AI-generated \u2018enhanced\u2019 article summaries</strong></h2><p>Currently news links are summarized by AI into 3-4 bullet points. Testing ways of enhancing these summaries, can AI\u2026:</p><ul><li>find historical data to contextualize the story?</li><li>advise forecast updates based on the story?</li><li>make counter-arguments?</li><li>translate articles from e.g. Chinese?</li><li>detect bias &amp; cross-reference claims?</li></ul><h2><strong>LessWrong-style reacts</strong></h2><p>As soon as I saw that \u2018Key Insight\u2019 react, I knew I had to steal it! Using LessWrong as an inspiration, the plan is to come up with ~5 reacts that are directly relevant to The Base Rate Times.</p><h2><strong>Community Notes-style overlay</strong></h2><p>Eventually I would like a \u2018Community Notes\u2019-style overlay on top of all of The Base Rate Times. For example, there might be a more liquid or better suited prediction market that I\u2019ve missed. Or I might\u2019ve grouped together markets in a way that requires more context. Readers could also correct AI-summaries and any headlines I might write.</p><h2><strong>Driving traffic to prediction platforms</strong></h2><p>Right now it\u2019s not that convenient to go from The Base Rate Times to one of the referenced prediction markets. Plan is \u2018instant check-out\u2019 for placing a trade and easier clickthrough to the platforms.</p><h2><strong>Tagging tweets with prediction markets</strong></h2><p>Ever read a tweet (perhaps with a news link) and thought, \u2018I bet that just isn\u2019t true?\u2019. Idea is to match dodgy viral tweets to relevant prediction markets.</p><p>Community Notes are well suited to adding missing info or making factual corrections. But prediction markets are better at fighting vague verbiage and contextualizing stories with concrete odds.</p><h2><strong>Helping top forecasters monetize their skillset</strong></h2><p>Prediction market traders are making a contribution to the public good by helping create accurate odds for current events. Unlike financial market traders, they are not richly compensated for \u2018price discovery\u2019 as many of the top platforms are \u2018play money\u2019. I have some ideas on how to get e.g. top Metaculus users paid, more on this later\u2026</p><h1>Feedback</h1><h2><strong>Still in the MVP stage</strong></h2><p>This is an MVP -- I welcome any and every bit of feedback, big or small. Please feel free to be critical.</p><p>What's one thing you would change about the website?&nbsp;</p><p>Some specific areas you might like to comment on:</p><ul><li>general website layout (should I change to 2 columns?)</li><li>quality of headlines</li><li>chart design (should I replace data labels with a Y-axis?)</li><li>link selection and presentation</li><li>quality of the AI-generated article summaries</li></ul><h2><strong>Grant suggestions please!</strong></h2><p>I would also appreciate any suggestions on grants (or other funding) to apply for.</p><p>Thank you for reading! If you made it this far, then damn u is gangsta af.</p>", "user": {"username": "vandemonian"}}, {"_id": "RfiEENFj8SaYty9Gj", "title": "National EA groups shouldn\u2019t focus on city groups", "postedAt": "2023-06-05T16:01:25.221Z", "htmlBody": "<h1><strong>Summary&nbsp;</strong></h1><p>National<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8zq3rlgv99h\"><sup><a href=\"#fn8zq3rlgv99h\">[1]</a></sup></span>&nbsp;EA groups have a variety of strategies available to them, but many seem to focus on supporting local city groups as the main activity with less consideration of other interventions. I think this leads to neglecting more impactful activities for national groups. Potentially this is because they are following more established groups/resources where city groups are given as a default example.&nbsp;<br>&nbsp;</p><ul><li>Most people interested in EA are not joining<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref333hqhra0tg\"><sup><a href=\"#fn333hqhra0tg\">[2]</a></sup></span>&nbsp;local EA groups, and most people who could get more involved in EA don\u2019t necessarily want to do that via joining a local group first<ul><li>From EA London&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/2RfQT7cybfS8zoy43/are-men-more-likely-to-attend-ea-london-events-attendance#Data\"><u>attendance data</u></a> for 2016-18, out of ~1300 people roughly 75% attended just 1 event, and only 10% attended 4 or more which suggests that most weren't aiming to become regular members</li></ul></li><li>There is an unseen majority of people who know about EA and want to have more impact who are neglected by a city-first strategy</li><li>EA should attract more people than those also looking for community</li><li>Community is still important, but should be seen as additional rather than a main focus&nbsp;<ul><li>Community can mean a lot of different things but I\u2019m defining community in this post as a more densely connected subset of a network based around a location<ul><li>In practice this means a community is more likely to involve social gatherings, daily/weekly in person touchpoints</li><li>A network will involve conferences, mentorship, newsletters/social media, monthly/yearly touchpoints&nbsp;</li></ul></li></ul></li><li>There is probably value to having some city organisers if there is a critical mass of people interested in EA and the city has&nbsp; strong comparative advantages<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6ava0iq0fps\"><sup><a href=\"#fn6ava0iq0fps\">[3]</a></sup></span></li><li>Alternative strategies could include cause specific field building, career advising, supporting professional networks nationally, organisation incubation, translation<br>&nbsp;</li></ul><figure class=\"image image_resized\" style=\"width:52.81%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RfiEENFj8SaYty9Gj/jb9jvkso4wjmsyo483ad\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RfiEENFj8SaYty9Gj/ocyapo8qcrsbfrcgohue 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RfiEENFj8SaYty9Gj/bkg1js90jkar1fu9z9x4 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RfiEENFj8SaYty9Gj/q5y2qf2vfonuil09tzwx 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RfiEENFj8SaYty9Gj/s41pd3ejypjomwhwyo34 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RfiEENFj8SaYty9Gj/xxb1s3jze5atvoupasft 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RfiEENFj8SaYty9Gj/culjkslgpyl67vpc41wq 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RfiEENFj8SaYty9Gj/decwmdtj7o9ejdpgtjzt 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RfiEENFj8SaYty9Gj/pwrqgblnodmnoindhxpw 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RfiEENFj8SaYty9Gj/poqd2wx22xunxdj6icu9 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RfiEENFj8SaYty9Gj/vjn6imu5amlktxbuaoqx 1024w\"><figcaption>Upside down lightbulb with a city inside</figcaption></figure><h1>&nbsp;</h1><h1><strong>The Unseen Majority</strong></h1><p>When most people hear about EA for the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/tzFcqGmCA6ePeD5wm/ea-survey-2020-how-people-get-involved-in-ea\"><u>first time</u></a>, it\u2019s usually via an online resource (80,000 Hours, GWWC, podcast) or word of mouth. The message they receive is that EA cares about having more impact and that EA as a movement is trying to help people have more impact.</p><p>This can contrast to the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/qSG8Q2KpCdFyT9tf9/my-local-ea-group-has-an-unfriendly-and-impersonal-vibe-via\"><u>experience</u></a> of&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xomFCNXwNBeXtLq53/bad-omens-in-current-community-building#Part_1___Reasons_I_and_others_did_not_become_an_EA\"><u>going</u></a> along to a local group (which is regularly&nbsp;<a href=\"https://80000hours.org/problem-profiles/promoting-effective-altruism/#how-to-enter-this-area\"><u>suggested</u></a> as a good way to get more involved with EA), and experiencing the main message as \u2018join our community\u2019, with less focus on helping that person have impact.&nbsp;This could lead to people who are focused on generating a lot of impact bouncing away from EA. Anecdotally I have heard people say that they don\u2019t find that much value from attending local group events but are still interested in EA and focus on having an impact in their career.&nbsp;</p><p>For the subset of people who are looking for community, local groups can be great. But for a lot of people who do not have that preference/have other life circumstances, this isn\u2019t what they are looking for.&nbsp; People already have communities they are a part of (family, friends, professional, hobbies) and often don\u2019t have time for many more. Anecdotally from conversations with other organisers the people most likely to join are those looking for a community -&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/t6bM2T2rohjvwdwYY/local-ea-group-organizers-survey-2019#Group_Types\"><u>students</u></a>, recent graduates or people who are new to the city.</p><p>This can be self reinforcing as the people who are likely to keep on attending meetups are the ones with spare time and lacking community. We often use neglectedness when choosing cause areas, leading to support of unseen majorities - people in poorer parts of the world, animals and future beings. But when it comes to movement building there is less thought paid to those who aren\u2019t visible. A lot of strategies I have seen are about increasing attendance or engagement at events rather than providing value to people who may not be as interested in attending lots of events each year but still want to consider career changes.</p><p>This can also affect user surveys, where the people most likely to respond are those more interested in the community, leading to group organisers doubling down on benefits to current attendees.&nbsp;</p><p>There will also be a lot of people who don\u2019t happen to live in the biggest cities, or live quite far from the city centre where most activities happen. Focusing on a few cities can lead to allocating fewer resources to these people.<br>&nbsp;</p><h1>&nbsp;</h1><h1><strong>Community is valuable but can be separate</strong></h1><p>If people do want a community with similar values in their local area, it may be better to optimise this&nbsp;<a href=\"https://www.lesswrong.com/posts/3p3CYauiX8oLjmwRF/purchase-fuzzies-and-utilons-separately\"><u>separately</u></a> from their impact. Trying to build a local group that is impactful as well as being fun seems harder than having impact with careers/donations/networking and then building friendships with other people interested in EA in your city outside of the context of an EA group. National groups could still have contacts in each city who are happy to chat to people about EA or specific causes/career paths.&nbsp;<br>&nbsp;</p><p>There is a lot of value to having a sense of community in a workplace and for there to be stronger links between various subsets of the wider EA network, but this doesn\u2019t necessarily have to be focused on location. Also community can often be self organising whereas national networks and projects are harder to get off the ground without dedicated time and employees.&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><h1><strong>What Else Could You Do?</strong></h1><p>An emphasis on getting people to run local groups might lead to them neglecting better opportunities such as gaining better career capital, volunteering with more impactful organisations, helping with the national group or cause specific field building work outside of local groups.&nbsp;</p><h2><strong>Alternatives</strong></h2><p>This will vary with comparative advantages and context but here are some ideas on other possible activities for national groups to focus more on.</p><ul><li>Supporting field building for specific causes</li><li><a href=\"https://forum.effectivealtruism.org/posts/8ZrdmwEnRRSdXdJe2/ea-israel-2022-progress-and-2023-plans#Israeli_EA_community_achievements\"><u>Organisation incubation</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/WrqmHASnyTgtpYcmu/effektiv-spenden-fundraiser-and-2022-plans-2\"><u>Supporting impactful giving</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/JkjZdb2wsYwB3pNCd/open-position-head-of-career-advice-at-ea-sweden\"><u>Career advising</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/JAKmTnrJWP5fGbaGk/celebrating-eagxlatam-and-eagxindia\"><u>Conferences</u></a>/specialised events</li><li>Marketing/media</li><li><a href=\"https://forum.effectivealtruism.org/topics/translation\"><u>Translation</u></a></li><li>National network/directory/point of contact/connecting key individuals with each other</li></ul><p>&nbsp;</p><p>If you have thoughts on any of the above<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref28gbbrmd3fj\"><sup><a href=\"#fn28gbbrmd3fj\">[4]</a></sup></span>&nbsp;add a comment, or send me a message and we can dive into any disagreements or think about alternative interventions.</p><p>&nbsp;</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8zq3rlgv99h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8zq3rlgv99h\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>This may only be relevant for national/city group organisers or people thinking about EA/cause area movement building strategies. This is anecdotal and based mainly on conversations with other organisers and people involved in EA but not that active with city groups</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn333hqhra0tg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref333hqhra0tg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;From the 2020 EA Survey with 1856 responses - 50% say that they are a&nbsp;local <a href=\"https://forum.effectivealtruism.org/s/YLudF7wvkjALvAgni/p/4xczoALF6adpQk3TN#Group_membership\"><u>group member</u></a></p><p>Compared to GWWC members - 9000+ and subscribers to 80,000 Hours - 160,000+, it looks like out of the potential population that is inclined towards EA ideas, few of them are regularly active in local groups</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6ava0iq0fps\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6ava0iq0fps\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Although maybe this would be better seen as people organising around a specific group - like Oxford university or the UK civil service</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn28gbbrmd3fj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref28gbbrmd3fj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Thanks to LT, EH and MC for feedback</p></div></li></ol>", "user": {"username": "DavidNash"}}, {"_id": "sQ6NdLT9g7Z84f9rL", "title": "Feedback requested: EA Forum reactions", "postedAt": "2023-06-05T16:48:11.071Z", "htmlBody": "<p>The EA Forum team is experimenting with design options for reacting to posts or comments, and we\u2019d like your feedback over the next week. This post presents some reasoning and a few design directions we are considering.<br>&nbsp;</p><h2><strong>Motivation and goals</strong></h2><p>We\u2019ve been working on a feature to allow users to react to content in more granular ways than upvoting without needing to write a full comment. This project is similar to what our friends at LessWrong&nbsp;<a href=\"https://www.lesswrong.com/posts/SzdevMqBusoqbvWgt/open-thread-with-experimental-feature-reactions\"><u>are working on</u></a>, with some differences in motivation and design constraints that we\u2019ll lay out below.</p><p><strong>Motivation</strong></p><p>We\u2019ve heard from a number of users that after putting in many hours of work into Forum posts, sometimes the only (positive) feedback they get is in the form of karma. Karma can sometimes feel opaque and sterile rather than motivating. While some users do write short positive comments, we\u2019ve heard that it can feel like the bar for commenting is high, requiring a nuanced take or useful feedback. We think reactions can provide a middle ground between an upvote and a comment.</p><p><strong>Why restrict reactions to positive feedback?</strong></p><p>In our first iteration of this feature, we tested several reactions with neutral or negative emotional connotations like \ud83e\udd14 (confused). We found that authors were motivated to follow up with users reacting in such a way and clarify confusion or feedback, but negative reaction emojis didn\u2019t have enough information to be actionable in the way that a full comment is. Therefore, we\u2019ve decided to limit ourselves to positive reactions. One counterargument we\u2019ve heard is that focusing only on positive reactions could make it harder to give constructive and critical feedback. We suspect this wouldn\u2019t deter most readers who have good points to make, but it is something we want to hear from you about and track over time.</p><p><strong>Keep it simple</strong></p><p>As we approach this feature, we want to keep the user experience simple and intuitive.</p><p>If we add another form of voting / reacting, it must be very easy to understand how it relates to karma and agree/disagree voting. Our first iteration of the feature added reactions at the comment level, and most users we interviewed found it confusing to deal with karma, agree/disagree, and reactions on the same piece of content. Based on that feedback, we\u2019re quite confident we need to prioritize simplicity. For instance, we\u2019re unlikely to add a third dimension of voting like LessWrong are considering without some way of maintaining simplicity, and we\u2019re unlikely to add a very large set of possible reactions.</p><p>&nbsp;</p><h2><strong>Design questions we have</strong></h2><p><strong>1. Reactions on posts, comments, or both?</strong></p><p>Importantly, if we add reactions to comments, we would likely change agree/disagree voting significantly (removing strong votes and possibly anonymity) to maintain simplicity. We\u2019re curious how users feel about this tradeoff.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/wvingedm2dfjfou9vnaw\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/unpvvaib3lmvwczftmrt 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/hpdvsln4enzrltab2gel 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/jvyggf2v0bzoduyw84pf 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/rcpcltqjbg5od0ptdauj 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/z6zr74wz4mzvtapwel53 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/i1jzhfewda7tnh2znlcm 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/uvxuwkbeexpdwywt23bi 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/wj54thumixqtumm7mhb1 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/xfgb0hchshhlshq5oih3 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/orslxudymjj6rttn0ocv 945w\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/ynpals2mvuvphwbnqueu\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/zuhxsh1gmtf5rjkehned 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/igizkzcwssev1fvzolzg 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/p2aqtbquqkpyovobuis3 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/tftx8e2e90qzoo6fp8ew 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/v0xdnzh0bqhbkymrxdds 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/sujsgdiw28higf8kpamc 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/dwgr0lihzj16zz6iyvef 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/s1d9ire0ot9cdtvboigx 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/ydwv45vhptaodudlyyoc 810w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/pnmgkfq4gh56zjappd3o 861w\"></p><p>&nbsp;</p><p>We hypothesize that the biggest gap in terms of content-interaction and feedback is at the post level, since comments are generally shorter and more specific, and already have agree/disagree voting. On the other hand, we recognize that a lot of interesting back-and-forth discussion happens in comments, where reactions can also be useful.<br>&nbsp;</p><p><strong>2. Anonymous or non-anonymous?</strong></p><p><strong><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/qonbhwdix1cf4fbls4yt\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/k4ad4kf9ggszxezu07fx 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/eyuokz4fc98ajlgwpalq 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/tsraypbgbillyzn23god 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/lvfmociuy5bhxpma5gow 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/bwtmiav3emwku5pvylhw 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/impq75ue4zom8g3pay7b 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/inclktvgke182797nabt 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/qptcsulcley2tc0nyiik 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/rflfs7tsbd6vbu8f4qar 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/py3oes6j6semfdwxipnb 1128w\"></strong></p><p>We\u2019ve heard that seeing positive feedback from identified individuals can feel more meaningful. On the other hand, we\u2019ve heard that non-anonymous reactions may deter some users from using the feature, particularly if we combine agree/disagree voting into reactions (which we would only do for comments, since posts don\u2019t have agree/disagree voting).&nbsp;</p><p>&nbsp;</p><p><strong>3. What set of reactions?</strong></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/lhwfgij9eejvd2jljxat\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/ziqhddzt4nqkmwngljxe 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/vznz3iblr3zmckpghlk6 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/jxfxyaccsougntwpn60a 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/zo85lsxq8kgj4uuc1aaa 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/d07wdlbeh71xhefw3wao 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/vxeg02ynfyrwqmxdxib3 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/irmuwfrqzkhwwfp5hnlg 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/cjp7srwdx68k8of8gr4l 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/ta8shjym4crbjxih7jhb 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/o03s5gwq3e1ltyln5xw1 945w\"></p><p>As mentioned above, we plan to introduce a fairly restricted set of reactions, but we\u2019re interested in what reactions are most compelling for users and communicate the most important concepts. Do you think we\u2019re missing any key reactions? Are any of these confusing?<br>&nbsp;</p><p><strong>4. (minor) Forum-styled or common emojis?</strong></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/w2gnsc7mpjjwndpnwsyx\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/c2yg7mvezoxqcfytrggi 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/mn9ut8nhhibcbdca3grq 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/yqktxrraldfehoeaw9z0 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/c4dgoy3jxdf5lfmtc8ih 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/csri5w8cicaejzzvt3jb 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/tep44sdpr0yjb14w500u 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/io1ofcmr7zhfbg0hz8lx 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/cygt2z1jgelzokxrqacd 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/wmiulv5rgoivcst2cc4p 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sQ6NdLT9g7Z84f9rL/pogql1i3zion5epyilz7 1600w\"></p><p>&nbsp;</p><p>Forum-styled icons would fit better with the aesthetic of the site, but common emojis are more recognizable and add a bit more energy and color to reactions.</p><p><br>&nbsp;</p><p>We\u2019re really looking forward to feedback and arguments from the community, so please share your thoughts!</p><p><br>&nbsp;</p>", "user": {"username": "Sharang Phadke"}}, {"_id": "BCoWhBsZbDzaywAdp", "title": "Moral Spillover in Human-AI Interaction", "postedAt": "2023-06-05T15:20:53.176Z", "htmlBody": "<p><i>This is a linkpost for https://www.sentienceinstitute.org/blog/moral-spillover-in-human-ai-interaction</i></p><p><i>Written by Katerina Manoli and Janet Pauketat. Edited by Jacy Reese Anthis. Many thanks to Michael Dello-Iacovo, Merel Keijsers, Ali Ladak, and Brad Saad for their thoughtful feedback.</i></p><p>&nbsp;</p><h1>Summary</h1><p><i>Moral spillover</i> is the transfer of moral attitudes or behaviors from one setting to another&nbsp;(e.g., from one being to another, from one behavior to a related behavior, from one group today to related groups in the future). Examples include the transfer of&nbsp;anti-slavery activism to animal rights activism (<a href=\"https://www.sentienceinstitute.org/british-antislavery\"><u>Anthis and Anthis 2017</u></a>)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6bbbsu9upmf\"><sup><a href=\"#fn6bbbsu9upmf\">[1]</a></sup></span>, children\u2019s moral consideration of a biological dog to a robot dog (<a href=\"https://psycnet.apa.org/record/2016-48422-007\"><u>Chernyak and Gary 2016</u></a>), and household energy conservation to water conservation (<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0921344921001774\"><u>Liu et al. 2021</u></a>). Moral spillover seems to be an important driver of&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S0016328721000641\"><u>moral circle expansion</u></a>. Here, we review moral spillover research with a focus on human-AI interaction.&nbsp;Psychological factors, such as pre-existing attitudes towards AIs, as well as AI attributes, such as human-likeness and social group membership, could influence moral spillover between humans and AIs. Spillover of moral consideration to AIs might be hindered by factors such as the&nbsp;<a href=\"https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/spc3.12265\"><u>intention-action gap</u></a> and might be facilitated by interventions such as human-AI contact and promoting a core belief that the moral consideration of AIs is important. We conclude with future research suggestions to examine how pre-existing attitudes affect moral spillover, the potential backfiring of spillover interventions, how spillover affects AIs on a spectrum of similarity to humans, and how temporal spillover functions to shape moral consideration of future AIs, especially based on core beliefs about AI.&nbsp;</p><h1>Introduction</h1><p>The well-being of future&nbsp;<a href=\"https://www.sentienceinstitute.org/blog/the-importance-of-artificial-sentience\"><u>sentient artificial intelligences</u></a> (AIs)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefodpv2i71cpm\"><sup><a href=\"#fnodpv2i71cpm\">[2]</a></sup></span>&nbsp;depends in part on whether moral consideration transfers to them from consideration already extended to other beings, such as humans, nonhuman animals, and AIs who already exist<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdj2hrzinmmo\"><sup><a href=\"#fndj2hrzinmmo\">[3]</a></sup></span>. The transfer of moral attitudes and behaviors, such as moral consideration, from one setting to another can be defined as&nbsp;<i>moral spillover</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmbmile48v4e\"><sup><a href=\"#fnmbmile48v4e\">[4]</a></sup></span>. Moral spillover may be an important part of&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S0016328721000641\"><u>moral circle expansion</u></a>, both for the circles of individual humans and of human societies. For example, a 2017 Sentience Institute&nbsp;<a href=\"https://www.sentienceinstitute.org/british-antislavery\"><u>report</u></a> on the 1800s anti-slavery movement found that the consideration anti-slavery activists had for humans transferred to animals, making them some of the first animal rights activists.&nbsp;</p><p>Given the&nbsp;<a href=\"https://ai100.stanford.edu/2021-report/standing-questions-and-responses/sq2-what-are-most-important-advances-ai\"><u>rapid growth</u></a> of AI application and sophistication and an increasing likelihood that the number of future sentient beings will be&nbsp;<a href=\"https://www.overcomingbias.com/p/a-galaxy-on-earthhtml\"><u>vast</u></a>, here we analyze whether the spillover of moral consideration is feasible or likely between beings that are granted (at least some) consideration (e.g., humans, animals) and future AIs. In psychology and human-robot interaction (HRI) studies, AIs are often used to improve the moral treatment of humans, suggesting that moral consideration can transfer from AIs to humans. For example, positive emotions from being hugged by a robot&nbsp;<a href=\"https://ieeexplore.ieee.org/document/8172336/\"><u>spilled over</u></a> to increase donations to human-focused charities. Some research suggests that the transfer of moral consideration from humans or nonhuman animals to AIs is also possible.&nbsp;<a href=\"https://psycnet.apa.org/record/2016-48422-007\"><u>A 2016 study</u></a> showed that 5- and 7-year-old children with biological dogs at home treated&nbsp;<a href=\"https://robots.ieee.org/robots/aibo2018/\"><u>robot dogs</u></a> better than children without biological dogs. This suggests that moral spillover to AIs might occur&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S2352250X21001378\"><u>incidentally or automatically</u></a> as part of humans\u2019&nbsp;<a href=\"https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00822/full\"><u>social relationships</u></a>.&nbsp;</p><p>We do not know whether or not moral consideration will reliably transfer to and across the diverse range of AIs with different appearances, inner features, or mental capacities who are&nbsp;<a href=\"https://longtermrisk.org/risks-of-astronomical-future-suffering/\"><u>likely to proliferate</u></a> in the future. There is little evidence on whether or not moral consideration would transfer from very different beings to AIs who display very few or none of the same features. For instance, moral consideration of a biological dog might spill over to moral consideration of a robot dog but it may not spill over to moral consideration of a disembodied large language model like&nbsp;<a href=\"https://en.wikipedia.org/wiki/OpenAI#Generative_models\"><u>GPT-n</u></a>. This might be especially significant if arguably superficial features such as appearance, substrate, or purpose override the effects of&nbsp;<a href=\"https://link.springer.com/article/10.1007/s43681-023-00260-1\"><u>features that grant moral standing</u></a> (e.g., sentience). For example, a sentient disembodied algorithm or a sentient cell-like robot, who could theoretically benefit from the transfer of moral consideration based on their sentience, might not.&nbsp;</p><p>This post reviews research on moral spillover in the context of AIs and examines factors that might influence its occurrence. We suggest that spillover might foster the moral consideration of AIs and call for more research to investigate spillover effects on a range of current and future AIs.</p><h1>Types of spillover</h1><p>In economics,&nbsp;<a href=\"https://en.wikipedia.org/wiki/Spillover_(economics)\"><u>spillovers</u></a>, also known as externalities, are a natural part of structural theories in which a transaction affects non-participants, such as if an event in an economy affects another\u2014usually more dependent\u2014economy. In epidemiology, a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Spillover_infection\"><u>spillover event</u></a> occurs when a pathogen transfers from its&nbsp;<a href=\"https://en.wikipedia.org/wiki/Natural_reservoir\"><u>reservoir population</u></a>, such as animals, to a novel host, such as humans. In psychology, a&nbsp;<a href=\"https://psycnet.apa.org/record/2017-08484-007\">spillover</a> occurs when the adoption of&nbsp;an attitude or behavior transfers to other related attitudes or behaviors. Based on the latter definition, moral spillover occurs when moral attitudes or behaviors towards a being or group of beings transfer to another setting (e.g., to another being or group of beings).&nbsp;<a href=\"https://psycnet.apa.org/record/2017-08484-007\"><u>Nilsson et al. (2017</u>)&nbsp;</a>suggested a distinction between three types of spillover:<br>&nbsp;</p><p><i>Figure 1</i>: Types of Spillover</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/bygn0fajlgdhaknbjbjm\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/k5swz7brx1smqp376gib 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/e2lvwojigjxftzx0argj 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/oczg1qepqbovqfaf6tn6 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/x8q3rwmj2xuyv6giytbz 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/tqhmhqlta4zu9f45mhar 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/nli8lddnqvplxxyy1idy 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/a85bl70zggseuc7ssqhx 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/kn5uysebxrtwedpsmahj 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/dyzib2bsqqzd7jake24f 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/ml6mkgqaz1z6vic4hjfi 1416w\"></p><ol><li><strong>Behavioral</strong>: Behavior A increases the probability of behavior B.&nbsp; <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0095069620300486\"><u>Carlsson and colleagues (2021)</u></a> showed that pro-environmental behaviors, such as conserving water, can spill over to other pro-environmental behaviors, such as conserving electricity. In the context of AI, researchers could ask questions like, can a prosocial<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsfibj9kl31\"><sup><a href=\"#fnsfibj9kl31\">[5]</a></sup></span>&nbsp;behavior towards AIs lead to other prosocial behaviors towards AIs? For example, could greeting an AI assistant spill over to protecting this assistant from mistreatment?</li><li><strong>Contextual</strong>: A behavior or attitude in context A increases the probability of this behavior or attitude in context B. Research on contextual AI moral spillover could address questions like, can the moral consideration of sentient beings such as animals spill over to AIs (e.g.,&nbsp;<a href=\"https://psycnet.apa.org/record/2016-48422-007\"><u>Chernyak and Gary 2016</u></a>)?&nbsp;</li><li><strong>Temporal</strong>: A behavior or attitude at time point A increases the frequency of the same or similar behavior or attitude at (a temporally distant) time point B. <a href=\"https://www.frontiersin.org/articles/10.3389/fpsyg.2018.02699/full\"><u>Elf et al. (2019)</u></a> showed that the frequency of pro-environmental behaviors, such as buying eco-friendly products, increased a year after their initial adoption. Temporal spillover may be especially important for AIs given that they are likely to proliferate in the future. Increasing the moral consideration of AIs now might increase the moral and social inclusion of sentient AIs hundreds of years in the future.</li></ol><p>Behavioral, contextual, and temporal spillovers can occur at the same time. These kinds of spillovers can also occur at multiple levels (e.g., from individual to individual, from individual to group, from group to group). Please see Table 1 in the <a href=\"https://www.sentienceinstitute.org/blog/moral-spillover-in-human-ai-interaction\">original post</a> for examples.&nbsp;</p><p>The examples outlined in Table 1 involve the transfer of positive or prosocial&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; attitudes and behaviors. However, negative attitudes and behaviors can also spill over. This transfer can also be behavioral (e.g., ignoring an AI\u2019s plea for help leads to turning off an AI without their consent), contextual (e.g., being rude to a household AI increases the probability of being rude to a workplace AI), or temporal (e.g., intentionally damaging one AI now leads to frequently damaging AIs at a later point). For instance,&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S1071581916301768\"><u>previous research</u></a> has shown that feeling threatened by a highly autonomous AI increases negative attitudes toward all AIs. See Figure 2 for a possible taxonomy of how these spillover types might intersect.&nbsp;</p><p>&nbsp;</p><p><i>Figure 2:&nbsp;</i>Possible Taxonomy of Spillover Types</p><p><br><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/jg2cxyicye6hbyvbkd58\" alt=\"A picture containing text\n\nDescription automatically generated\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/n4jw6vx5alx0guuimmz7 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/cut797u6ony1lrr0b5qn 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/brv3hlvybejb30xy8epn 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/ihm8spgiborecf22x5eu 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/qhduapkgua2gdtlxi3hq 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/yeiv3npx69s6m7vgpr2s 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/iodpthqq8fdeiinyhvs0 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/chqtlwramtksolnjn0mj 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/iuhvtfpnwvzkajimcn23 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BCoWhBsZbDzaywAdp/h17ymoekauurkmpoc8xo 1166w\"><br><br>&nbsp;In the moral domain, the transfer of positive attitudes and behaviors can be associated with increased moral consideration between different groups and settings, like when the moral consideration of a biological dog increased the moral consideration of a robot dog. The transfer of negative attitudes and behaviors might lead to decreased moral consideration.&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0010027712000820\"><u>Uhlmann et al. (2012)</u></a> showed that negative evaluations of a criminal spill over to their biological relatives, who are then more likely to be punished by law than non-biological relatives. The transfer of negative attitudes and behaviors can pose a significant risk to the well-being of sentient AIs, especially if they are&nbsp;held to different standards than other entities.&nbsp;<a href=\"https://www.degruyter.com/document/doi/10.1515/pjbr-2020-0017/html?lang=en\"><u>Bartneck and Keijsers (2020)</u></a> showed that a mistreated robot who fights back is perceived as more abusive than a mistreated human who fights back. The transfer of negative attitudes towards one AI to all AIs could decrease the moral consideration of AIs and obstruct their inclusion in the moral circle.</p><h1><br><strong>What factors shape the likelihood of moral&nbsp;spillover?</strong></h1><p>Whether or not spillover occurs depends on factors such as personality traits and social context. The impact of these factors on spillover has been studied largely in the context of environmental behavior. The same factors are likely valuable for understanding when and how moral spillover applies to AIs. Table 2 in the <a href=\"https://www.sentienceinstitute.org/blog/moral-spillover-in-human-ai-interaction\">original post</a> summarizes some factors identified in previous research.</p><p>One of the more studied factors shaping whether or not spillover occurs is pre-existing attitudes. If pre-existing attitudes towards a spillover target are negative, then the transfer of negative attitudes and behaviors is more likely. If pre-existing attitudes are positive, then the transfer of positive attitudes and behaviors is more likely. Below are three notable studies:</p><ul><li><a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0272494419306218?via%3Dihub\"><u>Henn et al. (2020)</u></a><u>&nbsp;</u>showed that pre-existing attitudes were the driving force behind spillovers in pro-environmental behavior across two separate cohorts: pre-existing positive attitudes towards the environment led to greater spillover between different kinds of pro-environmental behaviors (e.g., saving electricity and saving water).&nbsp;</li><li><a href=\"https://ieeexplore.ieee.org/abstract/document/7745228\"><u>Wullenkord et al. (2016)</u></a> showed that pre-existing negative emotions towards robots spilled over to feeling more negative emotions for robots in general following contact with a robot, compared to a control condition that involved no contact.</li><li><a href=\"https://ieeexplore.ieee.org/abstract/document/5598679\"><u>Stafford et al. (2010)</u></a> showed that pre-existing positive attitudes towards robots became more pronounced after meeting a robot; suggesting that the transfer of positive attitudes from one robot to all robots is easier when positive attitudes towards robots are already present.</li></ul><p>&nbsp;</p><h1>What are the implications of human-AI interaction research for moral spillover?&nbsp;</h1><p>General themes in human-AI interaction have emerged from HRI research: \u201ccomputers are social actors,\u201d the importance of social group membership, and how human and AI features affect interactions. In this section, we consider what these themes imply for moral spillover between humans and AIs, focusing on their implications for the moral consideration of AIs.</p><h2>\u201cComputers are social actors\u201d</h2><p>The&nbsp;<a href=\"https://dl.acm.org/doi/10.1145/191666.191703\"><u>\u201ccomputers are social actors\u201d (CASA) framework</u></a> suggests that machines with human-like capacities, such as verbal or written communication, interactivity (e.g., a response when a button is pressed), and the ability to perform traditional human tasks elicit an automatic attribution of social capacities. This affects responses to them. For example,&nbsp;<a href=\"https://www.mdpi.com/2414-4088/3/1/20\"><u>Lee et al. (2019)</u></a> showed that people felt more positively towards autonomous vehicle voice agents who conformed to social role stereotypes (i.e., informative male voice and social female voice) compared to agents who did not.</p><p>What does CASA imply for moral spillover? Moral spillover might be automatic between humans and AIs because of humans\u2019 propensity to think of AIs as social actors.&nbsp;Moral spillover might occur more for AIs with human-like capacities, given that such&nbsp;features are blatant CASA cues. Some studies using the CASA framework have shown the transfer of consideration from AIs to humans. For example,&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S0747563221000340\"><u>Peter et al. (2021)</u></a> showed that people who interacted with a prosocial robot, compared to a less prosocial robot, gave more resources to other humans.&nbsp;Whether or not similar spillover effects emerge from humans towards AIs is an open question. For instance, the spillover of consideration from humans to AIs could be hindered for AIs who do not display enough human-like capacities (e.g., communication, emotional expression) to trigger CASA attributions.</p><h2>Social group membership</h2><p>Social group membership will likely impact moral spillover from humans to AIs since AIs are increasingly coexisting with humans in&nbsp;<a href=\"https://www.techtarget.com/iotagenda/answer/What-is-human-robot-teaming-and-what-are-its-benefits\"><u>various group settings</u></a>. Generally, people tend to favor&nbsp;<i>ingroups&nbsp;</i>(i.e., members of the same social group) over&nbsp;<i>outgroups</i>, (i.e., members of another social group).&nbsp;<a href=\"https://psycnet.apa.org/record/2014-37732-001\"><u>Ingroup favoritism</u></a> increases cooperation with members of the same group, which can be based on features such as ethnicity, religion, gender, and ideology.&nbsp;</p><p>Moral consideration for ingroup humans could spill over onto ingroup AIs and consideration for ingroup AIs could spill on to other AIs. Shared social group membership can also translate into refusal to harm an ingroup AI.&nbsp;<a href=\"https://ieeexplore.ieee.org/abstract/document/8172280\"><u>Sembroski et al. (2017)</u></a> found that people refused to turn off a robot teammate despite being instructed to do so.&nbsp;<a href=\"https://dl.acm.org/doi/abs/10.5555/3523760.3523851\"><u>Preliminary research</u></a> has also shown that feelings of warmth towards a robot who was perceived as a friend spilled over to positive attitudes towards robots in general.&nbsp;</p><p>Ingroup-based spillover between humans and AIs likely has limits.&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S0747563220303320\"><u>Savela et al. (2021)</u></a> showed that humans identified less with a team that was mostly composed of robots, suggesting an underlying \u201cus\u201d versus \u201cthem\u201d distinction that threatens the identity,&nbsp;status, or control of the human minority. This could potentially inhibit the spillover of moral consideration between humans and AIs, at least in some cases. If humans and AIs coexist in social groups mostly composed of AIs (e.g., in the workplace), this could lead to the transfer of negative attitudes from the ingroup AIs to AIs in general, which might inhibit the inclusion of AIs in the moral circle.&nbsp;</p><h2>Human and AI features</h2><p>Additional research has focused on how the features of AIs (e.g., autonomy, usefulness/ease of operation) and of humans (e.g., cultural values towards AIs, owning AI or robotic devices) shape spillover. This research is likely important to understanding whether or not moral spillover occurs in the context of AIs. This research is summarized in Table 3 of the <a href=\"https://www.sentienceinstitute.org/blog/moral-spillover-in-human-ai-interaction\">original post</a>.&nbsp;</p><p>&nbsp;</p><h1>What is the difference between the spillover of actions and intentions?&nbsp;</h1><p>The transfer of moral consideration could increase intentions to treat AIs morally, but this may not necessarily translate into action. For example,&nbsp;even if someone recognizes the importance of including sentient AIs in the moral circle, they may not act on it by voting for a ban on AI discrimination. There is no guarantee that AIs will be treated well, even if moral consideration transfers to them from other beings. In the short term, humans might not intervene to help a mistreated AI. In the long term, human societies might not implement legislative infrastructure that safeguards the well-being of sentient AIs.</p><p>This phenomenon is known as the&nbsp;<a href=\"https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/spc3.12265\"><u>intention-action gap</u></a> and has been extensively studied in the context of environmental behavior.&nbsp;<a href=\"https://www.nature.com/articles/s41893-019-0263-9\"><u>A recent meta-analysis</u></a> showed that a pro-environmental behavior spills over to the&nbsp;<i>intention</i> to adopt similar behaviors but does not necessarily lead to action. For instance, taking shorter showers might spill over to an intention to start conserving electricity, but does not lead to turning off unused devices. In the context of human-AI interaction, most studies have focused on the spillover of intentions rather than behaviors. For example,&nbsp;<a href=\"https://ieeexplore.ieee.org/abstract/document/5598679\"><u>previous studies</u></a> have found that intentions to engage in prosocial interactions with all AIs increased after interacting with a single AI. However, positive attitudes do not necessarily transfer to behavior or even&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0747563218304825\"><u>behavioral intentions</u></a>. People might not seek out interactions with AIs even if they feel positively towards them or intend to engage in prosocial interactions.</p><p><a href=\"https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/spc3.12265\"><u>A synthesis of studies</u></a> suggested that the extent to which an action is in line with core beliefs (i.e., strong, long-held beliefs about oneself and the world) may underpin the intention-action gap. Intentions that align with core beliefs&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/1099-0992%28200007/08%2930%3A4%3C533%3A%3AAID-EJSP6%3E3.0.CO%3B2-F\"><u>are more likely</u></a> to consistently translate into action compared to intentions that are motivated by other factors (e.g., the need to conform to group norms), which might lead to performative or temporary behaviors. In the environmental conservation literature, promoting core beliefs to protect the environment&nbsp;<a href=\"https://www.nature.com/articles/s41893-019-0263-9\"><u>has been shown</u></a> to increase the spillover between pro-environmental behaviors. Promoting core beliefs about the importance of the moral consideration of AIs is likely important to closing the intention-action gap so that AIs can benefit from the transfer of moral consideration.&nbsp;</p><p>&nbsp;</p><h1>What interventions can we use to induce the spillover of moral consideration?</h1><p>A common technique used in HRI research is to examine attitude change towards robots in general after interaction with a single AI, usually a robot. This technique builds on a rich literature of human intergroup contact interventions that have been shown to effectively promote the spillover of moral consideration and reduction of prejudice between humans&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5vpdgljr5xn\"><sup><a href=\"#fn5vpdgljr5xn\">[7]</a></sup></span>.</p><p>HRI research has shown that human-AI contact might facilitate the transfer of moral consideration in the context of AIs.&nbsp;<a href=\"https://ieeexplore.ieee.org/abstract/document/5598679\"><u>Stafford et al. (2010)</u></a> found that prosocial contact with a robot increased positive attitudes toward the robot and towards all robots.&nbsp;<a href=\"https://dl.acm.org/doi/abs/10.5555/3523760.3523851\"><u>More recently</u></a>, researchers showed that mutual self-disclosure (e.g., sharing personal life details) with a robot increased positive perceptions of all robots. Additionally, contact with a robot caregiver&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S138650562030304X\"><u>increased acceptance</u></a> of technology and AIs in general, and social interaction with a robot&nbsp;<a href=\"https://www.liebertpub.com/doi/10.1089/cyber.2020.0162\"><u>has been shown</u></a> to increase positive perceptions of robots, regardless of their features. Positive attitudes after contact with a human-like robot&nbsp;<a href=\"https://ieeexplore.ieee.org/abstract/document/7451847\"><u>have been shown</u></a> to spill over to non-human-like robots. In-person interactions with a robot are not required to produce the spillover of positive attitudes and behaviors.&nbsp;<a href=\"https://ieeexplore.ieee.org/abstract/document/6926300\"><u>Wullenkord and Eyssel (2014)</u></a> demonstrated that imagining a prosocial interaction with a robot leads to positive attitudes and willingness to interact with other robots.</p><p>Another possible intervention to promote the transfer of moral consideration is&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0272494419306218?via%3Dihub\"><u>changing underlying negative attitudes</u></a> towards AIs.&nbsp;<a href=\"https://link.springer.com/article/10.1007/s12369-016-0357-8\"><u>Some research</u></a> has shown that pre-existing negative attitudes towards AIs can persist even after a positive interaction with an AI, highlighting the significance of promoting positive attitudes towards AIs in order to facilitate the transfer of moral consideration. However, even if an intervention is successful in changing pre-existing attitudes towards AIs, the effective scope of this intervention might be limited to attitudes and intentions rather than behavior because of the intention-action gap. &nbsp; &nbsp; &nbsp;</p><p>A&nbsp;<a href=\"https://compass.onlinelibrary.wiley.com/journal/17519004\"><u>more effective intervention</u></a> might be to target core beliefs. As discussed previously, such beliefs are more likely to overcome the intention-action gap and translate into behavior.&nbsp;<a href=\"https://psycnet.apa.org/record/2000-00512-004\"><u>Sheeran and Orbell (2000)</u></a> showed that individuals for whom exercising was part of their self-identity were better at translating intentions to exercise into action than \u2018non-exercisers\u2019. In the context of AIs, promoting the self-perception of being someone who cares about the well-being of all sentient beings might make it more likely for the transfer of moral consideration to produce positive behaviors towards sentient AIs. Likewise, holding a core belief that the moral consideration of AIs is important might improve the likelihood that moral consideration will transfer onto AIs.</p><p>Some interventions might be less effective in facilitating the transfer of moral consideration. Guilt interventions&nbsp;<a href=\"https://www.nature.com/articles/s41893-019-0263-9\"><u>are ineffective</u></a> in producing spillover in the environmental domain, and may even backfire. Specifically, inducing guilt over failing to adopt a pro-environmental behavior decreased the likelihood of adopting other pro-environmental behaviors. Even though guilt increases initial intentions to perform a pro-environmental behavior, the&nbsp;<a href=\"https://www.tandfonline.com/doi/abs/10.1080/13504622.2016.1250148?journalCode=ceer20\"><u>feelings of guilt dissipate</u></a> after the first behavior has been performed and this undermines motivation to engage in similar future behaviors. There is currently no research on guilt interventions in the context of AIs, but the risk of backfiring seems high given these previous findings.&nbsp;</p><p>Even though interventions designed around contact, pre-existing attitudes, and core beliefs might be effective in inducing the transfer of moral consideration, to date there is no evidence for a long-term change in the moral consideration of AIs. So far, interventions have focused on short-term behavioral and contextual spillover. It is unknown whether these interventions have long-lasting effects on the moral consideration of AIs.&nbsp;</p><p>Another limitation of existing spillover research in the context of AIs is that the interventions conducted so far have been small-scale (i.e., small samples with limited types of AIs), often focused on non-moral&nbsp;purposes (e.g., user experience), and disconnected from each other. Research on possible interventions with larger samples, for the purpose of studying moral spillover, and to track long-term effects (e.g., how moral consideration might transfer from current AIs to future AIs), would provide more insight into how spillover effects might facilitate or hinder the inclusion of AIs in the moral circle.</p><h1><br>What future research is needed?</h1><p>Research on moral spillover towards AIs is in its infancy. More empirical evidence is needed to understand how moral consideration may or may not&nbsp;transfer from humans to AIs and from existing AIs to future AIs.&nbsp;</p><p>Future research should investigate how and when positive and negative attitudes and behaviors transfer to AIs, and the consequences this might have for their inclusion in the moral circle. Prosocial interactions\u2014<a href=\"https://ieeexplore.ieee.org/abstract/document/7451847\"><u>real</u></a> or&nbsp;<a href=\"https://ieeexplore.ieee.org/abstract/document/6926300\"><u>imagined</u></a>\u2014with individual robots have been shown to increase positive attitudes and behaviors towards similar and very different AIs. Developing this research in the moral domain with studies that examine precursors (e.g., pre-existing negative attitudes) to spillover could help us understand positive and negative attitude transfer and the potential backfiring effects of spillover interventions. This matters because interaction with AIs is likely to increase as they become more widespread in society. If&nbsp;positive interactions with existing AIs facilitate moral consideration for AIs in general, future AIs might have a better chance of being included in the moral circle.</p><p>How future AIs will appear, think, feel, and behave is uncertain. They are likely to have diverse mental capacities, goals, and appearances. Future AIs could range from highly human-like robots to non-embodied sentient algorithms or minuscule cell-like AIs. The mental capacities of AIs are likely to vary on a spectrum (e.g., from minimally sentient to more sentient than humans). How these diverse future AIs will be affected by moral spillover is unknown. Future research could examine whether there is a minimal threshold of similarity with humans that AIs must meet for moral consideration to transfer. Future studies could also examine whether the inclusion of even one kind of AI in the moral circle spills over to all AIs regardless of their features.&nbsp;</p><p>Furthermore, a neglected but important research direction is the examination of temporal spillover. A change in how AIs are treated in the present might shape the moral consideration of AIs in the future. One possible way of investigating temporal spillover would be to use longitudinal studies to examine how present attitudes and behaviors towards AIs affect attitudes and behaviors towards different future AIs. Further research on the effectiveness of interventions that change core beliefs towards AIs is likely also an important part of understanding temporal moral spillover.&nbsp;</p><p>Expanding the research on moral spillover in the context of AIs has the potential to identify boundaries that shape human-AI interaction and the moral consideration of present AIs. Future research is also likely to broaden our understanding of how the many diverse AIs of the future will be extended moral consideration. Facilitating the transfer of moral consideration to AIs may be&nbsp;critical&nbsp;to fostering a future society where the well-being of all sentient beings matters.&nbsp;</p><p><br><br>&nbsp;</p><p><br>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6bbbsu9upmf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6bbbsu9upmf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See&nbsp;<a href=\"https://www.oah.org/tah/issues/2015/november/the-history-of-animal-protection-in-the-united-states/\"><u>Davis (2015)</u></a> and&nbsp;<a href=\"https://faunalytics.org/the-animal-rights-movement-history-and-facts-about-animal-rights/\"><u>Orzechowski (2020)</u></a> for more on the histories of the animal rights and anti-slavery social movements.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnodpv2i71cpm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefodpv2i71cpm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;In humans and animals, sentience is usually defined as the capacity to have positive and negative experiences, such as happiness and suffering. However, we understand that sentience does not necessarily have to look the same in AIs. We outline how one might assess sentience in AIs in a&nbsp;<a href=\"https://www.sentienceinstitute.org/blog/assessing-sentience-in-artificial-entities\"><u>previous post</u></a>.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndj2hrzinmmo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdj2hrzinmmo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We take the view that the well-being of an entity&nbsp;<a href=\"https://en.wikipedia.org/wiki/Animal_Liberation_(book)\">is tied to</a> judgments of&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/moral-patienthood\">moral patiency</a>, as opposed to&nbsp;<a href=\"https://en.wikipedia.org/wiki/Moral_agency\">moral agency</a>. Whether an AI is able to discern right from wrong, how the AI acts or how the AI is programmed&nbsp;<a href=\"https://link.springer.com/article/10.1007/s10676-020-09540-4\">does not necessarily change</a> whether or not they should be considered a moral patient. That is, even if an AI who does not care about moral treatment is developed, we still ought to treat them morally on the basis that they are a moral patient.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmbmile48v4e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmbmile48v4e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Moral spillover can also occur for the opposite of moral consideration (e.g., actively wishing harm upon someone). Negative moral spillover has been referred to as&nbsp;<i>moral taint</i>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsfibj9kl31\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsfibj9kl31\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;From a psychological perspective, the term \u201c<a href=\"https://dictionary.apa.org/prosocial\"><u>prosocial</u></a>\u201d refers to a behavior that benefits one or more other beings (e.g., offering one\u2019s seat to an older person on a bus). The term \u201c<a href=\"https://dictionary.apa.org/moral\"><u>moral</u></a>\u201d refers to a behavior that is ethical or proper (e.g., right or wrong). These terms can overlap. A prosocial behavior can in some cases also be a moral behavior (and vice versa), insofar as both kinds of actions promote the interests of other beings and can be construed as right or wrong. Given that much of the moral spillover research in HRI is framed as the study of prosocial behavior, we use the terms \u201cmoral\u201d and \u201cprosocial\u201d interchangeably.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7pk99nyxa5v\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7pk99nyxa5v\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Temporal spillover could include a huge range of attitudes and behavior, such as simply having one attitude persist over time (e.g., after seeing a compelling fundraiser for a charity, you still feel compelled two weeks later). A narrower definition of spillover would exclude situations where moral consideration merely transfers to the same individual or group at a different time, rather than to different entities.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5vpdgljr5xn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5vpdgljr5xn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See&nbsp;<a href=\"https://spssi.onlinelibrary.wiley.com/doi/full/10.1111/josi.12419\"><u>Boin et al. (2021)</u></a> and&nbsp;<a href=\"https://www.annualreviews.org/doi/abs/10.1146/annurev-psych-071620-030619\"><u>Paluck et al. (2021)</u></a> for recent reviews of the effectiveness of contact interventions.</p></div></li></ol>", "user": {"username": "katerinam"}}, {"_id": "AcHTcofk2nik5e8cG", "title": "Uncertainty about the future does not imply that AGI will go well", "postedAt": "2023-06-05T15:02:04.589Z", "htmlBody": "<p>Subtitle: A partial defense of high-confidence AGI doom predictions.</p><h1>Introduction</h1><p>Consider these two kinds of accident scenarios:</p><ol><li>In&nbsp;a <strong>default-success</strong> scenario, accidents are rare. For example, modern aviation is very safe thanks to decades of engineering efforts and a safety culture (e.g. the widespread use of checklists). When something goes wrong, it is often due to multiple independent failures that combine to cause a disaster (e.g. bad weather + communication failures + pilot not following checklist correctly).</li><li>In a <strong>default-failure</strong> scenario, accidents are the norm. For example, when I write a program to do something I haven\u2019t done many times already, it usually fails the first time I try it. It then goes on to fail the second time and the third time as well. Here, failure on the first try is overdetermined\u2015even if I fix the first bug, the second bug is still, independently, enough to cause the program to crash. This is typical in software engineering, and it can take many iterations and tests to move into the default-success regime.</li></ol><p>See also:&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/conjunctive-vs-disjunctive-risk-models\">conjuctive vs disjunctive risk scenarios</a>.</p><p>Default-success scenarios include most engineering tasks that we have lots of experience with and know how to do well: building bridges, building skyscrapers, etc. Default-failure scenarios, as far as I can tell, come in two kinds: scenarios in which we\u2019re trying to do something for the first time (rocket test launches, prototypes, new technologies) and scenarios in which there is a competent adversary that is trying to break the system, as in computer security.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl17iqh6p4wm\"><sup><a href=\"#fnl17iqh6p4wm\">[1]</a></sup></span></p><h2>Predictions on AGI risk</h2><p>In&nbsp;the following, I use <strong>P(doom)</strong> to refer to the probability of an AGI takeover and / or human extinction due to the development of AGI.</p><p>I often encounter the following argument against predictions of AGI catastrophes:</p><p><strong>Alice:</strong> We seem to be on track to build an AGI smarter than humans. We don\u2019t know how to solve the technical problem of building an AGI we can control, or the political problem of convincing people to not build AGI. Every plausible scenario I\u2019ve ever thought or heard of leads to AGI takeover. In my estimate, P(doom) is [high number].</p><p><strong>Bob:</strong>&nbsp;I disagree. It\u2019s overconfident to estimate high P(doom). Humans are usually bad at predicting the future, especially when it comes to novel technologies like AGI. When you account for how uncertain your predictions are, your estimate should be at most [low number].\u201d</p><p>I'm being vague about the numbers because I've seen Bob's argument made in many different situations. In one recent conversation I witnessed, the Bob-Alice split was P(doom) 0.5% vs. ~10%, and in another discussion it was 10% vs. 90%.</p><p><strong>My main claim</strong> is that Alice and Bob don\u2019t actually disagree about how uncertain or hard to predict the future is\u2015instead, they disagree about to what degree AGI risk is default-success vs. default-failure. If AGI risk is (mostly) default-failure, then uncertainty is a reason for pessimism rather than optimism, and Alice is right to predict failure.</p><p>In this sense I think Bob is missing the point. Bob claims that Alice is not sufficiently uncertain about her AI predictions, or has not integrated her uncertainty into her estimate well enough. This is not necessarily true; it may just be that Alice\u2019s uncertainty about her reasoning doesn't make her much more optimistic.</p><p>Instead of trying to refute Alice from general principles, I think Bob should instead point to concrete reasons for optimism (for example, Bob could say \u201cfor reasons A, B, and C it is likely that we can coordinate on not building AGI for the next 40 years and solve alignment in the meantime\u201d).</p><h1>Uncertainty does not (necessarily) mean you should be more optimistic</h1><p>Many people are skeptical of the \u2018default-failure\u2019 frame, so I'll give a bit more color here by listing some reasons why I think Bob's argument is wrong / unproductive. I won\u2019t go into detail about why AGI risk specifically might be a default-failure scenario; you can find a summary of those arguments in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/vC6v2iTafkydBvnz7/agi-ruin-scenarios-are-likely-and-disjunctive\">Nate Soares\u2019 post on why AGI ruin is likely</a>.</p><ol><li>It\u2019s true that the future is often hard to predict; for example, experts often fail to predict technological developments. This is not a reason for optimism. It would be kind of weird if it was! Humans are generally bad at predicting the future, especially for technological progress, and this is bad news for AI safety.<ol><li>In particular: if all the AI researchers are uncertain about what will happen, that is a bad sign much in the same way that it would be a bad sign if none of your security engineers understood the system they are supposed to secure.</li><li>Analogy: if I\u2019m in charge of software security for a company, and my impression is that the system is almost certainly insecure, it is not a good argument to say \u201cwell you don\u2019t completely understand the system, so you might be wrong!\u201d \u2015 I may be wrong, but being wrong does not bode well for our security.</li></ol></li><li>To believe P(doom) is high, all you really need to be convinced of is that the default outcome for messing up superhuman AGI is human extinction, and that we\u2019re not prepared. Our understanding here is incomplete but still relatively good compared to details that are harder to predict, e.g. when exactly AGI will arrive or what early forms of AGI will look like.</li><li>It is not always wrong to make high-confidence disaster predictions. For example, people saying \u201ccovid will be a disaster with high (~90%) probability\u201d in February 2020 were predictably correct, even though covid was a very novel situation. There was a lot of uncertainty, and the people who predicted disaster usually got the details wrong like everyone else, but the overall picture was still correct because the details didn\u2019t matter much.</li><li>A confidence of 90% is not actually much harder to achieve than 10%, relative to the baseline extinction risk for a new technology which is close to 0%. An estimate of P(doom) = 30% already leans very heavily on your&nbsp;<a href=\"https://www.lesswrong.com/tag/inside-outside-view\">inside view</a> of the risks involved; you don\u2019t need to trust your reasoning all that much more to estimate 90% instead.</li><li>Put differently: there\u2019s no reason in particular why Bob's uncertainty argument should cap your confidence at ~80%, rather than 1% or 0.1%.<ol><li>(It seems totally reasonable to me for a first reaction to AI X-risk to be \u201cEh I don\u2019t know, it\u2019s an interesting idea and I\u2019ll think more on it, but it does seem pretty crazy; if I had to estimate P(doom) right now I would say ~0.1%, though I would prefer not to give a number at all.\u201d Followed, to be clear, by rapid updates in favor of high p(doom), though not necessarily 90%; I think 90% makes sense for people who have slammed their head against the difficulties involved, and noticed a pattern where the wall they\u2019re slamming their heads against is pretty hard and doesn\u2019t have visible weak spots; but otherwise you wouldn\u2019t necessarily be that pessimistic.)</li></ol></li><li>More generally: estimates around 90% aren\u2019t all that \u201cconfident\u201d. If you\u2019re well-calibrated, changing your mind about something that you estimate to be 90% likely is something that happens all the time. So P(X) = 90% means \u201cI expect X to happen, though I\u2019m happy to change my mind and in fact regularly do change my mind about claims like this\u201d.</li><li>It makes sense to be uncertain about your beliefs, and about whether you thought of all the relevant things (usually you didn\u2019t). Rather than be generically uncertain about everything, it\u2019s usually better to be uncertain about specific parts of your model.<ol><li>For example: I\u2019m uncertain about the behavior and capability profile of the first AI that surpasses humans in scientific research. This makes me more pessimistic about alignment relative to a baseline where I was certain, because any strategy that depends on specific assumptions about the capabilities of this AI is unlikely to work.</li><li>For a second example: I think there probably won\u2019t be any international ban or regulation on large training runs that lengthens timelines by &gt;10 years, but I\u2019m pretty uncertain. This makes me more optimistic relative to a baseline where I was certain governments would do nothing.</li></ol></li><li>Put differently: most of your uncertainty about beliefs should be part of your model, not some external thing that magically pushes all your beliefs towards 50% or 0% or 100%.</li></ol><h1>Some things I\u2019m not saying</h1><p><i>This part is me hedging my claims. Feel free to skip if that seems like a boring thing to read.</i></p><p>I don\u2019t personally estimate P(doom) above 90%.</p><p>I\u2019m also not saying there are no reasons to be optimistic. I\u2019m claiming that reasons for optimism should usually be concrete arguments about possible ways to avoid doom. For example, Paul Christiano argues for a somewhat lower than 90% P(doom)&nbsp;<a href=\"https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer\">here</a>, and I think the general shape of his argument makes sense, in contrast to Bob\u2019s above.</p><p>I do think there is a correct version of the argument that, if your model says P(outcome) = 0.99, model uncertainty will generally be a reason to update downwards. I think people already take that into account when stating high P(doom) estimates. Here\u2019s a sketch of a plausible reasoning (summarized and not my numbers, but I do have similar reasoning, and I don\u2019t think the numbers are crazy):</p><ol><li>Almost every time I imagine a concrete scenario for how AGI development might go, that leads to an outcome where humans go extinct.</li><li>I can imagine some ways in which things go well, but they seem pretty fanciful; for example a sudden international treaty that forbids large training runs and successfully enforces this. (I do expect there\u2019ll be other government efforts, but I don\u2019t expect those to change things much for the better). So my \u201cwithin-model\u201d prediction is p(doom) = 0.99.</li><li>My model is almost certainly wrong. Sadly, for most scenarios I can imagine, being wrong would only make things worse. I\u2019m literally a safety researcher; me being totally wrong about e.g. what the first AGI looks like is not a good sign for safety (and I don\u2019t expect other safety researchers to have better models). Almost all surprises are bad.<ol><li>Analogy: if I\u2019m in charge of software security for a company, and my impression is that the system is almost certainly insecure, it is not a good argument to say \u201cwell you don\u2019t completely understand the system, so you might be wrong!\u201d \u2015 I may be wrong, but being wrong does not bode well for our security.</li></ol></li><li>That said: while&nbsp;<i>technical</i> surprises are probably bad, there\u2019s other kinds of positive surprises we could get, for example: more progress on AI safety than expected, better interpretability methods, more uptake of AI risk concerns by the broader ML community, more government action on regulating AI.<ol><li>In fact, there are some kinds of cumulative surprises that could add up to save us; as an example, enough regulation of AI could lead to ~10y longer timelines; more progress than expected in interpretability could lead to more compelling demonstrations of misalignment; more uptake of AI risk by the broader scientific community might lead to more safety progress and an overall more careful approach to AGI.</li><li>Note that this is not an update made from pure uncertainty\u2015there is a concrete story here about how exactly surprises might actually be helpful, rather than bad. It\u2019s not a particularly great story either; it needs many things to go better than expected.</li></ol></li><li>Now, that particular story is not likely at all. But it seems like there are many stories in that general category, such that the total likelihood of a good surprise adds up to 10%.<ol><li>Note the basic expectation of \u2018surprises are often bad\u2019 still applies. Not knowing how governments or society will react to AI is hardly helpful for the people who are currently trying to get governments or society to react in a useful way.</li></ol></li><li>So my overall, all-things-considered p(doom) is 90%, mostly due to a kind of sketchy downwards-update due to model uncertainty, without which the estimate would be around 99%.</li><li>It\u2019s debatable how large the downwards update here should be\u2015it could reasonably be more or less than 10%, and it\u2019s plausible that we\u2019re in the kind of domain where small quantified probability updates aren\u2019t very useful at all.</li></ol><p>I don\u2019t mean to say that the reasoning here is the only reasonable version out there. It depends a lot on how likely you think various definitely-useful surprises are, like long timelines to AGI and slow progress after proto-AGI. But I do think it is wrong to call high P(doom) estimates overconfident without any further more detailed criticism.</p><p>Finally, I haven\u2019t given an explicit argument for AGI risk; there\u2019s a lot of that elsewhere.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl17iqh6p4wm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl17iqh6p4wm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note how AGI somehow manages to satisfy both of these criteria at once.</p></div></li></ol>", "user": {"username": "Lauro Langosco"}}, {"_id": "cCK8p9skpfKHr9Nte", "title": "Would a much-improved understanding of regime transitions have a net positive impact?", "postedAt": "2023-06-05T14:53:46.748Z", "htmlBody": "<p>Suppose that we could build some tools that accelerated the progress of certain parts of social science substantially. Suppose further that using those tools, humanity would gain a substantially better understanding of the conditions under which authoritarian regimes transition towards liberal democracy and vice versa. Would that improved understanding be a good thing?</p><p>Please assume for the purpose of this question that:</p><ul><li>by the nature of the tools to be developed, the knowledge gained would be accessible to the general public</li><li>liberal democracy is much better than authoritarian regimes</li></ul><p>I will post my own thoughts in a comment.</p><p>Thank you for your help!</p>", "user": {"username": "Michael Latowicki"}}, {"_id": "gDRH2SrN34KdDvmHE", "title": "Abolishing factory farming in Switzerland: Postmortem", "postedAt": "2023-06-05T14:14:25.782Z", "htmlBody": "<p>The <a href=\"https://factory-farming.ch/\">Initiative to abolish factory farming</a> was a nationwide ballot in Switzerland, instigated by <a href=\"https://sentience.ch/en/\">Sentience Politics.</a></p><p>The contents of the postmortem below were written by Philipp Ryf, then co-president at Sentience Politics and co-lead of the campaign, for the 2022 <a href=\"https://www.sentience.ch/wp-content/uploads/SNT_jahresbericht2022_WEB_EN.pdf\">annual report</a> of Sentience Politics. The author of this post is Naoki Peter, co-president at Sentience Politics.</p><h2>The initiative at a glance</h2><ul><li>The initiative demanded the abolition of factory farming in Switzerland, granting a maximum transitional period of 25 years.</li><li>It aimed at anchoring stricter animal welfare guidelines as a new minimum standard in the Swiss constitution. These standards would have granted cows, pigs and chickens regular access to the outdoors and considerably more space.</li><li>On behalf of Swiss farmers the initiative included import regulations that take account of the new Swiss standards.</li><li>The central point of contention in the public debate was whether Switzerland's existing animal welfare law was sufficiently strict.</li><li>&nbsp;The initiative was rejected by a 62.9% majority of the voters in September 2022.</li></ul><p>For more information see the <a href=\"https://factory-farming.ch/initiative/\">initiative text</a> (in German, French and Italian) and the <a href=\"https://www.admin.ch/gov/de/start/dokumentation/abstimmungen/20220925/massentierhaltungsinitiative.html\">ballot results</a>.</p><h2>Postmortem</h2><ul><li>The living conditions of animals in agriculture have never been discussed so widely and publicly. Hundreds of thousands of people have engaged with the initiative beyond their usual scope, asking themselves the question: \"What does my consumption mean for animals, people, and the environment?\" The voting result has proven that the vision of a dignified, location-appropriate agriculture mobilises and moves people far beyond the base of the supporting parties.</li><li>For the first time a broad alliance of Swiss animal protection, agricultural, and environmental organisations joined forces and stood up to the agricultural lobby to advocate for an animal welfare cause.</li><li>Considering the high cost and resources required to launch an initiative, we aimed at including as many demands as possible. However, we now question whether a shorter, more targeted package of demands would have garnered more support. We will consider this for any future initiatives.&nbsp;</li><li>In 2018 Sentience Politics launched the Initiative to abolish factory farming single-handedly. We should have prioritised alliance-building earlier. This would have enabled us to tap into the resources of more organisations.&nbsp;</li><li>The opposition commanded resources which significantly exceeded our own. In order to have a realistic chance at the ballot box, any future initiative would likely require a substantially larger budget and campaign team.</li><li>In order to inspire the majority of the population for a cause, it is essential to highlight the urgency of the matter and exclude any room for doubt. We could have done this better. For future campaigns, we must ensure that the actual conditions in animal farming are made transparent sooner. Possibilities include a greater focus on highlighting scandals, publishing our own investigations, and doing more to attract media exposure.&nbsp;</li><li>Regional volunteer networks are very challenging to build. Here, we could have involved organisations that already have structures in place to manage regional leafleting, stand campaigns and billboards more effectively and at an earlier stage.&nbsp;</li><li>We built up a large network of influential personalities from society. However, we lacked credible ambassadors from agriculture who could more readily be trusted as experts in their field until a late stage in the campaign. The clearest take-home message from the follow-up surveys was that policy change in agriculture does not work without the producers. In the future, we need to involve stakeholders within agriculture in our campaigns at an earlier stage.</li></ul><h2>Alliance against factory farming</h2><p>The initiative to abolish factory farming was not successful at the ballot box in September 2022, but many questions about our nutrition and Swiss agriculture remain unresolved and highly topical. This is where the planned \u201cAlliance against factory farming\u201d comes in. The Alliance aims to ensure that the coordination and flow of information achieved between our <a href=\"https://factory-farming.ch/supporters/#ultimate-heading-3129647d8ad1eee08\">initiative partners</a> are maintained in the long term and that future political projects have as broad a foundation as possible through early visibility. The lessons learned from the ballot initiative show that political change is only possible through broad-based stakeholder representation. It is important that the animal-focused political forces in Switzerland work more closely together. Even if the \u201cend goal\u201d is not the same in all cases, the path to that goal is mostly compatible.</p>", "user": {"username": "naoki"}}, {"_id": "ncpHRjzcA9quHbuyQ", "title": "Two Big-Picture Critiques of GiveWell's Approach, and Six Lessons from Their Recent Work (Elie Hassenfeld on The 80,000 Hours Podcast)", "postedAt": "2023-06-05T13:51:24.656Z", "htmlBody": "<p><br>Over at <a href=\"https://80000hours.org/podcast/\">The 80,000 Hours Podcast</a> we just published an interview that is likely to be of particular interest to people who identify as involved in the effective altruism community: <a href=\"https://80000hours.org/podcast/episodes/elie-hassenfeld-givewell-critiques-and-lessons/\"><strong>Elie Hassenfeld on Two Big-Picture Critiques of GiveWell's Approach, and Six Lessons from Their Recent Work</strong></a><strong>.</strong></p><p>You can click through for the audio, a full transcript, and related links. Below is the episode summary and some key excerpts.</p><h2><strong>Episode summary</strong></h2><figure class=\"table\"><table><tbody><tr><td><p><i>It strikes me that there\u2019s more of a risk of doing harm here, by assuming that we do have the answer and pushing economic policy in a certain direction. There\u2019s just a lot of opportunity for unintended consequences of pushing countries to do things that are different even if we knew how to do it.</i></p><p><i>All that said, I do think the critique still stands, because ideal GiveWell would have said, \u201cWe spent a year on this because it\u2019s an important idea\u201d.</i></p><p><i>- Elie Hassenfeld</i></p></td></tr></tbody></table></figure><p><a href=\"https://www.givewell.org/\">GiveWell</a> is one of the world\u2019s best-known charity evaluators, with the goal of \u201csearching for the charities that save or improve lives the most per dollar.\u201d It mostly recommends projects that help the world\u2019s poorest people avoid easily prevented diseases, like intestinal worms or vitamin A deficiency.</p><p>But should GiveWell, as some critics argue, take a totally different approach to its search, focusing instead on directly increasing subjective well-being, or alternatively, raising economic growth?</p><p>Today\u2019s guest \u2014 cofounder and CEO of GiveWell, Elie Hassenfeld \u2014 is proud of how much GiveWell has grown in the last five years. Its \u2018money moved\u2019 has quadrupled to around $600 million a year.</p><p>Its research team has also more than doubled, enabling them to investigate a far broader range of interventions that could plausibly help people an enormous amount for each dollar spent. That work has led GiveWell to support dozens of new organisations, such as <a href=\"https://forum.effectivealtruism.org/posts/8CRFTxrRLiizypyBu/r-i-c-e-s-neonatal-lifesaving-partnership-is-funded-by\">Kangaroo Mother Care</a>, <a href=\"https://www.miraclefeet.org/\">MiracleFeet</a>, and <a href=\"https://www.givewell.org/research/grants/evidence-action-dispensers-for-safe-water-January-2022\">Dispensers for Safe Water</a>.</p><p>But some other researchers focused on figuring out the best ways to help the world\u2019s poorest people say GiveWell shouldn\u2019t just do more of the same thing, but rather ought to look at the problem differently.</p><p>Currently, GiveWell uses a range of metrics to track the impact of the organisations it considers recommending \u2014 such as \u2018lives saved,\u2019 \u2018household incomes doubled,\u2019 and for health improvements, the \u2018quality-adjusted life year.\u2019 To compare across opportunities, it then needs some way of weighing these different types of benefits up against one another. This requires estimating so-called \u201cmoral weights,\u201d which Elie agrees is far from the most mature part of the project.</p><p>The Happier Lives Institute (HLI) <a href=\"https://www.happierlivesinstitute.org/key-ideas/\">has argued</a> that instead, GiveWell should try to cash out the impact of all interventions in terms of improvements in subjective wellbeing. According to HLI, it\u2019s improvements in wellbeing and reductions in suffering that are the true ultimate goal of all projects, and if you quantify everyone on this same scale, using some measure like the <a href=\"https://www.happierlivesinstitute.org/report/wellby/\">wellbeing-adjusted life year</a> (WELLBY), you have an easier time comparing them.</p><p>This philosophy has led HLI to be more sceptical of interventions that have been demonstrated to improve health, but whose impact on wellbeing has not been measured, and to give a high priority to improving lives relative to extending them.</p><p>An <a href=\"https://forum.effectivealtruism.org/posts/bsE5t6qhGC65fEpzN/growth-and-the-case-against-randomista-development\">alternative high-level critique</a> is that really all that matters in the long run is getting the economies of poor countries to grow. According to this line of argument, hundreds of millions fewer people live in poverty in China today than 50 years ago, but is that because of the delivery of basic health treatments? Maybe <a href=\"https://en.wikipedia.org/wiki/Barefoot_doctor\">a little</a>), but mostly not.</p><p>Rather, it\u2019s because changes in economic policy and governance in China allowed it to experience a 10% rate of economic growth for several decades. That led to much higher individual incomes and meant the country could easily afford all the basic health treatments GiveWell might otherwise want to fund, and much more besides.</p><p>On this view, GiveWell should focus on figuring out what causes some countries to experience explosive economic growth while others fail to, or even go backwards. Even modest improvements in the chances of such a \u2018growth miracle\u2019 will likely offer a bigger bang-for-buck than funding the incremental delivery of deworming tablets or vitamin A supplements, or anything else.</p><p>Elie sees where both of these critiques are coming from, and notes that they\u2019ve influenced GiveWell\u2019s work in some ways. But as he explains, he thinks they underestimate the practical difficulty of successfully pulling off either approach and finding better opportunities than what GiveWell funds today.</p><p>In today\u2019s in-depth conversation, Elie and host Rob Wiblin cover the above, as well as:</p><ul><li>The research that caused GiveWell to flip from not recommending chlorine dispensers as an intervention for safe drinking water to spending tens of millions of dollars on them.</li><li>What transferable lessons GiveWell learned from investigating different kinds of interventions, like providing medical expertise to hospitals in very poor countries to help them improve their practices.</li><li>Why the best treatment for premature babies in low-resource settings may involve less rather than more medicine.</li><li>The high prevalence of severe malnourishment among children and what can be done about it.</li><li>How to deal with hidden and non-obvious costs of a programme, like taking up a hospital room that might otherwise have been used for something else.</li><li>Some cheap early treatments that can prevent kids from developing lifelong disabilities, which GiveWell funds.</li><li>The various roles GiveWell is currently hiring for, and what\u2019s distinctive about their organisational culture.</li></ul><p>Get this episode by subscribing to our podcast on the world\u2019s most pressing problems and how to solve them: type \u201880,000 Hours\u2019 into your podcasting app. Or read the transcript below.</p><p><i>Producer: Keiran Harris</i><br><i>Audio mastering: Simon Monsour and Ben Cordell</i><br><i>Transcriptions: Katy Moore</i></p><h2><strong>Highlights</strong></h2><h3><strong>The subjective wellbeing approach in contrast with GiveWell's approach</strong></h3><blockquote><p><strong>Elie Hassenfeld:</strong> First I think it would be helpful for me to just explain what GiveWell is doing today, which is we cash everything out either in terms of increased ability to consume (i.e. people have more money) or reductions in disability-adjusted life years \u2014 some of which are health-related and some are mortality-related.</p><p>But I very much take the point that subjective wellbeing is an important consideration. We don\u2019t view the two outcomes we use today as the only outcomes that make sense. They\u2019re just the two outcomes that we\u2019ve been able to use to date. I do think over time, as we continue to grow and increase the size of our team, we\u2019ll be in a position to include more factors explicitly in that analysis.</p><p>I think the pro of subjective wellbeing measures is that it\u2019s one more angle to use to look at the effectiveness of a programme. It seems to me it\u2019s an important one, and I would like us to take it into consideration.</p><p>I think the downside, or the reasons not to, might be that on one level, I think it can just be harder to measure. A death is very straightforward: we know what has happened. And the measures of subjective wellbeing are squishier in ways that it makes it harder to really know what it is. Also, I think some people might say, \u201cI really value reducing suffering and therefore I choose subjective wellbeing.\u201d I think other people might say, \u201cI think these measures are telling me something that is not part of my \u2018view of the good,\u2019 and I don\u2019t want to support that.\u201d That would cause someone to want to leave it out of their calculus and the donations they\u2019re making.</p><p>In some ideal world, I would love for GiveWell to be able to offer options for donors who have different philosophical perspectives about what they want to achieve. Obviously, GiveWell institutionally also needs to have a view, because there\u2019s funds that come to us directly. But ideally, in the future vision of GiveWell, for people who have subjective wellbeing as their core focus, other moral values, or maybe even a very different tradeoff between increasing income and reducing disability-adjusted life years (or increasing DALYs, maybe, depending on how you think about it), those are programmes we\u2019d like to be able to bring to donors and let them choose.</p><p>Because we\u2019re not trying to add value by being particularly good philosophically. That\u2019s not part of GiveWell\u2019s comparative advantage. It would be better if we could, where donors want it, allow them to use their own judgements to make decisions.</p></blockquote><h3><strong>The value of saving a life when that life is going to be very difficult</strong></h3><blockquote><p><strong>Rob Wiblin:</strong> I think to most people, it\u2019s intuitive that it\u2019s more valuable to save the life of someone who feels that they\u2019re really flourishing and is super glad to be alive than it is to save the life of someone who thinks their life is barely worth living, who maybe doesn\u2019t even care that much whether they live or die.</p><p>It could be useful to use some numbers to make it a bit clearer how this might end up affecting your relative priorities here. If you imagine people scoring their quality of life out of 10, that\u2019s kind of the standard subjective wellbeing scale. Let\u2019s say that we use the number 3 as the number at which someone is rating their existence as neutral, with the good and bad things in their life cancelling out: that\u2019s kind of a typical answer for what people say would be the neutral point for them if they were scoring themselves.</p><p>If someone is going to report a quality of life of 4 out of 10 for the rest of their lives, then from a wellbeing-adjusted life year, a WELLBY, point of view, then it\u2019s equally valuable to them to prevent them from dying as it is to increase their wellbeing permanently by one point out of 10. That would be from 4 to 5 in this case. On the other hand, if someone reports a quality of life of 5 out of 10, then from a WELLBY point of view, it\u2019s twice as valuable to save their life as to increase their wellbeing permanently by one point \u2014 in this case from 5 to 6 \u2014 because the difference from 3 to 5 is twice as great as from 5 to 6.</p><p>The Happier Lives Institute notes that many people in very poor countries \u2014 who otherwise might die of malaria in the absence of additional antimalarial bednets \u2014 have unsurprisingly pretty challenging lives with plenty of hardship in them. That, as I understand it, suggests that to them it\u2019s more likely to be cost effective to make people\u2019s lives better than to make them longer or less equal.</p><p>What do you and GiveWell make of that line of argument?</p><p><strong>Elie Hassenfeld:</strong> I think the place I want to start is this is a case where I feel most strongly that I would want to hear from the people themselves in low-income countries about this topic. Because if you kind of draw out this line of reasoning, it leads you to the conclusion that there is a very high proportion of people living in low-income countries who would choose death over continued living, based on their self-reported life satisfaction.</p><p>That\u2019s a very uncomfortable conclusion, but maybe more importantly, one that is so counterintuitive that I feel the need to follow up on it before accepting it at face value. That may be a somewhat minor point about where you draw the line on the scale, but still, in this case, I think the maybe purely emotional urge I have is to say that doesn\u2019t quite seem like it could be right. Intellectually, I know it could be right \u2014 therefore I need to follow up on it, because it\u2019s so inconsistent with my starting point for what people would say.</p><p><strong>Rob Wiblin:</strong> Yeah, it definitely can get uncomfortable or weird. Or maybe if you were surveying people on their subjective wellbeing, and you really said, \u201cIf you score yourself a 2, we\u2019re going to take it that you actually mean that you would rather not be alive right now,\u201d then maybe people would reassess. Because an interesting thing is that when you survey people, almost everywhere in the world, even people in serious poverty almost always say that they think their life is better than not existing, and they want to continue surviving and so on.</p><p>I\u2019ve heard some philosophers say that that kind of intuition that we all have about how great it is to continue existing might be suspicious, because we might have evolved to have that attitude. We necessarily almost have to evolve to have that attitude, even if our lives are very unpleasant. That kind of bias might affect all of us. But I\u2019m not really too keen to go there, and I feel extremely uncomfortable. If someone says that saving their life is really valuable, I\u2019m inclined to take that at face value and to trust that over some subjective wellbeing survey.</p><p><strong>Elie Hassenfeld:</strong> Right. I think that discomfort is a good starting point, though not an ending point. Certainly something that we are very committed to internally \u2014 one of our company values or whatever you want to call it \u2014 is <a href=\"https://www.givewell.org/about/values\">truth-seeking</a>. What we mean by that is we\u2019re going to have the hard conversations, and keep digging to try to get the answer that is correct, as far as we can see it. Therefore, in this case, I would say I am very suspicious of philosophising and reaching a conclusion that seems extremely counterintuitive and then running with it. But we\u2019re a place that wants to go deeper and be open to strange conclusions. Or maybe I should say it differently, like: conclusions that seem strange to us today that will not seem strange to us in the future once we\u2019ve spent more time with them and done more research on them.</p></blockquote><h3><strong>Whether economic policy is what really matters overwhelmingly</strong></h3><blockquote><p><strong>Elie Hassenfeld:</strong> I think I want to start with the parts of the critique that I take on board, and what I think we would ideally be doing differently, but then move into the critiques of the critique that I have and where I think it maybe is overstating its case.</p><p>The part of this critique that I really like, and I\u2019ve been thinking about recently, is that I don\u2019t think that we at GiveWell have put enough time into finding ways to explore the space of possibilities in this area, given its potential importance. I think that is something that I don\u2019t regret historically \u2014 I\u2019ll tell you why \u2014 but I do think going forward, as we\u2019ve grown and as we continue to grow, I\u2019d like to be in a position where we\u2019ve explored this enough to have a really great answer, which either is we\u2019re doing this in this area or we\u2019re not, because of this pretty compelling reason.</p><p>I think one of the things that explains GiveWell\u2019s history, largely, is that GiveWell did something very unique by going very deep on charitable interventions and understanding them very well. A lot of how we\u2019ve grown is by sticking to that core pretty intensively over a long period of time, while we expand out in many of the ways that we\u2019ve talked about today. I think in some ways that is our greatest institutional strength and maybe our greatest institutional weakness. We\u2019ve been very focused on maintaining quality and rigour, and that has been very hard as we\u2019ve grown a lot. I think we\u2019ve been successful at it, and also it has made us more deliberate in the approach that we take to things \u2014 and I think that\u2019s a fair characterisation of GiveWell.</p><p>So when there have been ideas that are more outside of our bailiwick, I think we\u2019ve been just less effective at engaging with them. Just looking at the trajectory we\u2019ve been on in the last three years and how we\u2019ve expanded, when I look out five more years with our growth, I think we will be in a much better position to be engaging more seriously with these ideas. Maybe that\u2019s the institutional critique and what I think we could do differently. But I\u2019m happy to move on and engage more substantively with the ideas.</p><p>\u2014-</p><p><strong>Rob Wiblin:</strong> Is it maybe the case that there\u2019s just fewer organisations who perceive this is their goal, this is their direct mission in the developing world, relative to how many health-related organisations there are?</p><p><strong>Elie Hassenfeld:</strong> Maybe, but I think it\u2019s also a question of how you would attack this philanthropically \u2014 like I also wonder how neglected this space truly is. There\u2019s the World Bank, IMF, there\u2019s other institutions. There are the Washington think tanks that are definitely focused on economic growth, and academics who focus on macroeconomics and how we can improve low income country conditions.</p></blockquote><h3><strong>Dispensers for Safe Water</strong></h3><blockquote><p><strong>Elie Hassenfeld:</strong> So in many parts of low-income countries, people don\u2019t have access to clean water, and drinking unclean water can lead to diarrhoeal disease, which most importantly leads to death among children under five. This intervention puts a small chlorine dispenser near a water point, so that when someone comes to collect water from a spring, a pipe, they quickly push down on the chlorine dispenser into the jerry can that holds the water. That puts chlorine into the storage container that they then carry home.</p><p>This intervention is potentially much more effective than other attempts at chlorination in the past, because the individual collecting water only needs to remember to put chlorine in their container one time, right at collection. Also, the chlorine remains effective while the water is in the container once they bring it back home. So it reduces the need, say, for an alternative programme, which would require someone to go to the store to purchase chlorine tablets, or get them from a nonprofit, have them at home, and then use them each time they choose to consume water. And that easier behavioural intervention makes it more effective.</p><p><strong>Rob Wiblin:</strong> Yeah. I\u2019m not sure how much I would sanitise my water if I had to stick something in it every time I poured a glass of water. Sounds super annoying.</p><p>What\u2019s the prima facie or conceptual, high-level case for why this wouldn\u2019t just be good, but it could be amazing and one of the best things for you to fund?</p><p><strong>Elie Hassenfeld:</strong> It\u2019s that unclean water leads to a great deal of mortality in low-income countries. Having a diarrhoeal disease not only can lead directly to death, but can exacerbate malnutrition, which itself is a risk factor for death from other infectious diseases. So it\u2019s a major problem.</p><p>Then, this is a <i>very</i> low technology type of intervention. It\u2019s very simple. I\u2019ve used it \u2014 I\u2019ve visited this in Kenya, and it just requires pushing down on this thing to deliver chlorine. So it\u2019s easy to implement and easy to monitor and follow up on; it\u2019s easy to check. You put that all together, and it\u2019s a fairly low-cost programme that has a direct effect on a major public health problem globally.</p><p><strong>Rob Wiblin:</strong> Yeah. Just so I can picture it in my head, people are getting a big bottle of water from a well, or from a common tap, and then they have to stick a little chlorine tablet in it? Or is it kind of a chlorine spray that they stick in the bottle once they\u2019re done?</p><p><strong>Elie Hassenfeld:</strong> Basically, imagine that the person is carrying a jerry can. This is often a yellow, several-gallon container. They\u2019re bringing that container up to maybe a pipe, or even without a pipe, just a spring that water is flowing from. Right next to that water point, there\u2019s a stand that\u2019s maybe two to three feet high with a little plastic container that holds liquid chlorine. You press down on the pump one time, almost like a soap pump that you\u2019d find in a public bathroom, and out of that pump comes the appropriate amount of chlorine for that jerry can. So it\u2019s just dispensing it directly into the water container.</p></blockquote><h3><strong>How to avoid attributing deaths incorrectly</strong></h3><blockquote><p><strong>Rob Wiblin:</strong> Could these issues be quite widespread in investigations that you and other groups do of other interventions and other programmes? So each individual study of whatever other interventions finds no effect, but then if you added them all together, you\u2019d find that there was a large effect? Or maybe if they focus on malaria, they look at deaths from malaria \u2014 but in fact it\u2019s had a much larger effect on mortality than what\u2019s apparent, because people are attributing deaths incorrectly. Did you worry about that?</p><p><strong>Elie Hassenfeld:</strong> It\u2019s definitely something that we\u2019re very focused on. For a long time, when we\u2019ve looked at malaria data, we\u2019ve focused when we can on all-cause mortality. And of the randomised trials that were done on malaria nets, historically a large number were on malaria rates, but a number were on all-cause mortality as a whole, because of this reason exactly.</p><p>One of the lessons we took away from this is that, years ago, when we first did this analysis, we insufficiently brought our conclusions to experts outside. I think, had we done that, it\u2019s possible that they would have raised this question in 2019 and we would have more quickly updated because we would have realised that we were too narrowly assessing the impact of the intervention. And that is a change we\u2019ve made with some of the other programmes (which I think we\u2019ll talk about in a minute): we\u2019ve taken our estimates to outsiders and they\u2019ve helped us see a broader picture of what they might be doing so we can home in on the best possible estimate we can.</p><p><strong>Rob Wiblin:</strong> If I recall, another thing that Kremer did (who was the person who did this early aggregation of all of these different studies, and tipped you off that maybe you would want to take another look at this): he had access to a bunch of data that wasn\u2019t entirely public, or maybe some studies that hadn\u2019t come out yet that allowed him to get a larger sample and notice this. Is that a common problem? That studies get done and either the data is not published yet for a long time, or perhaps you don\u2019t have access to the specific numbers that you need from that study in order to aggregate it to get a clearer picture?</p><p><strong>Elie Hassenfeld:</strong> I think it is a pretty common issue. And in many cases, when we go deeper on analysis, we\u2019re doing that via reaching out to authors and getting the underlying data itself, so we can understand what\u2019s happening. We\u2019ve done that a number of times historically. Mostly we\u2019re relying on publicly available data because the time costs involved in trying to track down that data and get more of it are high, relative to just relying on the data that\u2019s already out there.</p></blockquote><h3><strong>Bridging the gap between abstract arguments and ways to actually move forward</strong></h3><blockquote><p><strong>Rob Wiblin:</strong> I think [some listeners might] say, at least in the cases of countries going backwards massively, we know things that countries <i>shouldn\u2019t</i> do that quite consistently lead them to have economic disasters \u2014 like causing hyperinflation is one of them. They might say, even if we don\u2019t know what the very best policy is, we at least know some things that are clearly bad, and maybe more effort should be put into preventing those, given how catastrophic they are. Do you want to react to that one?</p><p><strong>Elie Hassenfeld:</strong> Off the cuff, they also seem like the countries that are hardest to influence. If it\u2019s so well known, then why are they doing it? Well, they\u2019re probably doing it because leadership in the country does not have their population\u2019s best interests in mind. That seems like quite a challenge for philanthropy to address.</p><p><strong>Rob Wiblin:</strong> Yeah, I think that\u2019s probably my biggest concern with this line of argument, which in general I\u2019m quite sympathetic to. Like you, I think there\u2019s a lot to it, but I feel that often it\u2019s not appreciating that there\u2019s reasons that countries have bad policy. Very often it\u2019s not merely just a mistake; it\u2019s because of the political settlement within a country and who has power. And coming in and telling people that they could be richer if they change their policy one way or another \u2014 the elites often don\u2019t want to implement those policies because they think it would weaken their position one way or another, or at least they\u2019re not suffering from the poverty. There\u2019s this whole other angle of political economy, trying to understand how countries end up with the policies that they do, given how the political system works.</p><p><strong>Elie Hassenfeld:</strong> That\u2019s why I think ultimately, where I think GiveWell has something to add to this conversation \u2014 many of the conversations we\u2019ve had \u2014 is to say that we can look at it from the 10,000-foot view or the 50,000-foot view. That\u2019s important because it can help us decide where to put our resources. It\u2019s hard to figure out what\u2019s true from such a high level.</p><p>I think to some extent what makes me really excited about our work, why I think it\u2019s really cool, is that we\u2019re trying to be good about thinking at the 50,000-foot level, but then dig all the way in and ask: What can we do in this case about this problem? When I think about this specifically, I have absolutely no idea what to give money to to improve economic growth in country A, B, or C. But I can imagine a proximate step of finding people to spend time on this for a while and see what they come back with.</p><p>Having watched a lot of different types of programmes over many years \u2014 from GiveWell, from Open Philanthropy \u2014 more research often leads to new ideas. And so we\u2019re excited, I\u2019m excited, about our opportunity to support work like that, because it can bridge this gap between very abstract arguments \u2014 where there\u2019s good arguments on both sides \u2014 to find opportunities to actually move things forward.</p></blockquote>", "user": {"username": "80000_Hours"}}, {"_id": "W93Pt7xch7eyrkZ7f", "title": "Cause area report: Antimicrobial Resistance", "postedAt": "2023-06-06T08:35:22.207Z", "htmlBody": "<p><i>This post is a summary of some of my work as a field strategy consultant at </i><a href=\"https://www.schmidtfutures.com/\"><i>Schmidt Futures</i></a><i>' Act 2 program, where I spoke with over a hundred experts and did a deep dive into antimicrobial resistance to find impactful investment opportunities within the cause area. The full report can be accessed&nbsp;</i><a href=\"https://drive.google.com/file/d/1wiY4w0QADOZzc8-ac9hbXr9_xcDlTi8k/view?usp=share_link\"><i><u>here</u></i></a><i>.</i></p><h2>AMR is a global health priority</h2><p>Antimicrobials, the medicines we use to fight&nbsp; infections, have played a foundational role in improving the length and quality of human life since penicillin and other antimicrobials were first developed in the early and mid 20th century.</p><p>Antimicrobial resistance, or AMR, occurs when bacteria, viruses, fungi, and parasites evolve resistance to antimicrobials. As a result, antimicrobial medicine such as antibiotics and antifungals become ineffective and unable to fight infections in the body.&nbsp;</p><p>AMR is responsible for millions of deaths each year, more than HIV or malaria (<a href=\"https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(21)02724-0/fulltext\"><u>ARC 2022</u></a>). The&nbsp;<a href=\"https://www.tropicalmedicine.ox.ac.uk/gram/research/antimicrobial-resistance-visualization-tool\"><u>AMR Visualisation Too</u></a>l, produced by Oxford University and IHME, visualises&nbsp;<a href=\"https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(21)02724-0/fulltext\"><u>IHME data</u></a> which finds that 1.27 million deaths per year are attributable to bacterial resistance and 4.95 million deaths per year are associated with bacterial resistance, as shown below.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/am28ofco2rw6za6jwejp\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/vkedk82xq1waqz1iakcw 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/yvni4pbolqypxhygtkxu 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/wgvsq513j3paulbmh04f 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/qg3wdsjlu948gyeo54tg 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/n1heyhddyz0rg59zcvzh 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/p3zbypnfszlaxn21tukd 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/loewpf6pjrxrk2dm3jil 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/mfza4fdzdptnfntovwm8 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/lhlr3to2jcvqcxqelqff 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/uhuem0lozxp4yulziaw9 1420w\"></p><p><i>Figure 1: Composition of global bacterial infection related deaths, from&nbsp;</i><a href=\"https://www.tropicalmedicine.ox.ac.uk/gram/research/antimicrobial-resistance-visualization-tool\"><i><u>AMR Visualisation Tool</u></i></a></p><p><br>This burden does not include that of non-bacterial infections, such as fungi or pathogens, which might increase this burden by several factors. For instance, every year, there are 150 million cases of severe fungal infections, which result in 1.7 million deaths annually (<a href=\"https://microbialcell.com/researcharticles/2020a-kainz-microbial-cell/\"><u>Kainz et al 2020</u></a>). Unlike for bacterial infections, we do not have good estimates of how many of those are associated or attributable to resistance.</p><p>Concerningly, AMR is&nbsp; escalating at an increasing rate (<a href=\"https://diagnosticsfirst.com/us/track/165-rate-of-antibiotic-resistant-infections-has-doubled-in-the-u-s\"><u>USA data,</u></a>&nbsp;<a href=\"https://www.bag.admin.ch/bag/en/home/krankheiten/infektionskrankheiten-bekaempfen/antibiotikaresistenzen/wie-entwickelt-sich-die-antibiotikaresistenzlage---.html\"><u>Swiss data</u></a>,&nbsp;<a href=\"https://bmcinfectdis.biomedcentral.com/articles/10.1186/s12879-019-4295-6\"><u>Mhondoro et al 2019</u></a>,&nbsp;<a href=\"https://main.icmr.nic.in/sites/default/files/upload_documents/AMR_Annual_Report_2021.pdf\"><u>Indian Council of Medical Research 2021</u></a>). One prominent report estimates that&nbsp;AMR will result in 10 million deaths every year by 2050&nbsp;(<a href=\"https://amr-review.org/sites/default/files/AMR%20Review%20Paper%20-%20Tackling%20a%20crisis%20for%20the%20health%20and%20wealth%20of%20nations_1.pdf\"><u>Jim O\u2019Neill report, 201</u></a>4).</p><p>Even more concerningly, we may be at a critical juncture, where if we do not drastically change our current trajectory, we could run out of effective antimicrobials. This would mean that our ability to&nbsp;<strong>perform surgery, give cancer patients chemotherapy, or manage chronic diseases like cystic fibrosis and asthma, all of which hinge on the effectiveness of antimicrobials, would be significantly impacted</strong>. The very foundations of modern medicine could be threatened;&nbsp;the WHO has warned that we could return to a pre-antibiotic age, which would result in the average human life expectancy going down from 70 to 50 (<a href=\"https://www.who.int/southeastasia/news/detail/09-09-2015-urgent-action-needed-to-prevent-a-return-to-pre-antibiotic-era-who\"><u>WHO, 2015</u></a>).</p><p>Beyond the health effects, there is a profound economic cost to AMR \u2013 for patients, healthcare systems and the economy. In the USA, the CDC estimates that the cost of AMR is $55 billion every year (<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6929930/\"><u>Dadgostar 2019</u></a>). Studies show that as a result of AMR,&nbsp; the annual global GDP could decrease by 1% and there would be a 5-7% loss in low and middle income countries by 2050 (<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6929930/\"><u>Dadgostar 2019</u></a>). In conjunction, the&nbsp;<a href=\"https://www.worldbank.org/en/topic/health/publication/drug-resistant-infections-a-threat-to-our-economic-future\"><u>World Bank</u></a> states that AMR might limit gains in poverty reduction, push more people into extreme poverty and have significant labour market effects.</p><p>The importance of AMR is recognised by major governments and multilateral organisations. The WHO calls AMR one of the greatest public health threats facing humanity, the UK government lists AMR on its&nbsp;<a href=\"https://unfoundation.org/blog/post/antimicrobial-resistance-is-the-silent-pandemic-we-can-no-longer-neglect/\"><u>National Risk Register</u></a>, and both&nbsp;<a href=\"https://www.gavi.org/vaccineswork/silent-pandemic-how-drug-resistant-superbugs-risk-becoming-worlds-number-one-killer\"><u>GAVI</u></a> and the&nbsp;<a href=\"https://unfoundation.org/blog/post/antimicrobial-resistance-is-the-silent-pandemic-we-can-no-longer-neglect/\"><u>United Nations Foundation</u></a> term AMR as a \u2018silent pandemic\u2019.&nbsp;</p><h2><br>AMR is a neglected field&nbsp;</h2><p>Although there has been some global response to AMR, it has not been proportional&nbsp; to its threat to healthcare systems and the economy. Despite many governments developing National Action Plans (NAPs)&nbsp; in response to the&nbsp;<a href=\"https://www.google.com/search?q=who+call+national+action+plan+amr&amp;oq=who+call+national+action+plan+amr&amp;aqs=chrome..69i57j33i160l2j33i22i29i30l2j33i22i29i30i625.3717j0j7&amp;sourceid=chrome&amp;ie=UTF-8\"><u>WHO call</u></a> for the same in 2015, and several public and private organisations, especially CARB-X and Wellcome Trust, making significant investments in the space (see the&nbsp;<a href=\"https://dashboard.globalamrhub.org/\"><u>Global AMR R&amp;D Hub</u></a> for details on this), it seems unlikely that current investment will be sufficient to curb the rise of AMR.</p><p>There are several overarching reasons for this:</p><ol><li>&nbsp;Various solutions that have the potential to reduce the threat of AMR are either public goods or suffer from market failures that mean that they are currently not economically viable- it is because of this that R&amp;D pipelines for new diagnostics and therapeutics to address AMR have dried up.&nbsp;</li><li>&nbsp;Secondly, there are inequities in global focus and funding on AMR- for instance, although many countries have established NAPs to address AMR, many resource constrained countries struggle to fund and implement their plans.</li><li>There are also competing interests and&nbsp; conflicts of interest between different stakeholders (.e. between stewardship and access in the community, between agricultural productivity and inappropriate use of antibiotics) that constrain impact</li><li>&nbsp;Finally, although there is a committed body of organisations and individuals working on AMR, work is often siloed and the infrastructure that they operate within is sclerotic.&nbsp;</li></ol><p>&nbsp;</p><h2>There are tractable interventions to reduce the burden of AMR</h2><p>There are a number of uniquely and highly impactful opportunities to significantly reduce the burden of AMR that are untapped by the current funding landscape.&nbsp;</p><p>AMR is a complex scientific phenomenon, and is driven by the use and overuse of antimicrobials in the human, animal and environmental sector. To provide a very brief overview into the types of interventions within the field, below is a simple systems maps of the drivers of AMR and the broad solution architecture:</p><p><i><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/evvlazk48wlrwrjzsysn\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/bwl7jepd1wrba1hygxpw 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/runbyb6yx4cpjlyg0wwg 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/owunxx5wslgmsonj5ymb 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/ynykx120clbuy9sfpcmz 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/trkvb4smczmmzskhyjxj 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/umzcu2vbgt928wwsnpy2 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/dlr7dcypqmadrl14apfn 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/wc03h8bsl98dhyj8uul5 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/bw3r4brgml9dqkpnuwqj 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/W93Pt7xch7eyrkZ7f/e6vcbpnmbob2nhuevl84 1176w\">Figure 2: Systems map of drivers of AMR and how it can be managed; Human drivers include poor sanitation and hygiene, lack of access to clean water and vaccinations, overuse of antibiotics, and lack of stewardship for responsible use of antibiotics. Animal health drivers include the overuse of antibiotics in intensive factory farming, and environmental drivers can include the overuse of antibiotics from&nbsp; pharmaceutical manufacturing and hospital systems.</i></p><p><br>As part of my work for Schmidt Futures, I specifically sought to identify opportunities where additional work might have \u2018big if true\u2019 implications, as well as&nbsp;interventions and areas that are currently being overlooked or are neglected by current actors</p><p>In my report,&nbsp;<a href=\"https://drive.google.com/file/d/1wiY4w0QADOZzc8-ac9hbXr9_xcDlTi8k/view?usp=share_link\"><u>which you can read here</u></a>, I offer some&nbsp; recommendations for important actions that can be taken to reduce the burden of AMR.&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Area</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>What the problem is</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Recommended actions</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Research and development pipelines for new antimicrobials</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Developing new antimicrobials may be crucial in&nbsp; reducing the burden of AMR. However, R&amp;D efforts have lagged- very few antibiotics have been developed and brought into market in the last few decades, with many large companies reducing or completely halting their AMR work.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Support market shaping tools to accelerate antimicrobial development</li><li>Identify and support platform technologies that may unlock and acceleration innovation&nbsp;&nbsp;</li><li>&nbsp;Fund a&nbsp; massive open online course (MOOC) which focuses on market incentives for global health R&amp;D</li></ol></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Building and translating evidence into policy</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Although there is a growing evidence base for what the drivers of AMR are, and what policies are likely to reduce its burden, there are some gaps in both the research that exists and its prioritisation and translation into policy, which constrain policy design and implementation around AMR</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Build quantitative, data-driven models of the drivers of AMR</li><li>Support work at the&nbsp; intersection of preventative health and AMR</li><li>Foster a generation of AMR policy makers</li></ol></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Aligning on diagnostic needs</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Developing and having access to appropriate diagnostic tests can help reduce inappropriate use and overprescription of antimicrobials. However, the role of diagnostics is underappreciated and underinvested in, and currently there are no diagnostic tests that are fit for purpose on a global scale.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ol><li>Establish a common agenda for AMR diagnostics</li><li>Support platform technologies</li><li>Fund organizations that are accelerating translational research for point of care diagnostics&nbsp;</li></ol></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Creating global momentum</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Despite AMR being a significant global problem, it receives relatively little attention. This constrains the flow of talent, funding and policy attention towards AMR.&nbsp;</p><p><br>&nbsp;</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><br>&nbsp;</p><ol><li>Raise the profile of AMR</li><li>Empower low and middle income countries to focus on AMR</li></ol></td></tr></tbody></table></figure><h2>&nbsp;</h2><h2>You can get involved</h2><h3>Fund impactful work on AMR</h3><p>If any of the recommendations or broader work in this report are of interest, I would invite you to get in touch at&nbsp;<a href=\"mailto:akhil@amrfundingcircle.com\"><u>akhil@amrfundingcircle.com</u></a>.&nbsp;</p><p>Further, if you have an interest in offering ongoing support to&nbsp; impactful work in the AMR space, I have recently launched the&nbsp;<a href=\"https://www.amrfundingcircle.com/\"><u>AMR Funding Circle</u></a>, which aims to identify, vet and prioritize different projects and organizations working on AMR for funders and grantmakers.&nbsp; In the process, the funding circle aims to support impactful work in the area, and to help coordinate the field at large.</p><p>&nbsp;&nbsp;It will bring together funders who commit to one hour a month for a meeting, plus about an hour outside of the meeting to consider opportunities. To join the network there is also a $50,000 per year minimum expected contribution to the cause area of AMR.</p><p>If you are interested in being involved in the AMR Funding Circle, please also get in touch with me at&nbsp;<a href=\"mailto:akhil@amrfundingcircle.com\"><u>akhil@amrfundingcircle.com</u></a><br>&nbsp;</p><h3>Found a charity</h3><p><a href=\"https://www.charityentrepreneurship.com/\"><u>Charity Entrepreneurship</u></a> (where I previously worked) did a round of research on health security, and amongst some&nbsp;<a href=\"https://www.charityentrepreneurship.com/post/announcing-our-2023-charity-ideas-apply-now\"><u>other great ideas for new charities in the space</u></a>, have recommended a charity that helps prevent the growth of antimicrobial resistance by advocating for better (pull) funding mechanisms to drive R&amp;D and responsible use of new antibiotics.&nbsp; If you are interested in this, please feel free to reach out to me or to the CE team directly!</p>", "user": {"username": "Akhil Bansal"}}, {"_id": "sz7sYj3XFENNecLDa", "title": "Expert trap: What is it? (Part 1 of 3) \u2013 how hindsight, hierarchy, and confirmation biases break conductivity and accuracy of knowledge", "postedAt": "2023-06-06T15:05:15.881Z", "htmlBody": "<p><i>Crossposted from&nbsp;</i><a href=\"https://sysiak.substack.com/p/expert-trap-1\"><i>Pawel\u2019s blog</i></a></p><p><i>This essay has three parts. In part one, I include notes on epistemic status, I give a summary of the topic. But mainly I describe what is the Expert trap.</i></p><p><a href=\"https://forum.effectivealtruism.org/posts/ifnhppnRxggbFt4sW/expert-trap-part-2-of-3-how-hindsight-hierarchy-and\"><i>Part two</i></a><i> is about context. In \u201cWhy is expert trap happening?\u201d I dive deeper explaining biases and dynamics behind it. Then in \u201cExpert trap in the wild\u201d I try to point out where it appears in reality.</i></p><p><a href=\"https://forum.effectivealtruism.org/posts/QJdCgwhaQm3c4hnFK/expert-trap-ways-out-part-3-of-3\"><i>Part three</i></a><i> is about \u201cWays out\u201d. I list my main ideas of how to counteract the Expert trap. I end with conclusions and with a short Q&amp;A.</i></p><h3>Intro</h3><p>Version: 0.3. This is still a very early version of this article. Yes, this thing has versions and will improve with time.</p><p>Feedback: I would love to understand what I am missing or learn about any counter-arguments. Feel free to write a comment, dm me, or share anonymous feedback here: <a href=\"http://sysiak.com/feedback\"><u>sysiak.com/feedback</u></a>.</p><p>Writing style: I don\u2019t normal write. English is not my native language. But I care about being simple and precise. Read more about my writing approach and values here: <a href=\"http://sysiak.com/about\"><u>sysiak.com/about</u></a></p><p>Epistemic status: I think it\u2019s a great practice, to begin with Epistemic status. That is state your certainty, effort, link to sources, and point to main counterarguments. In this case, however, feel free to skip it and come back to it at the end. I am proposing here quite a large statement and I am writing with quite a lot of uncertainty. It may be interesting to first evaluate claims on your own. At the end of the series, I will remind you about coming back to epistemic status and I will post a short Q&amp;A that will hopefully clarify some gaps.</p><h3>Epistemic status (optional)</h3><p>Certainty: If I were to pick one confidence interval for the main claim it would be <a href=\"https://sysiak.com/epistemic-status\"><i><u>Likely (70%)</u></i></a>. There are also parts of the described dynamics that I think are <i>Highly likely (90%)</i>or A<i>lmost certain (&gt;97%)</i>. These are for example explanations about my-side bias, confirmation bias, and hindsight bias which I think are established and largely non-controversial. But I also have quite a lot of uncertainty. There are a lot of claims that are around <i>Maybe (50%)</i> (hovering between 35% and 75%). I think the most uncertain part of the knowledge is <a href=\"https://sysiak.com/cognitive-biases/hierarchy-bias\"><u>Hierarchy bias</u></a> and ideas from \u201cElephant in the Brain\u201d. This is a sweeping reinterpretation of how human motivations work. Personally, I think it\u2019s <i>Likely (~70%)</i> that hierarchy bias largely explains our behavior. But I understand others will find it a lot less probable.</p><p>Effort: I have been exposed to these findings for five years now. Since then I had time to digest them and read thoroughly around the topic.</p><p>Evidence: The evidence mostly comes from these sources: \u201cElephant in the Brain\u201d, Stumbling on Happiness\u201d, Daniel Kahneman\u2019s work. Please let me know if you know any information that invalidates any of the research mentioned.</p><p>Antithesis: What would need to be true to invalidate the main claims of this essay? Here are three main axes of criticism 1) I may be missing some major factor (besides hierarchy and my-side bias) of why the Expert trap is happening 2) Hierarchy bias findings are not established and therefore there is a chance they are incorrect or have some gaps 3) All systems loose efficiency with getting more complex. Perhaps, it is universal and unavoidable that the more complex the knowledge the more corrupted and inaccessible it gets. The current norms around learning, sharing, encoding knowledge are rather efficient.</p><h3>Summary</h3><p>Summary in one paragraph: The main claim of this essay is that knowledge often gets locked in specific areas and levels of expertise. I identify two primary factors contributing to this phenomenon: hierarchy bias and my-side bias. I also present methods for how to counteract it.</p><p>Summary longer: This article explores the intuition that the way our civilization encodes knowledge is often faulty, inefficient and makes that knowledge difficult to use.</p><p>This essay explains my take on a cognitive bias that I call Expert trap. Those who are better informed have trouble or are unable to pass knowledge on to those who are less informed. Some call this bias \u201cThe curse of knowledge\u201d. I use expert trap because it\u2019s shorter, and has a closer association with the root of the problem. I see the Expert trap as a larger phenomenon than what one typically associates with \u201cThe curse of knowledge\u201d. I see it as driven by a couple of other biases. These are <a href=\"https://sysiak.com/cognitive-biases/hindsight-bias\"><u>Hindsight bias</u></a> \u2013 once you know the answer to the question you will think that you would have guessed it. <a href=\"https://sysiak.com/cognitive-biases/hierarchy-bias\"><u>Hierarchy bias</u></a> \u2013 people's hidden motive to acquire knowledge may be less about getting things right than elevating themselves in a hierarchy. <a href=\"https://sysiak.com/cognitive-biases/confirmation-bias\"><u>Confirmation bias</u></a> \u2013 once you have formed an opinion you will tend to select and believe in information that strengthens it and less in information that challenges it. At the root of all these biases is <a href=\"https://sysiak.com/cognitive-biases/my-side-bias\"><u>My-side-bias</u></a> \u2013what is mine is better. Whichever definition is mine will be questioned less.</p><p>I think the Expert trap has large and overlooked consequences. I will propose a hypothesis suggesting that the learning and sharing of knowledge within our civilization is largely inefficient, with the Expert trap serving as the primary explanation. I will describe this using examples of the educational system and our approach to learning in general. Finally, I will explain methods that may be helpful in counteracting it.</p><h1>What is the Expert trap?</h1><h3>Healthy knowledge</h3><p>First, let\u2019s define the opposite. What\u2019s the healthy state of knowledge? Knowledge that is efficient, robust, and useful? The metaphor I like is of conductivity. High-quality knowledge is highly conductive.</p><ul><li>It brings one smoothly from not knowing to knowing.</li><li>It enables one to easily access different levels of complexity. So if one wants to learn just a little bit one knows how to do it. A simple explanation should be a good mapping, representation, and stepping stone to a more complex one.</li><li>It also should be roughly correct at different levels of complexity. So if one chooses to stay at a lower level and decides to apply this knowledge to their area of expertise they are going to get approximately accurate results.</li></ul><p>I think knowledge created by our civilizations is often of low conductivity. I think one of the main drivers of this is the Expert trap dynamic.</p><h3>Hindsight bias</h3><p>The phrase \u201cthe curse of knowledge\u201d was first used in 1989 by Camerer and Loewenstein. They saw it as closely related to <a href=\"https://sysiak.com/cognitive-biases/hindsight-bias\"><u>Hindsight bias</u></a> \u2013 knowing the outcome makes people falsely confident that they would have predicted the answer.</p><p>\u201cStudy participants could not accurately reconstruct their previous, less knowledgeable states of mind, which directly relates to the curse of knowledge. This poor reconstruction was theorized by Fischhoff to be because the participant was \"anchored in the hindsightful state of mind created by receipt of knowledge\". Fischhoff, Baruch (2003).&nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1743746\"><u>\"Hindsight is not equal to foresight\"</u></a>.</p><p>It is as if our brains are wishfully reconstructing the knowledge to fit the outcome. If a person knows the outcome, they may be less inquisitive about its root causes, less motivated to look at it from first principles. They may be looking less carefully at each part of the process and therefore bending inputs so they match the outcome.</p><p>Historically, hindsight bias was the first clue to understanding the curse of knowledge (or what I call the Expert trap dynamic). My hypothesis is that it likely is an extension of my-side bias, which is also referred to by others as motivated reasoning (I write more on this later). That is we may be motivated to be critical about the knowledge as long as it strengthens our positive self-image. When I know the answer to the question I am not motivated to really dig deeper into its root causes. I already correctly guessed it and the reward, such as I am smart, was already delivered.</p><h3>Tapping experiment</h3><p>When subjects were asked to finger-tap a popular tune of their choosing they were hugely overconfident about how many people would get it. They estimated that 50% of people would get it whereas in reality, 1.33% got it <a href=\"https://creatorsvancouver.com/wp-content/uploads/2016/06/rocky-road-from-actions-to-intentions.pdf\"><u>from the 1990 Stanford experiment.</u></a>This may be the best metaphor for the Expert trap I stumbled upon. It clearly renders what\u2019s going on in the mind of somebody who \u201cknows\u201d. It looks like a person who \u201cknows\u201d is projecting that knowing onto their audience and is unable to see what they are really communicating.</p><figure class=\"image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa57eac-dca4-4cbd-ae27-006023ce6e22_2000x1952.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa57eac-dca4-4cbd-ae27-006023ce6e22_2000x1952.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa57eac-dca4-4cbd-ae27-006023ce6e22_2000x1952.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa57eac-dca4-4cbd-ae27-006023ce6e22_2000x1952.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa57eac-dca4-4cbd-ae27-006023ce6e22_2000x1952.png 1456w\"><figcaption>All experts look the same :)</figcaption></figure><h3>Knowledge silos</h3><p>Knowledge is often trapped in different expertise silos. It is trapped there in a state that is anywhere from \u2013 it\u2019s hard to use to it\u2019s unusable. I see two main mechanisms there.</p><p>When people learn, the knowledge gets trapped at each new level of understanding. Once a person acquired knowledge they often are unable to explain it to people who don\u2019t understand. What\u2019s fascinating here, is it seems that they wouldn\u2019t be able to explain it to the past version of themselves. Somehow the context gets lost. Perhaps, as a person aspires to understand further, they lose track of the \u201cAha!\u201d moments that brought them to where they are.</p><p>But also knowledge gets trapped across different disciplines. How easily can physicists talk to chemists about the same processes? I guess because of different terminology and mental models experts in adjacent areas often have a hard time. I will write more extensively on how this may work in \u201cWhy is expert trap happening?\u201d</p><h3>Expert trapped in Morgan Library</h3><p>But let\u2019s land in the real world. I have this silly example that I think renders well the dynamic of the Expert trap.</p><p>For two weeks I was living in Manhattan, New York, a ten-minute walk from the Morgan Library. I read that there is an interesting exhibition. I opened Google Maps and saw a photo of it. It just looked like an old library. I saw things like this before and decided not to go.</p><p>After a while, a different friend mentioned Morgan Library again. \u201cRecommendations from two different sources? The exhibition is still going. Let\u2019s go\u201d I went there and was blown away. I explored it very carefully, digested it and sent a photo to my partner. I expected she will respond with excitement. But it seems it was \u201cmeh\u201d to her. I looked at the photo and realized I made a very similar photo to the ones in Google Maps.</p><p>I think, like in Tapping experiment example, I projected my knowing on the photo. There were more things that make this place fascinating. The photo didn\u2019t communicate them. Perhaps be that this private mansion, with the feel inside of a rural villa, is in the midst of Manhattan, one of the most densely populated places of the United States. Also the juxtaposition of wealth. This is the office of J.P. Morgan a founder of Chase, the biggest bank in the US. Here is this dirty street, and here, behind a wall, the exhibit of the most insane wealth in the world. A picture on the wall? You get closer. It\u2019s Hans Memling. Some small altar? You zoom in and it is executed in thousand years ago in the Byzantine empire and is framing fragments of the cross of Jesus Christ.</p><p>I took the photo of this place and I have mentally overwritten it with new meanings. We may be victims of Expert Trap on many different layers and much more often than we assume.</p><h3>Illusory knowledge</h3><p>When learning, I think we very often fool ourselves about our comprehension level. We think we understand something, but what we actually did is familiarized ourselves with the area and memorized terminology. Memorized names, often, function as covers that are conveniently obstructing areas that are still fuzzy. Things start to feel familiar but we are not much deeper in understanding it. This seems like a pretty strong and redefining statement. I see this as a gradient. When a person learns something, they will acquire both illusory and true knowledge. I am claiming, however, that the proportion of illusory knowledge is much higher than it is conventionally assumed. Also, I don\u2019t think people are doing it intentionally. Most of this happens subconsciously.</p><p>So when a person has a breath of complex terminology connected to some knowledge area, it is easy to mistake it for knowledge that is robust, precise, practical, flexible, and applicable to many contexts. Very few people have a habit to learn things comprehensively. That is, when asked to explain things, they can approach it from many different perspectives \u2013 explain it to a kid, a high-schooler, or an expert. Learning this way involves taking concepts and applying them to a wide variety of areas, classes, contexts; testing them against edge cases, counterfactuals; thinking about them in the most practical way. How this abstract concept intersects with the real world? How, if true, it will change what I see in the real world?</p><p>Arriving closer to true knowledge seems more like a curvy path, like a system of paths that are traveled in many directions. It may be more type of thinking that comes from play, and curiosity. Whereas illusory knowledge often can be found in thinking that is instrumental, that is a mean to something else, that tries, in a more straightforward way, to get to conclusions.</p><p>I sense this illusory knowledge is abundant. I think it's a primary way we encode knowledge. Our learning methods and our educational systems may be full of it. I think it may be largely present in any level of education: from primary school to higher education.</p><p>This could be one of the main reasons why most schools are experienced as boring, and it might also explain why I cannot remember almost any useful knowledge I learned during my primary and high school education.</p><p>&nbsp;</p><p>Read <a href=\"https://forum.effectivealtruism.org/posts/ifnhppnRxggbFt4sW/expert-trap-part-2-of-3-how-hindsight-hierarchy-and\">Expert trap: Why is it happening? (Part 2 of 3)</a><br>&nbsp;</p>", "user": {"username": "pawsys"}}, {"_id": "K3XiFGMdAQXBGTFSH", "title": "A survey of concrete risks derived from Artificial Intelligence", "postedAt": "2023-06-08T22:09:00.021Z", "htmlBody": "<p><a href=\"https://riesgoscatastroficosglobales.com/\"><u>Riesgos Catastr\u00f3ficos Globales</u></a> has conducted a literature review and an expert elicitation exercise<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefb5tfz2nj5s9\"><sup><a href=\"#fnb5tfz2nj5s9\">[1]</a></sup></span>&nbsp;to categorize concrete risks associated with Artificial Intelligence (AI). This is part of our ongoing work on the implementation of the EU AI Act in Spain.</p><p>Here we present a short overview of the risks we have found. This is meant to be a mental framework for policymakers to consider when developing AI policies, but we think it might also be useful to incentivize discussion within the community. Please feel free to leave your thoughts as comments!</p><p>To facilitate comprehension, we have split the identified risks into two categories: adversarial and structural risks.&nbsp;<strong>Adversarial risks&nbsp;</strong>are those caused by the direct action of an agent, be it rogue groups, state actors, or misaligned AI.&nbsp;<strong>Structural risks&nbsp;</strong>are those derived from the wide-scale or high-impact deployment of AI, with diffuse causes.</p><p>The distinction builds upon the categorization between accidents, misuse, and structural risks (<a href=\"https://www.lawfareblog.com/thinking-about-risks-ai-accidents-misuse-and-structure\"><u>Zwetsloot &amp; Dafoe, 2019</u></a>). We preferred to merge the first two because we considered there was not always a clear difference in how accidents (AI misalignment) and misuses (humans exploiting an AI system to cause harm) materialize as specific threats<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1gog9getio7\"><sup><a href=\"#fn1gog9getio7\">[2]</a></sup></span>.</p><p>As for this materialization, we outline risks integrating present and future implications. That is to say, we state that their long-term impact is potentially large, but we ground them on existing and modest evidence. This choice is based on the assumption that policymakers will tend to underestimate speculative framings. The underlying logic we try to convey is that damage will increase along with capabilities and deployment.</p><p>We have identified nine concrete risks within these categories, which are summarized in the table below. The categorization is not perfect, but we tried to prioritize clarity and concreteness over accuracy and exhaustiveness<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv5bzknt9i7\"><sup><a href=\"#fnv5bzknt9i7\">[3]</a></sup></span>.</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#a4c2f4;border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Risk category</strong></p></td><td style=\"background-color:#a4c2f4;border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Risk</strong></p></td><td style=\"background-color:#a4c2f4;border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Example vignette</strong></p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\" rowspan=\"3\"><p><br>&nbsp;</p><p><strong>Adversarial risks:&nbsp;</strong>directly caused by agents, either humans or misaligned AI</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Cyberattacks and other unauthorized access</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>LLM-enabled spear-phishing campaigns</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Strategic technology development</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Development of a new biological weapon</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>User manipulation</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Individuals persuaded to support a certain political option</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\" rowspan=\"6\"><p><strong>Structural risks:&nbsp;</strong>caused by widespread automation</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Job market disruption</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>10% increase in unemployment over a year</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Socioeconomic inequality</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Leading companies capturing AI-created surpluses</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Bias amplification</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Minority groups being systematically denied access to housing or loans</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Epistemic insecurity</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Proliferation of deep fakes</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Faulty automation of critical processes</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Accidental nuclear attack from fully-automated C&amp;C</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Defective optimization</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Hospitals rejecting patients with serious conditions to maximize performance metrics</p></td></tr></tbody></table></figure><p><br>&nbsp;We briefly introduce these risks below, together with references for further reading.</p><h3><br><strong>Adversarial risks&nbsp;</strong></h3><p>This section compiles potential threats from rogue human actors and misaligned AI. The final list coincides with what <a href=\"https://arxiv.org/abs/2305.15324\">Shevlane et al. (2023)</a> call \"extreme risks\" and is slightly connected to the distinction between digital, physical, and political dimensions proposed by <a href=\"https://arxiv.org/abs/1802.07228\">Brundage et al. (2018)</a>.&nbsp;</p><p>Readers might note that our selected risks are commonly mentioned as instances of power-seeking behavior. We have not included vignettes about goal mispecification and misgeneralization for two reasons: they tended to be too vague and, to be impactful, most of them required the instrumental use of the actions listed below.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/n7bapg2lag5dwguzzktk\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/fszdfeaaregoewvxzw3f 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/kkk5kl3v7hi7igpcsb2z 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/sjpsdb4zlihjoebu6llv 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/hmuw1lgxrqht7berakcx 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/bixkzblgyhkq3dg0qsrs 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/bke1pt21t1rusfhcuryb 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/yo5oqm6xcjos0gdrgq0y 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/jqq9bfms1oqhz2mj8otd 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/ibkhfchmhy8gf3lldn6k 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/rqpm4mzuhi9gpftjnhge 1347w\"></figure><p>&nbsp;</p><ul><li><strong>Cyberattacks and other unauthorized access</strong></li></ul><p>AI promises to enhance the execution of cyber offenses, increasing their scale and impact (<a href=\"https://arxiv.org/abs/1802.07228\"><u>Brundage, et al., 2018</u></a>). New tools can automate manual tasks (see&nbsp;<a href=\"https://asu.pure.elsevier.com/en/publications/mechanical-phish-resilient-autonomous-hacking\"><u>Mechanical Phish</u></a> for vulnerability detection), improve current techniques (see&nbsp;<a href=\"http://export.arxiv.org/abs/2305.06972v2\"><u>GPT-4</u></a> for spear phishing campaigns), and add new capabilities (see&nbsp;<a href=\"https://arxiv.org/abs/1610.01969\"><u>DeepDGA</u></a> for evasion) (<a href=\"https://www.traficom.fi/sites/default/files/media/publication/TRAFICOM_The_security_threat_of_AI-enabled_cyberattacks%202022-12-12_en_web.pdf\"><u>Aksela et al., 2021</u></a>). Likewise, the AI systems themselves harbor specific vulnerabilities that can be exploited by adversaries to alter their behavior. Some examples include data poisoning (<a href=\"https://arxiv.org/abs/2006.12557\"><u>Schwarzschild et al., 2021</u></a>) or prompt injection (<a href=\"https://arxiv.org/abs/2211.09527\"><u>Perez &amp; Ribeiro, 2022</u></a>).</p><p>Both humans and AI systems could try to accumulate power through various cybernetic operations. Examples of this include looting financial resources, accessing C&amp;C, obtaining sensitive data, and self-replicating. In some cases, AI systems could even infiltrate other devices offline, by finding unexpected ways of interacting with the environment \u2013 see&nbsp;<a href=\"https://www.researchgate.net/publication/3949367_The_evolved_radio_and_its_implications_for_modelling_the_evolutionof_novel_sensors\"><u>Bird &amp; Layzell (2002)</u></a> for an example.&nbsp;&nbsp;</p><ul><li><strong>Strategic technological development</strong></li></ul><p>AI-powered weapons introduce an accountability gap (<a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-5930.2007.00346.x\"><u>Sparrow, 2007</u></a>) and are prone to making incorrect decisions, mainly due to out-of-distribution errors (<a href=\"https://sciencepolicyreview.org/wp-content/uploads/securepdfs/2022/10/v3_AI_Defense-1.pdf\"><u>Longpre et al., 2022</u></a>). Adversaries trying to manipulate performance could aggravate this problem (<a href=\"https://arxiv.org/abs/1707.08945\"><u>Eykholt et al., 2018</u></a>). On the other hand, AI lowers the barriers to entry for inflicting large-scale damage, favoring rogue actors (<a href=\"https://www.brookings.edu/research/democratizing-harm-artificial-intelligence-in-the-hands-of-non-state-actors/\"><u>Kreps, 2021</u></a>). This includes LAWS, but also facilitating the development of biological weapons (<a href=\"https://www.nature.com/articles/s42256-022-00465-9\"><u>Urbina et al., 2022</u></a>), among others.&nbsp;</p><p>Besides offensive uses, AI could also confer decisive strategic advantages by enabling scientific innovations with high practical impact. Monopolizing such an innovation could ensure undisputed hegemony for an actor, dangerously altering the balance of power. Take as an example nuclear fusion, where deep learning has already contributed to efforts to stabilize and control the plasma (<a href=\"https://www.nature.com/articles/s41586-021-04301-9\"><u>Degrave et al., 2022</u></a>), calculate its electric field (<a href=\"https://arxiv.org/abs/2107.02232\"><u>Aguilar &amp; Markidis, 2021</u></a>), and predict disruptions (<a href=\"https://www.nature.com/articles/s41586-019-1116-4\"><u>Kates-Harbeck et al., 2019</u></a>).&nbsp;</p><ul><li><strong>User manipulation&nbsp;</strong></li></ul><p>AI systems excel at profiling users and persuasion techniques. Relatively simple algorithms are already able to exploit human heuristics and influence individual preferences (<a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0249454\"><u>Agudo &amp; Matute, 2021</u></a>), decision-making (<a href=\"https://www.biorxiv.org/content/10.1101/2020.03.15.992875v1\"><u>Dezfouli et al., 2020</u></a>), and public opinion (Schippers, 2020).</p><p>APS systems could manipulate users through more sophisticated techniques, including emotional manipulation or extortion. For example, GPT-4 was already able to convince a contractor to solve a CAPTCHA (<a href=\"https://arxiv.org/abs/2303.08774\"><u>OpenAI, 2023</u></a>).</p><h3>&nbsp;</h3><h3><strong>Structural risks</strong></h3><p>This section compiles risks derived from the wide-scale or high-impact deployment of AI systems. The consequences of wide deployment are diverse, ranging from economic effects to fairness and political stability. High-impact deployment refers to the consequence of relinquishing human decision-making power in critical processes. This includes considerations on lack of supervision, increased speed, and bias towards quantifiable targets.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/pxjz5fvmhtfheom1gna1\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/mohrlpajj6gwnxzmkqnn 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/lp626osk4ujq9s8kkoru 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/r3aqtp4wgrrkxvogfr4w 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/dkuogs6pl4ptpfgy4wzb 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/eh8gapwnxfd2w07pcnwi 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/jmmdgxnyqepoabmhjzjo 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/o1tus0vlcjnkhm5eb7ko 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/f13dmndyis2jqrrcdrzd 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/kltyufkpsmdhnpkgk4wo 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/K3XiFGMdAQXBGTFSH/clrhqvb7geyifnhddubu 1017w\"></figure><p>&nbsp;</p><ul><li><strong>Job market disruption</strong></li></ul><p>The recent emergence of generative AI has the potential to rapidly accelerate task automation, leading to significant disruption in the labor market.&nbsp;<a href=\"https://www.gspublishing.com/content/research/en/reports/2023/03/27/d64e052b-0f6e-45d7-967b-d7be35fabd16.html\"><u>Hatzius et&nbsp;al. (2023)</u></a> suggest that up to one-fourth of current jobs could be replaced by current generative AI, with approximately two-thirds of jobs being exposed to some degree of automation. This could result in the automation of nearly 300 million jobs worldwide, affecting different countries to varying extents. To illustrate the risks more explicitly,&nbsp;<a href=\"https://arxiv.org/abs/2303.10130\"><u>Eloundou et al. (2023)</u></a> have analyzed the impact of large language models and have concluded that 19% of jobs in the U.S. have at least 50% of their tasks exposed to automation.</p><p>In addition to this,&nbsp;<a href=\"https://ideas.repec.org/a/fip/fedhep/y2005iqiip47-66nv.29no.2.html\"><u>Jacobsen et. al (2005)</u></a> have found that it takes between 1 and 4 years to reemploy displaced workers, who are anyway unlikely to find jobs similar to their previous ones. Therefore, these people experience long-term income losses.</p><ul><li><strong>Socioeconomic inequality</strong></li></ul><p>AI-generated value could be captured by AI companies or the countries where they are located, exacerbating wealth inequality (<a href=\"https://www.fhi.ox.ac.uk/windfall-clause/\"><u>O\u2019Keefe et al., 2020</u></a>). The nature of AI facilitates unfair competition and economic concentration, with data being one of the main drivers of this trend (<a href=\"https://www.nber.org/papers/w29247\"><u>Acemoglu, 2021</u></a>). In a feedback loop, data improves quality and quality attracts users, which increment data (<a href=\"https://www.bruegel.org/blog-post/dynamics-data-accumulation#_ftn10\"><u>Anderson, 2021</u></a>). High barriers to entry hinder competition.&nbsp;</p><p>Beyond moral objections, the sociopolitical consequences of inequality could unstabilize the world, e.g., increasing the risk of riots and crimes. Besides, controlling such a differential technology would grant its owners an excessive privilege, namely, the possibility of unilaterally making political decisions of great importance to the rest of society.&nbsp;</p><ul><li><strong>Bias amplification</strong></li></ul><p>AI systems adopt and reproduce bias found in the training datasets, which usually translates into disparate performance depending on their familiarity with the topic. This is particularly concerning when it comes to critical decisions in healthcare, justice, or finance (<a href=\"https://arxiv.org/abs/2108.07258\"><u>Bommasani, 2022</u></a>).&nbsp;</p><p>For instance, some algorithms have been detrimental for black people in recidivism prediction (<a href=\"https://www.science.org/doi/10.1126/sciadv.aao5580\"><u>Dressel &amp; Farid, 2018</u></a>) and for disadvantaged groups in credit approvals (<a href=\"https://arxiv.org/abs/2105.07554\"><u>Blattner &amp; Nelson, 2021</u></a>).&nbsp;</p><ul><li><strong>Epistemic insecurity</strong></li></ul><p>Access to reliable information is key for a healthy society to make based decisions, thus the proliferation of misinformation poses a threat to national security (<a href=\"https://www.cser.ac.uk/resources/epistemic-security/\"><u>Seger et al., 2020</u></a>).&nbsp;</p><p>AI exacerbates the risk in different ways. First, large language models (LLMs) are prone to accidental \u201challucination\u201d (<a href=\"https://arxiv.org/abs/2202.03629\"><u>Ji et al., 2023</u></a>). Second, LLMs could be exploited by malicious actors to carry out influence operations (<a href=\"https://arxiv.org/abs/2301.04246\"><u>Goldstein et al., 2023</u></a>), while image and video generation models are useful for deep fake creation (<a href=\"https://www.sciencedirect.com/science/article/pii/S1077314222001114\"><u>Nguyen et al., 2022</u></a>). Finally, AI-created content could contribute to information overload, which undermines individuals\u2019 abilities to discern relevant and accurate information.&nbsp;</p><ul><li><strong>Faulty automation of critical processes</strong></li></ul><p>Automatizing critical decision-making and management processes could cause serious incidents if the involved AI systems are prone to errors or not designed to respect human values and goals. Accidental errors are especially worrying because most AI systems are not sufficiently robust to distributional shifts (<a href=\"https://arxiv.org/pdf/1606.06565.pdf\"><u>Amodei et al., 2016</u></a>). If these processes are carried out without human supervision, their speed could make incidents easily spiral out of control (<a href=\"https://foreignpolicy.com/2018/09/12/a-million-mistakes-a-second-future-of-war/\"><u>Scharre, 2018</u></a>).&nbsp;</p><p>An extreme example is the automation of nuclear command and control. Transferring decision-making power would increase the probability of catastrophic errors in the interpretation of information. See the incident in the Soviet Union in 1983, when a radar set off alarms after confusing sunlight with an intercontinental ballistic missile. In that case, the presence of a human supervisor who decided to wait for more evidence prevented the launch of a Soviet attack. For more on how machine learning affects nuclear risk, see&nbsp;<a href=\"https://www.cser.ac.uk/resources/autonomy-nuclear-weapons/\"><u>Avin &amp; Amadae (2019)</u></a>.</p><ul><li><strong>Defective optimization</strong></li></ul><p>Since AI systems often seek to optimize a function, they tend to favor operationalizable and quantifiable objectives, thus neglecting other important values (<a href=\"https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like\"><u>Christiano, 2019</u></a>). That is an example of Goodhart\u2019s law, which states that \u201cwhen a measure becomes a target, it ceases to be a good measure\u201d (<a href=\"https://arxiv.org/abs/1803.04585\"><u>Manheim &amp; Garrabrant, 2019</u></a>). For instance, GDP is a useful variable for certain purposes, but focusing excessively on it implies ignoring factors such as subjective satisfaction, inequality rates, or environmental impact (<a href=\"https://www.weforum.org/agenda/2018/01/gdp-frog-matchbox-david-pilling-growth-delusion/\"><u>Pilling, 2018</u></a>).</p><p>As AI permeates more decision-making areas, there could be an increasing gap between the result of optimization and the nuanced objective we would ideally want to achieve. See the case of content selection algorithms for a hint. Maximizing engagement seemed to be a good proxy for economic profit and even social discussion, but turned out to favor incendiary content (<a href=\"https://www.nature.com/articles/s41599-020-00550-7\"><u>Munn, 2020</u></a>), animosity (<a href=\"https://www.pnas.org/doi/10.1073/pnas.2024292118\"><u>Rathje et al., 2021</u></a>), and outrage (<a href=\"https://www.science.org/doi/10.1126/sciadv.abe5641\"><u>Brady et al., 2021</u></a>). This toxicity deteriorates the public debate and might even end up affecting the platforms themselves by incrementing social media fatigue (<a href=\"https://www.sciencedirect.com/science/article/pii/S0736585321001350\"><u>Zheng and Ling, 2021</u></a>).&nbsp;</p><h3>&nbsp;</h3><h3><strong>Conclusion</strong></h3><p>In this post, we proposed an AI risk landscape based on two categories: adversarial risks and structural risks. The former are mostly ways in which a misaligned power-seeking agent could acquire human, financial, and technological resources. The latter usually imply collateral damages and pressures on the environment provoked by wide and high-consequence deployment. All of them are present to some extent at the current level of development, and could exacerbate as AI capabilities or adoption increase.&nbsp;</p><p>&nbsp;</p><p><strong>Acknowledgments&nbsp;</strong></p><p>We thank Jos\u00e9 Hern\u00e1ndez-Orallo, Pablo Moreno, Max R\u00e4uker, and Javier Prieto for participating in our elicitation exercise. We also thank Rose Hadshar for her valuable feedback on this post. All remaining errors are our own.&nbsp;<br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnb5tfz2nj5s9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefb5tfz2nj5s9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The elicitation exercise was run by the RCG staff and fed with the contributions of four external experts (see Acknowledgements). In the document, we asked participants to \u201csurface concrete risks derived from artificial intelligence, both present and future, that [they] would like to highlight to EU policymakers as risks to have present when designing regulation and auditing processes\u201d.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1gog9getio7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1gog9getio7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;One could go even further and argue that no human actor would be able to misuse an AI system if the latter is not prone to allowing that misuse.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv5bzknt9i7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv5bzknt9i7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Some other risks that we encountered but we considered too speculative or limited in scope include \u201ccognitive atrophy derived from user dependency of general-purpose AI\u201d, \u201cdisruption of grading and placement systems\u201d, and \u201cresource exhaustion and emissions from training and deployment\u201d.</p></div></li></ol>", "user": {"username": "Guillem Bas"}}, {"_id": "vxpqFFtrRsG9RLkqa", "title": "Announcement: You can now listen to the \u201cAI Safety Fundamentals\u201d courses", "postedAt": "2023-06-09T16:32:59.097Z", "htmlBody": "<p><strong>The </strong><a href=\"https://aisafetyfundamentals.com/\"><strong>AI Safety Fundamentals</strong></a><strong> courses are one of the best ways to learn about AI safety and prepare to work in the field.</strong></p><p><a href=\"https://www.bluedotimpact.org/\">BlueDot Impact</a> facilitates the courses several times per year, and the <a href=\"https://www.aisafetyfundamentals.com/curricula\">curricula are available online</a> for anyone to read.&nbsp;</p><p>The \u201cAlignment\u201d curriculum is created and maintained by <a href=\"https://twitter.com/RichardMCNgo\">Richard Ngo</a> (OpenAI), and the \u201cGovernance\u201d curriculum was developed in collaboration with a wide range of stakeholders.&nbsp;</p><p><strong>You can now listen to most of the core readings from both courses:</strong></p><figure class=\"table\"><table><tbody><tr><td style=\"width:25%\"><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vxpqFFtrRsG9RLkqa/vbz5lskf5chqgrvdaixs\"></figure></td><td><p><strong>AI Safety Fundamentals: Alignment</strong><br>Gain a high-level understanding of the AI alignment problem and some of the key research directions which aim to solve it.</p><p><br><a href=\"https://preview.type3.audio/playlists/agi-safety-fundamentals-alignment\"><strong>Listen online</strong></a><strong> or subscribe:</strong><br><a href=\"https://podcasts.apple.com/us/podcast/agi-safety-fundamentals-alignment/id1680794263\">Apple Podcasts</a> | <a href=\"https://podcasts.google.com/u/1/feed/aHR0cHM6Ly9mZWVkcy50eXBlMy5hdWRpby9hZ2ktc2FmZXR5LWZ1bmRhbWVudGFscy0tYWxpZ25tZW50LnJzcw?pageId=none\">Google Podcasts</a> | <a href=\"https://open.spotify.com/show/5664BSntGTMKOfVUTVXppO\">Spotify</a> | <a href=\"https://feeds.type3.audio/agi-safety-fundamentals--alignment.rss\">RSS</a></p></td></tr></tbody></table></figure><figure class=\"table\"><table><tbody><tr><td style=\"width:25%\"><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vxpqFFtrRsG9RLkqa/xssmvezokldx6dc31ap3\"></figure></td><td><strong>AI Safety Fundamentals: Governance</strong><br>Gain foundational knowledge for doing research or policy work on the governance of transformative AI.<br><br><a href=\"https://preview.type3.audio/playlists/agi-safety-fundamentals-governance\"><strong>Listen online</strong></a><strong> or subscribe:</strong><br><a href=\"https://podcasts.apple.com/us/podcast/agi-safety-fundamentals-governance/id1687830086\">Apple Podcasts</a> | <a href=\"https://podcasts.google.com/u/1/feed/aHR0cHM6Ly9mZWVkcy50eXBlMy5hdWRpby9hZ2ktc2FmZXR5LWZ1bmRhbWVudGFscy0tZ292ZXJuYW5jZS5yc3M?pageId=none\">Google Podcasts</a> | <a href=\"https://open.spotify.com/show/4LvXaqFpmG1GSBtIxcmJjc\">Spotify</a> | <a href=\"https://feeds.type3.audio/agi-safety-fundamentals--governance.rss\">RSS</a></td></tr></tbody></table></figure><p>We've also made narrations for some readings from the advanced \u201cAlignment 201\u201d course, and we may record more later this year:</p><figure class=\"table\"><table><tbody><tr><td style=\"width:25%\"><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vxpqFFtrRsG9RLkqa/ruvnmtu93ubppcmkamhl\"></figure></td><td><strong>AI Safety Fundamentals: Alignment 201</strong><br>Gain enough knowledge about alignment to understand the frontier of current research discussions.&nbsp;<br><br><a href=\"https://preview.type3.audio/playlists/agi-safety-fundamentals-alignment-201\"><strong>Listen online</strong></a><strong> or subscribe:</strong><br><a href=\"https://podcasts.apple.com/us/podcast/agi-safety-fundamentals-alignment-201/id1687829987\">Apple Podcasts</a> | <a href=\"https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy50eXBlMy5hdWRpby9hZ2ktc2FmZXR5LWZ1bmRhbWVudGFscy0tYWxpZ25tZW50LTIwMS5yc3M\">Google Podcasts</a> | <a href=\"https://open.spotify.com/show/20UUZUKgvxte52gukX1wYD\">Spotify</a> | <a href=\"https://feeds.type3.audio/agi-safety-fundamentals--alignment-201.rss\">RSS</a></td></tr></tbody></table></figure><hr><h3>Apply to join the \u201cAI Safety Fundamentals Governance Course\u201d July cohort!</h3><p>Gain foundational knowledge for doing research or policy work on the governance of transformative AI.</p><p>Successful applicants will <strong>participate in the </strong><a href=\"https://www.agisafetyfundamentals.com/governance-course-details\"><strong><u>AI Governance course</u></strong></a> with weekly virtual classes, and <strong>join the </strong><a href=\"https://www.agisafetyfundamentals.com/our-community\"><strong><u>AI Safety Fundamentals community</u></strong></a>.</p><p><strong>Apply before 26th June 2023!</strong></p><p><a href=\"https://apply.aisafetyfundamentals.com/governance?prefill_%5Ba%5Dsource=Type%20III%20Audio%20Forum%20Post&amp;utm_campaign=typeiii&amp;utm_source=forum\"><strong>https://apply.aisafetyfundamentals.com/governance</strong></a></p><hr><h3>Thoughts, feedback, suggestions?</h3><p>These narrations were created by <a href=\"https://twitter.com/perrinjwalker\">Perrin Walker</a> (<a href=\"http://type3.audio/\">TYPE III AUDIO)</a> on behalf of <a href=\"https://bluedotimpact.org/\">BlueDot Impact</a>, with support from the rest of the team at TYPE III AUDIO.</p><p>We would love to hear your feedback. Do you find the narrations helpful? How could they be improved? What other AI safety material would you like to listen to? Please comment below, complete our <a href=\"https://forms.gle/v9Pe5Ys57yRs4LQEA\">feedback form</a>, or write to&nbsp;<a href=\"mailto:team@type3.audio\">team@type3.audio</a>.</p>", "user": {"username": "Peter_Hartree"}}, {"_id": "vTcmucF4XmBLXbDqM", "title": "EA Architect: Dissertation on Improving the Social Dynamics of Confined Spaces & Shelters Precedents Report", "postedAt": "2023-06-06T11:58:15.073Z", "htmlBody": "<h2>TL;DR</h2><p>In this post, I will share the work I have done on the topic of civilisational shelters&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BtqhvcawMdrYJcKgK/help-us-make-civilizational-refuges-happen\"><u>(1)</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/u5JesqQ3jdLENXBtB/concrete-biosecurity-projects-some-of-which-could-be-big-1\"><u>(2)</u></a>, over the last year as an architecture master's student. I will share my&nbsp;<strong>dissertation on improving the social dynamics of confined spaces</strong>, including a practical&nbsp;<strong>design guide&nbsp;</strong>that can be used to design new or evaluate and improve existing confined spaces. I will also share the&nbsp;<strong>Shelters Precedents Report Draft</strong> I worked on last spring.</p><p>&nbsp;</p><p>Key links from this post include:</p><ul><li>My dissertation in&nbsp;<a href=\"https://drive.google.com/file/d/1Yn0_s5YEMRpIBaLft01Ui6JliCUnLacu/view?usp=sharing\"><u>pdf</u></a> or&nbsp;<a href=\"https://online.fliphtml5.com/icicp/pnkl/\"><u>flipbook</u></a> formats</li><li><a href=\"https://docs.google.com/spreadsheets/d/1_JKlkmv5Yu9bLUrpiBWibTOPbth56pcLvk6PbKWypgk/edit?usp=sharing\"><u>Link to the Wellbeing Worksheet</u></a>, an interactive design guide proposed in my dissertation</li><li><a href=\"https://www.youtube.com/watch?v=tiR_Y5tpQcs\"><u>Video</u></a> summarising the research and findings (especially useful if you want to learn about my design proposal and the design guide)</li><li>Link to the&nbsp;<a href=\"https://docs.google.com/document/d/1eO9nS5uAORUbBl11TFxL3rNq1eBc-lwq5kY6KmZaPcQ/edit?usp=sharing\"><u>Shelters Precedents Report Draft</u></a></li></ul><h2>&nbsp;</h2><h2>Outline</h2><p>Since last spring, I have explored ways to get involved in EA with my skills as an architect. So far, I wrote&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/mNBLzcdoiqCWFc8nL/becoming-an-ea-architect-my-first-month-as-an-independent\"><u>this</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/mcdeSBX3hz2JFKXrm/ea-architect-updates-on-civilizational-shelters-and-career\"><u>this</u></a> article about my ideas and journey of becoming the \u2018EA Architect\u2019, and have also started to help anyone with architectural or planning background get involved through the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ayikJeFfavmeEo22T/announcing-the-ea-architects-and-planners-group\"><u>EA Architects and Planners group</u></a>. One of the key areas I got involved in was&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ZkkeLBwRGgxmsiqrh/apply-to-join-shelter-weekend-this-august\"><u>civilisational shelters</u></a>. This summer, I am going to Zambia to intern with the <a href=\"https://chartercitiesinstitute.org/\">Charter Cities Institute</a>.</p><p>&nbsp;</p><p>This post has two parts:</p><ul><li>Part 1: My architectural research-led dissertation on&nbsp;<strong>\u2018Improving the Social Dynamics of Confined Spaces Located in Extreme Environments\u2019;&nbsp;&nbsp;</strong></li><li>Part 2: Sharing the&nbsp;<strong>Shelters Precedents Report Draft</strong> I developed last spring and so far only shared internally.<br>&nbsp;</li></ul><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/izznantdkbkzkzmurexc\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/hcgquvavqehicp4gkaio 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/dxnaeuyarr2k3hjn3nl5 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/hlmpymoeahrycai4u8yz 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/dgbtallwqllmmdm2bnjt 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/dmowd1xuke5k5qyefibx 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/ronjx4cota2dldgbje0a 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/utw3bca6gkgcvjwx3csk 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/daqc2dc5wsdwjtoboige 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/g9xbxxdbmgrsj41ypzwh 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/isemzhboucwbfmksz1zj 1600w\"><figcaption>Paths I have explored so far</figcaption></figure><h2><br>Part 1: Improving the Social Dynamics of Confined Spaces Located in Extreme Environments&nbsp;&nbsp;</h2><h2><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/ypbotvrfwyfaaarmdlwl\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/maf3brwcqgwwnknm1od6 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/eqc0dzrrlivkq5crua7t 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/fd2bpzvvkldgutsehvsu 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/a3rs9ebfde6v74ivyr5t 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/vrmslryx1abd5qumubns 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/djyd5zgjb9kydkhsp3lm 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/l0hbnglf9fvdn5j8k1ht 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/qm78rkp96dka7flqgjtr 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/neg8kfdt6qztuhvwh3vx 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/tfkxenqm0fxnvhnd8pxc 1600w\"></h2><p>After co-organising the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ZkkeLBwRGgxmsiqrh/apply-to-join-shelter-weekend-this-august\"><u>SHELTER Weekend</u></a> last summer (see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/DArtSnDxRH5AsE5RF/sheltering-humanity-against-x-risk-report-from-the-shelter\"><u>this post</u></a> by Janne for a summary of what has been discussed), as well as studying various precedents and talking to many experts, I concluded that the best way I can contribute to the shelters work is by understanding what influences the social dynamics of very confined spaces. Hence, I chose this as my master\u2019s thesis at Oxford Brookes.</p><h3>&nbsp;</h3><h3>Why I did it</h3><p>Global catastrophes, such as nuclear wars, pandemics, asteroid collisions or biological risks, threaten the very existence of mankind&nbsp;(Beckstead, 2015). These challenges have caused people to consider distant locations such as polar regions, deep sea, outer space, and even underground facilities as potential locations to seek safety during such crises&nbsp;(Beckstead, 2015; Jebari, 2015). However, living in confined spaces for prolonged periods brings prominent social challenges that might prevent their long-term success&nbsp;(Jebari, 2015). To ensure the successful habitation of confined spaces, special attention needs to be given to their design, allowing humans to survive and thrive long-term.&nbsp;<br>&nbsp;</p><p>While there is existing research on the design of specific confined spaces, like the design of research stations in polar regions&nbsp;(Bannova, 2014; Palinkas, 2003), space stations&nbsp;(Basner, Dinges, et al., 2014; Harrison et al., 1985), prisons&nbsp;(Karthaus et al., 2019; Lily Bernheimer, Rachel O\u2019Brien, Richard Barnes, 2017), biospheres testing space habitation&nbsp;(Nelson et al., 1994; Zabel et al., 1999) or nuclear bunkers&nbsp;(Graff, 2017;&nbsp;<i>NPR</i>, 2011), there seems to be a lack of a comprehensive architectural framework that can be utilised by designers of confined spaces in extreme environments to help improve their liveability. This is despite the fact there has been much research on the impacts of the physical environment&nbsp;(Klitzman and Stellman, 1989), including staying indoors&nbsp;(Rashid and Zimring, 2008), thermal comfort&nbsp;(Levin, 1995), the impact of light&nbsp;(Basner, Babisch, et al., 2014) and noise&nbsp;(Levin, 1995) on stress levels, psychological well-being and health.</p><p>&nbsp;</p><p>Therefore, \u200b\u200bmy dissertation aimed to investigate the role of architectural and spatial design strategies in creating confined spaces within extreme environments that improve social dynamics. The research question I asked was:&nbsp;<strong>How can architectural and spatial design strategies be used to create confined spaces within extreme environments that improve social dynamics?&nbsp;</strong></p><p>&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/u9qqlyzlztdy4udyosmt\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/rgkge4lp6jzvnhdks4xa 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/avyrgyx4r5ao9lowxb87 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/zcpbcjcjl48ws34bk6rj 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/iehebcay9npac4fpmh2l 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/drhiteyvy5k7xxd4ch4g 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/r7o82odslir8bho6rhx5 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/nribxuhqzi07anj0lz69 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/fls7vdsedp2qjrfhimy0 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/nv7klrspnreqaycfrij5 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/mplbjdrqlp0yjb1a4w66 1600w\"><figcaption>Gap in knowledge</figcaption></figure><p>&nbsp;</p><h3>What I did</h3><p>My dissertation investigates how architectural and spatial design strategies can be used to enhance social dynamics in confined spaces within extreme environments. I developed a practical design guide (which I called the \u2018Wellbeing Worksheet\u2019 and you can find&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1_JKlkmv5Yu9bLUrpiBWibTOPbth56pcLvk6PbKWypgk/edit?usp=sharing\"><u>here</u></a>), drawing from literature on environmental psychology, confined environment design, space psychology, and sociology. The design guide addresses individuals\u2019 physical and psychological needs, and I employed relevant case studies to determine the transferability of design principles across domains.&nbsp;</p><p>&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/bsjklbp7uwsmfkvpktig\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/wuunm3ybakmr6iys8hnz 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/adclibdp53axxdzl8yca 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/hulp03eimfhdsfecv4de 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/vgfjci850w1ckf9q5d61 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/uznbcaj5br10lhsfmbtx 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/hz9hq59ozzu7dfqkcith 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/fylh0aosvhx59ot4hklr 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/dyrempkhdvctmdjnlgoj 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/jziq1gcecqs62krao5b8 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/fbokbimrgqkosrrv9nqj 1600w\"><figcaption>Dissertation structure</figcaption></figure><p>&nbsp;</p><p>The <a href=\"https://docs.google.com/spreadsheets/d/1_JKlkmv5Yu9bLUrpiBWibTOPbth56pcLvk6PbKWypgk/edit?usp=sharing\">Wellbeing Worksheet</a> can be used to evaluate existing confined spaces, enhance existing spaces, and create new ones. Applying the design guide, I proposed a research station simulating life in confined spaces in extreme environments (<a href=\"https://drive.google.com/file/d/1OZCFLY8zAchfZSe6aI7v0Yn4Muy8HnHM/view?usp=share_link\"><u>plan view</u></a> of the final proposal. The proposal is covered in depth in Chapter 6. I also designed two more confined iterations shown in Chapter 5). The proposed station aims to examine the social dynamics and psychological aspects of living in confinement while providing training for crewmembers.&nbsp;</p><p>&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/hbwx8qgowlczdehzlyq1\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/hoa2nknhssaxdtgc7ygi 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/mysi0axrlxz15eb9hklg 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/np1zdwnuicbwxpmahb37 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/clqegqovwuvyq69iiogu 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/nhtp6pp0d5guctcexbow 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/xenmmbdqi6mo9vnty1ii 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/o4tu4u9gicuwekm3mehb 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/wa9hff6ob5mqxpfdcrbb 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/fxwcd39gkamsqbbwidxa 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/egx3rz8zs2ht4etffxzf 1600w\"><figcaption>Reading the Wellbeing Worksheet</figcaption></figure><p>&nbsp;</p><h3>Methodologies and methods</h3><p>I adopted a mainly qualitative research approach, using Maslow\u2019s hierarchy to examine confined spaces in extreme environments. The primary research methodology is the case study approach.<strong> Three cases - a nuclear bunker (Kelvedon Hatch, UK), a submarine (HMS Alliance, UK), and a warship (HMS Belfast, UK)</strong> - have been selected for relevance to the research question and firsthand site visit experience. Cases are evaluated using archival data and direct observation, with desktop analysis of other precedents supplementing findings.<br>&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/jsnhmtlqmejqlcupj6j2\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/iqajsfvmstfxfkrjisza 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/ptnl6oqbosu9c5jxrmey 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/phsxjwrnsegloh0qzlrc 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/makp8qfouwqxrvvscwdi 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/tczfrrdkf04qkjmsoxrf 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/kiv8lerfjnb3wkzktnit 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/hzzhlldwh83th9jmhba8 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/tlsyovpc5hwkrpienre2 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/vcyvzfcww2tkhswsan4u 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/rcymzw6mw2zsxpyybqpb 1600w\"><figcaption>Maslow\u2019s Hierarchy of Needs</figcaption></figure><p>&nbsp;</p><p><strong>Employed research methods include:</strong></p><ul><li><strong>Archival Data Analysis:&nbsp;</strong>Using various data sources to gain insight into occupants\u2019 experiences.</li><li><strong>Direct Observation:</strong> Observing case studies firsthand to identify design strategies.</li><li><strong>Application of the Wellbeing Worksheet:</strong> Applying the Wellbeing Worksheet (Chapter 7) as a systematic method for assessing and designing confined spaces.</li><li><strong>\u2018Day in the Life\u2019 Method:&nbsp;</strong>Imagining moments within case studies to inform design development.<br>&nbsp;</li></ul><h3>Conclusions and key findings</h3><p>I explored the role of architectural and spatial design strategies in creating confined spaces within extreme environments that improve social dynamics. I developed a framework and design guide and came to appreciate the significance of meeting and surpassing basic human needs, such as physiological, safety, love and belonging, esteem, and self-actualisation needs (Maslow, 1943). My case study analysis revealed key strategies for enhancing social dynamics in confined environments, including providing larger spaces, ensuring lower densities, incorporating variety and flexibility, and fostering a shared sense of purpose (Harrison et al., 2012).</p><p>&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/tm86nm21hywkwlkiazfv\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/d1wmyvgt4ztrxalnmtz6 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/duzdqg1zrxxqfmqi22xt 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/yrfd2pujadhyvoht41rx 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/va3ghqhtlvuph57pm67u 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/f9ww7icgrwv6f3dxjogv 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/ga8vyzklfvohn3b7mldc 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/rom0ug2qu9mu1lk0qreo 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/gnuoesqwyahqnoaoosuw 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/jrf4dyze80szkpaksdnb 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/zuifxk43ikdjbwtzqj2n 1600w\"><figcaption>Answering the Research Question</figcaption></figure><p>&nbsp;</p><p>The design guide was tested by designing a research station simulating life in extreme conditions, which could further test social dynamics and psychological aspects of living in confinement and provide training for crewmembers.</p><p>&nbsp;</p><p><strong>Key findings</strong> related to how architectural strategies can be employed to meet or surpass individual human needs and consequently improve social dynamics include:</p><ul><li>Meeting&nbsp;<strong>physiological needs</strong> by providing oxygen, food, water, and sleep facilities. To surpass these needs, offer daylight or its imitation, ventilation and filtration systems, heating and cooling, medical care, and nourishing food. Surprisingly, personal comfort alone does not significantly impact social dynamics.</li><li>Addressing&nbsp;<strong>safety needs</strong> by implementing monitoring systems, secure locks and access control, fire and smoke detection, emergency lighting and alarms, and fire suppression systems. Also, consider structural integrity and provide secure alternatives in case of key system failures.</li><li>Fulfilling<strong> love and belonging needs</strong> by respecting crew privacy, avoiding crowding, and designing areas promoting social interaction and collaboration, such as shared eating areas. Group rituals, consistent rules, expectation management, and activities like cooking and eating in shared spaces foster connections.</li><li>Meeting&nbsp;<strong>esteem needs</strong> by offering opportunities for individual achievement, recognition, and rewards. A shared sense of purpose is vital for satisfying esteem needs. Additionally, providing diverse spaces and activities helps prevent boredom, while adaptability and flexibility create universal spaces catering to personal preferences.</li><li>Addressing&nbsp;<strong>self-actualisation needs</strong> by granting access to resources and activities that encourage creativity and spiritual contemplation. Consider different personalities, allowing for highly social as well as private and quiet activities. This includes meditation and spiritual contemplation spaces, libraries, virtual reality, workshops, and comfortable common areas for discussions.</li></ul><p>&nbsp;</p><p>I have tried to fill a knowledge gap by providing a <a href=\"https://docs.google.com/spreadsheets/d/1_JKlkmv5Yu9bLUrpiBWibTOPbth56pcLvk6PbKWypgk/edit?usp=sharing\">design guide</a> for architects and designers to evaluate and enhance existing spaces or inform new confined environment designs. Moreover, I have demonstrated the relevance and application of these design strategies in real-world settings, contributing to the well-being and success of those living and working in extreme conditions (Harrison et al., 2012).</p><p>&nbsp;</p><h3>HMS Belfast: Examples of Findings from Successful Case Studies</h3><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/gk8zwitloo5virotlkrv\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/gb7ldkvbfzj4atmyp6fz 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/oog1bhckraswuk3jxbfh 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/hlbiz0rhvazcyztx7h9u 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/ojrbhajr7ethczttnpxd 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/kjctisztc7zitgcq12lf 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/v0zvcqgbb2vuaoxescdy 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/zympkynkc6evetfdhtec 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/p0lhqyae7rudh95f5pmy 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/y5o1s3jxavjd63mspcra 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/vw8pgx3pjniq6z5cp8c2 2048w\"><figcaption>HMS Belfast; Overview</figcaption></figure><p>&nbsp;</p><p>Some surprising learnings came from analysing confined spaces with rich social structures and well-functioning social dynamics, such as the UK warship HMS Belfast.<br>&nbsp;</p><p>Launched in 1938 and decommissioned in 1961, HMS Belfast offered various spaces to meet inhabitants\u2019 needs, including a medical centre, canteen, laundry, movie theatre, and chapel&nbsp;<a href=\"https://paperpile.com/c/VJ4xkv/iguY\">(Asbury, 2019)</a>. The ship\u2019s size enabled movement, exercise, and outdoor time, contributing to the crew\u2019s well-being. Personal accounts highlight strong relationships formed among crew members, with competition between groups fostering bonds. A shared purpose, clear social hierarchy, and the military chain of command facilitated conflict resolution.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/shtkkaifgzyw50k3yz7g\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/d23fn5ikd8gzr40j50rs 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/q5ikeeuekxner8p0dtkb 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/bdvwdz458skllfmxtams 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/ofiooccjrupl7aws9bfn 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/anwyhglrw6n4jo4m4wzb 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/quk7qsoog6zxwhvece5e 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/kbv2fswndsf5gfervtg0 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/rjucwd1suyaicwk8n1fd 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/p4gluzvc8mpbqunl9rpe 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/ylf1ficuqvhkaptj5fdb 2048w\"><figcaption>HMS Belfast; Rating Overview</figcaption></figure><p>&nbsp;</p><p>With 308 points scored, HMS Belfast was the most successful case study analysed. This result corresponds with strikingly positive testimonials from the ship. It might be due to the abundance of amenities fulfilling physiological and self-actualisation needs, but also due to a strong sense of purpose and comradeship. Furthermore, access to the outdoors was a huge benefit, providing fresh air and natural light.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/bp6batva1i0iuayh5vzn\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/zwx4yek8enrwrxc3mvr7 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/qjcvn3opdhgwnzilzjct 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/sxioc4qf4j9wkvkppvrf 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/eelmsjabcnufpg6ip7sg 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/sji1jrdpqombi4xtg47v 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/igr3s2bt9tdvkb79qyjq 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/ge2klefw1vnbzu3gu3ec 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/ffyuxtqe3c49jhrilzhj 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/eecvr07yvshnnmmyhcd2 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vTcmucF4XmBLXbDqM/sfw5xzjfpgw3p6facel0 2048w\"><figcaption>HMS Belfast; Rating Details</figcaption></figure><p>&nbsp;</p><h3>Limitations and Future Research</h3><p>This dissertation has limitations that require further research:</p><ul><li>Theoretical context: The study relies on a theoretical framework and case studies, but empirical research could strengthen the findings by validating the effectiveness of the design guide in improving social dynamics in confined spaces.</li><li>Lack of empirical testing: The proposed design guide has not been tested in real-life situations, limiting its validation. Constructing and testing spaces designed using the Wellbeing Worksheet can explore the impact of architectural strategies on social dynamics.</li><li>Limited scope of case studies: Additional case studies from different extreme environments would enhance understanding of the design strategies' applicability.</li><li>Duration of confinement: The relationship between confinement duration and the importance of self-actualisation needs further exploration, especially in case studies involving extended habitation.</li><li>Cost-effectiveness analysis: Future research should consider cost in scoring mechanisms.</li></ul><p>&nbsp;</p><h3>If you want to learn more</h3><ul><li><a href=\"https://online.fliphtml5.com/icicp/pnkl/\"><u>Flip through my dissertation</u></a> to learn more.</li><li>Or, see the&nbsp;<a href=\"https://drive.google.com/file/d/1Yn0_s5YEMRpIBaLft01Ui6JliCUnLacu/view?usp=sharing\"><u>pdf on Google Drive</u></a> to make comments directly in the document. Any comments are welcomed.</li><li>Or watch a&nbsp;<a href=\"https://www.youtube.com/watch?v=tiR_Y5tpQcs\"><u>video</u></a> where I guide you through it.</li></ul><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=tiR_Y5tpQcs\"><div><iframe src=\"https://www.youtube.com/embed/tiR_Y5tpQcs\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>If you are interested in learning more, using my design guide or applying my research, feel free to reach out to me at&nbsp;<u>tereza.flidrova13@gmail.com</u>, I would be excited to hear from you.&nbsp;</p><p>&nbsp;</p><h2>Part 2:&nbsp;<a href=\"https://docs.google.com/document/d/1ftffJdZhyfCD0xnHUdyEyWSrzbGexyTiYm_7B_vdg-8/edit?usp=sharing\"><u>Shelters Precedents Report Draft</u></a>&nbsp;</h2><p><a href=\"https://docs.google.com/document/d/1ftffJdZhyfCD0xnHUdyEyWSrzbGexyTiYm_7B_vdg-8/edit?usp=sharing\"><u>This report</u></a> offers a summary of precedents that have the potential to inform the design and processes of shelters. It is a tool that allows comparing different aspects of designs, like layouts, sizes, construction methods and costs, and, where relevant, draws parallels to shelters.<br>&nbsp;</p><p>Where relevant, this report also incorporates findings from key writings on shelters to date.&nbsp;</p><p>&nbsp;</p><p>It's been noted that we might need a diversity of different typologies of shelters. This report provides a sample of the existing variety and suggests a direction for future research.</p><p>&nbsp;</p><p><strong>The precedents this report focuses on are:</strong></p><ul><li>Continuity of Government Bunkers&nbsp;</li><li>Government Bunkers for Citizens&nbsp;</li><li>Private Shelters&nbsp;</li><li>Research Stations in Remote Locations</li><li>Research Experiments (like Biosphere 2)</li><li>Laboratories (BSL-4)&nbsp;</li><li>Island Refuges</li><li>Peoples Living in Seclusion</li><li>Submarines</li><li>Space Exploration</li><li>Seed Banks<br>&nbsp;</li></ul><p><strong>For each case study, there are the following sections that allow cross-comparison:</strong></p><ul><li>Quick facts&nbsp;</li><li>Why is it relevant? \u2699\ufe0f</li><li>Sources \u2139\ufe0f</li><li>Videos \ud83c\udfa5</li><li>Type</li><li>Location\ud83d\udccd</li><li>Size \ud83d\udccf</li><li>Capacity \ud83d\udc6d</li><li>Tags</li><li>Year built \u23f3</li><li>Owner</li><li>Supplier \ud83d\udc77&nbsp;</li><li>Costs \ud83d\udcb0</li><li>Layout</li><li>What threats does it protect from? \ud83d\udca3</li><li>Safety and design features \u26a0\ufe0f`</li><li>Fun facts \ud83e\uddec</li><li>Main design limitations \ud83d\udeab</li><li>People to reach out to&nbsp;<br>&nbsp;</li></ul><p>The report also includes Editor\u2019s picks to direct the reader to intriguing or relevant findings. I enabled anyone to comment on the document and created&nbsp;<a href=\"https://forms.gle/EJQQHwoWHRXKs6GB9\"><u>this form</u></a> to allow anyone to submit information about additional precedents.<br>&nbsp;</p><p>Thanks a lot for reading this post, and if anyone is interested in this topic, feel free to reach out, I would be happy to chat.</p><p>&nbsp;</p><p>Tereza</p><p><br>&nbsp;</p><h3><i>Bibliography</i></h3><p>Asbury J (2019)&nbsp;<i>HMS Belfast</i>. IWM Publishing.</p><p>Bannova O (2014)&nbsp;<i>Extreme Environments: Design and Human Factors Considerations</i>. Available at:&nbsp;https://www.proquest.com/openview/074fe7a38f93be1a6ce496da9976afc0/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y<a href=\"http://paperpile.com/b/VJ4xkv/7Vjg\">.</a></p><p>Basner M, Babisch W, Davis A, et al. (2014) Auditory and non-auditory effects of noise on health.&nbsp;<i>The Lancet</i> 383(9925): 1325\u20131332.</p><p>Basner M, Dinges DF, Mollicone DJ, et al. (2014) Psychological and behavioral changes during confinement in a 520-day simulated interplanetary mission to mars.&nbsp;<i>PloS one</i> 9(3): e93298.</p><p>Beckstead N (2015) How much could refuges help us recover from a global catastrophe?&nbsp;<i>Futures</i> 72: 36\u201344.</p><p>Graff GM (2017)&nbsp;<i>Raven Rock: The Story of the U.S. Government\u2019s Secret Plan to Save Itself--While the Rest of Us Die</i>. Simon and Schuster.</p><p>Harrison A, Sommer R, Struthers NJ, et al. (1985) Implications of privacy needs and interpersonal distancing mechanisms for space station design. Available at:&nbsp;<a href=\"https://www.semanticscholar.org/paper/8f53f718f073c0553950571708550889acf558e5\">https://www.semanticscholar.org/paper/8f53f718f073c0553950571708550889acf558e5</a><a href=\"http://paperpile.com/b/VJ4xkv/4dZu\"> (accessed 27 October 2022).</a></p><p>Jebari K (2015) Existential risks: exploring a robust risk reduction strategy.&nbsp;<i>Science and engineering ethics</i> 21(3): 541\u2013554.</p><p>Karthaus R, Block L and Hu A (2019) Redesigning prison: the architecture and ethics of rehabilitation.&nbsp;<i>Journal of Architecture</i> 24(2). Informa UK Limited: 193\u2013222.</p><p>Klitzman S and Stellman JM (1989) The impact of the physical environment on the psychological well-being of office workers.&nbsp;<i>Social science &amp; medicine</i> 29(6): 733\u2013742.</p><p>Levin H (1995) Physical factors in the indoor environment.&nbsp;<i>Occupational medicine&nbsp;</i>&nbsp;10(1): 59\u201394.</p><p>Lily Bernheimer, Rachel O\u2019Brien, Richard Barnes (2017)&nbsp;<i>Wellbeing in Prisons: A Design Guide</i>.</p><p>Nelson M, Dempster W, Alvarez-Romo N, et al. (1994) Atmospheric dynamics and bioregenerative technologies in a soil-based ecological life support system: initial results from Biosphere 2.&nbsp;<i>Advances in space research: the official journal of the Committee on Space Research&nbsp;</i>&nbsp;14(11): 417\u2013426.</p><p><i>NPR</i> (2011) The Secret Bunker Congress That Never Used. 26 March. NPR. Available at:&nbsp;<a href=\"https://www.npr.org/2011/03/26/134379296/the-secret-bunker-congress-never-used\">https://www.npr.org/2011/03/26/134379296/the-secret-bunker-congress-never-used</a><a href=\"http://paperpile.com/b/VJ4xkv/OgZm\"> (accessed 13 June 2022).</a></p><p>Palinkas LA (2003) The psychology of isolated and confined environments. Understanding human behavior in Antarctica.&nbsp;<i>The American psychologist</i> 58(5): 353\u2013363.</p><p>Rashid M and Zimring C (2008) A Review of the Empirical Literature on the Relationships Between Indoor Environment and Stress in Health Care and Office Settings: Problems and Prospects of Sharing Evidence.&nbsp;<i>Environment and behavior</i> 40(2). SAGE Publications Inc: 151\u2013190.</p><p>Zabel B, Hawes P, Stuart H, et al. (1999) Construction and engineering of a created environment: Overview of the Biosphere 2 closed system.&nbsp;<i>Ecological engineering</i> 13(1): 43\u201363.</p><p><br>&nbsp;</p>", "user": {"username": "Tereza Fl\u00eddrov\u00e1"}}, {"_id": "tTcwbafbTxDHYbFhh", "title": "Free One-to-One Coaching for Procrastination (~60 Spaces Available)", "postedAt": "2023-06-07T10:57:29.264Z", "htmlBody": "<p>We offered free behavior change coaching for EAs, but one problem predominated - procrastination. In hindsight, it's not surprising.&nbsp;</p><p><strong>Base rate:</strong> 20% of the population are chronic procrastinators.&nbsp;<br><strong>Risk factors:</strong> youth, studying or working at university, ADHD, depression, anxiety.&nbsp;</p><p><strong>Obvious problems that procrastination causes us:</strong></p><ul><li>Work gets rushed</li><li>More time spent on less impactful (but less scary) things</li><li>More deadlines missed</li></ul><p><strong>I think the majority of the harm, though, is less obvious:</strong></p><ul><li>Staying up late to hit deadlines disturbs sleep and raises the risk of mental illness and burnout</li><li>Missed deadlines -&gt; delay of all tasks dependent on it</li><li>Procrastination-inducing tasks are less likely to be completed and are thus more neglected. <a href=\"http://www.paulgraham.com/schlep.html\">The most successful startup incubator suggests that its incubatees deliberately choose to create products that are unappetizing to create for this exact reason!</a>&nbsp;</li></ul><p><strong>To address this, we're going to create a new service exclusively focused on addressing procrastination.&nbsp;</strong></p><ul><li>Four free sessions, 30-45 minutes each, via Zoom, over one fortnight.<ul><li>~50% of users report a &gt;50% reduction in procrastination by the end</li><li>Caveat: n =~15</li></ul></li><li>If you really like your coach and want to continue, you can keep them for ~$30 per session thereafter (all profits going to fund mental health interventions within EA or LMICs)</li><li>Rapid turnaround: we expect to get in touch within 48Hs of you completing the form, and have appointments available within one week of form completion.&nbsp;</li></ul><p><strong>Damn, that sounds epic, how come it's free?!</strong></p><p>Philanthropic funding + most of our coaches are volunteers. In return for being taught how to coach people effectively for free, they offer free coaching/therapy to our clients).</p><p><strong>Sign me up!</strong></p><p><a href=\"https://forms.gle/D7vbfymdQtbmab546\">Click here</a> (~60 seconds form). We have the capacity for ~60 clients. We'll prioritize based on severity and other factors, if in doubt please apply!)</p>", "user": {"username": "John Salter"}}, {"_id": "dbaai2HjyzoyPX9rQ", "title": "Reflective Equilibria and the Hunt for a Formalized Pragmatism", "postedAt": "2023-06-07T22:55:09.813Z", "htmlBody": "<p>In my more contemplative moments, when I really ask myself how I should act or what world I want to live in, I find it almost impossible. Not only do I have to make descriptive predictions about complex non-linear systems, I have to make normative judgements about which of those outcomes to prefer.&nbsp; The more I think about it, the less I\u2019m sure that any definite conclusions can be drawn.&nbsp;</p><p>At the same time, it still seems possible to make progress. Utilitarians, for example, were among the first in the west to argue for gender equality, decriminalization of homosexuality, and animal rights. I\u2019m much more sure than not that these were good developments for the world. This seems to point to utilitarianism being a useful tool in at least <i>some </i>contexts.&nbsp;</p><p>In the area of prediction, we are also starting to make progress. Research on forecasting has found ways to hold experts accountable for their predictions, and allowed us to identify <i>superforecasters</i> who can make much better predictions than the average expert.</p><p>Given this progress, it might be tempting to try to find the normative equivalent of forecasting. But while forecasts can be judged against the actual outcome they\u2019ve predicted, there is no observable \u201cground truth\u201d to normative questions. It\u2019s intuitions all the way down.&nbsp;&nbsp;</p><p>This leaves us in a situation somewhat analogous to what Ought has <a href=\"https://ought.org/updates/2022-04-06-process\">described</a> as <i>process </i>rather than <i>outcome.&nbsp; </i>This post focuses specifically on the process of reaching what Rawls has called a reflective equilibrium. While I don\u2019t think it\u2019s actually possible to reach such an equilibrium, I do believe it is at least possible to get <i>closer</i>.&nbsp;<br>&nbsp;</p><p>This blog post is part survey paper, part research plan, and part request for feedback. I\u2019m by no means an expert in any of these things (my background is in computer science), and I\u2019ll resort to examples more than formal arguments to try to get my point across. I\u2019m not sure exactly what here is original as both Ought and <a href=\"https://distill.pub/2019/safety-needs-social-scientists/\">Irving &amp; Askell</a> &nbsp;seem to be thinking along fairly similar lines, but I haven\u2019t seen it written up in this way and hope this post can serve as the start to a discussion. And while my main focus will be on moral theories, anything I say should apply to all forms of normative theories and so I tend to use the words interchangeably here.&nbsp;</p><p>&nbsp;</p><p>The rest of this post will focus on the following:&nbsp;&nbsp;</p><ol><li>Why reflective equilibrium is hard to reach</li><li>How pragmatism can help us</li><li>Attempts to formalize these ideas</li></ol><p><br>&nbsp;</p><h1><strong>Reflective Equilibrium is Hard to Reach</strong></h1><p>To describe the methodology of Reflective Equilibrium, I\u2019ll borrow the description from utilitarianism.net:</p><p><i>[Reflective Equilibrium] involves balancing two broad kinds of evidence as applied to moral theories:</i></p><ol><li><i>Intuitions about specific cases (thought experiments).</i></li><li><i>General theoretical considerations, including the plausibility of the theory's principles or systematic claims about what matters.</i></li></ol><p><i>General principles can be challenged by coming up with putative counterexamples, or cases in which they give an intuitively incorrect verdict. In response to such putative counterexamples, we must weigh the force of the case-based intuition against the inherent plausibility of the principle being challenged. This could lead you to either revise the principle to accommodate your intuitions about cases or to reconsider your verdict about the specific case, if you judge the general principle to be better supported.</i></p><p>&nbsp;</p><p>I think this process is a good one, but I believe the gap between intuitions and pure theory is large. As large as, say, the gap between quantum physics and aerospace engineering. Although I value thought experiments as a useful philosophical tool, real intuitions come from real experience in the messy, complicated world.</p><p>Take, for example, Robert Nozick\u2019s famous <a href=\"https://www.britannica.com/biography/Robert-Nozick/The-entitlement-theory-of-justice#ref828038\">Wilt Chamberlain argument</a>, which I am simplifying slightly for brevity. Nozick postulates a hypothetical society in which everyone has equal access to resources. In this society, people willingly pay to attend a basketball game in which Wilt Chamberlain is playing. This results in Chamberlain getting more money than any of the spectators, moving us from a society of equal distribution to one of unequal distribution.&nbsp;</p><p>Nozick argues that because the starting society was equal and people willingly chose the method of distribution, then the resulting society must be a just one. And within the scope of this thought experiment, I agree with him. But there are a few problems with it. For one, Libertarians will often conveniently ignore the egalitarian starting point in the argument and use this example to argue that we should allow any contract that is freely entered in our current society. More important to the context of this post, though, this \u201cfree exchange\u201d could lead to outcomes no one would agree with. If a world of free exchange led to a single company having a complete monopoly on all our resources, would we really care about the Wilt Chamberlain example?&nbsp;</p><p>My point is not that it is wrong to willingly give Wilt Chamberlain money, or that there are not many related cases where free exchange of property is okay. <strong>My point is that we should be able to note contradictory pieces of normative intuitions without having to immediately generalize them to first principles.&nbsp; A thought experiment is a data point, and we need lots of data points in order to build good theories.&nbsp;</strong></p><p>When I interact with my friends, I don\u2019t rely on psychology research or expected-value calculations to guide my actions. I think about their needs and balance them with my own so we can continue to have a good friendship. This doesn\u2019t contradict utilitarianism. Given the complex social machinery in my brain, the holistic approach to human interactions seems more likely to \u201cmaximize utility\u201d for the time I spend with my friends. It\u2019s <strong>the right tool for the job</strong>. I can then step back and ask, in a utilitarian (and hopefully guilt-free) way, whether I\u2019m spending too much time hanging out. I believe moving between these different tools is better than applying a single tool at all levels.<br><br>&nbsp;</p><h1><strong>Pragmatic, Modular, and Interdisciplinary&nbsp;&nbsp;</strong></h1><p>This idea of using \u201cthe right tool for the job\u201d is not new to philosophy. It is common<strong> </strong>to the school of pragmatism, which wikipedia defines as:</p><blockquote><p><i>[The] philosophical tradition that considers words and thought as tools and instruments for prediction, problem solving, and action, and rejects the idea that the function of thought is to describe, represent, or mirror reality.</i></p></blockquote><p>That last part might raise some eyebrows, but I think going into the subtleties of this definition of truth might be outside the scope of this post. For now, you\u2019ll have to trust me that this is not some hand-wavey \u201ctruth is meaningless\u201d type of philosophy. It is a serious analytic school that is practiced by logicians and philosophers of science from Quine to Putnam to Peirce.&nbsp;</p><p>One pragmatist, Elizabeth Anderson, has built her approach to ethics based on <a href=\"\">analogy</a> to the philosophy of science:&nbsp;</p><blockquote><p><i>Philosophers of science don\u2019t think we can come up with principles of science outside of actual empirical investigation&nbsp; \u2026 So why should we think we can come up with actual principles of ethics without looking at the actual problems that people confront in their actual lives, and how that changes historically as we have to meet new challenges?</i></p></blockquote><p><br>Anderson is maybe most famous for her theory on equality, which focuses on the relationships between people in a society rather than to the distribution of resources or application of specific deontological rules. This definition is difficult to link up to any more theoretical moral theory, but that does not mean it contradicts them. For example, If a society that adopted her definition of equality was found to have an overall happier population, we would probably consider it a good practice of utilitarianism.&nbsp;<br>&nbsp;</p><p>We can keep Anderson's analogy to science going. Before Newton, physicists were able to find mathematical laws about pendulums and planetary orbits. These laws are not \u201chard-coded\u201d into the universe, they are intermediate instances of the more fundamental law of gravity, which is in turn an instance of even more fundamental laws. In a similar way, <strong>we can look to sociologists and on-the-ground philosophers to try to find intermediate moral theories that we can then test against more abstract foundational ones.&nbsp; </strong>This is an technique that has been advocated elsewhere, including research on <a href=\"https://journals.sagepub.com/doi/full/10.1177/0162243917723153\">technology studies</a> and <a href=\"https://books.google.com/books?hl=en&amp;lr=&amp;id=kLmnDQAAQBAJ&amp;oi=fnd&amp;pg=PR15&amp;dq=moral+philosophy+economic+analysis&amp;ots=7rmGWCxzFI&amp;sig=HpGsAlc_TmhpwpIf4vp_Wt-OWKI#v=onepage&amp;q=moral%20philosophy%20economic%20analysis&amp;f=false\">economic analysis</a>.</p><p>A key difference between the physics analogy and the normative questions we face is that physics can make explicit predictions. If our model of pendulums is wrong, it will yield bad predictions, but what does it mean for a normative theory to be wrong?&nbsp;</p><p><br>This is where another pragmatist, Susan Haack can be helpful. Haack has made the <a href=\"https://en.wikipedia.org/wiki/Susan_Haack#Philosophical_work\">analogy</a> between knowledge and a crossword puzzle, where:</p><blockquote><p><i>&nbsp;Finding an answer using a clue is analogous to a foundational source (grounded in empirical evidence). Making sure that the interlocking words are mutually sensible is analogous to justification through coherence.&nbsp;</i></p></blockquote><p><br>&nbsp;</p><p>By looking at different types of real world ethical \u201cdata\u201d, such as people\u2019s intuitions, we can come up with intermediate theories. If these theories \u201ccohere\u201d meaning they point towards the same more formal theory (say, utilitarianism) that is good evidence for the validity of that formal theory,&nbsp; If they disagree, that might be evidence to study the intermediate theories in more detail.&nbsp; <strong>By moving back and forth from normative intuitions to intermediate theories to more formal theories, we can come up with better and better approximations of normative theories.&nbsp;</strong></p><p>A fitting analogy might be found in Derek Parfit\u2019s last book, \u201cOn What Matters\u201d, which attempted to show that the major theories of utilitarianism, contractarianism, and deontology all converge rather than disagree. In his words, each of these theories were <i>climbing the same mountain from different sides</i>.</p><p>&nbsp;</p><p>Having sketched out my views on the role of pragmatism in ethics, I\u2019ll conclude this post with a discussion about how to use more formal methods to incorporate these views.&nbsp;</p><p>&nbsp;</p><h1><strong>The Path to Formalization</strong></h1><p>There are many ways one could use the pragmatism described above, and not all of them should be mathematical. At the same time, Bayesian Reasoning and techniques from forecasting have done a lot to help clarify our own thinking. Finding a formalism for measuring coherence and \"back and forth\" between normative theories could help us find our own biases and clarify what is important to us. &nbsp;&nbsp;</p><p><br>The tool I will describe here, I have tentatively called <i>normative maps. </i>The goal of these maps is not to have something to point to and say \u201c<a href=\"https://miro.medium.com/max/1400/1*C-hi0-3tmhMt3rQL0MD7RQ.png\">See? We found the right answer using math</a>\u201d. I expect these maps to help us update more on orders of magnitude rather than fine-grained levels of certainty.&nbsp; I hope that anyone using them will be starting from a point of deep humility about the scope and complexity of the problems we are facing.&nbsp; Irving &amp; Askell, in their <a href=\"https://distill.pub/2019/safety-needs-social-scientists/#questions\">paper</a> on using Social Science in AI Safety, made a relevant analogy to Condorcet\u2019s Jury Theorem, which states that by taking the majority vote of independent people slightly likely to be correct we get a judgment that is very likely to be correct, but the majority vote of people who are slightly likely to be wrong will be very likely to be wrong. In the same way, the formalism I will describe here could get a diverse group of thoughtful people to <i>much better judgements</i>, but could also help people find post-hoc justifications for their own biases.&nbsp;&nbsp;</p><p><br>Here, I\u2019ll sketch out a few steps in a project that I believe could do this. I will order them from most formal to most practical.&nbsp;</p><p>&nbsp;</p><h3><strong>(1) Updating Credences in Normative Theories:</strong></h3><p>The book <a href=\"https://www.williammacaskill.com/info-moral-uncertainty\"><i>Moral Uncertainty</i></a> by Bykvist, Ord, and MacAskill discusses methods of making decisions under uncertainty about the \u201ctruth\u201d of different moral theories. These decisions are mostly built around the idea of <i>maximal expected choiceworthiness</i>, which is an extension of maximizing expected utility to moral domains.&nbsp; This book assumes that credences in different moral theories are given and shows how to go from those credences to individual decisions.&nbsp;</p><p>I consider this to be very important work, but I've seen much less work on <i>how </i>these credences should be formed. By attempting to formalize this process, we can allow philosophers to check their own biases against the real world \u201cdata\u201d they are getting.&nbsp; I believe this will involve a process very similar to bayesian updating. Unlike the moral uncertainty work, determining credences in theories will rely on credences in intuitions and the choiceworthiness of actions.&nbsp;</p><p>The \u201cright\u201d formalization here will likely be very difficult to find, but there are a few things we could expect it to have:</p><ul><li>If a moral theory leads to a very unintuitive consequence, I should be both less sure of the moral theory and slightly more sure of the consequence.</li><li>All other things being equal, there should be higher priors in easier-to-state theories (this is essentially, Occam\u2019s Razor).</li><li>As more theories/evidence point towards the same consequence, credence in the consequence should increase. The less correlated these theories/evidence are with each other, the more the consequence should increase.</li></ul><p>There would also need to be research into how to carry over credences when splitting up theories into subtheories (If we're 80% certain in some form of utilitarianism, how certain are we in preference utilitarianism? How should the arguments for utilitarianism map over?). In a sense every moral question we have, down to intuitions in a specific situation or thought-experiment are just subtheories of some other theory.&nbsp;</p><h3><strong>(2) Network Formalism:</strong></h3><p>Given a way of updating credences, we\u2019d then need to find a formalism that allows us to map the large space of intuitions and evidence we might encounter. I expect this will look a lot like <a href=\"https://en.wikipedia.org/wiki/Bayesian_network\">Bayesian Networks</a> and would have the following components:&nbsp;</p><ul><li><strong>Nodes: </strong>This will be anything you can have a credence in such as: people, theories, statements, intuitions, actions. They will have both a natural-language description and a machine-checkable logical statement. The underlying semantics of the statement will likely be a set of preferences or a function onto real numbers, similar to the semantics of theories in <i>Moral Uncertainty. &nbsp;</i></li><li><strong>Edges: </strong>The influence certain nodes have on other nodes, likely represented as a number. This might incorporate ideas of \u201capplicability/closeness\u201d between concepts. For example, I trust Peter Singer\u2019s opinions on animal welfare more than I trust his opinions on disability issues, so an edge from Peter Singer to a policy he recommends would be larger if it is an animal rights policy.</li><li><strong>Credences: </strong>A number between 0 and 1 indicating certainty in the truth of a node. It might be useful to incorporate more complicated probabilities such as those in <a href=\"https://en.wikipedia.org/wiki/Dempster%E2%80%93Shafer_theory\">Dempster-Shafer Theory</a> or <a href=\"https://en.wikipedia.org/wiki/Radical_probabilism#Conditioning_on_an_uncertainty_%E2%80%93_probability_kinematics\">Probability Kinematics</a>. Credences could come from a wide range of sources from personal opinions to peer-reviewed research.</li></ul><p>There are also some components which are more difficult to define, and might be approximable using the above three components:</p><ul><li><strong>Measure of cost: </strong>How much money or time or people would it take to accomplish something? How likely are our credences/nodes/edges to change given an input of resources? This latter question relates to the Moral Information chapter of <i>Moral Uncertainty</i>.</li><li><strong>Continuous/Large Variables: </strong>Sometimes the space of nodes or edges might have many values or be continuous. For example, a node for utilitarianism might not have a single utility function. It might have a continuous (or very large) space of different utility functions with different credences. Representing these maps might use techniques similar to those of <a href=\"https://en.wikipedia.org/wiki/Model_checking#Symbolic_model_checking\">symbolic model checking</a>.</li></ul><p>&nbsp;</p><h3><strong>(3) Software:</strong></h3><p>Given this formalism, we\u2019d then want to build a software system that allows us to map out our current understanding of a moral situation. This software will likely be built off of one of the many <a href=\"https://en.wikipedia.org/wiki/List_of_concept-_and_mind-mapping_software\">mind-mapping</a> projects out there.&nbsp;</p><p>One of the main features of this software will be the ability to \u201ccheck\u201d our maps for potential conflicts and biases. For example, if I state that I am a utilitarian, but I am giving more resources to something with less expected utility, it will bring it to my attention. It might be that I am falling prey to a <a href=\"https://psycnet.apa.org/record/2007-06350-001\">classical cognitive bias</a>. Or maybe it is a bias I am alright with, such as spending time helping a friend or family member. The important thing is not that all biases are wrong, it\u2019s that bringing them to our attention will allow us to reflect on them and make better decisions.&nbsp;</p><p>This software will also allow us to identify \u201cbottlenecks\u201d, where our reasoning is dependent on very few sources. For example, if we cite several people all with the same opinion, that is good evidence for that opinion. If their opinion is all based on the same study or example, however, we\u2019d be less sure. The software could point this out to us and allow us to judge how to better change our opinions. For this reason, it is likely that users will need to chose many of their labels from database of different concepts such as \u201cPeter Singer\u201d or \u201cAnimal Liberation\u201d.&nbsp; The field of \u201csocial epistemology\u201d will likely have many more examples of things to look out for.&nbsp;<br>&nbsp;</p><h3><strong>(4) Collection of Data and Reflection:</strong></h3><p>Finally, it will be important to use the system to collect data and user feedback. This data could allow us to find more cognitive biases or better priors for different theories. It\u2019s likely we won\u2019t be able to figure out what is interesting about the data until we see it. We can then take what we find and go back and alter anything as we see fit.</p><p>&nbsp;</p><h2><strong>Potential Problems &amp; Applications</strong>&nbsp;</h2><p>I\u2019ll conclude this post with a few last thoughts on potential problems and applications with the work.&nbsp;</p><h3><strong>Commitment and Accountability</strong></h3><p>Philip Tetlock's on Forecasting work is able to lessen the normal biases of human reasoning largely by keeping forecasters <a href=\"https://projects.iq.harvard.edu/lernerlab/files/lerner_tetlock_1999.pdf\">accountable</a>. &nbsp; Having forecasters commit to specific numerical credences to their predictions keeps them from using vague language in their predictions and claiming after-the-fact that they were right all along.&nbsp;</p><p>When filling in credences to these normative maps, we may be biased by knowing the credences we choose will influence us towards outcomes we don\u2019t want. There should be some form of \"commitment\" action where we lock in our own credences and map structure, before testing them in some way (say, by asking a friend for their credences). The psychological <a href=\"https://en.wikipedia.org/wiki/The_Honest_Truth_about_Dishonesty\">research in dishonesty</a> can likely be useful here in keeping us from lying to ourselves.&nbsp;</p><h3><strong>The Role of Trust</strong></h3><p>I mentioned that the formalism I\u2019m considering involves rating credences in different people. This might be considered a level of \u201ctrust\u201d in that person. For someone like Peter Singer, this might not be a problem; he\u2019s consciously chosen to be a public figure. But rating our level of trust in people we know might be psychologically damaging, especially if a person should find themselves given low-trust ratings by everyone in their community. For now, I would just say make these ratings private, but it will definitely be worth thinking about this in more detail.&nbsp;</p><p>Rating trust is also important because it will force us to justify who we do and don\u2019t trust. Researchers in Epistemic Injustice have identified many ways in which we are biased in who we do and don\u2019t trust. At the same time, some imbalance in trust is valuable in some situations; I trust a doctor more than the average person to tell me which vaccines I should take. By explicitly writing down these levels, we can be forced to justify them or change them.&nbsp; Checking our credences could involve, for example, performing a <i>minimal trust investigation</i> as outlined by Karnofsky.</p><h3><strong>Over-Reliance on Certain Maps</strong></h3><p>If a particular map a person produces becomes very popular, it could become much more influential than is justified. This could be a worse outcome than if the map didn\u2019t exist at all, since people who would otherwise trust their own opinion would instead trust the map.</p><h3><strong>Democracy</strong></h3><p>Research on <a href=\"https://en.wikipedia.org/wiki/Social_choice_theory\">Social Choice Theory</a> usually starts from a set of intuitive axioms and tries to find voting systems that satisfies these axioms. These axioms are often created by economics research without much social science to back them up. Using these maps, researchers can map out their axioms and try to test them against different sociological studies and qualitative judgements, leading to better axioms.&nbsp;</p><p>Research in <a href=\"https://en.wikipedia.org/wiki/Deliberative_democracy\">Deliberative Democracy</a>, rather than merely polling voters, has them deliberate together in order to try to come to a consensus on a particular political question. These maps could serve as a way of aggregating the opinions of the participants during this process.&nbsp;</p><h3><strong>AI Safety</strong></h3><p>This work could potentially have applications in AI safety. If these maps were to become more machine-understandable (likely through work in language processing), it's possible they could be given to an AI to bound the range of acceptable behaviors.&nbsp;</p><p>What's more likely, is that they could serve a role in solving the <a href=\"https://www.alignmentforum.org/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans\">Pointer's Problem</a> by helping humans find latent variables in their value system. For example, if people value security, they could create a map that specifies that as few people as possible should be killed. &nbsp;If they tried to incorporate policy that implemented complete government surveillance, the map might predict the user would approve. If the user did not, this could force them to incorporate a new value into the map, such as 'privacy' or 'freedom'. Or they could disagree without being able to articulate why. &nbsp;Either case could help in the discovery of human values. And although this <a href=\"https://www.aaai.org/AAAI22Papers/AAAI-6206.QiuL.pdf\">is already being done</a> by other researchers, the method described here could help to find more fine-grained values that would not appear in larger data sets, or to find difficult-to-articulate values that might not appear in language models.&nbsp;</p><p>Finally, these maps could help users delegate the creation of utility functions as described in the Indirect Normativity section of <a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.3y1okszgtslx\">this post</a> on Eliciting Latent Knowledge. The authors mention the possibility of delegating utility functions to 'future selves' or other people. The measure of 'trust' and 'applicability' as described in this post could help users determine who to delegate what tasks to.&nbsp;</p><p>&nbsp;</p><h2>Thanks</h2><p><br>Thank you for reading. As I mentioned above, this is all in a pretty early stage and I\u2019d appreciate any feedback. I'm in the process of looking for a place to continue this work and for potential collaborators or funding.&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "BenjaminCaulfield"}}]