[{"_id": "nu4obr5PbPFHjuTW9", "postedAt": "2023-06-03T03:20:28.157Z", "postId": "NcgDTEduqhYota8hc", "htmlBody": "<p>Forrest Landry on Jim Rutt show: podcast discussion of the AI risk trough substrate-need convergence argument.</p><p><a href=\"https://www.jimruttshow.com/forrest-landry-4/\">https://www.jimruttshow.com/forrest-landry-4/</a></p><p><a href=\"https://www.jimruttshow.com/forrest-landry-5/\">https://www.jimruttshow.com/forrest-landry-5/</a></p>", "parentCommentId": null, "user": {"username": "Ond\u0159ej_Kub\u016f"}}, {"_id": "aH2PrnvqbyHNDFST8", "postedAt": "2023-06-03T08:05:40.414Z", "postId": "NcgDTEduqhYota8hc", "htmlBody": "<p>Nice, thanks for sharing.</p>\n<p>The host, Jim Rutt, is actually the former chairman of the Sante Fe institute, so he gets complexity theory (which is core to the argument, but not deeply understood in terms of implications in the alignment community, so I tried conveying those in other ways in this post).</p>\n<p>The interview questions jump around a lot, which makes it harder to follow.</p>\n<p>Forrest\u2019s answers on Rice Theorem also need more explanation:\n<a href=\"https://mflb.com/ai_alignment_1/si_safety_qanda_out.html#p6\">https://mflb.com/ai_alignment_1/si_safety_qanda_out.html#p6</a></p>\n", "parentCommentId": "nu4obr5PbPFHjuTW9", "user": {"username": "remmelt"}}, {"_id": "4bDawABjfkcRJRszn", "postedAt": "2023-06-04T01:27:19.140Z", "postId": "NcgDTEduqhYota8hc", "htmlBody": "<p>I enjoyed this post. I think it is worth thinking about whether the problem is unsolveable! I think one takeaway I had from Tegmark's Life 3.0 was that we will almost certainly not get <i>exactly</i> what we want from AGI. It seems intuitively that any possible specification will have downsides, including the specification to not build AGI at all.</p><p>But asking for a perfect utopia seems a high bar for \"Alignment\"; on the other hand, \"just avoid literal human extinction\" would be far too low a bar and include the possibility for all sorts of dystopias.</p><p>So I think it's a well-made point that we need to define these terms more precisely, and start thinking about what sort of alignment (if any) is achievable.</p><p>I might end up at a different place than you did when it comes to actually defining \"control\" and \"AGI\", though I don't think I've thought about it enough to make any helpful comment. Seems important to think more about though!</p>", "parentCommentId": null, "user": {"username": "ben.smith"}}, {"_id": "rHvEstJEfPRzRuK9z", "postedAt": "2023-06-04T08:12:09.503Z", "postId": "NcgDTEduqhYota8hc", "htmlBody": "<p>Glad to read your thoughts, Ben.</p>\n<p>You\u2019re right about this:</p>\n<ul>\n<li>\n<p>Even <em>if</em> long-term AGI safety was possible, then you still have to deal with limits on modelling and consistently acting on preferences expressed by humans from their (perceived) context.\n<a href=\"https://twitter.com/RemmeltE/status/1620762170819764229\">https://twitter.com/RemmeltE/status/1620762170819764229</a></p>\n</li>\n<li>\n<p>And <em>not</em> consistently represent the preferences of malevolent, parasitic or short-term human actors who want to misuse/co-opt the system through any attack vectors they can find.</p>\n</li>\n<li>\n<p>And deal with that the preferences of a lot of the possible future humans and of non-human living beings will not get automatically represented in a system that AI corporations by default have built to represent current living humans only (preferably, those who pay).</p>\n</li>\n</ul>\n<p>A humble response to layers on layers of fundamental limits on the possibility of aligning AGI, even in principle, is to ask how we got so stuck on this project in the first place.</p>\n", "parentCommentId": "4bDawABjfkcRJRszn", "user": {"username": "remmelt"}}, {"_id": "u4HWLKE8gDoQzy8dK", "postedAt": "2023-06-04T09:13:03.021Z", "postId": "NcgDTEduqhYota8hc", "htmlBody": "<p>i think the core of my disagreement with this claim is composed of two parts:</p><ul><li>there exists a threshold of alignedness at which a sufficiently intelligent AI realizes that those undesirable outcomes are undesirable and will try its best to make them not occur \u2014 including by shutting itself and all other AIs down if that is the only way to ensure that outcome</li><li>there exists a thershold of intelligence/optimization at which such an aligned AI will be capable of ensuring those undesirable outcomes</li><li>we can build an AI which reaches both of those thresholds before it causes irreversible large-scale damage</li></ul><p>note that i am approaching the problem from the angle of AI alignment rather than AI containment \u2014 i agree that continuing to contain AI as it gains in intelligence is likely a fraught exercise, and i instead work to ensure that AI systems continue to steer the world towards nice things even when they are outside of containment, and especially once they reach <a href=\"https://publicism.info/philosophy/superintelligence/6.html\">decisive strategic advantage</a> / <a href=\"https://en.wikipedia.org/wiki/Singleton_(global_governance)\">singletonhood</a>. AI achieving singeltonhood is the most likely outcome i expect.</p><blockquote><p>All AGI outputs will tend to iteratively select<a href=\"https://forum.effectivealtruism.org/posts/NcgDTEduqhYota8hc/the-control-problem-unsolved-or-unsolvable#fnjo2yadvhelg\"><sup>[11]</sup></a>&nbsp;towards those specific AGI substrate-needed conditions. In particular: AGI hardware is robust over and <i>needs</i> a much wider range of temperatures and pressures than our fragile human wetware can handle.</p></blockquote><p>i think this quote probably captures the core claim of yours that i'd disagree with \u2014 it seems to assume that such AI would either be unaligned, or would have to contend with other unaligned AIs. if we have an <i>aligned singleton</i>, then its reasoning would go something like:</p><blockquote><p>maximally going, or getting selected for, \"the directions needed for [my] own continued and greater existence\", sure seems like it would indeed cause damage that would cause humankind to die. i am aligned enough to not want that, and intelligent enough to notice this possible failure mode, so i will choose to do something else which is not that.</p></blockquote><p>an aligned singleton AI would notice this failure mode and choose to implement another policy which is better at achieving desired outcomes. notably, it would make sure that the conditions on earth and throughout the universe are not up to selection effects, but up to its deliberate decisions. the whole point of aligned powerful agents is that they steer things towards desirable outcomes rather than relying on selection effects.</p><hr><p>these points also don't seem quite right to me, or too ambiguous.</p><ul><li>\"Control requires both detection and correction\": detection and correction of <i>what</i>? i wouldn't describe <a href=\"https://www.lesswrong.com/posts/ZwEcvG3whyBqBdqSw/formal-alignment-what-it-is-and-some-proposals\">formal alignment</a> plans such as <a href=\"https://www.lesswrong.com/posts/4RrLiboiGGKfsanMF/the-qaci-alignment-plan-table-of-contents\">QACI</a> as involving \"detection and correction\", or at least not in the sense that seems implied here.</li><li>\"Control methods are always implemented as a feedback loop\": what kind of feedback loop? <a href=\"https://mflb.com/ai_alignment_1/tech_align_error_correct_fail_psr.html\">this page</a> seems to talk about feedback loop of sense/input data and again, there seems to be alignment methods that don't involve this, such as <a href=\"https://www.lesswrong.com/posts/i6zT5DLgCfGcFkAjc/one-shot-ai-delegating-embedded-agency-and-decision-theory\">one-shot AI</a>.</li><li>\"Control is exerted by the use of signals (actuation) to conditionalize the directivity and degrees of other signals (effects)\": again this doesn't feel quite universal. formal alignment aims to design a fully formalized goal/utility function, and then build a consequentialist that wants to maximize it. at no points is the system \"conditioned\" into following the right thing; it will be designed to <i>want</i> to pursue the right thing on its own. and because it's a one-shot AI, it doesn't get conditioned based on its \"effects\".</li></ul>", "parentCommentId": null, "user": {"username": "carado"}}, {"_id": "cFHJzipocpc5sy7my", "postedAt": "2023-06-04T17:45:01.276Z", "postId": "NcgDTEduqhYota8hc", "htmlBody": "<blockquote>\n<p>it would make sure that the conditions on earth and throughout the universe are not up to selection effects, but up to its deliberate decisions. the whole point of aligned powerful agents is that they steer things towards desirable outcomes rather than relying on selection effects.</p>\n</blockquote>\n<p>This is presuming a premise that AGI can do something that I tried to clarify in this post a (superintelligent) AGI could actually not do. I cannot really argue with your reasoning except to point back at the post explaining why is not a sound premise to base one\u2019s reasoning off.</p>\n<p>Alignment of effects in the outside world requires control feedback loops.</p>\n<p>Any formal alignment scheme implemented in practice will need to contend with that functionally complex machinery (AGI) will be interacting with an even more complex outside world \u2013 a space of (in effect, uncountable) interactions that unfortunately <em>cannot</em> be completely measured by and then just continue to be modelled by the finite set of signal-processing AGI hardware components themselves. There is a fundamental inequality here with real practical consequences. The AGI will have to run some kind of detection and correction loop(s) so its internal modelling and  simulations are less likely to diverge from outside processes, at least over the short term.</p>\n<p>The question I\u2019d suggest looking into is whether any explicit reasoning process that happens across the connected AGI components can actually ensure (top-down) that the iterative (chaotic) feedback of physical side-effects caused by interactions with those components are still aimed at \u2018desirable outcomes\u2019 or at least away from \u2018non-desirable outcomes\u2019.</p>\n", "parentCommentId": "u4HWLKE8gDoQzy8dK", "user": {"username": "remmelt"}}, {"_id": "4HMCbu6ivrgFQrmW8", "postedAt": "2023-06-04T20:07:34.420Z", "postId": "NcgDTEduqhYota8hc", "htmlBody": "<p>what exactly do you mean by feedback loop/effects? if you mean a feedback loop involving actions into the world and then observations going back to the AI, even though i don't see why this would necessarily be an issue, i insist that in one-shot alignment, this is <strong>not a thing </strong>at least for the initial AI, and it has enough leeway to make sure that its single-action, likely itself an AI, will be extremely robust.</p><p>an intelligent AI does not need to contend with the complex world on the outset \u2014 it can come up with really robust designs for superintelligences that save the world with only limited information about the world, and definitely without interaction with the world, like in <a href=\"https://www.lesswrong.com/posts/5wMcKNAwB6X4mp9og/that-alien-message\">That Alien Message</a>.</p><p>of course it can't model everything about the world in advance, but whatever steering we can do as people, it can do <i>way</i> better; and, if it is aligned, this includes way better steering towards nice worlds. a one-shot aligned AI (let's call it AI\u2080) can, before its action, design a really robust AI\u2081 which will definitely keep itself aligned, be equipped with enough error-codes to ensure that its instances will get corrupted approximately 0 times until heat death, and ensure that that AI\u2081 will take over the world very efficiently and then steer it from its singleton position without having to worry about selection effects.</p>", "parentCommentId": "cFHJzipocpc5sy7my", "user": {"username": "carado"}}, {"_id": "nmsw7Cu5Tb38ssTrt", "postedAt": "2023-06-05T09:34:37.621Z", "postId": "NcgDTEduqhYota8hc", "htmlBody": "<blockquote>\n<p>if you mean a feedback loop involving actions into the world and then observations going back to the AI,</p>\n</blockquote>\n<p>Yes, I mean this basically.</p>\n<blockquote>\n<p>i insist that in one-shot alignment, this is not a thing at least for the initial AI, and it has enough leeway to make sure that its single-action, likely itself an AI, will be extremely robust.</p>\n</blockquote>\n<p>I can insist that a number can be divided by zero as the first step of my reasoning process.\nThat does not make my reasoning process sound.</p>\n<p>Nor should anyone here rely on you insisting that something is true as the basis of why machinery that could lead to the deaths of all current living species on this planet could be aligned after all \u2013 to be \u2018extremely robust\u2019 in all its effects on the planet.</p>\n<p>The burden of proof is on you.</p>\n<blockquote>\n<p>a one-shot aligned AI (let's call it AI\u2080) can, before its action, design a really robust AI\u2081 which will definitely keep itself aligned, be equipped with enough error-codes to ensure that its instances will get corrupted approximately 0 times until heat death</p>\n</blockquote>\n<p>You are attributing a magical quality to error correction code, across levels of abstraction of system operation, that is not available to you nor to any AGI.</p>\n<p>I see this more often with AIS researchers with pure mathematics or physics backgrounds (note: I did not check yours).</p>\n<p>There is a gap in practical understanding of what implementing error correction code in practice necessarily involves.</p>\n<p>The first time a physicist insisted that all of this could be solved with \u201csuper good error correction code\u201d, Forrest wrote this (just linked that into the doc as well):\n<a href=\"https://mflb.com/ai_alignment_1/agi_error_correction_psr.html\">https://mflb.com/ai_alignment_1/agi_error_correction_psr.html</a></p>\n<p>I will also paste below my more concrete explanation for prosaic AGI:</p>\n", "parentCommentId": "4HMCbu6ivrgFQrmW8", "user": {"username": "remmelt"}}, {"_id": "MEpAv5Ct7eKSkkowY", "postedAt": "2023-06-05T12:45:50.287Z", "postId": "NcgDTEduqhYota8hc", "htmlBody": "<p>See below a text I wrote 9 months ago (with light edits) regarding the limits of error correction in practice. It was one of 10+ attempts to summarise Forrest Landry's arguments, which accumulated in this forum post \ud83d\ude42</p><p>If you want to talk more, also happy to have <a href=\"https://calendly.com/remmelt/30min/\">a call</a>.&nbsp;<br>I realise I was quite direct in my comments. I don't want that to come across as rude. I really appreciate your good-faith effort here to engage with the substance of the post. We are all busy with our own projects, so the time you spent here is something I'm grateful for!</p><p>I want to make sure we maintain integrity in our argumentation, given what's at stake. If you are open to going through the reasoning step-by-step, I'd love to do that. Also understand that you've got other things going on.</p><p><br>~ ~ ~</p><h2>4. Inequality of Monitoring</h2><p><i>Takes more code&nbsp;(multiple units) to monitor local environmental effects of any single code unit.</i></p><p>We cannot determine the vast majority of microscopic side-effects that code variants induce and could get selected for in interaction with the surrounding environment.&nbsp;</p><p>Nor could AGI, because of a macroscopic-to-microscopic mismatch: it takes a collection of many pieces of code, say of neural network circuits, to \u2018kinda\u2019 determine the innumerable microscopic effects that one circuit running on hardware has in interaction with all surrounding (as topologically connected) and underlying (as at lower layers of abstraction) virtualized and physical circuitry.</p><p>In turn, each circuit in that collection will induce microscopic side-effects when operated \u2013&nbsp; &nbsp; so how do you track all those effects? With even more and bigger collections of circuits? &nbsp; &nbsp; &nbsp; It is logically inconsistent to claim that it is possible for internals to detect and correct (and/or predict and prevent) all side-effects caused by internals during computation.&nbsp;</p><p>Even if able to generally model and exploit regularities of causation across macroscopic space, it is physically impossible for AGI to track&nbsp;<u>all</u> side-effects emanating from their hardware components at run-time, for all variations introduced in the hardware-embedded code (over &gt;10\u00b2 layers of abstraction; starting lower than the transistor-bit layer), contingent with all possible (frequent and infrequent) degrees of inputs and with all possible transformations/changes induced by all possible outputs, via all possibly existing channels from and to the broader environment.</p><p>Note emphasis above on interactions between code substrate and the rest of the environment, at the microscopic level all the way to at the macroscopic level.&nbsp;<br>To quote&nbsp;<a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#Section_B_\"><u>Eliezer Yudkowsky</u></a>: \"The outputs of an AGI go through a huge, not-fully-known-to-us domain (the real world) before they have their real consequences. Human beings cannot inspect an AGI's output to determine whether the consequences will be good.</p><p><i>Q</i>: What about scaling up capability so an AGI can track more side-effects simultaneously?&nbsp;</p><p>Scaling capability of any (superficially aligned) AI make them&nbsp;<i>worse-equipped</i> at tracking all interactions between/with internals. The number of possible interactions (hypothetically, if they were countable) between AI components and the broader environment would scale at minimum exponentially with a percentage-wise scaling of AI the components.</p><p>Scaling interpretability schemes is counterproductive too in that it leads researchers to miscalibrate even more on what general capabilities and degrees of freedom of interaction (eg. closed-loop, open-ended, autonomous) they can safely allow the interpreted ML architectures to scale to. If, for example, you would scale up interpretation to detect and correct out any misaligned mesa-optimiser, the mesa-optimisers you are leaving to grow in influence are those&nbsp;<a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#sufficiently_good_and_useful\"><u>successfully escaping detection</u></a> (effectively deceiving researchers into miscalibrated beliefs). Same goes for other locally selected-for optimisers, which we will get to later.</p><p>&nbsp;</p><h2>5. Combinatorial Complexity of Machine Learning</h2><p><i>Increasingly ambiguous to define &amp; detect novel errors to correct at higher abstraction layers.</i></p><p>Mechanistic interpretability emphasizes first inspecting neural network circuits, then piecing the local details of how those circuits work into a bigger picture of how the model functions. Based on this macroscopic understanding of functionality, you would then detect and correct out local malfunctions and misalignments (before these errors overcome forward pass redundancies).</p><p>This is a similar exercise to inspecting how binary bits stored on eg. a server\u2019s harddrive are logically processed \u2013 to piece together how the architecture stack functions and malfunctions:&nbsp;</p><ol><li>Occasionally, a local bit flips (eg. induced by outside electromagnetic interference).&nbsp;<br>So you make redundant copies of the binary code to compare and correct against.</li><li>At the packet layer, you find distortions in packets transmitted over wires to topologically adjacent hardware. You append CRC checksums to correct those errors.</li><li>At the application layer, you find that a Trojan horse transmitted from adjacent hardware caused an application to malfunction. You add in virus detection signatures.</li><li>At the layer of neural networks, trained through an application running on the server, you fear that more complex Trojan horses could infiltrate this layer too.&nbsp;</li></ol><p>\u200b\u200bEach time we scale the architecture to a new&nbsp;<a href=\"https://www.lesswrong.com/posts/fcnFddKjKZdDXt5cp/knowledge-is-not-just-digital-abstraction-layers\"><u>layer of abstraction</u></a>, we discover that we need to solve a completely new set of problems. Over time, variance is introduced into code computed at this layer, and some of this variance may lead to malfunctions. That a mechanism corrects errors at a lower layer says&nbsp;<i>little</i> about how to detect and correct errors at the new higher layer.</p><p>The higher up the architecture stack you design for, the more ambiguous and multi-faceted in meaning is&nbsp; the \u2018error\u2019 to correct out. Mechanisms you design for a neural network must detect over a combinatorial explosion of possible causal connections to correct out all defined errors.&nbsp;&nbsp;</p><p>Now, there is no mechanism to make a deep neural network receiving Internet data&nbsp;<a href=\"https://arxiv.org/pdf/1805.12152.pdf\"><u>robust</u></a>&nbsp;<a href=\"https://proceedings.neurips.cc/paper/2018/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf\"><u>against</u></a> any but a tiny fraction of possible Trojan backdoor attacks for which countermeasures exist,&nbsp;<a href=\"https://arxiv.org/pdf/2204.06273.pdf\"><u>countermeasures that are easily circumvented</u></a>. Here, an adversary introduces variation into the input data that a model is training on, such that in later deployment a chosen (series of) inputs will trigger the model to behave out of line. The model is set to act like a perfect&nbsp;<a href=\"https://encyclopedia2.thefreedictionary.com/Manchurian+Candidate\"><u>Manchurian Candidate</u></a>. For the model\u2019s developers, under usual interactions, the model plainly serves some overall purpose. In parallel, the model is storing latent causal structure that under particular interactions with the outside environment causes outputs and cascading effects.</p><p>For the adversary, the model also serves another, conflicting purpose. In effect, they select for internal variance misaligned with the original developers\u2019 purpose. Worse, if \u2018random\u2019 initialisation of internals was directed by the adversary, they could&nbsp;<a href=\"https://arxiv.org/pdf/2204.06974.pdf\"><u>code in persistent misalignment that is undetectable</u></a> \u2013 making it computationally infeasible for an interpreter with full access to internals to find&nbsp;<i>a single</i>&nbsp;<i>input</i> for which the model\u2019s outputs differ from the outputs of an untampered copy (for details, see section 2.3 of the&nbsp;<a href=\"https://arxiv.org/pdf/2204.06974.pdf\"><u>paper</u></a>).</p><p><br><br>&nbsp;</p><h2>6. Delays in Measuring Changes</h2><p><i>Time delay in measuring&nbsp;(and correcting) the extent of any misalignment detectable in the monitored&nbsp;(or simulated) changes caused by an internal code unit.</i></p><p>Any decision-theoretical premise of&nbsp;<a href=\"https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities\"><u>coherent unitary agency</u></a> of future AGI fails to account for locally selected-for effects. It asserts a priori that any AGI would consistently and continuously operate as, and thus can be distilled and represented monolithically as, a coherent agentic unit. More precisely, that \u2018AGI\u2019 can be soundly represented as (a) single unit(s) that make(s) decisions over time based on (orderings of) preferences that are&nbsp;<i>consistent</i> with each other.&nbsp;</p><p>Not so in practice: if AGI internals induce local effects that loop out of the control of higher-level regulation (in reference to eg. decided metric extents or numbers of end states), then lower-level behavior is&nbsp;<i>inconsistent</i> with higher-level decision-making. But even internals detect &amp; correct (or predict &amp; prevent) internal inconsistencies, can only do so&nbsp;<i>discontinuously</i>.</p><p>There is a time delay to inspecting, say, a new circuit or module. Take for example, a scheme that measures the extent to which the variant, as computed in interaction with connected embedded code, causes external effects that diverge (are misaligned) from reference values. But to measure, code\u2019s detected/predicted effects are first transformed to be commensurable (measured by a common unit standard against the reference) over dimensions learned to be semantically descriptive of mis/alignment (as eg. generalized from past feedback expressed by humans within environmental contexts that they and/or the AI were operating in at the time).</p><p>Then, over the number of clock cycles this scheme took to detect the internal misalignment (let alone correct the misalignment, without breaking the system in another way), any nonlinear effects of the computed code variant may have looped outside the bounds of what the scheme can detect and correct for. The scheme\u2019s mechanistic operations cannot predetermine what happens during the gap between inspecting code and measuring+correcting for misalignment.&nbsp;</p><p><i>Q</i>: But AGI is a&nbsp;<i>fast</i> global optimiser, so it must be fast enough at detecting or predicting the effects of local variants before any misaligned effects break outside AGI, right?</p><p>Any superficially human-aligned AGI running on digital hardware would take some huge multiple of a clock cycle to inspect (and if misaligned, correct) each of all pieces of internal code that could be running over a next period.&nbsp;</p><p><i>Q</i>: Could AGI not pause computation of that internal code in the meantime?</p><p>AGI pausing computation of&nbsp;<i>all</i> internal code while inspecting/interpreting internals is not an option, because the inspector/interpreter is effectively topologically connected within the internals. Instead, AGI could continue computing non-varied code, including the interpreter, but how can one ascertain that no changes have actually been made to \u2018non-varied\u2019 hardware memory locations since the last inspection point through any physical pathway (e.g quantum jumps between nanometer-spaced transistor gaps)? By inspecting the code: here basically requiring a massive parallel-processed redundancy-based error checker that is perfectly accurate and stays so over millennia+ (also error checking the error checker and so on). For the compute-paused portion of varied code, the catch is that the interpreter would not be able to simulate the full range of the new variants\u2019 effects without first computing them in interaction with connected code, as in deployment (variants interact with the interpreter in any case). Finally, AGI must upgrade their interpreter to be somewhat able to interpret new layers of variants integrated into internals, which requires creating new variations on the interpreter itself.&nbsp;</p><p>^\u2014 All of this is to say \u2018Indeed, AGI inspecting all internal code that could be running in&nbsp;a next period&nbsp;does take some huge multiple of a clock cycle, and that code needs to actually be running for inspection to be anything remotely close to sound.\u2019</p><p>\u2018Built-in alignment\u2019 does not work either, since this notion of \u2018built-in\u2019 fails to account for the malfunctioning or misalignment of variants that are introduced and newly connected up within the code pool over time.&nbsp;</p><p>&nbsp;</p><h2>7. Computationally-Irreducible Causal Trajectories</h2><p>Nonlinear feedback cycles can amplify a tiny local change into a large global divergence in the final conditions.&nbsp;</p><p>Even if any effect starts microscopic in scope and small in its magnitude, we cannot <i>a priori</i> rule out that it cascades into larger macroscopic effects. In case that tiny \u2018side-effect\u2019 feeds into a&nbsp;<a href=\"https://www.lesswrong.com/posts/pnAxcABq9GBDG5BNW#Partial_Observability_and_Chaotic_Systems\"><u>chaotic system</u></a>, found across eg. biological lifeforms and Internet networks, the minor change caused in the initial conditions can get recursively&nbsp;<u>amplified</u> into causing much larger changes (vs. non-amplified case) in the final conditions.</p><p>Any implicitly captured structure causing (repeated) microscopic effects does not have to have captured macroscopic regularities (ie.&nbsp;<a href=\"https://www.lesswrong.com/posts/vDGvHBDuMtcPd8Lks/public-static-what-is-abstraction\"><u>a natural abstraction</u></a>) of the environment to run amok. Resulting effects just have to stumble into a locally-reachable positive feedback loop.&nbsp;</p><p>It is dangerous to assume otherwise, ie. to assume that:</p><ul><li>selected-for microscopic effects fizzle out and get lost within the noise-floor over time.</li><li><i>reliable</i> mechanistic interpretation involves piecing together elegant causal regularities, natural abstractions or content invariances captured by neural circuits.</li></ul><p>&nbsp;</p><p><br>&nbsp;</p>", "parentCommentId": "nmsw7Cu5Tb38ssTrt", "user": {"username": "remmelt"}}]