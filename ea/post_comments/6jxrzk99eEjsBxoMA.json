[{"_id": "mpho7pecCwgqtECiG", "postedAt": "2023-10-25T00:10:01.641Z", "postId": "6jxrzk99eEjsBxoMA", "htmlBody": "<p>Interesting post, certainly an interesting comparison and an existence proof that technology that's somewhat difficult to create but trivial to distribute and reproduce can be regulated to oblivion in large sections of the world for decades.<br><br>A point conspicuous by its absence: the overregulation of GM crops was (and remains) a mistake, or at the least is nearly universally agreed to be so by the people with the most technical knowledge (e.g. people with PhDs in plant biology).&nbsp;<br><br>I understand whether it was wise to grossly curb deployment of GM crops was not the point of the post, merely whether it was politically feasible starting with a relatively small contingent of protesters. I'm still miffed that the anti-science and overall almost certainly negative-EV-ness of the GM overregulation wasn't mentioned, especially given quotes like \"This coincided with key 'trigger events' like Mad Cow Disease and the arrival of GM crops in March 1996\" which would suggest a causal connection between the two.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Kasey Markel"}}, {"_id": "vgpSHJDHWp96wdTdo", "postedAt": "2023-10-25T14:36:37.096Z", "postId": "6jxrzk99eEjsBxoMA", "htmlBody": "<p>Thank you for your comments Kasey! Glad you think it's an interesting comparison. I agree with you that GMOs were over-regulated in Europe. Perhaps I should have said explicitly that the scientific consensus is that GMOs are safe. I do make a brief caveat in the Intro that I'm not comparing the \"credibility of AI safety concerns (which appear more legitimate than GMO concerns)\", though this deserves more detail.</p>", "parentCommentId": "mpho7pecCwgqtECiG", "user": {"username": "hptc123"}}, {"_id": "uH3LSAE2JwJ2avTje", "postedAt": "2023-10-26T03:32:26.280Z", "postId": "6jxrzk99eEjsBxoMA", "htmlBody": "<p>I suppose an interesting exercise for another research project could be to try to tally up in hindsight how many activist/protest movements seem directionally correct or mistaken in retrospect (eg anti-GM seems wrong, anti-nuclear seems wrong, anti-fossil-fuels seems right). I think even if the data came in that activists are usually wrong this wouldn't actually move me very much as the inside view arguments are quite strong for AI risk I think.</p>\n", "parentCommentId": "vgpSHJDHWp96wdTdo", "user": {"username": "Oscar Delaney"}}, {"_id": "h6codbsMX3d4QaKYi", "postedAt": "2023-10-26T03:37:47.315Z", "postId": "6jxrzk99eEjsBxoMA", "htmlBody": "<p>How epistemically and optically dangerous do you think the communication and allyship tactics you propose are? Reasons to worry are that:</p><ul><li>If we start talking about injustices and near-term non-existential risks that are more 'sexy' and easy to grasp, maybe this starts shaping our own thinking as well, which seems bad.</li><li>Conversely, if we maintain an x-risk focus while espousing other issues and allying ourselves with other groups, this is (or may be perceived) as a bit deceptive and manipulative.</li></ul><p>I think you may well still be right regardless of these risks, but they seem important to consider.</p>", "parentCommentId": null, "user": {"username": "Oscar Delaney"}}, {"_id": "BqcRKsJc27zvNTeq2", "postedAt": "2023-10-27T04:43:09.636Z", "postId": "6jxrzk99eEjsBxoMA", "htmlBody": "<p>Regarding allies:<br><br>\u2022 I agree that working with other groups is great when we have a common interest. Take, for example, the FLI letter. This was a highly successful example of a collaboration with some AI ethics people.</p><p>\u2022 At the same time, I'm less optimistic about any plans that involve developing our strategy in broad-tent groups which would possibly dilute our focus. This doesn't just apply to the AI ethics community, with whom we have an unfortunately fractious relationship, but would also apply to artists as well. Of course, think it makes sense to collaborate with them when our interests align.</p><p>\u2022 I'm less a fan of disruptive tactics, especially since we have allies within these firms. There's a sense in which these are a cheap way to get attention and I suspect that if we're strategic we can find other ways to draw attention to our concerns without risking turning the public against us. For example, persuading a large number of people to wear the same t-shirt at a conference might actually be more effective.</p>", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "8ndB5orWfDAKL7mDq", "postedAt": "2023-10-27T14:19:18.279Z", "postId": "6jxrzk99eEjsBxoMA", "htmlBody": "<p>Sounds interesting Oscar, though I wonder what reference class you'd use ... all protests? A unique feature of AI protests is that many AI researchers are themselves protesting. If we are comparing groups on epistemics, the Bulletin of the Atomic Scientists (founded by Manhattan Project scientists) might be a closer comparison than GM protestors (who were led by Greenpeace, farmers etc., not people working in biotech). I also agree that considering inside-view arguments about AI risk are important.&nbsp;</p>", "parentCommentId": "uH3LSAE2JwJ2avTje", "user": {"username": "hptc123"}}, {"_id": "pCx8R7RJHWfynme5n", "postedAt": "2023-10-27T14:36:18.676Z", "postId": "6jxrzk99eEjsBxoMA", "htmlBody": "<p>Thanks for these questions Oscar! To be clear, I was suggesting that effective messaging would emphasise the injustice of continued AI development in an emotionally compelling way: e.g. lack of democratic input to corporate attempts to build AGI. I wasn't talking so much about communicating near-term injustices. Though, I take your point, that by allying with other groups suffering from near-term harms, this would imply a combined near-term and long-term message.&nbsp;</p><p>On your first question: would thinking about near-term &amp; LT harms lead to worse thinking? Do you mean this would make us care about AI x-risk less?&nbsp;</p><p>And on your second point, on whether it would be perceived as manipulative. I don't think so. If AI protest can effectively communicate a 'We are fighting a shared battle' message, as <a href=\"https://forum.effectivealtruism.org/users/gideon-futerman?mention=user\">@Gideon Futerman</a> has written about, this could make AI protests seem less niche/esoteric. Identifying concrete examples of harms to specific people/groups is important part of 'injustice frames', and could make AI risk more salient. In addition, broad 'coalitions of the willing' (i.e. Baptists and Bootleggers) are very common in politics. What do you think?</p>", "parentCommentId": "h6codbsMX3d4QaKYi", "user": {"username": "hptc123"}}, {"_id": "kYsrRLFwunfe2ugc5", "postedAt": "2023-10-27T14:50:01.125Z", "postId": "6jxrzk99eEjsBxoMA", "htmlBody": "<p>Hi Chris, thank you for this.&nbsp;</p><p>1) Nice! Agreed</p><p>2) It really depends on what form of alliance this takes. It could be implicit: fundraising for artists' lawsuits for example, without any major change to public messaging. I don't think this would dilute the focus on existential risk. When Baptists allied with Bootleggers in the prohibition era, this did not dilute their focus away from Christianity! I also think that there are indeed common interests here: restrictions on GAI models. (https://forum.effectivealtruism.org/posts/q8jxedwSKBdWA3nH7/we-are-not-alone-many-communities-want-to-stop-big-tech-from).&nbsp;</p><p>That being said, if PauseAI did try to become a broad 'AI protest group', including via its messaging, this would dilute the focus on x-risk. Though, mixture of near-term and long-term messaging may more effective in reaching a broader audience. As mentioned in another comment, identifying concrete examples of harms to specific people/groups is important part of 'injustice frames'. (I am more unsure about this, though.)</p><p>3) I am also hesitant about more disruptive research tactics, in particular because of allies within firms. But, I don't think that disruptive protests necessarily have to turn the public against us... no more than blocking ships made GMO protestors unpopular. Efficacy of disruptive tactics are quite issue-dependent... &nbsp;I think it would be useful if someone did a thorough lit review of disruptive protests.</p>", "parentCommentId": "BqcRKsJc27zvNTeq2", "user": {"username": "hptc123"}}, {"_id": "vnAAiCzF8aMjuDYgL", "postedAt": "2023-10-28T01:26:52.306Z", "postId": "6jxrzk99eEjsBxoMA", "htmlBody": "<p>I suppose I meant something similar to what Chris has also written. I think being single-minded can be valuable. Hopefully it is possible to engage productively with non x-risk focused communities without being either deceptive or manipulative, I think it is doable, just requires some care I imagine.</p>\n", "parentCommentId": "pCx8R7RJHWfynme5n", "user": {"username": "Oscar Delaney"}}, {"_id": "Kuw2SxQXHsNmuEDRH", "postedAt": "2023-10-31T15:59:19.929Z", "postId": "6jxrzk99eEjsBxoMA", "htmlBody": "<p>I hadn\u2019t made the GMO protests - AI protests connection.</p>\n<p>This reads as a well-researched piece.</p>\n<p>The analysis makes sense to me \u2013 with an exception to seeing efforts to restrict facial recognition, the Kill Cloud, etc, as orthogonal. I would also focus more on preventing increasing AI harms and Big Tech power consolidation, which most AI-concerned communities agree on.</p>\n", "parentCommentId": null, "user": {"username": "remmelt"}}, {"_id": "WYhcfTpHGL5EHHsvL", "postedAt": "2023-10-31T21:46:42.418Z", "postId": "6jxrzk99eEjsBxoMA", "htmlBody": "<p>Appreciate that <a href=\"https://agi-moratorium-hq.slack.com/team/U054CLYQVV5\">@Remmelt Ellen</a>! In theory, I think these messages could work together. Though, given animosity between these communities, I think alliances are more challenging. Also I'm curious - what sort of policies would be mutually beneficial for people concerned about facial recognition and x-risk?&nbsp;</p>", "parentCommentId": "Kuw2SxQXHsNmuEDRH", "user": {"username": "hptc123"}}]