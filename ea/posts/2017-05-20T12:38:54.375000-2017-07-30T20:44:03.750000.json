[{"_id": "i9wgskaridaCrogER", "title": "Projects, People and Processes", "postedAt": "2017-06-26T13:27:27.030Z", "htmlBody": "<p>One of the challenges of large-scale philanthropy is: how can a small number of decision-maker(s) (e.g., donors) find a large number of giving opportunities that they understand well enough to feel good about funding?</p><p>Most of the organizations I\u2019ve seen seem to use some combination of project-based, people-based, and process-based approaches to delegation. To illustrate these, I\u2019ll use the hypothetical example of a grant to fund research into new malaria treatments. I use the term \u201cProgram Officers\u201d to refer to the staff primarily responsible for making recommendations to decision-makers.</p><ul><li><strong>Project-based</strong> approaches: the decision-makers hire Program Officers to look for projects; decision-makers ultimately evaluate the projects themselves. Thus, decision-makers delegate the process of searching for potential grants, but don\u2019t delegate judgment and decision-making. For example, a Program Officer might learn about proposed research on new malaria treatments and then make a presentation to a donor or foundation Board, explaining how the project will work, and trying to convince the donor or Board that it is likely to succeed.</li><li><strong>People-based</strong> approaches: decision-makers delegate essentially everything to trusted individuals. They look for staff they trust, and then defer heavily to them. For example, a Program Officer might become convinced of the merits of research on new malaria treatments and propose a grant, with the funder deferring to their judgment despite not knowing the details of the proposed research.</li><li><strong>Process-based</strong> approaches: the decision-makers establish consistent, systematic criteria for grants, and processes that aim to meet these criteria. Decisions are often made by aggregating opinions from multiple grant reviewers. For example, a donor might solicit proposals for research on new malaria treatments, assemble a technical review board, ask each reviewer to rate each proposal on several criteria, and use a pre-determined aggregation system to make the final decisions about which grants are funded. Government funders such as the <a href=\"http://www.nih.gov/\">National Institutes of Health</a> often use such approaches. These approaches often seek to minimize the need for individual judgment, effectively delegating to a process.</li></ul><p>These different classifications can also be useful in thinking about how Program Officers relate to grantees. Program Officers can recommend grants based on being personally convinced of a particular project; recommend grants based primarily on the people involved, deferring heavily on the details of those people\u2019s plans; or recommend grants based on processes that they set up to capture certain criteria.</p><p>This post discusses how I currently see the pros and cons of each, and what our current approach is. In large part, <strong>we find the people-based approach ideal</strong> for the kind of <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">hits-based giving</a> we\u2019re focused on. But we use elements of project-based evaluation (and to a much lesser degree, process-based evaluation) as well - largely in order to help us better evaluate people over time.</p><h1>Projects, processes, and people</h1><p>In some ways, <strong>project-based</strong> approaches are the most intuitive, and probably the easiest method for a funder to feel confident in by default. Organizations propose specific activities to Program Officers, who try to identify the ones that will appeal to decision-makers and make the case for them. Thus, decision-makers delegate the process of searching for potential grants, but don\u2019t delegate judgment and decision-making.</p><p>I think the fundamental problem with project-based approaches is that the decision-maker generally has much less knowledge and context than the Program Officer (who in turn has much less than the organization they\u2019re evaluating). This is a general problem with any kind of \u201ctop-down\u201d decision-making process, but I think it is a particularly severe problem for philanthropy, because:</p><ul><li>There\u2019s often a very small number of people (a wealthy individual or family) who are trying to give away a large amount of money.</li><li>They often do not personally have extensive background knowledge for the causes they\u2019re working in, and do not work full-time on any particular cause (and in many cases do not work full-time on philanthropy generally).</li><li>Decisions usually can\u2019t be subject to any straightforward or quick performance measurement. It usually takes a good deal of subjective judgment to decide whether a grant is going well.</li></ul><p>From what I\u2019ve seen, the result can often be that Program Officers recommend the giving opportunities they think they can easily justify, rather than the giving opportunities they personally think are best. This risks wasting much of the expertise and deep context Program Officers bring to the job. I think this is a major problem when trying to do <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">hits-based giving</a>, for reasons outlined previously.</p><p><strong>People-based</strong> approaches are the opposite in some sense. Grants are made based on trust in the people recommending them, rather than based on agreement with the specific activities proposed. This is \u201cbottom-up\u201d where project-based giving is \u201ctop-down\u201d; it delegates judgments to individuals, where project-based giving doesn\u2019t delegate judgment at all. I think the advantages here are fairly clear: the people with the most expertise and context are the ones who lead the decision-making. People-based approaches seem likely, to me, to achieve the best results when done well.</p><p>However, I also see major challenges to people-based approaches:</p><ul><li>Everything comes down to picking the right people. I generally consider it very hard to evaluate people, and don\u2019t know of any reliable and reasonably quick way to do so. Our experiences recruiting have generally left us feeling that the only good way to evaluate someone is to work with them for an extended period of time, and we\u2019ve heard similar sentiments when seeking advice from other organizations.</li><li>The more one defers to people, the more one is leaving the possibility open that they might make decisions arbitrarily, based on e.g. relationships rather than merit.</li><li>I also think there is at least some tension between taking people-based approaches - at least pure people-based approaches that focus on \u201cgeneral impressiveness\u201d of people - and pursuing <a href=\"https://www.openphilanthropy.org/focus\">neglected</a> causes. If one simply finds the most impressive people (in a general sense) and defers to them, one is likely to end up supporting people who have already had success and achieved influence, and one is likely to end up supporting fields that are already popular. I believe that we are bringing a fairly novel angle to philanthropy - focusing on <a href=\"https://www.openphilanthropy.org/focus\">important, neglected, tractable</a> causes. This angle is novel enough that we don\u2019t think we should be restricting ourselves to working with people who fully share it (this would greatly reduce the pool of people to choose from). But if we instead evaluate people only for general impressiveness, we risk losing much of this angle, and risk doing status-quo-biased giving.</li></ul><p><strong>Process-based</strong> approaches are another approach to scaling understanding. Rather than try to evaluate each project, or defer to individuals\u2019 judgment, the funder sets up processes that try to capture high-level criteria. For example, many National Institutes of Health (NIH) grants seek to optimize on <a href=\"https://grants.nih.gov/grants/peer_review_process.htm#Criteria\">criteria</a> such as the significance of the work, the experience/training/track records of the investigator(s), the degree to which a project is innovative, etc. Process-based approaches often (as with the NIH) involve systematic aggregation of a large number of individual judgments, reducing reliance on any one individual\u2019s judgment.</p><p>I think the appeal of process-based approaches is that they can integrate expertise and deep context into decisions more reliably than project-based approaches (which rely on the judgment of the decision-maker(s)), while also avoiding the disadvantages listed for people-based approaches: difficulty of choosing people, risks of arbitrariness and conflicts of interest, difficulty maintaining fidelity to unusual angles on giving that the decision-maker(s) might have. However, I believe that process-based approaches bring their own problems:</p><ul><li>Well-defined criteria and processes make it possible for potential grantees to \u201cgame the system\u201d - coming up with grant proposals that are designed to get through the process rather than to propose and communicate the best work possible. And once gaming the system becomes possible, it may quickly become <i>necessary</i> in a competitive environment.</li><li>Process-based approaches tend (I believe) to be fairly slow, unpredictable and inconvenient from a grantee\u2019s perspective. They make it hard to have honest, informative conversations with potential grantees about their likelihood of getting funded. They also tend to be rigid: unable to support work that\u2019s very different from (but perhaps better than) what the process designers anticipated.</li><li>Process-based approaches tend to minimize the role of individual judgment, often by aggregating many judgments. But the more they do this, the less room they leave for the kind of high-risk, high-reward, creative, unusual decisionmaking we associate with <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">hits-based giving</a>.</li></ul><p>My current view is that process-based approaches can be excellent for funders seeking to minimize risk (of being perceived as unfair, of supporting low-quality work, of supporting work for the wrong reasons). Government funders often fit this description. But process-based approaches seem much less appealing for <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">hits-based giving</a>, unless they are very carefully designed by people with strong expertise and context with the specific goal of pursuing \u201chits.\u201d</p><p>Note that the above classifications are fairly simplified, and many funders\u2019 decision-making processes have elements of more than one.</p><h1>Our current approach</h1><p>Our current approach is based on the idea that <strong>people-based giving is ideal</strong> for the kind of <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">hits-based</a> work we\u2019re trying to do. Specifically, our ideal is to find <strong>people who make decisions as we would, if we had more expertise, context and time for decision-making.</strong> (Here \u201cwe\u201d refers to myself and Cari Tuna, currently the people who sign off on Open Philanthropy Project grants.) We also encourage our Program Officers to take this attitude when seeking potential grantees, though they can ultimately choose whatever mix they want of project-, people- and process-based approaches for making recommendations to us.</p><p>Our ideal can be pursued at different levels of breadth. We can set particular <a href=\"https://www.openphilanthropy.org/focus\">focus areas</a>, then try to find Program Officers who make decisions as we would for each focus area. Program Officers can then try to find people who make decisions as they would for particular <i>sub-areas</i> of the focus area they work in; for example, after identifying corporate cage-free reforms as a promising sub-area of <a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\">farm animal welfare</a>, we sought to support a <a href=\"https://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms\">set of people working on cage-free reforms</a>.</p><p>The major challenge of this approach is determining which people we want to trust, and in what domains. We might find a particular person to be a great representative of our values in one area, but a poor representative in another. This is where a more project-based mentality comes in.</p><ul><li>We try to understand a focus area well enough to have fairly detailed discussions of strategy during our hiring process (for more on what we look for, see <a href=\"https://www.openphilanthropy.org/blog/process-hiring-our-first-cause-specific-program-officer\">this post</a>).</li><li>Our <a href=\"https://www.openphilanthropy.org/blog/our-grantmaking-so-far-approach-and-process\">grantmaking process</a> involves Program Officers\u2019 writing up the case for each grant and answering a number of key questions. We try to have enough general knowledge about the area they\u2019re working in to evaluate the case they\u2019re making at a high level. We ask critical questions and try to find ways in which we can learn from each other.</li></ul><p>A fairly common dynamic with new Program Officers has been that we know far less about their field than they do, and we often learn about the field when we question the parts of a grant writeup we find counterintuitive; at the same time, we often (at first) have better-developed views on philanthropy-specific topics, such as assessing <a href=\"http://www.givewell.org/how-we-work/criteria/room-for-more-funding\">room for more funding</a>. Our goal over time is to reach increasing common understanding with Program Officers and increasingly defer to their judgment.</p><p>One principle we\u2019ve been experimenting with is a \u201c50/40/10\u201d rule:</p><ul><li>We want to have fairly good understanding of, and high excitement about, at least 50% of a Program Officer\u2019s portfolio.</li><li>We want another 40% of the portfolio to fit the description, \u201cWe can see how this <i>might</i> appear very exciting if we had more context, though we don\u2019t feel personally convinced.\u201d</li><li>We\u2019re willing to defer entirely to the Program Officer on the remaining 10% of the portfolio, even if we can\u2019t see the case for it at all (as long as the downside risks are manageable).</li></ul><p>The idea here is to try to stay synced up with Program Officers on enough of their work - using a \u201cproject-based\u201d approach - to continually justify our confidence in them as decision-makers, while allowing a lot of leeway for them to use their own judgment and recommend grants that require deep expertise and context to appreciate.</p><p>Similar principles apply to how we support and evaluate grantees. Even when the main reason we\u2019re supporting an organization is as a bet on the people involved, we still find it helpful to have an outline of the projects they plan on. This helps us evaluate whether these people are aligned with our goals and whether our funds will help us do much more than they could have otherwise. But once we\u2019ve determined that the proposed activities seem promising and sensible, we tend to provide support with no strings attached, in case plans change.</p><p>Ultimately, I think that our work tends to look very \u201cproject-based\u201d in a sense: we put a lot of effort into learning about our focus areas and we ask Program Officers a lot of questions about their recommended grants, and Program Officers in turn tend to ask potential grantees a lot of questions about the specifics of their plans. But the intent of this is more to spot-check our alignment and understanding than to comprehensively understand the grants. When a particular question is hard to resolve, we tend to defer to the people with the most expertise and context. We know we\u2019ll never have the whole picture, and our goal is to understand enough of it to extrapolate the rest - by trusting the right people for the right purposes.</p>", "user": {"username": "HoldenKarnofsky"}}, {"_id": "BQjLoKcE5HZL5G7gj", "title": "New Report on Consciousness and Moral Patienthood", "postedAt": "2017-06-06T13:21:52.145Z", "htmlBody": "<p>As we\u2019ve written <a href=\"http://www.openphilanthropy.org/blog/radical-empathy\">previously</a>, we aim to extend empathy to every being that warrants moral concern, including <a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\">animals</a>. And while many experts, government agencies, and advocacy groups agree that some animals live lives worthy of moral concern, there seems to be little agreement on <i>which</i> animals warrant moral concern. Hence, to inform our long-term giving strategy, I\u2019ve prepared a <a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood\">new report</a> on the following question: \u201cIn general, which types of beings merit moral concern?\u201d Or, to phrase the question as some philosophers do, \u201cWhich beings are <i>moral patients</i>?\u201d</p><p>For this preliminary investigation, I focused on just <i>one</i> commonly endorsed criterion for moral patienthood: phenomenal consciousness, a.k.a. \u201csubjective experience.\u201d I have not come to any strong conclusions about which (non-human) beings are conscious, but I think some beings are more likely to be conscious than others, and I make several suggestions for how we might make progress on the question.</p><p>In the long run, to make well-grounded decisions about how much we should value grants aimed at (e.g.) chicken or fish welfare, we need to form initial impressions not just about which creatures are more and less likely to be conscious, but also about (a) <a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#ProposedCriteria\">other plausible criteria</a> for moral patienthood besides consciousness, and also about (b) the question of \u201c<a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#MoralWeight\">moral weight</a>.\u201d However, those two questions are beyond the scope of this initial report on consciousness. In the future I hope to build on the initial framework and findings of this report, and come to some initial impressions about other criteria for moral patienthood and about moral weight.</p><p>My goals for <a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood\">this report</a> on consciousness and moral patienthood were to:</p><ol><li>survey the <i>types</i> of evidence and argument that have been brought to bear on this question,</li><li>briefly describe <i>example pieces of evidence</i> of each type, without attempting to summarize the vast majority of the evidence (of each type) that is currently available,</li><li>report what my own intuitions and conclusions are as a result of my shallow survey of those data and arguments,</li><li>try to give <i>some</i> indication of why I have those intuitions, without investing the months of research that would be required to rigorously <i>argue</i> for each of my many reported intuitions, and</li><li>list some research projects that seem (to me) like they could make progress on the key questions of this report, given the current state of evidence and argument.</li></ol><p>The report\u2019s first section explains <a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#HowToRead\">how to read the report</a>, including brief descriptions of each section and major appendix.</p><p>In short, my tentative conclusions are that I think mammals, birds, and fishes are more likely than not to be conscious, while (e.g.) insects are unlikely to be conscious. However, my probabilities are very \u201cmade-up\u201d and difficult to justify, and it\u2019s <a href=\"https://www.openphilanthropy.org/blog/technical-and-philosophical-questions-might-affect-our-grantmaking#Making_decisions_under_different_kinds_of_uncertainty\">not clear to us</a> what actions should be taken on the basis of such made-up probabilities.</p><p>For more details, see <a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood\">the full report</a>.</p>", "user": {"username": "lukeprog"}}, {"_id": "iCFjrAeENiEiWkimi", "title": "Will MacAskill: Closing remarks (2017)", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=wBozU-FkMbI&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=27\"><div><iframe src=\"https://www.youtube.com/embed/wBozU-FkMbI\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: Boston 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "sqMgzYpvrdA6Dimfi", "title": "Amanda Askell: The moral value of information", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=o8rVscSHJT4&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=26\"><div><iframe src=\"https://www.youtube.com/embed/o8rVscSHJT4\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Below is an article based on a transcript of \"The Moral Value of Information\", a popular 2017 talk by the NYU philosopher Amanda Askell. She argues that we often underestimate the value of new information or knowledge when thinking of how to do&nbsp;good.</i></p><p><i>James Aung and Jessica Wong created this heavily edited version of the original transcript in order to condense, clarify, and clean up the argument.</i></p><h1>The Talk</h1><p>I'm going to start by making two claims. The first claim is that we generally prefer interventions with more evidential support, all else being equal. I hope you find that claim plausible, and I\u2019ll go into detail later about what it means. The second claim I\u2019m going to argue for is that having less evidence in favor of a given intervention means that your credences about the effectiveness of that intervention are what I call \u201clow&nbsp;resilience\u201d.</p><p>This second claim has been explored in decision theory to some extent. It holds even if your credences about the effectiveness of that intervention are the same value. So, if I thought there was a 50% chance that I would get $100, there\u2019s a difference between a low resilience 50% and a high resilience&nbsp;50%.</p><p>I\u2019m going to argue that, if your credences in a particular domain are low resilience, then the value of information in this domain is generally higher than it would be in a domain where your credences are high resilience. <strong>And, I\u2019m going to argue that this means in many cases, we should prefer interventions with less evidential support, all else being equal.</strong> Hopefully, you\u2019ll find that conclusion counterintuitive and&nbsp;interesting.</p><p>The first thing to say is that we generally think that expected value calculations are a good way of estimating the effectiveness of a given intervention. For example, let's imagine that there are two diseases: (very novelly named) Disease A and Disease B [Figure 1].</p><p>Say these two diseases are virtually impossible to differentiate. They both have the same symptoms, and they cause the same reduction in life expectancy, etc. Their key difference is that they respond very differently to different treatments, so any doctor who finds themselves with a patient with one of these conditions is in a difficult&nbsp;situation.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/46wOcwQAe4Q6eM6cq6cOOm/b7e49ca718c76b6b53e38ea8ef577060/moral_value1.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/46wOcwQAe4Q6eM6cq6cOOm/b7e49ca718c76b6b53e38ea8ef577060/moral_value1.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/46wOcwQAe4Q6eM6cq6cOOm/b7e49ca718c76b6b53e38ea8ef577060/moral_value1.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/46wOcwQAe4Q6eM6cq6cOOm/b7e49ca718c76b6b53e38ea8ef577060/moral_value1.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/46wOcwQAe4Q6eM6cq6cOOm/b7e49ca718c76b6b53e38ea8ef577060/moral_value1.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/46wOcwQAe4Q6eM6cq6cOOm/b7e49ca718c76b6b53e38ea8ef577060/moral_value1.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/46wOcwQAe4Q6eM6cq6cOOm/b7e49ca718c76b6b53e38ea8ef577060/moral_value1.png?w=1800&amp;q=70 1800w\"></figure><p>They can prescribe Drug A which costs $100. If the patient has Disease A, then Drug A will extend their life by another 10 years. If, on the other hand, the patient had Disease B, it won\u2019t extend their life at all. They will die of Disease B, because Disease B is completely non-responsive to Drug A. Therefore, the expected years of life that we get from Drug A is 0.05 per dollar. Drug B works in a very similar way, except it is used to treat Disease B. If you have Disease A, it will be completely non-responsive. So, it\u2019s got the same expected value as Drug&nbsp;A.</p><p>Then, we have Drug C, which also costs $100. Both Disease A and Disease B are somewhat responsive to Drug C, so this is a new and interesting drug. From Figure 1 we can see the expected value for Drug C is greater than the expected value for either Drug A or Drug B. So we think, \u201cOkay, great. Kind of obvious we should prescribe Drug&nbsp;C.\u201d</p><p>Suppose that Drug A and Drug B have been already heavily tested in numerous trials, and they\u2019ve been shown in meta-analyses to be highly effective, and that the estimates in Figure 1 are extremely robust. Drug C, on the hand, is completely new. It has only had a single trial, in which it increased patients\u2019 lives by many years. We assume that this was in a trial of patients with both&nbsp;diseases.</p><p>Say you have a conservative prior about the effectiveness of a drug; you think, \u201cIn likelihood, most random drugs that we were to select would either be net neutral or net negative\u201d. If you see one trial in which a drug massively extends someone\u2019s life, then your prior might bring you down to something like six years, regardless of whether they have Disease A or Disease B. We have the same expectation for Drug C as before, but suddenly it seems a bit more questionable whether we should prescribe&nbsp;it.</p><p>This idea that we should favor interventions with more evidence, and that expected utility theory can\u2019t capture this, is summed up in <a href=\"https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">this blog post from GiveWell</a> from&nbsp;2016.</p><blockquote><p><i>\u201cThere seems to be nothing in explicit expected value that penalizes relative ignorance, or relatively pearly grounded estimates. If I can literally save a child I see drowning by ruining a $1,000 suit, but in the same moment that I make a wild guess that this $1,000 could save two lives if I put it toward medical research, then explicit expected value seems to indicate that I should opt for the&nbsp;latter.\u201d</i></p></blockquote><p>The idea is that there\u2019s something wrong with expected value calculations because they kind of tell us to take wild guesses, as long as the expected value is higher. I want to argue that there are two claims that we might want to vindicate in these sorts of&nbsp;cases.</p><p>The first claim is one that I and hopefully you find quite plausible, and it\u2019s the claim that evidence matters. How much evidence we have about an intervention can make a difference in deciding what we should&nbsp;do.</p><p>The second claim is one that I think is implied by the above quote, which is that we should favor more evidence, all else being equal. So, if the expected value of two interventions is similar, we should generally favor investing in interventions that have more evidence supporting&nbsp;them.</p><p>Maybe we can say that this is relevantly similar to the case we have with Drugs A, B, and C. In a case where you have a lot of evidence that Drug A and Drug B have the effects as in Figure 1, this might favor giving one of these well-known drugs over a new one, such as Drug C, that has only been shown in one trial to be&nbsp;effective.</p><p>I\u2019m going to consider both of these claims, and whether expected value calculations can vindicate either or both of them. As mentioned at the beginning, I\u2019m going to argue that it can support the first claim (that evidence matters) but that it actually rejects the second claim (that we should favor interventions with more evidence, all else being&nbsp;equal).</p><p>Let's begin. I want to turn to this notion of resilience, and how we represent how much evidence we have, in terms of the credences we assign to propositions such as \u201cThis drug will cure this&nbsp;disease.\u201d</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/6BT3NmDOk8KUoyAiueISU2/89c8bab187fb11f6fd11df2af2ec8409/moral_value2.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/6BT3NmDOk8KUoyAiueISU2/89c8bab187fb11f6fd11df2af2ec8409/moral_value2.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/6BT3NmDOk8KUoyAiueISU2/89c8bab187fb11f6fd11df2af2ec8409/moral_value2.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/6BT3NmDOk8KUoyAiueISU2/89c8bab187fb11f6fd11df2af2ec8409/moral_value2.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/6BT3NmDOk8KUoyAiueISU2/89c8bab187fb11f6fd11df2af2ec8409/moral_value2.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/6BT3NmDOk8KUoyAiueISU2/89c8bab187fb11f6fd11df2af2ec8409/moral_value2.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/6BT3NmDOk8KUoyAiueISU2/89c8bab187fb11f6fd11df2af2ec8409/moral_value2.png?w=1800&amp;q=70 1800w\"></figure><p>See Figure 2. Take Case 1, which is an untested coin. I\u2019ve given you no information about how biased this coin is. It could be completely biased in favor of heads, it could be completely biased in favor of tails, or it could be a completely fair coin. You have no information to distinguish between any of these hypotheses. It seems like, in this case where you have no idea about what the bias of a coin is, if I were to ask you, \u201cWhat is the chance it lands heads on the next throw?\u201d, you\u2019re going to have to reply, \u201cIt\u2019s about 50%\", because you have no reason to favor a heads bias over a tails&nbsp;bias.</p><p>Now consider a different case, which is Case 2, the well-tested coin. When you flip the well-tested coin you get the following sequence: \u201cHeads, heads, heads, tails, heads, heads, tails, tails,\u201d until the coin has been flipped a million times. You had a very, very boring series of days flipping this&nbsp;coin.</p><p>For Case 1, in answer to the question \u201cWhat\u2019s the probability that the coin will land heads in the next flip?\u201d you should say, \"0.5 or 50%.\u201d In Case 2, where you tested the coin a lot and it\u2019s come up heads roughly 50% of the time, tails roughly 50% of the time, you should also say that the next flip is 50% likely to be&nbsp;heads.</p><p>The difference between the untested coin and the well-tested coin cases is reflected in the resilience levels of your credences. One kind of simple formulation of resilience, the credo-resilience, is how stable you expect your credences to be in response to new evidence. If my credences are high resilience, then there\u2019s more stability. I don\u2019t expect them to vary that much as new evidence comes in, even if the evidence is good and pertinent to the question. If they\u2019re low resilience, then they have low stability. I expect them to change a little in response to new evidence. This is the case with the untested coin, where I have no data about how good it is, so the resilience of my credence of 50% is fairly&nbsp;low.</p><p>It\u2019s worth noting that resilience levels can reflect either the set of evidence that you have about a proposition, or your prior about the proposition. For example, if you saw me simply pick the untested coin up out of a stack of otherwise fair coins, you would have evidence that it\u2019s fair. But if you simply live in a world that doesn\u2019t include a lot of very biased coins, then your prior might be doing a lot of the work that your evidence would otherwise do. These are the two things that generate&nbsp;credo-resilience.</p><p>In both cases with the coins, your credence that the coin will land heads on the next flip is the same \u2014 it\u2019s 0.5. Your credence of 0.5 about the tested coin is resilient, because you\u2019ve done a million trials of this coin. Whereas, your credence about the untested coin is quite fragile. It could easily move in response to new evidence, as we can see in Figure&nbsp;3.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/2x1JRYqFQo62YOeSagM2Gk/958e6882c964893ba7528d592d7ac841/moral_value3.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/2x1JRYqFQo62YOeSagM2Gk/958e6882c964893ba7528d592d7ac841/moral_value3.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/2x1JRYqFQo62YOeSagM2Gk/958e6882c964893ba7528d592d7ac841/moral_value3.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/2x1JRYqFQo62YOeSagM2Gk/958e6882c964893ba7528d592d7ac841/moral_value3.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/2x1JRYqFQo62YOeSagM2Gk/958e6882c964893ba7528d592d7ac841/moral_value3.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/2x1JRYqFQo62YOeSagM2Gk/958e6882c964893ba7528d592d7ac841/moral_value3.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/2x1JRYqFQo62YOeSagM2Gk/958e6882c964893ba7528d592d7ac841/moral_value3.png?w=1800&amp;q=70 1800w\"></figure><p>Take this third case. You start to test the untested coin, so you perform a series of flips with the coin, and you start to see a pattern. In a case like this, it looks like the coin is pretty heavily heads biased, or you at least start to quite rapidly increase your credence that it\u2019s heads biased. As a result, your credence that it\u2019s going to come up heads next time is much higher. Because you had less evidence about the untested coin, your credence of 0.5 in it landing heads was much more fragile, and now your credence changes in response to&nbsp;evidence.</p><p>You wouldn't change your credence if you got this sequence of all-heads on the well-tested coin, because more evidence means that your credences are more resilient. If you saw a series of five heads after performing a million trials where it landed heads roughly half the time, this all-heads sequence is just not going to make a huge difference to what you expect the next coin flip to&nbsp;be.</p><p>I think credo-resilience has some interesting implications. A lot of people seem to be kind of unwilling to assert probability estimates about whether something is going to work or not. I think a really good explanation for this is that, in cases where we don\u2019t have a lot of evidence, we have low credence in how good our credences&nbsp;are.</p><p>In essence, we think it\u2019s very likely that our credences are going to move around a lot in response to new evidence. We\u2019re not willing to assert a credence that we think is simply going to be false or inaccurate as soon as we gain a little bit more evidence. Sometimes people think you have mushy credences, that you don\u2019t actually have precise probabilities that you can assign to claims such as \u201cThis intervention is effective to Degree N.\u201d I actually think resilience might be a good way of explaining that away, by instead claiming, \u201cNo. You can have really precise estimates. You just aren\u2019t willing to assert&nbsp;them.\u201d</p><p>This has a huge influence on the value of information, which is the theme of this piece. Our drugs scenario is supposed to be somewhat analogous to different altruistic&nbsp;interventions.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/4bx0rnjYmA6AUU0amKIu2A/56116fdee058e558c5e705410ac208d6/moral_value4.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/4bx0rnjYmA6AUU0amKIu2A/56116fdee058e558c5e705410ac208d6/moral_value4.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/4bx0rnjYmA6AUU0amKIu2A/56116fdee058e558c5e705410ac208d6/moral_value4.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/4bx0rnjYmA6AUU0amKIu2A/56116fdee058e558c5e705410ac208d6/moral_value4.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/4bx0rnjYmA6AUU0amKIu2A/56116fdee058e558c5e705410ac208d6/moral_value4.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/4bx0rnjYmA6AUU0amKIu2A/56116fdee058e558c5e705410ac208d6/moral_value4.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/4bx0rnjYmA6AUU0amKIu2A/56116fdee058e558c5e705410ac208d6/moral_value4.png?w=1800&amp;q=70 1800w\"></figure><p>In the original case, we had the following kind of scenario, where we had expected 0.05, 0.05, and 0.06 for the three drugs. Of course, one thing that we can do here is gain valuable evidence about the world. Consider this case, where diagnosis is invented, at least as far as Disease A and Disease B are concerned. So, we can now diagnose whether you have Disease A or Disease B, and it costs 60 additional dollars to do so. Given this, if I diagnose you, then I can expect that conditional on diagnosis, if you have Disease A, you will live for 10 years, because I will be able to then pay an additional $100 to give you Drug A. If you have Disease B, I\u2019ll be able to pay an additional $100 to get you Drug&nbsp;B.</p><p>In this case, the value of diagnosis, including the cost of then curing you of the disease, is higher than any of the original interventions. Hopefully, it is intuitive that, rather than giving you Drug A, Drug B, Drug C, I should diagnose you and give you the correct&nbsp;drug.</p><p>The previous example was concerned with information about the world, which we think is valuable anyway. Now suppose I care about global poverty and I want to discover good interventions. To find out more about the world, I could, for example, research deficiencies that exist in India and see if there are good ways to improve&nbsp;them.</p><p>A different way we can gain valuable information is by finding out about interventions themselves [Figure 5]. An example would be to look at the actual intervention of Drug C and how effective it&nbsp;is.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/3wKKukmkfmQQOcScO8QYcw/20f837a210e5d830d36acdfd84502cda/moral_value5.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/3wKKukmkfmQQOcScO8QYcw/20f837a210e5d830d36acdfd84502cda/moral_value5.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/3wKKukmkfmQQOcScO8QYcw/20f837a210e5d830d36acdfd84502cda/moral_value5.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/3wKKukmkfmQQOcScO8QYcw/20f837a210e5d830d36acdfd84502cda/moral_value5.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/3wKKukmkfmQQOcScO8QYcw/20f837a210e5d830d36acdfd84502cda/moral_value5.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/3wKKukmkfmQQOcScO8QYcw/20f837a210e5d830d36acdfd84502cda/moral_value5.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/3wKKukmkfmQQOcScO8QYcw/20f837a210e5d830d36acdfd84502cda/moral_value5.png?w=1800&amp;q=70 1800w\"></figure><p>Suppose, in our ideal world, that it costs $5,000 to run a trial of Drug C that would bring you to certainty about its effectiveness. Suppose, also, that somehow you know it can only be very low impact or very high impact. You have a credence of about 0.5 that the results will show Drug C only extends life by two years in both Disease A and Disease B. This is our skeptical prior. But you also have a credence of about 0.5 that the drug will extend life by 10 years in both cases. We will also assume diagnosis has gone out the&nbsp;window.</p><p>Now imagine the scenario where you are currently prescribing Drug C. You obviously don't exist in any modern medical system, since you're ignoring the fact that there is low evidence here and you're going with the expected value as is. Then the question is \"What is the value of doing a trial of Drug C, especially given that you're already prescribing Drug C?\". If your credence of low impact goes to 1 \u2014 you suddenly discover that this drug is much less effective than you initially thought \u2014 then you're going to switch from Drug C to prescribing Drug A or B&nbsp;again.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/46008Tny3egkMO2YCk6qAA/1732b785b32a2d91b9dab1262d4c1477/moral_value6.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/46008Tny3egkMO2YCk6qAA/1732b785b32a2d91b9dab1262d4c1477/moral_value6.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/46008Tny3egkMO2YCk6qAA/1732b785b32a2d91b9dab1262d4c1477/moral_value6.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/46008Tny3egkMO2YCk6qAA/1732b785b32a2d91b9dab1262d4c1477/moral_value6.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/46008Tny3egkMO2YCk6qAA/1732b785b32a2d91b9dab1262d4c1477/moral_value6.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/46008Tny3egkMO2YCk6qAA/1732b785b32a2d91b9dab1262d4c1477/moral_value6.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/46008Tny3egkMO2YCk6qAA/1732b785b32a2d91b9dab1262d4c1477/moral_value6.png?w=1800&amp;q=70 1800w\"></figure><p>The per-patient benefit is going to go from two to five years of expected life in this case where you've run the trial. Whereas, if Drug C is a low-impact drug and you don't perform the trial, you don't spend the $5,000, but you will only get two years of additional life per $100. Every time you see Disease A and Disease B you will continue to prescribe Drug C and it will only give people an additional two years of life. Alternatively, if it is high-impact, then you will have been accidentally prescribing something that is very good and providing ten years of additional&nbsp;life.</p><p>We can see the trial adds 1.5 years of expected life per future treatment. Therefore, if there are more than 2,000 patients, the value of investing in a trial of Drug C is better than giving any of the drugs currently present. In this case the information value swamps the direct value of intervention&nbsp;here.</p><p>The value of investing in trials of Drug A or B is going to be negligible because credences about their effectiveness are already resilient. This builds up to the rather intrusive conclusion that expected value calculations might say that <strong>in cases where all else is equal, we should favour investing in interventions that have less evidence, rather than interventions that have&nbsp;more.</strong></p><p>This means, if the expected concrete value of two interventions is similar, we should generally favor investing in interventions that have less evidence supporting them. I'm going to use <i>concrete value</i> to mean \"non-informational value\". The idea here is that in such scenarios, the concrete values for interventions are the same, but the information value for one of them is much higher, namely the one where you have a much lower resilience credence generating your expected value&nbsp;calculation.</p><p>This gets us to the \"What does it mean, and what should we do\" part. Hopefully I have convinced you that, despite the fact that it was an intuitive proposition that we should favor things with more evidence, there is actually some argument that we should favor things that have less&nbsp;evidence.</p><p>When considering information value, there are three options available to us: \"explore, exploit or evade\" [Figure 7].</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/5R7RHbY3xmkIs0A8koCMS8/b8d7a0ac5ea3679a612979e488925360/moral_value7.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/5R7RHbY3xmkIs0A8koCMS8/b8d7a0ac5ea3679a612979e488925360/moral_value7.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/5R7RHbY3xmkIs0A8koCMS8/b8d7a0ac5ea3679a612979e488925360/moral_value7.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/5R7RHbY3xmkIs0A8koCMS8/b8d7a0ac5ea3679a612979e488925360/moral_value7.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/5R7RHbY3xmkIs0A8koCMS8/b8d7a0ac5ea3679a612979e488925360/moral_value7.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/5R7RHbY3xmkIs0A8koCMS8/b8d7a0ac5ea3679a612979e488925360/moral_value7.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/5R7RHbY3xmkIs0A8koCMS8/b8d7a0ac5ea3679a612979e488925360/moral_value7.png?w=1800&amp;q=70 1800w\"></figure><p>We can choose to explore; we can invest resources in interventions primarily for their information value. This means things like research, funding to gather data, and career trials. We can exploit, which means investing resources in an intervention for its concrete value. That means things like large project grants and entire career choices. Or we can evade; we can decide just not to invest in a given intervention \u2014 we either invest elsewhere, or completely delay&nbsp;investment.</p><p>The main difference between these three options is the reason for action. Take three people. Amy donates $100 to an existential risk charity to protect the future of humanity. She is just exploiting the value and just looks at the direct concrete value of this&nbsp;intervention.</p><p>Bella donates $100 to the same charity to find out how much good they can do in the world. She is donating mainly to explore and then later exploit. She'll think to herself, \"Let's see how valuable this is\", and if it is very valuable, then she will mine the value of&nbsp;it.</p><p>Carla donates $100 to the same charity, but for the reason of getting humanity to have more time to discover what the best causes are. She is exploiting the direct value the charity does in reducing existential risk in order to have more time to discover what the best causes are and then exploit those. In essence, she is exploiting to explore to&nbsp;exploit.</p><h2>When is exploring especially cost&nbsp;effective?</h2><p>Essentially, when there are three&nbsp;features:</p><ol><li>When there is more uncertainty about the direct value of an intervention. This means options that have high expected value, but low&nbsp;resilience.</li><li>When there are high benefits of certainty about the direct value. We would then be able to repeatedly mine it for&nbsp;value.</li><li>When there are low information costs. This means the information is not too costly to obtain and the delay is low cost (you don't want to be looking for information when cars are driving towards you, as the cost of not taking action and getting out the way is&nbsp;high!).</li></ol><p>The question I have is: \"Is gaining information especially valuable for effective altruists?\" Another way to put it is: \"Is information essentially its own cause area, within effective&nbsp;altruism?\"</p><p>There is a lot of uncertainty within and across good cause areas, particularly if we consider long-term indirect effects. We don't know about the long-term indirect effects of a lot of our&nbsp;interventions.</p><p>The benefits of certainty are high, as we expect to use this information in the long term. Effective altruism is a multi-generational project as opposed to a short-term intervention. So, you expect the value of information to be higher, because people can explore for longer and find optimal&nbsp;interventions.</p><p>To some degree there are low information costs while the movement is young and there is still a lot of low-hanging fruit. This comes with caveats. Maybe you're a bit like Carla and you're very worried that we're screwing up the climate, or that nuclear war is going to go terribly wrong, in which case, maybe you think we should be directly intervening in those&nbsp;areas.</p><h2>What difference would exploring more make to effective&nbsp;altruism?</h2><p>I think we could probably invest a lot more time and resources in interventions that are plausibly good, in order to get more evidence about them. We should probably do more research, although I realise this point is somewhat self-serving. For larger donors, this probably means diversifying their giving more if the value of information diminishes steeply enough, which I think might be the&nbsp;case.</p><p>Psychologically, I think we should be a bit more resilient to failure and change. When people consider the idea that they might be giving to cause areas that could turn out to be completely fruitless, I think they find it psychologically difficult. In some ways, just thinking, <i>\"Look, I'm just exploring this to get the information about how good it is, and if it's bad, I'll just change\"</i> or <i>\"If it doesn't do as well as I thought, I'll just change\"</i> can be quite comforting if you worry about these&nbsp;things.</p><p>The extreme view that you could have is <i>\"We should just start investing time and money in interventions with high expected value, but little or no evidential support.\"</i> A more modest proposal, that I tentatively endorse, is <strong>\"We should probably start explicitly including the value of information, and assessment of causes and interventions, rather than treating it as an afterthought to concrete value.\"</strong> In my experience, information value can swamp concrete value; and if that is the case, it really shouldn't be an afterthought. Instead it should be one of the primary drivers of values, not an afterthought in your calculation&nbsp;summary.</p><p>In summary, evidence does make a difference to expected value calculations via the value of information. If the expected concrete value to interventions is the same, this will favour testing out the intervention with less evidential support, rather than the one with more. Taking value of information seriously would change what effective altruists invest their resources in, be it time or&nbsp;money.</p><h1>Q&amp;A</h1><p><strong>Question:</strong> What does it mean to have credence in a credence \u2014 for example, maybe an 80% chance that it has 50% chance of it working, etc., etc.? Does it recourse down to&nbsp;zero?</p><p><strong>Amanda Askell:</strong> It's not that you have a credence, but your credence in your credence being the same or changing in response to new evidence. There are a lot of related concepts here. There are things like your credence about the accuracy of your credence. So, it's not \"I have a credence that I have a credence of 0.8.\" This is a separate thing \u2014 my credence that in response to this trial, I will adjust my credence from 0.5 to either 0.7 or 0.2 is the kind of credence that I'm talking&nbsp;about.</p><p><strong>Question:</strong> Do you think there's a way to avoid falling into the rabbit hole of the nesting credences of the kind that the person might have been referring&nbsp;to?</p><p><strong>Amanda Askell:</strong> I guess my view, in the boring philosophical jargon, is that credences are dispositional. So, I do think that you probably have credences over infinitely many propositions. I mean, if I actually ask you about the proposition, you'll give me an answer. So, this is a really boring kind of answer, which is to say, \"No, the rabbit hole totally exists and I just try and get away from it by giving you a weird non-psychological account of&nbsp;credences.\"</p><p><strong>Question:</strong> Is information about the resilience captured by a full description of your current credences across the hypothesis space? If not, is there a parsimonious way to convey the extra information about&nbsp;resilience?</p><p><strong>Amanda Askell:</strong> I'm trying to think about the best way of parsing that. Let's imagine that I'm just asking your credence. I say that the intervention has value N, for each N I'm considering. That will not capture the resilience of your credence, because it's going to be how you think that's going to adjust in response to a new state. If you include how things are going to adjust in response to a new state, then yes, that should cover resilience. So it just depends on how you're carving up the&nbsp;space.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "KYvhQhSzxnE6gtPoq", "title": "Claire Zabel: Open Philanthropy Project \u2014 general update and science highlights", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=nIGZ4Vh0IZk&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=25\"><div><iframe src=\"https://www.youtube.com/embed/nIGZ4Vh0IZk\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this talk, Claire Zabel shares some of the Open Philanthropy Project's big updates from 2017. She focuses on their progress in their scientific research program and discusses some of the grants they've made that she thinks exemplify their approach to hits-based giving.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "6hmiJtPjH98NL7qAp", "title": "Michelle Hutchinson: Harnessing the power of academia", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=ZpuJBLTfTTU&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=24\"><div><iframe src=\"https://www.youtube.com/embed/ZpuJBLTfTTU\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Although some of effective altruism\u2019s founders are academics, EA has up to now mostly spread outside of academia. Michelle Hutchinson describes the possible power of academia and how important it might be for EA to engage with it, followed by plans to take advantage of that power and how those plans are progressing.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "879KteNrmom3Tzsva", "title": "Neha Narula: The future of blockchain technology \u2014 beyond digital currency to global coordination", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=V2Ijy7A1fw4&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=23\"><div><iframe src=\"https://www.youtube.com/embed/V2Ijy7A1fw4\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Cryptocurrencies like Bitcoin and Ethereum have enabled a new architecture for peer-to-peer transactions without a trusted broker. As we have seen with the internet, distributed protocols can give users the ability to directly transact, circumventing slow-moving and sometimes corrupt intermediaries. One benefit of these systems is protecting consumers from payment censorship. Another potential benefit is opening up the currently privatized infrastructures underlying our financial and data systems. Beyond the realm of finance, cryptocurrency and blockchain technology development have sparked an effort to rethink the way that power and authority is embedded into the digital infrastructures that pervade our social, political, and economic lives.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "JxCCwfrq3BqfjB3C6", "title": "Vikash Mansinghka: AI-assisted data analysis for humanitarian causes", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=Uo5KZTUVP_k&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=22\"><div><iframe src=\"https://www.youtube.com/embed/Uo5KZTUVP_k\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Today there is an unprecedented opportunity to do good using public data. The main bottleneck is that we can't direct enough statisticians to work on humanitarian projects.&nbsp;</i></p><p><i>MIT engineers have been developing BayesDB, an open-source platform that addresses some of the these problems. Novice BayesDB users can answer data analysis questions in seconds or minutes with a level of rigor that otherwise requires hours or days of work by someone with advanced training in statistics plus good statistical judgment. This talk will focus on what and why BayesDB is, not how it works. It will use examples from collaborations with the Bill &amp; Melinda Gates Foundation and Boston Children's Hospital, showing how BayesDB can jointly analyze neuroimaging data and survey data collected from kids in slums in India. It will also discuss new initiatives aimed at using BayesDB to build empirical maps of poverty, inequality, and psychological suffering. Examples include data on PTSD vulnerability and resilience in US Army veterans, including data on adverse events caused by psychotropic medication, and on electronic health record data for members of poor, rural US populations. It will also include a brief review of other AI technology being developed by Vikash Mansinghka's lab, the MIT Probabilistic Computing Project.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "MjFyHTdhrJnsqLAY5", "title": "Adam Marblestone: What sets the exponent of neuroscience progress", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=0eX1UqMmaLM&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=21\"><div><iframe src=\"https://www.youtube.com/embed/0eX1UqMmaLM\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Neuroscience needs to make a transition from small science to large scale systems engineering and analysis, analogous to the changes in 20th century physics that gave rise to particle accelerators, space telescopes, the Standard Model and the transistor. Yet the complexity and diversity of neuroscience has made it difficult to know how to proceed: a different kind of integration is needed in the science of complexity than in the science of simplicity.&nbsp;</i></p><p><i>Using examples from brain mapping technology and from the neuroscience-AI theoretical interface, Adam Marblestone argues that the achievement of accelerated progress in the field is gated by support (organizational, cultural, financial) for truly unconventional integrations of otherwise disparate ideas and methods. Such approaches sometimes require us to deviate from familiar modes of hypothesis-driven science, as we search for integrative frameworks and build new kinds of observational tools, but there is a chance that they could finally allow us to explore the relevant regions of hypothesis space for understanding how the brain works.</i></p><p><i>In the future, we plan to post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "sbXQt25LXsiLjD5uH", "title": "Stefan Schubert: Moral cooperation ", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=roXYoAI949Y&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=20\"><div><iframe src=\"https://www.youtube.com/embed/roXYoAI949Y\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Stefan Schubert talks about moral cooperation: how people with diverging moral goals can cooperate to realize those goals.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "Zz3JukL2doKQobxMc", "title": "Expanding the moral circle (Kelly Witwicki)", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=TmxcDnV7DwE&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=19\"><div><iframe src=\"https://www.youtube.com/embed/TmxcDnV7DwE\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p>Approximately 95% of the 100 billion animals being raised for food right now suffer in \"factory farm\" conditions. There are at least a thousand times more wild animals, many of them in dire circumstances, and vastly more beings might exist in the distant future as humanity's technological capabilities grow. All of these individuals lie outside humanity's current moral circle, so substantial progress could be made by helping society account for their interests.&nbsp;</p><p>Sentience Institute researches and promotes the most effective strategies to expand humanity\u2019s moral circle to include all sentient beings, meaning those beings who experience happiness and suffering, regardless of their sex, race, species, substrate, location, or any other characteristic.&nbsp;</p><p>This talk covers Sentience Institute\u2019s plans and introduces its first publication: \"<a href=\"https://www.sentienceinstitute.org/foundational-questions-summaries\">A Summary of Evidence for Foundational Questions in Effective Animal Advocacy</a>\".</p><p>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript, contact <a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\">Aaron Gertler</a> \u2014 he can help you get started.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "arzY8HSfC4PtfxA4w", "title": "Jason Matheny: Effective altruism in government", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=g05om2NJwco&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=18\"><div><iframe src=\"https://www.youtube.com/embed/g05om2NJwco\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this 2017 talk, </i><a href=\"https://www.iarpa.gov/\"><i>IARPA</i></a><i> director Jason Matheny talks about how effective altruists can have an impact through entering government. He draws widely from his own experience in the US government.&nbsp;</i></p><p><i>The below transcript is lightly edited for&nbsp;readability.</i></p><h1>The Talk</h1><p>I don't have any slides just because they're much harder to get through government review, which will be a big part of my talk today on what to do and not to do in government. It's great to see so many young people who are thinking about career options. I feel like I'm close to dying now, so there's only so much good remaining I can do, unless I'm cryonically preserved and resuscitated later. You all have so many decades left to do an enormous amount of good with your lives, so thank you for spending at least 50 minutes of that life to think about options in&nbsp;government.</p><p>I'm going to describe a few different paths to doing good within government that don't rely on spending an entire career in government. It might be a two-year stint, or a five-year stint, but still allowing you to accomplish a huge amount of&nbsp;good.</p><p>I'm going to talk about three things. The first is how I came into government. The second is the roles for EAs working within government. The third is some practical advice on picking jobs within government, whether short-term or&nbsp;longer-term.</p><p>My own route was sort of circuitous in that I started working in college, planning to become an architect. Shortly after I graduated, I found an orphan copy of the <a href=\"https://openknowledge.worldbank.org/handle/10986/5976\">1993 World Bank World Development Report</a>, which both dates me and dates a lot of my perspective on problems of altruism. The report was really focused on how you can do cost-effectiveness analysis on health and development. For me, this was the first time I was ever exposed to an argument about how cheaply you can save lives and significantly reduce suffering. This was one of the first reports that looked at the cost per <a href=\"https://en.wikipedia.org/wiki/Disability-adjusted_life_year\">DALY</a> averted for a range of different health interventions. I thought this application of cost-effectiveness analysis to health and development was pretty eye-opening. For one, it showed that you could save millions of live for less than about $1,000 per life. But it also showed that there were tremendous differences between the most and the least cost-effective interventions, so that our decisions about what to invest in have a huge&nbsp;impact.</p><p>I ended up deciding not to become an architect, or else I'd be giving a talk on effective altruism in architecture. I decided to instead go into global health, and worked for several years on global infectious diseases - especially malaria, tuberculosis, and HIV in South Asia, but also in East Asia. It was in 2002 that I made another career shift. This was the same year that the first virus was synthesized from scratch, <i>de novo</i>. It turned biology into something more closely approximating computer science, or an engineering discipline. You could now take the chemical constituents of DNA and create a new&nbsp;genome.</p><p>At that time, the technology was fairly primitive. The longest virus that you could assemble was polio. That wasn't too much to worry about. We already had vaccines for polio. We knew how to control polio. But I and most of the people who I had worked with had worried that somebody would apply this technology to recreate smallpox - to recreate the <a href=\"https://en.wikipedia.org/wiki/1918_flu_pandemic\">1918 influenza</a> that killed over 50 million people in one year. Or, they could make something much worse than any naturally occurring virus, since there are limits to nature's ingenuity that might be outpaced by human&nbsp;ingenuity.</p><p>So I moved from working on naturally occurring global infectious diseases to working on defences against engineered threats. That work then led me to places like <a href=\"https://www.fhi.ox.ac.uk/\">the Future of Humanity Institute</a>, thinking about how we wrestle with risks from emerging technologies. I thought about ways that I could have an impact on this area. One way seemed to be doing research myself, but I didn't think I was especially smart in doing that research. It seemed like there were people who were smarter than me, like Nick Bostrom, who could be doing that research. I thought: how could I deliver more funding to people who were smarter than me? That's why I joined&nbsp;government.</p><p>I came to IARPA to put a multiplier effect on my effort and the effort of other researchers. My goal was to set up a budget in which I could fund important research in a range of areas including risk assessment, technical forecasting, work on biosecurity, nuclear security, cyber security, and assessments of future risks from things like autonomous weapons or AI accidents. The work that I've done at IARPA has convinced me that there's a lot of low-hanging fruit within government positions that we should be picking as effective altruists. There are many different roles that effective altruists can have within government&nbsp;organizations.</p><p>I'm going to tell you about a few of them. I'll limit it to the three areas that I have some background in, which are global health, animal welfare and catastrophic risks. But I think that government is in general a place that has leverage over a variety of different pressing societal issues. Thus if I'm leaving out some topics that's not because I think government doesn\u2019t have influence on those topics. It's just because I'm&nbsp;ignorant.</p><p>In global health, government funding has an enormous impact on the development of new vaccines, antivirals, and antibiotics, as well as other therapies. Much of that work is conducted at the <a href=\"https://www.niaid.nih.gov/\">National Institute for Allergies and Infectious Diseases</a>, that funds basic and applied research in infectious diseases, as well as the development of new therapies. The <a href=\"https://www.fda.gov/\">FDA</a> has an important role determining which kinds of drugs and vaccines are ultimately introduced. Despite being a regulatory agency, they make really interesting innovations, incentive systems, and reward systems, including things like priority review vouchers that can accelerate the introduction of new vaccines for important diseases. And there are places like <a href=\"https://www.fic.nih.gov/Pages/Default.aspx\">the Fogarty Center at the National Institutes of Health</a>, which does things like economic analysis of international health interventions. Then there are the more operational arms of the US government, like the <a href=\"https://www.cdc.gov/\">Centers for Disease Control</a> and <a href=\"https://www.usaid.gov/\">the US Agency for International Development</a>, that have direct impacts on health and development&nbsp;overseas.</p><p>In animal welfare, <a href=\"https://www.usda.gov/\">the USDA</a> has an important role to play in establishing policies that govern some treatments of animals. They also have a small research budget, some of which is used - and a larger amount of which could be used - to develop better, healthier alternatives. State legislatures also have a big role to play in animal welfare as they can pass laws surrounding animal handling and&nbsp;slaughter.</p><p>Then on catastrophic risks where I've spent most of my time, governments have a very significant role. A lot of those risks are influenced by government decisions, both positively and negatively. Multiple organizations work on preventing nuclear war, biological warfare, or accidents, as well as cyber warfare, and the misuse of various emerging technologies. I\u2019ll go through a few of the most important of those organizations. First you have <a href=\"https://en.wikipedia.org/wiki/United_States_National_Security_Council\">the National Security Council</a>, a part of the White House that informs national security decision making, including decisions about war. Another is <a href=\"https://www.whitehouse.gov/ostp\">the White House Office of Science and Technology Policy</a>, which looks at emerging technologies and risks associated with them. That office has groups that examine a range of important technologies, including AI, bio-technology, and neuroscience among other&nbsp;topics.</p><p>Within <a href=\"https://www.defense.gov/\">the Department of Defense</a>, there is <a href=\"https://en.wikipedia.org/wiki/Office_of_Net_Assessment\">the Office of Net Assessment</a>, which in my view is one of the most unusual organizations within government, as well as one of the most important. It looks at long-range security issues that could be decades in the making. For example, what changes in future weapon systems are likely to disrupt deterrents? What would be the consequences of strategic miscalculation with nuclear&nbsp;weapons?</p><p><a href=\"http://www.dtra.mil/\">The Defense Threat Reduction Agency</a> within the Department of Defense is the lead agency responsible for countering chemical, biological, radiological, and nuclear weapons. <a href=\"https://www.fema.gov/\">The Federal Emergency Management Agency</a> is responsible for considering worst-case scenarios that could affect the US and developing mitigations against them. Then there are many others that deal with very specific threats such as the <a href=\"http://www.stratcom.mil/\">Defense Department's Strategic Command</a>, which is responsible for the United States\u2019s nuclear weapons and their safety. <a href=\"https://www.phe.gov/about/BARDA/Pages/default.aspx\">BARDA</a>, which is a part of <a href=\"https://www.hhs.gov/\">Health and Human Services</a>, is responsible for developing medical countermeasures against bioterrorism. The intelligence agencies like <a href=\"https://www.cia.gov/\">CIA</a> and <a href=\"http://www.dia.mil/\">DIA</a>, assess how advanced a particular group's biological weapons program is, or their ability to access disruptive technologies, or the likelihood of industrial accidents, for example in foreign biology&nbsp;labs.</p><p>Across all three of the EA topics that I mentioned, in global health, in animal welfare, and in catastrophic risks, one cost-effective route to having an impact is to affect the funding of new technologies that could in some ways obviate the need for certain kinds of harmful technologies or reduce the risks of technologies by making sure they're sufficiently protected through safety engineering. For funding scientific and technological research, there are a few important organizations within government. There's the <a href=\"https://www.whitehouse.gov/omb\">White House Office of Management and Budget</a>, which helps to set the White House budget requests. Often we'll find even fairly junior folks who are putting their weight on multi-billion dollar decisions. It really is extraordinary that even fairly junior positions can have incredible influence. If you think of this just in terms of an expected value calculation, even a 10% probability of affecting a $10 billion decision means a billion dollars in expectation, and can be hugely consequential on topics such as nuclear safety, biological safety, future of autonomous weapons and so&nbsp;forth.</p><p>There are <a href=\"https://appropriations.house.gov/\">the Congressional Appropriations Committees</a> (link to <a href=\"https://www.appropriations.senate.gov/\">senate page</a>) that approve the budgets from the White House. Here, too, you find even fairly junior staffers that have an incredible impact. Then there are the organizations that take the budgets that they've been given and freely decide how to allocate them. Those include places like <a href=\"https://www.nih.gov/\">the National Institutes of Health</a>, <a href=\"https://www.nsf.gov/\">the National Science Foundation</a>, as well as the ARPAs - the intelligence <a href=\"https://www.iarpa.gov/\">IARPA</a> where I work, <a href=\"https://www.darpa.mil/\">DARPA</a>, <a href=\"https://www.dhs.gov/science-and-technology/hsarpa\">HSARPA</a> and <a href=\"https://arpa-e.energy.gov/\">ARPA-E</a>. At those organizations the program managers, who are typically in their 30s say, have come out of graduate school programs and a science or engineering discipline. They've spent a few years working in a lab, sometimes within academia or within industry. Then they spend a term-limited time in the government, usually not exceeding five years. They're given an extraordinary amount of latitude. They're given a budget of several tens of millions of dollars with the expectation, the trust, that they will invest that money as cost-effectively as possible in solving a particular technical problem. For IARPA, those problems are often associated with reducing the risks of emerging&nbsp;technologies.</p><p>As one example, we have a brilliant biologist at IARPA, John Julias, who runs a program called <a href=\"https://www.iarpa.gov/index.php/research-programs/fun-gcat\">Fun GCAT</a>, which is focused on developing new systems for screening the sequences that go into DNA synthesizers. Can you determine whether this is a safe sequence or a dangerous sequence? That's the kind of work that we really need program managers to do, and we've entrusted John with a +$50 million budget which he uses to fund work here at Harvard, at MIT, and at many other universities and companies in order to advance this goal of reducing risks from synthetic biology. It's much more money I think than at least I could've expected to earn in my lifetime, but we give it, we entrust it, to a program manager to spend as wisely as possible - with the rigor of spending a quarter of that money on testing and evaluation to figure out whether the investments that we're making are actually making a difference, and whether we can accurately assess the risks from, say, a novel sequence. Those are program&nbsp;managers.</p><p>Agency directors can further direct hundreds of millions or even billions of dollars to key projects. Again, even if those are only, say, 10% as effective as funding that would be given outside of a bureaucracy, the expected value of those investments is quite large and can have a dramatic impact. Within government, there are also other levers that one can pull. At IARPA, we've not only been able to erect a large budget on reducing catastrophic risks, but we've also been able to engage in policy discussions. We've led groups within the White House on the long-term impacts of AI and of biotechnology. We co-led the White House AI R&amp;D strategy, and we've advised the National Security Council on other emerging technologies. There are some decisions that are made only by governments, and some of those decisions are highly consequential. They include decisions like going to war, or what weapon systems will be fielded, or how technologies will be embedded within larger critical systems. It makes sense to engage more effective altruists within these positions where they can influence those&nbsp;decisions.</p><p>One can also have influence on the outside, working as a contractor within a government agency. Most of the people who work at IARPA are contractors rather than government employees. The amount of expertise that we have to draw on is too vast to hire them all ourselves directly, especially with short-term positions. So we hire computer scientists, and biologists, and chemists, and physicists, and neuroscientists, and sociologists, political scientists, and cognitive psychologists, because we need them all. We also need lawyers, and we even need philosophers. We have a program on applied philosophy called <a href=\"https://www.iarpa.gov/index.php/research-programs/create\">CREATE</a>, which is a program to develop new systems for argumentation and informal reasoning that can lead intelligence analysts to make better&nbsp;judgements.</p><p>So we need lots of help. We need them from lots of places including contractors, but also think-tanks. There are a range of think-tanks that inform the policy-making process that sometimes have a quite deep influence on administration. For instance, <a href=\"http://www.heritage.org/\">the Heritage Foundation</a> has a substantial influence on the current administration, while past administrations have been influenced by other think tanks such as <a href=\"https://www.brookings.edu/\">Brookings</a>, <a href=\"https://www.cnas.org/\">the Center for a New American Security</a>, <a href=\"https://www.csis.org/\">the Center for Strategic and International Studies</a>, and Harvard's own <a href=\"https://www.belfercenter.org/\">Belfer Center</a>. There are many others that help shape the decision-making of government leaders. Hence, that's another way you can have an influence on&nbsp;government.</p><p>I'm going to close in my final few minutes just by providing some general advice if you're interested in pursuing a job, whether short-term or long-term, within government. I would recommend really thinking about opportunities to move across and among the different sectors of society, government, industry, academia, and NGOs, because there's a need for horizontal transfer of knowledge and best practices. If all of our folks within government have come from government straight out of school, that will prevent us from being able to adopt best practices from industry or from academia. So there really is a need I think for continuous cycling throughout a career, bouncing around between the different sectors in order to bring knowledge across&nbsp;them.</p><p>My first suggestion is to reach out to <a href=\"https://80000hours.org/\">80,000 Hours</a>, which I think has been pulling together some advice about government jobs (see below). I think one of the pieces of advice is at least to consider it as an option, because we are nowhere near the saturation point of effective altruists going into government positions. There are fairly junior positions across government that have a high potential impact that we have trouble recruiting&nbsp;for.</p><p>My second suggestion is to get to know the people who work within the organizations where you'd like to work. You can learn a lot about those organizations, their structure, and their staff just from online websites as well as Wikipedia. You can find the biographies of some of the people whose careers you might want to mimic. One strategy is just to reverse engineer their biography. Figure out what the steps are that seem critical in getting to the positions that you would like to have in the future. On that point, many of these people are lawyers, but just as many of them are scientists and engineers, and we do need more philosophers in government as well. But I think you'll find the diversity of talent that we need is ever growing. There's a particular intersection between policy and technology that is extremely difficult to recruit for. So for people who are still picking their major or their concentration for a thesis, if you look at the science policy of blank, pretty much any of the topics that are critical on our list have not been saturated with attention. There's still lots of low-hanging fruit to&nbsp;pick.</p><p>My last suggestion is to reach out to me, especially if you're interested in pursuing a job, short-term or long-term, in national security or reducing global catastrophic risks. Mostly because I really need the help. That's&nbsp;it.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "6FoCyz3yy63MLZMwu", "title": "Bruce Friedrich: Can food technology and markets get us to animal liberation?", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=WByIMdjjZ3g&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=17\"><div><iframe src=\"https://www.youtube.com/embed/WByIMdjjZ3g\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Can food technology and markets get us to animal liberation more quickly than educational campaigns? Bruce Friedrich is interviewed by Brian Kateman in this EA Global: Boston 2017 discussion on clean and plant-based meat.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "ZHQaYgK9yasadpPgH", "title": "Alison Fahey: Is universal basic income a viable way to support humans in the face of technology", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=81JzQ55jIfQ&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=16\"><div><iframe src=\"https://www.youtube.com/embed/81JzQ55jIfQ\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>As technology advances, fewer jobs require human labor. Governments from Canada to Finland are experimenting with a universal basic income scheme as a way to ensure that their constituents maintain some level of economic security, even in the face of shrinking employment opportunities. Meanwhile, a pilot study in Kenya is testing whether universal basic income can be an effective way to lift citizens out of poverty. Can guaranteeing everyone a minimum amount of money reduce or prevent poverty? What effect will a universal basic income have on the overall economy?&nbsp;</i></p><p><i>Based at MIT, J-PAL is a network of economists who have run over 800 randomized controlled trials in over 80 countries to ensure that policy is informed by scientific evidence. Alison Fahey, Senior Policy Manager at J-PAL Global, shares insights from some of these randomized controlled trials that can help shed light on the possible impacts of universal basic income schemes.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "Ldyc8nPqmvoNrq6eN", "title": "Julia Wise: Giving What We Can", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=hg3X6cFe1xM&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=15\"><div><iframe src=\"https://www.youtube.com/embed/hg3X6cFe1xM\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Julia Wise gives a short talk and updates on the work of Giving What We Can, a community of people who have pledged to donate 10% of their income to the most effective organizations they can find.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "7g7ou3g8nM7seASBP", "title": "Why s-risks are the worst existential risks, and how to prevent them", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=jiZxEJcFExc&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=14\"><div><iframe src=\"https://www.youtube.com/embed/jiZxEJcFExc\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Effective altruists focussed on shaping the far future face a choice between different types of interventions. Of these, efforts to reduce the risk of human extinction have received the most attention so far. In this talk, Max Daniel makes the case that we may want to complement such work with interventions aimed at preventing very undesirable futures (\"s-risks\"), and that this provides a reason for, among the sources of existential risk identified so far, focussing on AI risk.</i></p><h2><strong>Transcript: Why S-risks are the worst existential risks, and how to prevent them</strong></h2><p>I'm going to talk about risks of large scale severe suffering in the far future or S-Risks. And to illustrate what S-Risks are about, I'd like to start with a fictional story from the British TV series Black Mirror, which some of you may have seen. So, in this fictional scenario, it's possible to upload human minds into virtual environments. In this way, sentient beings can effectively be stored and run on very small computing devices, such as the wide-act shaped gadget you can see on the screen here. Behind the computing device, you can see Matt.</p><p>Matt's job is to sell those virtual humans as virtual assistants. And because this isn't a job description that's particularly appealing to everyone, part of Matt's job is to convince these human uploads to actually comply with the request of their human owners. And so in this instance, human upload Greta, which you can see here, is unwilling to do this. She's not thrilled with the prospect of serving for the rest of her life as a virtual assistant. In order to break her will, in order to make her comply, Matt increases the rate at which time passes for greater. So, while Matt only needs to wait for a few seconds, during that time, Greta effectively endures many months of solitary confinement.</p><p>So, I hope you agree that this would be an undesirable scenario. And now, fortunately, of course, this particular scenario is quite unlikely to be realized. So, for any particular scenario we can imagine to unfold, it's pretty unlikely that it will be realized in precisely this form. So that's not the point here. However, I'll argue that there in fact is a broad range of scenarios that we face risks of scenarios that are in some ways like this scenario or even worse.</p><p>And I'll call these risks S-risks. I'll first explain what these S-risks are, contrasting them with the more familiar existential risks or X-risks. And then in a second part of the talk, I'll talk a bit about why as effective altruists we may want to prevent those S-risks and how we could do this. So, the way I'd like to introduce S-risks will be a subclass of the more familiar existential risks.</p><p>As you may recall, these have been defined by Nick Bostrom as risks where an adverse outcome would either completely annihilate Earth-originating intelligent life or at least permanently and drastically curtail its potential. Bostrom also suggested in one of his major publications on existential risk that one way to understand how these risks differ from other kinds of risks is to look at how bad this adverse outcome would be along two dimensions. And these dimensions are the scope and the severity of the adverse outcome that we're worried about. I've reproduced one of Bostrom's central figures here.</p><p>You can see a risk scope on the vertical axis. That is, here we ask how many individuals would be negatively affected if the risks were realized? Is it just a small number of individuals? Is it everyone in a particular region or even everyone alive on Earth? Or in the worst case, everyone alive on Earth plus some or even all future generations? And the second relevant dimension is the severity. So here we ask for each individual that would be affected, how bad would the outcome be? For instance, consider a fatal car crash, the risk of a single fatal car crash. If this happened, this would be pretty bad. You could die, so it would have pretty high severity, but it would only have personal scope because in any single car crash, only a small number of individuals are affected. However, there are other risks which would have even worse severity. For instance, consider factory farming. We commonly believe that, for instance, the life of say, chickens and battery cages is so bad that it would be better to not bring these chickens into existence in the first place. That's why we believe that it's a good thing that most of the food at this conference is vegan. Another way to look at this is that, I guess, some of you would think that the prospect of being tortured for the rest of your life probably would be even worse than a fatal car crash. So, there can be risks which even worse severity than terminal risks such as a fatal car crash. And as risks are now risks which with respect to their severity are about as bad as factory farming and that they are about outcomes that would be even worse than non-existence, but would also have much greater scope than a car crash or even factory farming. And that they could potentially affect a very large number of beings for the entire far future across the whole universe.</p><p>So, this explains why in the title of the talk I claimed that S-risks are the worst existential risks. I said this because I just defined them to be risks of outcomes which have the worst possible severity and the worst possible scope. So, one way to understand this and how they differ from other kinds of existential risk is to zoom in on the top right corner of the figure I showed before. This is the corner that shows existential risks.</p><p>These are risks that would at least affect everyone alive on earth plus all future generations. So, this is why Boston called them risks of pan-generational scope and risks which would be at least what Boston called crushing which we can roughly understand as removing everything that would be valuable for those individuals. And one central example of such existential risks are risks of extinction. And these are risks that have received a lot of attention in the EA community already. They have pan-generational scope because they would affect everyone alive and would also remove the rest of the future and they would be crushing because they would remove everything valuable. But S-risks are another type of existential risk which are also conceptually included in this concept of existential risk. They are risks that would be even worse than extinction because they contain a lot of things we disvalue such as for instance intense involuntary suffering and risks that would have even larger scope because they would affect a significant part of the universe.</p><p>So, you could think of the Black Mirror story from the beginning and imagine that Greta endures her solitary confinement for the rest of her life and that it's not only one upload but a large population of such uploads across the universe. Or you could think of something like factory farming with a much, much larger scope for some reason realized in many ways across the whole galaxy. So, I have now explained what conceptually S-risks are. They are risks of severe involuntary suffering on a cosmic scale thereby exceeding the total amount of suffering we've seen on earth so far. And this makes them a subclass of existential risk but a distinct subclass from the more well-known extinction risks. So far, I've just defined some conceptual term. I've called attention to some kind of possibility.</p><p>But what may be more relevant is as effective altruists, if reducing S-risks something we can do and if so, if it's something we should do. And let's make sure that we understand this question correctly. Because all plausible ethical views agree that intense involuntary suffering is a bad thing. So, I hope you can all agree that reducing S-risks is a good thing. But of course you're here because you're interested in effective altruism. That is, you just don't just want to know whether there is something good you can do or we're interested in identifying the most good we can do. We realize that doing good has opportunity cost and we really want to make sure to focus our time, focus our money on the most impact we can have. So, the question here really is, and the question I'd like you to discuss is, can reducing S-risks meet this higher bar? For at least some of us, could it be the best use of your time or your money to invest this into reducing as risk as opposed to doing other things that could be good in some sense? And this of course is a very challenging question and I won't be able to conclusively and comprehensively answer that question today. And to illustrate just how complex this question is and also to make really clear what kind of argument I'm not making here, I'll first introduce a flawed argument, an argument that doesn't work for focusing on reducing S-risks. So this argument will roughly go as follows.</p><p>First premise, the best thing to do is preventing the worst risks. Second premise, well, S-risks by definition are the worst risk, so conclusion you may think the best thing to do is to prevent S-risks. Now with respect to premise one, let's get one potential source of misunderstanding out of the way. One way to understand this first premise is that it could be a rock bottom feature of your ethical worldview.</p><p>So, you could think whatever you expect to happen in the future, you have some specific additional ethical reasons to focus on preventing the worst case outcomes. Some kind of maximum principle or perhaps prioritarianism applied to the far future. This however is not the sense I'm going to talk about today. So, if your ethical view contains some such principles, I think they give you additional reasons for focusing on S-risks, but this is not what I'm going to talk about. What I'm going to talk about is that there are more criteria that are relevant for identifying the ethically optimal action than the two dimension of risks we have looked at so far.</p><p>Right, so far we've only looked at if a risk was realized, how bad would the outcome be in terms of its severity and its scope. And in this sense, S-risks are the worst risks, but in this sense I think premise one is not clearly true. Because when deciding what's the best thing to do, there are more criteria that are relevant and many of you will be well familiar with those criteria because they are rightly getting a lot of attention in the EA community. So, in order to see if reducing S-risks is the best thing we can do, we really need to look at how likely would it be that these S-risks are realized, how easy is reducing them, in other words, how tractable is it and how neglected is this endeavor. Are there already a lot of people or organizations doing it, how much attention is it already getting? So, these criteria clearly are relevant. Even if you are a prioritarian or whatever and think you have a lot of reasons to focus on the worst outcomes, if for instance their probability was zero or there was absolutely nothing we could do to reduce them, it would make no sense to try to do it. So, we need to say something about the probability, the tractability and the neglectedness of these as risks and I'll offer some initial thoughts on these in the rest of the talk. What about the probability of these S-risks? What I'll argue for here is that S-risks are at least not much more unlikely than extinction risks from superintelligent AI, which are a class of risks at least some parts of the community take seriously and think we should do something about them.</p><p>And I'll explain why I think this is true and will address two kinds of objections you may have. So, reasons to think that these as risks in fact are too unlikely to focus on them. The first objection could be that well these kind of S-risks they are just too absurd. We can't even send humans to Mars yet so why should we worry about suffering on cosmic scales you could think.</p><p>And in fact when I first encountered related ideas I had a similar immediate intuitive reaction that this is a bit speculative, maybe this is not something I should focus on. But I think we should really be careful to examine such intuitive intuitions because as many of you I guess are well aware there is a large body of psychological research in the heuristics and biases approach that suggests that intuitive assessments of probability by humans are often driven by how easy it is for us to recall a prototypical example of the kind of scenarios we are considering. And for things that have never happened for which there is no historical precedent this leads to us intuitively systematically underestimating their probability known as the absurdity heuristics. So, I think we shouldn't go with this intuitive reaction but should rather really examine, okay what can we say about the probability of these S-risks.</p><p>And if we look at all of our best scientific theories and what the experts are saying about how the future could unfold I think we can identify two not too implausible technological developments that may plausibly lead to the realization of S-risks. This is not to say that these are the only possibilities there may be unknown unknowns things we can't foresee yet that could also lead to such S-risks but there are some known pathways that could get us into as risk territory. And these two are artificial sentience and super intelligent AI. So artificial sentience simply refers to the idea that the capacity to have subjective experience and in particular the capacity to suffer is in fact not limited in principle to biological animals. But that there could be novel kinds of beings perhaps computer programs stored on silicon based hardware about who's suffering we would also have reasons to care about. And while this isn't completely settled in fact few contemporary views and the philosophy of mind would say that artificial sentience is impossible in principle. So, it seems to be a conceptual possibility we should be concerned about. Now how likely is it that this will ever be realized. This may be less clear but in fact here as well we can identify one technological pathway that may lead to artificial sentience and this is the idea of whole brain emulation. Basically just understanding the human brain and sufficient detail so that we could build a functionally equivalent computer simulation of it. And for this technology it's still not completely certain that we will be able to do it but in fact researchers have looked at this and have outlined a quite detailed roadmap for this technology. So, they've identified concrete milestones and remaining uncertainties and have concluded that this definitely seems to be something we should take into account when thinking about the future. So, I'd argue there is a not too implausible technological possibility that we will get to artificial sentience.</p><p>I won't say as much about the second development, super intelligent AI because this is already getting a lot of attention in the EA community. If you aren't familiar with worries related to super intelligent AI I recommend Nick Bostrom's excellent book, Super Intelligence and I'll just add that super intelligent AI presumably could also unlock many more technological capabilities that we would need to get into S-risks territory. So, for instance the capacity to colonize space and spread out sentient beings into larger parts of the universe. This could conceivably be realized by super intelligent AI and I'd also like to add that some scenarios in which the interaction between super intelligent AI and artificial sentience could lead to S-risks scenarios have been discussed by Bostrom in super intelligent and other places under the term mind crime. So, this is something you could search for if you're interested in related ideas.</p><p>So in fact if we look at what we can say about the future I think it would be a mistake to say that S-risks are so unlikely that we shouldn't care about them. But maybe you have now a different objection. So maybe you're convinced that okay in terms of the technological capabilities we can't be sure that these S-risks are just too unlikely but you may think okay vast amounts of suffering there seems to be a pretty specific outcome even if we have much greater technological capabilities it seems unlikely that such an especially bad outcome will be realized. So you could think after all this would require some kind of evil agent some kind of evil intent that actively tries to make sure that we get these vast amounts of suffering. And I think I agree that this seems to be pretty unlikely but here again after reflecting on this a little bit I think we can see that this is only one and perhaps the most implausible route to get into S-risks territory. There also are two other routes I'd like to argue.</p><p>The first of these S-risks could arise by accident. So, one class of scenarios how this could happen could be the following. Imagine that the first artificially sentient beings we create aren't as highly developed as complete human minds but perhaps more similar to non-human animals in that we may create artificially sentient beings with the capacity to suffer but with a limited capability to communicate with us and to signal that they are suffering. In an extreme case we may create beings that are sentient that can suffer but who's suffering we overlook because there is no possibility of easy communication.</p><p>A second scenario where S-risks could be realized without evil intent are the toy example if the toy example of a paperclip maximizer which serves to illustrate the idea what would happen if we create a very powerful super intelligent AI that pursues some unrelated goal. A goal that's neither closely aligned with our values nor actively evil. And as Nick Bostrom and many people have argued conceivably such a paperclip maximizer could lead to human extinction for instance because it would convert the whole earth and all the matter around here into paperclips because it just wants to maximize the number of paperclips and has no regard for human survival. But it's only a small further step to worry well what if such a paperclip maximizer runs for instance sentient simulations say for scientific purposes to better understand how to maximize paperclip production or maybe similar to the way that our suffering serves some kind of evolutionary function maybe a paperclip maximizer would create some kind of artificially sentient sub-programs or work who suffering would be instrumentally useful for maximizing the production of paperclips. So, we only need to add very few additional examples and assumptions to see that scenarios which are already getting a lot of attention could not only lead to human extinction but effect to outcomes that would be even worse. Finally to understand the significance of the third route S-risks could be realized as part of a conflict note that it's often the case then that if we have a large number of agents competing for a shared pie of resources that this can incentivize negative sub-dynamics that lead to very bad outcomes even if none of the agents involved actually actively values those bad outcomes they are just resorting to them in order to out-compete the other agents. For instance look at most wars the countries waging them are rarely intrinsically valuing the suffering and the bloodshed implicated in them but sometimes wars still happen to further the strategic interests of the countries involved.</p><p>So I think if we critically examine the situation we are in we should conclude that in fact if we take seriously a lot of the thumb considerations that are already being widely discussed in the community such as risks on super intelligent AI there are only few additional assumptions we need to justify worries about a stress and it's not like we need to invent some completely made up technologies or need to assume extremely implausible or rare motivations such as sadism or hatred to substantiate worries about S-Risks. So this is why I've said that I think S-Risks are at least not much more unlikely than say extinction risks from super intelligent AI. Now of course the probability of S-Risks isn't the only criterion we need to address as I said we also need to ask how easy is it to reduce those S-Risks. And in fact I think this is a pretty challenging task. We haven't found any kind of silver bullet yet here but I'd also like to argue that reducing S-Risks is at least minimally tractable even today and one reason for this is that we are arguably already reducing S-Risks. So, as I just said some scenarios how S-Risks could be realized are super intelligent AI goes wrong in some way.</p><p>This is why some work in technical AI safety as well as AI policy probably already effectively reduces S-Risks. To give you one example I said that we might be worried about S-Risks arising because of the strategic behavior of say AI agents as part of a conflict. Some work in AI policy that reduces the likelihood of such multi-polar AI scenarios and makes unipolar AI scenarios with less competition more likely could in particular have the effect of reducing S-Risks. Similar with some work in technical AI safety. That being said it seems to me that a lot of the interventions that are currently undertaken reduce S-Risks by accident in a sense they aren't specifically tailored to reducing S-Risks and there may well be particular say sub-problems within technical AI safety that would be particularly effective at reducing S-Risks specifically and which aren't getting a lot of attention already. So, for instance to give you one toy example that's probably hard to realize in precisely this form but illustrates what might be possible. Consider the idea of achieving the goal of an AI being uncontrolled, conditional on our efforts on solving the control problem failing making sure that in such a scenario AI at least doesn't create say additional sentient simulations or artificially sentient sub-programs. If we could solve this problem through work in technical AI safety we would arguably reduce S-Risks specifically.</p><p>Of course there also are more broad interventions that don't directly aim to influence some kinds of levers that directly affect the far future but would have a more indirect effect on reducing S-Risks. So for instance we could think that strengthening international cooperation will enable us to at some point for instance prevent AI arms races that could again lead to negative sum dynamics that could lead us into S-Risk territory. Similarly because artificial sentience is such a significant worry when thinking about S-Risks we could think that expanding the moral circle and making it more likely that human decision makers in the future would care about artificially sentient beings that this would have a positive effect on reducing S-Risk. That all being said I think it's fair to say that we currently don't understand very well how best to reduce S-Risk. One thing we could do if we suspect that there are some low hanging fruits to reap there we could say okay let's go meta and do research about how best to reduce those S-Risk and in fact this is a large part of what we are doing at the Foundation Research Institute. Now there's also another aspect of tractability I'd like to talk about. This is not the question how easy is it intrinsically to reduce S-Risk but the question could we raise the required support. For instance can we get sufficient funding to get work on reducing S-Risk off the ground. And one worry we may have here is that well all these talk about suffering on a cosmic scale and so on this will seem too unlikely to many people in other words that S-Risks are just too weird a concern for us to be able to raise significant support and funding to reduce them.</p><p>And I think this is to some extent a legitimate worry but I also don't think that we should be too pessimistic and I think the history of the AI safety field substantiates this assessment. If you think back even 10 years ago you will find that back then worries about extinction risk from super intelligent AI were ridiculed, dismissed or misrepresented and misunderstood as for instance being about the terminator or anything something like that. Today we have Bill Gates blurbing a book talking openly and directly about these risks of super intelligent AI and also related concepts such as mind crime. And so I would argue that the recent history of the AI safety field provides some reason for hope that we are able to push even seemingly weird cause areas sufficiently far into the mainstream, into the window of acceptable discourse that we can raise significant support for them.</p><p>Last but not least what about the neglectedness of S-risk? So, as I said some work that's already underway in the X-Risk area arguably reduces S-risk so reducing S-risk is not totally neglected but I think it's fair to say that they get much less attention than say extinction risk. In fact I've sometimes seen people in the community either explicitly or implicitly equate existential risk and extinction risk which conceptually clearly seems to be untrue. And in fact while some existing interventions may be also effective at reducing S-risks there are few people that are specifically trying to identify interventions that are most effective at reducing S-risk specifically. And I think the foundational research institute is the only EA organization which at an organizational level has the mission of focusing on reducing S-risk. So to summarize I haven't conclusively answered the question for who exactly reducing S-risk is the best thing to do. I think this depends both on your ethical view and on some empirical questions such as the probability, the tractability and the neglectedness of S-risk. But I have argued S-risks are not much more unlikely than say extinction risk from super intelligent AI and so they warrant at least some attention. And I've argued that the most plausible known path that could lead us into S-risk territory so aside from unknown unknowns are AI scenarios that involve the creation of large numbers of artificially sentient beings. And this is why I think among the currently known sources of existential risk the AI risk cost area is unique in also being very relevant for reducing S-risk. Because if we don't get AI right there seems to be a significant chance that we get into S-risk territory whereas in other areas say an asteroid hitting earth or a deadly pandemic or wiping out a human life it seems much less likely that this could get us into scenarios that would be much, much worse than extinction because they in addition contain a lot of suffering. In this sense if you haven't considered S-risk before I think this is an update for caring more about the AI risk cost area as opposed to other S-risk cost areas. In this sense some but not all of the current work in the ex-risk area is already effective at reducing S-risk but there seems to be a gap of people and research specifically optimizing for reducing S-risk and trying to find those interventions that are most effective for this particular goal. And I'd argue in some that the Foundational Research Institute in having this unique focus occupies an important niche and I would very much like to see more people join us in that niche. So, people from say other organizations also doing some kind of research that's hopefully effective at reducing S-risk. So this all being said I hope to have raised some awareness for the worrying prospect of S-risks I don't think I have convinced all of you that reducing S-risk is the best use of your resources.</p><p>I don't think I could expect this both because our rock bottom ethical views differ to some extent and also because the empirical questions involved are just extremely complex and it seems very hard to reach agreement on them. So, I think realistically those of us who are interested in shaping the for future who are convinced that this is the most important thing to do among those of us we will be faced with a situation where there are people with different priorities in the community and we need to sort out how to manage this situation. And this is why I'd like to end this talk with a vision for this for future shaping community. So, shaping the for future as a metaphor could be seen as being involved in like a long journey. But what I hope I have made clear is that it's a misrepresentation to frame this journey as involving a binary choice between extinction or utopia. In another sense however I'd argue that this metaphor was apt. We do face a long journey but it's a journey through hard to traverse territory and on the horizon there is a continuum ranging from a very bad thunderstorm to the most beautiful summer day. And interest in shaping the for future in a sense determines who is with us in the vehicle but it doesn't necessarily comprehensively answer the question what more precisely to do with the steering wheel.</p><p>Some of us are more worried about not getting into the thunderstorm, others are more motivated by the existential hope of maybe arriving at that beautiful sunshine. And it seems hard to reach agreement on what more precisely to do and part of the reasons is that it's very hard to keep track of the complicated networks of roads far ahead and to see steering in what directions will lead to precisely what outcome. Thisis very hard by contrast we have an easy time seeing who else is with us in the vehicle. And so the concluding thought I'd like to offer is maybe among the most important things we can do is to make sure to compare our maps among the people in the vehicle and to find some way of handling the remaining disagreements without inadvertently derailing the vehicle and getting to an outcome that's worth for all. Thank you.</p><p>I'll start off with a question that a few people were asking which is something that you said isn't necessary to be concerned with S-risks but would help shed a little bit of clarity which is besides AI, besides a whole brain emulation and running like uploaded brains are there any other forms of an S-risk that you can try to visualize? Particularly people were trying to figure out ways in which you can work on the problem if you don't have a concrete sense of the way in which it might manifest. So, I do in fact think that artificial the most plausible scenarios we can foresee today involve artificial sentience partly because many people have talked about that it artificial sentience would come with novel challenges for instance it would presumably be very easy to spawn a large number of artificially sentient beings. There are a lot of efficiency advantages of silicon based substrates as compared to biological substrates and also we can observe that in many other areas the worst outcomes contain most of the expected value like the predominance of fat tailed institutions for instance with respect to the casualties in war and diseases and so on. So, it seems somewhat plausible to me that if we are concerned about reducing as much suffering as possible in expectation we should focus on these very bad outcomes and that most of these for various reasons involve artificial sentience. That being said I think there are some scenarios especially in future scenarios where we don't have this archetypical intelligence explosion scenario and the heart takeoff where we are faced with a more messy and complex future where there are maybe a lot of factions controlling AI and using that for various purposes that we could face risks that maybe aren't as don't have as a higher scope as the worst scenarios involving artificial sentience but would be maybe more akin to factory farming. Some kind of novel technology that would be misused in some way maybe just because people don't sufficiently care about the consequences and they pursue some kind of say economic goals and yeah create inadvertently create large amounts of suffering similar to the way we can see this happening today in for example the animal industry. You said that the debate is still kind of out whether or not you could actually have extend your moral concern to something that's in a silicon substrate that isn't flesh and bone in the way that we are.</p><p>Can you make the case for why we might in fact care about something that is an uploaded brain and isn't a brain in the way that we generally think of it? So one suggestive thought experiment that has been discussed in the philosophy of mind is imagine that you replace your brain not all at once but step by step with a silicon based hardware. So, you start with replacing just one neuron with some kind of chip that serves the same function. It seems intuitively clear that this doesn't make you less sentient or that we should care less about you in that way. And now you can imagine step by step replacing your brain one neuron at a time with a computer in some sense. And it seems that you have a hard time pinpointing any particular point in this transition where you say oh well now the situation flips and we should stop caring about the same after all the same information processing that's still going on in that brain. And yeah but there is a large body of literature in the philosophy of mind discussing this question. And assuming that these brains in do in fact have the capacity to suffer what reason would we have to think that it would be advantageous say for a superintelligence to emulate lots of brains in a way in which they suffer rather than just have them exist without any sort of positive or negative feeling.</p><p>So, one reason we may be worried is that if we look at the current successes in the AI field we see that they are often driven by machine learning techniques. That is techniques where we don't program the knowledge and the capabilities. We think the AI system should have directly into the system but rather set up some kind of algorithm that can be trained and that can learn via trial and error receiving some information about how good or bad it's doing and thereby increasing its capabilities. Now it seems very unlikely that the current machine learning techniques that involve giving some kind of reward signals to algorithms should be concerning to us to a large extent. I don't want to claim that current say reinforcement learning algorithms are suffering to a large extent but we may be worried that similar architectures where the capabilities of artificially sentient beings arise by them being trained in some things receiving some kind of reward signal that this is a feature of AI systems that will persist even at a point when the sentience of these algorithms is realized to a larger extent. So, in some way this is similar to the way as I mentioned our suffering serves some kind of evolutionary function that helps us navigating in the world and in fact people who don't feel pain have a great deal of difficulties for this reason because they don't intuitively avoid damaging outcomes. And yeah so this is certainly a longer discussion but I hope you can give a brief answer to this one.</p><p>A couple of people also wanted to know you have a particular suffering focus in the case of S-Risk but some people wonder that perhaps an agent might actually just prefer, it's not clear whether they would prefer death or suffering, they might actually prefer to exist even if their experiences are pretty negative. Is this a choice that you would be making on behalf of the agents that you're considering in your moral realm when you're trying to mitigate an S-Risk, is this a necessary precondition for caring about S-Risks? So, I think whatever your rock bottom ethical views there are nearly always are prudential reasons for considering the preferences of other agents. So, if I was faced with a situation where I think oh there is like some kind of being whose experiences are so negative that I think in a consequentialist sense it would be better that this being doesn't exist but this being has for whatever reason a strong preference to exist and then argues with me oh well should I continue or not and so on and so forth. I think there often can be prudential reasons to take these preferences into account. I think actually there will be some kind of convergence between different ethical views on the question of how to take such hypothetical preferences into account.</p><p>That being said I think it's fairly implausible to claim that no imaginable amount of suffering would be intrinsically worse than non-existence. This seems fairly implausible to me so one intuition pump for this could be if you face the choice between one hour of sleep or one hour of torture what do you prefer. It seems fairly clear I would guess to most of us that one hour of sleep having no experience at all is if the better choice in the sense. And you said that hopefully we'll come to some sort of convergence on what the true moral philosophy in so far as there is one is but there might also be reason to think that we wouldn't do this in the time scales of the development of a super intelligent AI or the development of whole brain emulations that we can run on many computers. What do we do in that case where we haven't solved moral philosophy in time? So that I think is a very important question because to me it seems to be fairly likely that there won't be such convergence at least to a large extent of detail.</p>", "user": {"username": "Max_Daniel"}}, {"_id": "oL9mDDNpdExx6zPSJ", "title": "George Church: CRISPR and other technologies for global effective altruism", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=JBp0JoF1utE&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=13\"><div><iframe src=\"https://www.youtube.com/embed/JBp0JoF1utE\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: Boston 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "6uiXHHJQEtMaYQrti", "title": "Max Tegmark: Effective altruism, existential risk, and existential hope", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=2f1lmNqbgrk&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=12\"><div><iframe src=\"https://www.youtube.com/embed/2f1lmNqbgrk\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: Boston 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "3538iKtS2YmN67som", "title": "The AI revolution and international politics (Allan Dafoe)", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=Zef-mIKjHAk&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=11\"><div><iframe src=\"https://www.youtube.com/embed/Zef-mIKjHAk\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p>Artificial intelligence (AI) is rapidly improving. Superhuman AI in strategically relevant domains is likely to arrive in the next several decades; some experts think two. This will transform international politics, could be profoundly destabilizing, and could pose existential risks. Urgent research is needed on AI grand strategy.&nbsp;</p><p>This requires a careful examination of humanity\u2019s highest interests in the era of <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Sec1\">transformative AI</a>, of the international dynamics likely to arise from AI, and of the most promising strategies for securing a good future. Much work will be required to design and enact effective global AI policy.</p><p>Below is a transcript of Allan Dafoe's talk on this topic from EA Global: Boston 2017. We've lightly edited the transcript for clarity.</p><h1>The Talk</h1><p><strong>Nathan Labenz:</strong> It is my honor to introduce Professor Allan Dafoe. He is an Assistant Professor of Political Science at Yale University and a Research Associate at the Future of Humanity Institute at Oxford, where he is involved in building the AI Politics and Policy Group. His research seeks to understand the causes of world peace and stability. Specifically, he has examined the causes of the liberal peace and the role of reputation and honor as motives for war. Along the way, he has developed methodological tools and approaches to enable more transparent, credible, causal inference. More recently, he has focused on artificial intelligence grand strategy, which he believes poses existential challenges and also opportunities, and which requires us to clearly perceive the emerging strategic landscape in order to help humanity navigate safely through it. Please welcome Professor Allan Dafoe to discuss his work on AI and international politics.</p><p><strong>Allan Dafoe: </strong>Thank you, Nathan.</p><p>I will start by talking about war, and then we'll get to AI, because I think there are some lessons for effective altruism.&nbsp;</p><p>War is generally understood to be a bad thing. It kills people. It maims them. It destroys communities and ecosystems, and is often harmful to economies. We\u2019ll add an asterisk to that because in the long run, war has had dynamic benefits, but in the current world, war is likely a negative that we would want to avoid.</p><p>So if we were going to start a research group to study the causes of war for the purposes of reducing it, we might ask ourselves, \u201cWhat kinds of war should we study?\u201d There are many kinds of wars that have different causes. Some are worse than others.&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3704d0af382f2c6f6a81f5e343d8b235de7194c13eadd941.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3704d0af382f2c6f6a81f5e343d8b235de7194c13eadd941.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3704d0af382f2c6f6a81f5e343d8b235de7194c13eadd941.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3704d0af382f2c6f6a81f5e343d8b235de7194c13eadd941.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3704d0af382f2c6f6a81f5e343d8b235de7194c13eadd941.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3704d0af382f2c6f6a81f5e343d8b235de7194c13eadd941.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3704d0af382f2c6f6a81f5e343d8b235de7194c13eadd941.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3704d0af382f2c6f6a81f5e343d8b235de7194c13eadd941.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3704d0af382f2c6f6a81f5e343d8b235de7194c13eadd941.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3704d0af382f2c6f6a81f5e343d8b235de7194c13eadd941.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3704d0af382f2c6f6a81f5e343d8b235de7194c13eadd941.png/w_1083 1083w\"></figure><p>One classification that we might employ is how many people different kinds of wars kill. There are some wars that kill only 1,000 to 10,000, and some that kill 100,000 to a million. And so the x-axis [on this slide] shows the battle deaths in wars, and the y-axis is the fraction of wars of those kinds.</p><p>An instinct we might have is to say, \u201cLet's study the wars that are most common \u2014 those in the first bin.\u201d These are the wars that are happening today in Syria: civil wars. Availability bias would suggest that those are the wars we should worry about. Some of my colleagues have argued that great-power war \u2014 the big world wars \u2014 are a thing of the past. They say that the liberal peace, democracy, capitalism, and nuclear weapons have all rendered great-power war obsolete, and that we're not going to have them again. The probability is too low.</p><p>But as effective altruists, we know that you can't just round a small number down to zero. You don't want to do that. You want to try to think carefully about the expected value of different kinds of actions. And so it's important that even though the probability of a war killing a million, 10 million, 100 million, or a billion is very small, it's not zero.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/88d31db53c751e30c56002f433b71e481555d3f991354ca0.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/88d31db53c751e30c56002f433b71e481555d3f991354ca0.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/88d31db53c751e30c56002f433b71e481555d3f991354ca0.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/88d31db53c751e30c56002f433b71e481555d3f991354ca0.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/88d31db53c751e30c56002f433b71e481555d3f991354ca0.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/88d31db53c751e30c56002f433b71e481555d3f991354ca0.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/88d31db53c751e30c56002f433b71e481555d3f991354ca0.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/88d31db53c751e30c56002f433b71e481555d3f991354ca0.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/88d31db53c751e30c56002f433b71e481555d3f991354ca0.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/88d31db53c751e30c56002f433b71e481555d3f991354ca0.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/88d31db53c751e30c56002f433b71e481555d3f991354ca0.png/w_1088 1088w\"></figure><p>If we look at the past 70 years of history, World War II stands out as the source of most of the battle deaths that have been experienced. That would suggest that fatalities are a good enough proxy for whatever metric of importance you have, and that we first want to make sure we understand World War II and the kinds of wars that are like it (i.e. that are likely to kill many people).&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/46adadd40e60253045d8682e04506a9575f94933db1214e5.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/46adadd40e60253045d8682e04506a9575f94933db1214e5.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/46adadd40e60253045d8682e04506a9575f94933db1214e5.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/46adadd40e60253045d8682e04506a9575f94933db1214e5.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/46adadd40e60253045d8682e04506a9575f94933db1214e5.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/46adadd40e60253045d8682e04506a9575f94933db1214e5.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/46adadd40e60253045d8682e04506a9575f94933db1214e5.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/46adadd40e60253045d8682e04506a9575f94933db1214e5.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/46adadd40e60253045d8682e04506a9575f94933db1214e5.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/46adadd40e60253045d8682e04506a9575f94933db1214e5.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/46adadd40e60253045d8682e04506a9575f94933db1214e5.png/w_1057 1057w\"></figure><p>We can zoom out more, and see that World War I comes out of the froth of civil wars. And really, those two wars loom above everything else as what we want to explain, at least if we're prioritizing importance.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cbb5a0b533a6a66bae3fa4b71cf03a0487096bd8552ac85.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cbb5a0b533a6a66bae3fa4b71cf03a0487096bd8552ac85.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cbb5a0b533a6a66bae3fa4b71cf03a0487096bd8552ac85.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cbb5a0b533a6a66bae3fa4b71cf03a0487096bd8552ac85.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cbb5a0b533a6a66bae3fa4b71cf03a0487096bd8552ac85.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cbb5a0b533a6a66bae3fa4b71cf03a0487096bd8552ac85.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cbb5a0b533a6a66bae3fa4b71cf03a0487096bd8552ac85.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cbb5a0b533a6a66bae3fa4b71cf03a0487096bd8552ac85.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cbb5a0b533a6a66bae3fa4b71cf03a0487096bd8552ac85.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cbb5a0b533a6a66bae3fa4b71cf03a0487096bd8552ac85.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cbb5a0b533a6a66bae3fa4b71cf03a0487096bd8552ac85.png/w_1025 1025w\"></figure><p>We can see that in this graph, which again has these bins depicting the size of violent quarrels. Relative to homicides, we see that the world wars in the \u201c10 million\u201d bin on the right contain, again, most of the deaths in violent quarrels. That also suggests that it's really important that we understand these big wars.&nbsp;</p><p>But of course, the next war need not limit itself to 99 million deaths. There could be a war that kills hundreds of millions or even 6.5 billion. The problem, empirically speaking, is that we don't have those wars and that data set, so we don't know how to estimate, non-parametrically, the expected value in those. We can try to extrapolate from what's very close to a power-law distribution. And no matter how we do it, unless we're extremely conservative, we get a distribution like this, which shows that most of the harm from war comes from the wars that kill a billion people or more:</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3ad408f0ce60ad9764934ed99fe9672932aaedf48ec33101.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3ad408f0ce60ad9764934ed99fe9672932aaedf48ec33101.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3ad408f0ce60ad9764934ed99fe9672932aaedf48ec33101.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3ad408f0ce60ad9764934ed99fe9672932aaedf48ec33101.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3ad408f0ce60ad9764934ed99fe9672932aaedf48ec33101.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3ad408f0ce60ad9764934ed99fe9672932aaedf48ec33101.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3ad408f0ce60ad9764934ed99fe9672932aaedf48ec33101.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3ad408f0ce60ad9764934ed99fe9672932aaedf48ec33101.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3ad408f0ce60ad9764934ed99fe9672932aaedf48ec33101.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3ad408f0ce60ad9764934ed99fe9672932aaedf48ec33101.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3ad408f0ce60ad9764934ed99fe9672932aaedf48ec33101.png/w_1074 1074w\"></figure><p>Of course, we haven't had those wars. Nevertheless, this follows from other reasoning.</p><p>We can go still further. The loss from a war that killed 6.5 billion is not about just those 6.5 billion people who die. It's also about future people. And this is where the idea of existential risk and existential value comes in. We have to ask ourselves, \u201cWhat is the value of the future? How much worth do we give to future human lives?\u201d&nbsp;</p><p>There are many ways to answer that question. And you want to discount for model uncertainty and various things. But one thing that drives concern with existential risk, which I'm concerned with and the Future of Humanity Institute is concerned with, is that there's so much potential in the future. There are so many potential lives that could be lived. Anything that cuts those lives off has tremendous disvalue.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5df0afb5e3d84f15b7548845bf803a545534af84b4272b07.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5df0afb5e3d84f15b7548845bf803a545534af84b4272b07.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5df0afb5e3d84f15b7548845bf803a545534af84b4272b07.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5df0afb5e3d84f15b7548845bf803a545534af84b4272b07.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5df0afb5e3d84f15b7548845bf803a545534af84b4272b07.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5df0afb5e3d84f15b7548845bf803a545534af84b4272b07.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5df0afb5e3d84f15b7548845bf803a545534af84b4272b07.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5df0afb5e3d84f15b7548845bf803a545534af84b4272b07.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5df0afb5e3d84f15b7548845bf803a545534af84b4272b07.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5df0afb5e3d84f15b7548845bf803a545534af84b4272b07.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5df0afb5e3d84f15b7548845bf803a545534af84b4272b07.png/w_1083 1083w\"></figure><p>One estimate that\u2019s quite conservative is this: If there are only a billion people living on Earth, and they continue living in a sustainable way for a billion years (which is doable), as long as we don't muck it up, that yields 10,000 trillion lives. That is a lot. And so anything that reduces the probability of extinction and of losing those 10,000 trillion lives, by even a little bit, has a lot of expected value.&nbsp;</p><p>Now, that's a very small number, one ten trillionth (0.0000000000001), and it's hard to know if you're making less of an effect than that. It looks close to zero. The numbers aren't meant to be taken too seriously. They're more thinking heuristics. They illustrate that if you give value to the future, you really want to worry about anything that poses a risk of extinction. And one thing that I and others, such as the Open Philanthropy Project, have identified as a risk to the future is artificial intelligence.&nbsp;</p><p>Before telling you about the risk, I'm going to first tell you about what's up with AI these days.&nbsp;</p><p>For a long time, artificial intelligence consisted of what we would now call good old-fashioned AI. There was a programmer who wrote if-then statements. They tried to encode some idea of what was a good behavior that was meant to be automated. For example, with chess algorithms, you would have a chessmaster say, \u201cHere are the heuristics I use. Here's the value function.\u201d You put those heuristics in a machine, and the machine runs it more reliably and more quickly than a human. And that's an effective algorithm. But it turns out that good old-fashioned AI just couldn't hack a number of problems \u2014 even simple ones that we do in an instant, like recognizing faces, images, and other things.</p><p>More recently, what has sort of taken over is what's called \u201cmachine learning.\u201d This means what it sounds like it means: machines learning, for themselves, solutions to problems. Another term for this is \u201cdeep learning,\u201d which is especially flexible machine learning. You can think of it as a flexible optimization procedure. It's an algorithm that's trying to find a solution and has a lot of parameters. \u201cNeural networks\u201d is another term you've probably heard.&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bc2e02dcbd7ccd91aecd035ada6d068e67b0489d534d628.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bc2e02dcbd7ccd91aecd035ada6d068e67b0489d534d628.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bc2e02dcbd7ccd91aecd035ada6d068e67b0489d534d628.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bc2e02dcbd7ccd91aecd035ada6d068e67b0489d534d628.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bc2e02dcbd7ccd91aecd035ada6d068e67b0489d534d628.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bc2e02dcbd7ccd91aecd035ada6d068e67b0489d534d628.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bc2e02dcbd7ccd91aecd035ada6d068e67b0489d534d628.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bc2e02dcbd7ccd91aecd035ada6d068e67b0489d534d628.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bc2e02dcbd7ccd91aecd035ada6d068e67b0489d534d628.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bc2e02dcbd7ccd91aecd035ada6d068e67b0489d534d628.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bc2e02dcbd7ccd91aecd035ada6d068e67b0489d534d628.png/w_970 970w\"></figure><p>This slide shows the breakthrough recently in image classification arising from neural networks and the year-on-year improvements to the point where machines are now better than humans at image classification.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c7874af0d4a75ba434c4f1e4d8e89d9d26c205afa815b9e1.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c7874af0d4a75ba434c4f1e4d8e89d9d26c205afa815b9e1.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c7874af0d4a75ba434c4f1e4d8e89d9d26c205afa815b9e1.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c7874af0d4a75ba434c4f1e4d8e89d9d26c205afa815b9e1.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c7874af0d4a75ba434c4f1e4d8e89d9d26c205afa815b9e1.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c7874af0d4a75ba434c4f1e4d8e89d9d26c205afa815b9e1.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c7874af0d4a75ba434c4f1e4d8e89d9d26c205afa815b9e1.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c7874af0d4a75ba434c4f1e4d8e89d9d26c205afa815b9e1.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c7874af0d4a75ba434c4f1e4d8e89d9d26c205afa815b9e1.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c7874af0d4a75ba434c4f1e4d8e89d9d26c205afa815b9e1.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c7874af0d4a75ba434c4f1e4d8e89d9d26c205afa815b9e1.png/w_1082 1082w\"></figure><p>Another domain is generalized game-playing or arcade game-playing. Here are our target games. Probably not many of us have played these. DeepMind is a leading AI group within Google that learned to play Atari games at a superhuman level with no instructions about the nature of the universe (e.g., \u201cWhat is time and space?\u201d, \u201cWhat is a bad guy?\u201d, \u201cHere's Pac-Man \u2014 what is a pellet and what is a ghost?\u201d). All the machine is told or given is pixel input. It just sees the screen from a blank-state type of beginning. And then it plays the game again and again and again, getting its score, and it tries to optimize for the score. Gradually, over the span of about a day, it learns to make sense of this pixel input. It derives concepts of the bad guy, the ghosts, and the pellets. It devises strategies and becomes superhuman at a range of games.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/476507ddedcab38aee856c03dc8c9426c2ec63bd7e506a01.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/476507ddedcab38aee856c03dc8c9426c2ec63bd7e506a01.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/476507ddedcab38aee856c03dc8c9426c2ec63bd7e506a01.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/476507ddedcab38aee856c03dc8c9426c2ec63bd7e506a01.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/476507ddedcab38aee856c03dc8c9426c2ec63bd7e506a01.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/476507ddedcab38aee856c03dc8c9426c2ec63bd7e506a01.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/476507ddedcab38aee856c03dc8c9426c2ec63bd7e506a01.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/476507ddedcab38aee856c03dc8c9426c2ec63bd7e506a01.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/476507ddedcab38aee856c03dc8c9426c2ec63bd7e506a01.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/476507ddedcab38aee856c03dc8c9426c2ec63bd7e506a01.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/476507ddedcab38aee856c03dc8c9426c2ec63bd7e506a01.png/w_1083 1083w\"></figure><p>Another domain where we see this is Go. The solution to Go is very similar. You take a blank-slate neural network that's sufficiently flexible and expose it to a lot of human games, and it learns to predict the human moves. Then you have the machine play itself again and again, on the order of about 10 million games. And it becomes superhuman.&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/964474271c838297f75c82280f0e5e5f5b30189aca07ddd2.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/964474271c838297f75c82280f0e5e5f5b30189aca07ddd2.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/964474271c838297f75c82280f0e5e5f5b30189aca07ddd2.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/964474271c838297f75c82280f0e5e5f5b30189aca07ddd2.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/964474271c838297f75c82280f0e5e5f5b30189aca07ddd2.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/964474271c838297f75c82280f0e5e5f5b30189aca07ddd2.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/964474271c838297f75c82280f0e5e5f5b30189aca07ddd2.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/964474271c838297f75c82280f0e5e5f5b30189aca07ddd2.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/964474271c838297f75c82280f0e5e5f5b30189aca07ddd2.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/964474271c838297f75c82280f0e5e5f5b30189aca07ddd2.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/964474271c838297f75c82280f0e5e5f5b30189aca07ddd2.png/w_1083 1083w\"></figure><p>Here's Lee Sedol and AlphaGo, which is another product of DeepMind. And Lee Sedol was the first really excellent Go player who publicly played AlphaGo. And here he's saying in February 2016, \u201cI think I won the game by a near-landslide this time.\u201d Well, as I've alluded to, that didn't work out.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d4c8c30a4d907469baff3f5ec0c68dade29901fef0bec641.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d4c8c30a4d907469baff3f5ec0c68dade29901fef0bec641.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d4c8c30a4d907469baff3f5ec0c68dade29901fef0bec641.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d4c8c30a4d907469baff3f5ec0c68dade29901fef0bec641.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d4c8c30a4d907469baff3f5ec0c68dade29901fef0bec641.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d4c8c30a4d907469baff3f5ec0c68dade29901fef0bec641.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d4c8c30a4d907469baff3f5ec0c68dade29901fef0bec641.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d4c8c30a4d907469baff3f5ec0c68dade29901fef0bec641.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d4c8c30a4d907469baff3f5ec0c68dade29901fef0bec641.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d4c8c30a4d907469baff3f5ec0c68dade29901fef0bec641.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d4c8c30a4d907469baff3f5ec0c68dade29901fef0bec641.png/w_1050 1050w\"></figure><p>Here he is after the first game: \u201cI was very surprised because I didn't think I would lose.\u201d&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/06f9a5d06e11384568202d4866d87f907c207c8217ed6f93.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/06f9a5d06e11384568202d4866d87f907c207c8217ed6f93.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/06f9a5d06e11384568202d4866d87f907c207c8217ed6f93.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/06f9a5d06e11384568202d4866d87f907c207c8217ed6f93.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/06f9a5d06e11384568202d4866d87f907c207c8217ed6f93.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/06f9a5d06e11384568202d4866d87f907c207c8217ed6f93.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/06f9a5d06e11384568202d4866d87f907c207c8217ed6f93.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/06f9a5d06e11384568202d4866d87f907c207c8217ed6f93.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/06f9a5d06e11384568202d4866d87f907c207c8217ed6f93.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/06f9a5d06e11384568202d4866d87f907c207c8217ed6f93.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/06f9a5d06e11384568202d4866d87f907c207c8217ed6f93.png/w_1079 1079w\"></figure><p>And unfortunately, he lost again: \u201cI'm quite speechless. I'm in shock. I can admit that. The third game is not going to be easy for me.\u201d He lost the third game. He did win the fourth game and he talks about how it was sort of the greatest moment of his life. And it's probably the last and only game played against a level of AlphaGo of that level or better that a human will ever win.</p><p>I'm bringing up Lee Sedol and his losses for a reason. I think it serves as an allegory for humanity: We don't want to be caught off-guard when it's our AlphaGo moment. At some point, machine intelligence will be better than we are at strategically relevant tasks. And it would be prudent for us to see that coming \u2014 to have at least a few years\u2019 notice, if not more, to think through how we can adapt our international systems, our politics, our notion of the meaning of life, and other areas.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5c1463d2ae0a9796f9c804527a085b944314b6c0d463f1f2.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5c1463d2ae0a9796f9c804527a085b944314b6c0d463f1f2.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5c1463d2ae0a9796f9c804527a085b944314b6c0d463f1f2.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5c1463d2ae0a9796f9c804527a085b944314b6c0d463f1f2.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5c1463d2ae0a9796f9c804527a085b944314b6c0d463f1f2.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5c1463d2ae0a9796f9c804527a085b944314b6c0d463f1f2.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5c1463d2ae0a9796f9c804527a085b944314b6c0d463f1f2.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5c1463d2ae0a9796f9c804527a085b944314b6c0d463f1f2.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5c1463d2ae0a9796f9c804527a085b944314b6c0d463f1f2.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5c1463d2ae0a9796f9c804527a085b944314b6c0d463f1f2.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5c1463d2ae0a9796f9c804527a085b944314b6c0d463f1f2.png/w_1083 1083w\"></figure><p>What's driving this progress? Algorithms, talent, and data, but another big thing driving it is hardware. Computing inputs keep getting better at an exponential rate. This is sort of a generalized Moore's law across a range of inputs. And it's this persistent progress that makes Kurzweil's graph from 2001 seem not totally absurd.&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8340d08346567587be5442e31eafa1bf3da9396555268898.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8340d08346567587be5442e31eafa1bf3da9396555268898.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8340d08346567587be5442e31eafa1bf3da9396555268898.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8340d08346567587be5442e31eafa1bf3da9396555268898.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8340d08346567587be5442e31eafa1bf3da9396555268898.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8340d08346567587be5442e31eafa1bf3da9396555268898.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8340d08346567587be5442e31eafa1bf3da9396555268898.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8340d08346567587be5442e31eafa1bf3da9396555268898.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8340d08346567587be5442e31eafa1bf3da9396555268898.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8340d08346567587be5442e31eafa1bf3da9396555268898.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8340d08346567587be5442e31eafa1bf3da9396555268898.png/w_1055 1055w\"></figure><p>Here we have, on the first y-axis, calculations per second, per thousand dollars, and I added four recent dots from the past 17 years. And you see that basically we're on track. We have exponential improvements. What we don't know is when we get to transformative AI.</p><p>Now, Kurzweil has this evocative second y-axis, where you have organisms we recognize: mice, humans, and all humans. It's not obvious what the mapping should be between calculations per second and transformative AI. Intelligence is not a single dimension, so we could get superhuman AI in some domains long before other domains. But what I think is right about this graph is that at some point between 2020 or 2025 and 2080, big things are going to happen with machine intelligence.</p><p>In our work, we want to be a bit more systematic about these timelines, and there are various ways to do it. One way is to survey AI researchers. This is the result of a recent survey.&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c3cbdda22da9887bdf8d12ef450f76ebdc91408359111991.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c3cbdda22da9887bdf8d12ef450f76ebdc91408359111991.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c3cbdda22da9887bdf8d12ef450f76ebdc91408359111991.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c3cbdda22da9887bdf8d12ef450f76ebdc91408359111991.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c3cbdda22da9887bdf8d12ef450f76ebdc91408359111991.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c3cbdda22da9887bdf8d12ef450f76ebdc91408359111991.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c3cbdda22da9887bdf8d12ef450f76ebdc91408359111991.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c3cbdda22da9887bdf8d12ef450f76ebdc91408359111991.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c3cbdda22da9887bdf8d12ef450f76ebdc91408359111991.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c3cbdda22da9887bdf8d12ef450f76ebdc91408359111991.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c3cbdda22da9887bdf8d12ef450f76ebdc91408359111991.png/w_1034 1034w\"></figure><p>Some takeaways are:</p><ol><li>There's huge disagreement about when human-level machine intelligence, defined here as machines better than all humans at every task, will come, as you can see by the gray S-curves.</li><li>The group that we surveyed gives enough probability mass that by 100 years, there still won't be human-level machine intelligence.</li><li>But in the next 10 or 20 years, this group gives a 10-20% chance that we will have reached it already. And if that probability seems right (and upon consideration, I think it does, and not just from using this as evidence), there\u2019s more than sufficient warrant for us to invest a lot of resources into thinking very hard about what human-level AI would mean for humanity.</li></ol><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b8af218dbc98985f543afd818a90274a072ee5cf7b07c0e0.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b8af218dbc98985f543afd818a90274a072ee5cf7b07c0e0.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b8af218dbc98985f543afd818a90274a072ee5cf7b07c0e0.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b8af218dbc98985f543afd818a90274a072ee5cf7b07c0e0.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b8af218dbc98985f543afd818a90274a072ee5cf7b07c0e0.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b8af218dbc98985f543afd818a90274a072ee5cf7b07c0e0.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b8af218dbc98985f543afd818a90274a072ee5cf7b07c0e0.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b8af218dbc98985f543afd818a90274a072ee5cf7b07c0e0.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b8af218dbc98985f543afd818a90274a072ee5cf7b07c0e0.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b8af218dbc98985f543afd818a90274a072ee5cf7b07c0e0.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b8af218dbc98985f543afd818a90274a072ee5cf7b07c0e0.png/w_974 974w\"></figure><p>Here are some tasks. I like to think of them as milestones and canaries. What are some things that machines will eventually achieve that are either noteworthy or strategically relevant (i.e. milestones)? And the canaries are those things that when they happen, signal to us that we\u2019d better be paying attention because things are going to change quickly. I expect most of these tasks on the right-hand column will soon be moving over to the left-hand column, if they're not there already. So this is something else we're working on.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/95de8fd3d4eed18e5e5d86d36460f888ff5bf369bb13628e.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/95de8fd3d4eed18e5e5d86d36460f888ff5bf369bb13628e.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/95de8fd3d4eed18e5e5d86d36460f888ff5bf369bb13628e.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/95de8fd3d4eed18e5e5d86d36460f888ff5bf369bb13628e.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/95de8fd3d4eed18e5e5d86d36460f888ff5bf369bb13628e.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/95de8fd3d4eed18e5e5d86d36460f888ff5bf369bb13628e.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/95de8fd3d4eed18e5e5d86d36460f888ff5bf369bb13628e.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/95de8fd3d4eed18e5e5d86d36460f888ff5bf369bb13628e.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/95de8fd3d4eed18e5e5d86d36460f888ff5bf369bb13628e.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/95de8fd3d4eed18e5e5d86d36460f888ff5bf369bb13628e.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/95de8fd3d4eed18e5e5d86d36460f888ff5bf369bb13628e.png/w_1039 1039w\"></figure><p>More generally, there's a whole host of challenges \u2014 near, medium, and long term \u2014 that we will be working on as a society, but also at the Future of Humanity Institute. There's a range of near-term issues that I'm not going to talk about. Each of those could occupy a workshop or a conference. I will say that when thinking about long-term issues, we also confront the near-term issues for one reason: because the long-term issues often look like the near-term issues, magnified a hundred-fold, and because a lot of our long-term insights, strategies, and policy interventions\u2019 most appropriate place of action is in the near term.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bb96db8553ccebce756846c9bc7d53a3dea39b7b027d822.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bb96db8553ccebce756846c9bc7d53a3dea39b7b027d822.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bb96db8553ccebce756846c9bc7d53a3dea39b7b027d822.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bb96db8553ccebce756846c9bc7d53a3dea39b7b027d822.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bb96db8553ccebce756846c9bc7d53a3dea39b7b027d822.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bb96db8553ccebce756846c9bc7d53a3dea39b7b027d822.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bb96db8553ccebce756846c9bc7d53a3dea39b7b027d822.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bb96db8553ccebce756846c9bc7d53a3dea39b7b027d822.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bb96db8553ccebce756846c9bc7d53a3dea39b7b027d822.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bb96db8553ccebce756846c9bc7d53a3dea39b7b027d822.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9bb96db8553ccebce756846c9bc7d53a3dea39b7b027d822.png/w_914 914w\"></figure><p>But what are the long-term opportunities and risks? The opportunities are tremendous. We often just think it's greater wealth, and that maybe Google stock will go up. But it's a lot more: longevity, health, preventative medicine, material bounty that could be used to end poverty ( though it need not, because it will also likely come in a more unequal world), reduced environmental impact. DeepMind was able to reduce energy usage at Google data centers by 40%. AI could basically help with anything of value that is either the product of intelligence or depends on intelligence for its protection. And so if we have superhuman intelligence, then in principle, we can use that to achieve all of our goals.&nbsp;</p><p>I'll also emphasize the last point: resilience to other existential risks. We're likely to face those in the next 100 years. And if we solve AI \u2014 if we build it well \u2014 then that could reduce those other risks by a large margin.</p><p>But of course, there are also risks with bringing into the ecosystem a creature that is better than we are at the thing that matters most for our survival and flourishing. I'm not going to go through this topology.&nbsp;</p><p>I will illuminate the risk by quoting Max Tegmark and others:</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/003a94eb08240aea330c5281af1f484627f25c2097eb4424.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/003a94eb08240aea330c5281af1f484627f25c2097eb4424.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/003a94eb08240aea330c5281af1f484627f25c2097eb4424.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/003a94eb08240aea330c5281af1f484627f25c2097eb4424.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/003a94eb08240aea330c5281af1f484627f25c2097eb4424.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/003a94eb08240aea330c5281af1f484627f25c2097eb4424.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/003a94eb08240aea330c5281af1f484627f25c2097eb4424.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/003a94eb08240aea330c5281af1f484627f25c2097eb4424.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/003a94eb08240aea330c5281af1f484627f25c2097eb4424.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/003a94eb08240aea330c5281af1f484627f25c2097eb4424.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/003a94eb08240aea330c5281af1f484627f25c2097eb4424.png/w_907 907w\"></figure><p>I will also appeal to another authority from my world:</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/abaf1bc735f5bd59ac1ae90686093b9ac58ecc08c3b94b26.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/abaf1bc735f5bd59ac1ae90686093b9ac58ecc08c3b94b26.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/abaf1bc735f5bd59ac1ae90686093b9ac58ecc08c3b94b26.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/abaf1bc735f5bd59ac1ae90686093b9ac58ecc08c3b94b26.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/abaf1bc735f5bd59ac1ae90686093b9ac58ecc08c3b94b26.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/abaf1bc735f5bd59ac1ae90686093b9ac58ecc08c3b94b26.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/abaf1bc735f5bd59ac1ae90686093b9ac58ecc08c3b94b26.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/abaf1bc735f5bd59ac1ae90686093b9ac58ecc08c3b94b26.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/abaf1bc735f5bd59ac1ae90686093b9ac58ecc08c3b94b26.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/abaf1bc735f5bd59ac1ae90686093b9ac58ecc08c3b94b26.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/abaf1bc735f5bd59ac1ae90686093b9ac58ecc08c3b94b26.png/w_987 987w\"></figure><p>I'm a scholar of international relations. Henry Kissinger, as you know, is also very worried about AI. And this is sort of the definition of a grand strategist.</p><p>In addition to these quotes, the AI researchers we surveyed agree. We asked them what the long-term prospects are for human-level machine intelligence. And while most of the probability mass was on \u201cextremely good\u201d or on \u201cbalanced good,\u201d the median is a 5% probability of AI being extremely bad or leading to human extinction.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cd2c30fca6d274c2de7972fa289e67a0f78a2769beaa7a8.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cd2c30fca6d274c2de7972fa289e67a0f78a2769beaa7a8.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cd2c30fca6d274c2de7972fa289e67a0f78a2769beaa7a8.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cd2c30fca6d274c2de7972fa289e67a0f78a2769beaa7a8.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cd2c30fca6d274c2de7972fa289e67a0f78a2769beaa7a8.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cd2c30fca6d274c2de7972fa289e67a0f78a2769beaa7a8.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cd2c30fca6d274c2de7972fa289e67a0f78a2769beaa7a8.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cd2c30fca6d274c2de7972fa289e67a0f78a2769beaa7a8.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cd2c30fca6d274c2de7972fa289e67a0f78a2769beaa7a8.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cd2c30fca6d274c2de7972fa289e67a0f78a2769beaa7a8.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2cd2c30fca6d274c2de7972fa289e67a0f78a2769beaa7a8.png/w_1030 1030w\"></figure><p>It's not often you get an industry that says that their activities give rise to a 5% chance of human extinction. I think it should be our goal to take whatever the real and most frequent number is, and push it as close to zero, getting as much of that probability mass up to the top.&nbsp;</p><p>There are two broad ways we can do that. One is to work on what's called AI safety. The computer scientists in the room and mathematicians can help build AI systems that are unlikely to misbehave without our intentions. And the other way is AI strategy, which I'm going to talk about.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bdceccf65b1f386dc1e3fbd36650b6d5c92100a2b5e0f5b3.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bdceccf65b1f386dc1e3fbd36650b6d5c92100a2b5e0f5b3.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bdceccf65b1f386dc1e3fbd36650b6d5c92100a2b5e0f5b3.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bdceccf65b1f386dc1e3fbd36650b6d5c92100a2b5e0f5b3.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bdceccf65b1f386dc1e3fbd36650b6d5c92100a2b5e0f5b3.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bdceccf65b1f386dc1e3fbd36650b6d5c92100a2b5e0f5b3.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bdceccf65b1f386dc1e3fbd36650b6d5c92100a2b5e0f5b3.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bdceccf65b1f386dc1e3fbd36650b6d5c92100a2b5e0f5b3.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bdceccf65b1f386dc1e3fbd36650b6d5c92100a2b5e0f5b3.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bdceccf65b1f386dc1e3fbd36650b6d5c92100a2b5e0f5b3.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bdceccf65b1f386dc1e3fbd36650b6d5c92100a2b5e0f5b3.png/w_1060 1060w\"></figure><p>So here's Stuart Russell explaining that we're not worried about machines suddenly waking up and deciding they want to build their own machine utopia and get rid of humans. It's not some kind of emergent consciousness. The worry is that they will be hyper optimizers, which is what we're building them to do \u2014 and that we will have not specified their value function correctly. Therefore, they could optimize for the wrong thing. This is broadly called \u201cthe control problem\u201d or \u201cthe value alignment problem.\u201d Here are some groups working on this or funding it.&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eba0d7dbb8307d5c2222e169316a99be1837f4eafbfb9b57.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eba0d7dbb8307d5c2222e169316a99be1837f4eafbfb9b57.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eba0d7dbb8307d5c2222e169316a99be1837f4eafbfb9b57.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eba0d7dbb8307d5c2222e169316a99be1837f4eafbfb9b57.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eba0d7dbb8307d5c2222e169316a99be1837f4eafbfb9b57.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eba0d7dbb8307d5c2222e169316a99be1837f4eafbfb9b57.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eba0d7dbb8307d5c2222e169316a99be1837f4eafbfb9b57.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eba0d7dbb8307d5c2222e169316a99be1837f4eafbfb9b57.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eba0d7dbb8307d5c2222e169316a99be1837f4eafbfb9b57.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eba0d7dbb8307d5c2222e169316a99be1837f4eafbfb9b57.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eba0d7dbb8307d5c2222e169316a99be1837f4eafbfb9b57.png/w_1083 1083w\"></figure><p>I'm going to keep moving. Here are some people who you can't really make out, but I will tell you that they are leading researchers and industrialists from the most prominent AI groups in the world. And they came together at the Asilomar Conference to really think seriously about AI safety, which I think is a tremendous achievement for the community \u2014 that we've brought everyone together like this. I'm showing this as a reflection of how exciting a time it is for you to be involved in this.</p><hr><p>One conjecture that's been posed is that AI value alignment is actually not that hard of a problem. As long as we have enough time, once we have the final system we want to deploy, we can test it, right? It's like drug tests. You have the thing you are thinking about deploying in the population. You just make sure it undergoes sufficient scrutiny. However, this conjecture goes, it is almost impossible to test AI value alignment if we don't have enough time. And if that's right, which seems plausible, then it directs attention to the world in which this system is being deployed. Is it one where the developers have the incentives and the time to do these safety tests? This is one of the big things people working on AI strategy think about \u2014 this issue of how to prevent a race in AI.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4bae8114220ae884f5fd4e047931e5cec40546b525140be3.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4bae8114220ae884f5fd4e047931e5cec40546b525140be3.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4bae8114220ae884f5fd4e047931e5cec40546b525140be3.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4bae8114220ae884f5fd4e047931e5cec40546b525140be3.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4bae8114220ae884f5fd4e047931e5cec40546b525140be3.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4bae8114220ae884f5fd4e047931e5cec40546b525140be3.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4bae8114220ae884f5fd4e047931e5cec40546b525140be3.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4bae8114220ae884f5fd4e047931e5cec40546b525140be3.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4bae8114220ae884f5fd4e047931e5cec40546b525140be3.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4bae8114220ae884f5fd4e047931e5cec40546b525140be3.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4bae8114220ae884f5fd4e047931e5cec40546b525140be3.png/w_1042 1042w\"></figure><p>Here's the CEO of DeepMind, basically reinforcing this point: \u201cWe want to avoid a harmful race to the finish, where corner-cutting starts happening and safety gets cut. This is a big issue on a global scale, and it's extra hard when you're talking about national governments.\u201d It's not just a race between companies in the US. It's a race between countries.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/527b6c489177f0ab61b9b3026f96fb446aa122f86555d657.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/527b6c489177f0ab61b9b3026f96fb446aa122f86555d657.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/527b6c489177f0ab61b9b3026f96fb446aa122f86555d657.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/527b6c489177f0ab61b9b3026f96fb446aa122f86555d657.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/527b6c489177f0ab61b9b3026f96fb446aa122f86555d657.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/527b6c489177f0ab61b9b3026f96fb446aa122f86555d657.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/527b6c489177f0ab61b9b3026f96fb446aa122f86555d657.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/527b6c489177f0ab61b9b3026f96fb446aa122f86555d657.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/527b6c489177f0ab61b9b3026f96fb446aa122f86555d657.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/527b6c489177f0ab61b9b3026f96fb446aa122f86555d657.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/527b6c489177f0ab61b9b3026f96fb446aa122f86555d657.png/w_944 944w\"></figure><p>So what do we think about in AI strategy? A lot. One topic we think a lot about is what AI races and AI arms races would look like. Another whole class of issues is what AI could look like militarily. What are some implications of AI for the military in terms of balance of power, crisis stability, and uncertainty over capabilities?&nbsp;</p><p>Another thing we think about is what it means economically if we live in a world where AI is the engine of growth and value in societies, which increasingly seems to be the case. Of the top 10 firms by market capitalization, either five or six are AI companies: Google, Amazon, Apple, Microsoft. In such a world, what do countries do like Saudi Arabia or France, which don't have their own Google or Amazon, but want to be part of that value chain? We may be entering an era of AI nationalism, where countries want to build their own national champion. China is certainly in the business of doing this.</p><p>The last high-level category I\u2019ll mention is the massive challenge of AI governance. This is an issue on a small, near-term scale. For example, how do we govern algorithms that are being used for judicial sentencing or self-driving cars? It\u2019s also an issue on a long-run scale, when we consider what kind of electoral system or voting rules we want to use for the organization that's going to be deciding how to test and deploy superintelligence. These are very hard questions.&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/69205e89fb37c8e4749f2e54ee44a600396973c900e71eb4.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/69205e89fb37c8e4749f2e54ee44a600396973c900e71eb4.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/69205e89fb37c8e4749f2e54ee44a600396973c900e71eb4.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/69205e89fb37c8e4749f2e54ee44a600396973c900e71eb4.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/69205e89fb37c8e4749f2e54ee44a600396973c900e71eb4.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/69205e89fb37c8e4749f2e54ee44a600396973c900e71eb4.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/69205e89fb37c8e4749f2e54ee44a600396973c900e71eb4.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/69205e89fb37c8e4749f2e54ee44a600396973c900e71eb4.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/69205e89fb37c8e4749f2e54ee44a600396973c900e71eb4.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/69205e89fb37c8e4749f2e54ee44a600396973c900e71eb4.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/69205e89fb37c8e4749f2e54ee44a600396973c900e71eb4.png/w_1038 1038w\"></figure><p>I want to make clear that these are questions being asked today by the leading AI groups. Here are Sam Altman and Demis Hassabis asking questions like \u201cWhat is the voting rule that we should use for control of our superintelligence once we build it?\u201d&nbsp;</p><p>The site for governance today is the <a href=\"https://www.partnershiponai.org/\"><u>Partnership on AI</u></a>. This is a private-sector organization that has recently brought in NGOs, including the Future of Humanity Institute. And it's plausible that this will do a good job guiding AI in the near term, and could grow in the longer term. At some point, governments are likely to get more involved. So that's a site for study and for intervention.</p><p>Another thing we can do is try to articulate principles of good governance of AI. Here are some principles that came out of the Asilomar Conference.&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/de51d0a3c02f9140c300db799aceae13f6a419c83787f971.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/de51d0a3c02f9140c300db799aceae13f6a419c83787f971.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/de51d0a3c02f9140c300db799aceae13f6a419c83787f971.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/de51d0a3c02f9140c300db799aceae13f6a419c83787f971.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/de51d0a3c02f9140c300db799aceae13f6a419c83787f971.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/de51d0a3c02f9140c300db799aceae13f6a419c83787f971.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/de51d0a3c02f9140c300db799aceae13f6a419c83787f971.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/de51d0a3c02f9140c300db799aceae13f6a419c83787f971.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/de51d0a3c02f9140c300db799aceae13f6a419c83787f971.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/de51d0a3c02f9140c300db799aceae13f6a419c83787f971.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/de51d0a3c02f9140c300db799aceae13f6a419c83787f971.png/w_1011 1011w\"></figure><p>Again, a hat tip to Max Tegmark for putting that together, and especially these principles. We might want to work to identify important principles that we can get different communities to agree on, and then formalize and institutionalize them to make sure they stick.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28fb0ad537936cd3c1fe895a57c8d0c4d3e5f0765af1b0bf.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28fb0ad537936cd3c1fe895a57c8d0c4d3e5f0765af1b0bf.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28fb0ad537936cd3c1fe895a57c8d0c4d3e5f0765af1b0bf.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28fb0ad537936cd3c1fe895a57c8d0c4d3e5f0765af1b0bf.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28fb0ad537936cd3c1fe895a57c8d0c4d3e5f0765af1b0bf.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28fb0ad537936cd3c1fe895a57c8d0c4d3e5f0765af1b0bf.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28fb0ad537936cd3c1fe895a57c8d0c4d3e5f0765af1b0bf.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28fb0ad537936cd3c1fe895a57c8d0c4d3e5f0765af1b0bf.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28fb0ad537936cd3c1fe895a57c8d0c4d3e5f0765af1b0bf.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28fb0ad537936cd3c1fe895a57c8d0c4d3e5f0765af1b0bf.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28fb0ad537936cd3c1fe895a57c8d0c4d3e5f0765af1b0bf.png/w_983 983w\"></figure><p>In summary, what's to be done? A lot of work. What kind of skills do we need? Virtually every skill set \u2014 people who can help us grow the community, conduct operations and administration, or just be enthusiastic and efficient. We need people to do policy engagement, outreach, and media strategy. For example, what should the media strategy of a group like the Future of Humanity Institute be when there's another self-driving car incident, or when truckers are being massively displaced from their sites of employment? These are important near-term issues, but also they're sites for having conversations about longer-term issues.</p><p>We're doing work on strategy, theoretical work, and mathematical modeling of tech races. We\u2019re trying to understand AI development and what predicts innovation, measuring actual capabilities in different sites in the world's countries to understand the value chain and the supply chain of AI. We're surveying publics and elites around the world. We're trying to design safety standards and working with AI safety researchers, and tackling a range of other issues.</p><p>If this seems important and interesting to you, I strongly encourage you to get involved. The <a href=\"https://80000hours.org/articles/ai-policy-guide/\"><u>AI Policy Career Guide</u></a> has some texts that can point you in the right direction. There\u2019s also a reading list. And in general, just reach out to me. There are also people working on this at a range of sites, and we'd be very happy to help you be productively engaged. Thanks.</p><h1>Q&amp;A</h1><p><strong>Nathan:</strong> Let\u2019s have a seat. Thank you for the talk and for being here. We'll give a second for questions to come in from the audience [via the conference app].</p><p>One thing that I'm struck by is it seems like we're making a lot of progress. I've been involved in this community, sort of from the edges, for a number of years. And there was a time (seven to 10 years ago) when to even talk about something like this was extremely \u201cfringe.\u201d Only weirdos seemed to be willing to go there. Now we have respectable people like you and a growing body of academics who are getting involved. So it seems like there has been a lot of social progress.&nbsp;</p><p>What about technical or practical progress on these issues? It seems that we're bringing people together, but what are those people producing so far? And should we feel any safer than we did 10 years ago?</p><p><strong>Allan:</strong> I can speak to the strategy side of it. One comment that was made at the Asilomar Conference that resonated as true to many people is that AI strategy today is where AI safety was two years ago. The first beneficial meeting for AI safety was in Puerto Rico. Just a handful of individuals in the world were thinking seriously and full-time about it. That has changed today. Now the leading AI groups have safety teams. There are people doing PhDs with an eye towards AI safety. And that's very exciting. And there has been a lot of technical progress that's coming out of that.</p><p>AI strategy is just where AI safety was two years ago, but I think it's rapidly scaling up. A lot of thinking has been done, but it's not public yet. I think in the coming year you will start to see a lot of really insightful strategic analysis of AI existential risk.&nbsp;&nbsp;</p><p>I will also say I've given some talks like this (e.g., political science workshops) to other respectable audiences and none of the PhD students think this is crazy. Some of them think, \u201cOh, I don't have to quit smoking because of this.\u201d That was one comment. But they all think this is real. And the challenge for them is that the discipline doesn't entirely support work on the future.</p><p>One question I got when I presented at Yale was \u201cHow will you be empirical about this?\u201d Because we're social scientists, we like data, and we like to be empirical. And I remarked that it is about the future, and it's hard to get data on that, but we try. So I think it's a challenge for currently existing disciplines to adapt themselves to this problem, but increasingly we're finding good people who recognize the importance of the problem enough to take the time to work on it.</p><p><strong>Nathan:</strong> So, the first audience question, which I think is a good one, is this: Could you provide more detail on what an AI governance board might look like? Are you thinking it will be a blue-ribbon panel of experts or more of a free-for-all, open-democracy type of structure, where anyone can contribute?</p><p><strong>Allan:</strong> I think there are a lot of possibilities and I don't have a prescription at this point, so I'm not going to answer your question. But these are the issues we need to think through. There are trade-offs that I can talk about.&nbsp;</p><p>There's the issue of legitimacy. The UN General Assembly, for example, is often seen as a legitimate organization because every country gets a vote, but it's not necessarily the most effective international body. Also, you have to weigh your institutions in terms of power holders. So if you have the most ideal governance proposal, it might be rejected by the people who actually have the power to enact it. So you need to work with those who have power, make sure that they sign onto a regime, and try to build in sites of intervention \u2014 the key properties of whatever this development and governance regime is \u2014 so that good comes out of it.</p><p>I'll mention a few suggestions. Whatever development regime it is, I think it should have:</p><ol><li>A constitution \u2014 some explicit text that says what it's about and what it\u2019s trying to achieve.</li><li>Enough transparency so that the relevant stakeholders \u2014 be that the citizens of the country, if not citizens of the world \u2014 can see that the regime is, in fact, building AI according to the constitution. And I should say that the constitution should be a sort of common-good principle type thing.</li><li>Accountability, so that if the regime isn\u2019t working out, there's a peaceful mechanism for changing the leadership.</li></ol><p>These are basic principles of institutional design, but it's important to get those built in.</p><p><strong>Nathan:</strong> So speaking of people in power, I haven\u2019t seen this clip, but President Obama apparently was asked about risks related to artificial intelligence. And the answer that he gave seemed to sort of equate AI risk with cybersecurity risks that we know and love today. Do you think that there is a sufficient understanding at the highest levels of power to even begin to make sense of this problem? Or do we have a fundamental lack of understanding that may be quite hard to overcome?</p><p><strong>Allan:</strong> Yeah. That's a great clip. I wish I could just throw it up really quickly. We don't have the AI yet to just do that. I encourage you to watch it. It's hard to find it. <a href=\"https://www.youtube.com/watch?v=72bHop6AIcc\"><i>Wired</i> put it on their page</a>. He's asked about superintelligence and if he's worried about it and he pauses. He hesitates and gives sort of a considered \u201chmm\u201d or sigh. And then he says, \u201cWell, I've talked to my advisors and it doesn't seem to be a pressing concern.\u201d But the pause and hesitation is enough to suggest that he really did think about it seriously. I mean, Obama is a science fiction fan. So I think he probably would have been in a good place to appreciate other risks as they arise.&nbsp;</p><p>But I think a lot of people in government are likely to dismiss it. Many reports from the military or government have put superintelligence worries at least sufficiently distant enough that we don't really need to think about or address it now. I will say though, that cybersecurity is likely to be a site where AI is transformative, at least in my assessment. So, that's one domain to watch in particular.</p><p><strong>Nathan:</strong> Here\u2019s another question from the audience: If there is a 5% chance of extinction due to AI, one would not be unreasonable to jump to the conclusion that maybe we should just not do this at all. It's just too hot to touch. What do you think of that idea? And second, is there any prospect of making that decision globally and somehow sticking to it?</p><p><strong>Allan:</strong> Yeah. I'll flick back to the slide on opportunities. I actually had a conversation the other day with family members and friends, and one person at the table asked that question: If it's so risky, why don't we not do it? And then another friend of the family asked, \u201cWhat are the impacts of AI for medicine and health, and for curing diseases?\u201d And I think in many ways those are two sides of the policy decision. There are tremendous opportunities from AI, and not just material ones, but opportunities like curing Alzheimer's. Pretty much any problem you can imagine that's the product of intelligence could [be solved with] machine intelligence. So there's a real trade-off to be made.</p><p>The other issue is that stopping AI progress is politically infeasible. So I don't think it's a viable strategy, even if you thought that the trade-off weighed in favor of doing so. And I could talk a lot more about that, but that is my position.</p><p><strong>Nathan:</strong> Probably the last question that we can take due to time constraints is this: Thinking about the ethical direction that we want to take as we go forward into the future \u2014 the value alignment problem \u2014 you had posed that notion that if we have two years, we can probably figure it out.</p><p><strong>Allan:</strong> Yeah.</p><p><strong>Nathan:</strong> But if we don't, maybe more than likely we can't. That strikes someone in the audience and I would say me too, as maybe a little too optimistic, because we've been working for thousands of years on what it means to have a good life and what good is.</p><p><strong>Allan:</strong> Right.</p><p><strong>Nathan:</strong> Do you think that we are closer to that then than maybe I think we are? Or how do you think about the kind of fundamental question of \u201cWhat is good?\u201d in the first place?</p><p><strong>Allan:</strong> Right. To be clear, this conjecture is about whether a single person who knows what they want can build a superintelligent machine to advance those interests. It says nothing about whether we all know what we want and could agree on what we want to build. So can we agree on the political governance question: Even if we all have fundamental preferences, how do we aggregate those in a good way? And then there's the deeper question of what should we want? And yeah, those are hard questions that we need people working on, as I mentioned. In terms of a moral philosophy in politics, what do we want? We need your help figuring that out.</p><p><strong>Nathan: </strong>Well, thank you for wrestling with these issues and for doing your best to protect our future. Professor Allan Dafoe, thank you very much.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "DjfgneqsLkCyFbrTR", "title": "James Turitto: Are we facing a reproducibility crisis, and what can we do about it?", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=RNeL_sj2vAQ&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=10\"><div><iframe src=\"https://www.youtube.com/embed/RNeL_sj2vAQ\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Many in the social and health sciences claim their fields are facing reproducibility crises, in which researchers have failed to replicate many of the major experimental findings in their fields, and that much of the problem is due to bad research. What practices and incentives are driving the problem and how bad is it? What practical steps can we take to improve research practice, transparency, and reproducibility?&nbsp;</i></p><p><i>Based at MIT, J-PAL is a network of economists who have run over 800 randomized controlled trials in over 80 countries to ensure that policy is informed by scientific evidence. James Turitto, head of J-PAL\u2019s Research Transparency and Reproducibility Initiative, tackles these questions and shares how trial registries, pre-publication replications, and data publication can help address these challenges in social science.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "AmwrDXR7QPzTEGjZi", "title": "George Church, Kevin Esvelt, & Nathan Labenz: Open until dangerous \u2014 gene drive and the case for reforming research", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=N1EyMRGrmho&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=9\"><div><iframe src=\"https://www.youtube.com/embed/N1EyMRGrmho\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>The wisdom with which we develop and deploy new technologies will define the future of our civilization. Why do we conduct reseearch in small teams of specialists who cannot reliably anticipate consequences on their own? Might it be better to share our best ideas and plans with others, actively inviting concerns, criticism, and possible improvements? Unfortunately, the current system has evolved to punish sharing. By highlighting the benefits of an open approach and the dangers of the status quo, gene drive may allow us to test a new approach and change the governing incentives.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "kqzLFMYB7Zg9BBC2k", "title": "Lewis Bollard: Changing corporate and public policy \u2014 lessons from farm animal advocacy", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=G-t_wflnPiw&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=8\"><div><iframe src=\"https://www.youtube.com/embed/G-t_wflnPiw\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the last few years farm animal advocates have secured major corporate policy wins, a few state policy wins, and almost no federal policy wins. Why? And what lessons does this experience hold for future farm animal advocacy and other EA policy work?</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "yPpg662jzHujqPJNo", "title": "Brigid Slipka: Impact vs. principle at the American Civil Liberties Union", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=xXceGPUWOLs&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=7\"><div><iframe src=\"https://www.youtube.com/embed/xXceGPUWOLs\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>The ACLU has a 97-year history of impact litigation, using singular lawsuits at specific points of injustice to impact broad populations. In recent years, lobbying and grassroots mobilization have amplified this targeted political pressure. It is this combination of tactics and decades of history that has put the ACLU at the forefront of the resistance in this era of Trump.</i></p><p><i>In the future, we plan to post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "64qNar2QHsWKYiJ8z", "title": "Heidi McAnnally-Linz: Using evidence to inform policy", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=PAYVIXlk4rw&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=6\"><div><iframe src=\"https://www.youtube.com/embed/PAYVIXlk4rw\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this 2017 talk, </i><a href=\"https://www.poverty-action.org/\"><i>Innovations for Poverty Action's</i></a><i> Heidi McAnnally-Linz argues the evidence movement has been successful at designing and evaluating effective solutions, but that it now should move to mobilizing policy-makers to use those solutions. She also gives examples of IPA's work to influence policy-makers in Zambia, Peru, and&nbsp;Ghana.</i></p><p><i>The below transcript is lightly edited for&nbsp;readability.</i></p><h1>The Talk</h1><p>Hi, everyone. My name is Heidi, and I work for an organization called Innovations for Poverty Action. We run randomized evaluations in the field, and we've done about 650 of them in over 50 countries around the world, in the last 15&nbsp;years.</p><p>So I'm going to start the way I normally introduce IPA, and I'm going to ask you guys what might seem like a pretty silly question. If you had a dollar, and you wanted to buy oranges, you wanted some tasty, good-for-you oranges, which one would you buy? It's not supposed to be a tricky question. It's supposed to be very straightforward, because it&nbsp;is.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/62Dd7Y2BeoUaAU8IuMYqkc/1076dfd8e13f399c2270fb5535426eb0/Oranges.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/62Dd7Y2BeoUaAU8IuMYqkc/1076dfd8e13f399c2270fb5535426eb0/Oranges.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/62Dd7Y2BeoUaAU8IuMYqkc/1076dfd8e13f399c2270fb5535426eb0/Oranges.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/62Dd7Y2BeoUaAU8IuMYqkc/1076dfd8e13f399c2270fb5535426eb0/Oranges.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/62Dd7Y2BeoUaAU8IuMYqkc/1076dfd8e13f399c2270fb5535426eb0/Oranges.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/62Dd7Y2BeoUaAU8IuMYqkc/1076dfd8e13f399c2270fb5535426eb0/Oranges.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/62Dd7Y2BeoUaAU8IuMYqkc/1076dfd8e13f399c2270fb5535426eb0/Oranges.png?w=1800&amp;q=70 1800w\"></figure><p>So now I'm gonna ask something that might seem even more simple to you guys in this room, but I always get varied answers to this question. So let's say you wanted to increase attendance at schools in Kenya. Let's say the reason why kids don't go to school is because they don't have the clothes, they don't have the right shoes, uniforms are required, so they don't have uniforms, so they don't show up. Or, would you give them deworming pills, so that they don't have to feel bad, and they come to&nbsp;school?</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/4FcpaE3WzCuGE2Wwk2YiaO/291c3567a4a4eb006d9e928db80b5c1b/Uniforms_vs_deworming.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/4FcpaE3WzCuGE2Wwk2YiaO/291c3567a4a4eb006d9e928db80b5c1b/Uniforms_vs_deworming.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/4FcpaE3WzCuGE2Wwk2YiaO/291c3567a4a4eb006d9e928db80b5c1b/Uniforms_vs_deworming.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/4FcpaE3WzCuGE2Wwk2YiaO/291c3567a4a4eb006d9e928db80b5c1b/Uniforms_vs_deworming.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/4FcpaE3WzCuGE2Wwk2YiaO/291c3567a4a4eb006d9e928db80b5c1b/Uniforms_vs_deworming.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/4FcpaE3WzCuGE2Wwk2YiaO/291c3567a4a4eb006d9e928db80b5c1b/Uniforms_vs_deworming.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/4FcpaE3WzCuGE2Wwk2YiaO/291c3567a4a4eb006d9e928db80b5c1b/Uniforms_vs_deworming.png?w=1800&amp;q=70 1800w\"></figure><p>Okay, in this room, everybody gets the right answer, right? But you'd be surprised, around the world, whether it's in the Ministry of Education in Ghana, or in a room with non-EA philanthropists in San Francisco, the message still isn't quite out there. However, in this room, it's pretty obvious, the additional years of schooling that you get for $100, for school-based deworming versus for uniforms, is quite significant. And I won't go into more details on&nbsp;that.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/7AQFEqu9LGa6WEqwEUougY/03585d296f46a254331799790a6a6750/Uniforms_vs_deworming_2.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/7AQFEqu9LGa6WEqwEUougY/03585d296f46a254331799790a6a6750/Uniforms_vs_deworming_2.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/7AQFEqu9LGa6WEqwEUougY/03585d296f46a254331799790a6a6750/Uniforms_vs_deworming_2.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/7AQFEqu9LGa6WEqwEUougY/03585d296f46a254331799790a6a6750/Uniforms_vs_deworming_2.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/7AQFEqu9LGa6WEqwEUougY/03585d296f46a254331799790a6a6750/Uniforms_vs_deworming_2.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/7AQFEqu9LGa6WEqwEUougY/03585d296f46a254331799790a6a6750/Uniforms_vs_deworming_2.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/7AQFEqu9LGa6WEqwEUougY/03585d296f46a254331799790a6a6750/Uniforms_vs_deworming_2.png?w=1800&amp;q=70 1800w\"></figure><p>But really over the last 15 years or so, this movement, together, all of us, have really changed standards for evidence. No longer is it acceptable for charities just to say that their outputs are impactful. This is even becoming much more mainstream. All kinds of donors, who are not just involved in the EA movement, are now asking charities, asking governments even, to say what is the impact of their programs. And increasingly, people have answers to&nbsp;this.</p><p>So this is something that you've all been involved in over the years, and a lot of the research behind some of these cost-effective, fundable programs was carried out by IPA. So this is just to say, look, we've been a part of this, together with all of you, for a very long time, and we've seen these things that are really pill-like have lots of success, particularly at scale, because they're really cost-effective, they're fundable, all the analytical brains can get behind it and say, \"Yes, we're gonna do&nbsp;this.\"</p><p>So we've seen it scale. We're seeing it with <a href=\"https://www.evidenceaction.org/dewormtheworld/\">Deworm the World</a>, with <a href=\"https://www.againstmalaria.com/\">the Against Malaria Foundation</a>, with <a href=\"https://www.givedirectly.org/\">GiveDirectly</a>, a lot of this thanks to GiveWell and many others. And that's great, and there's been a lot of impact of this, and I'm in no way arguing that this should change. This is certainly a part of our legacy, and something we're all really proud&nbsp;of.</p><p>But what happens when the questions get a little bit more complicated? Let's say instead of attendance, you actually want to increase learning levels. A study that we did in Ghana showed that, when we did the baseline, only 6% of kids in third grade could read a really basic sentence. Imagine your kid being in school for three years, or four years in some cases, and not learning&nbsp;anything.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/2kYDAxnGao4EMMCSIGQmei/48a5b868e9f5bc33de01218f8f572510/Scholarships_vs_learning_levels.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/2kYDAxnGao4EMMCSIGQmei/48a5b868e9f5bc33de01218f8f572510/Scholarships_vs_learning_levels.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/2kYDAxnGao4EMMCSIGQmei/48a5b868e9f5bc33de01218f8f572510/Scholarships_vs_learning_levels.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/2kYDAxnGao4EMMCSIGQmei/48a5b868e9f5bc33de01218f8f572510/Scholarships_vs_learning_levels.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/2kYDAxnGao4EMMCSIGQmei/48a5b868e9f5bc33de01218f8f572510/Scholarships_vs_learning_levels.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/2kYDAxnGao4EMMCSIGQmei/48a5b868e9f5bc33de01218f8f572510/Scholarships_vs_learning_levels.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/2kYDAxnGao4EMMCSIGQmei/48a5b868e9f5bc33de01218f8f572510/Scholarships_vs_learning_levels.png?w=1800&amp;q=70 1800w\"></figure><p>So learning levels are a real problem. What would you do? Which one would you buy to solve this learning level problem? Would you spend money on merit-based scholarships for girls? Or would you want to re-assign students based on learning&nbsp;level?</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/4OrYuvP62cGGmoCwgIiMAQ/42e936eedc0193760b1a1b2e6fd7d675/Scholarships_vs_learning_levels_2.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/4OrYuvP62cGGmoCwgIiMAQ/42e936eedc0193760b1a1b2e6fd7d675/Scholarships_vs_learning_levels_2.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/4OrYuvP62cGGmoCwgIiMAQ/42e936eedc0193760b1a1b2e6fd7d675/Scholarships_vs_learning_levels_2.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/4OrYuvP62cGGmoCwgIiMAQ/42e936eedc0193760b1a1b2e6fd7d675/Scholarships_vs_learning_levels_2.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/4OrYuvP62cGGmoCwgIiMAQ/42e936eedc0193760b1a1b2e6fd7d675/Scholarships_vs_learning_levels_2.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/4OrYuvP62cGGmoCwgIiMAQ/42e936eedc0193760b1a1b2e6fd7d675/Scholarships_vs_learning_levels_2.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/4OrYuvP62cGGmoCwgIiMAQ/42e936eedc0193760b1a1b2e6fd7d675/Scholarships_vs_learning_levels_2.png?w=1800&amp;q=70 1800w\"></figure><p>So it wasn't obvious to us, when we did these original studies. And by we, I mean that IPA did the implementation of a number of these studies. We also work closely with <a href=\"https://www.povertyactionlab.org/\">J-PAL</a>, so you've probably seen some of this from them as&nbsp;well.</p><p>But when we think about how we then put this into practice at scale, it becomes a lot more complicated, because it's not just a matter of going a couple times a year into the schools, and giving out deworming pills. If you really want to re-assign students to classes based on learning level, and teach to their level, you\u2019ve got to change the way classrooms are structured, you have to change the way teachers teach, and particularly if you want to do this at scale in Africa, you have to do this at the government level. And that becomes a lot more&nbsp;complicated.</p><p>So to take a step back, as I said, we have over 650 studies, randomized trials with few exceptions, things like complement randomized trials, in over 51 countries across sectors. But the number of these studies that all of you have heard of may be slightly larger than your average person, but when I talk to folks even here in EA Global, people are only aware of somewhere around five studies beyond the ones I mentioned already, the deworming one, GiveDirectly's,&nbsp;etc.</p><p>Is that because we didn't find positive results? Or is it because applying those results is a little bit more complicated? I argue that it's because applying those results is a little bit more complicated. Certainly, all 650 do not have positive results, and we've talked about a lot of the negative ones a lot, and I'm not going to get into that today, but there are a lot more positive results than are being&nbsp;used.</p><p>So what are we doing about all of this? And how do we actually get these more complicated kinds of programs implemented at scale, such that we're leveraging all of the money, all of the effort, that has been put into all of these 650 studies, to actually get the most cost-effective outcomes at scale? Not just things that are going to get us the best pill-like interventions, but things that might change systems, and leverage existing aid money, leverage existing donor money, to make those interventions more&nbsp;effective.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/2AUKrJ6iJuoOOcGeSskA6/46da189d862431383191976b756a9c1a/Theory_of_change.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/2AUKrJ6iJuoOOcGeSskA6/46da189d862431383191976b756a9c1a/Theory_of_change.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/2AUKrJ6iJuoOOcGeSskA6/46da189d862431383191976b756a9c1a/Theory_of_change.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/2AUKrJ6iJuoOOcGeSskA6/46da189d862431383191976b756a9c1a/Theory_of_change.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/2AUKrJ6iJuoOOcGeSskA6/46da189d862431383191976b756a9c1a/Theory_of_change.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/2AUKrJ6iJuoOOcGeSskA6/46da189d862431383191976b756a9c1a/Theory_of_change.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/2AUKrJ6iJuoOOcGeSskA6/46da189d862431383191976b756a9c1a/Theory_of_change.png?w=1800&amp;q=70 1800w\"></figure><p>So for IPA, for the last 15 years, this has kind of been what we call our theory of change. We see the really basic problem in the world is that there's limited evidence on what works best, and then there's limited use of that available evidence, and therefore you get ineffective programs and policies. And our solution is two-fold as well: we design and evaluate effective solutions, and then we mobilize and support decision makers to use evidence, and bippity boppity boo, better programs and&nbsp;policies.</p><p>The first part of our theory of change is really scientific. We've got this down, we've been doing it for 15 years, design and evaluate potential solutions to poverty problems. We know how to run a good randomized evaluation, we have over a thousand people in the field across 20 countries right now who do that, who know how to do that really&nbsp;well.</p><p>But we, and my hypothesis is that this is true in EA as well, and I'm looking forward to hearing others speak on how they're influencing policy as well, we don't really know how to do the mobilizing and supporting decision makers to use evidence part yet. At IPA, we've only really been starting to focus on this, in spite of it being a part of our mission, a part of what we do. We've only really been putting actual staff time into this over the last maybe three or four years. And really, the investment of staff time in this is maybe 3% of what our staff are doing right&nbsp;now.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/1066elVaNEGCoe6imAcKyY/068c20642b7219b0fcf7b94c01931162/Policy_world_map.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/1066elVaNEGCoe6imAcKyY/068c20642b7219b0fcf7b94c01931162/Policy_world_map.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/1066elVaNEGCoe6imAcKyY/068c20642b7219b0fcf7b94c01931162/Policy_world_map.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/1066elVaNEGCoe6imAcKyY/068c20642b7219b0fcf7b94c01931162/Policy_world_map.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/1066elVaNEGCoe6imAcKyY/068c20642b7219b0fcf7b94c01931162/Policy_world_map.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/1066elVaNEGCoe6imAcKyY/068c20642b7219b0fcf7b94c01931162/Policy_world_map.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/1066elVaNEGCoe6imAcKyY/068c20642b7219b0fcf7b94c01931162/Policy_world_map.png?w=1800&amp;q=70 1800w\"></figure><p>So I think this is the next frontier for the evidence movement. So even what little we have invested is paying off. We're working with governments in all of these countries, we're investing more in the ones in orange, and we're starting to create relationships, and starting to help think about how to scale interventions with evidence that already exists, but also how to co-create evidence together with these policy makers, answer the questions that they actually have, and create more effective solutions for them in their&nbsp;context.</p><p>I'm gonna tell you about three different successes in that, and even in some of these cases, I would still consider them only along the way, and not there yet. We still haven't figured out how to do this well. But I think there's an opportunity for us, in the EA community, to think more about how to actually use evidence at real scale, and create real&nbsp;change.</p><p>So the first example is in Zambia, where we did a really basic study, together with the Ministry of Health. The second is from Peru, where we created kind of a nudge unit. And then the third is from Ghana, where we've been working for almost a decade with the Ministry of Education&nbsp;there.</p><p>The first example came to us because we've been working in Zambia for quite a few years with <a href=\"https://ashrafnava.wordpress.com/\">Nava Ashraf</a>, who's a researcher. And the Ministry of Health came to her because she's been very helpful, before it came to us, because we've been very helpful before. And they said, \"Look, we're going to scale up our program for community health workers. We don't have enough doctors in this country, we don't have enough nurses in this country. We\u2019ve got to recruit more community health workers. We want to do this in the most effective way possible, but we basically have no budget. What do you think we should&nbsp;do?\"</p><p>Well, this is a very common problem around the developing world. These are very under-trained workers, so we said, \"Okay, what if we just varied the way you recruit them?\" Because there's a very big debate about should you recruit people who are more motivated to serve their community, or should you recruit people who are more motivated to advance their career? This was a debate that the Ministry wanted to solve, so we said, \"Well this is something we can&nbsp;randomize.\"</p><p>So all we did was randomize the flyer. It was very low-cost, in terms of what the government had to do. And it turns out that just changing to the career flyer meant that the community health workers who were recruited that way were 29% more effective at reaching households, and we're starting to see results coming out, seeing that it actually leads to even better health outcomes for those households. And as they scale this, and they are using this tactic to scale to 5,000 community health workers, we estimate that they'll capture an additional 315,000 households, just with this small&nbsp;tweak.</p><p>What's really exciting about this story is that not only are they using the results, but now they're saying, \"Okay, now we've made this career promise. How do we deliver on this career promise? How do we create effective careers for these folks? Can you help us with evidence for&nbsp;that?\"</p><p>So this one study is leading to what we're starting to call, at IPA, a culture of evidence-based decision making, and we think that's an exciting success. I'll tell you more when I come back, as I'm going to be in Zambia next month, learning a little bit more about&nbsp;this.</p><p>My next example, and we did this together with our partners at J-PAL, is working with the Ministry of Education in Peru. So the Ministry of Education in Peru has, or at least had, a lot of very sophisticated thinkers, and they wanted to think about what kinds of things they could do that are low-cost, to improve their system. So we spent some effort, and had an embedded staff member in the Ministry, and they created this lab, together with researchers, Ministry officials, and folks from IPA and J-PAL. And their goal really was just to say, \"What can we do with the administrative data that we have, to test small things that we could then use to improve learning outcomes?\" Or attendance, or whatever the outcome they wanted to improve&nbsp;was.</p><p>And so we've done quite a few small-scale studies with them in the past few years, and the results now are just starting to come out. There are a couple things that are really exciting about this. The first is that it's institutionalized. So we created an institution, a nudge unit within the Ministry of Education in Peru. So when government changes are happening, this is already within that. Of course, there's always a risk that these kinds of things get cut, but at least it's already kind of there and&nbsp;institutionalized.</p><p>Another really exciting thing that's happening from that, is that people who work in the Ministry of Education, who worked with us on this, have now moved to the Ministry of Health, and also to the Ministry of Social Protection, and we're exploring doing labs with those ministries as&nbsp;well.</p><p>And then finally, the first results are out, and the study was essentially what we will call a field replication plus, a replication of a study in the field that kind of tweaked a couple of things. The original study was about what happens when you give information to students about what the returns to education are: are they more likely to invest in education? There were big impacts in the Dominican Republic, and we're seeing similar results in Peru, and we tried a couple of different technology tweaks there. So the Ministry is excited to start working on scaling that up. We still don't know exactly how that's going to play out, but it's in&nbsp;process.</p><p>And then the third example is in Ghana. Ghana is the place where we've invested the most in policy staff. Like I said, we haven't quite figured out what the right mix of research and policy staff is, and quite frankly, there hasn't been a lot of funding for the policy side, because we've all been so excited about generating the research, which we've done a really great job&nbsp;of.</p><p>But we started by doing another field replication of a successful program run by an NGO in India, which targets teaching at the level of the child, and separates kids based on their levels. We did this at a very large scale with the government, nationally representative, and we saw quite strong results, even though there was varied implementation, even though this was being implemented by the government. It's very&nbsp;impressive.</p><p>And we thought \"Great. Done deal. We're going to scale.\" But it's more complicated than that. We were using teaching assistants that were being paid for by the youth employment program, and now that youth employment program is defunct. So where are we going to get these teaching assistants? It's very&nbsp;complicated.</p><p>And they said \"Look, what we really want to do is what we told you at the beginning. We want to do this model, but we want our teachers to do&nbsp;it.\"</p><p>We said \"Okay, well that's why one of the arms of the study was the teacher-led intervention.\" It wasn't as strong as when the teaching assistants did it, but it still has a positive impact, and it still was&nbsp;cost-effective.</p><p>So they said, \"Well look, what we want to do is we want to make this even more effective. We want our teachers to lead it, and we want it to be more effective. What can we&nbsp;do?\"</p><p>So there are even more questions. They are still planning to grow this at scale, but we're now testing how we can use people in the existing system, so district monitors, the head teachers, to help improve implementation of this program, such that the results are even stronger. So we're working together with them on&nbsp;this.</p><p>But what that led to is all kinds of other questions. It led to, again, what we've been calling this culture of evidence-based decision making. And now, today, with the Ministry, we have five ongoing studies with them, and all of them are influencing what they're doing in their curriculum, in their teacher training, even in their large, large policies on what kinds of things to fund. With all five of these, from pre-kindergarten all the way through secondary, they're calling us with questions, and they're working with us on answering them. And really, we're in this position where we can help answer these questions, and help improve the overall&nbsp;system.</p><p>So how do we do this, IPA, and as a community? I think there are a couple of important things that we need to remember about the necessary conditions in which evidence can be used. These are based on not only these examples that I've told you, but even when we think back to how it was that we got deworming to be taken up at such a big level, this was also&nbsp;involved.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/Pdj923mPG8c48MgM0ma4y/07e1d96dee726b7a92109ced602971e3/Evidence_needs.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/Pdj923mPG8c48MgM0ma4y/07e1d96dee726b7a92109ced602971e3/Evidence_needs.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/Pdj923mPG8c48MgM0ma4y/07e1d96dee726b7a92109ced602971e3/Evidence_needs.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/Pdj923mPG8c48MgM0ma4y/07e1d96dee726b7a92109ced602971e3/Evidence_needs.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/Pdj923mPG8c48MgM0ma4y/07e1d96dee726b7a92109ced602971e3/Evidence_needs.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/Pdj923mPG8c48MgM0ma4y/07e1d96dee726b7a92109ced602971e3/Evidence_needs.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/Pdj923mPG8c48MgM0ma4y/07e1d96dee726b7a92109ced602971e3/Evidence_needs.png?w=1800&amp;q=70 1800w\"></figure><p>So first, the evidence has to exist and it needs to be credible, which means we want it to be rigorous. We're doing really well on this. We've got the 650 randomized trials so we're doing well. It's with everything else that we're&nbsp;struggling.</p><p>It also should be relevant, and sometimes these things take time. Sometimes the studies take time, and by the time we get to the end of them, the decision has already been made. So we need to be thinking together, otherwise we share the results with the policy makers, and they go, \"I don't really care about that. Thanks for telling me, but this doesn't really help.\" And so, we've learned that co-creation of evidence from the beginning is really&nbsp;important.</p><p>And this evidence has to be accessible. Right now, with a lot of these 650 studies, if we're not out there talking about them, and writing clear summaries of them, people don't get access to them. And they need access to them at the right time. There are policy windows. Sometimes those policy windows are right after a study, and sometimes they are ten years, and people need the right information at the right&nbsp;time.</p><p>You have to get buy-in, from the beginning, ideally, and this is what we've learned the hard way. You have to get it from users and influencers, people who are going to use this information, and critically, the major donors. It's one thing to get the Ministry involved, but if USAID is their main donor, and they're not on board, we're in trouble. There also has to be funding for this. So the fact that the youth employment program got cut, well that really messed up that&nbsp;plan.</p><p>And users have to know how to apply it. So I like to think of building a culture of evidence-based decision making as the middle piece between evidence creation and evidence use at scale. And it's not so easy, it's more of an art than a science, and we don't like it as much. But it is the critical piece that's gonna help us get the ultimate outcomes that we&nbsp;want.</p><p>And so at IPA, we're doing this a lot by focusing on the local. So we're doing all these things, we're sharing the solutions, we're providing technical assistance, we're doing advocacy, and critically, as you've seen, we're engaging very deeply, particularly with our government partners, and helping them be able to use and understand evidence. But we've learned that to do this, we have to be on the ground, and we have to be there long-term. And so that's why IPA, in the past couple of years, has made a decision not to work in all 50 countries, but we have a presence in 20 countries, and offices in 14, and we're really focusing on how we can help with this iterative, collaborative process, and help answer next-phase questions, such that the evidence actually gets used along the&nbsp;way.</p><p>So to conclude, I don't want this to suggest that I don't think that everyone who is here for the GiveWell talk shouldn't continue giving their money to GiveWell. I'm our policy and communications person, I think that is the best kind of larger message for EA, to recruit more people. Get the best bang for your buck, invest in the most effective things, here are the most effective charities, if you're giving $100, this is where you want to give your&nbsp;money.</p><p>But I think we, as a community, also have to think about the creating of impact tomorrow. And we're doing this already, by continuing to support really good research, thinking about what is the next deworming. Open Phil is doing a really great job of this. And you all are doing this, but I think we can all do this more, help people understand how evidence can make their lives better, how creating evidence can actually be like a negative cost to them. So if they create evidence, it helps them cost-save, then the impact of whatever it is that they're doing, can be more cost-effective, wherever you&nbsp;are.</p><p>And then finally, I think this is something we're not yet doing. There's serious leverage that can happen if we think about how to actually create policy influence from all of the research that we already have. Small investments could leverage millions of dollars that have already been spent on research, to actually create sustained action, at the government level, in the countries where we all care about people's&nbsp;futures.</p><p>So I'm happy to take&nbsp;questions.</p><h1>Q&amp;A</h1><p><strong>Question:</strong> We had questions along a few different veins, but I'm going to start with some that talk a little bit more about the structure of IPA, who exactly works with whom, and how you establish your partnerships, that sort of thing. One of the questions asks what your relationship is to developed world funders. You mentioned a little bit that you get funded by USAID in some respects, but you're also working with developing countries, and yet you're a non-profit, you're not technically part of the US government. How does that work&nbsp;out?</p><p><strong>Heidi McAnnally-Linz:</strong> So the question really is how IPA gets funded, and how do we collaborate with funders. I'll divide it into two parts. The answer to the first question, how does IPA get funded is that we have funding from all of the large organizations that fund research, from the Gates Foundation to USAID to a number of large individuals, and the World Bank, 3ie,&nbsp;etc.</p><p>How do we work with funders? It's different in every country. And one of the things that we have not invested in is working on influencing funders at a global level. I don't know if anybody from J-PAL is here, but I think in terms of the family, they have the comparative advantage to do that a little bit more than we do, because they have a much larger global policy team. Our comparative advantage as an organization is really our presence on the ground, because we're running all these trials on the ground, and we have most of our staff on the ground. We have about 60 in the US, but a thousand people around the world. So we\u2019re really leveraging that local&nbsp;presence.</p><p>So for example, I was mentioning the Ghana example because we've been working closely with USAID, with a couple of other funders there, to varying degrees of success. And what we've found is that the larger the funder, the harder in. So some of the funders that have been really supportive of us are your business networks, your UBS, people who are from the business world, who are thinking about how to create leverage. When you start talking about larger aid, it gets much harder and more complicated, and requires more sustained investment of time and&nbsp;resources.</p><p><strong>Question:</strong> I'm particularly curious, and I think a couple people voiced similarly, that you have a unique spread. You said you're not going out to literally every country, but you're still spread across quite a few countries, and that sort of network development seems difficult. At least, I can't imagine dropping into a random country and getting close to the people who are making policies there. And that seems like both a sensitive thing, and also just a really difficult place to put your foot in the door. How do you build those initial&nbsp;contacts?</p><p><strong>Heidi McAnnally-Linz:</strong> So when I talk about the people who are doing this, I'm mostly talking about local staff. Our education policy person in Ghana is a woman who was in the education system before, she's incredibly networked, she's personable, and she just bought into why evidence matters, and actually applied for a different job with us, and we were like, \"No, no, no. Come over here and do this.\" And this is what I mean by building evidence-based&nbsp;culture.</p><p>Same kind of thing in Zambia, and in Peru. So that's really what I mean about building the long-term presence. We've only been able to do that because we've invested heavily in capacity-building of our staff, and recruiting people from those countries, and having a long-term presence&nbsp;locally.</p><p><strong>Question:</strong> I had a question or two also about how you choose which projects to focus on. One person said that they really like the cost-effectiveness work you've done in education, but they haven't found other attempts at cost-effectiveness calculations. They wonder, how do you choose education out of any number of things you could be&nbsp;doing?</p><p><strong>Heidi McAnnally-Linz:</strong> I think that with a lot of the cost-effectiveness analysis, we can thank our friends at J-PAL for doing that. But one of the things that has happened, is that because IPA is so decentralized, we've worked with over 400 researchers in all kinds of universities, and collecting cost data has strangely been an afterthought. And so, my team said \"Wait a minute, what do you mean you don't have accurate cost data? We've got impact data. We don't have cost&nbsp;data.\"</p><p>So we are making a particularly concerted effort now to collect cost data, because doing cost-effectiveness analysis when you have cost data, and you have an RCT, is pretty simple, all things considered. But when you don't have good cost data, which is difficult to collect, you don't have it. And we usually don't think about that until something's&nbsp;over.</p><p>So we have collected more cost data in education, partially because of the excitement around cost-effectiveness in education, and partially because of the researchers we've been working with, where we're already thinking about that. So one of the things that we're hoping to do with some of the strategic funding that we've been raising, as an organization, is more cost-effectiveness analysis in different&nbsp;sectors.</p><p><strong>Question:</strong> Might you be able to shed light on what those other sectors might&nbsp;be?</p><p><strong>Heidi McAnnally-Linz:</strong> Not yet. I mean health is also an easy and obvious one. I think we've been doing some in social protection, particularly with the older poor graduation studies, and we're doing a lot of replication plus of that work. So there will definitely be a cost-effectiveness analysis&nbsp;there.</p><p><strong>Question:</strong> And another thing in choosing projects. Do you factor in the difficulty of implementation when you're choosing what to do, rather than just, what seems like it would be a good&nbsp;intervention?</p><p><strong>Heidi McAnnally-Linz:</strong> That makes it sound like IPA has agency in deciding a lot of things! So IPA is in the middle, and we've got three groups to deal with. You've got the donors, the researchers, and the implementing partners. Our job is to bring all of them together to do a&nbsp;study.</p><p>Now sometimes the donor pushes us to do something, and we go out and we get the researcher and the implementing partner. Sometimes the researcher is very interested in a particular question, and so he says, \"We want to do this. Are there any partners?\" So we find the partners. And typically, the researcher can also pretty easily find the&nbsp;money.</p><p>So a lot of it is driven by donors. But increasingly, what we want to be doing more of is where the policy maker comes to us and says \"This is a question we have\" and we then help find the funding, and in a lot of cases, do the research ourselves, or bring our network&nbsp;along.</p><p>And in terms of what IPA invests in strategically, a lot of it has been in education, but I think that has to do with our leadership, and what we think the best opportunities have been, historically, not because of any particular other strategy&nbsp;reason.</p><p><strong>Question:</strong> If you personally, or the people that you work with pretty closely, are trying to make something happen, are you now trying to create new policy partners? If you're saying, \"We should do a new project,\" what's the first action that you would decide to&nbsp;take?</p><p><strong>Heidi McAnnally-Linz:</strong> For the last year, I had to play an interim development director role for IPA, which was a lot of work. So now that I'm not doing that anymore, my personal work has been building our policy team on the ground. We've only, in the last year, recruited people in Kenya, and in Zambia, to do this work. In Burkina Faso and in the Philippines, we're hoping to recruit more, particularly to pursue this strategy. And we're working to identify where are the opportunities where we can have the most impact. And in those countries, there are particular projects around which I think we can go for a similar Ghana model. So we're focusing on that, and raising money to do more of that. That's my personal&nbsp;work.</p><p><strong>Question:</strong> Might you be able to talk a little bit more about what it's like on the ground, insofar as you've been involved, or you know other people who have done on-the-ground work? It feels easier to talk in abstract, of these policy partners in Peru or Ghana or somewhere else, but what's the messy actual situation&nbsp;like?</p><p><strong>Heidi McAnnally-Linz:</strong> I\u2019m just trying to think of what's the right story. So I'm better at marketing stories, so the more nitty-gritty ones are not coming to the top of my head right&nbsp;now.</p><p>But I had not spent time in our Ghana office, other folks had led that. And I was just there in March, and I showed up to what I already knew was this really deep relationship, and I really just had no idea. I had no idea that we were co-hosting this conference together with the Ministry, and they really felt like it was their conference about evidence, until I was really there. And so we asked the director of the Ghana Education Service, which is the implementing arm of the Ministry, \"So how did you think our conference went? What did you think?\" She goes \u201cWell you tell me. It was our conference.\" So I just didn't really understand the depth with which that was&nbsp;happening.</p><p>There's a USAID story along with that. They also were incredibly skeptical that Ghana was even remotely ready for something as advanced as teaching at the right level. I mean, they even said to me \"Look, they need to get their teacher attendance right.\" And I said \"Well, but I'm pretty sure,\" and I had to go look at the study to be sure. I was pretty sure that teaching at the right level, even the teacher-led version, increased teaching attendance. And I looked, and sure enough I was&nbsp;right.</p><p>So I went back to them, and I said \"Look, I understand you want to improve teacher attendance. Quite frankly, that's what we're trying to do with improving implementation. What that means is, improving teachers doing what they were doing. Which, in a lot of cases, means improving them showing up. And we saw that even when poorly implemented, this was doing that, and that was one of the reasons we had outcomes. And so, let's work together and think about&nbsp;this.\"</p><p>I don't know if he completely came around, but I know that since then, we have had a lot more interaction with him, and with his team, and they're sounding a lot more excited about what we're&nbsp;doing.</p><p>And again, in the flip side, my team on the ground spends a lot of time sitting on the bench outside the Ministry, waiting for somebody to help them get a meeting. So in the places where we don't have this relationship already, getting in the door has been incredibly challenging. Once we've been in the door, like in Ghana, it's much easier. But there's a lot of time spent waiting for the assistant of the right person and asking \u201cCan you help me out?\u201d So it\u2019s not all&nbsp;glamorous.</p><p><strong>Question:</strong> I imagine we have a couple of cynics in the audience, but I'm just projecting on them that they would say \"Well, there's no reason that these governments should care in the first place. Especially if it's not a particularly democratic government, why should they even care about the efficacy of these programs?\" Do you find some sort of golden ticket in the midst of a corrupt government? How do you find your way in? Are we just too&nbsp;cynical?</p><p><strong>Heidi McAnnally-Linz:</strong> There are lots of ways in. Donors is one way, and that's definitely been a way in for us. But I think you do find your gems, you find your champions, and those champions tend to grow and move around, and that's the long-term nature of it as well, and that's why we do think this is not a three-year project, and it's going to change&nbsp;everything.</p><p>That said, your skepticism is well-founded. I think it's a difficult thing, I think we don't know how to do it well, and I think it'll fail in a lot of places. But I think where it doesn't, it'll be high impact. The last thing I'll say is that we did one study that found that health workers did not actually want to take bribes, which we were very surprised by. And I think that just speaks to, when we're implementing policy, we're not necessarily talking at a very high level. We've had a couple cases in which we've engaged at an incredibly high level, with the President and the Minister himself and all that, but really, what we're talking about is changing the way things operate. And the people at those levels typically are bureaucrats that want their jobs to be doing something, more often than&nbsp;not.</p><p><strong>Question:</strong> I'm going to finish with a last question, which is presumably from some people in the audience who would be interested in actually helping out this work, not just observing from the sidelines. If you could just drag and drop them into the positions you'd really like to see filled, what would they be doing? Should they be researchers? Should they try to move to one of your on-the-ground offices somewhere? Or should they just donate&nbsp;money?</p><p><strong>Heidi McAnnally-Linz:</strong> At IPA, we've never struggled with getting really good entry-level researchers, RAs who are good, and then go on to do PhDs. Certainly, if that's something you want to do, we're a great place to come for that. We haven't struggled with that. Where we've really struggled is getting folks at the management level, people with a couple years of experience on the&nbsp;ground.</p><p>I've been pretty successful growing the policy and communications team, but people don't really think of us for that. And I saw a slide that was presented here earlier, it said that there are skills lacking both in management and operations, and also in policy outreach and in marketing, and I think that's true. So if you have any inkling in those kinds of skills, I'd love to talk to&nbsp;you.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "oKwg3Zs5DPDFXvSKC", "title": "Marc Lipsitch: Preventing catastrophic risks by mitigating subcatastrophic ones", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=ml3JQSbbXn8&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=5\"><div><iframe src=\"https://www.youtube.com/embed/ml3JQSbbXn8\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Many of the catastrophic risks about which effective altruists concern themselves are likely to result from the failure to control much smaller events, which then grow so large that they do become catastrophic. In this EA Global: Boston 2017 talk, Marc Lipsitch describes examples from infectious diseases \u2014 a paradigm case of a growing risk \u2014 and implications for the allocation of our attention, effort and funds.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "5ugAidYCHvERXqiKo", "title": "Duncan Sabien: Convinced, not convincing (Boston)", "postedAt": "2017-06-02T08:48:42.273Z", "htmlBody": "<p><i>Duncan also </i><a href=\"https://forum.effectivealtruism.org/posts/PZkhfNMQ9MxoN7rp7/duncan-sabien-convinced-not-convincing-san-francisco\"><i>gave a version of this talk</i></a><i> at EA Global: San Francisco 2017.</i></p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=SFULyVo4uts&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=4\"><div><iframe src=\"https://www.youtube.com/embed/SFULyVo4uts\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: Boston 2017 talk, Duncan Sabien presents an introduction to the Centre for Applied Rationality's tools for increasing motivation, avoiding mistakes, and collaborating effectively.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "v7CxPpDsihJAdYGPJ", "title": "Ben Todd: How can we best work together as a community?", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=S7SFq4v3tRo&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=3\"><div><iframe src=\"https://www.youtube.com/embed/S7SFq4v3tRo\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this 2017 talk, 80,000 Hours' Ben Todd argues that it is easy for effective altruists to overlook the importance of coordinating with other members of the effective altruism community. This perspective has a number of implications \u2014 e.g., it increases the importance of gaining knowledge and sharing it with the community, as well as the importance of&nbsp;specialization.</i></p><p><i>The below transcript is lightly edited for&nbsp;readability.</i></p><h1>The Talk</h1><p>Hi, everyone. Great to be here. I'm Ben, the CO and co-founder of 80,000 Hours. Here's one of the most powerful ways to have more impact with your career: working with a great community. One reason for that is that it's really motivating. If you're around people who want to help others, then that changes the social norms and makes you more keen to contribute. Another thing is it's kind of like networking on steroids. If you meet one person in the community, who can then vouch for you to everyone else, then you can make hundreds of connections at&nbsp;once.</p><p>The third thing I wanted to talk about more today is that you can trade and coordinate with the rest of the community. Let's suppose I want to, say, build and sell a piece of software. One approach would be for me to go learn all the things I need myself - engineering, management, marketing. I probably wouldn't get that far. A much better approach would be to instead form a team of people who are specialists in each area, and then build it, and sell it together. Although I'll then have to share the profits with other people, the total size of the gains will be much larger. Overall we'll all win, and we'll all be better&nbsp;off.</p><p>What's going on under this is, firstly, we've got division of labor. So by specializing, we can each become much better at the individual skills and therefore more productive as a group. Another thing is that we can share fixed costs. We can all share the same company registration. We can share the same operational procedures. It's not three times harder to fundraise three times as much money from an angel investor. This allows us to achieve economies of scale. In total, we've got what's called the gains from&nbsp;trade.</p><p>An interesting thing about trade is that you can do it with people even if you don't particularly share their values, even if you don't have a common aim. So here's a purely hypothetical example. Suppose I run a global poverty charity, and I meet someone who runs an animal welfare charity. Say I don't think animal welfare is a very pressing problem, so I don't think their project has much impact. Then it turns out they feel exactly the same way about me. So neither of us thinks each other's project has much&nbsp;impact.</p><p>But then, consider, maybe I know a donor who isn't going to donate to my charity but might be interested in donating to them. So I can make an introduction to that donor to the other charity. This is just a small cost to me, but it might be a really huge deal to them. And then suppose they could do the same. Maybe they know another donor. Then we could trade. This means we end up with a situation where we both get a large benefit for a small cost to ourselves. So we've both been made better off, relative to our own aims. This is why a hundred people working together can have much more impact than a hundred people doing individually what seems&nbsp;best.</p><p>Now what I've said so far applies to any community that you might want to get involved with. There are lots of great communities out there, but I know many people who feel that getting involved with the Effective Altruism community gives a particularly big boost to their impact and their career and so on. Why might that be? Well, I think although you can trade with people who don't particularly share your values, if you're in a community and you do share their values, then you don't even need to trade. What do I mean by that? Well, if I help someone else in the community have a greater impact, then I've also had a greater impact as well. So we've both achieved our goals. That means I don't even need to try and make sure that if I do someone a favor, I get a favor back from them later, which would be trading. Instead I can just help the other person and that's already having a social&nbsp;impact.</p><p>This unleashes all kinds of opportunities to work together that just wouldn't be efficient in a community where we didn't share our values as much. Technically, we're reducing the transaction costs and principal agent problems involved with working together. So here's an example. We don't normally think about it like this, but actually earning to give can be an example of coordination. Back in the early days of 80,000 Hours, we needed someone to run the organization and we needed funding. There were two of us, me and someone called Matt, who were considering working at 80,000 Hours. But we realized that Matt had much higher earning potential than me, while I would be better suited to running 80,000 Hours, at least&nbsp;hopefully.</p><p>What happened is that Matt went to earn to give and became one of our largest early donors, while I went to run 80,000 Hours. Matt also ended up seed funding several other new charities. So alternatively, we could have both earned to give, in which case 80,000 Hours would have never existed, or we could have both gone and worked for 80,000 Hours, in which case it would have been much harder to fundraise early on, so we would have grown more slowly. And those other charities wouldn't have&nbsp;benefited.</p><p>So then thinking about the community as a whole, there are going to be some people who are like Matt, and they're relatively best suited to earning money. We can have a greater impact as a group if those people earn to give and then fund everyone else to do direct work. In these ways, by working together, we all have the potential to achieve far more if we work together as a group then we could&nbsp;alone.</p><p>But I don't think we actually do work together maybe as effectively as we could a lot of the time. Effective Altruism encourages us to ask what individual actions are highest impact. Some critiques of the community has suggested this could make us biased. This is perhaps the most well known critique of this kind from the London Review of Books: \"There is a small paradox in the growth of effective altruism as a movement when it is so profoundly individualistic. The tacit assumption is that the individual, not the community, class or state, is the proper object of moral&nbsp;theorising.\"</p><p>Now Professor McMahan at Oxford had a response to this article which I agree with. He said \"I am neither a community nor a state. I can determine only what I will do, not what my community or state will do, though I can of course, decide to concentrate my individual efforts on changing my state's institutions.\" I think that's absolutely right, in the sense I'm an individual, so ultimately all I have control over is what my individual actions are and what their impact is going to be. But I also think there is truth in the criticism in the London Review of Books, that although maybe individual actions are ultimately what matters, we still have to be wary of the biases in that perspective. If we ask the question of which individual actions are highest impact, then it might cause us to overlook individual actions that would actually have a greater&nbsp;impact.</p><p>So in particular, I often see people in the community taking what I call a narrow single player analysis and making a narrow single player analysis of their options, not fully factoring in the relevant counterfactuals, not thinking about how the community will adjust to their actions. While this might have worked when we didn't really have a community, nowadays it doesn't work so well. Instead we need to adopt what I call a multi-player perspective. That breaks down into these two things, which I'm going to cover over the rest of the talk. Firstly, we need new rules of thumb for choosing between our actions. And secondly, there are new options that become worth considering when there's a community involved. These will just be some rough ideas from our latest&nbsp;research.</p><p>So firstly, how to choose between our options? I'm going to consider just a single question: should I work at a charity in the community, like Against Malaria Foundation, Givewell, CEA, or so on. Say Amy is considering taking a job in the community. What will her impact be? Now the na\u00efve view of that might be, well the job is high impact so if Amy takes the job, then Amy will have a big impact. But then you hear about effective altruism and someone says \"Ah, but if you don't take the job then Bob will take it instead.\" Therefore Amy won't have much impact. In fact, it would only be worth Amy taking the job if she was going to be much better at it than&nbsp;Bob.</p><p>Okay. So I call that the simple analysis of replaceability, and it's an example of single player style of thinking. It leads to a lot of people thinking they shouldn't do direct work but instead should earn to give. But I think this is wrong, and I apologize because it's partly our fault for talking loosely about the simple analysis of replaceability back in the early days of 80,000 Hours. Today I want to try and stamp it out. So the first problem is you might not actually be replaced. There's a chance the charity just wouldn't hire anyone otherwise. In fact, this often seems to be true. When we talk to charities in the community, they often have positions they've been trying to fill for a while but they haven't been able to&nbsp;fill.</p><p>One reason for this, one cause that's driving this is that there are donors who want to support the community with money on the sidelines. That means if you're a charity that finds someone who's worth hiring, then you can just fundraise extra money to hire that person. This creates a situation where you have threshold hiring, where anyone who's above a bar can get hired because you just raise more money to cover that person. So then all those people are not replaceable. In fact, I think there's a bunch of other ways you can end up not being replaceable, such as through supply/demand effects, which we cover&nbsp;elsewhere.</p><p>This actually means that you can end up being pretty valuable to the organization that you're working at. One way to estimate how much impact you might have at the organization is to ask them to make this trade off. You say \"I could either work for you or I could donate X dollars per year to you. At what value of X would you be indifferent between these, roughly?\" That's just a way of gauging the relative impact of you working there. We actually did this with 12 organizations in the community last summer and these were some of the figures that came&nbsp;out.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/2pyvNbqhaIQUi866WWMQy4/c73cb59f1df7c70f8140c0dbd95ebc74/Value_of_hires.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/2pyvNbqhaIQUi866WWMQy4/c73cb59f1df7c70f8140c0dbd95ebc74/Value_of_hires.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/2pyvNbqhaIQUi866WWMQy4/c73cb59f1df7c70f8140c0dbd95ebc74/Value_of_hires.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/2pyvNbqhaIQUi866WWMQy4/c73cb59f1df7c70f8140c0dbd95ebc74/Value_of_hires.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/2pyvNbqhaIQUi866WWMQy4/c73cb59f1df7c70f8140c0dbd95ebc74/Value_of_hires.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/2pyvNbqhaIQUi866WWMQy4/c73cb59f1df7c70f8140c0dbd95ebc74/Value_of_hires.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/2pyvNbqhaIQUi866WWMQy4/c73cb59f1df7c70f8140c0dbd95ebc74/Value_of_hires.png?w=1800&amp;q=70 1800w\"></figure><p>This is all pretty rough. There's also reason to think that the organizations might be biased upwards. I think it at least suggests that because these figures are much more than most people would be able to donate, these people are having a greater impact than they would through earning to&nbsp;give.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/3BJ8vG2Eyk28ys6QqwoKcY/20faf5e38a817cce4b8000252b04b29e/Talent_vs_funding_constraint.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/3BJ8vG2Eyk28ys6QqwoKcY/20faf5e38a817cce4b8000252b04b29e/Talent_vs_funding_constraint.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/3BJ8vG2Eyk28ys6QqwoKcY/20faf5e38a817cce4b8000252b04b29e/Talent_vs_funding_constraint.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/3BJ8vG2Eyk28ys6QqwoKcY/20faf5e38a817cce4b8000252b04b29e/Talent_vs_funding_constraint.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/3BJ8vG2Eyk28ys6QqwoKcY/20faf5e38a817cce4b8000252b04b29e/Talent_vs_funding_constraint.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/3BJ8vG2Eyk28ys6QqwoKcY/20faf5e38a817cce4b8000252b04b29e/Talent_vs_funding_constraint.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/3BJ8vG2Eyk28ys6QqwoKcY/20faf5e38a817cce4b8000252b04b29e/Talent_vs_funding_constraint.png?w=1800&amp;q=70 1800w\"></figure><p>We also just asked the organizations how talent versus funding constrained they think they are. You can see there's a clear tilt towards being talent constrained. The interesting exception is the animal charities, which report being more funding constrained relative to the&nbsp;others.</p><p>So the first problem with the simple view of replaceability is that you might not actually be replaced. The second problem is where the community really comes in and that's what I call spillovers. Suppose Amy takes job one and she would have been replaced, so someone else would have taken the job anyway. But by taking job one, that means that Amy actually then frees up someone else to go and do job two. Now if you were just considering a job that would be filled by someone who doesn't really care about social impact anyway, and would just go and do maybe some random job in the corporate sector, which you don't think has much impact, then you can safely ignore the&nbsp;spillover.</p><p>But in the current community, it's not so obvious that you can ignore it. Probably this person, Bob, would also be concerned by social impact and would go and do some other high impact thing. They might earn to give. They might work at another charity in the community. This is actually then a significant component of the impact. In fact, I think this kind of case is not even a hypothetical example. I've actually seen cases where someone didn't take job one because they thought they'd be replaceable. That meant that someone actually had to be pulled out of job two to go and take job one instead, where job two was a similarly important role as job&nbsp;one.</p><p>Taking into account these spillovers, then what is the impact of taking a job? How do we analyze it? I think this is actually still an unsolved problem, but here's a sketch of our current thinking. Basically, if Amy takes job one than she frees up someone else to do job two, who then frees up someone else to do job three. It causes a chain of replacements. Where does the chain end? There are two main endings it could have. One is that someone goes into a job that wouldn't have been filled otherwise, a threshold hiring situation and then it stops. Or it can keep going until you hit the marginal opportunity, so the best job that wouldn't have been taken otherwise. That means that at worst, even if Amy was fully replaceable, she'd be adding someone to the margin of the community, which would still be a significant impact. This shows that the simple analysis of replaceability is actually underestimating impact in both of the cases we've&nbsp;covered.</p><p>So the first point is knocking someone out into the margin. Secondly, hopefully you're enabling all these people to switch into jobs they're a slightly better fit for, because they'll be choosing jobs that are slightly better. Amy's taking job one because she's a better fit for that one, and then that's freeing up Bob to go and do something else that's a better fit for him. So potentially there's another benefit as well, that the community gets to a better allocation&nbsp;overall.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/20FL6voCQoKWO60eE2SQMC/c7c327ab3c24ebe8f0a15af4d40779a7/Ideal_community_allocation.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/20FL6voCQoKWO60eE2SQMC/c7c327ab3c24ebe8f0a15af4d40779a7/Ideal_community_allocation.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/20FL6voCQoKWO60eE2SQMC/c7c327ab3c24ebe8f0a15af4d40779a7/Ideal_community_allocation.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/20FL6voCQoKWO60eE2SQMC/c7c327ab3c24ebe8f0a15af4d40779a7/Ideal_community_allocation.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/20FL6voCQoKWO60eE2SQMC/c7c327ab3c24ebe8f0a15af4d40779a7/Ideal_community_allocation.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/20FL6voCQoKWO60eE2SQMC/c7c327ab3c24ebe8f0a15af4d40779a7/Ideal_community_allocation.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/20FL6voCQoKWO60eE2SQMC/c7c327ab3c24ebe8f0a15af4d40779a7/Ideal_community_allocation.png?w=1800&amp;q=70 1800w\"></figure><p>I think if we zoom out a bit, when you're working in a community the picture is more like this one, where we have a pool of thousands of roles and thousands of people. From the community's perspective, you want to match the people into the ideal allocation over those roles. From an individual point of view, the question then becomes what can I do to move the community towards that ideal allocation? That becomes the highest impact role for me. I think the key concept here is comparative advantage compared to the rest of the community. Comparative advantage can be a bit counterintuitive so I'm just going to explore that in a little bit more&nbsp;depth.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/2VaFSS0qXYoeuesqAMoSuO/7ba4cc4d36bef28c51d8af38439efd8a/Comparative_advantage.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/2VaFSS0qXYoeuesqAMoSuO/7ba4cc4d36bef28c51d8af38439efd8a/Comparative_advantage.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/2VaFSS0qXYoeuesqAMoSuO/7ba4cc4d36bef28c51d8af38439efd8a/Comparative_advantage.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/2VaFSS0qXYoeuesqAMoSuO/7ba4cc4d36bef28c51d8af38439efd8a/Comparative_advantage.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/2VaFSS0qXYoeuesqAMoSuO/7ba4cc4d36bef28c51d8af38439efd8a/Comparative_advantage.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/2VaFSS0qXYoeuesqAMoSuO/7ba4cc4d36bef28c51d8af38439efd8a/Comparative_advantage.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/2VaFSS0qXYoeuesqAMoSuO/7ba4cc4d36bef28c51d8af38439efd8a/Comparative_advantage.png?w=1800&amp;q=70 1800w\"></figure><p>Suppose the community needs two more roles at the margin. It needs someone doing research and it needs someone doing outreach. If Charlie did research then he would have two units of impact, whereas if he did outreach, he'd have one unit of impact. Meanwhile Dora would have ten and one, respectively. So then the question is what is the best allocation? There are two possibilities and the best one is this one, where Dora does research and Charlie does outreach, because ten plus one is bigger than two plus two. But what's weird about this example is that Charlie is actually worse at outreach than he is at research. He's also worse at outreach than Dora is at outreach. So in no sense does he have an absolute advantage, nor does he have good personal fit with outreach. But nevertheless, it's the highest impact thing for him to do. That's because he has a comparative advantage in outreach. Basically, he is relatively less bad at outreach then Dora&nbsp;is.</p><p>As we saw, the simple analysis of replaceability encourages you to only do a job if you're better than the person who would have replaced you in that job. However here's an example where you should do something that you're actually worse at because then it enables the community to achieve the best overall allocation. I actually have a suspicion that an example like this might even be a real example. Lots of people in the community reason, well I'm good at analytical things, so I probably should do research. But that doesn't actually follow. What actually matters is how good you are at research compared to the other people who might do research and&nbsp;outreach.</p><p>If we have lots of good analytical people but few outreach people, then even though you might be good at research compared to people in general, you might have a comparative advantage in outreach. Something similar seems maybe true in operations roles as compared to research roles, and may be also with earning to give. People sometimes say \"Well, I have high earning potential so therefore I should earn to give.\" But again that doesn't follow. What actually matters is your earning potential relative to others who might do direct work. If it's high relative to them, then you should earn to give. But if the other people doing direct work would also have high earning potential, then you might still have a comparative advantage in direct work. Unless of course, you're Chuck Norris, who has a comparative advantage in&nbsp;everything.</p><p>How can you find your comparative advantage in real cases? I think basically it boils down to asking people in charge of hiring at the organizations about your relative strengths and&nbsp;weaknesses.</p><p>So summing up. How do you analyze the impact of taking a job? Firstly, you probably cause some boost to the organization. You're not fully replaceable. Then you can try and ask them about it or estimate it yourself by asking them to trade donations against you working there. Secondly, you cause some spillover effect potentially to the rest of the community. And then the question is, is that role above the bar for the community as a whole and does it play to your comparative advantage? Are you getting the community to a better overall&nbsp;allocation?</p><p>And I think similar considerations apply to donating as well. I sometimes see people saying \"Well, I shouldn't donate to charity one because someone else will donate there instead, and so I'll be replaceable. I won't actually have much impact.\" But then I think a whole kind of similar analysis applies. So firstly, you probably won't be fully replaceable. Not all the money would have been given to that charity otherwise, especially if you consider over time. Maybe the money would have been given otherwise, but with a delay, which would slow down the charity. And then secondly, even if it is fully replaced, you're still then just freeing up some other donor in the community to go and donate somewhere else, which would probably be somewhere reasonably high impact as&nbsp;well.</p><p>There's a lot more to say about that and there's a bit more detail in this article online called <a href=\"https://80000hours.org/2016/02/the-value-of-coordination/\">The Value of Coordination</a>. That was all about the new rules of thumb for choosing between your options. But being part of a community also changes your career by opening up new options that might not be on the table if you were just thinking from a single player point of view. The EA mindset however, with a focus on individual actions, can lead us to neglect paths that allow us to have an impact through helping others do more, because the impact is less salient. But now that there are thousands of Effective Altruists at least, the highest impact option for you could well be one that just involves helping others have more impact, boosting&nbsp;others.</p><p>So I'm just going to give five examples. These aren't exhaustive or exclusive, but they're just some ideas. So first one is five minute favors. We all have different strengths and weaknesses, knowledge and resources, and now that there are thousands of people involved, there are probably lots of small ways that we can help others have much more impact at very little cost to ourselves. I call those five minute favors, which is just a term from Adam Grant. That means these kinds of things are really worth looking for. Do you know a job that needs filling? There's a good chance that there's someone in this room or at this conference who would be a good fit for that job. If you could make an introduction, that might only take you an hour, but it would help them for years. Or there's probably someone in this room who has a problem that you know how to solve. You know someone who's solved it before. You know a book that could help them solve it and so&nbsp;on.</p><p>Second example, Operations roles in general. Kyle moved to Oxford and ended up becoming Nick Bostrom's assistant. He thought if I can save Nick Bostrom some time, then that will enable more research and outreach to be done, which I think could actually be really high impact. Instead, people often feel like these operations roles are replaceable by someone from outside of the community. But actually because you have to make lots of little decisions in these roles, that require quite a good understanding of the aims of the organization, they're often very hard to outsource and hard to&nbsp;replace.</p><p>Third example. Community infrastructure becomes much more valuable the larger the community. So for instance, having a job board isn't really needed when there's just a hundred people, but when there's thousands of people, it can play quite a useful role, which is why we added one to the 80,000 Hours site a few months ago. By community infrastructure, I mean anything that helps make the community coordinate more efficiently, such as this event, or it could even mean stuff like setting up good norms of communication that make it easier to work together, like always stating the evidence you\u2019ve used, or just being nice. If you can help a thousand people have one percent more impact, then that's like having the impact of ten people. On the other hand, it means if you do something destructive, then you ruin it for everyone&nbsp;else.</p><p>Fourth example is sharing knowledge with the community. The more people there are in the community, the more worthwhile it is to do research into what the community should do, and then share it with everyone else, because then there's just more people who can act on the findings, so that the value of new information is higher. An example of this is just writing up reports in areas where we have special knowledge of. These are some examples recently from the Effective Altruism&nbsp;forum.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/5qVZEw6qaW6M2MgE2ykYqK/17bf23818216867f3639ae02fca7073d/Writing_up_what_we_know.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/5qVZEw6qaW6M2MgE2ykYqK/17bf23818216867f3639ae02fca7073d/Writing_up_what_we_know.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/5qVZEw6qaW6M2MgE2ykYqK/17bf23818216867f3639ae02fca7073d/Writing_up_what_we_know.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/5qVZEw6qaW6M2MgE2ykYqK/17bf23818216867f3639ae02fca7073d/Writing_up_what_we_know.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/5qVZEw6qaW6M2MgE2ykYqK/17bf23818216867f3639ae02fca7073d/Writing_up_what_we_know.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/5qVZEw6qaW6M2MgE2ykYqK/17bf23818216867f3639ae02fca7073d/Writing_up_what_we_know.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/5qVZEw6qaW6M2MgE2ykYqK/17bf23818216867f3639ae02fca7073d/Writing_up_what_we_know.png?w=1800&amp;q=70 1800w\"></figure><p>For instance, Lee Sharkey is a public health consultant to the World Health Organization and he wrote up some ideas for a new cause area, which is increasing access to pain relief in the Balkan Countries. It can also mean that it's sometimes worth going and learning about areas that don't seem like the highest priority but just might turn out to be. In a smaller community this exploration wouldn't be worth the time, but as we become larger it becomes more and more worth&nbsp;it.</p><p>The fifth and final example is specialization. If the community were just a couple of people, then we'd all need to become generalists, but in a community of 10,000 people, then we can all become experts in our individual areas, and therefore be more than 10,000 times as productive as an individual. This is just the division of labor like we mentioned right at the start with the software&nbsp;example.</p><p>For instance, Dr. Greg Lewis did our research into how many lives a doctor saves. This convinced him that his greatest impact wouldn't come through his direct clinical practice. Instead he decided to go and do a Masters in public health. Part of the reason for that is because it's an important area for the community, especially around pandemics, but there's a lack of people with that skill set. Greg actually thinks that AI risk might be a higher priority in general, but as a doctor he has a comparative advantage in doing public health. Right now I and many others think that one of the greatest weaknesses of the community is a lack of specialist expertise and knowledge. We're all pretty young and inexperienced. These are just some areas that I'd like to highlight but I'm not going to go over right&nbsp;now.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/28XgAvR0RKkuKqyqGwaaO0/191e5c590c1c9ca99c2933b88956b848/Knowledge_and_skills_gaps.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/28XgAvR0RKkuKqyqGwaaO0/191e5c590c1c9ca99c2933b88956b848/Knowledge_and_skills_gaps.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/28XgAvR0RKkuKqyqGwaaO0/191e5c590c1c9ca99c2933b88956b848/Knowledge_and_skills_gaps.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/28XgAvR0RKkuKqyqGwaaO0/191e5c590c1c9ca99c2933b88956b848/Knowledge_and_skills_gaps.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/28XgAvR0RKkuKqyqGwaaO0/191e5c590c1c9ca99c2933b88956b848/Knowledge_and_skills_gaps.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/28XgAvR0RKkuKqyqGwaaO0/191e5c590c1c9ca99c2933b88956b848/Knowledge_and_skills_gaps.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/28XgAvR0RKkuKqyqGwaaO0/191e5c590c1c9ca99c2933b88956b848/Knowledge_and_skills_gaps.png?w=1800&amp;q=70 1800w\"></figure><p>Summing up everything. When choosing whether to take a job or donate somewhere, don't just assume that you're replaceable. Rather, ask the organization about how big the boost would be or try and estimate it yourself. Consider the trade off between donations and working there. Secondly, consider the spillover benefits and whether the role plays to your comparative advantage. Then look for new options that become available. Doing five minute favors. Operations roles in general. Setting up better community infrastructure. Gaining and sharing knowledge with the rest of the community. And&nbsp;specialization.</p><p>We still have a lot to learn about how best to work together. I think there's a lot more we could do, but I really believe that if we do work together effectively then in our lifetimes we can make a major impact on reducing catastrophic risks, eliminating factory farming, reducing global poverty, and many other&nbsp;issues.</p><p>Thanks for listening.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "nCumkmGp7ra3mGscY", "title": "Will MacAskill: Effective altruism in 2017", "postedAt": "2017-06-02T08:48:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=HOWb3ycqFa0&amp;list=PLwp9xeoX5p8Pi7rm-vJnaJ4AQdkYJOfYL&amp;index=2\"><div><iframe src=\"https://www.youtube.com/embed/HOWb3ycqFa0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this opening talk for EA Global: Boston 2017, Will McAskill celebrates some big wins achieved by effective altruism during 2016-2017, makes recommendations for effective altruism as a movement, and makes the original announcement for EA Grants.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "r4uYo6KqxDSvEDxc6", "title": "Effective Altruism in Government", "postedAt": "2017-06-03T07:00:00.000Z", "htmlBody": "<p><i>In this 2017 talk, </i><a href=\"https://www.iarpa.gov/\"><i>IARPA</i></a><i> director Jason Matheny talks about how effective altruists can have an impact through entering government. He draws widely from his own experience in the US government. At the very end you will find an added section on what you can do to&nbsp;help.</i></p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=g05om2NJwco&amp;feature=emb_logo\"><div><iframe src=\"https://www.youtube.com/embed/g05om2NJwco\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p><i>The below transcript is lightly edited for&nbsp;readability.</i></p><p>I don't have any slides just because they're much harder to get through government review, which will be a big part of my talk today on what to do and not to do in government. It's great to see so many young people who are thinking about career options. I feel like I'm close to dying now, so there's only so much good remaining I can do, unless I'm cryonically preserved and resuscitated later. You all have so many decades left to do an enormous amount of good with your lives, so thank you for spending at least 50 minutes of that life to think about options in&nbsp;government.</p><p>I'm going to describe a few different paths to doing good within government that don't rely on spending an entire career in government. It might be a two-year stint, or a five-year stint, but still allowing you to accomplish a huge amount of&nbsp;good.</p><p>I'm going to talk about three things. The first is how I came into government. The second is the roles for EAs working within government. The third is some practical advice on picking jobs within government, whether short-term or&nbsp;longer-term.</p><p>My own route was sort of circuitous in that I started working in college, planning to become an architect. Shortly after I graduated, I found an orphan copy of the <a href=\"https://openknowledge.worldbank.org/handle/10986/5976\">1993 World Bank World Development Report</a>, which both dates me and dates a lot of my perspective on problems of altruism. The report was really focused on how you can do cost-effectiveness analysis on health and development. For me, this was the first time I was ever exposed to an argument about how cheaply you can save lives and significantly reduce suffering. This was one of the first reports that looked at the cost per <a href=\"https://en.wikipedia.org/wiki/Disability-adjusted_life_year\">DALY</a> averted for a range of different health interventions. I thought this application of cost-effectiveness analysis to health and development was pretty eye-opening. For one, it showed that you could save millions of live for less than about $1,000 per life. But it also showed that there were tremendous differences between the most and the least cost-effective interventions, so that our decisions about what to invest in have a huge&nbsp;impact.</p><p>I ended up deciding not become an architect, or else I'd be giving a talk on effective altruism in architecture. I decided to instead go into global health, and worked for several years on global infectious diseases - especially malaria, tuberculosis, and HIV in South Asia, but also in East Asia. It was in 2002 that I made another career shift. This was the same year that the first virus was synthesized from scratch, <i>de novo</i>. It turned biology into something more closely approximating computer science, or an engineering discipline. You could now take the chemical constituents of DNA and create a new&nbsp;genome.</p><p>At that time, the technology was fairly primitive. The longest virus that you could assemble was polio. That wasn't too much to worry about. We already had vaccines for polio. We knew how to control polio. But I and most of the people who I had worked with had worried that somebody would apply this technology to recreate smallpox - to recreate the <a href=\"https://en.wikipedia.org/wiki/1918_flu_pandemic\">1918 influenza</a> that killed over 50 million people in one year. Or, they could make something much worse than any naturally occurring virus, since there are limits to nature's ingenuity that might be outpaced by human&nbsp;ingenuity.</p><p>So I moved from working on naturally occurring global infectious diseases to working on defences against engineered threats. That work then led me to places like <a href=\"https://www.fhi.ox.ac.uk/\">the Future of Humanity Institute</a>, thinking about how we wrestle with risks from emerging technologies. I thought about ways that I could have an impact on this area. One way seemed to be doing research myself, but I didn't think I was especially smart in doing that research. It seemed like there were people who were smarter than me, like Nick Bostrom, who could be doing that research. I thought: how could I deliver more funding to people who were smarter than me? That's why I joined&nbsp;government.</p><p>I came to IARPA to put a multiplier effect on my effort and the effort of other researchers. My goal was to set up a budget in which I could fund important research in a range of areas including risk assessment, technical forecasting, work on biosecurity, nuclear security, cyber security, and assessments of future risks from things like autonomous weapons or AI accidents. The work that I've done at IARPA has convinced me that there's a lot of low-hanging fruit within government positions that we should be picking as effective altruists. There are many different roles that effective altruists can have within government&nbsp;organizations.</p><p>I'm going to tell you about a few of them. I'll limit it to the three areas that I have some background in, which are global health, animal welfare and catastrophic risks. But I think that government is in general a place that has leverage over a variety of different pressing societal issues. Thus if I'm leaving out some topics that's not because I think government doesn\u2019t have influence on those topics. It's just because I'm&nbsp;ignorant.</p><p>In global health, government funding has an enormous impact on the development of new vaccines, antivirals, and antibiotics, as well as other therapies. Much of that work is conducted at the <a href=\"https://www.niaid.nih.gov/\">National Institute for Allergies and Infectious Diseases</a>, that funds basic and applied research in infectious diseases, as well as the development of new therapies. The <a href=\"https://www.fda.gov/\">FDA</a> has an important role determining which kinds of drugs and vaccines are ultimately introduced. Despite being a regulatory agency, they make really interesting innovations, incentive systems, and reward systems, including things like priority review vouchers that can accelerate the introduction of new vaccines for important diseases. And there are places like <a href=\"https://www.fic.nih.gov/Pages/Default.aspx\">the Fogarty Center at the National Institutes of Health</a>, which does things like economic analysis of international health interventions. Then there are the more operational arms of the US government, like the <a href=\"https://www.cdc.gov/\">Centers for Disease Control</a> and <a href=\"https://www.usaid.gov/\">the US Agency for International Development</a>, that have direct impacts on health and development&nbsp;overseas.</p><p>In animal welfare, <a href=\"https://www.usda.gov/\">the USDA</a> has an important role to play in establishing policies that govern some treatments of animals. They also have a small research budget, some of which is used - and a larger amount of which could be used - to develop better, healthier alternatives. State legislatures also have a big role to play in animal welfare as they can pass laws surrounding animal handling and&nbsp;slaughter.</p><p>Then on catastrophic risks where I've spent most of my time, governments have a very significant role. A lot of those risks are influenced by government decisions, both positively and negatively. Multiple organizations work on preventing nuclear war, biological warfare, or accidents, as well as cyber warfare, and the misuse of various emerging technologies. I\u2019ll go through a few of the most important of those organizations. First you have <a href=\"https://en.wikipedia.org/wiki/United_States_National_Security_Council\">the National Security Council</a>, a part of the White House that informs national security decision making, including decisions about war. Another is <a href=\"https://www.whitehouse.gov/ostp\">the White House Office of Science and Technology Policy</a>, which looks at emerging technologies and risks associated with them. That office has groups that examine a range of important technologies, including AI, bio-technology, and neuroscience among other&nbsp;topics.</p><p>Within <a href=\"https://www.defense.gov/\">the Department of Defense</a>, there is <a href=\"https://en.wikipedia.org/wiki/Office_of_Net_Assessment\">the Office of Net Assessment</a>, which in my view is one of the most unusual organizations within government, as well as one of the most important. It looks at long-range security issues that could be decades in the making. For example, what changes in future weapon systems are likely to disrupt deterrents? What would be the consequences of strategic miscalculation with nuclear&nbsp;weapons?</p><p><a href=\"http://www.dtra.mil/\">The Defense Threat Reduction Agency</a> within the Department of Defense is the lead agency responsible for countering chemical, biological, radiological, and nuclear weapons. <a href=\"https://www.fema.gov/\">The Federal Emergency Management Agency</a> is responsible for considering worst-case scenarios that could affect the US and developing mitigations against them. Then there are many others that deal with very specific threats such as the <a href=\"http://www.stratcom.mil/\">Defense Department's Strategic Command</a>, which is responsible for the United States\u2019s nuclear weapons and their safety. <a href=\"https://www.phe.gov/about/BARDA/Pages/default.aspx\">BARDA</a>, which is a part of <a href=\"https://www.hhs.gov/\">Health and Human Services</a>, is responsible for developing medical countermeasures against bioterrorism. The intelligence agencies like <a href=\"https://www.cia.gov/\">CIA</a> and <a href=\"http://www.dia.mil/\">DIA</a>, assess how advanced a particular group's biological weapons program is, or their ability to access disruptive technologies, or the likelihood of industrial accidents, for example in foreign biology&nbsp;labs.</p><p>Across all three of the EA topics that I mentioned, in global health, in animal welfare, and in catastrophic risks, one cost-effective route to having an impact is to affect the funding of new technologies that could in some ways obviate the need for certain kinds of harmful technologies or reduce the risks of technologies by making sure they're sufficiently protected through safety engineering. For funding scientific and technological research, there are a few important organizations within government. There's the <a href=\"https://www.whitehouse.gov/omb\">White House Office of Management and Budget</a>, which helps to set the White House budget requests. Often we'll find even fairly junior folks who are putting their weight on multi-billion dollar decisions. It really is extraordinary that even fairly junior positions can have incredible influence. If you think of this just in terms of an expected value calculation, even a 10% probability of affecting a $10 billion decision means a billion dollars in expectation, and can be hugely consequential on topics such as nuclear safety, biological safety, future of autonomous weapons and so&nbsp;forth.</p><p>There are <a href=\"https://appropriations.house.gov/\">the Congressional Appropriations Committees</a> (link to <a href=\"https://www.appropriations.senate.gov/\">senate page</a>) that approve the budgets from the White House. Here, too, you find even fairly junior staffers that have an incredible impact. Then there are the organizations that take the budgets that they've been given and freely decide how to allocate them. Those include places like <a href=\"https://www.nih.gov/\">the National Institutes of Health</a>, <a href=\"https://www.nsf.gov/\">the National Science Foundation</a>, as well as the ARPAs - the intelligence <a href=\"https://www.iarpa.gov/\">IARPA</a> where I work, <a href=\"https://www.darpa.mil/\">DARPA</a>, <a href=\"https://www.dhs.gov/science-and-technology/hsarpa\">HSARPA</a> and <a href=\"https://arpa-e.energy.gov/\">ARPA-E</a>. At those organizations the program managers, who are typically in their 30s say, have come out of graduate school programs and a science or engineering discipline. They've spent a few years working in a lab, sometimes within academia or within industry. Then they spend a term-limited time in the government, usually not exceeding five years. They're given an extraordinary amount of latitude. They're given a budget of several tens of millions of dollars with the expectation, the trust, that they will invest that money as cost-effectively as possible in solving a particular technical problem. For IARPA, those problems are often associated with reducing the risks of emerging&nbsp;technologies.</p><p>As one example, we have a brilliant biologist at IARPA, John Julias, who runs a program called <a href=\"https://www.iarpa.gov/index.php/research-programs/fun-gcat\">Fun GCAT</a>, which is focused on developing new systems for screening the sequences that go into DNA synthesizers. Can you determine whether this is a safe sequence or a dangerous sequence? That's the kind of work that we really need program managers to do, and we've entrusted John with a +$50 million budget which he uses to fund work here at Harvard, at MIT, and at many other universities and companies in order to advance this goal of reducing risks from synthetic biology. It's much more money I think than at least I could've expected to earn in my lifetime, but we give it, we entrust it, to a program manager to spend as wisely as possible - with the rigor of spending a quarter of that money on testing and evaluation to figure out whether the investments that we're making are actually making a difference, and whether we can accurately assess the risks from, say, a novel sequence. Those are program&nbsp;managers.</p><p>Agency directors can further direct hundreds of millions or even billions of dollars to key projects. Again, even if those are only, say, 10% as effective as funding that would be given outside of a bureaucracy, the expected value of those investments is quite large and can have a dramatic impact. Within government, there are also other levers that one can pull. At IARPA, we've not only been able to erect a large budget on reducing catastrophic risks, but we've also been able to engage in policy discussions. We've led groups within the White House on the long-term impacts of AI and of biotechnology. We co-led the White House AI R&amp;D strategy, and we've advised the National Security Council on other emerging technologies. There are some decisions that are made only by governments, and some of those decisions are highly consequential. They include decisions like going to war, or what weapon systems will be fielded, or how technologies will be embedded within larger critical systems. It makes sense to engage more effective altruists within these positions where they can influence those&nbsp;decisions.</p><p>One can also have influence on the outside, working as a contractor within a government agency. Most of the people who work at IARPA are contractors rather than government employees. The amount of expertise that we have to draw on is too vast to hire them all ourselves directly, especially with short-term positions. So we hire computer scientists, and biologists, and chemists, and physicists, and neuroscientists, and sociologists, political scientists, and cognitive psychologists, because we need them all. We also need lawyers, and we even need philosophers. We have a program on applied philosophy called <a href=\"https://www.iarpa.gov/index.php/research-programs/create\">CREATE</a>, which is a program to develop new systems for argumentation and informal reasoning that can lead intelligence analysts to make better&nbsp;judgements.</p><p>So we need lots of help. We need them from lots of places including contractors, but also think-tanks. There are a range of think-tanks that inform the policy-making process that sometimes have a quite deep influence on administration. For instance, <a href=\"http://www.heritage.org/\">the Heritage Foundation</a> has a substantial influence on the current administration, while past administrations have been influenced by other think tanks such as <a href=\"https://www.brookings.edu/\">Brookings</a>, <a href=\"https://www.cnas.org/\">the Center for a New American Security</a>, <a href=\"https://www.csis.org/\">the Center for Strategic and International Studies</a>, and Harvard's own <a href=\"https://www.belfercenter.org/\">Belfer Center</a>. There are many others that help shape the decision-making of government leaders. Hence, that's another way you can have an influence on&nbsp;government.</p><p>I'm going to close in my final few minutes just by providing some general advice if you're interested in pursuing a job, whether short-term or long-term, within government. I would recommend really thinking about opportunities to move across and among the different sectors of society, government, industry, academia, and NGOs, because there's a need for horizontal transfer of knowledge and best practices. If all of our folks within government have come from government straight out of school, that will prevent us from being able to adopt best practices from industry or from academia. So there really is a need I think for continuous cycling throughout a career, bouncing around between the different sectors in order to bring knowledge across&nbsp;them.</p><p>My first suggestion is to reach out to <a href=\"https://80000hours.org/\">80,000 Hours</a>, which I think has been pulling together some advice about government jobs (see below). I think one of the pieces of advice is at least to consider it as an option, because we are nowhere near the saturation point of effective altruists going into government positions. There are fairly junior positions across government that have a high potential impact that we have trouble recruiting&nbsp;for.</p><p>My second suggestion is to get to know the people who work within the organizations where you'd like to work. You can learn a lot about those organizations, their structure, and their staff just from online websites as well as Wikipedia. You can find the biographies of some of the people whose careers you might want to mimic. One strategy is just to reverse engineer their biography. Figure out what the steps are that seem critical in getting to the positions that you would like to have in the future. On that point, many of these people are lawyers, but just as many of them are scientists and engineers, and we do need more philosophers in government as well. But I think you'll find the diversity of talent that we need is ever growing. There's a particular intersection between policy and technology that is extremely difficult to recruit for. So for people who are still picking their major or their concentration for a thesis, if you look at the science policy of blank, pretty much any of the topics that are critical on our list have not been saturated with attention. There's still lots of low-hanging fruit to&nbsp;pick.</p><p>My last suggestion is to reach out to me, especially if you're interested in pursuing a job, short-term or long-term, in national security or reducing global catastrophic risks. Mostly because I really need the help. That's&nbsp;it.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "zFiJHemx4DXubDvSA", "title": "Does Effective Altruism Lead to the Altruistic Repugnant Conclusion?", "postedAt": "2017-07-27T20:49:06.425Z", "htmlBody": "<html><body><p>Gianfranco Pellegrino has written an interesting <a href=\"http://commons.pacificu.edu/cgi/viewcontent.cgi?article=1579&amp;context=eip\">essay</a> arguing that effective altruism leads to what he calls the Altruistic Repugnant Conclusion. In this post, I will provide a brief version of his&#xA0;argument and then note&#xA0;one possible response.</p>\n<p><strong>The Argument</strong></p>\n<p>Pellegrino beings by identifying the following as the core tenet of effective altruism:</p>\n<p>&quot;Effective Altruist Maximization (AM): We ought to do the most good we can, maximizing the impact of donating to charities on the margin and counterfactually &#x2014;which means that among the available charities, the one that is most effective on the margin should be chosen&quot; (2).</p>\n<p>He next argues that&#xA0;this core tenet can best be articulated as the following principle:</p>\n<p>&quot;Doing the most good amounts to bringing about the greatest benefit to the greatest number&quot; with &quot;gains in diffusion compensat[ing] for losses in size, and vice versa&quot;&#xA0;(7, 9).</p>\n<p>He then&#xA0;poses a hypothetical in which an altruist is offered a choice.* The altruist can:</p>\n<p>&quot;[1] provide consistent, full nutrition and health care to 100 people, such . . . that instead of growing up malnourished they spend their 40-years long lives relatively healthy; [or]</p>\n<p>[2] prevent[] one case of relatively mild non-fatal malaria [say, a fever that lasts a few days] for [each of] 1 billion people, without having a significant impact on the rest of their lives&quot; (14).</p>\n<p>Pellegrino argues that choosing the second option (the Altruistic Repugnant Conclusion) is a &quot;necessary consequence&quot; of the principle&#xA0;from above, but that &quot;[b]ringing about very tiny, but immensely diffused, benefits instead [of] less diffused, but more substantial, benefits is seriously wrong&quot; (15).</p>\n<p>Based on this, he claims&#xA0;that &quot;either effective altruists should&#xA0;accept [the Altruistic Repugnant Conclusion], thereby swallowing its repugnance, or they should give up their core tenet [of Effective Altruist Maximization]&quot; (20-21).</p>\n<p>You can read Pellegrino&apos;s full essay <a href=\"http://commons.pacificu.edu/cgi/viewcontent.cgi?article=1579&amp;context=eip\">here</a>.</p>\n<p><strong>A Possible Response</strong></p>\n<p>As Pellegrino acknowledges, &quot;EA has often been the target of criticisms historically pressed against standard Utilitarianism[,] [and his] paper [is] no exception&quot; (21). In light of this, one way to respond to his argument is to borrow from responses to other critiques of effective altruism that are premised on effective altruism accepting&#xA0;utilitarianism.&#xA0;</p>\n<p>Specifically, <a href=\"https://assets.contentful.com/es8pp29e1wp8/4C7WHsxZLWeaQgiIksMS0y/73404620d8c355f94f7d4d1130e967e8/Gabriel_published_14_April_3.pdf#page=7\">one could argue</a>&#xA0;that &quot;[Pellegrino&apos;s]&#xA0;arguments appeal only to hypothetical (rather than actual) cases in which there is a supposed conflict between effective altruist recommendations and [intuition]&#xA0;and thus fail to show that effective altruist recommendations actually do [lead to a repugnant conclusion].&quot;&#xA0;</p>\n<p><em>Feel free to share&#xA0;other responses to Pellegrino&apos;s argument.&#xA0;</em></p>\n<p>*Pellegrino&apos;s hypothetical is based on <a href=\"http://blog.givewell.org/2008/07/28/significant-life-change/\">a similar hypothetical</a> posed by Holden Karnofsky. In both Karnofsky&apos;s hypothetical and Pellegrino&apos;s hypothetical, there are three options. I have limited the hypothetical to two options for the sake of simplicity.&#xA0;</p></body></html>", "user": {"username": "RandomEA"}}, {"_id": "ugdPy7r54vgcG4fKp", "title": "EAGx Relaunch", "postedAt": "2017-07-23T06:12:19.625Z", "htmlBody": "<html><body><p><span>For those of you who have been following,</span><a href=\"https://www.eaglobal.org/eagx/\"> <span>Effective Altruism Global X (EAGx)</span></a><span> &#x2014; the locally organized international conference series of the Effective Altruism community &#x2014; has been relatively inactive since late last year. This post is an update on why it had a hiatus and how it has changed.</span></p>\n<p>&#xA0;</p>\n<p><span>For full information about EAGx in its current form &#x2014; including application information, and publicly-accessible resources for EA event organizers &#x2014; check out our extended</span><a href=\"https://www.eaglobal.org/eagx-organizer/\"> <span>EAGx Organizer FAQ</span></a><span>.</span></p>\n<h1><span>Why did it take so long for this to go live?</span></h1>\n<p><span>We&#x2019;d like to apologize once again for the long delays in republishing the EAGx organizer application. We&#x2019;ve had some shifting of staff within the organization that has left the project rather lacking in staff time. Additionally, the feedback we received from organizers and attendees engendered some skepticism about the cost-benefit ratio of EAGx relative to other things we could encourage people to do. Rather than push forward with a project of uncertain benefit, we waited until we had the time to more properly consider our options and pivot our strategy accordingly.</span></p>\n<h1><span>What&#x2019;s changed from last year?</span></h1>\n<p><span>Many things!</span></p>\n<h2><span>We&#x2019;re imposing less structured event oversight.</span></h2>\n<p><span>One of the biggest pains both on the organizers&#x2019; end and on our own was getting approval and access to the relevant files, accounts, and funding to make their events happen according to our (former) organizer agreement. While it was there for a reason &#x2014; we wanted to help events to be good, and had some resources to try to help with that &#x2014; it often ended up becoming more prohibitive than useful. So we&#x2019;re offering resources, but leaving it to the organizers to make use of those resources or their own arrangements as makes sense to them. If you&#x2019;re accepted as an EAGx event it&#x2019;s because, beyond thinking it would be great for the event to happen, we trust your motives, discretion, and operating ability. We want the resources we offer to be provided by demand, rather than imposed by assumption of demand.</span></p>\n<h2><span>We&#x2019;re offering more flexibility and dependability of funding.</span></h2>\n<p><span>We found that the funding needs of EAGx events were all over the board, from self-funded, free-venue affairs to expensive full-fledged productions. (This isn&#x2019;t a comment on the competency of the organizers; some environments are just far more amenable to cheap events than others.) Given the range of organizers&#x2019; funding needs, we want to offer the opportunity to apply for funding that makes sense for their circumstances. We&#x2019;re offering US$5000 upon acceptance, to be used at the organizers&#x2019; discretion, as well as </span><a href=\"https://goo.gl/soAHQk\"><span>straightforward funding criteria</span></a><span> so you know what to expect.</span></p>\n<h2><span>We&#x2019;ve standardized the conference structure.</span></h2>\n<p><span>Historically the conferences have varied quite a bit depending on the context and aims of the event organizers. Events targeting students were afternoon- or day-long affairs, comprised of e.g. an introductory talk, 80,000 Hours workshop, and call to action in a lecture hall or community hub. We focused on conveying the core principles and inciting interest in them and our associated community. While we continue to think that this is an important function, CEA believes that, at least at the moment, our efforts to improve the world are bottlenecked by our ability to help promising people become fully engaged, rather than attracting new interest.</span></p>\n<p>&#xA0;</p>\n<p><span>To focus on this aim and make the brand less confusing, EAGx events are whole-weekend events. They generally take place in established communities and focus on building concrete projects or forwarding the field. Events of other structures such as large speaker events, workshops, and salons no longer fall under this title, although they are more often the structure of event we recommend to local groups. </span></p>\n<p>&#xA0;</p>\n<p><span>We also used to offer a &#x201C;themed&#x201D; conference option &#x2014; that is, allow conferences to use the EAGx brand while focusing on some specific topic within EA &#x2014; but are now refocusing EAGx events on EA more generally. We may sponsor themed events but not under this brand.</span></p>\n<h2><span>We&#x2019;re accepting fewer and more-developed organizing groups.</span></h2>\n<p><span>We think that there are </span><span>particular circumstances</span><span> in which EAGx events are highly effective means of engagement, but that many circumstances benefit more from shorter events of a different structure. We have come to believe that we should focus our time on increasing the engagement of people who are highly promising, rather than growing the number of people who are loosely aware of EA. Given this shift in strategy, EAGx events are to be closer to EA Global events in scale and caliber, aimed at engaging particularly skilled current or potential EAs.</span></p>\n<p>&#xA0;</p>\n<p><span>This is also operationally useful; restricting EAGx to just a few of the most established groups should prevent organizers from getting too overwhelmed, help participants know what to expect, and let CEA use its resources where they will go the farthest.</span></p>\n<p>&#xA0;</p>\n<p><span>We are therefore expecting to have no more than half a dozen EAGx conferences over the next twelve months, contrasting the dozen we had over the past year. This events will look more like those by the </span><a href=\"https://www.eaglobal.org/events/eagx-oxford-2016/\"><span>Oxford</span></a><span>, </span><a href=\"https://www.facebook.com/events/963638127114712/\"><span>Berlin</span></a><span>, and </span><a href=\"https://www.eaglobal.org/events/eagxaustralia/\"><span>Australia</span></a><span> groups than like other previous EAGx events.</span></p>\n<h2><span>We&#x2019;re offering stipends to primary organizers.</span></h2>\n<p><span>Running an EAGx conference is a big undertaking, to which the primary organizers have generously donated many hours. Where this conflicts with time spent on work or school, this can be a costly endeavor. To support the people who make these events possible we&#x2019;re offering stipends of &#xA3;1200 (US$1500) per primary organizer for up to three organizers.* If possible, we hope this will help organizers free up their time and attention by e.g. reducing hours at work or paying for time-saving services.</span></p>\n<p>&#xA0;</p>\n<p><span>* This is based on a high estimate of 85 hours of work, at CEA&#x2019;s contractor rate of &#xA3;14 (US$18) per hour. Organizers may apply for more funding, to be disbursed depending on their skills and needs.</span></p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Roxanne_Heston"}}, {"_id": "38ZpdnHMcJG9vHKJf", "title": "Towards a measure of Autonomy and what it means for EA", "postedAt": "2017-07-21T10:35:11.952Z", "htmlBody": "<html><body><p>Autonomy is a human value that is <a href=\"/ea/19s/where_should_antipaternalists_donate/\">referenced in discussions</a> around effective altruism. However I have not seen any attempts to formalise autonomy so we can try and discern the impacts our decisions will have on autonomy in the future.</p>\n<p>&#xA0;</p>\n<p>&#xA0;*Epistemic Status: Exploratory*</p>\n<p>&#xA0;&#xA0;</p>\n<p>In this article I shall introduce a relatively formal measure of autonomy, based on the intuition that it is the ability to do things by yourself with what you have. The measure introduced allows you to move from less to more autonomy, without being black and white about it. Then I shall talk about how increasing autonomy fits in with the values of movements such as poverty reduction, ai risk reduction and the reduction of suffering.</p>\n<p>&#xA0;</p>\n<p>Autonomy is not naturally encouraged the capitalist system due to the incentives involved. So if we wish for a future with increased autonomy we need to think about it and how best to promote it.</p>\n<p>&#xA0;</p>\n<h2>Making autonomy an explicit value&#xA0;</h2>\n<p>&#xA0;</p>\n<p>Part of Effective altruism is finding shared human values that we can work together towards. &#xA0;Whether it is existential risk reduction or the reduction of suffering, our endeavours are underpinned by our shared values.</p>\n<p>&#xA0;</p>\n<p>Autonomy is one of those values. It has not been I made fully explicit and so I think aspects of it have been neglected by the effective altruism community. With that in mind I want to propose a way of measuring autonomy to spark further discussion.&#xA0;</p>\n<p>&#xA0;</p>\n<p>Autonomy is valuable in many value systems that do not make it a primary value, as it allows you to exist outside the dominant economic and political systems. There are lots of reasons you might want to do so, these include:</p>\n<p>&#xA0;</p>\n<ul>\n<li>The larger system is fragile and you want insulate parts of it&#xA0;from catastrophic failure in other parts of the system (reducing existential risk, by making the system more resilient to losses of parts of it).</li>\n<li>The larger system has no need for you. For example if you are slowly becoming less economically valuable as more jobs are heavily automated. If something like universal basic income is not implemented, then becoming more autonomous might be the only way to survive.</li>\n<li>You disagree with the larger system for moral reasons, for example if it is using slavery or polluting the seas. You may wish to opt out of the larger system in whole or in part so you are not contributing to the activity you disagree with.</li>\n<li>The larger system is hostile to you. It is an authoritarian or racist government. &#xA0;There are plenty examples of this happening in history, so it will probably happen again.</li>\n<li>You wish to go somewhere outside the dominant system, for example to live in space.</li>\n</ul>\n<p>&#xA0;</p>\n<h2>Concepts around Autonomy</h2>\n<p>&#xA0;</p>\n<p>Autonomy, by my definition, is the ability to do a thing by yourself. An example of something you can (probably) do autonomously is open a door. You need no-ones help to walk over and manipulate the door in such a way that it opens. Not all everyday activities are so simple, things become more complicated when you talk about switching on a light. You can throw the light switch by yourself, but you still rely on there being an electricity grid maintained by humans (unless you happen to be off-grid), for the light to come on. You do not make the light go on by yourself. The other agents in the process are hidden from you and know nothing of your actions with regards to the light switch (apart from a very small blip in energy usage), but they are still required for the light to go on. So you cannot turn on the light autonomously.&#xA0;</p>\n<p>&#xA0;</p>\n<p>A mental concept useful in talking about autonomy is the capability footprint of an activity. The capability footprint is basically the volume of physical space required to do an activity. If that footprint to contains other agents (or actors maintained by another agent) then you are not autonomous in that activity. If there are agents involved but there are sufficient numbers and they have an incentive to carry on doing the activity, then you can just treat it like the environment. You only lose autonomy when agents can decide to stop you performing an action. An example of people relying on lots of agents while performing a task that seems autonomous is breathing. We rely on plants to produce the oxygen we need to breathe, but there is little chance that all the plants will one day decide to stop producing oxygen. So we can still be said to breathe autonomously. The free market in its idealised state can be seen to be like that (if one company decides to stop selling you a product, then another one will fill the gap). However in the real world natural monopolies, government regulation, intellectual property and changing economic times might mean that products are no longer available to you. So we are going to assume that no humans can be involved in an autonomous activity.</p>\n<p>&#xA0;</p>\n<p>So for our light switch example the footprint includes the wiring in your house, the wiring of the grid, the people maintaining the grid and the people maintaining the power stations (and the miners/riggers). So you are not autonomous in that activity</p>\n<p>&#xA0;</p>\n<p>Another example is navigation. If you rely on GPS your capability footprint expands to include the satellites in orbit, these are actors from another agent and they may stop maintaining it (or have <a title=\"selective availability of gps\" href=\"http://www.gps.gov/systems/gps/modernization/sa/\">degraded it&#x2019;s performance</a>) on their whim. If you rely on a compass and map, your capability footprint expands to molten iron core of earth, but you are autonomous, at least with regards to this activity, because that does not rely on another agent.&#xA0;</p>\n<p>&#xA0;</p>\n<p>Trying to make individual activities autonomous is not very interesting. For example being able autonomously navigate does not insulate yourself from catastrophe if you cannot produce your own food. So we need the concept of the important activities in your life. Those things that will sustain you and give your life meaning. These will be the vital&#xA0;set of activities. We can get an idea of how much you rely on others by taking each non-autonomous capability footprint of the activities in the vital&#xA0;set and creating a union of them to get a non-autonomous&#xA0;vital&#xA0;footprint. &#xA0;This is something we can try to minimise, the larger your vital&#xA0;footprint the more easily your essential activities can be disrupted and the more agents you rely upon. But it doesn&#x2019;t capture everything people value. People, myself included, choose to use google rather than setting up their own mail servers. So we need to look at what is inside each vital&#xA0;set to get to the reason why.</p>\n<p>&#xA0;</p>\n<p>Some vital capability sets allow you to do more things than others, you can achieve a very small footprint if you adopt stone age technology but the number of activities you can do is limited. Being able to do more things is better as we can be more adaptable, so we need a measure that captures that. The vital&#xA0;capability set has a size, the number of activities you can perform, so we can divide that by the footprint to get the vital&#xA0;capability density. This measure captures both intuitions that doing more things is good, and doing things that are less spread out and intermingled with other people is good.</p>\n<p>&#xA0;</p>\n<p>The history of human development has been of increasing both the vital capability&#xA0;footprint and the size of the vital capability&#xA0;set. So the vital capability density has probably been going up (these things are somewhat hard to measure, it is easy to see direction of change less easy to measure magnitudes of both things). So the current economic and political system seems very good at increasing the vital capability&#xA0;set. So there is little need for us to do work in that direction. But the expansion of the vital capability footprint has been going in the wrong direction and seems set to keep going in that direction. This is due to it not being incentivised by our current system.&#xA0;</p>\n<p>&#xA0;</p>\n<p>Companies are incentivised to try and keep control of their products and revenue streams so that they can get a return on their investment and stay solvent. Trade is the heart of capitalism. This might mean moving towards a larger vital capability footprint. You can see this in the transition to&#xA0;Software as a Service from&#xA0;shrinkwrapped&#xA0;software that you own. There are some products that makes moves towards shrinking the footprint, things such as solar panels. However what you really need to be independent is the ability to manufacture and recycle solar panels for yourself, else you are only energy independent for the life time of those panels.</p>\n<p>&#xA0;</p>\n<p>The only people likely to work on technologies that reduce&#xA0;the vital&#xA0;capability footprint are the military and the space industry, neither of which will necessarily democratize the technology or have incentives to make things long term autonomous.</p>\n<p>&#xA0;</p>\n<p>So there is potential work here to improve the human condition, that would not get done otherwise. We can try and help people to shrink their&#xA0;vital&#xA0;footprint while maintaining or expanding the vital capability set with the goal of allowing each human to increase their vital&#xA0;capability density over the long term. This is what I will mean when I talk about increasing humanities autonomy. &#xA0;</p>\n<p>&#xA0;</p>\n<h2>What is to be done?</h2>\n<p>&#xA0;</p>\n<p>To increase humanities autonomy successfully we will need to figure out how&#xA0;to prevent any negative aspects&#xA0;of making more independent capable people and to create&#xA0;advanced technologies that do not exist today.</p>\n<p>&#xA0;</p>\n<p>It has been hypothesised by Pinker that the increased interdependence of our society is what has led to <a href=\"https://en.wikipedia.org/wiki/Long_Peace\">the long peace.</a> We rely on other people for things necessary for our livelihood so that we do not want to disrupt their business as that disrupts our lives is how the story goes. We would lose that mechanism, if it is important. &#xA0;There is also the risks of giving people increased ability, they might do things by accident that have large negative consequences for other people&#x2019;s lives, such as releasing deadly viruses. So we need to make sure we have found ways to mitigate these scenarios, so increased autonomy does not lead to more chaos and strife.</p>\n<p>&#xA0;</p>\n<p>This is more pertinent when you consider what technologies are needed to reduce the vital&#xA0;footprint.The most important one is intelligence augmentation. Currently our economy is as partially as distributed as it is, because of the complexity of dealing with all the myriad things we create and how to create them. People and companies specialise in doing a few things and doing them well because it reduces the complexity they need to manage. So to reduce the size of the vital&#xA0;footprint you need to be able to get people able to do more things. Which means increasing their ability to manage complexity, which means intelligence augmentation. What exactly this&#xA0;looks like is not known at this time. Initiatives like link <a href=\"https://www.neuralink.com/\">neuralink</a>&#xA0;seem like part of the solution.&#xA0;We would also need to have computers we actually would want to interface with, ones that are more resistant to subversion and less reliant on human maintenance. We need to deal with issues of alignment of these systems with our goals, so they are not external agents (reducing our autonomy) and also the issues around potential intelligence explosions. I am working on these questions, but more people would be welcome.</p>\n<p>&#xA0;</p>\n<p>Making us reliant on some piece of computer hardware that we cannot make ourselves would not decrease our vital footprint. So we need to be able to make them ourselves. Factories and recycling facilities are vast so we would need to shrink these too. There are hobbyist movements already for decentralising some manufacturing, things like 3d printing, but other manufacturing is still heavily centralised like solar panels construction, metal smelting&#xA0;and chip manufacturing. We do not have any current obvious pathways to decentralisation for these things. You also want to make make sure everything is <a href=\"http://wwwf.imperial.ac.uk/blog/cepresearch/2013/01/31/the-closed-loop-or-circular-economy/\">fully recyclable</a>&#xA0;as much as possible. If not you increase your vital&#xA0;footprint to include both the mines and also the places to dispose of your rubbish.&#xA0;</p>\n<p>&#xA0;</p>\n<h2>Other EA views and autonomy</h2>\n<p>&#xA0;</p>\n<p>I don&#x2019;t take increasing autonomy as the only human value, but it is interesting to think how it might interact with other goals of the effective altruist community by itself. Each of these probably deserves an essay, but a brief sketch will have to do for now.</p>\n<p>&#xA0;</p>\n<h4>AIrisk</h4>\n<p>&#xA0;</p>\n<p>The autonomy view vastly prefers a certain outcome to the airisk question. It is not in favour of creating a single AI that looks after us all (especially not by uploading), but prefers the outcome where everyone is augmented and we create the future together. However if decisive strategic advantages are possible and human will definitely seek them out or create agents that do so, then creating an AI to save us from that fate may be preferable. But trying to find a way that does not involve that is a high priority of the autonomy view.&#xA0;</p>\n<p>&#xA0;</p>\n<h4>Poverty reduction</h4>\n<p>&#xA0;</p>\n<p>This can be seen as bringing everyone up to a more similar set of vital activities. So it is not in conflict. The autonomy view of decreasing the footprint points at something to do even when everyone is at an equal vital set. Aiming for that might be something currently neglected which could have a large impact. For example the a-fore mentioned human operated self replicating solar cell factory could have a large impact in Africa&apos;s development. Also trying to reduce poverty by getting people involved in a global economic system, which seems to have less need of people in the future, may not be the most effective long term strategy.</p>\n<p>&#xA0;</p>\n<h4>Suffering reduction</h4>\n<p>&#xA0;</p>\n<p>Increasing every human&#x2019;s vital&#xA0;set to be similar should allow everyone to do the activities needed to avoid the same suffering. So in this regard it is compatible.</p>\n<p>&#xA0;</p>\n<p>However my view of autonomy is not currently universal, in that I am not trying to increase the autonomy of animals in the same way as I want to increase humanities. I&#x2019;m not sure what it would look like to try and give hens the same autonomy as humans. This in part is because I rely on people choosing more autonomy and I&#x2019;m not sure how I could communicate that choice to a hen. It is also that humans are currently not very autonomous so the work to help them seems enormous. Perhaps the circle of moral concerm&#xA0;will increase as autonomy becomes easier.</p>\n<p>&#xA0;</p>\n<h2>In conclusion</h2>\n<p>&#xA0;</p>\n<p>I hope I have given you an interesting view of autonomy. I mainly hope to spark a discussion of what it means to be autonomous. I look forward to other people&#x2019;s views on whether I have captured the aspects of autonomy important to them.</p>\n<p>&#xA0;</p>\n<p>Thanks to my partner great philosophical discussions about this concept with me and for someone from an EA meetup in London who saw that I was mainly talking about autonomy and inspired me to try and be explicit about what I cared about.</p></body></html>", "user": {"username": "WillPearson"}}, {"_id": "FfJ4rMTJAB3tnY5De", "title": "Why I think the Foundational Research Institute should rethink its approach", "postedAt": "2017-07-20T20:46:27.298Z", "htmlBody": "<html><body><p><span>The following is my considered evaluation of the Foundational Research Institute, circa July 2017. I discuss its goal, where I foresee things going wrong with how it defines suffering, and what it could do to avoid these problems.</span></p>\n<p><span><em>TL;DR version</em>: functionalism (&quot;consciousness is the sum-total of the functional properties of our brains&quot;) sounds a lot better than it actually turns out to be in practice. In particular, functionalism makes it impossible to define ethics &amp; suffering in a way that can mediate disagreements.</span></p>\n<p>&#xA0;</p>\n<p><span>I. What is the Foundational Research Institute?</span></p>\n<p>&#xA0;</p>\n<p><span>The Foundational Research Institute (FRI) is a Berlin-based group that &quot;conducts research on how to best reduce the suffering of sentient beings in the near and far future.&quot; Executive Director Max Daniel </span><a href=\"https://www.youtube.com/watch?v=jiZxEJcFExc\"><span>introduced them</span></a><span> at EA Global Boston as &#x201C;the only EA organization which at an organizational level has the mission of focusing on reducing s-risk.&#x201D; S-risks are, according to Daniel, &#x201C;risks where an adverse outcome would bring about suffering on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far.&#x201D;</span></p>\n<p>&#xA0;</p>\n<p><span>Essentially, FRI wants to become the research arm of suffering-focused ethics, and help prevent artificial general intelligence (AGI) failure-modes which might produce suffering on a cosmic scale.</span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p><span>What I like about FRI:</span></p>\n<p><span>While I have serious qualms about FRI&#x2019;s research framework, I think the people behind FRI deserve a lot of credit- they seem to be serious people, working hard to build something good. In particular, I want to give them a shoutout for three things:</span></p>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><span>First, FRI takes suffering seriously, and I think that&#x2019;s important. When times are good, we tend to forget how tongue-chewingly horrific suffering can be. S-risks seem particularly horrifying.</span></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><span>Second, FRI isn&#x2019;t afraid of being weird. FRI has been working on s-risk research for a few years now, and if people are starting to come around to the idea that s-risks are worth thinking about, much of the credit goes to FRI.</span></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><span>Third, I have great personal respect for Brian Tomasik, one of FRI&#x2019;s co-founders. I&#x2019;ve found him highly thoughtful, generous in debates, and unfailingly principled. In particular, he&#x2019;s always willing to bite the bullet and work ideas out to their logical end, even if it involves repugnant conclusions. </span></p>\n</li>\n</ul>\n<p><strong>&#xA0;</strong></p>\n<p><span>What is FRI&#x2019;s research framework?</span></p>\n<p><span>FRI believes in </span><a href=\"http://www.iep.utm.edu/functism/\"><span>analytic functionalism</span></a><span>, or what David Chalmers calls &#x201C;Type-A materialism&#x201D;. Essentially, what this means is there&#x2019;s no &#x2019;theoretical essence&#x2019; to consciousness; rather, consciousness is the sum-total of the functional properties of our brains. Since &#x2018;functional properties&#x2019; are rather vague, this means consciousness </span><span>itself</span><span> is rather vague, in the same way words like &#x201C;life,&#x201D; &#x201C;justice,&#x201D; and &#x201C;virtue&#x201D; are messy and vague. </span></p>\n<p>&#xA0;</p>\n<p><span>Brian</span><a href=\"http://reducing-suffering.org/hard-problem-consciousness/\"> <span>suggests</span></a><span> that this vagueness means there&#x2019;s an inherently subjective, perhaps arbitrary element to how we define consciousness:</span></p>\n<blockquote>\n<p><span>Analytic functionalism looks for functional processes in the brain that roughly capture what we mean by words like &quot;awareness&quot;, &quot;happy&quot;, etc., in a similar way as a biologist may look for precise properties of replicators that roughly capture what we mean by &quot;life&quot;. Just as there can be room for fuzziness about where exactly to draw the boundaries around &quot;life&quot;, different analytic functionalists may have different opinions about where to define the boundaries of &quot;consciousness&quot; and other mental states. This is why consciousness is &quot;up to us to define&quot;. There&apos;s no hard problem of consciousness for the same reason there&apos;s no hard problem of life: consciousness is just a high-level word that we use to refer to lots of detailed processes, and it doesn&apos;t mean anything </span><span>in addition</span><span> to those processes.</span></p>\n</blockquote>\n<p>&#xA0;</p>\n<p><span>Finally, Brian argues that the phenomenology of consciousness is identical with the</span><a href=\"https://foundational-research.org/flavors-of-computation-are-flavors-of-consciousness/\"> <span>phenomenology of computation</span></a><span>:</span></p>\n<blockquote>\n<p><span>I know that I&apos;m conscious. I also know, from neuroscience combined with Occam&apos;s razor, that my consciousness consists only of material operations in my brain -- probably mostly patterns of neuronal firing that help process inputs, compute intermediate ideas, and produce behavioral outputs. Thus, I can see that consciousness is just the first-person view of certain kinds of computations -- as Eliezer Yudkowsky puts it, &quot;</span><a href=\"http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/\"><span>How An Algorithm Feels From Inside</span></a><span>&quot;. Consciousness is not something separate from or epiphenomenal to these computations. It </span><span>is</span><span> these computations, just from their own perspective of trying to think about themselves.</span></p>\n<p>&#xA0;</p>\n<p><span>In other words, </span><a href=\"http://www.utilitarian-essays.com/boundaries-of-consciousness.html\"><span>consciousness is what minds compute</span></a><span>. Consciousness is the collection of input operations, intermediate processing, and output behaviors that an entity performs.</span></p>\n</blockquote>\n<p><span>And if consciousness is all these things, so too is suffering. Which means suffering is </span><span>computational</span><span>, yet also inherently fuzzy, and at least a bit arbitrary; a leaky high-level reification impossible to speak about accurately, since there&#x2019;s no formal, objective &#x201C;ground truth&#x201D;.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>II. Why do I worry about FRI&#x2019;s research framework?</span></p>\n<p>&#xA0;</p>\n<p><span>In short, I think FRI has a worthy goal and good people, but its metaphysics </span><span>actively prevent</span><span> making progress toward that goal. The following describes </span><span>why</span><span> I think that, drawing heavily on Brian&#x2019;s writings (of FRI&#x2019;s researchers, Brian seems the most focused on metaphysics):</span></p>\n<p>&#xA0;</p>\n<p><span>Note: FRI is not the only EA organization which holds functionalist views on consciousness; much of the following critique would also apply to e.g. MIRI, FHI, and OpenPhil. I focus on FRI because (1) Brian&#x2019;s writings on consciousness &amp; functionalism have been hugely influential in the community, and are clear enough *to* criticize; (2) the fact that FRI is particularly clear about what it cares about- suffering- allows a particularly clear critique about what problems it will run into with functionalism; (3) I believe FRI is at the forefront of an important cause area which has not crystallized yet, and I think it&#x2019;s critically important to get these objections bouncing around this subcommunity.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Objection 1: Motte-and-bailey</span></p>\n<p><a href=\"http://reducing-suffering.org/dissolving-confusion-about-consciousness/\"><span>Brian</span></a><span>: &#x201C;Consciousness is not a thing which exists &#x2018;out there&#x2019; or even a separate property of matter; it&apos;s a definitional category into which we classify minds. &#x2018;Is this digital mind really conscious?&#x2019; is analogous to &#x2018;Is a rock that people use to eat on really a table?&#x2019; [However,] That consciousness is a </span><a href=\"http://lesswrong.com/lw/nl/the_cluster_structure_of_thingspace/\"><span>cluster in thingspace</span></a><span> rather than a concrete property of the world does not make reducing suffering less important.&#x201D;</span></p>\n<p>&#xA0;</p>\n<p><span>The FRI model seems to imply that suffering is </span><span>ineffable</span><span> enough such that we can&apos;t have an objective definition, yet sufficiently </span><span>effable</span><span> that we can coherently talk and care about it. This attempt to have it both ways seems contradictory, or at least in deep tension. </span></p>\n<p>&#xA0;</p>\n<p><span>Indeed, I&#x2019;d argue that the degree to which you </span><span>can</span><span> care about something is proportional to the degree to which you can define it objectively. E.g., If I say that &#x201C;gnireffus&#x201D; is literally the most terrible thing in the cosmos, that we should spread gnireffus-focused ethics, and that minimizing g-risks (far-future scenarios which involve large amounts of gnireffus) is a moral imperative, but also that what is and what isn&#x2019;t gnireffus is rather subjective with no privileged definition, and it&#x2019;s impossible to </span><span>objectively </span><span>tell if a physical system exhibits gnireffus, you might raise any number of objections. This is not an exact metaphor for FRI&#x2019;s position, but I worry that FRI&#x2019;s work leans on the intuition that suffering </span><span>is</span><span> real and we </span><span>can</span><span> speak coherently about it, to a degree greater than its metaphysics formally allow.</span></p>\n<p>&#xA0;</p>\n<p><span>Max Daniel (personal communication) suggests that we&#x2019;re comfortable with a degree of ineffability in other contexts; &#x201C;Brian claims that the concept of suffering shares the allegedly problematic properties with the concept of a table. But it seems a stretch to say that the alleged tension is problematic when talking about tables. So why would it be problematic when talking about suffering?&#x201D; However, if we take the anti-realist view that suffering is &#x2018;merely&#x2019; a node in the network of language, we have to live with the consequences of this: that &#x2018;suffering&#x2019; will </span><span>lose meaning</span><span> as we take it away from the network in which it&#x2019;s embedded (</span><a href=\"https://en.wikipedia.org/wiki/Philosophical_Investigations\"><span>Wittgenstein</span></a><span>). But FRI wants to do exactly this, to speak about suffering in the context of AGIs, simulated brains, even video game characters. </span></p>\n<p>&#xA0;</p>\n<p><span>We can be anti-realists about suffering (suffering-is-a-node-in-the-network-of-language), or we can argue that we can talk coherently about suffering in novel contexts (AGIs, mind crime, aliens, and so on), but it seems inherently troublesome to claim we can do both at the same time.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Objection 2: Intuition duels</span></p>\n<p><span>Two people can agree on FRI&#x2019;s position that there is no objective fact of the matter about what suffering is (no privileged definition), but this also means they have no way of coming to any consensus on the object-level question of whether something can suffer. This isn&#x2019;t just an academic point: Brian has written extensively about how he believes non-human animals can and do suffer extensively, whereas Yudkowsky (who holds computationalist views, like Brian) has written about how</span><a href=\"https://rationalconspiracy.com/2015/12/16/a-debate-on-animal-consciousness/\"> <span>he&#x2019;s confident that animals are </span><span>not</span><span> conscious and </span><span>cannot</span><span> suffer</span></a><span>, due to their lack of higher-order reasoning.</span></p>\n<p>&#xA0;</p>\n<p><span>And if functionalism is having trouble adjudicating the </span><span>easy</span><span> cases of suffering--whether monkeys can suffer, or whether dogs can&#x2014; it doesn&#x2019;t have a </span><span>sliver of a chance</span><span> at dealing with the upcoming hard cases of suffering: whether a given AGI is suffering, or engaging in</span><a href=\"http://lesswrong.com/lw/l9t/superintelligence_12_malignant_failure_modes/\"> <span>mind crime</span></a><span>; whether a whole-brain emulation (WBE) or synthetic organism or emergent intelligence that doesn&#x2019;t have the capacity to tell us how it feels (or that we don&#x2019;t have the capacity to understand) is suffering; if any aliens that we meet in the future can suffer; whether changing the internal architecture of our qualia reports means we&#x2019;re also changing our qualia; and so on.</span></p>\n<p>&#xA0;</p>\n<p><span>In short, FRI&#x2019;s theory of consciousness </span><span>isn&#x2019;t actually a theory of consciousness</span><span> at all, since </span><span>it doesn&#x2019;t do the thing we need a theory of consciousness to do</span><span>: adjudicate disagreements in a principled way. Instead, it gives up any claim on the sorts of objective facts which could </span><span>in principle</span><span> adjudicate disagreements.</span></p>\n<p>&#xA0;</p>\n<p><span>This is a source of friction in EA today, but it&#x2019;s mitigated by the sense that</span></p>\n<p><span>(1) The EA pie is growing, so it&#x2019;s better to ignore disagreements than pick fights;</span></p>\n<p><span>(2) Disagreements over the definition of suffering don&#x2019;t really matter yet, since we haven&#x2019;t gotten into the business of making morally-relevant synthetic beings (that we know of) that might be unable to vocalize their suffering.</span></p>\n<p><span>If the perception of one or both of these conditions change, the lack of some disagreement-adjudicating theory of suffering will matter </span><span>quite a lot</span><span>.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Objection 3: Convergence requires common truth</span></p>\n<p><a href=\"https://www.facebook.com/groups/effective.altruists/permalink/1333798200009867/?comment_id=1333823816673972&amp;comment_tracking=%7B%22tn%22%3A%22R9%22%7D\"><span>Mike</span></a><span>: &#x201C;[W]hat makes one definition of consciousness better than another? How should we evaluate them?&#x201D;</span></p>\n<p><span>Brian: &#x201C;Consilience among our feelings of empathy, principles of non-discrimination, understandings of cognitive science, etc. It&apos;s similar to the question of what makes one definition of justice or virtue better than another.&#x201D;</span></p>\n<p>&#xA0;</p>\n<p><span>Brian is hoping that affective neuroscience will slowly converge to accurate views on suffering as more and better data about sentience and pain accumulates. But convergence to truth implies something (objective) </span><span>driving</span><span> the convergence- in this way, Brian&#x2019;s framework still seems to </span><span>require an objective truth of the matter</span><span>, even though he disclaims most of the benefits of assuming this.</span></p>\n<p>&#xA0;</p>\n<p><span> &#xA0;</span></p>\n<p><span>Objection 4: Assuming that consciousness is a reification produces </span><span>more</span><span> confusion, not less</span></p>\n<p><a href=\"http://reducing-suffering.org/dissolving-confusion-about-consciousness/\"><span>Brian</span></a><span>: &#x201C;Consciousness is not a reified thing; it&apos;s not a physical property of the universe that just exists intrinsically. Rather, instances of consciousness are algorithms that are implemented in specific steps. &#x2026; Consciousness involves specific things that brains do.&#x201D;</span></p>\n<p>&#xA0;</p>\n<p><span>Brian argues that we treat conscious/phenomenology as more &apos;real&apos; than it is. Traditionally, whenever we&#x2019;ve discovered something is a leaky reification and shouldn&#x2019;t be treated as &#x2018;too real&#x2019;, we&#x2019;ve been able to break it down into more coherent constituent pieces we </span><span>can</span><span> treat as real. Life, for instance, wasn&#x2019;t due to </span><span>&#xE9;lan vital </span><span>but a bundle of self-organizing properties &amp; dynamics which generally co-occur. But carrying out this &#x201C;de-reification&#x201D; process on consciousness-- enumerating its coherent constituent pieces-- has proven difficult, especially if we want to preserve some way to speak cogently about suffering.</span></p>\n<p>&#xA0;</p>\n<p><span>Speaking for myself, the more I stared into the depths of functionalism, the less certain </span><span>everything</span><span> about moral value became-- and arguably, I see the same trajectory in Brian&#x2019;s work and </span><a href=\"http://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood\"><span>Luke Muehlhauser&#x2019;s report</span></a><span>. Their model uncertainty has seemingly become </span><span>larger</span><span> as they&#x2019;ve looked into techniques for how to &#x201C;de-reify&#x201D; consciousness while preserving some flavor of moral value, not </span><span>smaller</span><span>. Brian and Luke seem to interpret this as evidence that moral value is intractably complicated, but this is also consistent with consciousness </span><span>not</span><span> being a reification, and instead being a real thing. Trying to &#x201C;de-reify&#x201D; something that&#x2019;s not a reification will produce deep confusion, just as surely trying to treat a reification as &#x2018;more real&#x2019; than it actually is will.</span></p>\n<p>&#xA0;</p>\n<p><span>Edsger W. Dijkstra famously noted that &#x201C;The purpose of abstraction is not to be vague, but to create a new semantic level in which one can be absolutely precise.&#x201D; And so if our ways of talking about moral value fail to &#x2018;carve reality at the joints&#x2019;- then by all means </span><span>let&#x2019;s build better ones</span><span>, rather than giving up on precision.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Objection 5: The Hard Problem of Consciousness is a red herring</span></p>\n<p><span>Brian spends a lot of time discussing Chalmers&#x2019; &#x201C;Hard Problem of Consciousness&#x201D;, i.e. the question of </span><span>why</span><span> we&#x2019;re subjectively conscious, and seems to base at least part of his conclusion on not finding this question compelling&#x2014; he suggests &#x201C;There&apos;s no hard problem of consciousness for the same reason there&apos;s no hard problem of life: consciousness is just a high-level word that we use to refer to lots of detailed processes, and it doesn&apos;t mean anything </span><span>in addition</span><span> to those processes.&#x201D; I.e., no &#x2018;why&#x2019; is necessary; when we take consciousness and subtract out the details of the brain, we&#x2019;re left with an empty set.</span></p>\n<p>&#xA0;</p>\n<p><span>But I think the &#x201C;Hard Problem&#x201D; isn&#x2019;t helpful as a contrastive centerpiece, since it&#x2019;s unclear what the problem </span><span>is</span><span>, and whether it&#x2019;s analytic or empirical, a statement about cognition or about physics. At the Qualia Research Institute (QRI), we don&#x2019;t talk much about the Hard Problem; instead, we talk about </span><span>Qualia Formalism</span><span>, or the idea that </span><span>any phenomenological state can be crisply and precisely represented by some mathematical object</span><span>. I suspect this would be a better foil for Brian&#x2019;s work than the Hard Problem.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Objection 6: Mapping to reality</span></p>\n<p><span>Brian argues that consciousness should be defined at the functional/computational level: given a Turing machine, or neural network, the right &#x2018;code&#x2019; will produce consciousness. But the problem is that this doesn&#x2019;t lead to a theory which can &#x2018;compile&#x2019; to physics. Consider the following:</span></p>\n<p>&#xA0;</p>\n<p><span>Imagine you have a bag of popcorn. Now shake it. There will exist a certain ad-hoc interpretation of bag-of-popcorn-as-computational-system where you just simulated someone getting tortured, and other interpretations that don&apos;t imply that. Did you torture anyone? If you&apos;re a computationalist, no clear answer exists- you both did, and did not, torture someone. This sounds like a ridiculous edge-case that would never come up in real life, but in reality it comes up </span><span>all the time</span><span>, since there is no principled way to *objectively derive* what computation(s) any physical system is performing.</span></p>\n<p>&#xA0;</p>\n<p><span>I don&#x2019;t think this is an outlandish view of functionalism; Brian suggests much the same in</span><a href=\"http://reducing-suffering.org/interpret-physical-system-mind/\"> <span>How to Interpret a Physical System as a Mind</span></a><span>: </span><span>&#x201C;Physicalist views that directly map from physics to moral value are relatively simple to understand. Functionalism is more complex, because it maps from physics to computations to moral value. Moreover, while physics is real and objective, computations are fictional and &#x2018;observer-relative&#x2019; (to use John Searle&apos;s terminology). There&apos;s no objective meaning to &#x2018;the computation that this physical system is implementing&#x2019; (unless you&apos;re referring to the specific equations of physics that the system is playing out).&#x201D;</span></p>\n<p>&#xA0;</p>\n<p><span>Gordon McCabe (</span><a href=\"http://philsci-archive.pitt.edu/1891/1/UniverseCreationComputer.pdf\"><span>McCabe 2004</span></a><span>) provides a more formal argument to this effect&#x2014; that precisely mapping between physical processes and (Turing-level) computational processes is inherently impossible&#x2014; in the context of simulations. First, McCabe notes that:</span></p>\n<blockquote>\n<p><span>[T]here is a one-[to-]many correspondence between the logical states [of a computer] and the exact electronic states of computer memory. Although there are bijective mappings between numbers and the logical states of computer memory, there are no bijective mappings between numbers and the exact electronic states of memory.</span></p>\n</blockquote>\n<p><span>This lack of an exact bijective mapping means that subjective interpretation necessarily creeps in, and so a computational simulation of a physical system can&#x2019;t be &#x2018;about&#x2019; that system in any </span><span>rigorous</span><span> way: </span></p>\n<blockquote>\n<p><span>In a computer simulation, the values of the physical quantities possessed by the simulated system are represented by the combined states of multiple bits in computer memory. However, the combined states of multiple bits in computer memory only represent numbers because they are deemed to do so under a numeric interpretation. There are many different interpretations of the combined states of multiple bits in computer memory. If the numbers represented by a digital computer are interpretation-dependent, they cannot be objective physical properties. Hence, there can be no objective relationship between the changing pattern of multiple bit-states in computer memory, and the changing pattern of quantity-values of a simulated physical system.</span></p>\n</blockquote>\n<p><span>McCabe concludes that, metaphysically speaking,</span></p>\n<blockquote>\n<p><span>A digital computer simulation of a physical system cannot exist as, (does not possess the properties and relationships of), anything else other than a physical process occurring upon the components of a computer. In the contemporary case of an electronic digital computer, a simulation cannot exist as anything else other than an electronic physical process occurring upon the components and circuitry of a computer.</span></p>\n</blockquote>\n<p>&#xA0;</p>\n<p><span>Where does this leave ethics? In</span><a href=\"https://foundational-research.org/flavors-of-computation-are-flavors-of-consciousness/\"> <span>Flavors of Computation Are Flavors of Consciousness</span></a><span>, Brian notes that</span><span> &#x201C;In some sense all I&apos;ve proposed here is to think of different flavors of computation as being various flavors of consciousness. But this still leaves the question: Which flavors of computation matter most? Clearly whatever computations happen when a person is in pain are vastly more important than what&apos;s happening in a brain on a lazy afternoon. How can we capture that difference?&#x201D;</span></p>\n<p>&#xA0;</p>\n<p><span>But if Brian grants the former point- that &quot;</span><span>There&apos;s no objective meaning to &#x2018;the computation that this physical system is implementing&#x2019;&#x201D;</span><span>- then this latter task of figuring out &#x201C;which flavors of computation matter most&#x201D; is </span><span>provably impossible</span><span>. There will </span><span>always</span><span> be multiple computational (and thus ethical) interpretations of a physical system, with no way to figure out what&#x2019;s &#x201C;really&#x201D; happening. No way to figure out if something is suffering or not. No consilience; not now, not </span><span>ever</span><span>. </span></p>\n<p>&#xA0;</p>\n<p><span>Note: despite apparently granting the point above,</span><a href=\"https://foundational-research.org/flavors-of-computation-are-flavors-of-consciousness/\"> <span>Brian also remarks that</span></a><span>:</span></p>\n<blockquote>\n<p><span>I should add a note on terminology: All computations occur within physics, so any computation is a physical process. Conversely, any physical process proceeds from input conditions to output conditions in a regular manner and so is a computation. Hence, the set of computations equals the set of physical processes, and where I say &quot;computations&#x201D; in this piece, one could just as well substitute &quot;physical processes&quot; instead.</span></p>\n</blockquote>\n<p><span>This seems to be (1) incorrect, for the reasons I give above, or (2) taking substantial poetic license with these terms, or (3) referring to</span><a href=\"https://arxiv.org/abs/math/0209332\"> <span>hypercomputation</span></a><span> (which might be able to salvage the metaphor, but would invalidate many of FRI&#x2019;s conclusions dealing with the computability of suffering on conventional hardware).</span></p>\n<p>&#xA0;</p>\n<p><span>This objection may seem esoteric or pedantic, but I think it&#x2019;s </span><span>important</span><span>, and that it ripples through FRI&#x2019;s theoretical framework with disastrous effects.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Objection 7: FRI doesn&apos;t fully bite the bullet on computationalism</span></p>\n<p><span>Brian suggests that &#x201C;flavors of computation are flavors of consciousness&#x201D; and that some computations &#x2018;code&#x2019; for suffering. But if we do in fact bite the bullet on this metaphor and place suffering within the realm of computational theory, we need to think in &#x201C;near mode&#x201D; and accept all the paradoxes that brings. Scott Aaronson, a noted expert on quantum computing, raises the</span><a href=\"http://www.scottaaronson.com/blog/?p=1951\"> <span>following objections</span></a><span> to functionalism:</span></p>\n<blockquote>\n<p><span>I&#x2019;m guessing that many people in this room side with Dennett, and (not coincidentally, I&#x2019;d say) also with Everett. I certainly have sympathies in that direction too. In fact, I spent seven or eight years of my life as a Dennett/Everett hardcore believer. But, while I don&#x2019;t want to talk anyone out of the Dennett/Everett view, I&#x2019;d like to take you on a tour of what I see as some of the extremely interesting questions that that view leaves unanswered. I&#x2019;m not talking about &#x201C;deep questions of meaning,&#x201D; but about something much more straightforward: </span><span>what exactly does a computational process have to do to qualify as &#x201C;conscious&#x201D;?</span></p>\n<p><span>&#x2026;</span></p>\n<p><span>There&#x2019;s this old chestnut, what if each person on earth simulated one neuron of your brain, by passing pieces of paper around. It took them several years just to simulate a single second of your thought processes. Would </span><span>that</span><span> bring your subjectivity into being? Would you accept it as a replacement for your current body? If so, then what if your brain were simulated, not neuron-by-neuron, but by a gigantic </span><span>lookup table</span><span>? That is, what if there were a huge database, much larger than the observable universe (but let&#x2019;s not worry about that), that hardwired what your brain&#x2019;s response was to every sequence of stimuli that your sense-organs could possibly receive. Would </span><span>that</span><span> bring about your consciousness? Let&#x2019;s keep pushing: if it would, would it make a difference if anyone actually </span><span>consulted</span><span> the lookup table? Why can&#x2019;t it bring about your consciousness just by sitting there doing nothing? </span></p>\n<p>&#xA0;</p>\n<p>To these standard thought experiments, we can add more. Let&#x2019;s suppose that, purely for error-correction purposes, the computer that&#x2019;s simulating your brain runs the code three times, and takes the majority vote of the outcomes. Would that bring three &#x201C;copies&#x201D; of your consciousness into being? Does it make a difference if the three copies are widely separated in space or time&#x2014;say, on different planets, or in different centuries? Is it possible that the massive redundancy taking place in your brain right now is bringing multiple copies of you into being?</p>\n<p><span>...</span></p>\n<p><span>Maybe my favorite thought experiment along these lines was invented by my former student Andy Drucker. &#xA0;In the past five years, there&#x2019;s been a revolution in theoretical cryptography, around something called </span><a href=\"http://en.wikipedia.org/wiki/Homomorphic_encryption#Fully_homomorphic_encryption\"><span>Fully Homomorphic Encryption</span></a><span> (FHE), which was first discovered by Craig Gentry. &#xA0;What FHE lets you do is to perform arbitrary computations on encrypted data, without ever decrypting the data at any point. &#xA0;So, to someone with the decryption key, you could be proving theorems, simulating planetary motions, etc. &#xA0;But to someone without the key, it looks for all the world like you&#x2019;re just shuffling random strings and producing other random strings as output.</span></p>\n<p>&#xA0;</p>\n<p><span>You can probably see where this is going. &#xA0;What if we homomorphically encrypted a simulation of your brain? &#xA0;And what if we hid the only copy of the decryption key, let&#x2019;s say in another galaxy? &#xA0;Would </span><span>this</span><span> computation&#x2014;which looks to anyone in </span><span>our</span><span> galaxy like a reshuffling of gobbledygook&#x2014;be silently producing your consciousness?</span></p>\n<p>&#xA0;</p>\n<p><span>When we consider the possibility of a conscious </span><span>quantum</span><span> computer, in some sense we inherit all the previous puzzles about conscious classical computers, but then also add a few new ones. &#xA0;So, let&#x2019;s say I run a quantum subroutine that simulates your brain, by applying some unitary transformation U. &#xA0;But then, of course, I want to &#x201C;uncompute&#x201D; to get rid of garbage (and thereby enable interference between different branches), so I apply U</span><span>-1</span><span>. &#xA0;Question: when I apply U</span><span>-1</span><span>, does your simulated brain experience the same thoughts and feelings a second time? &#xA0;Is the second experience &#x201C;the same as&#x201D; the first, or does it differ somehow, by virtue of being reversed in time? Or, since U</span><span>-1</span><span>U is just a convoluted implementation of the identity function, are there no experiences at all here?</span></p>\n<p>&#xA0;</p>\n<p><span>Here&#x2019;s a better one: many of you have heard of the </span><a href=\"http://en.wikipedia.org/wiki/Elitzur%E2%80%93Vaidman_bomb_tester\"><span>Vaidman bomb</span></a><span>. &#xA0;This is a famous thought experiment in quantum mechanics where there&#x2019;s a package, and we&#x2019;d like to &#x201C;query&#x201D; it to find out whether it contains a bomb&#x2014;but if we query it and there </span><span>is</span><span> a bomb, it will explode, killing everyone in the room. &#xA0;What&#x2019;s the solution? &#xA0;Well, suppose we could go into a superposition of querying the bomb and not querying it, with only &#x3B5; amplitude on querying the bomb, and &#x221A;(1-&#x3B5;</span><span>2</span><span>) amplitude on not querying it. &#xA0;And suppose we repeat this over and over&#x2014;each time, moving &#x3B5; amplitude onto the &#x201C;query the bomb&#x201D; state if there&#x2019;s no bomb there, but moving &#x3B5;</span><span>2 </span><span>probability</span><span> onto the &#x201C;query the bomb&#x201D; state if there is a bomb (since the explosion decoheres the superposition). &#xA0;Then after 1/&#x3B5; repetitions, we&#x2019;ll have order 1 probability of being in the &#x201C;query the bomb&#x201D; state if there&#x2019;s no bomb. &#xA0;By contrast, if there </span><span>is</span><span> a bomb, then the total probability we&#x2019;ve ever entered that state is (1/&#x3B5;)&#xD7;&#x3B5;</span><span>2</span><span> = &#x3B5;. &#xA0;So, either way, we learn whether there&#x2019;s a bomb, and the probability that we set the bomb off can be made arbitrarily small. &#xA0;(Incidentally, this is extremely closely related to how Grover&#x2019;s algorithm works.)</span></p>\n<p>&#xA0;</p>\n<p><span>OK, now how about the Vaidman brain? &#xA0;We&#x2019;ve got a quantum subroutine simulating your brain, and we want to ask it a yes-or-no question. &#xA0;We do so by querying that subroutine with &#x3B5; amplitude 1/&#x3B5; times, in such a way that </span><span>if</span><span> your answer is &#x201C;yes,&#x201D; then we&#x2019;ve only ever activated the subroutine with total probability &#x3B5;. &#xA0;Yet you still manage to communicate your &#x201C;yes&#x201D; answer to the outside world. &#xA0;So, should we say that you were conscious only in the &#x3B5; fraction of the wavefunction where the simulation happened, or that the entire system was conscious? &#xA0;(The answer could matter a lot for anthropic purposes.)</span></p>\n</blockquote>\n<p>&#xA0;</p>\n<p><span>To sum up: Brian&#x2019;s notion that consciousness is the same as computation raises more issues than it solves; in particular, the possibility that if suffering is computable, it may also be </span><span>uncomputable/reversible</span><span>, would suggest s-risks aren&#x2019;t as serious as FRI treats them.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Objection 8: Dangerous combination</span></p>\n<p><span>Three themes which seem to permeate FRI&#x2019;s research are:</span></p>\n<p><span>(1) Suffering is the thing that is bad.</span></p>\n<p><span>(2) It&#x2019;s critically important to eliminate badness from the universe. </span></p>\n<p><span>(3) Suffering is impossible to define objectively, and so we each must define what suffering means for ourselves.</span></p>\n<p>&#xA0;</p>\n<p><span>Taken individually, each of these seems reasonable. Pick two, and you&#x2019;re still okay. Pick all three, though, and you get </span><span>A Fully General Justification For Anything</span><span>, based on what is ultimately a subjective/aesthetic call.</span></p>\n<p>&#xA0;</p>\n<p><span>Much can be said in FRI&#x2019;s defense here, and it&#x2019;s unfair to single them out as risky: in my experience they&#x2019;ve always brought a very thoughtful, measured, </span><a href=\"https://foundational-research.org/research/#cooperation-and-foresight\"><span>cooperative</span></a><span> approach to the table. I would just note that ideas are powerful, and I think theme (3) is especially pernicious if incorrect.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>III. QRI&#x2019;s alternative</span></p>\n<p>&#xA0;</p>\n<p><span>Analytic functionalism is essentially a negative hypothesis about consciousness: it&apos;s the argument that there&apos;s no order to be found, no rigor to be had. It obscures this with talk of &quot;function&quot;, which is a red herring it not only doesn&apos;t define, but admits is </span><span>undefinable</span><span>. It doesn&apos;t make any positive assertion. Functionalism is skepticism- nothing more, nothing less.</span></p>\n<p>&#xA0;</p>\n<p><span>But is it </span><span>right</span><span>?</span></p>\n<p>&#xA0;</p>\n<p><span>Ultimately, I think these </span><span>a priori</span><span> arguments are much like people in the middle ages arguing whether one could ever formalize a Proper System of Alchemy. Such arguments may in many cases hold water, but it&apos;s often difficult to tell good arguments apart from arguments where we&apos;re just cleverly fooling ourselves. In retrospect, the best way to *prove* systematized alchemy was possible was to just go out and *do* it, and invent Chemistry. That&apos;s how I see what we&apos;re doing at QRI with Qualia Formalism: we&#x2019;re </span><span>assuming it&#x2019;s possible to build stuff</span><span>, and we&#x2019;re working on </span><span>building the object-level stuff</span><span>.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>What we&#x2019;ve built with QRI&#x2019;s framework</span></p>\n<p><em><span>Note: this is a brief, surface-level tour of our research; it will probably be confusing for readers who haven&apos;t dug into our stuff before. Consider this a down-payment on a more substantial introduction.</span></em></p>\n<p>&#xA0;</p>\n<p><span>My most notable work is </span><span><a href=\"http://opentheory.net/PrincipiaQualia.pdf\">Principia Qualia</a></span><span>, in which I lay out my</span><a href=\"http://opentheory.net/wp-content/uploads/2016/11/Eight-Problems2-1.png\"> <span>meta-framework</span></a><span> for consciousness</span><span> (a flavor of dual-aspect monism, with a focus on Qualia Formalism) and put forth the</span><a href=\"http://opentheory.net/2017/04/stov-explain-like-im-5-edition/\"> <span>Symmetry Theory of Valence</span></a>&#xA0;(STV)<span>. Essentially, the STV is an argument that much of the apparent complexity of emotional valence is evolutionarily contingent, and if we consider a mathematical object isomorphic to a phenomenological experience, the mathematical property which corresponds to how pleasant it is to be that experience is the object&#x2019;s </span><span>symmetry</span><span>. This implies a bunch of testable predictions and reinterpretations of things like what &#x2018;pleasure centers&#x2019; do (Section XI; Section XII). Building on this, I offer the</span><a href=\"http://opentheory.net/2017/05/why-we-seek-out-pleasure-the-symmetry-theory-of-homeostatic-regulation/\"> <span>Symmetry Theory of Homeostatic Regulation</span></a><span>, which suggests understanding the structure of qualia will translate into knowledge about the structure of human intelligence, and I briefly touch on the idea of</span><a href=\"http://opentheory.net/2017/06/taking-brain-waves-seriously-neuroacoustics/\"> <span>Neuroacoustics</span></a><span>.</span></p>\n<p>&#xA0;</p>\n<p><span>Likewise, my colleague Andr&#xE9;s Gomez Emilsson has written about the likely mathematics of phenomenology, including</span><a href=\"https://qualiacomputing.com/2017/05/28/eli5-the-hyperbolic-geometry-of-dmt-experiences/\"> <span>The Hyperbolic Geometry of DMT Experiences</span></a><span>,</span><a href=\"https://qualiacomputing.com/2016/11/19/the-tyranny-of-the-intentional-object/\"> <span>Tyranny of the Intentional Object</span></a><span>, and</span><a href=\"https://qualiacomputing.com/2016/06/20/algorithmic-reduction-of-psychedelic-states/\"> <span>Algorithmic Reduction of Psychedelic States</span></a><span>. If I had to suggest one thing to read in all of these links, though, it would be the transcript of his recent talk on</span><a href=\"https://qualiacomputing.com/2017/06/18/quantifying-bliss-talk-summary/\"> <span>Quantifying Bliss</span></a><span>, which lays out the world&#x2019;s first method to objectively measure valence from first principles (via fMRI) using Selen Atasoy&#x2019;s</span><a href=\"https://www.nature.com/articles/ncomms10340\"> <span>Connectome</span></a><a href=\"https://qualiacomputing.com/2017/06/18/connectome-specific-harmonic-waves-on-lsd/\"> <span>Harmonics</span></a><span> framework, the Symmetry Theory of Valence, and Andr&#xE9;s&#x2019;s CDNS model of experience.</span></p>\n<p>&#xA0;</p>\n<p><span>These are risky predictions and we don&#x2019;t yet know if they&#x2019;re right, but we&#x2019;re confident that if there </span><span>is</span><span> some elegant structure intrinsic to consciousness, as there is in many other parts of the natural world, these are the right </span><span>kind</span><span> of risks to take. </span></p>\n<p>&#xA0;</p>\n<p><span>I mention all this because I think analytic functionalism- which is to say radical skepticism/eliminativism, the metaphysics of last resort- only looks as good as it does because nobody&#x2019;s been building out any alternatives.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>IV. Closing thoughts</span></p>\n<p>&#xA0;</p>\n<p><span>FRI is pursuing a certain research agenda, and QRI is pursuing another, and there&#x2019;s lots of value in independent explorations of the nature of suffering. I&#x2019;m glad FRI exists, everybody I&#x2019;ve interacted with at FRI has been great, I&#x2019;m happy they&#x2019;re focusing on s-risks, and I look forward to seeing what they produce in the future.</span></p>\n<p>&#xA0;</p>\n<p><span>On the other hand, I worry that nobody&#x2019;s pushing back on FRI&#x2019;s metaphysics, which seem to unavoidably lead to the intractable problems I describe above. FRI seems to believe these problems are part of the territory, unavoidable messes that we just have to make philosophical peace with. But I think that functionalism is a bad map, that the metaphysical messes it leads to are </span><span>much</span> <span>worse</span><span> than most people realize (</span><span>fatal</span><span> to FRI&#x2019;s mission), and there are other options that avoid these problems (which, to be fair, is not to say they have </span><span>no</span><span> problems).</span></p>\n<p>&#xA0;</p>\n<p><span>Ultimately, FRI doesn&#x2019;t owe me a defense of their position. But if they&#x2019;re open to suggestions on what it would take to convince a skeptic like me that their brand of functionalism is viable, or at least </span><span>rescuable</span><span>, I&#x2019;d offer the following:</span></p>\n<p>&#xA0;</p>\n<p><span>Re: Objection 1 (motte-and-bailey)</span><span>, I suggest FRI should be as clear and complete as possible in their basic definition of suffering. In which </span><span>particular </span><span>ways is it ineffable/fuzzy, and in which particular ways is it precise? What can we definitely say about suffering, and what can we definitely never determine? Preregistering ontological commitments and methodological possibilities would help guard against FRI&#x2019;s definition of suffering changing based on context.</span></p>\n<p>&#xA0;</p>\n<p><span>Re: Objection 2 (intuition duels)</span><span>, FRI may want to internally &#x201C;war game&#x201D; various future scenarios involving AGI, WBE, etc, with one side arguing that a given synthetic (or even extraterrestrial) organism </span><span>is</span><span> suffering, and the other side arguing that it </span><span>isn&#x2019;t</span><span>. I&#x2019;d expect this would help diagnose what sorts of disagreements future theories of suffering will need to adjudicate, and perhaps illuminate implicit ethical intuitions. Sharing the results of these simulated disagreements would also be helpful in making FRI&#x2019;s reasoning less opaque to outsiders, although making </span><span>everything</span><span> transparent could lead to certain strategic disadvantages.</span></p>\n<p>&#xA0;</p>\n<p><span>Re: Objection 3 (convergence requires common truth)</span><span>, I&#x2019;d like FRI to explore </span><span>exactly</span><span> might drive consilience/convergence in theories of suffering, and what </span><span>precisely</span><span> makes one theory of suffering better than another, and ideally to evaluate a range of example theories of suffering under these criteria.</span></p>\n<p>&#xA0;</p>\n<p><span>Re: Objection 4 (assuming that consciousness is a reification produces </span><span>more</span><span> confusion, not less)</span><span>, I would love to see a historical treatment of reification: lists of reifications which were later dissolved (e.g., &#xE9;lan vital), vs scattered phenomena that were later unified (e.g., electromagnetism). What patterns do the former have, vs the latter, and why might consciousness fit one of these buckets better than the other?</span></p>\n<p>&#xA0;</p>\n<p><span>Re: Objection 5 (the Hard Problem of Consciousness is a red herring)</span><span>, I&#x2019;d like to see a more detailed treatment of what </span><span>kinds</span><span> of problem people have interpreted the Hard Problem as, and also more analysis on the prospects of Qualia Formalism (which I think is the maximally-empirical, maximally-charitable interpretation of the Hard Problem). It would be helpful for </span><span>us</span><span>, in particular, if FRI preregistered their expectations about QRI&#x2019;s predictions, and their view of the relative evidence strength of each of our predictions.</span></p>\n<p>&#xA0;</p>\n<p><span>Re: Objection 6 (mapping to reality)</span><span>, this is perhaps the heart of most of our disagreement. From Brian&#x2019;s quotes, he seems split on this issue; I&#x2019;d like clarification about whether he believes we can ever precisely/objectively map specific computations to specific physical systems, and vice-versa. And if so&#x2014; how? If not, this seems to propagate through FRI&#x2019;s ethical framework in a disastrous way, since anyone can argue that </span><span>any</span><span> physical system does, or does not, &#x2018;code&#x2019; for massive suffering, and there&#x2019;s no principled way derive any &#x2018;ground truth&#x2019; or even pick between interpretations in a principled way (e.g. my popcorn example). If this isn&#x2019;t the case&#x2014; why not?</span></p>\n<p>&#xA0;</p>\n<p><span>Brian has suggested that &#x201C;certain high-level interpretations of physical systems are more &#x2018;natural&#x2019; and useful than others&#x201D; (personal communication); I agree, and would encourage FRI to explore systematizing this.</span></p>\n<p>&#xA0;</p>\n<p><span>It would be non-trivial to port FRI&#x2019;s theories and computational intuitions to the framework of &#x201C;</span><a href=\"https://arxiv.org/abs/math/0209332\"><span>hypercomputation</span></a><span>&#x201D;-- i.e., the understanding that there&#x2019;s a formal hierarchy of computational systems, and that Turing machines are only one level of many-- but it may have benefits too. Namely, it might be the only way they could avoid Objection 6 (which I think is a </span><span>fatal</span><span> objection) while still allowing them to speak about computation &amp; consciousness in the same breath. I think FRI should look at this and see if it makes sense to them.</span></p>\n<p>&#xA0;</p>\n<p><span>Re: Objection 7 (FRI doesn&apos;t fully bite the bullet on computationalism)</span><span>, I&#x2019;d like to see responses to Aaronson&#x2019;s aforementioned thought experiments. </span></p>\n<p>&#xA0;</p>\n<p><span>Re: Objection 8 (dangerous combination)</span><span>, I&#x2019;d like to see a clarification about why my interpretation is unreasonable (as it very well may be!).</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>---</span></p>\n<p><span>In conclusion- I think FRI has a critically important goal- reduction of suffering &amp; s-risk. However, I also think FRI has painted itself into a corner by explicitly disallowing a clear, disagreement-mediating definition for what these things </span><span>are</span><span>. I look forward to further work in this field.</span></p>\n<p><span>---</span></p>\n<p>&#xA0;</p>\n<p><span>Mike Johnson</span></p>\n<p><span>Qualia Research Institute</span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p><span>Acknowledgements: thanks to Andr&#xE9;s Gomez Emilsson, Brian Tomasik, and Max Daniel for reviewing earlier drafts of this.</span></p>\n<p>&#xA0;</p>\n<hr>\n<p>&#xA0;</p>\n<p><span>Sources:</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>My sources for FRI&#x2019;s views on consciousness:</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Flavors of Computation are Flavors of Consciousness:</span></p>\n<p><a href=\"https://foundational-research.org/flavors-of-computation-are-flavors-of-consciousness/\"><span>https://foundational-research.org/flavors-of-computation-are-flavors-of-consciousness/</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Is There a Hard Problem of Consciousness?</span></p>\n<p><a href=\"http://reducing-suffering.org/hard-problem-consciousness/\"><span>http://reducing-suffering.org/hard-problem-consciousness/</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Consciousness Is a Process, Not a Moment</span></p>\n<p><span>http://reducing-suffering.org/consciousness-is-a-process-not-a-moment/</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>How to Interpret a Physical System as a Mind</span></p>\n<p><a href=\"http://reducing-suffering.org/interpret-physical-system-mind/\"><span>http://reducing-suffering.org/interpret-physical-system-mind/</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Dissolving Confusion about Consciousness</span></p>\n<p><a href=\"http://reducing-suffering.org/dissolving-confusion-about-consciousness/\"><span>http://reducing-suffering.org/dissolving-confusion-about-consciousness/</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Debate between Brian &amp; Mike on consciousness:</span></p>\n<p><a href=\"https://www.facebook.com/groups/effective.altruists/permalink/1333798200009867/?comment_id=1333823816673972&amp;comment_tracking=%7B%22tn%22%3A%22R9%22%7D\"><span>https://www.facebook.com/groups/effective.altruists/permalink/1333798200009867/?comment_id=1333823816673972&amp;comment_tracking=%7B%22tn%22%3A%22R9%22%7D</span></a></p>\n<p><strong><br><br></strong></p>\n<p><span>Max Daniel&#x2019;s EA Global Boston 2017 talk on s-risks:</span></p>\n<p><a href=\"https://www.youtube.com/watch?v=jiZxEJcFExc\"><span>https://www.youtube.com/watch?v=jiZxEJcFExc</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Multipolar debate between Eliezer Yudkowsky and various rationalists about animal suffering:</span></p>\n<p><a href=\"https://rationalconspiracy.com/2015/12/16/a-debate-on-animal-consciousness/\"><span>https://rationalconspiracy.com/2015/12/16/a-debate-on-animal-consciousness/</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>The Internet Encyclopedia of Philosophy on functionalism:</span></p>\n<p><a href=\"http://www.iep.utm.edu/functism/\"><span>http://www.iep.utm.edu/functism/</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Gordon McCabe on why computation doesn&#x2019;t map to physics:</span></p>\n<p><a href=\"http://philsci-archive.pitt.edu/1891/1/UniverseCreationComputer.pdf\"><span>http://philsci-archive.pitt.edu/1891/1/UniverseCreationComputer.pdf</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Toby Ord on hypercomputation, and how it differs from Turing&#x2019;s work:</span></p>\n<p><a href=\"https://arxiv.org/abs/math/0209332\"><span>https://arxiv.org/abs/math/0209332</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Luke Muehlhauser&#x2019;s OpenPhil-funded report on consciousness and moral patienthood:</span></p>\n<p><a href=\"http://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood\"><span>http://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Scott Aaronson&#x2019;s thought experiments on computationalism:</span></p>\n<p><a href=\"http://www.scottaaronson.com/blog/?p=1951\"><span>http://www.scottaaronson.com/blog/?p=1951</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Selen Atasoy on Connectome Harmonics, a new way to understand brain activity:</span></p>\n<p><a href=\"https://www.nature.com/articles/ncomms10340\"><span>https://www.nature.com/articles/ncomms10340</span></a></p>\n<p><strong><br><br></strong></p>\n<p><span>My work on formalizing phenomenology:</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>My meta-framework for consciousness, including the Symmetry Theory of Valence:</span></p>\n<p><a href=\"http://opentheory.net/PrincipiaQualia.pdf\"><span>http://opentheory.net/PrincipiaQualia.pdf</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>My hypothesis of homeostatic regulation, which touches on why we seek out pleasure:</span></p>\n<p><a href=\"http://opentheory.net/2017/05/why-we-seek-out-pleasure-the-symmetry-theory-of-homeostatic-regulation/\"><span>http://opentheory.net/2017/05/why-we-seek-out-pleasure-the-symmetry-theory-of-homeostatic-regulation/</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>My exploration &amp; parametrization of the &#x2018;neuroacoustics&#x2019; metaphor suggested by Atasoy&#x2019;s work:</span></p>\n<p><a href=\"http://opentheory.net/2017/06/taking-brain-waves-seriously-neuroacoustics/\"><span>http://opentheory.net/2017/06/taking-brain-waves-seriously-neuroacoustics/</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>My colleague Andr&#xE9;s&#x2019;s work on formalizing phenomenology:</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>A model of DMT-trip-as-hyperbolic-experience:</span></p>\n<p><a href=\"https://qualiacomputing.com/2017/05/28/eli5-the-hyperbolic-geometry-of-dmt-experiences/\"><span>https://qualiacomputing.com/2017/05/28/eli5-the-hyperbolic-geometry-of-dmt-experiences/</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>June 2017 talk at Consciousness Hacking, describing a theory and experiment to predict people&#x2019;s valence from fMRI data:</span></p>\n<p><a href=\"https://qualiacomputing.com/2017/06/18/quantifying-bliss-talk-summary/\"><span>https://qualiacomputing.com/2017/06/18/quantifying-bliss-talk-summary/</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>A parametrization of various psychedelic states as operators in qualia space:</span></p>\n<p><a href=\"https://qualiacomputing.com/2016/06/20/algorithmic-reduction-of-psychedelic-states/\"><span>https://qualiacomputing.com/2016/06/20/algorithmic-reduction-of-psychedelic-states/</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>A brief post on valence and the fundamental attribution error:</span></p>\n<p><a href=\"https://qualiacomputing.com/2016/11/19/the-tyranny-of-the-intentional-object/\"><span>https://qualiacomputing.com/2016/11/19/the-tyranny-of-the-intentional-object/</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>A summary of some of Selen Atasoy&#x2019;s current work on Connectome Harmonics:</span></p>\n<p><a href=\"https://qualiacomputing.com/2017/06/18/connectome-specific-harmonic-waves-on-lsd/\"><span>https://qualiacomputing.com/2017/06/18/connectome-specific-harmonic-waves-on-lsd/</span></a></p>\n<p><br><br></p></body></html>", "user": {"username": "MikeJohnson"}}, {"_id": "kNKpyf4WWdKehgvRt", "title": "An Argument for Why the Future May Be Good", "postedAt": "2017-07-19T22:03:17.393Z", "htmlBody": "<html><body><p><span>In late 2014, I ate &#xA0;lunch with an EA who prefers to remain anonymous. I had originally been of the opinion that, should humans survive, the future is likely to be bad. He convinced me to change my mind about this.</span></p>\n<p><span>I haven&#x2019;t seen this argument written up anywhere and so, with his permission, I&apos;m attempting to put it online for discussion.</span></p>\n<p><span>A sketch of the argument is:</span></p>\n<ol>\n<li>\n<p><span>Humans are generally not evil, just lazy</span></p>\n</li>\n<li>\n<p><span>Therefore, we should expect there to only be suffering in the future if that suffering enables people to be lazier</span></p>\n</li>\n<li>\n<p><span>The most efficient solutions to problems don&#x2019;t seem like they involve suffering</span></p>\n</li>\n<li>\n<p><span>Therefore, as technology progresses, we will move more towards solutions which don&#x2019;t involve suffering</span></p>\n</li>\n<li>\n<p><span>Furthermore, people are generally willing to exert some (small) amount of effort to reduce suffering</span></p>\n</li>\n<li>\n<p><span>As technology progresses, the amount of effort required to reduce suffering will go down</span></p>\n</li>\n<li>\n<p><span>Therefore, the future will contain less net suffering</span></p>\n</li>\n<li>\n<p><span>Therefore, the future will be good</span></p>\n</li>\n</ol>\n<h1><span>My Original Theory for Why the Future Might Be Bad</span></h1>\n<p><span>There are about </span><a href=\"http://www.humanesociety.org/news/resources/research/stats_slaughter_totals.html\"><span>ten billion</span></a><span> farmed land animals killed for food every year in the US, which has a population of ~320 million humans. </span></p>\n<p><span>The farmed animals are overwhelmingly living in factory farming conditions, which results in enormous cruelties, and probably have lives which are not worth living. Since (a) farmed animals so completely outnumber humans, (b) humans are the cause of their cruelty, and (c) humans haven&apos;t caused an equal/higher # of beings to lead happy lives, human existence is plausibly bad on net. </span></p>\n<p><span>Furthermore, technology seems to have instigated this problem. Animal agriculture has never been great for the animals which were being slaughtered, but there was historically some modicum of welfare. For example: chickens had to be let outside at least some of the time, because otherwise they would develop vitamin D deficiencies. But with the discovery of vitamins and methods for synthesizing them, chickens </span><a href=\"http://www.americanheritage.com/content/chicken-story\"><span>could now be kept indoors</span></a><span> for their entire lives. Other scientific advancements like antibiotics enabled them to be packed densely, so that now the </span><a href=\"http://www.humanesociety.org/issues/confinement_farm/facts/cage-free_vs_battery-cage.html\"><span>average chicken has</span></a><span> 67 inches of space (about two thirds the size of a sheet of paper). </span></p>\n<p><span>It&apos;s very hard to predict the future, but one reasonable thing you can do is guess that current trends will continue. Even if you don&apos;t believe society is currently net negative, it seems fairly clear that the trend has been getting worse (e.g. the number of suffering farmed animals grew much more rapidly than the [presumably happy] human population over the last century), and therefore we should predict that the future will be bad.</span></p>\n<h1><span>His Response</span></h1>\n<p><span>Technology is neither good nor bad, it&#x2019;s merely a tool which enables the people who use it to do good or bad things. In the case of factory farming, it it seemed to me (Ben) that people overwhelmingly wanted to do bad things, and therefore technological progress was bad. Technological progress will presumably continue, and therefore we might expect this ethical trend to continue and the future to be even worse than today.</span></p>\n<p><span>He pointed out that this wasn&#x2019;t an entirely accurate way of viewing things: people didn&#x2019;t actively want to cause suffering, they are just lazy, and it turns out that the lazy solution in this case causes more suffering.</span></p>\n<p><span>So the key question is: when we look at problems that the future will have, will the lazy solution be the morally worse one?</span></p>\n<p><span>It seems like the answer is plausibly &#x201C;no&#x201D;. To give some examples:</span></p>\n<ol>\n<li>\n<p><span>Factory farming exists because the easiest way to get food which tastes good and meets various social goals people have causes cruelty. Once we get more scientifically advanced though, it will presumably become even more efficient to produce foods without any conscious experience at all by the animals (i.e. </span><a href=\"http://cleanmeat.com/\"><span>clean meat</span></a><span>); at that point, the lazy solution is the more ethical one.</span></p>\n</li>\n<ol>\n<li>\n<p><span>(This arguably is what happened with domestic work animals on farms: we now have cars and trucks which replaced horses and mules, making even the phrase &#x201C;beat like a rented mule&#x201D; seem appalling.)</span></p>\n</li>\n</ol>\n<li>\n<p><span>Slavery exists because there is currently no way to get labor from people without them having conscious experience. Again though, this is due to a lack of scientific knowledge: there is no obvious reason why conscious experience is required for plowing a field or harvesting cocoa, and therefore the more efficient solution is to simply have nonconscious robots do these tasks.</span></p>\n</li>\n<ol>\n<li>\n<p><span>(This arguably is what happened with human slavery in the US: industrialization meant that slavery wasn&#x2019;t required to create wealth in a large chunk of the US, and therefore slavery was outlawed.)</span></p>\n</li>\n</ol>\n</ol>\n<p><span>Of course, this is not a definitive proof that the future will be good. One can imagine the anti-GMO lobby morphing into an anti-clean meat lobby as part of some misguided </span><a href=\"https://en.wikipedia.org/wiki/Appeal_to_nature\"><span>appeal to nature</span></a><span>, for example. </span></p>\n<p><span>But this does give us hope that the lazy &#x2013; and therefore default &#x2013; position on issues will generally be the more ethical one, and therefore people would need to actively work against the grain in order to make the world less ethical.</span></p>\n<p><span>If anything, we might have some hope towards the opposite: a small but nontrivial fraction of people are currently vegan, and a larger number of people spend extra money to buy animal products which (they believe) are less inhumane. I am not aware of any large group which does the opposite (go out of their way to cause more cruelty to farmed animals). Therefore, we might guess that the average position of people is slightly ethical and so people would be willing to not just be vegan if that was the cheaper option, but also be willing to pay a small amount of money to live more ethically.</span></p>\n<p><span>The same thing goes for slavery: a small fraction of consumers go out of their way to buy slave-free chocolate, with no corresponding group of people who go out of their way to buy chocolate produced with slavery. Once machines come close to human cocoa growing abilities, we would expect chocolate industry slavery to die off.</span></p>\n<h1><span>Summary</span></h1>\n<p><span>If the default course of humanity is to be ethical, our prior should be that the future will be good, and the burden of proof shifts to those who believe that the future will be bad.</span></p>\n<p><span>I do not believe it provides a knockdown counterargument to </span><a href=\"https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/\"><span>concerns about s-risks</span></a><span>, but I hope this argument&#x2019;s publication encourages more discussion of the topic, and a viewpoint some readers have not before considered.</span></p>\n<p>&#xA0;</p>\n<p><span>This post represents a combination of my and the anonymous EA&#x2019;s views. Any errors are mine. I would like to thank Gina Stuessy and this EA for proofreading a draft of this post, and for talking about this and many other important ideas about the far future with me.</span></p></body></html>", "user": {"username": "Ben_West"}}, {"_id": "8qyqx6mvAb96t5FEd", "title": "An argument for broad and inclusive \"mindset-focused EA\"", "postedAt": "2017-07-16T16:10:13.384Z", "htmlBody": "<html><body><p><em>Summary:&#xA0;</em>I argue for a very broad, inclusive EA, based on the premise that the culture of a region&#xA0;is more important than any specific group within that region, and that broad and inclusive EA will help shift the overall culture of the world in a better direction. As a concrete strategy, I propose a division into low- and high-level EA - a division which I argue already exists within EA - and then selling people on low-level EA (using EA concepts within their chosen cause area to make that cause more effective), even if they are already committed to causes which traditional EA would consider low-impact or ineffective. I argue that in the long term, this will both boost the general effectiveness of&#xA0;<em>all</em>&#xA0;altruistic work done in the world, and also bring in more people into high-level EA as well.</p>\n<p><em>Related post / see also: <a href=\"/ea/120/all_causes_are_ea_causes/\">All causes are EA causes</a></em>, by Ian David Moss.</p>\n<p><strong>An analogy</strong></p>\n<p>Suppose that you were a thinker living in a predominantly theocratic world, where most people were, if not exactly hostile to science, then at least utterly uninterested in it. You wanted to further scientific understanding in the world, and were pondering between two kinds of strategies:</p>\n<p>1) Focus on gathering a small group of exceptional individuals to do research and to directly further scientific progress, so that the end result of your life&apos;s work would be the creation of a small elite academy of scientists who did valuable research.</p>\n<p>2) Focus on spreading ideas and attitudes that made people more amenable to the idea of scientific inquiry, so that the end result of your life&apos;s work would be your society shifting towards modern Western-style attitudes to science about a hundred years earlier than they would otherwise.</p>\n<p>(I am not assuming that these strategies would have absolutely no overlap: for instance, maybe you would start by forming a small elite academy of scientists to do impressive research, and then use their breakthroughs to impress people and convince them of the value of science. But I am assuming that there are tradeoffs between the two goals, and that you ultimately have to choose to focus more on one or the other.)</p>\n<p>Which of these outcomes, if successful, would do more to further scientific progress in the world?</p>\n<p>It seems clear&#xA0;to me that the second outcome would: most obviously, because if people become generally pro-science, then that will lead to the creation of&#xA0;<em>many</em>&#xA0;elite scientific academies, not just one. Founding an academy composed of exceptional individuals may cause them to make a lot of important results, but they still have to face the population&apos;s general indifference, and many of their discoveries may eventually be forgotten entirely.&#xA0;The combined output of a whole civilization&apos;s worth of scientists, is unavoidably going to outweigh the accomplishments of any small group.</p>\n<p><strong>Mindsets&#xA0;matter more than groups</strong></p>\n<p>As you have probably guessed, this is an analogy for EA, and a commentary on some of the debates that I&apos;ve seen on whether to make EA <em>broad and inclusive,</em> or <em>narrow and weird</em>. My argument is that, in the long term a civilization will do a lot more good if core <a href=\"https://concepts.effectivealtruism.org/concepts/\">EA concepts</a>, such as evaluating charities based on their tractability, have permeated throughout the whole civilization. Such a civilization will do much&#xA0;more good than&#xA0;a civilization where just a small group of people are&#xA0;focusing on particularly high-impact interventions. Similarly to one&#xA0;elite scientific academy vs. a civilization of science-minded people, the civilization that has been permeated by EA ideas will&#xA0;form&#xA0;<em>lots</em> of&#xA0;groups focused on high-impact interventions.</p>\n<p>This could be summed as the intuition that&#xA0;<em>civilizational mindsets are more important than any group or individual. (</em>Donella Meadows places a system&apos;s mindset or paradigm as <a href=\"http://donellameadows.org/archives/leverage-points-places-to-intervene-in-a-system/\">one&#xA0;of the most effective points</a> to intervene in a system.)&#xA0;Any given group can only do as much: but mindsets will consistently&#xA0;lead&#xA0;to the formation of&#xA0;<em>many</em>&#xA0;different groups. Consider for instance the spread of environmentalist ideas over the last century or so: we are now at a point where these ideas are taken for so granted that a lot of different people think that environmentalist charities are self-evidently a good idea and that people who do such work are praiseworthy.&#xA0;Or similarly the spread of the idea that education is important, with the result that an enormous number of education-focused charities now exists. E.g. Charity Navigator alone lists <a href=\"https://www.charitynavigator.org/index.cfm?bay=search.categories&amp;categoryid=3\">close to 700 education-focused charities</a> and <a href=\"https://www.charitynavigator.org/index.cfm?bay=search.categories&amp;categoryid=4\">over 400 environment-focused charities</a>.</p>\n<p>If EA ideas were thought to be similarly obvious, we could have hundreds of EA organizations - or thousands or tens of thousands, given that I expect Charity Navigator to only list a small fraction of all the existing charities in the world.</p>\n<p>Now, there are currently a lot of people working on what many EAs would probably consider ineffective causes, and who have emotional and other commitments to those causes. Many of those people would likely resist the spread of EA ideas, as EA implies that they should change their focus to doing something else.</p>\n<p>I think this happening would be bad - and I don&apos;t mean &quot;it&apos;s bad that we can&apos;t convert these people to more high-impact causes&quot;. I mean &quot;I consider&#xA0;<em>everyone</em>&#xA0;who tries to make the world a better place to be my ally, and I&apos;m happy to see people do&#xA0;<em>anything</em>&#xA0;that contributes to that; and if they have personal reasons for sticking with some particular cause, then I would at least want to enable them to be as effective as possible&#xA0;<em>within that cause</em>&quot;.</p>\n<p>In other words, if someone is committed to getting guide dogs to blind people, then I think that that&apos;s awesome! It may not be the most high-impact thing to do, but I do want to enable blind people to live the best possible lives too, and altruists working to enable that is many times better than those altruists doing nothing at all. And if this is the field that they are committed to, then I hope that they will use EA concepts within that field: figure out whether there are neglected approaches towards helping blind people (could there be something even better than guide dogs?), gather more empirical data to verify existing assumptions about which dog breeds and dog training techniques are the best for&#xA0;helping blind people, consider things like job satisfaction and personal fit in determining whether they want to personally train guide dogs / do administrative work in matching those dogs to blind people / do earn-to-give, etc.</p>\n<p>If EA ideas do spread in this way to everybody who does altruistic work, then that will make <em>all</em> altruistic work more effective. <em>And</em> by the ideas becoming more generally accepted, it will also increase the proportion of people who end up taking EA ideas for granted and consider them obvious. Such people&#xA0;are more likely to apply them to the question of career choice <em>before</em> they&apos;re committed to any specific cause. Both outcomes - all altruistic work becoming more effective, and more people going into more high-impact causes -&#xA0;are fantastic.</p>\n<p><strong>A concrete proposal for mindset-focused EA strategy</strong></p>\n<p>Maybe you grant that all of that sounds like a good idea in principle, but how to apply it in practice?</p>\n<p>My proposal is to&#xA0;<em>explicitly talk about two kinds of EA&#xA0;</em>(these may need catchier names):</p>\n<p>1. High-level EA: taking various EA concepts of tractability, neglectedness, room for funding, etc., and applying them generally, to find&#xA0;<em>whatever cause or intervention</em>&#xA0;that can be expected to do the most good in the world.</p>\n<p>2. Low-level EA: taking some specific cause for granted, and using EA concepts to find the most effective ways of furthering&#xA0;<em>that specific cause</em>.</p>\n<p>With this distinction in place, we can talk about how people can do high-level EA if they are interested in generally doing the most good in the world, or if they are interested in some specific cause, applying low-level EA within that cause. And, to some extent, this is what&apos;s already happening within the EA community: while some people are focused specifically on high-level EA and general cause selection, a lot of others &quot;dip their toes&quot; into high-level EA for a bit to pick their preferred cause area (e.g. global poverty, AI, animal suffering), and then do low-level EA on their chosen cause area from that moment forward. As a result, we already have detailed case studies of applying low-level EA into specific areas: e.g.&#xA0;Animal Charity Evaluators is a low-level EA organization within the cause of animal charity, and has&#xA0;documented ways in which they have applied&#xA0;EA concepts into that cause.</p>\n<p>The main modification is to talk about this distinction more explicitly, and phrase things so as to make it more obvious that people from all cause areas are welcome to apply EA principles into their work. Something like the program of EA Global events could be kept mostly the same, with some of the programming focused on high-level EA content, and some of it focused on low-level EA; just add in some talks/workshops/etc. on applying low-level EA more generally. (Have a workshop about doing this in general, find a guide dog charity that has started applying low-level EA to its work and have their leader give a talk on what they&apos;ve done, etc.) Of course, to more effectively spread the EA ideas, some people would need to focus on making contact with existing charities that are outside the current umbrella of EA causes and, if the people in those charities are receptive to it, work together with them to figure out how they could apply EA to their work.</p></body></html>", "user": {"username": "Kaj_Sotala"}}, {"_id": "wRAZkgBbeEpeZ7TEM", "title": "The marketing gap and a plea for moral inclusivity", "postedAt": "2017-07-08T11:34:52.445Z", "htmlBody": "<html><body><p>In this post, I make three points. First, I note there seems to be a gap between what EA markets itself as being about (effective poverty reduction) and what many EAs really believe is important (poverty isn&#x2019;t the top priority) and this marketing gap is potentially problematic. Second, I propose a two-part solution. One part is that EA outreach-y orgs should be upfront about what they think the most important problems are. The other is that EA outreach-y orgs, and, in fact, the EA movement as a whole, should embrace &#x2018;morally inclusivity&#x2019;: we should state what the most important problems are for a range on moral outlooks but not endorse a particular moral outlook. I anticipate some will think we should adopt &#x2018;moral exclusivity&#x2019; instead, and just endorse or advocate the one view. My third point is a plea for moral inclusivity. I suggest even those who strongly consider one moral position to be true should still be in favour EA being morally inclusive as a morally inclusive movement is likely to generate better outcomes by the standards of everyone&#x2019;s individual moral theory. Hence moral inclusivity is the dominant option.</p>\n<p>Part 1</p>\n<p>One thing that&apos;s been bothering me for a while is the gap between how EA tends to market itself and what lots of EA really believe. I think the existence of this gap (or even if the perception of it) is probably&#xA0;a bad idea and also probably avoidable.&#xA0;I don&apos;t think I&apos;ve seen this discussed elsewhere, so I thought I&apos;d bring it up here.</p>\n<p>To explain, EA often markets itself as being about helping those&#xA0;in poverty (e.g. see GWWC&apos;s website) and exhorts the general public to give their money to effective charities in that area. When people learn a bit more about EA, they discover that only some EAs believe poverty is the most important problem. They realise many EAs think we should really be focusing on the far future, and AI safety in particular, or on helping animals, or finding ways to improving the lives of presently existing humans that aren&#x2019;t do to with alleviating poverty, and that&#x2019;s where those EAs put their money and time.</p>\n<p>There seem to be two possible explanations for the gap between EA marketing and EA reality. The first is historical. Many EAs were inspired by Singer&apos;s&#xA0;<em>Famine, Affluence and Morality</em>&#xA0;which centres on saving a drowning child and preventing those in poverty dying from hunger. Poverty was the original focus. Now, on further reflection, many EAs have decided the far future is the important area but, given its anti-poverty genesis, the marketing/rhetoric is still about poverty.</p>\n<p>The second is that EAs believe, rightly or wrongly, talking about poverty is a more effective marketing strategy than talking about comparatively weird stuff like AI and animal suffering. People understand poverty and it&#x2019;s easier to start with than before moving on to the other things.</p>\n<p>I think the gap is problematic. If EA wants to be effective over the long run, one thing that&apos;s&#xA0;important is that people see it as a movement of smart people with high integrity. I think it&apos;s damaging to EA if there&apos;s the perception,&#xA0;<em>even if this perception is false</em>, that effective altruists are the kind of people that say you should do one thing (give money to anti-poverty charities) but themselves believe in and do something else (e.g. AI safety is the most important).</p>\n<p>I think this is bad for the outside perception of EA: we don&apos;t want to give critics of the movement any more ammo than necessary. I think it&#xA0;potentially disrupts within-community cohesion too. Suppose person X joins EA because they were sold on the anti-poverty line by outreach officer Y. X then become heavily involved in the community and subsequently discovers Y&#xA0;really believes something different from what X&#xA0;was originally sold on. In this case, the new EA X&#xA0;would likely to distrust outreach officer Y, and maybe others in the community too.</p>\n<p>Part 2</p>\n<p>It seems clear to me this gap should go. But what should we do instead? I suggest a solution in two parts.</p>\n<p>First, EA marketing should tally with the sort of things EAs believe are important. If we really think animals, AI, etc. are what matters, we should lead with those, rather than suggesting EA is about poverty and then mentioning other cause areas.</p>\n<p>This doesn&#x2019;t quite settle the matter. Should the marketing represent what current EAs believe is important? This is problematically circular: it&#x2019;s not clear how to identify who counts as an &#x2018;EA&#x2019; except by what they believe. In light of that, maybe the marketing should just represent what the heads or members of EA organisations believe is important. This is also problematic: what if EAs orgs&#x2019; beliefs substantially differ from the rest of the EA community (however that&#x2019;s construed)?</p>\n<p>Here, we seem to face a choice between what I&#x2019;ll call &#x2018;moral inclusivism&#x2019;, stating what the most important problems are for a range on moral outlooks but not endorsing a particular moral outlook, and &#x2018;moral exclusivism&#x2019;, picking a single moral view and endorsing that.</p>\n<p>With this choice in mind, I suggest inclusivism. I&#x2019;ll explain how I thing this works in this section and defend it in the final one.</p>\n<p>Roughly, I think the EA pitch should be &quot;EA is about doing more good, whatever your views&quot;. If that seems too concessive, it could be welfarist &#x2013; &quot;we care making things better or worse for humans and animals&quot; &#x2013; but neutral on makes things better or worse - &quot;we don&apos;t all think happiness is the only thing&#xA0;matters&quot; - and neutral on population ethics &#x2013; &quot;EAs disagree about how much the future matters. Some focus on helping current people, others are worried about the survival of humanity, but we work together wherever we can. Personally, I think cause X is the most important because I believe theory Y...&quot;.&#xA0;</p>\n<p>I don&apos;t think all EA organisations need to be inclusive. What the Future of Humanity Institute works on is clearly stated in its name and it would be weird if it started claim the future of humanity was unimportant. I don&apos;t think individuals EAs need to pretend endorse multiple view eithers. But I think the central, outreach-y ones should adopt inclusivism.</p>\n<p>The advantage of this sort of approach is it allows EA to be entirely straightforward about what effective altruists stand for and avoids even the perception of saying one thing and doing another. Caesar&#x2019;s wife should be above suspicion, and all that.</p>\n<p>An immediate objection is that this sort of approach - front-loading all the &apos;weirdness&apos; of EA views when we do outreach - would be off-putting. I think this worry, so much as it does actually exist, is overblown and also avoidable. Here&apos;s how I think the EA pitch goes:</p>\n<p>-Talk about the drowning child story and/or the comparative wealth of those in the developed world.</p>\n<p>-Talk about ineffective and effective charities.</p>\n<p>-Say that many people became EAs because they were persuaded of the idea we should help others when it&apos;s only a trivial cost to ourselves.</p>\n<p>-Point out people understand this in different ways because of their philosophical beliefs about what matters: some focus on helping humans alive today, others on animals, others on trying to make sure humanity doesn&apos;t accidentally wipe itself out, etc.</p>\n<p>-For those worried about how to &#x2018;sell&#x2019; AI in particular, I recently heard Peter Singer give a talk when he said something like (can&apos;t remember exactly): &quot;some people are very worried about about the risks from artificial intelligence. As Nick Bostrom, a philosopher at the University of Oxford pointed out to me, it&apos;s probably not a very good idea, from an evolutionary point of view, to build something smarter than ourselves.&quot; At which point the audience chuckled. I thought it was a nice, very disarming way to make the point.</p>\n<p>In conclusion, think the apparent gap between rhetoric and reality is problematic and also avoidable. Organisations like GWWC should make it clearer that EAs support causes other than global poverty.</p>\n<p>Part 3</p>\n<p>One might think EA organisations, faced with the inclusivist-exclusivist dilemma, should opt for the latter. You might think most EAs, at least within certain organisations, do agree one a single moral theory, so endorsing morally inclusivity would be dishonest. Instead, you could conclude we should be moral exclusivists, fly the flag for our favourite moral theory, lead with it and not try to accommodate everyone.</p>\n<p>From my outsider&#x2019;s perspective, I think this is sort of direction 80,000 Hours has started to move in more recently. They are now much more open and straightforward about saying the far future in general, and AI safety in particular, is what really matters. Their <a href=\"https://80000hours.org/articles/cause-selection/\">cause selection choices</a>, which I think they updated a few months ago only really make sense if you adopt total utilitarianism (maximise happiness throughout history of the universe) rather than if you prefer a person-affecting view in population ethics (make people happy, don&#x2019;t worry about creating happy people) or you just want to focus on the near future (maybe due to uncertainty about what we can do or pure time discounting).</p>\n<p>An obvious worry about being a moral exclusivist and picking one moral theory is that you might be wrong; if you&#x2019;re endorsing the wrong view, that&#x2019;s really going to set back your ability to do good. But given you have to take some choices, let&#x2019;s putthis worry aside. I&#x2019;m now going to make a plea for making/keeping EA morally inclusive whatever your preferred moral views are. I offer three reasons.</p>\n<p>1.</p>\n<p>Inclusivity reduces group think. If EA is known as a movement where people believe view X, people who don&#x2019;t like view X will exit the movement (typically without saying anything). This deprives those who remain of really useful criticism that would help identify intellectual blind spots and force the remainers to keep improving their thinking. This also creates a false sense of confidence in the remainers because all their peers now agree with them.</p>\n<p>Another part of this is that, if you want people to seek the truth, you shouldn&#x2019;t give them incentives to be yes-humans. There are lots of people that like EA and want to work in EA orgs and be liked by other (influential) EAs. If people think they will be rewarded (e.g. with jobs) for adopting the &#x2018;right&#x2019; views and signalling them to others, they will probably slide towards what they think people want to hear, rather than what they think is correct. Responding to incentives is a natural human thing to do, and I very much doubt EAs are immune to it. Similar to what I said in part 1, even a perception there are &#x2018;right&#x2019; answers can be damaging to truth seeking. Like a good university seminar leader, EA should create an environment where people feel inspired to seek the truth, rather than just agree with the received wisdom, as honest truth seeking and disagreement seems mostly likely to reveal the truth.</p>\n<p>2.</p>\n<p>Inclusivity increases movement size. If we only appeal to a section of the &apos;moral market&apos; then there won&apos;t be so many people in the EA world. Even if people have different views, they can still can work together, engage in moral trade, personally support each other, share ideas, etc.</p>\n<p>I think organisations working on particular, object-level problems, need to be value-aligned to aid co-ordination (if I want to stop global warming and you don&apos;t care, you shouldn&apos;t join my global warming org) but this doesn&apos;t seem relevant at the level of a community. Where people meet in at EA hubs, EA conferences, etc. they&#x2019;re not working together anyway. Hence this isn&#x2019;t an objection for EA outreach-y orgs being morally inclusive.</p>\n<p>&#xA0;3.</p>\n<p>Inclusivity minimises in-fighting. If people perceive there&#x2019;s only one accepted and acceptable view, then people will spend their time fighting the battle of hearts and minds to ensure that their view wins, and this will do this rather than working on solving real world problems themselves. Or they&apos;ll split, stop talking to each other and fail to co-ordinate. Witness, for instance, the endless schisms within churches about doctrinal matters, like gay marriage, and the seemingly limited interest they have in helping other people. If people instead believe there&apos;s a broad range of views within a community, this is okay, and there&#x2019;s no point fighting for ideological supremacy, they can instead engage in dialogue, get along and help each other. More generally, I think I&#x2019;d rather be in a community where people thought different things and this was accepted, rather than one where there were no disagreements and none allowed.</p>\n<p>On the basis of these three reasons, I don&#x2019;t think even those who believe they&#x2019;ve found the moral truth should want EA as a whole to be morally exclusive. Moral inclusivity seems to increase ability of effective altruists to collectively seek the truth and work together, which looks like it leads to more good being done&#xA0;from the perspective of each moral theory.</p>\n<p>What followed from parts 1 and 2 is that, for instance, GWWC should close the marketing gap and be more upfront about what EAs really believe. People should not feel surprised about what EAs value when they get more involved in the movement.</p>\n<p>What follows from part 3 is that, for instance, 80,000 Hours should be much morally inclusive than they presently are. Instead of &#x201C;these are the most important things&#x201D;, it should &#x201C;these are the most important things if you believe A, but not everyone believes A. If you believe B, you should think these are the important things [new list pops up]. As an organisation, we don&#x2019;t take a stand on A or B, but here are some arguments you might find relevant to help you decide&#x201D;.</p>\n<p>Here end my pleas for moral inclusivity.</p>\n<p>There may be arguments for keeping the marketing gap and adopting moral exclusivism I&#x2019;ve not considered and I&#x2019;d welcome discussion.&#xA0;</p>\n<p>Edit (10/07/2017): Ben Todd points out in the comment below that 1) 80k have stated their preferred view since 2014 in order to be transparent and that 2) they provide a decision tool for those who disagree with 80k&apos;s preferred view. I&apos;m pleased to learn the former and admit my mistake. On the latter, Ben and I seem to disagree whether adding the decision tool makes 80k morally inclusive or not (I don&apos;t think it does).</p></body></html>", "user": {"username": "MichaelPlant"}}, {"_id": "YRzELEmB8eqqwpp8J", "title": "How can we best coordinate as a community?", "postedAt": "2017-07-07T04:45:55.619Z", "htmlBody": "<html><body><p>I recently gave a talk at EAG:Boston on this topic.</p>\n<p><a href=\"https://www.youtube.com/watch?v=S7SFq4v3tRo&amp;t=1s\">The video is now up.</a></p>\n<p>Below is the blurb, and a rough script (it&apos;ll be easier to understand if you watch the video with boosted speed).</p>\n<p>All these ideas are pretty speculative, and we&apos;re working on some more in-depth articles on this topic, so I&apos;d be keen to get feedback.</p>\n<p>* * *</p>\n<p><em><span>A common objection to effective altruism is that it encourages an overly &#x201C;individual&#x201D; way of thinking, which reduces our impact. Ben will argue that, at least in a sense, that&#x2019;s true.</span></em></p>\n<p><em><span>When you&#x2019;re part of a community, which careers, charities and actions are highest-impact changes. An overly narrow, individual analysis, which doesn&#x2019;t take account of how the rest of the community will respond to your actions, can lead you to have less impact than you could. Ben will suggest some better options and rules of thumb for working together.</span></em></p>\n<p>* * *</p>\n<p><span>Introduction - why work together?</span></p>\n<ul>\n<li><span>Here&#x2019;s one of the most powerful ways to have more impact: join a community.</span></li>\n<li><span>Why&#x2019;s that?</span></li>\n<li><span>Well, one reason is it&#x2019;s motivating - being around other people who want to help others changes your social norms, and makes you more motivated.</span></li>\n<li><span>It&#x2019;s like networking on steroids - once one person vouches for you, they can introduce you to everyone else.</span></li>\n<li><span>And also, what I want to talk about today, you can trade and coordinate.</span></li>\n<ul>\n<li><span>Let&#x2019;s suppose I want to build and sell a piece of software. One approach would be to learn all the skills needed myself - design, engineering, marketing and so on.</span></li>\n<li><span>A much better approach is to form a team who are skilled in each area, and then build it together. Although I&#x2019;ll have to share the gains with the other people, the size of the gains will be much larger, so we&#x2019;ll all win.</span></li>\n<ul>\n<li><span>[diagram]</span></li>\n</ul>\n<li><span>One thing that&#x2019;s going on here is specialisation - each person can focus on a specific skill, which lets them be more effective.</span></li>\n<li><span>Another thing is that the team can also share fixed costs (same office, same company registration, same operational procedures etc.), letting up achieve economies of scale.</span></li>\n<li><span>In total, we get what&#x2019;s called the &#x201C;gains from trade&#x201D;.</span></li>\n<li><span>An important thing about trade is that you can do it with people who </span><span>don&#x2019;t</span><span> especially share your values, and both gain.</span></li>\n<ul>\n<li><span>Suppose, hypothetically, one group runs an animal charity, and they don&#x2019;t think global poverty is that high impact.</span></li>\n<li><span>Another group runs a global poverty charity, and they don&#x2019;t think factory farming is that high impact.</span></li>\n<li><span>But imagine both groups know some donors who might be interested in the other cause. They can make mutual introductions. Making an introduction isn&#x2019;t much cost, but could be a huge benefit to the other group. So, if both groups trade, they both gain.</span></li>\n</ul>\n<li><span>This is why 100 people working together have the potential to have far more impact than 100 people doing what individually seems best.</span></li>\n</ul>\n</ul>\n<p><strong>&#xA0;</strong></p>\n<p><span>Why the EA community?</span></p>\n<ul>\n<li><span>What I&#x2019;ve said so far applies to </span><span>any</span><span> community, and there are lots of great communities out there.</span></li>\n<li><span>But I know many people who think that getting involved in the EA community has been an especially big boost to their impact.</span></li>\n<li><span>Why&#x2019;s that?</span></li>\n<li><span>Well, as we&#x2019;ve shown you can trade with in general to have a greater impact, even when you don&#x2019;t especially share their values.</span></li>\n<li><span>But if you </span><span>do</span><span> share values you don&#x2019;t even need to trade.</span></li>\n<li><span>What do I mean?</span></li>\n<li><span>Well, if I help someone else in the EA community have more impact, then </span><span>I&#x2019;ve</span><span> also had more impact, so we </span><span>both</span><span> achieve our goals. </span></li>\n<li><span>This means I don&#x2019;t need to worry about getting a favour back from the other person to break even. Just helping them is already valuable.</span></li>\n<li><span>This unleashes far more opportunities to work together, that just wouldn&#x2019;t be efficient in a community where people don&#x2019;t share my aims as much.</span></li>\n<ul>\n<li><span>(Technically speaking, transaction costs and principal-agent problems are dramatically reduced.)</span></li>\n</ul>\n</ul>\n<p><strong>&#xA0;</strong></p>\n<ul>\n<li><span>We don&#x2019;t normally think about it like this, but earning to give can actually an example of this kind of coordination.</span></li>\n<ul>\n<li><span>In the early days of 80k, we needed one person to run the org and we needed funding. Me and another guy called Matt considered the position. We realised Matt had higher earning potential than me, while I was better suited to running 80,000 Hours, hopefully.</span></li>\n<li><span>There were other factors at play, but in part, this is why I became the CEO, and Matt earned to give and became one of our largest donors, as well as seed funding several other orgs.</span></li>\n<li><span>The alternative would have been to for us both to earn to give, in which case, no 80k; or for us both to work at 80k, in which case it would have taken us much longer to fundraise (and the other orgs wouldn&#x2019;t have benefited).</span></li>\n<li><span>With the community as a whole, some people are like Matt, relatively better suited to earning money, and others to running non-profits. We can achieve more if the people best suited to earning money earn to give and fund everyone else.</span></li>\n</ul>\n<li><span>In sum, by working together effectively, we have the potential to achieve far more.</span><span><br><br></span></li>\n</ul>\n<p><span>How can we work together better?</span></p>\n<ul>\n<li><span>However, I don&#x2019;t think we work together as a community as well as we could.</span></li>\n<li><span>Effective altruism encourages us to ask </span><span>which individual actions </span><span>lead to the most impact.</span></li>\n<li><span>Some critiques of the community have suggested this question could bias us so that we don&#x2019;t actually do the highest impact things.</span></li>\n<li><span>Here is perhaps the most well-known criticism in this vein, in the London Review of Books.</span></li>\n<ul>\n<li><span>[read out]</span></li>\n<li><span>I agree with Jeff McMahan&#x2019;s response to this article. He points out that ultimately what we can control are our individual actions, so </span><span>they are</span><span> the most relevant. We can&#x2019;t direct the whole of society.</span></li>\n</ul>\n<li><span>However, I think there&#x2019;s also truth in Amia&#x2019;s view: when we think about what&#x2019;s best from an individual perspective, we have to be wary of the biases of this way of thinking, otherwise we might not actually find the highest-impact individual actions.</span><span><br><br></span></li>\n<li><span>I often see people in the community taking what I call a &#x201C;narrow, single player&#x201D; perspective to figuring out what&#x2019;s best - not fully factoring in the relevant counterfactuals and how the community will adjust to their actions. </span></li>\n<li><span>This might have worked before we had a community, but these days it doesn&#x2019;t. </span></li>\n<li><span>Instead we need to move to what I call a &#x201C;multiplayer perspective&#x201D;, </span></li>\n</ul>\n<ul>\n<li><span>In particular:</span></li>\n<ul>\n<li><span>First, we need to take a different approach to choosing between our options.</span></li>\n<li><span>Second, new options become promising.</span></li>\n<li><span>I&#x2019;ll cover both in turn.</span><span><br><br></span></li>\n</ul>\n</ul>\n<p><span>1) How to choose between our options</span></p>\n<ul>\n<ul>\n<li><span>Let&#x2019;s consider this question: should I take a job at a charity in the community? Like GiveWell, or AMF or CEA.</span></li>\n<li><span>Amy is considering working at a charity. What&#x2019;s her impact?</span></li>\n<li><span>The naive view is that the job is high impact, so if I take it, I&#x2019;ll have a big impact.</span></li>\n<li><span>But then you hear about EA, and someone points out: if you don&#x2019;t take the job, someone else will take it, so actually your impact is small. The job is only worth taking if you&#x2019;d be much better than the person who replaces you.</span></li>\n<li><span>We call this analysis &#x201C;simple replaceability&#x201D; and it&#x2019;s an example of single player thinking.</span></li>\n<li><span>This leads to lots of people not wanting to do direct work, and thinking it&#x2019;s better to earn to give instead.</span></li>\n<li><span>But this is wrong. And I apologise, because it&#x2019;s partly our fault for talking loosely about the simple replaceability view in the past. But today I want to help stamp it out.</span></li>\n</ul>\n</ul>\n<p><strong>&#xA0;</strong></p>\n<ul>\n<li><span>The first problem is that you won&#x2019;t always be replaced. There&#x2019;s a chance that the charity just won&#x2019;t hire anyone otherwise.</span></li>\n<ul>\n<li><span>In fact this seems to be often the case. When we talk to the organisations, there are roles they&#x2019;ve been trying to fill for a while, but haven&#x2019;t been able to.</span></li>\n<li><span>One reason this is that, because there are donors with money on the sidelines, if the organisations were able to find someone with a good level of fit, they could fundraise enough money to pay for their salaries.</span></li>\n<li><span>This means the organisations do what&#x2019;s called &#x201C;threshold hiring&#x201D; - hire anyone above that level of fit. </span></li>\n<li><span>And there ways you can end up being not replaceable, such as supply-demand effects, which we cover elsewhere.</span></li>\n<li><span>Either way, the simple analysis of replaceability ends up underestimating the impact.</span></li>\n<li><span>Instead, you can end up being pretty valuable to the organisation you work at.</span></li>\n<li><span>One way to estimate the effect is to ask the org how much you&#x2019;d need to donate to them to be indifferent between you taking the job and them getting donations. This helps to measure the size of the benefit to the organisation. [show Q on slide]</span></li>\n<li><span>We actually did this with 12 organisations in the community, and for their most recent hire, they gave figures of</span></li>\n<ul>\n<li><span>[slide: average of $126,000 &#x2013; $505,000 and a median of $77,000 &#x2013; $307,000 per year.]</span></li>\n<li><span>The organisations may well be biased upwards, but since it&#x2019;s significantly more than most people who could work at EA orgs could donate, it at least suggests they&#x2019;re having much more impact than they would through earning to give. </span></li>\n<li><span>We also asked the orgs simply how funding vs talent constrained they are, and you can see a clear bias towards talent constraint rather than funding constraint.</span></li>\n<ul>\n<li><span>Interestingly, the animal focused orgs were more funding constrained, so if you remove them, the figures were higher.</span><span><br><br></span></li>\n</ul>\n</ul>\n</ul>\n<li><span>So, the first problem with the simple analysis of replaceability is that you might not actually be replaced. The second problem is where the community comes in. </span></li>\n<li><span>Even if you take the job, and someone else </span><span>would have taken it</span><span> anyway, </span><span>that</span><span> person is freed up to go and do something else that&#x2019;s valuable. So there&#x2019;s a spillover benefit to the rest of the community.</span></li>\n<li><span>If you were considering a job that would be filled by someone who didn&#x2019;t care about social impact otherwise, like a random job in the corporate world, then it&#x2019;s fine, you can mostly ignore these spillovers. The &#x201C;single player&#x201D; view would be fine.</span></li>\n<li><span>But in the current community, however, that person will probably go and do something else you think is high-impact, so it&#x2019;s a significant benefit.</span></li>\n<ul>\n<li><span>This is not hypothetical - I&#x2019;ve seen real cases where someone didn&#x2019;t take a job because they thought they&#x2019;d be replaceable, which then meant someone else had to be taken from a high-impact role.</span></li>\n</ul>\n<li><span>So, there&#x2019;s a second benefit that&#x2019;s ignored by the simple analysis of replaceability.</span></li>\n<ul>\n<li><span>And, for both reasons, the impact of taking the job is </span><span>higher</span><span>.</span></li>\n</ul>\n<li><span>How valuable? I think this is still an unsolved problem, but here is a sketch of our thinking.</span></li>\n</ul>\n<p><strong>&#xA0;</strong></p>\n<ul>\n<li><span>Basically, you cause a chain reaction of replacements. The chain can end if either (i) it hits a threshold job where the person isn&#x2019;t replaceable, or you (ii) hit the marginal opportunity in the community - the best role that has not yet been taken.</span></li>\n<ul>\n<li><span>So, at the worst, you&#x2019;re adding someone to the marginal position in the community, which is still a significant impact.</span></li>\n<li><span>Plus, by triggering the chain, you&#x2019;re hopefully helping people switch into roles that better play to their relative strengths. So we also get to a more efficient allocation.</span></li>\n</ul>\n</ul>\n<p><strong>&#xA0;</strong></p>\n<ul>\n<li><span>Zooming out, rather than identify the single highest-impact job in general, and try to take that, instead we have several thousand roles to fill, and want to achieve the optimal allocation over them. The question is: </span><span>what can you do to take the community towards the optimal allocation over the best roles?</span><span><br><br></span></li>\n<li><span>I think the key concept here is your comparative advantage, compared to the rest of the community.</span><span><br><br></span></li>\n<li><span>Comparative advantage can be a little counterintuitive, and is not always the same as personal fit, so let&#x2019;s explore a little more.</span></li>\n<li><span>Let&#x2019;s imagine there are two roles that need filling, research and outreach; and two people.</span></li>\n<ul>\n<li><span>[diagram]</span></li>\n<li><span>Charlie is 1 at outreach and 2 at research.</span></li>\n<li><span>Dora is 2 at outreach and 10 at research.</span></li>\n<li><span>What&#x2019;s best?</span></li>\n<li><span>It depends on exactly how we interpret these numbers, but probably Charlize should do outreach and Dexter should do research, because 1 + 10 &gt; 2 + 2</span></li>\n</ul>\n<li><span>This is surprising because, in a sense, Charlie is actually worse at outreach than Dora, and worse at outreach than research, so he in no sense has an absolute advantage in outreach, but it turns out he does have a comparative advantage in it. This is because he&#x2019;s </span><span>relatively </span><span>less bad at outreach compared to Dora.</span></li>\n<li><span>I have a suspicion this might be a real example.</span></li>\n<ul>\n<li><span>[slide]</span></li>\n<li><span>Lots of people in the community are good at analytical things compared to people in general, so they figure they should do research.</span></li>\n<li><span>But this doesn&#x2019;t follow. What actually matters is how good they are at research </span><span>relative</span><span> to others in the community.</span></li>\n<li><span>If we have lots of analytical people and few outreach people, then even though you&#x2019;re good at research compared to people in general, you might have a </span><span>comparative </span><span>advantage in outreach.</span></li>\n<li><span>Something similar seems true with operations roles.</span></li>\n<li><span>It may also be true with earning to give. People often reason that because they have high earning potential, they should earn to give. But this doesn&#x2019;t quite follow. What actually matters is their earning potential </span><span>relative</span><span> to others who could do direct work. If the other direct work people </span><span>also</span><span> have high earning potential, then they might have a comparative advantage in direct work.</span></li>\n<li><span>Unless you&#x2019;re chuck norris, who has a comparative advantage in everything.</span></li>\n</ul>\n<li><span>How can you figure out your comparative advantage in real cases? Ask people in charge of hiring at the orgs - what would the next best hire do anyway, and what would their donation potential be compared to yours?</span><span><br><br></span></li>\n<li><span>So, let&#x2019;s sum up. How to work out your impact by taking a job in the community?</span></li>\n<ul>\n<li><span>First, you probably cause some boost to the org itself, and you&#x2019;re not fully replaceable. You can roughly estimate the size of this boost by asking the org to make tradeoffs, or making your own estimates.</span></li>\n<li><span>Second, you cause some spillover benefit to the rest of the community, because you free up someone else to go and work elsewhere.</span></li>\n<ul>\n<li><span>The key question then is whether the role is the above the bar for the community as a whole, and whether it plays to your comparative advantage compared to the other people who might take the role?</span></li>\n<li><span>Are you getting the community towards a better overall allocation?</span></li>\n</ul>\n</ul>\n</ul>\n<p><span>&#xA0;</span></p>\n<ul>\n<li><span>I think basically the same analysis applies to donating as well.</span></li>\n<ul>\n<li><span>Sometimes people don&#x2019;t want to donate because they think someone else will fund the charity anyway.</span></li>\n<li><span>But they ignore the fact that if even if their donation were 100% replaced, at worst they&#x2019;d be freeing up another donor, sooner, to go and donate to something else.</span></li>\n<li><span>There&#x2019;s lots more to say here, I go into a bit more detail in this article.</span></li>\n</ul>\n</ul>\n<p><span><br><br></span></p>\n<p><span>2) The new options that come available</span></p>\n<p><strong>&#xA0;</strong></p>\n<ul>\n<li>\n<p>Being part of a community also changes your career by opening up new options that wouldn&#x2019;t be on the table if you were just in a single player game.</p>\n</li>\n<li><span>The EA mindset, with a focus on individual actions, can lead us to neglect paths to impact through helping others achieve more, because the impact is less salient. However, now that there are 1000s of other effective altruists, the highest impact option for you could well be one that involves &#x201C;boosting&#x201D; others in the community.</span><span><br><br></span></li>\n<ul>\n<li><span>Here are five examples. They aren&#x2019;t exhaustive or exclusive, but are just some ideas.</span><span><br><br></span></li>\n<li><span>First, five minute favours. </span></li>\n</ul>\n<li><span>We all have different strengths and weaknesses, knowledge and resources. Now the community is so big, there are probably lots of small ways we can help others in the community have far more impact, that would be very little cost to ourselves- </span><span>5 minute favours. </span><span>(a term I took from Adam Grant). These are really worth looking for.</span></li>\n<ul>\n<ul>\n<li><span>Do you know a job that needs filling? There&#x2019;s a good chance someone in this room would be a good candidate. If you could introduce them to the job, it might only take you under an hour, but it would have a major impact for years.</span></li>\n<li><span>There&#x2019;s probably someone in this room who has a problem you could easily solve, because you&#x2019;ve already solved it, or you know a book that contains the answer, and so on.</span></li>\n</ul>\n<li><span>Here&#x2019;s a second, more involved example of helping others be more effective: Operations roles in general.</span></li>\n<li><span>Kyle went to Oxford, and ended up becoming Nick Bostrom&#x2019;s assistant. He reasoned that if he could save Bostrom time, then he could let more research and outreach get done. I think this could be really high impact.</span></li>\n<li><span>People feel like these roles are easily replaceable by people outside the community, but because you have to make lots of little decisions that require a good understanding of the aims of the organisation, they&#x2019;re actually often very hard to outsource.</span></li>\n</ul>\n</ul>\n<p><strong>&#xA0;</strong></p>\n<ul>\n<ul>\n<li><span>A third category is that building community </span><span>infrastructure </span><span>becomes much more valuable.</span></li>\n<ul>\n<li><span>Having a job board isn&#x2019;t really needed when there are only a few hundred people in the community, but now there are 1000s it can play a useful role, so we recently made one.</span></li>\n<ul>\n<li><span>[show screenshot]</span></li>\n</ul>\n<li><span>Infrastructure is anything that helps to make the community coordinate more efficiently, such as this event, or setting up good norms, that make it easier to work together&#x2026;.like stating the evidence for your views, or being nice.</span></li>\n<li><span>If you can help 1000 people be 1% more effective, then that&#x2019;s like having the impact of 10 people.</span></li>\n<li><span>On the other hand, if you do something destructive, then it ruins it for everyone else</span></li>\n</ul>\n</ul>\n</ul>\n<p>&#xA0;</p>\n<ul>\n<ul>\n<li><span>A fourth category, is </span><span>knowledge sharing</span><span>.</span></li>\n<li><span>The more people there are in the community, the more valuable it is to do research into what the community should do and share it, because there are more people who might act on the findings.</span></li>\n<ul>\n<li><span>One example is writing up reports on areas we have special knowledge of</span></li>\n<ul>\n<li><span>[give examples from EA forum]</span></li>\n</ul>\n<li><span>This can mean it&#x2019;s sometimes worth going and learning about areas that don&#x2019;t seem like the highest priority but might turn out to be. In a smaller community, this exploration wouldn&#x2019;t be worth the time, but as we become larger, it is.</span></li>\n</ul>\n</ul>\n<li><span>A fifth example, </span><span>specialisation</span><span> becomes more worth doing.</span></li>\n<ul>\n<ul>\n<li><span>If the community were just a couple of people, we&#x2019;d all need to become generalists. But in a community of, say, 1000 people, we can all become experts in our individual areas, and be more than 1000 times as productive as an individual. This is just division of labour like we mentioned at the very start, with the software example.</span></li>\n<li><span>For instance, Dr. Greg Lewis did the research on 80,000 Hours into how many lives doctors save, and this convinced him that he wouldn&#x2019;t have much social impact as a doctor through clinical practice. </span></li>\n<li><span>Instead he decided to do a masters in public health.</span></li>\n<li><span>Part of the reason was because it&#x2019;s an important area for the community, especially around pandemics, but there&#x2019;s a lack of people with the skillset.</span></li>\n<li><span>Greg actually thinks AI risk might be higher priority in general, but as a doctor, he has a comparative advantage in public health.</span></li>\n<li><span>Right now, I, and many others, think that one of the greatest weaknesses of the community is a lack of specialist expertise. We&#x2019;re generally pretty young and inexperienced.</span></li>\n<li><span>Some particular gaps include the following, which I&#x2019;m not going to read out:</span></li>\n<ul>\n<li><span>Policy experts - go and work in politics or at a think tank</span></li>\n<li><span>Bioengineering PhDs</span></li>\n<li><span>Machine learning PhDs</span></li>\n<li><span>Economics PhDs</span></li>\n<li><span>Other under-represented areas - history, anthropology,</span></li>\n<li><span>Entrepreneurial managers and operational people</span></li>\n<li><span>Marketing and outreach experts</span></li>\n</ul>\n</ul>\n</ul>\n</ul>\n<p><span>Summary</span></p>\n<ul>\n<li><span>When choosing whether to take a job, or donate somewhere, </span><span>don&#x2019;t assume you&#x2019;re replaceable. </span></li>\n<ul>\n<li><span>Rather, ask the organisation, or others who are aware of the alternative options, about your relative strengths and weaknesses.</span></li>\n<li><span>You might also trigger a chain of people going into other roles. Consider whether the role plays to your</span><span> comparative advantage. </span></li>\n</ul>\n<li><span>Look for ways to boost the impact of others in the community.</span></li>\n<ul>\n<li><span>5 minute favours</span></li>\n<li><span>Operations roles</span></li>\n<li><span>Community infrastructure</span></li>\n<li><span>Sharing knowledge.</span></li>\n<li><span>Specialisation.</span></li>\n</ul>\n</ul>\n<p><strong>&#xA0;</strong></p>\n<p><span>End</span></p>\n<ul>\n<li><span>We still have a lot to learn about how to best work together, and there&#x2019;s a lot more we could do. </span>But I really believe that if we do work together effectively, then, in our lifetimes, the community can make major progress on reducing catastrophic risks, eliminating factory farming, ending global poverty, and many other issues.</li>\n</ul>\n<p>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Benjamin_Todd"}}, {"_id": "SEL9PW8jozrvLnkb4", "title": "My current thoughts on MIRI's \"highly reliable agent design\" work", "postedAt": "2017-07-07T01:17:24.909Z", "htmlBody": "<html><body><h2>Interpreting this writeup:</h2>\n<p>I lead the Open Philanthropy Project&apos;s work on technical AI safety research. In our MIRI <a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support\">grant writeup</a> last year, we said that we had strong reservations about MIRI&#x2019;s research, and that we hoped to write more about MIRI&apos;s research in the future. This writeup explains my current thinking about the subset of MIRI&apos;s research referred to as &quot;highly reliable agent design&quot; in the <a href=\"https://intelligence.org/technical-agenda/\">Agent Foundations Agenda</a>. My hope is that this writeup will help move the discussion forward, but I definitely do not consider it to be any kind of final word on highly reliable agent design. I&apos;m posting the writeup here because I think this is the most appropriate audience, and I&apos;m looking forward to reading the comments (though I probably won&apos;t be able to respond to all of them).</p>\n<p>After writing the first version of this writeup, I received comments from other Open Phil staff, technical advisors, and MIRI staff. Many comments were disagreements with arguments or credences stated here; some of these disagreements seem plausible to me, some comments disagree with one another, and I place significant weight on all of them because of my confidence in the commentators. Based on these comments, I think it&apos;s very likely that some aspects of this writeup will turn out to have been miscalibrated or mistaken &#x2013; i.e. incorrect given the available evidence, and not just cases where I assign a reasonable credence or make a reasonable argument that may turn out to be wrong &#x2013; but I&apos;m not sure which aspects these will turn out to be.</p>\n<p>I considered spending a lot of time heavily revising this writeup to take these comments into account. However, it seems pretty likely to me that I could continue this comment/revision process for a long time, and this process offers very limited opportunities for others outside of a small set of colleagues to engage with my views and correct me where I&apos;m wrong. I think there&apos;s significant value in instead putting an imperfect writeup into the public record, and giving others a chance to respond in their own words to an unambiguous snapshot of my beliefs at a particular point in time.</p>\n<h2>Contents</h2>\n<ol>\n<li><a href=\"#s1\">What is &quot;highly reliable agent design&quot;?</a></li>\n<li><a href=\"#s2\">What&apos;s the basic case for HRAD?</a></li>\n<li><a href=\"#s3\">What do I think about HRAD?</a></li>\n<ol>\n<li><a href=\"#s4\">Low credence that HRAD will be applicable (25%?)</a></li>\n<li><a href=\"#s5\">HRAD has few advocates among AI researchers</a></li>\n<li><a href=\"#s6\">Other research, especially &quot;learning to reason from humans,&quot; looks more promising than HRAD (75%?)</a></li>\n<li><a href=\"#s7\">MIRI staff are thoughtful, aligned with our values, and have a good track record</a></li>\n</ol>\n<li><a href=\"#s8\">How much should Open Phil support HRAD work?</a></li>\n</ol>\n<h2>1. What is &quot;highly reliable agent design&quot;?</h2>\n<p>I understand MIRI&apos;s &quot;highly reliable agent design&quot; work (coined in <a href=\"https://intelligence.org/technical-agenda/\">this research agenda</a>, &quot;HRAD&quot; for short) as <strong>work that aims to describe basic aspects of reasoning and decision-making in a complete, principled, and theoretically satisfying way.</strong> Here&apos;s a non-exhaustive list of research topics in this area:&#xA0;</p>\n<ul>\n<li>Epistemology: developing a formal theory of induction that accounts for the facts that an AI system will be implemented in the physical world it is reasoning about (&quot;naturalistic world models&quot;) and that other intelligent agents may be simulating the AI system (&quot;benign universal prior&quot;).</li>\n<li>Decision theory: developing a decision theory that behaves appropriately when an agent&apos;s decisions are logically entangled with other parts of the environment (e.g. in the presence of other copies of the agent, other very similar systems, or other agents that can predict the agent), and that can&apos;t be profitably threatened by other agents.</li>\n<li>Logical uncertainty: developing a rigorous, satisfying theory of probabilistic reasoning over facts that are logical consequences of an agent&apos;s current beliefs, but that are too expensive to reason out deductively.</li>\n<li>Vingean reflection: developing a theory of formal reasoning that allows an agent to reason with high reliability about similar agents, including agents with considerably more computational resources, without simulating those agents.</li>\n</ul>\n<p>To be really satisfying, it should be possible to put these descriptions together into a full and principled description of an AI system that reasons and makes decisions in pursuit of some goal in the world, not taking into account issues of efficiency; this description might be understandable as a modified/expanded version of <a href=\"https://jan.leike.name/AIXI.html\">AIXI</a>. Ideally this research would also yield rigorous explanations of why no other description is satisfying.</p>\n<h2>2. What&apos;s the basic case for HRAD?</h2>\n<p>My understanding is that MIRI (or at least Nate and Eliezer) believe that if there is not significant progress on many problems in HRAD, the probability that an advanced AI system will cause catastrophic harm is very high. (They reserve some probability for other approaches being found that could render HRAD unnecessary, but they aren&apos;t aware of any such approaches.)</p>\n<p>I&apos;ve engaged in many conversations about why MIRI believes this, and have often had trouble coming away with crisply articulated reasons. So far, the basic case that I think is most compelling and most consistent with the majority of the conversations I&apos;ve had is something like this (phrasing is mine / Holden&apos;s):&#xA0;</p>\n<ol>\n<li>Advanced AI systems are going to have a huge impact on the world, and for many plausible systems, we won&apos;t be able to intervene after they become sufficiently capable.</li>\n<li>If we fundamentally &quot;don&apos;t know what we&apos;re doing&quot; because we don&apos;t have a satisfying description of how an AI system should reason and make decisions, then we will probably make lots of mistakes in the design of an advanced AI system.</li>\n<li>Even minor mistakes in an advanced AI system&apos;s design are likely to cause catastrophic misalignment.</li>\n<li>Because of 1, 2, and 3, if we don&apos;t have a satisfying description of how an AI system should reason and make decisions, we&apos;re likely to make enough mistakes to cause a catastrophe. The right way to get to advanced AI that does the right thing instead of causing catastrophes is to deeply understand what we&apos;re doing, starting with a satisfying description of how an AI system should reason and make decisions.</li>\n<li>This case does not revolve around any specific claims about specific potential failure modes, or their relationship to specific HRAD subproblems. This case revolves around the value of fundamental understanding for avoiding &quot;unknown unknown&quot; problems.</li>\n</ol>\n<p>I also find it helpful to see this case as asserting that HRAD is one kind of &quot;basic science&quot; approach to understanding AI. Basic science in other areas &#x2013; i.e. work based on some sense of being intuitively, fundamentally confused and unsatisfied by the lack of explanation for something &#x2013; seems to have an outstanding track record of uncovering important truths that would have been hard to predict in advance, including the work of Faraday/Maxwell, Einstein, Nash, and Turing. Basic science can also provide a foundation for high-reliability engineering, e.g. by giving us a language to express guarantees about how an engineered system will perform in different circumstances or by improving an engineer&apos;s ability to design good empirical tests. Our lack of satisfying explanations for how an AI system should reason and make decisions and the importance of &quot;knowing what we&apos;re doing&quot; in AI make a basic science approach appealing, and HRAD is one such approach. (I don&apos;t think MIRI would say that there couldn&apos;t be other kinds of basic science that could be done in AI, but they don&apos;t know of similarly valuable-looking approaches.)</p>\n<p>We&apos;ve spent a lot of effort (100+ hours) trying to write down more detailed cases for HRAD work. This time included conversations with MIRI, conversation among Open Phil staff and technical advisors, and writing drafts of these arguments. These other cases didn&apos;t feel like they captured MIRI&apos;s views very well and were not very understandable or persuasive to me and other Open Phil staff members, so I&apos;ve fallen back on this simpler case for now when thinking about HRAD work.</p>\n<h2>3. What do I think about HRAD?</h2>\n<p>I have several points of agreement with MIRI&apos;s basic case:&#xA0;</p>\n<ul>\n<li>I agree that existing formalisms like AIXI, Solomonoff induction, and causal decision theory are unsatisfying as descriptions of how an AI system should reason and make decisions, and I agree with most (maybe all) of the ways that MIRI thinks they are unsatisfying.</li>\n<li>I agree that advanced AI is likely to have a huge impact on the world, and that for certain advanced AI systems there will be a point after which we won&apos;t be able to intervene.</li>\n<li>I agree that some plausible kinds of mistakes in an AI system&apos;s design would cause catastrophic misalignment.</li>\n<li>I agree that without some kind of description of &quot;what an advanced AI system is doing&quot; that makes us confident that it will be aligned, we should be very worried that it will cause a catastrophe.&#xA0;</li>\n</ul>\n<p>The fact that MIRI researchers (who are thoughtful, very dedicated to this problem, aligned with our values, and have a good track record in thinking about existential risks from AI) and some others in the effective altruism community are significantly more positive than I am about HRAD is an extremely important factor to me in favor of HRAD. These positive views significantly raise the minimum credence I&apos;m willing to put on HRAD research being very helpful.</p>\n<p>In addition to these positive factors, I have several reservations about HRAD work. In relation to the basic case, these reservations make me think that HRAD isn&apos;t likely to be significantly helpful for getting a confidence-generating description of how an advanced AI system reasons and makes decisions.</p>\n<p>1. It seems pretty likely that early advanced AI systems won&apos;t be understandable in terms of HRAD&apos;s formalisms, in which case HRAD won&apos;t be useful as a description of how these systems should reason and make decisions.</p>\n<p>Note: I&apos;m not sure to what extent MIRI and I disagree about how likely HRAD is to be applicable to early advanced AI systems. It may be that our overall disagreement about HRAD is more about the feasibility of other AI alignment research options (see 3 below), or possibly about strategic questions outside the scope of this document (e.g. to what extent we should try to address potential risks from advanced AI through strategy, policy, and outreach rather than through technical research).</p>\n<p>2. HRAD has gained fewer strong advocates among AI researchers than I&apos;d expect it to if it were very promising -- including among AI researchers whom I consider highly thoughtful about the relevant issues, and whom I&apos;d expect to be more excited if HRAD were likely to be very helpful.</p>\n<p>Together, these two concerns give me something like a 20% credence that if HRAD work reached a high level of maturity (and relatively little other AI alignment research were done) HRAD would significantly help AI researchers build aligned AI systems around the time it becomes possible to build any advanced AI system.</p>\n<p>3. The above considers HRAD in a vacuum, instead of comparing it to other AI alignment research options. My understanding is that MIRI thinks it is very unlikely that other AI alignment research can make up for a lack of progress in HRAD. I disagree; HRAD looks significantly less promising to me (in terms of solving object-level alignment problems, ignoring factors like field-building value) than learning to reason and make decisions from human-generated data (described more below), and HRAD seems unlikely to be helpful on the margin if reasonable amounts of other AI alignment research is done.</p>\n<p>This reduces my credence in HRAD being very helpful to around 10%. I think this is the decision-relevant credence.</p>\n<p>In the next few sections, I&apos;ll go into more detail about the factors I just described. Afterward, I&apos;ll say what I think this implies about how much we should support HRAD research, briefly summarizing the other factors that I think are most relevant.</p>\n<h3>3a. Low credence that HRAD will be applicable (25%?)</h3>\n<p>The basic case for HRAD being helpful depends on HRAD producing a description of how an AI system should reason and make decisions that can be productively applied to advanced AI systems. In this section, I&apos;ll describe my reasons for thinking this is not likely. (As noted above, I&apos;m not sure to what extent MIRI and I disagree about how likely HRAD is to be applicable to early advanced AI systems; nevertheless, it&apos;s an important factor in my current beliefs about the value of HRAD work.)&#xA0;</p>\n<p>I understand HRAD work as aiming to describe basic aspects of reasoning and decision-making in a complete, principled, and theoretically satisfying way, and ideally to have arguments that no other description is more satisfying. I&apos;ll refer to this as a &quot;complete axiomatic approach,&quot; meaning that an end result of HRAD-style research on some aspect of reasoning would be a set of axioms that completely describe that aspect and that are chosen for their intrinsic desirability or for the desirability of the properties they entail. This property of HRAD work is the source of several of my reservations:</p>\n<ul>\n<li>I haven&apos;t found any instances of complete axiomatic descriptions of AI systems being used to mitigate problems in those systems (e.g. to predict, postdict, explain, or fix them) or to design those systems in a way that avoids problems they&apos;d otherwise face. AIXI and Solomonoff induction are particularly strong examples of work that is very close to HRAD, but don&apos;t seem to have been applicable to real AI systems. While I think the most likely explanation for this lack of precedent is that complete axiomatic description is not a very promising approach, it could be that not enough effort has been spent in this direction for contingent reasons; I think that attempts at this would be very informative about HRAD&apos;s expected usefulness, and seem like the most likely way that I&apos;ll increase my credence in HRAD&apos;s future applicability. (Two very accomplished machine learning researchers have told me that AIXI is a useful source of inspiration for their work; I think it&apos;s plausible that e.g. logical uncertainty could serve a similar role, but this is a much weaker case for HRAD than the one I understand MIRI as making.) If HRAD work were likely to be applicable to advanced AI systems, it seems likely to me that some complete axiomatic descriptions (or early HRAD results) should be applicable to current AI systems, especially if advanced AI systems are similar to today&apos;s.</li>\n<li>From conversations with researchers and from my own familiarity with the literature, my understanding is that it would be extremely difficult to relate today&apos;s cutting-edge AI systems to complete axiomatic descriptions. It seems to me that very few researchers think this approach is promising relative to other kinds of theory work, and that when researchers have tried to describe modern machine learning methods in this way, their work has generally not been very successful (compared to other theoretical and experimental work) in increasing researchers&apos; understanding of the AI systems they are developing.</li>\n<li>It seems plausible that the kinds of axiomatic descriptions that HRAD work could produce would be too taxing to be usefully applied to any practical AI system. HRAD results would have to be applied to actual AI systems via theoretically satisfying approximation methods, and it seems plausible that this will not be possible (or that the approximation methods will not preserve most of the desirable properties entailed by the axiomatic descriptions). I haven&apos;t gathered evidence about this question.&#xA0;</li>\n<li>It seems plausible that the conceptual framework and axioms chosen during HRAD work will be very different from the conceptual framework that would best describe how early advanced AI systems work. In theory, it may be possible to describe a recurrent neural network learning to predict future inputs as a particular approximation of Solomonoff induction, but in practice the differences in conceptual framework may be significant enough that this description would not actually be useful for understanding how neural networks work or how they might fail.</li>\n</ul>\n<p>Overall, this makes me think it&apos;s unlikely that HRAD work will apply well to advanced AI systems, especially if advanced AI is reached soon (which would make it more likely to resemble today&apos;s machine learning methods). A large portion of my credence in HRAD being applicable to advanced AI systems comes from the possibility that advanced AI systems won&apos;t look much like today&apos;s. I don&apos;t know how to gain much evidence about HRAD&apos;s applicability in this case.</p>\n<h3>3b. HRAD has few advocates among AI researchers</h3>\n<p>HRAD has gained fewer strong advocates among AI researchers than I&apos;d expect it to if it were very promising, despite other aspects of MIRI&apos;s research (the alignment problem, value specification, corrigibility) being strongly supported by a few prominent researchers. <a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support#Our_investigation_process\">Our review of five of MIRI&apos;s HRAD papers last year</a> provided more detailed examples of how a small number of AI researchers (seven computer science professors, one graduate student, and our technical advisors) respond to HRAD research; these reviews made it seem to us that HRAD research has little potential to decrease potential risks from advanced AI relative to other technical work with the same goal, though we noted that this conclusion was &quot;particularly tentative, and some of our advisors thought that versions of MIRI&#x2019;s research direction could have significant value if effectively pursued&quot;.</p>\n<p>I interpret these unfavorable reviews and lack of strong advocates as evidence that:</p>\n<ol>\n<li>HRAD is less likely to be good basic science of AI; I&apos;d expect a reasonable number of external AI researchers recognize good basic science of AI, even if its aesthetic is fairly different from the most common aesthetics in AI research.</li>\n<li>HRAD is less likely to be applicable to AI systems that are similar to today&apos;s; I would expect applicability to AI systems similar to today&apos;s to make HRAD research significantly more interesting to AI researchers, and our technical advisors agreed strongly that HRAD is especially unlikely to apply to AI systems that are similar to today&apos;s.</li>\n</ol>\n<p>I&apos;m frankly not sure how many strong advocates among AI researchers it would take to change my mind on these points &#x2013; I think a lot would depend on details of who they were and what story they told about their interest in HRAD.</p>\n<p>I do believe that some of this lack of interest should be explained by social dynamics and communication difficulties &#x2013; MIRI is not part of the academic system, and the way MIRI researchers write about their work and motivation is very different from many academic papers, and both of these could cause mainstream AI researchers to be less interested in HRAD research than they would be if these factors weren&apos;t in play. However, I think our review process and conversations with our technical advisors each provide some evidence that this isn&apos;t likely to be sufficient to explain AI researchers&apos; low interest in HRAD.</p>\n<p>Reviewers&apos; descriptions of the papers&apos; main questions, conclusions, and intended relationship to potential risks from advanced AI generally seemed thoughtful and (as far as I can tell) accurate, and in several cases (most notably <a href=\"https://intelligence.org/files/ProofProducingReflection.pdf\">Fallenstein and Kumar 2015</a>) some reviewers thought the work was novel and impressive; if reviewers&apos; opinions were more determined by social and communication issues, I would expect reviews to be less accurate, less nuanced, and more broadly dismissive.</p>\n<p>I only had enough interaction with external reviewers to be moderately confident that their opinions weren&apos;t significantly attributable to social or communication issues. I&apos;ve had much more extensive, in-depth interaction with our technical advisors, and I&apos;m significantly more confident that their views are mostly determined by their technical knowledge and research taste. I think our technical advisors are among the very best-qualified outsiders to assess MIRI&apos;s work, and that they have genuine understanding of the importance of alignment as well as being strong researchers by traditional standards. Their assessment is probably the single biggest data point for me in this section.</p>\n<p>Outside of HRAD, some other research topics that MIRI has proposed have been the subject of much more interest from AI researchers. For example, researchers and students at <a href=\"http://humancompatible.ai/\">CHAI</a> have published papers on and are continuing to work on value specification and error-tolerance (particularly corrigibility), these topics have consistently seemed more promising to our technical advisors, and Stuart Russell has adopted the value alignment problem as a central theme of his work. In light of this, I am more inclined to take AI researchers&apos; lack of interest in HRAD as evidence about its promisingness than as evidence of severe social or communication issues.</p>\n<p>The most convincing argument I know of for not treating other researchers&apos; lack of interest as significant evidence about the promisingness of HRAD research is:</p>\n<ol>\n<li>I&apos;m pretty sure that MIRI&apos;s work on decision theory is a very significant step forward for philosophical decision theory. This is based mostly on conversations with a very small number of philosophers who I know to have seriously evaluated MIRI&apos;s work, partially on an absence of good objections to their decision theory work, and a little on my own assessment of the work (which I&apos;d discard if the first two considerations had gone the other way).</li>\n<li>MIRI&apos;s decision theory work has gained significantly fewer advocates among professional philosophers than I&apos;d expect it to if it were very promising.</li>\n</ol>\n<p>I&apos;m strongly inclined to resolve this conflict by continuing to believe that MIRI&apos;s decision theory work is good philosophy, and to explain 2 by appealing to social dynamics and communication difficulties. I think it&apos;s reasonable to consider an analogous situation with HRAD and AI researchers to be plausible a priori, but the analogue of point 1 above doesn&apos;t apply to HRAD work, and the other reasons I&apos;ve given in this section lead me to think that this is not likely.</p>\n<h3>3c. Other research, especially &quot;learning to reason from humans,&quot; looks more promising than HRAD (75%?)</h3>\n<p>How promising does HRAD look compared to other AI alignment research options? The most significant factor to me is the apparent promisingness of designing advanced AI systems to reason and make decisions from human-generated data (&quot;learning to reason from humans&quot;); if an approach along these lines is successful, it doesn&apos;t seem to me that much room would be left for HRAD to help on the margin. My views here are heavily based on <a href=\"https://ai-alignment.com/\">Paul Christiano&apos;s writing on this topic</a>, but I&apos;m not claiming to represent his overall approach, and in particular I&apos;m trying to sketch out a broader set of approaches that includes Paul&apos;s. It&apos;s plausible to me that other kinds of alignment research could play a similar role, but I have a much less clear picture of how that would work, and finding out about significant problems with learning to reason from humans would make me both more pessimistic about technical work on AI alignment in general and more optimistic that HRAD would be helpful. The arguments in this section are pretty loose, but the basic idea seems promising enough to me to justify high credence that something in this general area will work.</p>\n<p>&quot;Learning to reason from humans&quot; is different from the most common approaches in AI today, where decision-making methods are implicitly learned in the process of approximating some function &#x2013; e.g. a reward-maximizing policy, an imitative policy, a Q-function or model of the world, etc. Instead, learning to reason from humans would involve directly training a system to reason in ways that match human demonstrations or are approved of by human feedback, as in Paul&apos;s article <a href=\"https://ai-alignment.com/approval-directed-algorithm-learning-bf1f8fad42cd\">here</a>.</p>\n<p>If we are able to become confident that an AI system is learning to reason in ways that meet human approval or match human demonstrations, it seems to me that we could also become confident that the AI system would be aligned overall; a very harmful decision would need to be generated by a series of human-endorsed reasoning steps (and unless human reasoning endorses a search for edge cases, edge cases won&apos;t be sought). Human endorsement of reasoning and decision-making could not only incorporate valid instrumental reasoning (in parts of epistemology and decision theory that we know how to formalize), but also rules of thumb and sanity checks that allow humans to navigate uncertainty about which epistemology and decision theory are correct, as well as human value judgements about which decisions, actions, short-term consequences, and long-term consequences are desirable, undesirable, or of uncertain value.</p>\n<p>Another factor that is important to me here is the potential to design systems to reason and make decisions in ways that are calibrated or conservative. The idea here is that we can become more confident that AI systems will not make catastrophic decisions if they can reliably detect when they are operating in unfamiliar domains or situations, have low confidence that humans would approve of their reasoning and decisions, have low confidence in predicted consequences, or are considering actions that could cause significant harm; in those cases, we&apos;d like AI systems to &quot;check in&quot; with humans more intensively and to act more conservatively. It seems likely to me that these kinds of properties would contribute significantly to alignment and safety, and that we could pursue these properties by designing systems to learn to reason and make decisions in human-approved ways, or by directly studying statistical properties like calibration or &quot;conservativeness&quot;.</p>\n<p>&quot;Learning to reason and make decisions from human examples and feedback&quot; and &quot;learning to act &apos;conservatively&apos; where &apos;appropriate&apos;&quot; don&apos;t seem to me to be many orders of magnitude more difficult than the kinds of learning tasks AI systems are good at today. If it was necessary for an AI system to imitate human judgement perfectly, I would be much more skeptical of this approach, but that doesn&apos;t seem to be necessary, as <a href=\"https://ai-alignment.com/act-based-agents-8ec926c79e9c\">Paul argues</a>:</p>\n<p>&quot;You need only the vaguest understanding of humans to guess that killing the user is: (1) not something they would approve of, (2) not something they would do, (3) not in line with their instrumental preferences.</p>\n<p>So in order to get bad outcomes here you have to really mess up your model of what humans want (or more likely mess up the underlying framework in an important way).</p>\n<p>If we imagine a landscape of possible interpretations of human preferences, there is a &apos;right&apos; interpretation that we are shooting for. But if you start with a wrong answer that is anywhere in the neighborhood, you will do things like &apos;ask the user what to do, and don&#x2019;t manipulate them.&apos; And these behaviors will eventually get you where you want to go.</p>\n<p>That is to say, the &apos;right&apos; behavior is surrounded by a massive crater of &apos;good enough&apos; behaviors, and in the long-term they all converge to the same place. We just need to land in the crater.&quot;</p>\n<p>Learning to reason from humans is a good fit with today&apos;s AI research, and is broad enough that it would be very surprising to me if it were not productively applicable to early advanced AI systems.</p>\n<p>It seems to me that this kind of approach is also much more likely to be robust to unanticipated problems than a formal, HRAD-style approach would be, since it explicitly aims to learn how to reason in human-endorsed ways instead of relying on researchers to notice and formally solve all critical problems of reasoning before the system is built. There are significant open questions about whether and how we could make machine learning robust and theoretically well-understood enough for high confidence, but it seems to me that this will be the case for any technical pathway that relies on learning about human preferences in order to act desirably.</p>\n<p>Finally, it seems to me that if a lack of HRAD-style understanding does leave us exposed to many important &quot;unknown unknown&quot; problems, there is a good chance that some of those problems will be revealed by failures or difficulties in achieving alignment in earlier AI systems, and that researchers who are actively thinking about the goal of aligning advanced AI systems will be able to notice these failings and relate them to a need for better HRAD-style understanding. This kind of process seems very likely to be applicable to learning to reason from humans, but could also apply to other approaches to AI alignment. I do not think that this process is guaranteed to reveal a need for HRAD-style understanding in the case that it is needed, and I am fairly sure that some failure modes will not appear in earlier advanced AI systems (the failure modes Bostrom calls &quot;treacherous turns&quot;, which only appear when an AI system has a large range of general-purpose capabilities, can reason very powerfully, etc.). It&apos;s possible that earlier failure modes will be too rare, too late, or not clearly enough related to a need for HRAD-style research. However, if a lack of fundamental understanding does expose us to many important &quot;unknown unknown&quot; failure modes, it seems more likely to me that some informative failures will happen early than that all such failures will appear only after systems are advanced enough to be extremely high-impact, and that researchers motivated by alignment of advanced AI will notice if those failures could be addressed through HRAD-style understanding. (I&apos;m uncertain about how researchers who aren&apos;t thinking actively about alignment of advanced AI would respond, and I think one of the most valuable things we can do today is to increase the number of researchers who are thinking actively about alignment of advanced AI and are therefore more likely to respond appropriately to evidence.)</p>\n<p>My credence for this section isn&apos;t higher for three basic reasons:</p>\n<ul>\n<li>It may be significantly harder to build an aligned AI system that&apos;s much more powerful than a human if we use learned reasoning rules instead of formally specified ones. Very little work has been done on this topic.</li>\n<li>It may be that some parts of HRAD &#x2013; e.g. logical uncertainty or benign universal priors &#x2013; will turn out to be necessary for reliability. This currently looks unlikely to me, but seems like the main way that parts of HRAD could turn out to be prerequisites for learning to reason from humans.</li>\n<li>Unknown unknowns; my arguments in this section are pretty loose, and little work has been done on this topic.</li>\n</ul>\n<h3>3d. MIRI staff are thoughtful, aligned with our values, and have a good track record</h3>\n<p>As I noted above, I believe that MIRI staff are thoughtful, very dedicated to this problem, aligned with our values, and have a good track record in thinking about existential risk from AI. The fact that some of them are much more optimistic than I am about HRAD research is a very significant factor in favor of HRAD. I think it would be incorrect to place a very low credence (e.g. 1%) on their views being closer to the truth than mine are.</p>\n<p>I don&apos;t think it is helpful to try to list a large amount of detail here; I&apos;m including this as its own section in order to emphasize its importance to my reasoning. My views come from many in-person and online conversations with MIRI researchers over the past 5 years, reports of many similar conversations by other thoughtful people I trust, and a large amount of online writing about existential risk from AI spread over several sites, most notably LessWrong.com, agentfoundations.org, arbital.com, and intelligence.org.</p>\n<p>The most straightforward thing to list is that MIRI was among the first groups to strongly articulate the case for existential risk from artificial intelligence and the need for technical and strategic research on this topic, <a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support#Early_articulation_of_the_value_alignment_problem\">as noted in our last writeup</a>:</p>\n<p>&quot;We believe that MIRI played an important role in publicizing and sharpening the value alignment problem. This problem is described in the introduction to MIRI&#x2019;s Agent Foundations technical agenda. We are aware of MIRI writing about this problem publicly and in-depth as early as 2001, at a time when we believe it received substantial attention from very few others. While MIRI was not the first to discuss potential risks from advanced artificial intelligence, we believe it was a relatively early and prominent promoter, and generally spoke at more length about specific issues such as the value alignment problem than more long-standing proponents.&quot;</p>\n<h2>4. How much should Open Phil support HRAD work?</h2>\n<p>My 10% credence that &quot;if HRAD reached a high level of maturity it would significantly help AI researchers build aligned AI systems&quot; doesn&apos;t fully answer the question of how much we should support HRAD work (with our funding and with our outreach to researchers) relative to other technical work on AI safety. It seems to me that the main additional factors are:</p>\n<p><strong>Field-building value:</strong> I expect that the majority of the value of our current funding in technical AI safety research will come from its effect of increasing the total number of people who are deeply knowledgeable about technical research on artificial intelligence and machine learning, while also being deeply versed in issues relevant to potential risks. HRAD work appears to be significantly less useful for this goal than other kinds of AI alignment work, since HRAD has not gained much support among AI researchers. (I do think that in order to be effective for field-building, AI safety research directions should be among the most promising we can think of today; this is not an argument for work on non-promising, but attractive &quot;AI safety&quot; research.)</p>\n<p><strong>Replaceability:</strong> HRAD work seems much more likely than other AI alignment work to be neglected by AI researchers and funders. If HRAD work turns out to be significantly helpful, we could make a significant counterfactual difference by supporting it.</p>\n<p><strong>Shovel-readiness:</strong> My understanding is that HRAD work is currently funding-constrained (i.e. MIRI could scale up its program given more funds). This is not generally true of technical AI safety work, which in my experience has also required significant staff time.</p>\n<p>The difference in field-building value between HRAD and the other technical AI safety work we support makes me significantly more enthusiastic about supporting other technical AI safety work than about supporting HRAD. However, HRAD&apos;s low replaceability and my 10% credence in HRAD being useful make me excited to support at least some HRAD work.</p>\n<p>In my view, enough HRAD work should be supported to continue building evidence about its chance of applicability to advanced AI, to have opportunities for other AI researchers to encounter it and become advocates, and to generally make it reasonably likely that if it is more important than it currently appears then we can learn this fact. MIRI&apos;s current size seems to me to be approximately right for this purpose, and as far as I know MIRI staff don&apos;t think MIRI is too small to continue making steady progress. Given this, I am ambivalent (along the lines of <a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support\">our previous grant writeup</a>)&#xA0;about recommending that Good Ventures funds be used to increase MIRI&apos;s capacity for HRAD research.</p></body></html>", "user": {"username": "Daniel_Dewey"}}, {"_id": "L32odKCdp7xqJ3p5w", "title": "Testing an EA network-building strategy in the Netherlands", "postedAt": "2017-07-03T11:28:33.393Z", "htmlBody": "<html><body><p><span>Last January, </span><a href=\"http://effectiefaltruisme.nl/en/effective-altruism-netherlands/#team\"><span>Effective Altruism Netherlands</span></a><span> (EAN) became a registered charity in the Netherlands. The organisation consists of a three-person board and two full-time employees, Sjir Hoeijmakers and yours truly (Remmelt Ellen). </span><span>Note: as of writing, we are still fundraising to cover our salaries.</span><span><br><br></span></p>\n<p><span>On 28 May, we publicly launched with a large event. Since then, we have pivoted from supporting EA projects to collaborating with EA networks in the Netherlands.</span><span><br></span><span><br></span><span>This post outlines our new strategy. We would appreciate your feedback in the comments, to enable us to improve it further. </span></p>\n<p>&#xA0;</p>\n<p><span><br></span><span>Terminal Goal</span></p>\n<p><span>Have as much positive impact as possible on the lives of others.</span></p>\n<p><span><br></span><span>Instrumental goal</span></p>\n<p><span>Engage impactful and potentially impactful individuals in the Netherlands with effective altruism and stimulate and facilitate them to bring effective altruism into practice.</span><span><br><br></span></p>\n<p><span>Strategy</span></p>\n<ol>\n<li>\n<p><span>Work with selected &#x2018;network builders&#x2019; to build networks of self-identified effective altruists by </span></p>\n</li>\n<ol>\n<li>\n<p><span>helping define targets of the network</span></p>\n</li>\n<li>\n<p><span>helping set up the network infrastructure</span></p>\n</li>\n<li>\n<p><span>providing resources that differentiate effective altruism and the effective altruism community.</span></p>\n</li>\n</ol>\n<li>\n<p><span>Do targeted EA outreach to fill these networks with qualified individuals</span></p>\n</li>\n<li>\n<p><span>Provide a national infrastructure for these networks to connect with each other to share motivation, knowledge, skills and other resources</span></p>\n</li>\n<li>\n<p><span>Be the point of contact for anything EA-related in the Netherlands to protect the correct use of the concept &#x2018;effective altruism&#x2019;</span></p>\n</li>\n</ol>\n<p>&#xA0;</p>\n<p><span>Before the pivot, we had a tendency to jump on an appealing opportunity, construct a concrete project framework and then look around for people who could carry it out. &#xA0;It was difficult to find people with a good personal fit and the project&#x2019;s impact was limited by its duration. </span><span><br></span><span>We now focus on selecting </span><a href=\"http://www.paulgraham.com/founders.html\"><span>people</span></a><span> first &#x2013;&#xA0;those who we deem both EA-aligned and highly-capable of building EA networks within promising cause and talent areas, based on their track record and our experience in working with them. Note that with &#x2018;self-identified effective altruists&#x2019; we mean network members who identify with and strive after </span><a href=\"https://docs.google.com/document/d/1tvw5HsxNvAMNyITOy78bN7Jmfw7-XDNjhSAXd5jDMJg/edit#heading=h.x29zl4zf974z\"><span>CEA&#x2019;s Guiding Principles</span></a><span>.</span></p>\n<p>&#xA0;</p>\n<p><span>Assumptions</span></p>\n<ol>\n<li>\n<p><span>We can have more impact by making others more effective than by doing direct work ourselves.</span></p>\n</li>\n</ol>\n<p><span>The main argument for this is the </span><a href=\"https://80000hours.org/problem-profiles/promoting-effective-altruism/#multiplier-effect\"><span>multiplier effect</span></a><span>.</span></p>\n<p>&#xA0;</p>\n<ol>\n<li>\n<p><span>We can have more impact by getting people fully aligned with effective altruism than by spreading parts of the effective altruism idea (e.g. cause-specific optimisation).</span></p>\n</li>\n</ol>\n<p><span>Here are two arguments for this: </span></p>\n<ol>\n<li>\n<p><span>Impact being created by the </span><a href=\"https://youtu.be/TH4_ikhAGz0?t=10m38s\"><span>multiplication of parameters</span></a><span>. </span><span><br></span><span>e.g. open-mindedness without scientific rigour leads to failure. </span></p>\n</li>\n<li>\n<p><span>There being a log/power law distribution of impact </span><a href=\"https://80000hours.org/articles/cause-selection/\"><span>across cause areas</span></a><span>, just like there is across interventions within a particular cause area.</span></p>\n</li>\n</ol>\n<p>&#xA0;</p>\n<ol>\n<li>\n<p><span>We can have more impact by focusing on individuals directly and on organisations indirectly rather than on organisations directly.</span></p>\n</li>\n</ol>\n<p><span>The underlying idea is that rather than focusing our efforts on a representative who has more say in the organisation&#x2019;s direction but is less likely to be particularly aligned with EA values, we should instead focus on people within the organisation who are. </span></p>\n<p>&#xA0;</p>\n<ol>\n<li>\n<p><span>We can have more impact by focusing on (potentially) impactful individuals than on the general public.</span></p>\n</li>\n</ol>\n<p><span>With &#x2018;impactful&#x2019;, we mean people who we deem capable of having an outsized positive impact. The argument here is to spend most of our resources on identifying and targeting those people instead of undirected broad outreach in the hope that a tiny portion of people who hear from us work their way up the </span><a href=\"/ea/11h/how_to_measure_and_optimize_ea_marketing/\"><span>ladder of engagement</span></a><span> to become EA-aligned. </span></p>\n<p>&#xA0;</p>\n<ol>\n<li>\n<p><span>We can have more impact by focusing on the Netherlands than on other geographic areas.</span></p>\n</li>\n</ol>\n<p><span>In the Netherlands, only a tiny minority know of the term &#x2018;effective altruism&#x2019;, meaning that there are still many low-hanging fruits to pick. We also have a geographic advantage because we speak Dutch and are able to connect in-person in this densely populated country. More tentatively, Dutch culture seems suitable for effective altruism, as the Dutch are seen to be pragmatic, cosmopolitan/ outward-looking, and consistently </span><a href=\"https://en.wikipedia.org/wiki/World_Giving_Index#World_Giving_Index_rankings\"><span>rank high</span></a><span> at philanthropic giving.</span></p>\n<p>&#xA0;</p>\n<p><span>Metrics</span></p>\n<p><span>These are </span><a href=\"https://www.effectivealtruism.org/articles/why-nonprofits-should-apply-to-y-combinator/#relevance-of-advice\"><span>key metrics</span></a><span> selected to track EAN&#x2019;s underlying impact (i.e. wellbeing-adjusted life years) as best as we can to compare with the opportunity costs of the money spent by donors and the time spent by EAN. To test and adapt our strategy, we are also setting falsifiable hypotheses for each link in the chain of processes needed to build impact.</span></p>\n<ol>\n<li>\n<p><span>Inputs: </span></p>\n</li>\n<ol>\n<li>\n<p><span>Time committed by EAN </span></p>\n</li>\n<li>\n<p><span>Money spent by EAN</span><span><br><br></span></p>\n</li>\n</ol>\n<li>\n<p><span>Outputs</span></p>\n</li>\n<ol>\n<li>\n<p><span>Behavioural change: impact-adjusted significant plan changes (</span><a href=\"https://80000hours.org/2016/07/update-on-number-of-significant-plan-changes/#impact-adjustment-of-significant-plan-changes\"><span>IASPC</span></a><span>) attributable to EAN/ &#x20AC;1,000</span></p>\n</li>\n</ol>\n</ol>\n<p><span>As an ambitious benchmark for how well we&#x2019;re doing, we intend to use 80,000 Hours&#x2019; trajectory after it officially started in Nov 2011. Their current output is around </span><a href=\"https://80000hours.org/2016/12/has-80000-hours-justified-its-costs/#whats-the-marginal-cost-per-plan-change\"><span>4 IASPC/&#x20AC;1,000</span></a><span> spent </span><span>(probably an overestimate)</span><span>. If we assume that this figure stood at 1 IASPC/&#x20AC;1,000 in its first year, that&#x2019;s what we should be aiming for now for the networks we&#x2019;re helping build (a more rigorous approach would also include estimated salary opportunity costs).</span><span><br><br></span></p>\n<ol>\n<ol>\n<li>\n<p><span>Belief change: increase in the number of self-identified effective altruists</span></p>\n</li>\n</ol>\n</ol>\n<p><span>This is about tracking whether people within the EA Networks have an applied understanding of </span><a href=\"https://docs.google.com/document/d/1tvw5HsxNvAMNyITOy78bN7Jmfw7-XDNjhSAXd5jDMJg/edit#heading=h.x29zl4zf974z\"><span>CEA&#x2019;s Guiding Principles</span></a><span>, possibly through periodic surveys. This is to ensure that the behavioural change we see is not fragile. i.e. that people within the Dutch EA community are able to update with new evidence over the next decades.</span><span><br><br></span></p>\n<p><span>Current collaborations</span></p>\n<p><span>Here are three networks that we&#x2019;re starting collaborations with (network builders in brackets):</span></p>\n<ol>\n<ol>\n<li>\n<p><a href=\"https://www.effectivegiving.nl/english/\"><span>Effective Giving</span></a><span> (Kellie Liket &amp; Robert Boogaard)</span></p>\n</li>\n</ol>\n</ol>\n<p><span>Quote: &#x201C;Effective Giving is a community of foundations and large philanthropists learning together how to apply our unique resources to make the maximum contribution to a better world.&#x201D; &#xA0;</span><span><br></span><span>In our view, this is an example of a well-functioning network with which we have had fruitful collaborations (Kellie and Robert have also supported the development of EAN from the start).</span><span><br><br></span></p>\n<ol>\n<ol>\n<li>\n<p><span>Career Workshops (</span><a href=\"https://translate.google.com/translate?hl=nl&amp;sl=nl&amp;tl=en&amp;u=https%3A%2F%2Ftalent2gather.nl%2Fwie-zijn-wij%2F\"><span>Alje van den Bosch &amp; Ya&#xEB;l Duindam</span></a><span>)</span></p>\n</li>\n</ol>\n</ol>\n<p><span>This is a network that develops and gives career workshops in the Netherlands and would help connect potentially impactful people to other Dutch EA networks. The biggest bottleneck right now to Alje and Ya&#xEB;l working on this at least part-time is building a sustainable revenue model.</span></p>\n<p>&#xA0;</p>\n<ol>\n<ol>\n<li>\n<p><a href=\"https://wiki.lesswrong.com/wiki/Accelerating_AI_Safety_Adoption_in_Academia\"><span>Road to AI Safety Excellence [RAISE]</span></a><span> (Toon Alfrink)</span></p>\n</li>\n</ol>\n</ol>\n<p><span>Quote: &#x201C;RAISE... is an initiative to improve the pipeline for AI safety researchers. The road to research-level understanding is hard and uncertain. We want it to be easy and clear. To this end, we are creating a MOOC.&#x201D;</span><span><br></span><span>This network has just started recording its first lecture and is still looking for </span><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdEbO3ViOeJeYHAIUHRhIWRhZHmvJQliW23oXRFeVUfAHRiJw/viewform?c=0&amp;w=1\"><span>volunteers</span></a><span>.</span><span><br><br></span></p>\n<p><span>Here are potential network collaborations that we&#x2019;re exploring right now:</span></p>\n<ul>\n<ul>\n<li>\n<p><span>Local/student groups </span><span><br></span><span>These would clearly define their target groups and offer newcomers a path to learn about EA principles and build up their capacity to do good (established groups like EA London and EA Berkeley are inspirations to us here).</span><span><br><br></span></p>\n</li>\n<li>\n<p><span>Animal Welfare Research network </span><span><br></span><span>This network would compile and produce rigorous animal welfare intervention research that can be applied by Dutch and international charities.</span><span><br><br></span></p>\n</li>\n<li>\n<p><span>Rationality hub</span><span><br></span><span>This network would enable (potentially) impactful individuals in the Netherlands to improve their epistemic and instrumental rationality.</span><span><br><br></span></p>\n</li>\n<li>\n<p><span>Startup Founders network</span><span><br></span><span>This network would support the creation of effective for-profit and nonprofit startups.</span><span><br><br></span></p>\n</li>\n<li>\n<p><span>Corporate Intrapreneurs network</span><span><br></span><span>Similar to the work of </span><a href=\"http://eaworkplaceactivism.org/\"><span>EA Workplace Activism</span></a><span>, these would be entrepreneurial employees who spread EA thinking within companies through EA projects, charity drives, changes in pension funds investments and the like.</span><span><br><br></span></p>\n</li>\n<li>\n<p><span>Policy network</span><span><br></span><span>This network would search for the most high-impact lobby opportunities for improving Dutch government or EU-wide policy and start projects to implement them.</span><span><br><br></span></p>\n</li>\n</ul>\n</ul>\n<p><span>Examples of actions</span></p>\n<p><span>Concretely, here are examples of actions that EAN would and would not take based on this strategy:</span></p>\n<p><span>Would do</span><span>:</span></p>\n<ul>\n<li>\n<p><span>Sit down with a high-potential and highly committed person working in a corporation to set specific targets (e.g. moving money of corporations&apos; foundations) and plan out when and how their Corporate EA network would communicate (e.g. arrange meeting spot, online platform) to reach these targets.</span></p>\n</li>\n<li>\n<p><span>Suggest to a Startup Founders network to organise a cause prioritisation training and connect them with an experienced trainer or online self-learning material for this.</span></p>\n</li>\n<li>\n<p><span>Connect an AI Safety Research network with a Policy network through a shared event where the former informs the latter about promising policies to pursue.</span></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<p><span>Would not do (although someone in a network could)</span><span>:</span></p>\n<ul>\n<li>\n<p><span>Run a campaign to promote GiveWell-recommended charities.</span></p>\n</li>\n<li>\n<p><span>Do research into the most effective ways to lobby the Dutch government or the European Union.</span></p>\n</li>\n<li>\n<p><span>Incubate an effective NGO startup.</span></p>\n</li>\n<li>\n<p><span>Work with existing organisations or networks to improve their impact</span></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p><span><br></span><span>To avoid becoming long-winded in this post, we left out tactical details.</span><span><br></span><span>Feel free though to write out your questions and feedback below to improve this strategy.</span></p></body></html>", "user": {"username": "remmelt"}}, {"_id": "bwoqzGiz9NNgowsGt", "title": "Changes to the EA Forum", "postedAt": "2017-07-02T17:34:18.441Z", "htmlBody": "<html><body><p><span>Over the last several years, the EA Forum has been run on a volunteer-led basis. Given how much the EA community has grown, the volunteers who have been running the Forum have decided to transition primary responsibility for the &#xA0;EA Forum to the Centre for Effective Altruism. In practice, this should mean that things run much as they have done, but means that volunteers no longer have to be responsible for coordinating the technical maintenance of the Forum. </span></p>\n<p>&#xA0;</p>\n<p><span>In the near term, we plan to maintain the Forum largely as-is. One change we do plan to make is to change the primary domain from effective-altruism.com to forum.effectivealtruism.org for more consistency across the websites that make up the effective altruism ecosystem. However, we&#x2019;ll ensure that the old domain continues to work and that all existing links are redirected, as don&#x2019;t want anything about this transition to disrupt the functioning of the Forum.</span></p>\n<p>&#xA0;</p>\n<p><span>Who&#x2019;s doing what</span></p>\n<p><span>Trike Apps set up the Forum and will continue to provide free hosting for the Forum </span><span>&#x2014;</span><span> we thank them for providing this! </span></p>\n<p>&#xA0;</p>\n<p><span>Thanks to Ryan Carey for founding, managing, and moderating the Forum! Thanks to Rethink Charity (formerly .impact), including Tom Ash, Peter Hurford, Patrick Brinich-Langlois and many others, for maintaining and improving the Forum over the years. Going forward, volunteers from Rethink Charity will continue helping with tech maintenance.</span></p>\n<p>&#xA0;</p>\n<p><span>Thanks to outgoing moderators Rebecca Raible and Alison Woodman for moderating over the past years! Larissa Hesketh-Rowe and I are coming on as the new moderators. We&#x2019;d like to have more than two moderators, and especially moderators who aren&#x2019;t CEA staff. If you&#x2019;d like to volunteer or nominate someone, please let us know at forum@effectivealtruism.org.</span></p>\n<p>&#xA0;</p>\n<p><span>We look forward to helping the Forum continue its role as a key place for original content and discussion!</span></p></body></html>", "user": {"username": "Julia_Wise"}}, {"_id": "bdssHHKSze7irv8zB", "title": "Strategic implications of AI scenarios", "postedAt": "2017-06-29T07:31:27.891Z", "htmlBody": "<html><body><p><em>[Originally posted on my new <a href=\"http://prioritizationresearch.com/\">website on cause prioritization</a>. This article is an introductory exploration of what different AI scenarios imply for our strategy in shaping advanced AI and might be interesting to the broader EA community, which is why I crosspost it here.]</em></p>\n<p>Efforts to mitigate the risks of advanced artificial intelligence <a href=\"http://prioritizationresearch.com/should-altruists-focus-on-artificial-intelligence/\">may be a top priority for effective altruists</a>. If this is true, what are the best means to shape AI? Should we write math-heavy papers on open technical questions, or opt for broader, non-technical interventions like values spreading?</p>\n<p>The answer to these questions hinges on how we expect AI to unfold. That is, what do we expect advanced AI to look like, and how will it be developed?</p>\n<p>Many of these issues have been <a href=\"https://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">discussed at length</a>, but the implications for the action-guiding question of how to best work on the problem often remain unclear. This post aims to fill the gap with a rigorous analysis of how different views on AI scenarios relate to the possible ways to shape advanced AI.</p>\n<h1>Key questions</h1>\n<p>We can slice the space of possible scenarios in infinitely many ways, some of which are more useful for our thinking than others. Commonly discussed questions about AI scenarios include:</p>\n<ul>\n<li>When will humanity build <em>general</em> artificial intelligence, assuming that it happens at all?</li>\n<li>Will the takeoff be <a href=\"https://wiki.lesswrong.com/wiki/AI_takeoff\">hard or soft</a>? That is, how long will it take to get from a human-level AI to a superintelligence?</li>\n<li>To what extent will the goals of an AI be aligned with human values?</li>\n<li>What architecture will be used to build advanced AI? For instance, will it use an explicit utility function or a reward module? Will it be based on &#x201C;clean&#x201D; mathematical principles or on a &#x201C;messy&#x201D; collection of heuristics?</li>\n<li>Will advanced AI act as a single agent, as <a href=\"https://intelligence.org/\">MIRI</a>&#x2019;s models tend to assume, or will superintelligence reside in a distributed system <a href=\"https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering#AI_More_like_the_economy_than_like_robots\">like the economy</a>?</li>\n</ul>\n<p>The reason why we ask these questions is that the answers determine how we should work on the problem. We can choose from a plethora of possible approaches:</p>\n<ul>\n<li>We might work on technical aspects of the <a href=\"https://arbital.com/p/ai_alignment/\">AI alignment problem</a>.</li>\n<li>We could do other kinds of technical research, such as finding <a href=\"https://ai-alignment.com/a-possible-stance-for-ai-control-research-fe9cf717fc1b\">scalable solutions to short-term problems</a> or specifically trying to <a href=\"https://foundational-research.org/suffering-focused-ai-safety-why-fail-safe-measures-might-be-our-top-intervention/\">prevent the worst possible outcomes</a>.</li>\n<li>We could focus on philosophical and conceptual work to raise awareness of AI-related issues.</li>\n<li>We could work on <a href=\"https://80000hours.org/articles/ai-policy-guide/\">AI policy or AI strategy</a>.</li>\n<li>Instead of shaping AI directly, we might opt for broader, more indirect interventions such as improving international cooperation and spreading altruistic values.</li>\n</ul>\n<h1>Which factors determine the value of technical AI safety work?</h1>\n<p>To avoid the complexity of considering many strategic questions at the same time, I will focus on whether we should work on AI in a technical or non-technical way, which I believe to be the most action-guiding dimension.</p>\n<h2>The control problem</h2>\n<p>The value of technical work depends on whether it is possible to find well-posed and tractable technical problems whose solution is essential for a positive AI outcome. The most common candidate for this role is the <a href=\"https://arbital.com/p/ai_alignment/\">control problem</a> (and subproblems thereof), or how to make superintelligent AI systems act in accordance with human values. The viability of technical work therefore depends to some extent on whether it makes sense to think about AI in this way &#x2013; that is, whether the control problem is of central importance.</p>\n<p>This, in turn, depends on our outlook on AI scenarios. For instance, we might think that the technical side of AI safety may be less difficult than it seems, that they will likely be solved anyway, or that the most serious risks may instead be related to security aspects, <a href=\"https://bashibazuk.wordpress.com/2017/03/28/utopia-in-the-fog/\">coordination problems</a>, and selfish values.</p>\n<p>The following views support work on the control problem:</p>\n<ul>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Unfriendly_artificial_intelligence\">Uncontrolled AI</a> is the &#x201C;default outcome&#x201D; or at least somewhat likely, which makes technical work on the problem a powerful lever for influencing the far future. Also, uncontrolled AI would mean that human values will matter less in the future, which renders many other interventions &#x2013; such as values spreading &#x2013; futile.</li>\n<li>A hard takeoff or <a href=\"https://wiki.lesswrong.com/wiki/Intelligence_explosion\">intelligence explosion</a> is likely. This matters because it correlates with the likelihood of uncontrolled AI. Also, it might mean that humans will quickly be &#x201C;out of the loop&#x201D;, making it more difficult to shape AI with non-technical means.</li>\n<li>We cannot rule out very short timelines. In this case, the takeoff will be unexpected and research on AI safety will be more neglected, which means that we can have a larger impact.</li>\n</ul>\n<p>In contrast, if AI is <a href=\"https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering#AI_More_like_the_economy_than_like_robots\">like the economy</a>, then the control problem does not apply in its usual form &#x2013; there is no unified agent to control. Influencing the technical development of AI would be harder because of its gradual nature, just as it was arguably <a href=\"http://prioritizationresearch.com/should-altruists-focus-on-artificial-intelligence/#Effective_altruists_in_the_past\">difficult to influence industrialization in the past</a>.</p>\n<p>It is often argued that an agent-like superintelligence would ultimately emerge even if AI takes a different form at first. I think this is likely, but not certain. But even so, the strategic picture is radically different if economy-like AI comes first. This is because we can mainly hope to (directly) shape <em>the first</em> kind of advanced AI since it is hard to predict, and hard to influence, what happens afterward.</p>\n<p>In other words, the first transition may constitute an &#x201C;<a href=\"https://en.wikipedia.org/wiki/Event_horizon\">event horizon</a>&#x201D; and therefore be most relevant to strategic considerations. For example, if agent-like AI is built second, then the first kind of advanced AI systems will be the driving force. They will be intellectually superior to us by many orders of magnitude, which makes it all but impossible to (directly) influence the agent-like AI via technical work.</p>\n<h2>How much safety work will be done anyway?</h2>\n<p>This brings us to another intermediate variable, namely how much technical safety work will be done by others anyway. If the timeline to AI is long, if the takeoff is soft, or if AI is <a href=\"https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering#AI_More_like_the_economy_than_like_robots\">like the economy</a>, then large amounts of money and skilled time may be dedicated to AI safety, comparable to contemporary mainstream discussion of climate change.</p>\n<p>As AI is applied to more and more industrial contexts, large-scale failures of AI systems will likely become dangerous or costly, so we can expect that the AI industry will be forced to make them safe, either because their customers demand it or because of regulation. We may also experience an <a href=\"https://wiki.lesswrong.com/wiki/AGI_Sputnik_moment\">AI Sputnik moment</a> that leads to more investment in safety research.</p>\n<p>Since the resources of effective altruists are small in comparison to large companies and governments, this scenario reduces the value of technical AI safety work. Non-technical approaches such as spreading altruistic values among AI researchers or <a href=\"https://80000hours.org/articles/ai-policy-guide/\">work on AI policy</a> might be more promising in these cases. However, the argument does not apply if we are interested in specific questions that would otherwise remain neglected, or if we think that safety techniques will not work anymore once AI systems reach a certain threshold of capability. (It&#x2019;s unclear to what extent this is the case.)</p>\n<p>This shows that how we work on AI depends not only on our predictions of future scenarios, but also on our goals. Personally, I&#x2019;m mostly interested in <a href=\"https://foundational-research.org/suffering-focused-ai-safety-why-fail-safe-measures-might-be-our-top-intervention/\">suffering-focused AI safety</a>, that is, how to prevent <a href=\"https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/\">s-risks</a> of advanced AI. This may lead to slightly different strategic conclusions compared to AI safety efforts that focus on loading human values. For instance, it means that fewer people will work on the issues that matter most to me.</p>\n<p>A related question is whether strong intelligence enhancement, such as emulations or <a href=\"http://www.nickbostrom.com/papers/embryo.pdf\">iterated embryo selection</a>, will become feasible (and is employed) before strong AI is built. In that case, the enhanced minds will likely work on AI safety, too, which might mean that future generations can tackle the problem more effectively (given sufficiently long timelines). In fact, this may be true even without intelligence enhancement because we are <a href=\"http://www.fhi.ox.ac.uk/the-timing-of-labour-aimed-at-reducing-existential-risk/\">nearsighted with respect to time</a>, that is, it is harder to predict and influence events that are further in the future.</p>\n<p>It&#x2019;s not clear whether strong intelligence enhancement technology will be available before advanced AI. But we can view modern tools such as blogs and online forums as a <em>weak form of intelligence enhancement</em> in that they facilitate the exchange of ideas; extrapolating this trend, future generations may be even more &#x201C;intelligent&#x201D; in a sense. Of course, if we think that AI may be built unexpectedly soon, then the argument is less relevant.</p>\n<h2>Uncertainty about AI scenarios</h2>\n<p>Technical work requires a sufficiently good model of what AI will look like, or else we cannot identify viable technical measures. The more uncertain we are about all the different parameters of how AI will unfold, the harder it is to influence its technical development. That said, radical uncertainty also affects other approaches to shape AI, potentially making it a <a href=\"http://prioritizationresearch.com/should-altruists-focus-on-artificial-intelligence/#Unpredictability_of_technological_developments\">general argument against focusing on AI</a>. Still, the argument applies to a larger extent to technical work than to non-technical work.</p>\n<p>In a nutshell, AI scenarios inform our strategy via three intermediate variables:</p>\n<ol>\n<li>Is the control problem of central importance?</li>\n<li>How much (quality-adjusted) technical work will others do anyway?</li>\n<li>How certain can we be about how AI will develop?</li>\n</ol>\n<p>Technical work seems more promising if we think the control problem is pivotal, if we think that others will invest sufficient resources, and if we have a clear picture of what AI will look like.</p>\n<h1>AI strategy on the movement level</h1>\n<p>Effective altruists <a href=\"https://80000hours.org/2016/02/the-value-of-coordination/\">should coordinate their efforts</a>, that is, think in terms of comparative advantages and what the movement should do on the margin rather than just considering individual actions. Applied to the problem of how to best shape AI, this might imply that we should pursue a variety of approaches as a movement rather than committing to any single approach.</p>\n<p>Still, my impression is that non-technical work on AI is somewhat neglected in the EA community. (80000 hours&#x2019; <a href=\"https://80000hours.org/articles/ai-policy-guide/\">guide on AI policy</a> tends to agree.)</p>\n<h1>My thoughts on AI scenarios</h1>\n<p>My position on AI scenarios is close to <a href=\"https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering#A_soft_takeoff_seems_more_likely\">Brian Tomasik</a>, that is, I lean toward a soft takeoff, relatively long timelines, and distributed, <a href=\"https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering#AI_More_like_the_economy_than_like_robots\">economy-like</a> AI rather than a single actor. Also, we should <a href=\"http://magnusvinding.blogspot.de/2016/08/new-book-reflections-on-intelligence.html\">question</a> the notion of <a href=\"http://lesswrong.com/lw/ksa/the_metaphormyth_of_general_intelligence/\">general (super)intelligence</a>. AI systems will likely achieve superhuman performance in more and more domain-specific tasks, but not across all domains at the same time, which makes it a gradual process rather than an <a href=\"https://wiki.lesswrong.com/wiki/Intelligence_explosion\">intelligence explosion</a>. But of course, I cannot justify high confidence in these views given that many experts disagree.&#xA0;</p>\n<p>Following the analysis of this post, this is reason to be mildly sceptical about whether technical work on the control problem is the best way to shape AI. That said, it&#x2019;s still a viable option because I might be wrong and because technical work has indirect benefits in that it influences the AI community to take safety concerns more seriously.</p>\n<p>More generally, one of the best ways to handle pervasive uncertainty may be to focus on &#x201C;meta&#x201D; activities such as increasing the influence of effective altruists in the AI community by building expertise and credibility. This is valuable regardless of one&#x2019;s views on AI scenarios.&quot;</p></body></html>", "user": {"username": "Tobias_Baumann"}}, {"_id": "8qMDseJTE3vCFiYec", "title": "How long does it take to research and develop a new vaccine?", "postedAt": "2017-06-28T23:20:04.289Z", "htmlBody": "<html><body><p><em>This essay was jointly written by Peter Hurford and Marcus A. Davis.</em><em>&#xA0;This is part of&#xA0;<a href=\"/ea/1o6/what_is_the_costeffectiveness_of_researching/\">a series exploring the cost-effectiveness of vaccines</a>.</em></p>\n<p><br>Interventions related to vaccines seem to be highly cost-effective. The World Health Organization calls vaccines &#x201C;one of the most powerful and cost-effective of all health interventions.&quot; (<a href=\"http://apps.who.int/iris/bitstream/10665/44169/1/9789241563864_eng.pdf\">WHO, 2009</a>, pXIV) and the Copenhagen Consensus says that &quot;[v]accination may be the most effective public health intervention of all time&#x201D; (<a href=\"https://issuu.com/copenhagenconsensus/docs/guidetogiving\">Copenhagen Consensus Center Guide to Giving</a>&#xA0;2011, p37). GiveWell finds vaccine-related interventions <a href=\"http://www.givewell.org/international/technical/programs/immunization\"> to be highly cost-effective</a>&#xA0;and have recommended one-off donations to vaccines on multiple occasions <a href=\"#en1\">[1]</a> through <a href=\"http://www.givewell.org/research/incubation-grants\"> Incubation Grants</a>. However, GiveWell <a href=\"http://blog.givewell.org/2016/07/06/dont-currently-recommend-charities-focused-vaccine-distribution/\"> has historically struggled to find room for more funding</a>&#xA0;in this area<a href=\"#en2\">[2]</a>. <br> <br>How cost-effective might developing new vaccines be? GAVI, the leading funder for vaccine-related work, is cited by a few different sources as potentially saving a life for under $1000<a href=\"#en3\">[3]</a>, though this estimate is not robust and this estimate is not related solely to work on developing new vaccines. I&#x2019;d be curious to see more work on evaluating this for a few reasons:</p>\n<ol>\n<li>\n<p>It seems valuable as a benchmark to see how other interventions compare to vaccine-related work.</p>\n</li>\n<li>\n<p>Comparing differences of cost-effectiveness within vaccine-related work (e.g., developing new vaccines versus further distributing vaccines that already exist) can aid our understanding of how interventions and implementations of interventions can differ.</p>\n</li>\n<li>\n<p>Assessing the value of R&amp;D for new vaccines could help us understand the value of funding R&amp;D more generally.</p>\n</li>\n</ol>\n<p>Before looking in depth at the cost-effectiveness of vaccines, my first question was how long it takes to make a vaccine.</p>\n<p>&#xA0;</p>\n<p><strong>What does the literature say?</strong> <br> <br>Turning to the literature, <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/19665605\"> Light, Andrus, and Warburton (2009)</a>&#xA0;outline the short pithy answer -- you need &#x201C;a variable amount of time [...] plus about 5 years&#x201D;. Here, the five years are the years necessary to get past clinical trials and registration<a href=\"#en4\">[4]</a>, and the highly variable amount of time is the time you need to find a vaccine that will get past the clinical trials. After passing clinical trials, one would then need about 5-10 years to scale and distribute the vaccine (<a href=\"http://www.resultsfordevelopment.org/sites/resultsfordevelopment.org/files/Rethink%20HIV%202011%20Assessment%20Paper%20-%20Benefit-cost%20analysis%20of%20AIDS%20Vaccine%20Research.pdf\">Hecht and Jameson, 2011</a>).</p>\n<p>It is reported anecdotally to take 12-15 years to discover new medicine (Light, Andrus, &amp; Warburton, 2009), though it&#x2019;s not clear what this estimate is based on or how this generalize to vaccines. Instead, maybe we could look more at the vaccine lifecycle. For a vaccine to be successful, it has to be successfully developed and then tested through pre-clincal stages, three stages of clinical trials with increasing sample sizes, and then be registered and licensed.</p>\n<p>Conditional on a vaccine candidate being successful at every stage, it takes an average of ten years to develop from preclinical stages to launch and an average of 7.6 years to go from Phase I to launch (<a href=\"http://www.nature.com/nbt/journal/v14/n5/abs/nbt0596-591.html\">Struck, 1996</a>)<a href=\"#en5\">[5]</a>. The total time to develop a vaccine would be longer, since vaccine candidates can fail and be restarted, multiple candidates are tried simultaneously, and there is additional unaccounted discovery time before preclinical trials.</p>\n<p>Only 22% of developed vaccines are successful, start to finish (Struck, 1996)<a href=\"#en6\">[6]</a>, which would mean that 4.5 vaccine candidates are needed on average to produce a workable vaccine.</p>\n<p><a href=\"https://ec.europa.eu/research/health/pdf/event17/s3-3-gerald-voss_en.pdf\"> GSK themselves says</a>&#xA0;it can take up to $1B and 20-50 years to create and fully distribute a vaccine at scale.</p>\n<p>&#xA0;</p>\n<p><strong>What does the historical &#x201C;outside view&#x201D; say?</strong></p>\n<p>Besides looking at literature, another potentially good way to learn how long it takes to make a vaccine is to use base rates and look at how long it took historically to make all of the past vaccines. <br> <br> This method, however, has a number of limitations. There is a surprising amount of uncertainty about the correct start year of each vaccine, since it is difficult to know what the beginning of a research actually is and the transition from &quot;not researched&quot; to &quot;researched&quot; is very gradual. Also, there are often several distinct strands of research that contribute to the final discovery that can be somewhat overlapping (Light, Andrus, &amp; Warburton, 2009).</p>\n<p>When looking at the time between when the viral agent was first linked to the disease and the date that a vaccine has been licensed in the US for a few vaccines, it looks to be an average of 52 years (<a href=\"http://vaccineenterprise.org/conference/2013/sites/default/files/AIDS%20VAccine%202013-JG-History%20of%20HIV%20vax%20development_Gilmour%20v5.pdf\">Gilmour, 2013</a>, slide 3). However, this is misleading as many viral agents were identified before vaccine technology existed in earnest, creating very long lag times before vaccines could be created and inflating the estimate in a way that is not representative of current vaccine manufacturing capabilities. <br> <br> It also unclear at what point the research can be said to have ended with a finished vaccine. The most intuitive approach is to use the date of licensure, but this date can vary wildly between countries (sometimes spanning multiple decades) and countries have inconsistent standards for what is needed to license a vaccine. Additionally, many of the earlier vaccines the modern clinical trial and licensing system did not yet exist<a href=\"#en7\">[7]</a> and it&#x2019;s not clear how much additional testing was needed from a prototype vaccine to mass rollout of the vaccine.</p>\n<p>Regardless, looking back at history myself for 27 different vaccines<a href=\"#en8\">[8]</a>, I find the following rough timelines:</p>\n<ul>\n<li>\n<p>Rabies - <strong>4 years</strong>, 1881-1885 (<a href=\"http://onlinelibrary.wiley.com/doi/10.1046/j.1365-2672.2001.01495.x/full\">Schwartz, 2001</a>; <a href=\"https://en.wikipedia.org/wiki/Rabies_vaccine\"> Wikipedia</a>)</p>\n</li>\n<li>\n<p>Rubella - <strong>7 years</strong>, 1962-1969 (<a href=\"https://en.wikipedia.org/wiki/Rubella#History\">Wikipedia</a>)</p>\n</li>\n<li>\n<p>Pertussis - <strong>8 years</strong>, 1906-1914 (<a href=\"https://www.cdc.gov/vaccines/pubs/pinkbook/pert.html\">CDC, 2017</a>)</p>\n</li>\n<li>\n<p>Measles - <strong>9 years</strong>, 1954-1963 (<a href=\"https://timelines.issarice.com/wiki/Timeline_of_measles\">Rice, 2017a</a>)</p>\n</li>\n<li>\n<p>Influenza - <strong>14 years</strong>, 1931-1945 (<a href=\"http://apps.who.int/iris/bitstream/10665/44169/1/9789241563864_eng.pdf\">WHO, 2009 </a> , p104; <a href=\"https://en.wikipedia.org/wiki/Influenza_vaccine#Origins_and_development\"> Wikipedia</a>)</p>\n</li>\n<li>\n<p>Japanese encephalitis - <strong>20 years</strong>, 1934-1954 (<a href=\"https://smile.amazon.com/Vaccines-Biography-Andrew-W-Artenstein/dp/1441911073\">Artenstein (ed.), 2010</a>, p317; <a href=\"http://www.who.int/immunization/sage/meetings/2014/october/2_Barrett_JE_SAGE_Oct2014.pdf\"> Barrett, 2014</a>, p4)</p>\n</li>\n<li>\n<p>Polio - <strong>20 years</strong>, 1935-1955 (<a href=\"https://www.cdc.gov/vaccines/pubs/pinkbook/polio.html\">CDC, 2017</a>; <a href=\"https://timelines.issarice.com/wiki/Timeline_of_poliomyelitis\"> Wikipedia 1</a>; <a href=\"https://en.wikipedia.org/wiki/Polio_vaccine\"> Wikipedia 2</a>; <a href=\"https://en.wikipedia.org/wiki/Jonas_Salk#Polio_research\"> Wikipedia 3</a>)</p>\n</li>\n<li>\n<p>Tuberculosis - <strong>21 years</strong>, 1900-1921 (<a href=\"https://timelines.issarice.com/wiki/Timeline_of_tuberculosis\">Rice, 2017b</a>)</p>\n</li>\n<li>\n<p>Mumps - <strong>22 years</strong>, 1945-1967 (<a href=\"http://vaccines.procon.org/view.additional-resource.php?resourceID=005969\">ProCon, 2017</a>)</p>\n</li>\n<li>\n<p>HPV - <strong>23 years</strong>, 1983-2006 (<a href=\"http://apps.who.int/iris/bitstream/10665/44169/1/9789241563864_eng.pdf\">WHO, 2009</a>, p116-117; <a href=\"https://en.wikipedia.org/wiki/HPV_vaccines#History\"> Wikipedia</a>)</p>\n</li>\n<li>\n<p>Hepatitis A - <strong>24 years</strong>, 1967-1991 (<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/7876643\">Melnick, 1995</a>; <a href=\"https://en.wikipedia.org/wiki/Hepatitis_A_vaccine\"> Wikipedia</a>)</p>\n</li>\n<li>\n<p>Rotavirus - <strong>26 years</strong>, 1980-2006 (<a href=\"https://en.wikipedia.org/wiki/Rotavirus#History\">Wikipedia 1</a>; <a href=\"https://en.wikipedia.org/wiki/Rotavirus_vaccine#History\"> Wikipedia 2</a>)</p>\n</li>\n<li>\n<p>Smallpox - <strong>26 years</strong>, 1770-1796 (<a href=\"http://www.jameslindlibrary.org/articles/the-origins-of-vaccination-no-inoculation-no-vaccination/\">Boylston, 2012</a>; <a href=\"https://en.wikipedia.org/wiki/Smallpox_vaccine\"> Wikipedia</a>)</p>\n</li>\n<li>\n<p>Yellow Fever - <strong>27 years</strong>, 1912-1939 (<a href=\"https://en.wikipedia.org/wiki/Yellow_fever_vaccine#History\">Wikipedia</a>; <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2892770/\"> Frierson, 2010</a>)</p>\n</li>\n<li>\n<p>Cholera - <strong>30 years</strong>, 1854-1884 (<a href=\"http://apps.who.int/iris/bitstream/10665/44169/1/9789241563864_eng.pdf\">WHO, 2009</a>, p104; <a href=\"https://en.wikipedia.org/wiki/Cholera#Research\"> Wikipedia</a>)</p>\n</li>\n<li>\n<p>Chickenpox - <strong>34 years</strong>, 1954-1988 (<a href=\"https://www.cdc.gov/vaccines/pubs/pinkbook/downloads/varicella.pdf\">CDC, 2015</a>; <a href=\"https://en.wikipedia.org/wiki/Varicella_vaccine\"> Wikipedia</a>)<a href=\"#en9\">[9]</a></p>\n</li>\n<li>\n<p>Hepatitis B - <strong>38 years</strong>, 1943-1981 (<a href=\"https://www.cdc.gov/vaccines/pubs/pinkbook/hepb.html\">CDC, 2017</a>)</p>\n</li>\n<li>\n<p>Tick-borne encephalitis - <strong>39 years</strong>, 1937-1976 (<a href=\"https://en.wikipedia.org/wiki/Tick-borne_encephalitis_vaccine\">Wikipedia</a>; <a href=\"http://www.who.int/immunization/research/meetings_workshops/8_HMeyer_zika_june16.pdf\"> WHO, 2016, Slide 5</a>; <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/21843576\"> Baselli, et. al., 2011</a>)<a href=\"#en10\">[10]</a></p>\n</li>\n<li>\n<p>Diptheria - <strong>40 years</strong>, 1883-1923 (<a href=\"http://apps.who.int/iris/bitstream/10665/44169/1/9789241563864_eng.pdf\">WHO, 2009</a>, p106; <a href=\"https://timelines.issarice.com/wiki/Timeline_of_diphtheria\"> Rice, 2017c</a><a href=\"#en11\">[11]</a>)</p>\n</li>\n<li>\n<p>Tetanus - <strong>40 years</strong>, 1884-1924 (<a href=\"https://www.cdc.gov/vaccines/pubs/pinkbook/tetanus.html\">CDC, 2017</a>)</p>\n</li>\n<li>\n<p>Hib disease - <strong>44 years</strong>, 1933-1977 (<a href=\"https://en.wikipedia.org/wiki/Haemophilus_influenzae\">Wikipedia 1</a>; <a href=\"https://en.wikipedia.org/wiki/Hib_vaccine\">Wikipedia 2</a>)</p>\n</li>\n<li>\n<p>Ebola - <strong>~46? years</strong>, 1976-2022(?) (<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/65661\">Johnson, Lange, Webb, &amp; Murphy, 1977</a>; <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/65663\">Pattyn, et. al., 1977</a>;&#xA0;<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/6108462\">Lupton, et. al., 1980</a>)<a href=\"#en12\">[12]</a></p>\n</li>\n<li>\n<p>HIV - <strong>~46? years</strong>, 1984-2030(?) (<a href=\"http://www.sciencedirect.com/science/article/pii/S0264410X13005963\">Esparza, 2013</a>; <a href=\"http://www.resultsfordevelopment.org/sites/resultsfordevelopment.org/files/Rethink%20HIV%202011%20Assessment%20Paper%20-%20Benefit-cost%20analysis%20of%20AIDS%20Vaccine%20Research.pdf\"> Hecht &amp; Jameson, 2011</a>)</p>\n</li>\n<li>\n<p>Typhoid - <strong>58 years</strong>, 1838-1896 (<a href=\"https://en.wikipedia.org/wiki/Typhoid_vaccine\">Wikipedia 1</a>; <a href=\"https://en.wikipedia.org/wiki/Typhoid_fever#Development_of_vaccination\"> Wikipedia 2</a>)</p>\n</li>\n<li>\n<p>Malaria - <strong>~58? years</strong>, 1967-2025(?) (<a href=\"http://www.gavi.org/about/governance/gavi-board/minutes/2016/22-june/minutes/09---malaria-vaccine-pilots---appendices/\">GAVI, 2016</a>; <a href=\"https://en.wikipedia.org/wiki/Malaria_vaccine#History\"> Wikipedia</a>)</p>\n</li>\n<li>\n<p>Pneumococcal disease - <strong>66 years</strong>, 1911-1977 (<a href=\"https://www.cdc.gov/vaccines/pubs/pinkbook/pneumo.html\">CDC, 2017</a>)</p>\n</li>\n<li>\n<p>Meningitis - <strong>68 years</strong>, 1906-1974 (<a href=\"http://jem.rupress.org/content/jem/10/1/141.full.pdf\">Flexner &amp; Jobling, 1907</a>; <a href=\"https://www.cdc.gov/vaccines/pubs/pinkbook/downloads/mening.pdf\"> CDC, 2015</a>)<a href=\"#en13\">[13]<br><br></a></p>\n</li>\n</ul>\n<p>Taking these numbers literally, this gives a mean of 31.8 years of development, with a median of 27 years and a standard deviation of 17.7 years. If you exclude the vaccines still under development (HIV, malaria, and ebola), the mean is 29.5 years (median 26, SD 17.4). <br><br></p>\n<p><strong>Conclusion</strong></p>\n<p>It&#x2019;s unclear what conclusions we should draw from either of these approaches. Firstly, it&#x2019;s important to keep in mind that these statistics are unlikely to be representative of future vaccines, because the low-hanging fruit is likely already gone, early vaccines did not have to go through the modern clinical trial system, and because modern technology and investment could speed up R&amp;D. On the other hand, modern vaccine development may be sped up by significant advances in technology. Lastly, the vaccines that take the longest are the least likely to have already been developed, simply because they stretch out over more time, which introduces a natural bias toward dealing with &quot;harder&quot; vaccines today above and beyond the &quot;low hanging fruit&quot; factor.</p>\n<p>However, if you compare the eight vaccines that started development after 1940 and have completed with the 16 vaccines developed <em>before</em> 1940 and have completed<a href=\"#en14\">[14]</a>, the difference in completion time is marginally significant at best (t-test p = 0.12<a href=\"#en15\">[15]</a>).</p>\n<p>Lastly, it&#x2019;s interesting to see the difference between the timeline pointed to by the academic literature (&#x201C;12-15 years&#x201D; for medicine generally and around 20 years for vaccines), the timeline pointed to by GSK (&#x201C;20-50 years&#x201D;), and the timeline implied by the historical record (&#x201C;mean of 31.2 years&#x201D;). Taken together and weighing these three sources of evidence evenly, this suggests an average of 29 years for the typical vaccine<a href=\"#en16\">[16]</a>, though with high uncertainty based on uncertainties in each approach and on many particular vaccines not being typical.</p>\n<p>&#xA0;</p>\n<p><em>Next in the series: <a href=\"/ea/1l4/how_much_does_it_cost_to_research_and_develop_a/\">How much does it cost to research and develop a vaccine?</a></em></p>\n<p>&#xA0;</p>\n<p><strong>Endnotes</strong></p>\n<p><span>[1]: For examples, <a href=\"http://www.givewell.org/international/charities/GAVI\"> reviewing GAVI in 2009 as a potential top charity</a>, <a href=\"http://www.goodventures.org/research-and-ideas/blog/co-funding-with-the-gates-foundation\"> co-funding a drug-related intervention with the Gates Foundation</a>&#xA0;in 2012, <a href=\"http://www.givewell.org/charities/IDinsight/september-2014-grant\"> funding IDInsight to do an RCT on incentives for vaccines</a>&#xA0;in 2014, <a href=\"http://www.givewell.org/JPAL-IRD-grant\"> funding JPAL to do an RCT on vaccines</a>&#xA0;in 2015, <a href=\"http://www.givewell.org/charities/new-incentives/november-2016-grant\"> funding New Incentives to work on immunization-related conditional cash transfers</a>&#xA0;in 2016, and <a href=\"http://www.givewell.org/charities/charity-science/charity-science-health/november-2016-grant\"> funding Charity Science Health to work on expanding demand for immunizations in India</a>&#xA0;in 2016. </span></p>\n<p><span>[2]: See also <a href=\"http://blog.givewell.org/2011/06/14/gavi-appears-to-be-out-of-room-for-more-funding-good-news/\"> notes on GAVI in particular</a>&#xA0;and <a href=\"http://blog.givewell.org/2017/03/21/march-2017-open-thread/#comment-942811\"> this comment on funding vaccine R&amp;D</a>. For more detail, see <a href=\"http://www.givewell.org/international/charities/vaccination-organizations\"> GiveWell&#x2019;s overview of the vaccine funding landscape</a>. </span></p>\n<p><span>[3]: <a href=\"http://gsid.org/downloads/successes_and_failures_final.pdf\"> Francis (2010)</a>&#xA0;quotes GAVI as averting 5M deaths against $3.7B in funding, for a cost-effectiveness of $740 per life saved. <a href=\"http://rstb.royalsocietypublishing.org/content/royptb/366/1579/2743.full.pdf\"> Lob-Levyt (2011)</a>&#xA0;quotes GAVI as averting 5.4M vaccine-related deaths against $4491M in vaccine-related spending or $831.67 per life saved. <a href=\"http://www.gavi.org/library/news/press-releases/2010/gavi-alliance-set-to-save-four-million-lives-by-2015/\"> A GAVI press release</a>&#xA0;quotes GAVI as saving 4M lives against $3.7B or $925 per life saved.</span></p>\n<p><span>[4]: Except the amount of time to get past clinical trials and registration is more like 6.3 years, on average (Struck, 1996;&#xA0;<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/16522587\">Keyhani, Diener-West, &amp; Powe, 2006</a>; <a href=\"http://www.tandfonline.com/doi/abs/10.1586/14760584.2013.850035?journalCode=ierv20\"> Waye, Jacobs, &amp; Schryvers, 2013</a>).</span></p>\n<p><span>[5]: Struck (1996) (Table 4) specifies that vaccines take an average of 2.4 years to go from preclinical trials to Phase I, 2.0&#xA0;years to go to Phase II from Phase I, 1.8 years to get to Phase III, 1.1 years to get to preregistration, and 1.3 years to get to registration. Each time is conditional on the prior step being successful.</span></p>\n<p><span>[6]: Struck (1996) (Table 3) specifies there is a 96% chance of a vaccine moving from registration to launch, a 68% chance of moving from Phase III to launch, a 54% chance of moving from Phase II to launch, a 39% chance of moving from Phase I to launch, and a 22% chance of moving from preclinical to launch. Inverting these probabilities using <a href=\"https://en.wikipedia.org/wiki/Conditional_probability\">math</a>, we can derive a 56% chance of moving from preclinical trials to Phase I, a 72% chance of moving from Phase I to Phase II, a 79% chance of moving from Phase II to Phase III, a 71% chance of moving from Phase III to registration, and a 96% chance of moving from registration to launch. </span></p>\n<p><span>[7]: Though it wasn&#x2019;t until 1959 when a modern scientific vaccine was submitted for licensing after modern scientific scrutiny (<a href=\"http://apps.who.int/iris/bitstream/10665/44169/1/9789241563864_eng.pdf\">WHO 2009</a>, p105).</span></p>\n<p><span>[8]: This is not intended to be an exhaustive list of all vaccines, but is intended to be exhaustive of all vaccines that would be considered &quot;important&quot;, such as the vaccines on <a href=\"https://en.wikipedia.org/wiki/WHO_Model_List_of_Essential_Medicines#Vaccines\"> the WHO list of essential medicines</a>&#xA0;and notable vaccines under current development.</span></p>\n<p><span>[9]: Though the varicella vaccine was not actually licensed in the US until 1995.</span></p>\n<p><span>[10]: There was an earlier vaccine, approved in Russia in 1941, that was incubated in a mouse brain and only worked against a few strains. I am unsure how to count this.</span></p>\n<p><span>[11]: As a disclaimer, <a href=\"http://peterhurford.com/other/donations.html\">I paid</a> $400 for the creation of this source via <a href=\"https://contractwork.vipulnaik.com/\">Vipul Naik</a>. While the citation is to Issa Rice, given that it is on his domain, the actual author is <a href=\"https://contractwork.vipulnaik.com/worker.php?worker=Sebastian+Sanchez\"> Sebastian Sanchez</a>.</span></p>\n<p><span>[12]: While <a href=\"https://en.wikipedia.org/wiki/Ebola_vaccine\">a candidate&#xA0;ebola vaccine is currently in Phase III trials</a>&#xA0;(see also <a href=\"http://www.who.int/medicines/emp_ebola_q_as/en/\">WHO, 2015</a>), it is not yet clear whether or not that candidate will succeed, so it is difficult to forecast how long it will take to develop an ebola vaccine. However, analysis in&#xA0;</span>Struck (1996) (Table 4)[5] would suggest there is only ~5 more years left until the ebola vaccine is registered (i.e., registration predicted for&#xA0;2022) (see also Light, Andrus, &amp; Warburton, 2009). This is a personal prediction with weak confidence and I can&apos;t find any&#xA0;public pronouncements from any official organizations or scientists about an estimated license date for an ebola vaccine.</p>\n<p><span>[13]: This is a good example of how there can be wide uncertainty about when vaccine research started. It&apos;s certainly not possible to start work on a vaccine before the cause of the disease is identified, which would be 1887 <a href=\"https://en.wikipedia.org/wiki/Neisseria_meningitidis#History\"> when the bacteria was first isolated</a>.&#xA0;Also work would have to have started by 1917, when the first detailed scientific paper on the vaccine was published (<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2017723/\">Greenwood, 1917</a>). However, this gives me a 30 year range within which I am uncertain as to whether research had started or not.</span></p>\n<p><span>[14]: I chose 1940 as a split because it felt &quot;modern&quot; enough (corresponding somewhat to the definition of a modern era as post-World War II) while still encompassing enough vaccines to have a reasonable amount on both sides. I did not look at how the numbers played out before committing to my split date.</span></p>\n<p><span>[15]: early mean 22.9 years, early median 23.5 years, early SD 10.7 years, late mean 32.8 years, late median 28.5 years, late SD 19.3 years.</span></p>\n<p><span>[16]: This is the rounded average of 20 years (academic literature), 35 years (midpoint of GSK&apos;s &#x201C;20-50 years&#x201D;), and 32 years (retrospective analysis).</span></p></body></html>", "user": {"username": "Peter_Hurford"}}, {"_id": "N25rHWerRFLaTtDMm", "title": "2017 LEAN Statement", "postedAt": "2017-06-28T20:12:33.491Z", "htmlBody": "<html><body><p><img src=\"https://i.imgur.com/blrrvAf.png\" alt=\"LEAN Logo\"></p>\n<p><span>Hello! I&#x2019;m Richenda, Manager of LEAN (the Local Effective Altruism Network). In March I assumed responsibility for the project, having previously worked on it in 2015 and 2016 under Tom Ash. The aim of this post is to keep the EA community informed about our operations, and to share some of the goals we have for the future.</span></p>\n<h2><span>Who Are We?</span></h2>\n<p><span>LEAN is a </span><a href=\"/ea/1au/impact_is_now_rethink_charity/\"><span>Rethink Charity</span></a><span> (formerly </span><a href=\"https://www.facebook.com/groups/dotimpact/\"><span>.impact</span></a><span>) project dedicated to expanding the growth and accessibility of the EA movement. LEAN is a continuation of a grassroots initiative which first emerged in 2013, and it&#x2019;s purpose is to provide material and strategic support for existing local EA groups, and to </span><a href=\"https://docs.google.com/document/d/1aflaZxbGUf1ndAzpuxWXy1_J6-64hPx2CRen5IBl35c/edit\"><span>start new local groups</span></a><span>. We presently support over 350 local groups around the world.</span></p>\n<h2><span>What Do We Do?</span></h2>\n<p><span>We offer a variety of services and tools to groups in the EA network.</span></p>\n<h3><span>Guidance &amp; Expertise</span></h3>\n<ul>\n<li>\n<p><span>In-depth advice and one-to-one support for local organisers across different communication channels</span></p>\n</li>\n</ul>\n<ul>\n<li>\n<p><span>Guidance on how to start new groups</span></p>\n</li>\n<li>\n<p><span>Resources for group growth and management</span></p>\n</li>\n<li>\n<p><span>The annual survey of group organizers, which gathers information alongside .impact&#x2019;s </span><a href=\"https://eahub.org/survey\"><span>annual EA Survey</span></a><span> (Click </span><a href=\"https://docs.google.com/document/d/1ztKzMUAWnDRR6Rv9yz99GyBWqLxdqkD5zcU6HjFDnIs/edit\"><span>here</span></a><span> for the last impact assessment)</span></p>\n</li>\n<li>\n<p><span>Recommendations for </span><a href=\"https://eahub.org/groups/resources/possible-events\"><span>possible group activities</span></a><span>, such as donation decision events and career workshops</span></p>\n</li>\n<li>\n<p><span>Conference calls, bringing local organisers together</span></p>\n</li>\n<li>\n<p><span>The local EA group newsletter*</span></p>\n</li>\n<li>\n<p><span>The Local Organisers </span><a href=\"https://docs.google.com/document/d/1-oAMaRiTQpZY4xqZ3buIu6yO-YKQOT3zU7uTS9jt8j4/edit?usp=sharing\"><span>Mentoring Programme</span></a></p>\n</li>\n<li>\n<p><span>The use of our network to initiate connections between EAs who have common needs.</span></p>\n</li>\n</ul>\n<p><span>*Now in collaboration with CEA</span></p>\n<h3><span>Administrative</span></h3>\n<ul>\n<li>\n<p><span>Maintaining a global, up-to-date group database </span></p>\n</li>\n<li>\n<p><span>EA peer-to-peer fundraisers, which provide one of the most effective actions local groups can take (e.g. </span><a href=\"http://www.livingonless.causevox.com/\"><span>Living On Less</span></a><span>, </span><a href=\"http://www.seasonsgivings.causevox.com\"><span>Christmas</span></a><span> and </span><a href=\"http://www.birthday.causevox.com\"><span>birthdays</span></a><span>)</span></p>\n</li>\n<li>\n<p><span>Providing spaces for groups to communicate and share resources amongst each other, including our </span><a href=\"https://www.facebook.com/groups/localea\"><span>Facebook</span></a><span> and </span><a href=\"https://groups.google.com/forum/#!forum/ea-group-leaders\"><span>Google</span></a><span> groups</span></p>\n</li>\n<li>\n<p><span>Overseeing the transition of email addresses and online profiles when groups change leaders</span></p>\n</li>\n<li>\n<p><span>Local </span><a href=\"https://docs.google.com/document/d/1aflaZxbGUf1ndAzpuxWXy1_J6-64hPx2CRen5IBl35c/edit\"><span>group seeding </span></a><span>and activation.</span></p>\n</li>\n</ul>\n<h3><span>Technical</span></h3>\n<ul>\n<li>\n<p><span>Email addresses for every EA group</span></p>\n</li>\n<li>\n<p><span>The </span><a href=\"https://eahub.org/groups\"><span>public directory of groups</span></a><span> which is featured on the </span><a href=\"https://eahub.org/map\"><span>Map of EAs</span></a><span>, allowing people to find EA groups nearby</span></p>\n</li>\n<li>\n<p><span>Free, fully customisable websites, with support</span></p>\n</li>\n<li>\n<p><span>Free</span><a href=\"https://www.meetup.com/\"><span> Meetup.com</span></a><span> groups.</span></p>\n</li>\n</ul>\n<h2><span>Where We Are Headed</span></h2>\n<ol>\n<li>\n<p><span>LEAN will significantly expand the selection of resources available to groups by creating content backed by empirical and professional expertise.</span></p>\n</li>\n<li>\n<p><span>LEAN will diversify and fine tune the level of support offered in order to assist different kinds of groups and organisers at different stages of development.</span></p>\n</li>\n<li>\n<p><span>LEAN will coordinate concentrated efforts between local groups to take effective action, known as &#x201C;Impact Missions.&#x201D;</span></p>\n</li>\n<li>\n<p><span>LEAN will draw upon other Rethink Charity projects, like </span><a href=\"http://www.shicschools.org\"><span>SHIC</span></a><span>, to increase the opportunities for action available to local groups.</span></p>\n</li>\n<li>\n<p><span>LEAN will develop the EA Hub, with the view to improving user experience.</span></p>\n</li>\n<li>\n<p><span>LEAN will build on existing assessment procedures in order to optimise our impact.</span></p>\n</li>\n<li>\n<p><span>LEAN will be proactive in coordinating with other organisations in the EA outreach space, promoting efficient and coherent provision for local groups. We are presently collaborating on:</span></p>\n</li>\n<ol>\n<li>\n<p><span>Creating an EA-wide cooperation agreement for local groups</span></p>\n</li>\n<li>\n<p><span>Integrating distinct organisation databases of group records</span></p>\n</li>\n<li>\n<p><span>Centralising and standardising the materials available to local groups.</span></p>\n</li>\n</ol>\n</ol>\n<p><strong>&#xA0;</strong></p>\n<p><span>To receive support or to make an enquiry, contact the team through </span><a href=\"mailto:lean@eahub.org\"><span>lean@eahub.org</span></a></p>\n<p><span>You can also schedule a video call with Richenda </span><a href=\"https://calendly.com/richendaherzig\"><span>here</span></a><span>.</span></p></body></html>", "user": {"username": "Richenda"}}, {"_id": "sxukPJiS5ZnWDX5E3", "title": "Hi, I'm Luke Muehlhauser. AMA about Open Philanthropy's new report on consciousness and moral patienthood", "postedAt": "2017-06-28T15:49:11.655Z", "htmlBody": "<html><body><p>Hi EA Forum,</p>\n<p>I&apos;m <a href=\"http://www.openphilanthropy.org/about/team/luke-muehlhauser\">Luke Muehlhauser</a> and I&apos;m here to answer your questions and respond to your feedback about the <a href=\"http://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood\">report on consciousness and moral patienthood</a> I recently prepared&#xA0;for the <a href=\"http://www.openphilanthropy.org/\">Open Philanthropy Project</a>. I&apos;ll be here today (June 28th) from 9am Pacific onward, until the flow of comments drops off or I run out of steam, whichever comes first. (But I expect to be avaliable through at least 3pm and maybe later, with a few breaks in the middle).</p>\n<p>Feel free to challenge the claims, assumptions, and inferences I make in the report. Also feel free to ask questions that you worry might be &quot;dumb questions,&quot; and questions you suspect might be answered <em>somewhere</em> in the report (but you&apos;re not sure where) &#x2014; it&apos;s a long report! Please do limit your questions to&#xA0;the topics of the report, though: consciousness, moral patienthood, animal cognition, meta-ethics, moral weight, illusionism, hidden qualia, etc.</p>\n<p>As noted in the <a href=\"/ea/1bo/upcoming_ama_with_luke_muehlhauser_on/\">announcement post</a>, much of the most interesting content in the report is in the appendices and even some footnotes, e.g. on <a href=\"http://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#AppendixC\">unconscious vision</a>, on <a href=\"http://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#AppendixB\">what a more satisfying theory of consciousness might look like</a>, and a visual explanation of attention schema theory (footnote 288). I&apos;ll be happy to answer questions about those topics as well.</p>\n<p>I look forward to chatting with you all!</p>\n<p>EDIT: Please post different questions as separate comments, for&#xA0;discussion threading. Thanks!</p>\n<p>EDIT: Alright, I think I replied to everything. My thanks to everyone who participated!</p></body></html>", "user": {"username": "lukeprog"}}, {"_id": "m49ZMamTnNJj8wbQo", "title": "Can we apply start-up investing principles to non-profits?", "postedAt": "2017-06-27T03:16:49.074Z", "htmlBody": "<p>How do you find the best non-profits to donate to? This is an important question that is critical to effective altruism.</p><p>One suggestion comes from Holden Karnofsky at the Open Philanthropy Project, who describes a strategy called <a href=\"http://www.openphilanthropy.org/blog/hits-based-giving\">\u201chits-based giving\u201d</a>. In this framework, you make a number of investments, some of which are very counter-intuitive and against expert consensus, with the understanding that many will not amount to much but those that work will generate excess returns to make the overall portfolio have a high altruistic return on philanthropic investment.</p><p>This strategy originates from YCombinator. In the essay <a href=\"http://paulgraham.com/swan.html\">\u201cBlack Swan Farming\u201d</a>, Paul Graham argues that funding for-profit startups is the art of hunting for the one deal that will make it big. You have a lot of \u201cmisses\u201d when you invest, but the one time you make a \u201chit\u201d, it will hit big and repay all your losses and then some. In order to guess right, you have to make many gambles. <a href=\"http://www.ycombinator.com/\">YCombinator</a> has been working on this problem since 2005, and has since invested over $170M into over 1400 different start-ups. The combined valuation of their current start-up batch is stated to now be over $80B. &nbsp;&nbsp; &nbsp;&nbsp;</p><p>\u201cBlack swan farming\u201d seems to work well for YCombinator. But does it apply well when donating to non-profits? Does hits-based giving work? &nbsp;Since writing that post on April 2016, OpenPhil has already allocated <a href=\"http://www.openphilanthropy.org/giving/grants\">over $197M</a> according to this philosophy. YCombinator is also applying hits-based giving to their own batch of non-profits, to which they have donated $3M.</p><p>&nbsp;</p><h2>The \u201cStart Up\u201d Approach</h2><p>Separately, Ben Todd outlines <a href=\"https://80000hours.org/2015/11/take-the-growth-approach-to-evaluating-startup-non-profits-not-the-marginal-approach/\">that many donors concerned with effectiveness judge organizations based on their short-term marginal impact</a>. For example, as Todd mentions, <a href=\"http://www.givewell.org/about/impact\">GiveWell had returns lower than its costs</a> for the first four years, but then quickly exploded in its fundraising ratio, doubling money moved from 2012 to 2013, again from 2013 to 2014, and moving more money in 2015 than twice as much raised in 2013 and 2014 combined. An impact assessment focused solely on short-run fundraising ratios in 2011 would have missed GiveWell as an incredibly valuable investment.</p><p>In contrast, Todd argues for evaluating early \u201cstart-up\u201d non-profits with standard start-up metrics, such as making sure they have a high-quality product, a large addressable market, and the ability to \u201csell\u201d to this market at scale. Similarly, the organization should have a good growth rate and the team should ideally demonstrate competence and have a track record. For example, GiveWell had a superior research product with the ability to scale to millions of small donors plus dozens of interested large-scale foundations. While the team did not have much of a prior track record, they showed their competence through their early research and early traction with donors.</p><p>Lastly, Todd implies that upfront, early investments in rigorous cost-effectiveness analyses are premature, as they draw attention away from growing the core product in quality and scale, and they likely focus too much on the short-run impact, ignoring long-run opportunities.</p><p>&nbsp;</p><h2>Venture Capital vs. Hedge Funds<br>&nbsp;</h2><p>While the terminology applied is very loose and hard to generalize, the arguments by Karnosfky and Todd seem to compare non-profit donations to \u201cstart up\u201d investments via venture capital -- doing what Graham and Thiel suggest and making hundreds of guesses to find the few diamonds in the rough that provide outsized returns.</p><p>However, this is not the only form of for-profit investing. One might also consider the approach of hedge funds, which appear to relatively employ less of a \u201chits-based\u201d approach and more of an upfront investment in analytics. While <a href=\"https://www.sla.org/career-center/the-role-of-research-in-venture-capital-investing/\">VCs do some due diligence</a>, hedge funds often employ very complex risk modeling when making investments.</p><p>This means if we could successfully generalize and compare venture capital versus hedge funds and see if one strategy generates superior returns compared to the other we could have some preliminary evidence for whether it is better to be \u201chits-based\u201d or \u201cevidence-based\u201d.</p><p>Frustratingly, it is very difficult to compare the average returns of venture capital and hedge funds because the intra-group variation between individual firms is massive and dwarfs comparisons between the two groups. Also, the private nature of firms and selection bias in reporting makes finding accurate, systematic summary statistics quite hard.</p><p>A literature review of relevant research on VC firms finds that the average returns of VC are roughly equivalent to that of the stock market though with significant variation and methodological uncertainty (<a href=\"http://www.nber.org/papers/w17523.pdf\">Rin, Hellmann, &amp; Puri, 2011, p78-80</a>). Furthermore, choices of sampling periods and methodology can dramatically change whether venture capital is determined to be more or less profitable than private equity on average (<a href=\"http://www.nber.org/papers/w17523.pdf\">Rin, Hellmann, &amp; Puri, 2011, p90</a>).<br><br>Overall, high variation between individual firms in the same general category, the looseness of category definitions, the highly privatized nature of individual firm strategies, and the high uncertainty in results means that we unfortunately cannot draw firm conclusions from this line of investigation. This might mean that either approach is fine, but varies a lot more based on management and circumstances than approach to investing, but it is very hard to generalize this to non-profits.</p><p>&nbsp;</p><h2>How Similar Are For-Profit and Non-Profit Investments?</h2><p>However, even comparing non-profit donations to hedge funds implies that making a donation is like for-profit investing. This is a view that many impact-interested donors appear to hold -- the prevalence of the phrase <a href=\"https://en.wikipedia.org/wiki/Impact_investing\">\u201cimpact investing\u201d</a> as a term for efficient giving drives this analogy home. However, the standard advice for individual for-profit investors is to <a href=\"http://money.usnews.com/money/personal-finance/mutual-funds/articles/2015/05/11/why-investors-should-stop-trying-to-beat-the-market\">avoid trying to \u201cbeat the market\u201d</a> by searching for investment opportunities on one\u2019s own and <a href=\"https://www.betterment.com/resources/investment-strategy/index-fund-portfolios-win/\">instead to invest in index funds</a>. Does this mean that non-profit investors should be advised to donate to altruistic \u201cindex funds\u201d as well?</p><p>How similar is for-profit and non-profit investing? It appears to me like there are numerous key differences:</p><ul><li><strong>Non-profit investing affords you the opportunity to be far more risk-neutral than you can in for-profit investing, which changes your options.</strong> Index funds are typically chosen less because the diversification increases average returns, but rather because the diversification decreases the variance of the investment, exposing you to less risk. A risk-neutral for-profit investor might be pursuing variance increasing strategies instead, <a href=\"https://en.wikipedia.org/wiki/Leverage_(finance)\">like leverage</a>. However, altruistic investments are not used with the intention of saving for one\u2019s own future, which allows the altruist to <a href=\"https://concepts.effectivealtruism.org/concepts/risk-aversion/\">be more risk-neutral to chase higher expected returns</a>.<br>&nbsp;</li><li><strong>The prices of non-profit investments don\u2019t instantly change when they\u2019re identified as more valuable, allowing good deals to be available much longer.</strong> If analysis shows that a particular stock is very hot, say offering the opportunity to invest $5 to get $50, according to the <a href=\"https://en.wikipedia.org/wiki/Efficient-market_hypothesis\">efficient market hypothesis</a>, that analysis will nearly instantly be <a href=\"http://www.obliviousinvestor.com/what-does-it-mean-for-something-to-be-priced-in/\">priced into</a> the stock and the stock will quickly become worth ~$50. However, if a donation opportunity allows you to donate $3500 to save a life, <a href=\"https://en.wikipedia.org/wiki/Value_of_life#Life_value_in_the_US\">arguably worth about $9.1M</a>, the donation opportunity does not suddenly get bid up to $9.1M per life saved. Instead, the donation opportunity is used up until diminishing marginal returns mean it no longer exists, which happens significantly more slowly than a hot stock changes price. Therefore we should expect <a href=\"http://blog.givewell.org/2011/06/11/why-we-should-expect-good-giving-to-be-hard/\">good giving to be hard</a>, but not nearly as hard as finding a hot stock.<br>&nbsp;</li><li><strong>For-profit investing typically does not have massive negative returns, but non-profit investing can.</strong> Unless you\u2019re investing with leverage, breaking the law, are a massive <a href=\"https://en.wikipedia.org/wiki/Too_big_to_fail\">\u201ctoo big to fail\u201d</a> financial institution, or otherwise are doing something weird, the for-profit investments you make will typically not lose any more money than you put in. If you invest $1M, the worst that can happen is that you lose $1M. However, with non-profit investing, when you donate $1M, you run the risk of the non-profit being <a href=\"https://foundational-research.org/charity-cost-effectiveness-in-an-uncertain-world/#Introductory_dialogue\">somehow net negative in confusing ways</a>. Being able to guard against these risks is important in non-profit investing, but not in for-profit investing.<br>&nbsp;</li><li><strong>Non-profit investing lets you arbitrage based on your values, whereas for-profit investing does not.</strong> Many foundations spending millions of dollars spend it based on values that are different than EA principles of being neutral toward the location or species of those that you help, or being neutral toward taking very far-future bets. The more your values depart from the global mainstream, the easier it should be to find good giving opportunities (e.g., donating or starting something yourself) that maximize those values, because the good ones won\u2019t have been taken yet (unless your values are so weird that no one is willing to help you make progress on them).<br>&nbsp;</li><li><strong>More people are trying a lot harder to \u201cbeat you\u201d in for-profit investing than non-profit investing.</strong> In the for-profit world, your quest to find a hot stock with amazing ROI is going up against hundreds of thousands of incredibly well-funded analysts collectively working billions of hours a year to outcompete you. On the other hand, in the non-profit world, <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/\">while some sharp non-profit investors are buying up the best opportunities already</a>, it seems like most people don\u2019t care, and the total community of people chasing optimal donations is a few thousand people perhaps collectively spending at most 2M hours a year. This makes it many times easier to outcompete the market in non-profit investing.<br>&nbsp;</li><li><strong>Non-profit investing isn\u2019t even a competition and analysts will share their best opportunities with you for free.</strong> Unlike effective for-profit investing, <a href=\"/ea/15t/effective_altruism_is_not_a_competition/\">effective altruism isn\u2019t a competition</a> and since great donation opportunities take years to go away, <a href=\"https://80000hours.org/2016/12/the-effective-altruism-guide-to-donating-this-giving-season/\">they can be shared with you for free</a>. If every single for-profit hedge fund gave you instant, free access to their advice in an easily summarized form, I imagine for-profit investing would slant away from index funds too.<br>&nbsp;</li><li><strong>The returns for for-profit funds are relatively clear, but non-profit returns require a lot of work to understand.</strong> While there might be issues of applying the correct methodology, you can generally look at how much cash you get back for how much cash you put in. With non-profit investing, <a href=\"http://everydayutilitarian.com/essays/in-non-profits-who-is-the-customer/\">there is no clear measure of your return on investment</a>. Instead, you have to use complex analysis to assess your return and some investments will never be able to show a conclusive return even if they do have one.<br>&nbsp;</li><li><strong>Non-profit investors do not have a clear \u201cindex fund\u201d.</strong> An index fund tries to diversify as much as possible by investing in a wide variety of stocks from a wide variety of markets. The S&amp;P 500 is basically like investing a tiny bit in every large company in the US. On the other hand, investing in <a href=\"http://www.givewell.org/charities/top-charities\">GiveWell\u2019s top charities</a> or in <a href=\"https://app.effectivealtruism.org/funds\">Effective Altruism Funds</a> is a lot more like investing in an actively managed fund that has no expense ratio -- a fund that looks at many possible stocks but selects only the few that they think will beat the average. A true altruistic \u201cindex fund\u201d would instead invest a small amount in every single charity and diversify as much as possible. The fact that <a href=\"https://www.givingwhatwecan.org/post/2013/11/should-you-only-donate-to-one-charity/\">this sounds like such a bad idea</a> (and conversely that investing everything in only one stock sounds like a bad idea for personal finance) shows the difference between for-profit and non-profit investing.<br>&nbsp;</li></ul><p>Therefore, while pursuing higher returns at the chance of higher risk can be a good strategy for both start-up investing and optimal donating, there are also important differences between these two activities. For-profit investors have to exploit their insider knowledge and connections via start-up investing to beat the market, but in the non-profit world, the differences in pricing, the pooling of wisdom, and the relative lack of competition means that high returns might be found through an evidence-based approach. Moreover, the difficulty of understanding non-profit returns would suggest that non-profit investors would have to collect a lot of evidence just to understand how well their portfolios are doing.<br>&nbsp;</p><h2>The Incubation Approach</h2><p>The difficulty of understanding non-profit returns, the ability to widely disseminate impact analysis, plus the lack of quickly diminishing returns, places <a href=\"/ea/yq/how_should_we_prioritize_cause_prioritization/\">a high premium on the value of collecting information</a> and communicating it with the rest of the impact-interested community. Arguably, <a href=\"/ea/170/ea_should_invest_more_in_exploration\">the effective altruism community currently under-invests in exploration</a>, and this analysis provides some additional theoretic reasons why exploration could be so highly valuable.</p><p>The mere presence of large \u201chits\u201d combine with the possibility of missing them is not, in itself, persuasive -- that\u2019s just <a href=\"https://en.wikipedia.org/wiki/Fear_of_missing_out\">FOMO</a>. For example, the possibility of there being \u201chit\u201d lottery tickets does not suggest it is a good idea to do <a href=\"https://xkcd.com/1827/\">\u201chits-based\u201d lottery ticket buying</a>. Indeed, any investing strategy has both a false positive and a false negative rate, and care needs to be paid to both. If this comes at the cost of occasionally missing out on a big \u201chit\u201d, that doesn\u2019t mean your strategy is wrong. Instead, we would want to ascertain whether we have properly balanced our false positive and false negative rates to produce the highest expected returns. While I know Karnofsky and Todd have thought about this a lot and are not solely driven by FOMO, I do not think there has been enough published analysis of this type.<br><br>On the other hand, I agree with Todd that many EAs overemphasize the false negative rate with too much desire for rigor. I agree it\u2019s important to have principles that would allow for taking risk on innovative ideas, and would have allowed you to fund organizations like The Against Malaria Foundation in the beginning, before they began to show signs of impact.</p><p>Another non-profit investing framework I think is worth considering is represented by <a href=\"http://www.givewell.org/research/incubation-grants\">GiveWell\u2019s incubation grants</a>, the <a href=\"http://www.globalinnovation.fund/\">Global Innovation Fund</a>, <a href=\"http://www.charityentrepreneurship.com/blog/seeking-co-founder-for-highly-effective-global-health-charity\">Charity Science\u2019s search for co-founders for future exceptional global poverty charities</a>, and maybe <a href=\"https://www.effectivealtruism.org/grants/\">EA Grants</a> (though I'm not sure yet). While I don\u2019t understand the exact process for vetting and approving these grants from these orgs, these grants seem like a great way to buy information.<br><br>My idealized version of the vetting process might go something like this:<br>&nbsp;</p><ol><li>Get a large pool of candidate projects and project teams, whether by soliciting applications and/or waiting for people to apply.</li><li>Following Todd\u2019s approach, evaluate each project based on the quality of the team members, the quality of the core service or product the non-profit aims to provide, the upside opportunity of what the non-profit could potentially achieve, and the likelihood of achieving this scale. This could be done with a few interviews and some guesswork. It\u2019s important to acknowledge <a href=\"http://everydayutilitarian.com/essays/the-challenges-of-great-people-as-a-model-for-social-change/\">there are many ways in which this evaluation may not accurately predict impact</a>.</li><li>Identify the top projects you are willing to fund that meet the cut based on the criteria in step 2. Challenge these projects to come up with a plan to \u201cprove\u201d their model within a short but reasonable timeframe (e.g., 1-3 years). Offer them funding to cover all of their costs while they prove themselves, re-evaluating after each year. If they don\u2019t appear to make the bar, <a href=\"http://blog.givewell.org/2016/10/06/new-incentives-update/\">require them to try something else</a>.</li><li>When an organization does demonstrate their cost-effectiveness to an adequate degree, give them all the funding they need to scale up (e.g., by being a GiveWell top charity and receiving millions in funding).</li><li>Whether the organization succeeds or fails, write up and publicly publish copious notes and retrospectives, both qualitative and quantitative, on why the organization succeeded or failed.<br>&nbsp;</li></ol><p>My understanding is that this approach has two key differences from the approach employed by Karnofsky and Todd. First, step three requires each team to be producing information in a very tangible way -- they either succeed and demonstrate a successful program for scale-up or they fail and we learn from their failure. Second, step two could include additional selection criteria, such as <a href=\"http://www.givewell.org/research/intervention-reports#Priorityprograms\">being on a list of priority programs</a> or generating information relevant to identifying priority programs.</p><p>This contrasts with Todd\u2019s view that thorough evaluative work is not worth doing within the first few years of a new non-profit, since it takes a lot of work to know whether what you\u2019re doing is working. This also elaborates on Karnofsky\u2019s view that while you may not \u201crequire a strong evidence base before funding something\u201d, you still should aim toward building that evidence base.</p><p>I cannot claim that this process would properly balance false positives and false negatives, but it does look good that this process avoids the dual traps of continuing a program that appears to work but doesn\u2019t actually work (c.f., <a href=\"/ea/3i/an_example_of_dogooding_done_wrong/\">PlayPumps</a> and <a href=\"https://80000hours.org/articles/can-you-guess/\">many</a> <a href=\"http://blog.givewell.org/2009/12/28/celebrated-charities-that-we-dont-recommend/\">many</a> <a href=\"http://www.nyudri.org/aidwatcharchive/2011/02/in-zambia-pittsburgh-won/\">many</a> <a href=\"https://denisonvpc.wordpress.com/2012/06/21/the-gray-of-social-change/\">others</a>) while also not falling into the short-sightedness that Todd warns us about by being able to fund an early GiveWell, AMF, Charity Science Outreach, Giving What We Can, or Google.<br>&nbsp;</p><h2>Can We Fund the Future?</h2><p>A larger concern would be whether the process could risk falling into the narrow-mindedness that Karnofsky and Todd warn us about -- would we be able to recognize and fund work with long payoffs, like the Green Revolution, Peter Singer's early animal rights work, LGBTQ rights work in the '60s, or civil rights work in the '30s? Certainly you can\u2019t do a randomized controlled trial to see whether MIRI is actually reducing existential risk, but would that mean they could never get a grant under this incubation approach?</p><p>Accounting for long payoffs is possible, but would require a lot more domain expertise, including a few breakthroughs, in how we measure and evaluate charities. This may involve investing in more fundamental research to understand, e.g. protesting, political influencing, or science R&amp;D, before making concrete-level grants in very long-run areas.</p><p>Perhaps individual donors with sharp domain knowledge in a particular field may feel comfortable that they can identify hits without waiting for more fundamental research. I see that as the best argument for \u201chits-based giving\u201d. Whether or not making these type of long-term bets with high domain knowledge would outdo mid-range bets or short-term marginal improvements is, naturally, unclear.</p><p>Either way, I\u2019d encourage transparent grantmaking with a process that generates as much useful information for other future grantmakers. This incubation process seems quite promising to me, and I\u2019d love to see it scaled up to expand to other cause areas beyond global poverty, with the large-scale funding and transparency needed to find demonstrably good opportunities across many cause areas.</p>", "user": {"username": "Peter_Hurford"}}, {"_id": "Eh8Ep3fEPoRnnZwfK", "title": "Getting to the Mainstream", "postedAt": "2017-06-25T12:30:21.946Z", "htmlBody": "<html><body><p>&#xA0;</p>\n<p><span>I&#x2019;ve spent a lot of time thinking and writing about how we ought to live. The aim has always been to get people to behave more ethically, and so to bring about a better world. &#xA0;The Life You Can Save (TLYCS), an organization I founded and which I now serve as board president, focuses on making effective giving part of mainstream thinking about giving</span> <span>. &#xA0;I hope some of you will want to take a look at the organization&#x2019;s </span><a href=\"https://www.thelifeyoucansave.org/Strategic-Plan-2017-The-Life-You-Can-Save.pdf\"><span>Strategic Plan</span></a><span> for producing large-scale behavioral change. &#xA0;It provides some insights into our &#xA0;thinking, where we are today, and where we want to be in the future.</span></p>\n<p>&#xA0;</p>\n<p><span>In 2012, The Life You Can Save was a book, which I published in 2009, and a website set up by a friend and sjupporter, intended to promote the ideas of the book and encourage people to pledge to donate a percentage of their income to effective charities. &#xA0;At that time Charlie Bresler &#xA0;approached me, offering to turn TLYCS as a meta-charity, based both on his time and work (he became its unpaid Executive Director), and on his funding. &#xA0;I thought it was risky (relative to Charlie&#x2019;s alternative of giving his money to the Against Malaria Foundation) but a risk worth taking. &#xA0;Since then, I&#x2019;ve become more and more convinced that the bet has paid off handsomely. &#xA0;Last year, we moved $2.7m (a conservative estimate) to our recommended charities, more than $9 for every dollar spent on operating expenses. &#xA0;These metrics should continue to improve as growth in money moved has so far been strong in the current year, while expenses are roughly he same as last year.</span></p>\n<p>&#xA0;</p>\n<p><span>TLYCS has built a small but talented team led by Charlie (former president of the Men&#x2019;s Wearhouse) and COO Jon Behar (a 10 year veteran of the world&#x2019;s largest hedge fund). &#xA0;&#xA0;Our impact to date has been significant, and is growing at a steep trajectory, but this progress only represents the early stages of our plans. &#xA0;Ultimately, TLYCS wants to develop the capacity to introduce huge numbers of people to the idea of effective giving, and to have available the tools and messaging to get them to act. &#xA0;We also want to build a community that will nurture and increase their involvement over time. &#xA0;Our Strategic Plan explains the vision for making this happen, and how added capacity will translate to more impact. I hope you&#x2019;ll read the Strategic Plan </span><span>and consider </span><a href=\"https://tlycs.networkforgood.com/causes/3949-the-life-you-can-save\"><span>supporting TLYCS</span></a><span>.</span></p>\n<p><span>&#xA0;</span></p></body></html>", "user": {"username": "PeterSinger"}}, {"_id": "fFAH7b3orCexrFN78", "title": "Earning to Give as Costly Signalling", "postedAt": "2017-06-24T16:43:25.995Z", "htmlBody": "<html><body><p><span><span>There&apos;s a background belief that informs a lot of my Effective Altruism thinking, that might be a good time to challenge:</span></span></p>\n<p><span><span>I think most of the value of most earning-to-give is primarily&#xA0;a sort of costly signaling to attract the attention of the extremely rich (who completely dwarf the funding capabilities of the bulk of EA donors), *or* in donating to places that are for various reasons can use smaller amounts of startup money. (Either you have good reason to think they&apos;re useful that the current super-rich don&apos;t, funding smaller scale experiments, etc)</span></span></p>\n<p><span><span>(This comes with the caveat that, say, getting Elon Musk&apos;s attention isn&apos;t obviously net positive because he may or may not have actually understood what Superintelligence was warning about)</span></span></p>\n<p><span><span>This doesn&apos;t mean that earning to give isn&apos;t important, but it changes a bit about what sorts of earning to give are most important and why.</span></span></p>\n<p><span><span>The main argument I&apos;ve seen that points in a different direction is the notion that having all of your funding come from a few super-rich people makes you much more beholden to them, which can warp your choices. I think even in light of this I still believe the above, but maybe I should weight it differently.</span></span></p>\n<p><span><span>This has informed how I participated in a few different discussions, but I haven&apos;t had a discussion directly examining this belief. I&apos;m curious about people&apos;s thoughts.</span></span></p></body></html>", "user": {"username": "Raemon"}}, {"_id": "mH5Wq59rAqHCsjNEs", "title": "The Philanthropist\u2019s Paradox", "postedAt": "2017-06-24T10:23:58.519Z", "htmlBody": "<html><body><p>TL;DR. Many effective altruists wonder whether it&apos;s better to give now or invest and give later. I&#x2019;ve realised there is an additional worry for those who (like me) are sceptical of the value of the far future. Roughly, it looks like such people are rationally committed to investing their money and spending it in the future (potentially, at the end of time) even though they don&apos;t think this will do any good and they can see this whole problem coming. I don&apos;t think I&apos;ve seen this mentioned anywhere else, so I thought I&apos;d bring it to light. I don&#x2019;t have a resolution, which is why I call it a paradox</p>\n<p><strong>Setting a familiar scene: should you give now or invest and give later?</strong></p>\n<p>You&apos;re thinking about giving money to charity because you want to do some good. Then someone points out to you that, if you invested your money, it would grow over time and therefore you&apos;d be able to do more good overall. You don&apos;t believe in pure time discounting - i.e. you don&apos;t think 1 unit of happiness is morally worth more today than it is tomorrow - so you invest.</p>\n<p>As you think about this more, you realise it&#x2019;s always going to be better to keep growing the money instead of spending it now. You set up a trust that runs after your death and tell the executors of the trust to keeping investing the money until it will do as much good as possible. But when does the money get spent? It seems the money keeps on growing and never gets given away, so your investment ends up doing no good at all. Hence we have the philanthropist&#x2019;s paradox.</p>\n<p><strong>How to resolve the philanthropist&#x2019;s paradox?</strong></p>\n<p>There are lots of practical reasons you might think push you one way or the other: if you don&apos;t give now you&apos;ll never actually make yourself give later; there are better opportunities to give now; you&apos;ll know more later, so it&#x2019;s better to wait; the Earth might get destroyed, so you should give sooner; and so on. I won&apos;t discuss these as I&apos;m interested in the pure version of the paradox that leads to the conclusion you should give later.<a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftn1\"><span><!-- [if !supportFootnotes]--><span><span>[1]</span></span><!--[endif]--></span></a></p>\n<p>What&#x2019;s the solution if we ignore the practical concerns? One option is to note that, at some stage, you (or, your executors) will have enough money to solve all the world&#x2019;s problems. At that point, you should spend it as there&#x2019;s no value in growing your investment further. This won&#x2019;t work if the financial costs of solving the world&#x2019;s problems keeps growing and grows faster than your investment increases. However, if one supposes the universe will eventually end &#x2013; all the stars will burn out at some point &#x2013; then you will eventually reach a stage where it&#x2019;s better to spend the money. If you wait any longer there won&#x2019;t be any people left. This might not be a very satisfactory response, but then it is called a &#x2018;paradox&#x2019; for a reason.</p>\n<p><strong>A new twist for those who aren&#x2019;t convinced about the value of the far future</strong></p>\n<p>The above problem implicitly assumed something like totalism, the view on which the best history of the universe is the one with the greatest total of happiness. If you&#x2019;re totalist, you will care about helping those who wiil potentially exist in millions of years.</p>\n<p>However, totalism is not the only view you could take about the value of future people. We might agree with Jan Narveson who stated &#x201C;we are in favour of making people happy, but neutral about making happy people&#x201D;<a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftn2\"><span><!-- [if !supportFootnotes]--><span><span>[2]</span></span><!--[endif]--></span></a>. Views of this sort are typically called &apos;person-affecting&#x2019; (PA) views.</p>\n<p>There isn&#x2019;t a single person-affecting view, but a family of them. I&#x2019;ll quickly introduce them before explaining the new version of the paradox the face. The three most common person-affecting theories are:</p>\n<p>Presentism: the only people who matter are those who presently exist (rather than those who might or will exist in the future)</p>\n<p>Actualism: the only people who matters are those who actually, rather than merely possibly, exist (this means future actual people do count)</p>\n<p>Necessitarianism: the only people who matter, when deciding between a set of outcomes, are those people who exist in all the outcomes under consideration. This is meant to exclude those whose existence is contingent on outcome of the current decision.</p>\n<p>Each of the view captures the intuitive that creating some new person is not good: that person does not presently, actually, or necessarily exist. I won&#x2019;t try to explain why you might like these views here (but see this footnote if you&#x2019;re interested).<a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftn3\"><span><!-- [if !supportFootnotes]--><span><span>[3]</span></span><!--[endif]--></span></a></p>\n<p>I should note you could also think the far future doesn&#x2019;t matter (as much) because you believe in pure time discounting (e.g. 1 unit of happiness next year is morally worth 98% of one unit of happiness this year). Whether you give now or later, if you endorse pure time discounting, just depends on whether the percentage annual increase in your money is higher or lower than the percentage annual decrease in the moral value of the future. I don&#x2019;t think pure time discounting is particularly plausible, but discussing it is outside the scope of this essay.<a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftn4\"><span><!-- [if !supportFootnotes]--><span><span>[4]</span></span><!--[endif]--></span></a></p>\n<p><strong>The (Person-Affecting) Philanthropist&#x2019;s Paradox, a tale of f</strong><strong><span>oreseeable r</span></strong><strong>egret</strong></p>\n<p>I&#x2019;ll come back to other person-affecting views later, but, for now, suppose you&#x2019;re a presentist philanthropist, which means you just care about benefitting currently existing people, and you found the &#x2018;give later&#x2019; argument convincing. What should you do?</p>\n<p>Option 1: You could give away your money now in 2017. Say that will bring about 100 units of happiness.</p>\n<p>Option 2: You could invest it. Following the logic of the paradox you put the money in trust and it doubles every 50 years or so. Now, after 200 years, in 2217, your investment can do 16 times more good.</p>\n<p>We can feel a problem coming. The presentist judges outcomes by how they affect presently existing people. Assuming that no one alive at 2017 is also alive in 200 years, at 2217, nothing that happens at 2217 can count as good or bad from the perspective of a decision made at 2017. So, although we might have thought waiting until 2217 and giving later would do more good, it turns out the presentist should think it does no good at all.</p>\n<p>Realising the trap awaiting him, what should the presentist do? What he could do is invest the money for just 50 years before giving it away (assume he&#x2019;s a young philanthropist). This allows him to double his donation. Let&#x2019;s assume he can use the money at 2067 to benefit only people who were presently alive at 2017. There is a clearly superior outcome to giving now at 2017 as he has less money than he would do at 2067. Remember, presentists doesn&#x2019;t entail pure time discounting: a presentist can be neutral about giving someone a painkiller now versus giving a painkiller to that same person in 50 years&#x2019; time. Why? that person presently existed at the time when the decision was taken. Hence providing twice as many benefits at 2067 rather than 2017, given they are to the same people, is twice as good.</p>\n<p>Yet now we find a new oddness. Suppose those 50 years have passed and the presentist is now about to dole out his investment. The presentist pauses for a moment and thinks &#x201C;how can I most effectively benefit presently existing people?&#x201D; He&#x2019;s at 2067 and there is a whole load of new presently-existing people. They didn&#x2019;t exist at 2017, it&#x2019;s true, but the presentist is presently at 2067 and is a making a decision on that basis. Now the presentist finds himself facing exactly the same choice at 2067 that he faced at 2017, whether to give now or give later.</p>\n<p>All the same logic applies so he decides, once again, that he should give later. Knowing he won&#x2019;t live forever he puts the money in a trust and instructs the executors to &#x201C;most effectively benefit those will presently exist at 2117&#x201D;. But this situation will recur. Every time he (or rather, his executors) consider whether to give now or give later it will always do more good, on presentism, to invest with a view to giving later. This leads him through a series of decisions that means the money ends up being donated in the far future (at the death of the universe), at which point none of the people who presently existed at 2017 will be alive. Thus, the donation ends up being valueless, whereas if he&#x2019;d just donated immediately, in 2017, he would have done at least some good.</p>\n<p>It&#x2019;s worth noting the difference between the presentist case and the earlier, totalist one. It might seem strange that the totalist should wait so long until his money gets spent, but at least this counted as doing a lot of good on totalism. Whereas the presentist runs through the same rational process as the totalist, also gives away his money at the end of time, but this counts as doing no good on presentism at all. Further, the presentist could foresee he would choose to later do things he currently (at 2017) considers will have no value. Hence the presentist has an extra problem in this paradox.</p>\n<p><strong>What should the presentist do?</strong></p>\n<p>One thing the presentist might do is to pre-commit himself to spending him money at some later time. Here, he faces a trade-off. He knows, if he invests the money, it will grow at X% a year. He also knows that the people who presently exist will all eventually die. Say 1% of the Earth&#x2019;s population who are alive at 2017 will die each year (assume this doesn&#x2019;t include him; he&#x2019;s immortal, or something, for our purposes). Hence at 2067 half of them are alive. At 2117 they&#x2019;ve all died and, from the perspective of 2017, nothing that happens can now good or bad. Let&#x2019;s assume he works this out at realises the most good he can do is by legally binding himself at 2017 to spend the money he&#x2019;ll have at 2067.</p>\n<p>This seems to solve the problem, but there is something weird about it. When 2067 rolls around, they&#x2019;ll be new people who will presently exist. He&#x2019;ll be annoyed at his past self for tying his hands because what he, at 2067, wants to do is invest the money for just a bit longer to help them. I don&#x2019;t have a better solution that this, but I would welcome someone suggesting one.</p>\n<p>We could consider this a <em>reductio ad absurdam</em> against presentism, a fatal problem with presentism that causes us to abandon it. I&#x2019;m not sure it is &#x2013;&#xA0; a point I&#x2019;ll come back to at the end &#x2013; but it does seem paradoxical.<a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftn5\"><span><!-- [if !supportFootnotes]--><span><span>[5]</span></span><!--[endif]--></span></a> If it is a <em>reductio</em>, isn&#x2019;t uniquely a problem for presentism either: necessitarianism will face a similar kind of problem. I won&#x2019;t discuss actualism because, for reasons also not worth getting into here, actualism isn&#x2019;t action-guiding.<a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftn6\"><span><!-- [if !supportFootnotes]--><span><span>[6]</span></span><!--[endif]--></span></a></p>\n<p><strong>Why necessitarian philanthropists get same problem</strong></p>\n<p>Necessitarians think the only people that matter are those who exist in all outcomes under consideration, hence we exclude the people whose existence is contingent on what we do.</p>\n<p>As Parfit and others have noted, it looks like nearly any decision will eventually change the identity of all future people.<a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftn7\"><span><!-- [if !supportFootnotes]--><span><span>[7]</span></span><!--[endif]--></span></a> Suppose the necessitarian philanthropist decides not to spend his money now, but to invest it instead. This causes the people who would have benefitted from his money, had he spent it now, to make slightly different decisions. Even tiny decisions will ripple through society, causing different people to meet and conceive children at very slightly different times. As one&#x2019;s DNA is a necessary condition for your identity, this changes the identities of future people, who become contingent people.</p>\n<p>This means the necessitarian has a similar difficulty in valuing the far future as the presentist, albeit it for different reasons. To a presentist there&#x2019;s no point benefitting those who will live in 10,000 years because such people do not presently exist. To a necessitarian there&#x2019;s no way you could benefit people in 10,000 years&#x2019; time, no matter how hard you try, because whatever you do will change who those people are (hence making them the non-necessary people who don&#x2019;t matter on the theory).</p>\n<p>To illustrate, you want to help the X people, some group of future humans who you know will get squashed by an asteroid that will hit the Earth in 10,000 years&#x2019; time. You decide to act &#x2013; you raise awareness, build a big asteroid-zapping laser, etc. &#x2013; but your actions effect who gets born, meaning the Y people to be created instead of the original X people. On necessitarianism it&#x2019;s not good for the X people if they are replaced with the Y people, nor it is good to create the Y people either (it&#x2019;s never good for anyone to be created).</p>\n<p>Hence, given that all actions eventually change all future identities, necessitarians should accept there&#x2019;s a practical limit to how far in the future they can do good.<a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftn8\"><span><!-- [if !supportFootnotes]--><span><span>[8]</span></span><!--[endif]--></span></a> There might be uncertainty on what this limit is.<a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftn9\"><span><!-- [if !supportFootnotes]--><span><span>[9]</span></span><!--[endif]--></span></a> However, just as the presentist should worry about acting sooner rather than later because the number of presently existing people will dwindle the further from 2017 his money gets used, so the necessitarian will find himself with an effective discount rate (even though he doesn&#x2019;t engage in pure time discounting): his act to give now or give later causes different people to be born. Hence if he invests for 50 years and then gives that money to a child who, at 2067, is currently aged 10, that child&#x2019;s existence is presumably contingent on his investing the money. As the necessitarian discounts contingent people, he cannot claim investing and then using that money to benefit a contingent existing child is good. This is analogous to the presentist in 2017 realising there&#x2019;s no point saving money to give to 10-year old in 2067 because that 10-year-old does not, in 2017, presently exist</p>\n<p><strong>What can the necessitarian do to avoid the paradox?</strong></p>\n<p>I can think of one more move the necessitarian could make. He could argue his investing the money doesn&#x2019;t change any identities of future people, so it really is better, on his theory, to invest for it for many years.</p>\n<p>This is less helpful that it first appears. If investing makes no difference to who gets born, presumably the necessitarian is now back in the same boat as the totalist: both agree it&#x2019;s best to keep growing the cash until the end of time. The problem for the necessitarian is one of the things he view seems to commit him to is believing we can&#x2019;t help people in the far future because anything we do will alter all the identities. He&#x2019;s in a bind: he can&#x2019;t simultaneously believe far future people don&#x2019;t matter and that his investment does the most good if it&#x2019;s spent in the far future.</p>\n<p>All this is to say person-affecting views faces an additional twist to the philanthropist&#x2019;s paradox. These aren&#x2019;t to be waved away as theoretical fancies: there are real-world philanthropists who appear to have person-affecting views: they don&#x2019;t care about the far future and they think it&#x2019;s better to make people happy, rather than make happy people. If they want to do the most good with their money, this is paradox they should find a principled response to when they consider whether to give nor or give later.</p>\n<p><strong>Epilogue: a new reason not to be a person-affecting philanthropist?</strong></p>\n<p>Should we give up on person-affecting views because of this paradox? Maybe, but I doubt it. Two thoughts. First, there&#x2019;s well-established fact that all views in population ethics have weird outcomes. The bar for &#x2018;plausible theories&#x2019; is accepted to be pretty low.<a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftn10\"><span><!-- [if !supportFootnotes]--><span><span>[10]</span></span><!--[endif]--></span></a> I can imagine an advocate of presentism or necessitarianism acknowledging this as just another bullet to bite, and this is still, all things considered, the theory he believes it the least-worst one.</p>\n<p>Second, it&#x2019;s not clear to me exactly where the problem lies. I&#x2019;m unsure if this should be understand of a problem for rationality (making good decisions), axiology (what counts as &#x2018;good&#x2019;), or maybe the way they are linked.<a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftn11\"><span><!-- [if !supportFootnotes]--><span><span>[11]</span></span><!--[endif]--></span></a> Perhaps what person affecting theories need is an account of why you should (or shouldn&#x2019;t) be able to foreseeably regret your future decisions.</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<div>\n<p><!-- [if !supportFootnotes]--></p>\n<hr>\n<p><a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftnref1\"><span><span><span>[1]</span></span><!--[endif]--></span></a> See this summary for a note on the practical concerns: <a href=\"/ea/4e/giving_now_vs_later_a_summary/\">http://effective-altruism.com/ea/4e/giving_now_vs_later_a_summary/</a></p>\n<p><a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftnref2\">[2]<!--[endif]--></a> <!-- [if supportFields]><span style='font-size:\n10.0pt'><span style='mso-element:field-begin;mso-field-lock:yes'></span>ADDIN\nCSL_CITATION { &quot;citationItems&quot; : [ { &quot;id&quot; :\n&quot;ITEM-1&quot;, &quot;itemData&quot; : { &quot;author&quot; : [ {\n&quot;dropping-particle&quot; : &quot;&quot;, &quot;family&quot; :\n&quot;Narveson&quot;, &quot;given&quot; : &quot;Jan&quot;,\n&quot;non-dropping-particle&quot; : &quot;&quot;, &quot;parse-names&quot; :\nfalse, &quot;suffix&quot; : &quot;&quot; } ], &quot;container-title&quot; :\n&quot;The Monist&quot;, &quot;id&quot; : &quot;ITEM-1&quot;, &quot;issued&quot;\n: { &quot;date-parts&quot; : [ [ &quot;1973&quot; ] ] }, &quot;page&quot; :\n&quot;62-86&quot;, &quot;title&quot; : &quot;Moral problems of\npopulation&quot;, &quot;type&quot; : &quot;article-journal&quot; },\n&quot;uris&quot; : [ &quot;http://www.mendeley.com/documents/?uuid=e51b9c2f-0397-4b0c-8c5d-c57c8faf0187&quot;\n] } ], &quot;mendeley&quot; : { &quot;formattedCitation&quot; : &quot;Jan Narveson,\n\\u201cMoral Problems of Population,\\u201d &lt;i&gt;The Monist&lt;/i&gt;, 1973,\n62\\u201386.&quot;, &quot;plainTextFormattedCitation&quot; : &quot;Jan Narveson,\n\\u201cMoral Problems of Population,\\u201d The Monist, 1973, 62\\u201386.&quot;,\n&quot;previouslyFormattedCitation&quot; : &quot;Jan Narveson, \\u201cMoral\nProblems of Population,\\u201d &lt;i&gt;The Monist&lt;/i&gt;, 1973,\n62\\u201386.&quot; }, &quot;properties&quot; : { &quot;noteIndex&quot; : 0 },\n&quot;schema&quot; :\n&quot;https://github.com/citation-style-language/schema/raw/master/csl-citation.json&quot;\n}<span style='mso-element:field-separator'></span></span><![endif]-->Jan Narveson, &#x201C;Moral Problems of Population,&#x201D; The Monist, 1973, 62&#x2013;86.<!-- [if supportFields]><span style='font-size:10.0pt'><span\nstyle='mso-element:field-end'></span></span><![endif]--></p>\n<p><a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftnref3\">[3]<!--[endif]--></a> A key motivation for PA is the person-affecting restriction(PAR): one state of affairs can only be better than another if it better for someone. This is typically combined with existence non-comparativism:&#xA0;existence is neither better nor worse for someone than non-existence. The argument for existence non-comparativism most famously comes from John Broome, who puts it:</p>\n<p>...[I]t cannot ever be&#xA0; true that&#xA0; it&#xA0; is&#xA0; better&#xA0; for&#xA0; a&#xA0; person&#xA0; that&#xA0; she&#xA0; lives&#xA0; than&#xA0; that&#xA0; she&#xA0; should&#xA0; never&#xA0; have&#xA0; lived&#xA0; at&#xA0; all.&#xA0; If&#xA0; it&#xA0; were&#xA0; better&#xA0; for&#xA0; a&#xA0; person&#xA0; that&#xA0; she&#xA0; lives&#xA0; than&#xA0; that&#xA0; she should never have lived at all, then if she had never lived at all, that would have been worse for her than if she had lived. But if she had never lived at all, there would have been no her for it to be worse for, so it could not have been worse for her.</p>\n<p>I won&#x2019;t motivate them or critique them further. My objective here is just to indicate a problem for them.</p>\n<p><a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftnref4\">[4]<!--[endif]--></a> As Greaves put the argument against pure time discounting: &#x201C;But of course (runs the thought)&#xA0; the&#xA0; value&#xA0; of utility is&#xA0; independent&#xA0; of&#xA0; such&#xA0; locational&#xA0; factors:&#xA0; there&#xA0; is&#xA0; no&#xA0; possible&#xA0; justification&#xA0; for&#xA0; holding&#xA0; that&#xA0; the&#xA0; value&#xA0; of(say) curing someone&#x2019;s headache,&#xA0; holding fixed her psychology,&#xA0; circumstances and&#xA0; deservingness,&#xA0; depends&#xA0; upon&#xA0; which&#xA0; year&#xA0; it&#xA0; is&#x201D;&#xA0; From Greaves, H. Discounting and public policy: A survey&apos;. Conditionally accepted at&#xA0;Economics and Philosophy (link: <a href=\"http://users.ox.ac.uk/~mert2255/papers/discounting.pdf\">http://users.ox.ac.uk/~mert2255/papers/discounting.pdf</a>)</p>\n<p><a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftnref5\">[5]<!--[endif]--></a> By &#x2018;paradoxical&#x2019; I mean that seeming acceptably premises and seemingly acceptable reasoning leading to seemingly unacceptable conclusions.</p>\n<p><a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftnref6\"><!-- [if !supportFootnotes]-->[6]<!--[endif]--></a> How good an outcome is depends on which outcome you choose to bring about, so you can&#x2019;t know what you should do until you already know what you&#x2019;re going to do. Actualists might respond this is the best we can do.</p>\n<p><a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftnref7\">[7]<!--[endif]--></a> See Reasons and Persons (1984).</p>\n<p><a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftnref8\">[8]<!--[endif]--></a> As an example, if a necessitarian put nuclear missiles on the Moon set to explode in 5,000 years time, and an alien space ships happens to appear in 5,000 years, the necessitarian will admit he&#x2019;s (unwittingly) made things worse. A presentist will (oddly) claim this is not bad on the assumption the Moon-visitors haven&#x2019;t yet been born. However, if they Moon-visitors were presently alive when the missiles were put on the moon, the presentist would say the outcome is bad.</p>\n<p><a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftnref9\">[9]<!--[endif]--></a> For instance, we might think it takes some months or years before me choosing to buy tea rather than coffee at the super-market changes the identities of all future people. If you find the idea actions could change who gets born, ask yourself if you think you would have been born if World War One had never occurred.</p>\n<p><a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftnref10\">[10]<!--[endif]--></a> For a list of some problems see <!-- [if supportFields]><span style='mso-element:\nfield-begin;mso-field-lock:yes'></span>ADDIN CSL_CITATION {\n&quot;citationItems&quot; : [ { &quot;id&quot; : &quot;ITEM-1&quot;,\n&quot;itemData&quot; : { &quot;author&quot; : [ { &quot;dropping-particle&quot;\n: &quot;&quot;, &quot;family&quot; : &quot;Greaves&quot;, &quot;given&quot; :\n&quot;Hilary&quot;, &quot;non-dropping-particle&quot; : &quot;&quot;,\n&quot;parse-names&quot; : false, &quot;suffix&quot; : &quot;&quot; } ],\n&quot;container-title&quot; : &quot;Philosophy Compass&quot;, &quot;id&quot; :\n&quot;ITEM-1&quot;, &quot;issued&quot; : { &quot;date-parts&quot; : [ [\n&quot;2017&quot; ] ] }, &quot;title&quot; : &quot;Population Axiology&quot;,\n&quot;type&quot; : &quot;article-journal&quot; }, &quot;uris&quot; : [\n&quot;http://www.mendeley.com/documents/?uuid=8e3bbf18-8641-4d42-bc61-65a75d671da2&quot;\n] } ], &quot;mendeley&quot; : { &quot;formattedCitation&quot; : &quot;Hilary\nGreaves, \\u201cPopulation Axiology,\\u201d &lt;i&gt;Philosophy\nCompass&lt;/i&gt;, 2017.&quot;, &quot;plainTextFormattedCitation&quot; :\n&quot;Hilary Greaves, \\u201cPopulation Axiology,\\u201d Philosophy Compass,\n2017.&quot; }, &quot;properties&quot; : { &quot;noteIndex&quot; : 0 },\n&quot;schema&quot; :\n&quot;https://github.com/citation-style-language/schema/raw/master/csl-citation.json&quot;\n}<span style='mso-element:field-separator'></span><![endif]-->Hilary Greaves, &#x201C;Population Axiology,&#x201D; Philosophy Compass, 2017.<!-- [if supportFields]><span\nstyle='mso-element:field-end'></span><![endif]--></p>\n<div>\n<p><a title=\"\" href=\"file:///C:/Users/Michael/Desktop/phil%20paradox.docx#_ftnref11\"><!-- [if !supportFootnotes]-->[11]<!--[endif]--></a> In recent discussion Patrick Kaczmarek informs me I&#x2019;m absolutely mistaken to think it can problem with decision theory and helpfully suggested the issue might be the bridging principle between one&#x2019;s axiology and one&#x2019;s decisions theory.&#xA0;</p>\n</div>\n</div></body></html>", "user": {"username": "MichaelPlant"}}, {"_id": "P6ndNnsF9axjBgGXq", "title": "Upcoming AMA with Luke Muehlhauser on consciousness and moral patienthood (June 28, starting 9am Pacific)", "postedAt": "2017-06-21T21:56:48.696Z", "htmlBody": "<html><body><p>Luke Muehlhauser of the&#xA0;<a href=\"http://www.openphilanthropy.org/\">Open Philanthropy Project</a>&#xA0;recently published a&#xA0;<a href=\"http://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood\">major report</a>&#xA0;on animal consciousness and the question of &quot;moral patienthood&quot; &#x2014; i.e. which beings merit moral concern? The purpose of the report is to inform Open Phil&apos;s grantmaking, especially in its&#xA0;<a href=\"http://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\">farm animal welfare</a>&#xA0;focus area. Luke would like to hear your questions and objections, and he will host an &quot;Ask Me Anything&quot; session on the issues discussed in the report, here on the Effective Altruism Forum, starting at&#xA0;<span>9am</span>&#xA0;Pacific on&#xA0;<strong>Wednesday, June 28th</strong>.</p>\n<p>I hope you will read the report and then join in with lots of questions about the topics it covers: consciousness, moral patienthood, animal cognition, meta-ethics, moral weight, illusionism, hidden qualia, and more!</p>\n<p>Luke&#xA0;would also like to note that much of the most interesting content in the report is in the appendices and even some footnotes, e.g. on&#xA0;<a href=\"http://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#AppendixC\">unconscious vision</a>,&#xA0;on&#xA0;<a href=\"http://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#AppendixB\">what a more satisfying theory of consciousness might look like</a>,&#xA0;and an explanation of attention schema theory (footnote 288).</p>\n<p>&#xA0;</p>\n<p><em>(In case it&apos;s confusing why I&apos;m posting this: I&apos;m coming on as a moderator of the Forum, and will post shortly with more info about that.)</em></p></body></html>", "user": {"username": "Julia_Wise"}}, {"_id": "SrnWxsApLoNTGnc5P", "title": "80,000 Hours articles aimed at the EA community", "postedAt": "2017-06-20T22:19:34.311Z", "htmlBody": "<html><body><p>We&apos;ve recently produced a few&#xA0;significant pieces that we expect to be of interest to people who are quite involved in the effective altruism community:</p>\n<ul>\n<li>Career review of <a href=\"https://80000hours.org/career-reviews/working-at-effective-altruist-organisations/\">Working at effective altruist organisations</a></li>\n<li>Guide to <a href=\"https://80000hours.org/articles/ai-policy-guide/\">working in AI policy and strategy</a></li>\n<li><a href=\"https://itunes.apple.com/us/podcast/80-000-hours-podcast/id1245002988\">The 80,000 Hours podcast</a>. First episode: &apos;<a href=\"https://80000hours.org/2017/06/the-world-desperately-needs-ai-strategists-heres-how-to-become-one/\">The world desperately needs AI strategists. Here&#x2019;s how to become one</a><em>.</em>&apos;</li>\n</ul>\n<p>Other content&#xA0;that should interest folks here includes:</p>\n<ul>\n<li><a href=\"https://80000hours.org/job-board/\">June update to our Job Board</a></li>\n<li>Explanation of <a href=\"https://80000hours.org/career-guide/community/\">why being involved in the EA community is valuable</a></li>\n<li><a href=\"https://80000hours.org/2017/05/most-people-report-believing-its-incredibly-cheap-to-save-lives-in-the-developing-world/\">Most people report believing it&#x2019;s incredibly cheap to save lives in the developing world</a></li>\n<li><a href=\"https://80000hours.org/2017/06/the-schwarzman-scholarship-an-exciting-opportunity-to-learn-more-about-china-and-get-a-masters-in-global-affairs/\">An exciting opportunity to learn more about China and get a Masters in Global Affairs</a></li>\n</ul>\n<p>And for those who want to earn to give:</p>\n<ul>\n<li><a href=\"https://80000hours.org/2017/05/how-much-do-hedge-fund-traders-earn/\">How much do hedge fund traders earn?</a></li>\n<li><a href=\"https://80000hours.org/articles/highest-paying-jobs/\">What are actually the highest paying jobs?</a></li>\n</ul>\n<p>&#xA0;</p></body></html>", "user": {"username": "80000_Hours"}}, {"_id": "C665bLMZcMJy922fk", "title": "What is valuable about effective altruism? Implications for community building", "postedAt": "2017-06-18T14:49:56.832Z", "htmlBody": "<p>If we\u2019re interested in building the best version of effective altruism, it\u2019s natural to spend some time thinking about why people should join.</p>\n<p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994954/mirroredImages/C665bLMZcMJy922fk/f8skk0mcdo1jgafom7mq.png\"></p>\n<p>Presumably people should join because it\u2019s valuable, but how is it valuable? In fact there are a couple of different versions of the question, according to whose perspective of \u201cvaluable\u201d we are using. They both seem relevant, and they have different answers:</p>\n<p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994954/mirroredImages/C665bLMZcMJy922fk/k7pkir6wvzb4flbcypw6.png\"></p>\n<p>First is the <em>impersonal perspective</em>: in what ways does effective altruism (as a community, or an intellectual project) help the world? This is important to understand because it can help us to focus on steering towards versions which are more valuable. [One might also have other personal reasons for wanting to get people to join the movement, but in practice I think these are usually significantly weaker.]</p>\n<p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994954/mirroredImages/C665bLMZcMJy922fk/qzppzszn0n4ylx6xfplb.png\"></p>\n<p>Second is the <em>personal perspective</em>: for individuals who might engage with effective altruism, why is it valuable for them personally to do so? Impersonal reasons can be a part of this, but may not be the whole story. We might think of this as asking: what is the <a href=\"https://www.wikipedia.com/en/Value_proposition\"> value proposition </a> for effective altruism? This is important to understand because it can help us to build a version which appeals more strongly to the people we would like to attract.</p>\n<h3>Impersonal perspective</h3>\n<p>Effective altruism helps the world by causing individuals to do more good in their lives. Roughly, the degree to which it helps depends on:</p>\n<ol>\n<li>\n<p>The number of individuals who take valuable action thanks to effective altruism</p>\n</li>\n<li>\n<p>The capabilities and resources of those individuals</p>\n</li>\n<li>\n<p>The degree to which they take valuable action</p>\n</li>\n</ol>\n<p>The degree to which individuals take valuable action in turn depends on:</p>\n<ol>\n<li>\n<p>Their <strong>values</strong> -- how much they are aiming for something which creates a better world by our lights;</p>\n</li>\n<li>\n<p>Their <strong>knowledge</strong> about what actions are effective in pursuit of those values;</p>\n</li>\n<li>\n<p>The degree to which they take <strong>action</strong> based on that knowledge.</p>\n</li>\n</ol>\n<p>In order to be significantly valuable, effective altruism wants to affect people who between them have significant resources, and to shift them significantly in a positive direction on one or more of these axes. The relative importance of each of the axes depends on the degree to which each is currently a bottleneck on actually helping the world (for the people who engage with effective altruism).</p>\n<h3>Personal perspective</h3>\n<p>Effective altruism also offers significant personal benefits for people who engage with it. In practice these can be a large part of what appeals to and motivates individuals. (This section is just observational, rather than an attempt at comprehensive analysis, and it might miss something important.)</p>\n<p>I see three potentially large value propositions for individuals engaging with effective altruism:</p>\n<ol>\n<li>The <strong>knowledge </strong>about how to be effective at altruism;</li>\n<ul>\n<li>Offering people knowledge about what is effective (or tools for finding this) directly helps them to pursue goals they already have.</li>\n</ul>\n<li>A <strong>community</strong>;</li>\n<ul>\n<li>Communities offer socialising, friendship, support. A common goal lets people help each other and band together in ways often lacking in today\u2019s society. Communities can create opportunities for their members.</li>\n</ul>\n<li>A sense of <strong>meaning</strong>.</li>\n<ul>\n<li>For some people, effective altruism can give them an enhanced sense of personal purpose or significance (which can be reinforced by having a community but does not necessarily rely on one). It might offer a turn away from nihilism or generally feeling helpless in such a large world.</li>\n<li>This is a mechanism for what is valuable from the impersonal perspective to also be valuable from the personal perspective.</li>\n</ul>\n</ol>\n<p>This suggests an empirical question:</p>\n<p><em>How important are these different value propositions in attracting people? How does it vary by person?</em></p>\n<p>There has been some work on this already, for example asking people what they found valuable about local communities in a <a href=\"/ea/zw/the_2015_survey_of_effective_altruists_results/\"> 2015 survey of effective altruists </a> . It would be great to see much more explicit investigation.</p>\n<p>It also suggests a strategic question:</p>\n<p><em>To what extent should we try to separate the value propositions?</em></p>\n<p>Separating the value propositions means making it easier for people to access one without the others. For example by presenting the community as <strong>a</strong> community of people following the ideas of effective altruism rather than <strong>the</strong> community, or making it easy for people to access the knowledge without getting pushed to join the community.</p>\n<p>Tightly linking the value propositions might produce a more attractive total package (to individuals), and help people to move from one source of value to another. But separation could be helpful if some people were put off by some aspects. For example, although meaning can be a strong draw for some people, it can play into perceptions that effective altruism is weird or demanding. Separation could also help to insulate against catastrophic failure: for example if there were a scandal in the community, the reputation of the knowledge and tools that have been developed would likely be less damaged if they were more separate.</p>\n<h3>Synergies and tensions between impersonal and personal perspectives</h3>\n<p>From the impersonal perspective, there were three axes (values, knowledge, action) that it could be good to move individuals on. Is being moved also valuable for the individuals?</p>\n<p>I think it depends. For each axis there are ways of encouraging people to move which are likely to feel valuable for the individual, and ways which may feel hostile or inconsiderate.</p>\n<p>Increasing knowledge about what is effective is helpful from the impersonal perspective, but it\u2019s also one of the value propositions for individuals. This is great: sharing knowledge becomes a win-win, since it\u2019s desired by both sides. It also gives us perspective on why decision-relevant research is particularly important: it creates knowledge which will often be acted on, and through being shared can attract people towards effective altruism.</p>\n<p>(This is true of offering knowledge in an even-handed way. Trying to get someone to believe a particular proposition can get into the realm of propaganda, which is generally not considerate.)</p>\n<p>In contrast to offering people knowledge, which generally feels cooperative, trying to improve someone\u2019s values (by your standards) or pressuring them to take greater action can more easily be a non-cooperative interaction. I actually think that discussions about ethical issues or what we should do are very valuable if undertaken with a spirit of open-mindedness and humility, but they can take on an adversarial character if people rather take the approach of trying to persuade others of what they find to be certain truths.This creates a tension between what is impersonally valuable (them making large such shifts) and what is valuable to them personally (perhaps not doing so).</p>\n<p>In the case of demanding action, this tension has been <a href=\"https://www.effectivealtruism.org/faqs-criticism-objections/#is-effective-altruism-too-demanding\"> recognised </a> and <a href=\"http://lukemuehlhauser.com/effective-altruism-as-opportunity-or-obligation/\"> discussed </a> at least somewhat within the community (without consensus). I think the case of trying to shift values is analogous; more of the <a href=\"https://rationalaltruist.com/2013/06/13/against-moral-advocacy/\"> related discussion </a> seems to be <a href=\"/ea/mx/effective_altruism_is_a_big_tent/\"> against pushing people </a> , but this may be <a href=\"/ea/176/use_care_with_care/\"> in response to </a> observing <a href=\"https://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai\"> potentially damaging behaviour </a> .</p>\n<p>In both cases my personal view is that we should err on the side of not pushing people. There are substantial benefits to being <a href=\"https://www.centreforeffectivealtruism.org/blog/considering-considerateness-why-communities-of-do-gooders-should-be/\"> considerate in our interactions </a> with people who might join the community. A particular worry is that pushing people can create hostility towards effective altruism (anecdotally, it has in some cases; it would be great to see a thorough investigation of this). I\u2019ve argued it\u2019s important for the long-term growth of the movement that we take pains to <a href=\"http://globalprioritiesproject.org/wp-content/uploads/2015/05/MovementGrowth.pdf\"> avoid hostility </a> . And I\u2019m particularly concerned that extremely reasonable people (whom we would like to attract) are particularly attracted to communities they see as offering support rather than pressure.</p>\n<p>There are costs to being careful about this. A desire to avoid pushing people\u2019s values might mean being slightly more hesitant to recommend working to <a href=\"http://www.existential-risk.org/concept.html\"> reduce existential risk </a> without discussing values first, since the importance of the cause area depends on ethical views.</p>\n<p>However, I don\u2019t think we need to push values or actions where not wanted, because there are ways to help people move on these axes which build out of the personal value propositions:</p>\n<ul>\n<li>Having a sense of meaning can be motivating for action.</li>\n<li>A supportive community can:</li>\n<ul>\n<li>Help people to think through and clarify their values.</li>\n<ul>\n<li>This is only valuable in the impersonal sense if many people, when they reflect on their values they move more closely into alignment with typical \u2018effective altruist\u2019 values; for example thinking that cost-effectiveness is morally relevant. I do think this is probably true, but my reasons for believing this are mostly anecdotal.</li>\n</ul>\n<li>Help support them to take action that might otherwise be difficult.</li>\n<ul>\n<li>For example, I think that the Giving What We Can community has been great in helping support people in making a decision to donate a slice of their income (although it has attracted at least <a href=\"/ea/180/advisory_panel_at_cea/aej\">some criticism</a> for pushing lifetime commitments to students).</li>\n</ul>\n</ul>\n<li>We can offer knowledge which helps people to reflect on their own core values, or to plan to be personally more effective.</li>\n</ul>\n<p>While there could still be tension when there is an effective way to shift people which is not considerate to their preferences, not doing so is likely to mean shifting them less efficiently rather than not at all, which is a smaller price and easier for the benefits of considerateness to overcome.</p>\n<h3>Conclusions:</h3>\n<ul>\n<li>\n<p>For researchers:</p>\n</li>\n<ul>\n<li>\n<p>Consider further investigation of these two questions:</p>\n</li>\n<ul>\n<li>\n<p>[Empirical] How important are the different value propositions in attracting people? How does it vary by person?</p>\n</li>\n<li>\n<p>[Strategic] To what extent should the EA community try to separate the value propositions?</p>\n</li>\n</ul>\n</ul>\n<li>\n<p>For community-builders:</p>\n</li>\n<ul>\n<li>\n<p>We should be cooperative towards potential members:</p>\n</li>\n<ul>\n<li>\n<p>We should not mislead people about what is hoped-for from the impersonal perspective;</p>\n</li>\n<li>\n<p>We should commit to offering some things which are valuable from the personal perspective rather than the impersonal perspective (when the costs are worthwhile).</p>\n</li>\n</ul>\n</ul>\n<li>\n<p>For community members:</p>\n</li>\n<ul>\n<li>\n<p>When talking to other people who might get involved in effective altruism, it can be helpful to bear in mind how what they are likely to get out of it differs from the benefits they are likely to provide by being involved.</p>\n</li>\n<li>\n<p>In particular:</p>\n</li>\n<ul>\n<li>\n<p>We should share knowledge widely (but not overstate confidence);</p>\n</li>\n<li>\n<p>We should provide support to take action, not push people to action;</p>\n</li>\n<li>\n<p>We should provide resources to help people reflect on their values, not push value change at them.</p>\n</li>\n</ul>\n</ul>\n</ul>\n<p><small><em>Thanks to Goodwin Gibbins, Sam Hilton, Stefan Schubert and others for conversations and comments which fed into this article.</em></small></p>\n<div>&nbsp;</div>", "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "uGz86QFqs7mSLK9cc", "title": "Note: Jeff Bezos posted on Twitter today asking for philanthropy ideas", "postedAt": "2017-06-15T19:38:25.106Z", "htmlBody": "<html><body><p>https://twitter.com/JeffBezos/status/875418348598603776</p>\n<p>&#xA0;</p>\n<p>He also notes that he welcomes hearing that his strategy is wrong.</p></body></html>", "user": {"username": "nonzerosum"}}, {"_id": "574d3ybRG3ZT4a74o", "title": "Introducing the EA Involvement Guide", "postedAt": "2017-06-15T19:20:05.907Z", "htmlBody": "<html><body><p>The Centre for Effective Altruism&#xA0;is publishing a new feature on effectivealtruism.org: an EA Involvement Guide. Hosted under the &quot;Get Involved&quot; tab of the website, this guide exists to help&#xA0;people locate and learn about the many, diffuse resources the community has to offer and decide which opportunities are best suited to them.</p>\n<p>You can check out the guide here: <a href=\"https://www.effectivealtruism.org/get-involved/\">https://www.effectivealtruism.org/get-involved/</a></p>\n<p>To help users sort through the many activities suggested there, we&#x2019;ve tagged each activity with some decision-relevant specifications:&#xA0;</p>\n<p>1. Time commitment (hours, part-time, full-time)</p>\n<p>2. Duration (short-term, medium-term, long-term)</p>\n<p>3. Familiarity with EA (new, familiar, regular)</p>\n<p>4. Occupation (student, professional, retiree, group)&#xA0;</p>\n<p>Eventually we&#xA0;intend to include a filter-by-specification function so people can narrow the list at the outset, rather than needing to click through each activity&apos;s description. Since we&#xA0;won&apos;t have the capacity to add that functionality until we&apos;ve&#xA0;hired another&#xA0;<a href=\"https://www.centreforeffectivealtruism.org/careers/full-stack-developer/\">full stack developer (apply here)</a>, we decided to launch this feature now.</p>\n<p>If you have other thoughts on how to improve the guide, please leave comments below or use the chat function in the bottom right corner of&#xA0;the Get Involved page.</p></body></html>", "user": {"username": "Roxanne_Heston"}}, {"_id": "RzpnxTkbetWaNX7Aw", "title": "EA should beware concessions", "postedAt": "2017-06-14T01:58:47.207Z", "htmlBody": "<html><body><p><strong>Updates</strong>: I&apos;ve expanded the first paragraph and added in a second paragraph about Andrew and Bob to better illustrate the situation.</p>\n<p>One of our goals is to be welcoming, but our highest priority is to be impactful. We have to be willing to bear a certain amount of negative publicity by people with ideological agendas at times, otherwise we surrender the ability to set our own agenda. Many people have trouble making concessions because of pride. This is indeed a cognitive bias that humans&#xA0;possess. However, we should remember that cognitive bias tend to be adaptive, at least in some&#xA0;circumstances. In particular, pride pushes humans to maintain their autonomy. This is important as&#xA0;unilateral concessions inevitably lead to more more unilateral concessions. At best they provide a temporary reprieve, but soon enough the new status quo becomes the baseline for further negotiation and we should not expect past concessions to be credited by the other side. Taking a realpolitik view, they have no incentive to do so as whatever the situation may have been in the past does not affect the negotiating power of the present. The only exception is for a short time after a deal, lest they destroy any incentive for parties to engage them in negotiation. After this waiting period, previous concession can even be used against you, by suggesting that you are a hypocrite or have failed to deliver on your promises. Further the kinds of people who absolutely refuse to be part of a movement unless condition X are likely to reduce group cohesion and therefore may not actually provide a net benefit.</p>\n<p>Even though this may not be perfectly analogous, the following example may make this principle more concrete. Suppose that Andrew and Bob are a couple, with Andrew being stay at home. Bob demands that Andrew have a cup of tea ready for him when he gets home. This is not very much effort for Andrew and although he might not like doing this, he would prefer to do this than to have Bob shout at him.&#xA0;Andrew acknowledges that Bob has a stressful job and that this might help. At the same time, Andrew has a reason to resist this demand because it is a unilateral concession. Surrendering on this point would allow Bob to simply make more demands in the future, since he would know that Andrew can be influenced by pressure. If Andrew has a sense of pride (he is not Bob&apos;s slave!), he will refuse the demand. Otherwise, Andrew getting Bob coffee will become the new status quo and hence the new baseline for negotiation when Bob also wants a massage to help destress. To respond to&#xA0;<a href=\"/user/Michael_Wulfsohn/\">Michael_Wulfsohn</a>&apos;s comment, even if Bob does not see their relationship as adversarial (I work so hard for us and Andrew can&apos;t do this one simple thing for me! It&apos;s obvious that I&apos;m in the right, it&apos;s reasonable to try to shout some sense into him!), and indeed even if&#xA0;they both love each other,&#xA0;it still creates a harmful precedent when Bob is incentivised to shout at Andrew next time he thinks that he is in the right, rather than engaging in discussion.</p>\n<p>What I would suggest is that middle way, that if we make a concession it is because we have decided that it is the right thing to do or because it is something that is effective in and of itself and not because of illusory and temporary PR benefits. Even in these cases, it may be necessary to delay a change to the point where it is clear that we are making the decision on our own terms, rather than allowing ourselves to be dictated to, as this only incentivises further interference.&#xA0;</p>\n<p>This is a purposefully vague warning for reasons that should not need to be said. Unfortunately, this forces this post to discuss these issues at a higher level of generality than might be ideal, and so there is definitely merit to the claim that this post only deals in generalisations. For this reason, this post should be understood more as an outline of an argument than as an actual crystalized argument.</p>\n<p>I have decided to post this now as there aren&#x2019;t any obvious ongoing controversies that it could be directly linked to.</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "casebash"}}, {"_id": "ucEgGZDYpenXLDCWR", "title": "Projects I'd like to see", "postedAt": "2017-06-12T16:19:52.178Z", "htmlBody": "<html><body><h3>We&apos;ve just launched the Effective Altruism Grants program to help people put promising ideas into practice. I&apos;m hoping that the program will enable some people to transition onto higher-impact paths that they otherwise wouldn&apos;t have been able to pursue.</h3>\n<p>Here I&apos;m going to list some applications I&apos;d personally like to see. The list of ideas isn&apos;t close to exhaustive, and you&apos;re not guaranteed funding if you apply with one of these ideas. And I&apos;m not claiming that any particular version of these ideas is good. But they represent some projects I&apos;m potentially excited about, depending on execution. For some of them, I&apos;d be happy to provide mentorship in order to help them succeed. More potential ideas are listed on the <a href=\"https://www.effectivealtruism.org/grants/\">Effective Altruism Grants</a> webpage. Note that CEA might not be able to fund all of the following types of projects, but we may share promising proposals that we are unable to fund with our partners.</p>\n<h2>&#xA0;<br>General types of applications I&apos;d like to see</h2>\n<p>&#xA0;</p>\n<h3><strong>Further study</strong></h3>\n<p>You need to pursue graduate study in order to move into an impactful line of work.</p>\n<h3>Exploring a career switch</h3>\n<p>You think you could do more good in a career other than the one you&apos;re currently in, but you&apos;re not sure what, exactly, is the best alternative. Funding for around three months might allow you to do internships, make applications, and get advice from people. You&apos;d like to do this, but you can&apos;t afford it.</p>\n<h3>Earning-to-give buy-out</h3>\n<p>You&apos;re currently earning to give, because you think that your donations are doing more good than your direct work would. It might be that we think that it would be more valuable if you did direct work. If so we could donate a proportion of the amount that you were donating to wherever you were donating it, and you would move into work.</p>\n<h3>Buying research time</h3>\n<p>You&apos;re a professor and could spend more time on impactful research if you were bought out of your teaching and administrative duties.</p>\n<h3>Unpaid internships</h3>\n<p>You have an opportunity to do an unpaid internship, but couldn&apos;t otherwise afford it.</p>\n<h3>New organisation</h3>\n<p>You have an idea for a new non-profit or for-profit organisation, and need some startup funding to test it out.</p>\n<h3>Running a local group</h3>\n<p>You&apos;re currently leading a local group, and would like to run it full-time.</p>\n<p>&#xA0;</p>\n<h2>More specific ideas that I&apos;d like to see</h2>\n<p>&#xA0;</p>\n<h3>EA Outreach and Community</h3>\n<p>I&apos;d be excited to see people moving into part-time or full-time positions running local groups. For instance, perhaps someone is a successful local group leader while a student, and feel they could continue that work full-time after they graduate.</p>\n<p>I&apos;d be excited to see applications from countries where we don&apos;t currently have a large presence. For instance, we don&apos;t have much of a presence in China, even though it&apos;s very likely that it will be one of the most important and influential countries over the 21st century. There are big challenges to adapting EA to resonate with Chinese culture, but I&apos;d be particularly excited to see applications aimed at trying to figure out how to address those challenges.</p>\n<p>With respect to local groups, I&apos;d love to see group leaders trying out new activities and then writing up an assessment. If such experiments are successful, they could be rolled out to other local groups. (The Oxford Prioritisation Project is a recent example of this - a write-up of their project is coming soon.)</p>\n<p>&#xA0;</p>\n<h3>A few ideas I&apos;d like to see tested are as follows:</h3>\n<p>&#xA0;</p>\n<h3>Anti-Debates</h3>\n<p>Debating is a very common activity at universities, but the usual style of debating is antithetical to the EA approach to reasoning. The aim is to defend a particular point of view, rather than to figure out what the truth is. It&apos;s combative rather than collaborative, and rhetoric tends to take precedence over evidence and logic.</p>\n<p>Instead, we could run &quot;anti-debates&quot;, where two people publicly discuss a topic, stating their views at the outset. They get scored by a panel of judges on a set of criteria that we believe to be genuinely epistemically valuable, such as:</p>\n<p>&#xA0;</p>\n<blockquote>\n<p>Quality and breadth of arguments given</p>\n<p>Understanding of the opposite point of view (and avoidance of &apos;straw man&apos;)</p>\n<p>Appropriate degree of belief given the level of evidence at hand</p>\n<p>Willingness to change your mind in face of contrary argument</p>\n</blockquote>\n<p>&#xA0;</p>\n<h3>Prediction tournaments</h3>\n<p>You lead a group which gets together to make forecasts, with real money on the line, in order to improve your forecasting ability. You might share your predictions with others, to help inform their decisions.</p>\n<h3><a href=\"https://en.wikipedia.org/wiki/Dragons%27_Den\">Dragon&apos;s Den</a>/Shark Tank-style career choice discussions</h3>\n<p>You lead a group which gets together every week. Each week, one of the members has to stand up in front of everyone and outline your career plans, explaining why you&apos;re choosing what you&apos;re choosing, and why that&apos;s the best way for you to do the most good. People would then debate with you whether you&apos;re choosing the right path. A variant would be the &apos;reciprocity ring&apos;. where people offer you any help they can (such as things to read, or introductions), or &apos;peer coaching&apos; networks, where people can mentor each other to talk through their career plans and offer advice.</p>\n<h3>Research working groups</h3>\n<p>&#xA0;A group of you could work on a shared research project, over the course of a semester. This could be on cause prioritisation, or on a specific topic of EA importance (e.g. going through the GiveWell charity cost-effectiveness models and criticising them or investigating what the best policy is within a certain area).</p>\n<h3>Specific skill-building</h3>\n<p>I worry that at the moment too many of the most dedicated community members are building general-purpose skills, such as by going into consulting, rather than getting skills in particular areas that are underrepresented within the effective altruism movement.</p>\n<p>This could include graduate level study in biology, machine learning, economics, or political science, taking up fellowships at a think-tank, or going into government. For those with a quantitative PhD, it could involve applying for the Google Brain Residency program or AI safety fellowship at ASI.</p>\n<h3>New organisations</h3>\n<p>I&apos;d love to see people making a concerted effort to develop EA in new areas. One example would be a think-tank, where people would work out what policies look most promising from an EA perspective. (There are risks involved in this area - in particular of EA becoming partisan - so I think that at this stage the best approach would be research and investigation, rather than activism.) Another would be a GiveWell for impact investing, where you could search for the best impact investing opportunities from an EA perspective.</p>\n<h3>Writing</h3>\n<p>I&apos;d be keen to see more long-form writing done on EA topics, whether for blogs, mainstream media, or books. In general, I&apos;m much more interested by deep substantial pieces of writing rather than short think-pieces. Topics could include:</p>\n<blockquote>\n<p>Cause prioritisation</p>\n<p>CRISPR and eradicating malaria</p>\n<p>What life is really like on $1.90 per day</p>\n<p>Geoengineering</p>\n<p>Pandemics from novel pathogens</p>\n<p>Open borders</p>\n<p>Wild animal suffering</p>\n<p>Further analysis of common ways of doing good (e.g. recycling, fair trade, divestment, or campaigns) in terms of their effectiveness.</p>\n</blockquote>\n<p>&#xA0;</p>\n<p>Often there&apos;s already quite a lot of high-quality material on these topics, scattered across blogs and research articles. What&apos;s needed is for someone to gather together those materials and write a single go-to introduction to the topic. (To a significant extent that&apos;s what Doing Good Better was doing.)</p>\n<p>I&apos;d be keen to see more people take ideas that we think we already know, but haven&apos;t ever been put down in writing, and write them up in a thorough and even-handed way; for example, why existential risk from anthropogenic causes is greater than the existential risk from natural causes, or why global health is a particularly promising area within global development.&#xA0;</p>\n<p>For younger writers, one strategy could be to co-author a book with an established academic. They might have produced a body of research on an important topic, but not be very good at or very interested in writing clearly for a wider audience. In which case, you could suggest to them that you could produce a co-authored book on their topic.&#xA0;</p></body></html>", "user": {"username": "William_MacAskill"}}, {"_id": "waYZEYXkH6wp4qqM8", "title": "Review of EA New Zealand\u2019s \u2018Doing Good Better\u2019 book giveaway", "postedAt": "2017-06-09T18:38:36.017Z", "htmlBody": "<html><body><p>By&#xA0;<span>Finn Whittington, Catherine Low, David Allis </span><a href=\"mailto:effectivealtruismnz@gmail.com\"><span>effectivealtruismnz@gmail.com</span></a></p>\n<p><span>Introduction</span></p>\n<p><span>The Effective Altruism NZ Charitable Trust initiated a book giveaway programme in September 2016, providing copies of William MacAskill&#x2019;s book &#x2018;Doing Good Better&#x2019; for free, with the intention that the recipient reads and shares the book and its ideas. The underlying idea behind the giveaway is that the spreading of ideas translates into action, through donations to effective charities, changes in career path, or by reducing harm to animals through their purchasing habits. </span></p>\n<p><span>We gave out 250 books, each with a sticker with EANZ&#x2019;s contact details, and have so far had 80 people do the follow up survey. Our methodology is not robust enough and these numbers are not large enough to be certain of our conclusions, however the results of the survey suggest that the book giveaway was well worth doing, and may be significantly more effective than donating directly to effective charities at this time. EA NZ are continuing to give away books, and we encourage other groups to do their own giveaway using books they purchase themselves, or books provided by CEA (contact Harri </span><a href=\"mailto:harri.besceli@centreforeffectivealtruism.org\"><span>harri.besceli@centreforeffectivealtruism.org</span></a><span> to see if CEA is able to give books to your group).. </span></p>\n<p>&#xA0;</p>\n<p><span>Method</span></p>\n<p><span>We put an online form on our website </span><a href=\"http://effectivealtruism.nz/book-giveaway-good-better/\"><span>http://effectivealtruism.nz/book-giveaway-good-better/</span></a><span>, and advertised the giveaway on facebook (on EA pages, and on the personal pages of EANZ members) and whenever a member of EANZ gave a talk or giving game, or attended a conference with a relevant topic to effective altruism. People also found out about the book through word of mouth, or through finding our website using google after learning of effective altruism through local or international media. </span></p>\n<p><span>We got the books in bulk from the publisher, so that the average cost of the book is $15 NZ dollars including postage. The books were kindly paid for by an independent charitable trust that is supportive of EANZ.</span></p>\n<p><span>So far in the programme 250 books have been sent to individuals. About two months after sending of the book an anonymous survey was sent to the recipients with questions designed to assess the efficacy of the giveaway. Questions included asking of their perspectives/knowledge about EA before receiving the book, how much money they gave to charities beforehand and to which charities, if they had read the book and how many times they had shared it or its ideas, and finally whether it has changed their donating, career path, dietary choices and general perspective of the world. </span><span><br></span><span><br></span><span>Out of 250 surveys sent out there were 80 (32.0%) valid responses from individuals.</span></p>\n<p>&#xA0;</p>\n<p><span>Results</span></p>\n<p><span>The first sections of the survey assessed the knowledge, beliefs and donations of the participants before they were sent the book. The participants were asked how much they knew about Effective Altruism before applying for the book. With 74 responses, 28 (37.8%) said they didn&#x2019;t know what EA was, 18 (24.3%) said they had heard of EA but didn&#x2019;t know much about it, 17 (23.0%) said that have had read/watched/had one article/video/conversation about EA, 14 (18.9%) said that they had read several articles or a book about EA. With 73 responses, 13 (17.8%) said they already considered themselves effective altruists, with 17 (23.3%) saying maybe, and 43 (58.9%) saying they did not.</span></p>\n<p><span>The participants were asked how much approximately in NZ dollars they donated to charity per year before reading the book. This ranged from $0 (23% of people) to over $5000 (4 people)</span></p>\n<p><span>The participants were asked to list up to 4 charities that they had supported in the past. Many responses included local charities, religious groups/churches, UNICEF, World Vision, various NZ environmental charities and charities focussed on specific diseases (ie the Cancer Society NZ). However out of 66 responses, 6 individuals (9.1%) mentioned a charity that featured on Givewell&#x2019;s Top charities list or featured in &#x2018;Doing Good Better&#x2019;.</span></p>\n<p><span>The second part of the survey was designed to assess the effectiveness of the book giveaway. 42 out of 78 individuals (53.8%) said that they had finished reading the book, with the further 23 (29.5%) reporting they had read at least part of it. When asked how many people they had lent the book to, the mean was 0.92 people, ranging from 0-5. However, many individuals who stated they had lent it to no one yet expressed great interest in doing so later when they had finished the book.</span></p>\n<p><span>31 out of 62 (50.0%) respondents considered themselves effective altruist after receiving the book, an increase from 13 before they read the book. 16 out of 65 (24.6%) said that the book had made them reconsider their career path, and 20 out of 65 (30.8%) said that the book had made them reconsider their dietary choices.</span></p>\n<p><span>The participants were then asked if giveaway had helped them decide which charities to donate to, with 44 out of 65 (67.7%) saying yes, and 16 (24.6%) saying they were unsure, and 5 (7.7%) saying no. When asked again which specific charities they planned to donate to in the future, there were 59 responses with 25 individuals (42.4%) supporting &#x2018;effective charities&#x2019; (those that featured on GiveWell&#x2019;s or Animal Charity Evaluator&#x2019;s top charities list or featured in &#x2018;Doing Good Better&#x2019;), with a further 6 individuals (10.2%) stating they would incorporate Effective Altruist thinking and ideas in their new donation choices.</span></p>\n<p><span>28 individuals out of 63 (44.4%) respondents stated that they would increase the amount that they donated to charity per year. The average increase was $711.80 NZ dollars, with one individual stating that they were considering the 10% income pledge. </span></p>\n<p><span>Across all 80 survey responses the total stated increase in money going to effective charities was at least $20,000 per year</span><span> (this is very imprecise as several people were approximate with their numbers, and several people said they would donate more but didn&#x2019;t say by how much). This number comes from the people who intend to shift their money from less effective to effective charities, and those that are donating more and are donating to effective charities. The largest intended donation was $4000 per year, and 8 people intend to shift $1000 or more to effective charities.</span><span><br></span><span><br><br></span></p>\n<p><span>Discussion</span></p>\n<p><span>Even with the relatively small number of participants in the survey so far, there was a large difference in the beliefs, ideas and actions of the respondents before and after receiving the book. Some respondents gave a good indication of how the book has affected their life:</span></p>\n<p><span>&#x2018;[The book] has influenced me to try to do a lot more good in my life (with my career, extra time etc.). And I will likely donate a lot more in the future (I&apos;m a student now so I don&apos;t have a lot of left over income).&#x2019;</span></p>\n<p><span>Perhaps even more useful than the book itself is that the average recipient introduced the idea of EA and its principles to an average of 4.9 other people (range 0-30), with a few other people spreading the ideas on blog posts etc. The spreading of ideas and sharing the book is likely to have some positive impact that cannot be calculated. Still it is very encouraging to see.</span></p>\n<p><span>Before they read the book, only 6 individuals donated to the recommended &#x2018;top charities&#x2019; from Givewell, which increased to 25 individuals after receiving the book. This represents a considerable increase, from only 9.1% of respondents supporting these effective charities to 42.4%. This means that the giveaway was effective at convincing 33.3% of respondents to begin supporting these effective charities. For example one individual stated that they would be &#x2018;</span><span>Starting new monthly donations to three &quot;effective&quot; charities&#x2019;.</span></p>\n<p><span>Since the total stated increase in money going to effective charities was at least $20,000 per year, it appears that this book giveaway is a more effective use of EA NZ members money than donating directly to effective charities. This is $250 per respondent per year, and if we assume that the people that did not fill in the survey are not going to change their donations at all, that gives $80 per book recipient per year, indicating that the book giveaway will pay for itself 5 times over with donations to effective charities, in just one year. Of course, the next 250 books may have very different results. </span><span><br></span><span> &#xA0;</span></p>\n<p><span>Some of the people who said they would change or increase their donations may not follow through with their stated intentions, and of course many people who set up regular donations will cancel them in the future. However, we still think this $20,000 additional money to effective charities per year is likely to be an underestimation &#x2014; at least for the first couple of years after the survey:</span></p>\n<ul>\n<li>\n<p><span>There were several people who had responded who had yet to read the book.</span></p>\n</li>\n<li>\n<p><span>Some people who received the book but did not fill in the survey may also change their donations.</span></p>\n</li>\n<li>\n<p><span>It is likely that some of the other people that were lent the book by a recipient would also change their donation habits. </span><span><br><br></span></p>\n</li>\n</ul>\n<p><span>Overall, we believe the book giveaway programme is an effective way for members of EA NZ to spend their money at this time, so EA NZ is continuing to give away books, and we encourage other groups to do the same. </span></p>\n<p><span>Problems and thoughts about the Giveaway and Survey</span></p>\n<p><span>When we suggest that &#x201C;indicating that the book giveaway pays for itself 5 times over with donations to effective charities, in just one year&#x201D; we make the assumption that the effective charities chosen by the book recipients are just as effective as the effective charities the members of EA NZ would choose. This may well not be the case, depending on your way of measuring effectiveness. We don&#x2019;t think that issue is significant for us during this giveaway because the money would probably have been donated to AMF if it wasn&#x2019;t used to buy books, and AMF was one of the most common choices for book recipients to choose. </span></p>\n<p><span>We specifically chose to make the survey anonymous in the hope that that would increase people&#x2019;s willingness to do the survey, and to be honest in the survey. We therefore did not track who had completed the survey, so we were unable to send reminders to those who had not done the survey already. If we were to repeat this we&#x2019;d work out a system so we could give reminders. </span></p>\n<p><span>It is unknown how many recipients would have read the book or other sources of EA ideas without the giveaway, so some of this effect may have happened anyway. </span></p>\n<p><span>We didn&#x2019;t do any selection of who we should or should not give the book to &#x2014; basically anyone who wanted a book got a book, and our survey didn&#x2019;t test whether some groups of people were more receptive to the messages than other groups. This information would have been really good to find out so we knew more about how to target advertising for the next giveaway. </span><span><br></span><span><br></span><span>We chose to giveaway paper copies - the thought is that this is a tangible &#x201C;gift&#x201D; and perhaps people are more likely to read it and are more able to pass it on than an electronic copy. However we didn&#x2019;t test this. </span></p>\n<p><span>EA NZ member Catherine has run giving games with groups around the country, and has found giving away the book (and collecting their email) particularly helpful as a way to follow up with interested attendees, and to continue their EA learning. It is a nice way of following up in a non-pushy - instead of them doing a favour to us by adding their details to the sign up sheet we are offering them something that they way. Our suspicion is that the book, read over several hours, days or weeks, may be far more effective than the giving game which is over in an hour or two, however we haven&#x2019;t tested this hypothesis. </span></p>\n<p><span>If you would like a copy of the questions or the raw data send a message to Catherine at </span><a href=\"mailto:effectivealtruismnz@gmail.com\"><span>effectivealtruismnz@gmail.com</span></a><span> and she&#x2019;ll share the survey. </span></p>\n<p><span>&#xA0;</span></p></body></html>", "user": {"username": "cafelow"}}, {"_id": "wYaSxg4uGFLz4kJ6d", "title": "Announcing Effective Altruism Grants", "postedAt": "2017-06-09T10:28:15.441Z", "htmlBody": "<html><body><p>I&apos;m announcing a new project from the Centre for Effective Altruism: <a href=\"https://www.effectivealtruism.org/grants/\">Effective Altruism Grants</a>.&#xA0;Effective Altruism Grants aims to provide grants of up to &#xA3;100,000 (~$130,000) to help individuals work on promising projects.</p>\n<p>We hope to fund a wide range of effective projects that will directly or indirectly contribute to making the world a better place. We primarily expect to fund individuals who explicitly endorse the principles of effective altruism, broadly construed, but are in principle open to funding projects of any form.</p>\n<p>This project is being run by the Centre for Effective Altruism. Individual applicants may receive up to &#xA3;100,000 (~$130,000). CEA will cap our total funding for the Effective Altruism Grants in 2017 at &#xA3;500,000 (~$650,000), but will present promising applications that we have not chosen to fund to other significant donors. These include <a href=\"https://app.effectivealtruism.org/funds/ea-community\">the Effective Altruism Community Fund</a>. This means that the total pool of available funding may be significantly higher.</p>\n<p>We are running this project as a way to support the effective altruism community and to allow people to pursue useful projects. The Centre for Effective Altruism will only fund projects that further its charitable objects.[1] &#xA0;However, we also welcome applications that may be of interest to our partners who are also looking to fund promising projects.</p>\n<h1>Motivation and aims</h1>\n<p>Effective altruism has attracted many people of outstanding talent and motivation. We believe that providing those people with the resources that they need to realize their potential could be a highly effective use of resources.</p>\n<p>Currently, this funding opportunity is relatively neglected within the effective altruism community. There are not many donors directly funding small projects run by individuals. For that reason we believe that funding the very best projects after a thorough application process could be a great use of funds. We want to try out a grants project&#xA0;during 2017. If we feel that we have been able to use money well through this project, we will allocate new funds to it in 2018.</p>\n<p>We want to use the grants to increase the diversity of approaches within effective altruism. We believe that untested strategies could yield significant information value for the effective altruism community, and will fund projects accordingly. We hope that the Effective Altruism Grants will also allow people from less privileged backgrounds (and therefore less financial stability) to pursue the highest expected value career paths open to them.</p>\n<h1>What projects could get funded?</h1>\n<p>We welcome applications from individuals of any background and hope that applicants will include senior professionals and academics as well as recent graduates. Our hope is to fund a diverse array of projects, both in terms of causes and approaches.</p>\n<p>In terms of length and size, we are both looking to give large grants to longer projects (up to &#xA3;100,000, e.g., over more than a year), and smaller grants to shorter projects, or parts of projects (e.g., &#xA3;10,000 (~$13,000) over a three-month period). The smaller grants could go to applicants who wish to transition into a more impactful career or who want to get started on a larger project. Grants of any size may be renewed. If you have a project that would require significantly more than &#xA3;100,000, we still encourage you to apply as we may be able to find you funding from other sources.&#xA0;</p>\n<p>Similarly, due to legal restrictions, CEA may not be able to fund certain types of projects. However, if the project seems promising, we may, with your permission, share your application with other individuals who are looking to fund projects in the effective altruism movement.&#xA0;</p>\n<p>We welcome applications in the following areas:</p>\n<ul>\n<li>Writing a book or book proposal</li>\n<li>Unsalaried fellowships or internships at think tanks or media outlets</li>\n<li>Studies (e.g., Masters or PhDs)</li>\n<li>Academic research, e.g., by providing teaching buy-out for professors</li>\n<li>Independent research</li>\n<li>High quality writing for blogs or other media outlets</li>\n<li>Public outreach on effective altruism or effective altruism-related topics (such as prioritization or rationality)</li>\n<li>Seed-funding for an independent project; e.g. founding a new charity</li>\n</ul>\n<p>Some illustrative examples: &#xA0;</p>\n<ul>\n<li>Write a book proposal on technological solutions to factory farming</li>\n<li>Take an unpaid internship at a think-tank as a way of transferring into a career in policy</li>\n<li>Start a blog on some important and understudied future technology</li>\n<li>Pursue a PhD in economics on how to factor variable-population ethics considerations into cost-benefit analysis</li>\n<li>Run a local effective altruist group, part-time or full-time</li>\n<li>Take several months off work in order to volunteer for effective altruist organisations and figure out your next career steps</li>\n</ul>\n<h1>How the grants work</h1>\n<ul>\n<li>The grants may be paid out by August 15th 2017 at the earliest.&#xA0;</li>\n<li>The grant&#x2019;s duration can vary between a month and several years. We expect many of our grants to be short. &#xA0;</li>\n<li>The grant maximum is &#xA3;100,000.</li>\n<li>Grants can be renewed.</li>\n<li>The Centre for Effective Altruism will not act as your employer. We will not be responsible for the grantees.</li>\n<li>Funding will be paid out on a quarterly basis, conditional on itemized reports on spending in the last quarter, as well as spending &#xA0;and activity plans for the coming quarter. However, we are consciously taking a &#x2018;[hits-based giving](http://www.openphilanthropy.org/blog/hits-based-giving)&#x2019; approach and will not discontinue funding merely because initial results of the project were less promising than was hoped.</li>\n<li>We welcome requests for funding of expenses, such as tuition fees, travel, buying grantees out of existing contracts etc, as well as living costs. You should provide us with an estimate of your living costs (subject to revision&#x2014;relevant factors include seniority, location, etc.) Note that we will likely refer applications that request funding for living costs and overheads to our partners, rather than funding them ourselves.</li>\n</ul>\n<h1>Evaluation criteria</h1>\n<p>Our evaluation criteria are:</p>\n<ul>\n<li><strong>Understanding of, and commitment to, the principles of effective altruism.</strong> We are looking primarily for people who can show clear evidence that they want to benefit others as effectively as possible.</li>\n<li><strong>Demonstrated ability and drive.</strong> We are looking to fund people who are both highly competent and strongly motivated. In particular, it is important that you are able to bring your plans to completion.&#xA0;</li>\n<li><strong>Quality of the project plan.</strong> We are looking to fund projects which have high expected value, either directly, or through enabling yourself or others to have an impact at a later point. We would prioritize a project which might fail over a safer bet &#xA0;if the former has higher expected value. We also believe that information for the community is an important source of value. For that reason, we look favourably on projects exploring previously untested strategies.</li>\n<li><strong>Quality of the career plan.</strong> We are also looking at the quality of your overall career plan, and how your project fits in with that plan. In particular, we will be looking at how your proposal fits into your overall career plan, because this helps us to assess how committed you are to your proposal as well as your understanding of Effective Altruism. (This criterion may be weighted less heavily for senior applicants.)</li>\n</ul>\n<p>In addition, any project funded by CEA must further CEA&apos;s charitable objects.</p>\n<p>We are an equal opportunity organization and value diversity. We do not discriminate on the basis of religion, color, national origin, gender, sexual orientation, age, marital status, or disability status. Please contact us to discuss adjustments to the application process.</p>\n<h1>Application process</h1>\n<p>Applicants should apply through <a href=\"https://cea-core.typeform.com/to/JtpDqY\">this application form</a>. The deadline for applications is 1 July 2017. Please send any inquiries to <a href=\"mailto:eagrants@centreforeffectivealtruism.org\">eagrants@centreforeffectivealtruism.org</a>.</p>\n<p>Applications will be blinded to assessors. After an initial screening, the most promising candidates will be invited to interviews in the week of 24th July. These candidates will have three separate short interviews. Applications for smaller grants will other things equal have a proportionately greater chance of success. Assessors will recuse themselves if there is a conflict of interest. We aim to make the final decisions by 1 August.</p>\n<p>Any promising projects that do not further CEA&apos;s objects, as well as the most promising rejected applications may, with permission, be presented to other significant effective altruist donors. These donors may at some as yet undecided point in time choose to fund some of those applicants.</p>\n<p>* * *</p>\n<p>[1]: Our charitable objects are listed in the &quot;Documents&quot; tab of <a href=\"http://beta.charitycommission.gov.uk/charity-details/?regid=1149828&amp;subid=0\">this site</a>.&#xA0;</p></body></html>", "user": {"username": "Maxdalton"}}, {"_id": "XpuWDpTeeYn37SFui", "title": "A Paradox in the Measurement of the Value of Life", "postedAt": "2017-06-06T22:20:58.709Z", "htmlBody": "<html><body><p>The following link is to a document that was created as external research for CEA. I was tasked with attempting to resolve a paradox presented by the experimental results concerning two units of measurement of the value of life (as the document explains). It is quite long (~8000 words), but any feedback at all would be much appreciated!<br><br>https://drive.google.com/file/d/0B7SK7Kd9FjRzSzhnem5oLTFybGM/view?usp=sharing</p></body></html>", "user": {"username": "klloyd"}}, {"_id": "WdMnmmqqiP5zCtSfv", "title": "Cognitive Science/Psychology As a Neglected Approach to AI Safety", "postedAt": "2017-06-05T13:46:49.688Z", "htmlBody": "<html><body><p>All of the advice on getting into AI safety research that I&apos;ve seen recommends studying computer science and mathematics: for example, the 80,000 hours <a href=\"https://80000hours.org/ai-safety-syllabus/\">AI safety syllabus</a>&#xA0;provides a computer science-focused reading list, and mentions that &quot;<em>Ideally your undergraduate degree would be mathematics and computer science</em>&quot;.</p>\n<p>There are obvious good reasons for recommending these two fields, and I agree that anyone wishing to make an impact&#xA0;in AI safety should have <em>at least</em> a basic proficiency in them. However, I find it&#xA0;a little concerning that cognitive science/psychology are&#xA0;rarely even mentioned in these guides. I&#xA0;believe that it would be valuable to have more people working in AI safety whose&#xA0;<em>primary</em> background is from one of cogsci/psych,&#xA0;or who&#xA0;have at least done a minor in them.</p>\n<p>Here are&#xA0;examples of four&#xA0;lines of research into AI safety which I think could benefit from such a background:</p>\n<ul>\n<li><strong>The psychology of developing an AI safety culture.&#xA0;</strong>Besides the technical problem of &quot;how can we create safe AI&quot;, there is the social problem of &quot;how can we&#xA0;ensure that&#xA0;the AI research community develops a culture where safety concerns are taken seriously&quot;.&#xA0;At least two existing papers&#xA0;draw on psychology to consider this problem: Eliezer Yudkowsky&apos;s&#xA0;&quot;<a href=\"https://intelligence.org/files/CognitiveBiases.pdf\">Cognitive Biases Potentially Affecting Judgment of Global Risks</a>&quot;&#xA0;uses cognitive psychology to discuss why people might&#xA0;misjudge the probability of risks in general, and Seth Baum&apos;s &quot;<a href=\"http://sethbaum.com/ac/fc_AI-Promotion.html\">On the promotion of safe and socially beneficial artificial intelligence</a>&quot; uses social psychology to discuss the specific&#xA0;challenge of motivating AI researchers to choose beneficial AI designs.</li>\n<li><strong>Developing better analyses of &quot;AI takeoff&quot; scenarios.</strong> Currently humans are the only general intelligence we know of, so any&#xA0;analyzes of what &quot;expertise&quot;&#xA0;consists of and how it can be acquired would benefit from the study of humans. Eliezer Yudkowsky&apos;s &quot;<a href=\"https://intelligence.org/files/IEM.pdf\">Intelligence Explosion Microeconomics</a>&quot; draws on a number of fields to analyze the&#xA0;possibility of a <a href=\"https://wiki.lesswrong.com/wiki/AI_takeoff\">hard takeoff</a>,&#xA0;including some knowledge of human intelligence differences as well as the history of human evolution, whereas my &quot;<a href=\"https://foundational-research.org/how-feasible-is-rapid-development-artificial-superintelligence\">How Feasible is the Rapid Development of Artificial Superintelligence?</a>&quot; draws extensively on the work of a number of psychologists to make the case that based on what we know of human expertise,&#xA0;scenarios with AI systems becoming major actors within timescales on the order of mere days or weeks seem to remain within the range of plausibility.</li>\n<li><strong>Defining just what it is that human values are.&#xA0;</strong>The project of AI safety can roughly be defined as &quot;the challenge of ensuring that AIs remain aligned with human values&quot;, but it&apos;s also widely acknowledged that&#xA0;nobody really knows what exactly human values <em>are&#xA0;</em>- or at least, not to a sufficient extent that they could be given a formal definition and programmed into an AI. This seems like&#xA0;one of the core problems of AI safety, and one which can&#xA0;<em>only<strong>&#xA0;</strong></em>be understood with a psychology-focused research program. Luke Muehlhauser&apos;s article &quot;<a href=\"http://lesswrong.com/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">A Crash Course in the Neuroscience of Human Motivation</a>&quot; took one look at&#xA0;human values from the perspective of neuroscience, and my &quot;<a href=\"https://intelligence.org/files/DefiningValuesForValueLearners.pdf\">Defining Human Values for Value Learners</a>&quot; sought to provide a preliminary definition of human values in a computational language,&#xA0;drawing from the intersection of artificial intelligence, moral psychology, and emotion research. Both of these are very preliminary papers, and it would take a full research program to pursue this question in more detail.&#xA0;</li>\n<li><strong>Better understanding multi-level world-models. <a href=\"https://intelligence.org/files/TechnicalAgenda.pdf\">MIRI defines</a></strong>&#xA0;the technical problem of &quot;multi-level world-models&quot; as &quot;<em>How can multi-level world-models be constructed from sense data in a manner amenable to ontology identification?</em>&quot;. <a href=\"http://lesswrong.com/lw/mjx/miris_approach/\">In other words</a>,&#xA0;suppose that we had built an AI to make diamonds (or anything else we care about) for us.&#xA0;How should that AI be programmed so that it could still accurately estimate the number of diamonds in the world after it had learned&#xA0;more about physics, and after it had learned that the things it calls &quot;diamonds&quot; are actually composed of protons, neutrons, and electrons? While&#xA0;I haven&apos;t seen any papers&#xA0;that would explicitly tackle this question yet, a reasonable starting point would seem to be the question of &quot;well, how do humans do it?&quot;. There, psych/cogsci may offer some clues. For instance, in the book <a href=\"https://mitpress.mit.edu/books/cognitive-pluralism\">Cognitive Pluralism</a>, the philosopher Steven Horst&#xA0;offers an argument for believing that humans have multiple different, mutually incompatible mental models / reasoning systems - ranging from <a href=\"http://kajsotala.fi/2017/05/cognitive-core-systems-explaining-intuitions-behind-belief-in-souls-free-will-and-creation-myths/\">core knowledge&#xA0;systems</a>&#xA0;to <a href=\"https://www.facebook.com/Xuenay/posts/10156081961813662\">scientific theories</a> - that they flexibly switch between depending on the situation. (Unfortunately, Horst approaches this as a philosopher, so he&apos;s mostly content at making the argument for this being the case in general, leaving it up to actual cognitive scientists to work out how&#xA0;<em>exactly</em> this works.) I previously&#xA0;also offered a general argument along these lines in my article <a href=\"http://lesswrong.com/lw/m4w/concept_safety_worldmodels_as_tools/\">World-models as tools</a>, suggesting that at least part of the choice of a mental model may be&#xA0;driven by reinforcement learning in the basal ganglia. But&#xA0;this isn&apos;t saying much, given that&#xA0;<em>all</em> human thought and behavior&#xA0;seems to be in at least part driven by reinforcement learning in the basal ganglia. Again, this would take a dedicated research program.</li>\n</ul>\n<p>From these four special cases, you could derive more general use cases for psychology and cognitive science within AI safety:</p>\n<ul>\n<li>Psychology as the study and understanding of human thought and behavior, <strong>helps guide actions that are aimed at&#xA0;understanding and influencing&#xA0;people&apos;s behavior</strong>&#xA0;in a more safety-aligned direction (related example: the psychology of developing an AI safety culture)</li>\n<li><strong>The study of the only general intelligence we know about, may provide information about the properties of other general intelligences</strong> (related example: developing better analyzes of &quot;AI takeoff&quot; scenarios)</li>\n<li><strong>A better understanding of how human minds work, may help figure out how we want the cognitive processes of AIs to work so that they end up aligned with our values</strong> (related examples: defining human values, better understanding multi-level world-models)</li>\n</ul>\n<p>Here I would ideally offer reading recommendations, but the fields are so broad that any given book can&#xA0;only give a rough idea of the basics; and for instance, the topic of world-models that human brains use is just one of many, many subquestions that the fields cover. Thus my suggestion&#xA0;to have some safety-interested people who&apos;d actually study these fields as a major or at least a minor.</p>\n<p>Still, if I&apos;d have to suggest a couple of books, with the main idea of getting a basic grounding in the mindsets&#xA0;and theories&#xA0;of the fields so that it would be easier to read more specialized research... on the cognitive psychology/cognitive science side I&apos;d suggest <a href=\"https://www.amazon.com/Cognitive-Science-Introduction-Mind/dp/0521882001/\">Cognitive Science by&#xA0;Jose Luis Bermudez</a> (haven&apos;t read it, but Luke Muehlhauser <a href=\"http://lesswrong.com/lw/3gu/the_best_textbooks_on_every_subject/\">recommends it</a> and it looked good to me based on the table of contents; see also Luke&apos;s&#xA0;follow-up recommendations behind that link); <a href=\"https://smile.amazon.com/Cognitive-Psychology-Students-Michael-Eysenck/dp/1848724160/\">Cognitive Psychology: A Student&apos;s Handbook by&#xA0;Michael W. Eysenck &amp;&#xA0;Mark T. Keane</a>; and maybe <a href=\"https://smile.amazon.com/Sensation-Perception-8th-Bruce-Goldstein/dp/0495601497/\">Sensation and Perception by E. Bruce Goldstein</a>. I&apos;m afraid that I don&apos;t know of any good introductory textbooks on the social psychology side.</p></body></html>", "user": {"username": "Kaj_Sotala"}}, {"_id": "zkq8GY9PXmXRA3Bnh", "title": "Introducing Sentience Institute", "postedAt": "2017-06-02T14:43:45.784Z", "htmlBody": "<html><body><p><span><img src=\"https://i.imgur.com/ZJMaOf5.jpg\" alt=\"Fish\"><br><br><br>We&#x2019;re excited to announce the launch of a new EA organization, </span><a href=\"http://sentienceinstitute.org\"><span>Sentience Institute</span></a><span> (SI). It&#x2019;s a new think tank dedicated to the expansion of humanity&#x2019;s moral circle. We envision a society in which the interests of all sentient beings are fully considered, regardless of their sex, race, species, substrate, location, or any other characteristic. Our mission is to build on the body of evidence for how to most effectively expand humanity&#x2019;s moral circle, and to encourage advocates to make use of that evidence.</span></p>\n<p>&#xA0;</p>\n<p><span>Because the scope of this mission is so large, we&#x2019;re initially focusing on effective strategies to increase concern (in attitudes and in policy) for farmed animals. There are big changes happening in this field, such as the </span><a href=\"http://www.openphilanthropy.org/blog/why-are-us-corporate-cage-free-campaigns-succeeding\"><span>recent wave of corporate cage-free reforms</span></a><span> and the development of new </span><a href=\"http://www.sfchronicle.com/meat/\"><span>animal-free foods</span></a><span> like the Impossible Burger. The farmed animal advocacy movement has a significant demand for strategic research as indicated by the</span><a href=\"https://animalcharityevaluators.org/wp-content/uploads/2017/02/ace-2016-year-in-review-rgb.pdf\"><span> rapid growth</span></a><span> of Animal Charity Evaluators, the contributions of </span><a href=\"http://www.openphilanthropy.org/giving/grants?field_focus_area_target_id_selective=531\"><span>tens of millions of dollars</span></a><span> by the Open Philanthropy Project, and the evidence-based approach of the leading nonprofit organizations in the space like </span><a href=\"https://animalcharityevaluators.org/charity-review/mercy-for-animals/\"><span>Mercy For Animals</span></a><span> and the </span><a href=\"https://animalcharityevaluators.org/charity-review/humane-society-of-the-united-states-farm-animal-protection-campaign/\"><span>Humane Society of the United States</span></a><span>.</span></p>\n<p>&#xA0;</p>\n<p><span>Our two founding staff (Executive Director Kelly Witwicki and Research Director Jacy Reese) previously worked for </span><a href=\"http://sentience-politics.org/de\"><span>Sentience Politics</span></a><span>, a project of the Effective Altruism Foundation (EAF). Sentience Politics worked in three areas: research, movement-building, and political initiatives, and is now splitting into two independent organizations (i.e. no longer part of EAF), one continuing under the name of Sentience Politics and running political initiatives in the German-speaking area, and the other SI. See </span><a href=\"https://ea-foundation.org/update-on-the-future-of-sentience-politics/\"><span>EAF&#x2019;s post</span></a><span> about the split for more information on what Sentience Politics is doing going forward.</span></p>\n<p>&#xA0;</p>\n<p><span>SI is collaborating heavily with others in the effective animal advocacy (EAA) field to maximize the usefulness of our research. We expect to fill the niche of expanding the foundational evidence base in EAA with a strong focus on intellectual rigor and an emphasis on far future impact. We don&#x2019;t plan to conduct charity evaluations or spend much time directly moving money to effective organizations, but instead create research content that can be used to inform that work and advocacy decisions, such as randomized controlled trials, literature reviews of fields like anti-smoking and voter turnout advocacy, and case studies of social movements like British anti-slavery and environmentalism.</span></p>\n<p>&#xA0;</p>\n<p><span>Our first completed project is our </span><a href=\"http://sentienceinstitute.org/foundational-questions-summaries\"><span>Summary of Evidence for Foundational Questions in Effective Animal Advocacy</span></a><span>, which aggregates the foundational research in this field and will be updated with new research results. This will help us track and prioritize research projects, as well as communicate research results with impact-focused advocates. Check it out if you want to learn about the arguments for and against confrontational advocacy, individual versus institutional advocacy, reducetarian versus vegan asks, and other important questions. You can also take a look at our </span><a href=\"http://www.sentienceinstitute.org/research-agenda\"><span>Research Agenda</span></a><span> to see what&#x2019;s next!</span></p>\n<p>&#xA0;</p>\n<p><span>We are applying for our 501(c)(3) status, and in the meantime, the Centre for Effective Altruism (CEA), a registered 501(c)(3), has generously offered to act as our fiscal sponsor. This means you can donate to us through a special fund for Sentience Institute on CEA&#x2019;s Effective Altruism Funds &#x2013; just use the link below. (UK residents can receive Gift Aid too.) In these early days, we could really use your support. We&#x2019;re hoping to hire a researcher soon if we can find sufficient funding. Please also follow us on </span><a href=\"https://facebook.com/SentienceInstitute\"><span>Facebook</span></a><span> and </span><a href=\"http://twitter.com/SentienceInst\"><span>Twitter</span></a><span>, and consider sharing our content!</span></p>\n<p>&#xA0;</p>\n<p><span>If you have questions, please don&#x2019;t hesitate to reach out to us at </span><a href=\"mailto:kelly@sentienceinstitute.org\"><span>kelly@sentienceinstitute.org</span></a><span> or </span><a href=\"mailto:jacy@sentienceinstitute.org\"><span>jacy@sentienceinstitute.org</span></a><span>, or comment on this post. Kelly will also be giving a talk on Sentience Institute at EA Global Boston this weekend. We&#x2019;re excited to share our work with you!</span></p>\n<p>&#xA0;</p>\n<p><span>Sincerely,</span></p>\n<p><span>Kelly Witwicki &amp; Jacy Reese, Co-Founders</span></p>\n<p>&#xA0;</p>\n<h2><span>More info</span></h2>\n<h3><span>Outreach</span></h3>\n<p><span>While we are mostly focusing on research in the near future, we will take some low-hanging fruit in promoting research results. Our two co-founders regularly give talks on effective animal advocacy, primarily for university students and at conferences. Kelly is speaking at Effective Altruism Global: Boston, and Jacy will be speaking at the 2017 Animal Rights National Conference this fall. Jacy might also give a TEDx talk this fall. Jacy is currently writing a book on </span><a href=\"http://jacyreese.com\"><span>The End of Animal Farming</span></a><span>, and its launch will involve op-eds and other forms of outreach.</span></p>\n<p>&#xA0;</p>\n<p><span>We will also post research updates and summaries on our blog, and are running Facebook and Twitter pages. As we expand our outreach efforts, we might eventually create career content such as an EAA job board and career profiles similar to those published by </span><a href=\"https://80000hours.org/\"><span>80,000 Hours</span></a><span> in other EA cause areas.</span></p>\n<h3><span>Self-evaluation</span></h3>\n<p><span>We&#x2019;re committed to evaluating our own efforts and changing directions or even disbanding the organization if we determine that we can make a greater impact elsewhere. If our research fails to generate new, actionable insights that make a significant difference to advocates&#x2019; decisions, we plan to shift our priorities towards outreach and movement-building, such as the career content described above. Or if we do generate new, actionable insights but feel that advocates are neglecting the cost-effective strategies those insights point to, we will consider pursuing them ourselves.</span></p>\n<h3><span>Research standards</span></h3>\n<p><span>We will strive to maintain the best possible research standards, such as preregistering our randomized controlled trials and seeking out peer review on all our research, mostly from other people who study EAA but also from other experts when possible, such as a social movement&#x2019;s historian for a case study on that movement. Additionally, we plan to stay up-to-date on the latest research and expert opinions on research standards so we can update our methods accordingly.</span></p>\n<h3><span>Transparency</span></h3>\n<p><span>We also plan to publish financial records, internal policies, regular updates, and a Mistakes page similar to those of </span><a href=\"http://www.givewell.org/about/our-mistakes\"><span>GiveWell</span></a><span> and </span><a href=\"https://animalcharityevaluators.org/about/transparency/corrections/\"><span>Animal Charity Evaluators</span></a><span>. We&#x2019;re committed to building a collaborative environment so we can help advocates, donors, researchers, and everyone in the effective animal advocacy community.</span></p>\n<h3><span>Finances</span></h3>\n<p><span>The Effective Altruism Foundation has granted SI $60,000 to cover our initial costs and all expenses through December. Once we&#x2019;ve raised $23,000, we&#x2019;ll start interviews for a third team member so we can conduct more research. (We have some promising candidates in mind.) Funding beyond that will enable us to run several polls and surveys, costing approximately $1,000-5,000 each, and will secure salaries beyond our first half year. If we are able to raise over $100,000 in the near future, we could make a fourth hire, run more polls and surveys, and conduct more outreach. We are also open to restricted funding from excited donors for specific purposes such as increasing salaries to more competitive rates to attract top talent, creating EAA career content, or conducting research focused specifically on wild animals.</span></p></body></html>", "user": {"username": "Jacy"}}, {"_id": "CHsrPENNDnXwRfw8w", "title": "Discussion: Adding New Funds to EA Funds", "postedAt": "2017-06-01T18:52:21.513Z", "htmlBody": "<html><body><p>&#xA0;</p>\n<p><span>In our <a href=\"/ea/17v/ea_funds_beta_launch/\">EA Funds launch post</a>, we noted that:</span></p>\n<p>&#xA0;</p>\n<blockquote>\n<p><span>[I]n the future we hope to encourage new fund managers to create new funds with different focus areas than the current options.</span></p>\n</blockquote>\n<p>&#xA0;</p>\n<p><span>As our three-month trial draws to a close we&#x2019;re now thinking more seriously about adding new funds to EA Funds. However, there are a number of open questions that would determine how many funds we might add, which funds might be added, and how quickly we&#x2019;d be able to add new funds. I outline the relevant open questions as I see them below.</span></p>\n<p>&#xA0;</p>\n<p><span>CEA plans to discuss adding new funds during our team retreat after <a href=\"https://www.eaglobal.org/events/ea-global-2017-boston/\">EA Global: Boston</a>. The goal of this post is to get feedback on these question from the community to help inform that discussion. Please provide feedback in the comments below. If you&#x2019;re attending EA Global: Boston you can also grab me for a quick chat there.</span></p>\n<p>&#xA0;</p>\n<p><span>Below I present each open question, try to explain the full range of options available, and then outline some of the considerations that I think are relevant in addressing the question. The goal is to remain neutral on the answer while still providing relevant information. The inclusion of an option or a consideration does not necessarily imply endorsement of that option or consideration by me or by others at CEA.</span></p>\n<p>&#xA0;</p>\n<p><span>If you think there are open questions to address that I have missed, please feel free to suggest them in the comments.</span></p>\n<p>&#xA0;</p>\n<h1><strong><span><span>Open Questions</span></span></strong></h1>\n<p>&#xA0;</p>\n<h2><span><span>Question 1: Should we add new funds? If so, when?</span></span></h2>\n<p><span>The first question is whether we should add new funds at all and, if so, on what timeline should we add them? Part of this answer depends on how much money is moving through EA Funds. For reference, EA Funds has processed $775,000 so far with $31,000 in monthly recurring donations. We expect the pace of growth in the near future to be slower than it was in the first three months as the initial buzz around EA Funds dies down.</span></p>\n<p>&#xA0;</p>\n<h2>&#xA0;</h2>\n<h2><span><span>Potential options</span></span></h2>\n<p><span>Don&#x2019;t add new funds</span></p>\n<p><span>The first option is that we shouldn&#x2019;t add new funds at all. For example, we might want to tweak the existing funds by selecting new fund managers or by having multiple people manage certain funds, but we might not want to expand past a small number of funds that represent the most widely-supported causes.</span></p>\n<p>&#xA0;</p>\n<p><span>Add new funds, but later</span></p>\n<p><span>We might want to add new funds, but only after EA Funds has a longer track record or has reached certain milestones. For example, we might only want to add funds after a year, or once we&#x2019;ve moved a certain amount of money, or once we&#x2019;ve reached a certain amount of money in monthly recurring donations.</span></p>\n<p>&#xA0;</p>\n<p><span>Add new funds now</span></p>\n<p><span>Finally, we might opt to add new funds very soon.</span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<h2><span><span>Considerations</span></span></h2>\n<p><span>Future growth of EA Funds</span></p>\n<p><span>Adding new funds depends, at least in part, on how much money might be available to support the funds which depend in turn on EA Fund&#x2019;s future growth prospects.</span></p>\n<p>&#xA0;</p>\n<p><span>This is hard to determine, but here are some guesses. First, I don&#x2019;t expect us to raise as much money over the next three months as we did over the initial three months. Much of the money we do raise will be driven by the $31,000 in monthly recurring donations that have already been set. However, it is unlikely that donors with recurring donations will change their allocation to include new funds. This means that it may be relatively difficult to move significant amounts of money through new funds in the short term.</span></p>\n<p>&#xA0;</p>\n<p><span>On the other hand, the user base of EA Funds is still relatively small (around 665 unique donors), so there may be significant low-hanging fruit in getting people already involved in EA to consider using the platform. Additionally, adding a fund that meets an as-yet unmet demand could cause additional money to flow through the platform in a way that doesn&#x2019;t cannibalize existing funds.</span></p>\n<p>&#xA0;</p>\n<p><span>Viewpoint diversity</span></p>\n<p><span>All of our current funds are run by GiveWell/Open Phil staff members. As we&#x2019;ve stated in the past, we aim to have 50% or less of the program officers work at GiveWell/Open Phil. Adding more funds seems like the most plausible way to achieve this goal.</span></p>\n<p>&#xA0;</p>\n<p><span>Reputation</span></p>\n<p><span>Adding new funds that are significantly worse than the existing options might harm the reputation of EA Funds, CEA, and EA in general. Conversely, adding high-quality funds in new areas may improve the reputation of EA by further showcasing the ability of the EA community to find interesting ways of improving the world.</span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<h2><span><span>Question 2: What kinds of funds should we add?</span></span></h2>\n<p><span>Our existing funds each focus on a single broad cause area that EAs have historically supported. The existing funds were designed to give fund managers relatively wide latitude to decide what use of funds is best while also making it clear to donors what the funds might donate to.</span></p>\n<p>&#xA0;</p>\n<p><span>One question for the future is whether we should expand EA Funds by adding new funds in new cause areas or whether we should expand by adding new funds built around themes other than cause areas.</span></p>\n<p>&#xA0;</p>\n<h2><span><span>Potential options</span></span></h2>\n<p><span>Below are some options for the kind of funds we might add. Keep in mind that these options are not mutually exclusive, so we could include multiple plausible options. </span></p>\n<p>&#xA0;</p>\n<p><span>New funds in new cause areas</span></p>\n<p><span>We could simply add new funds in new cause areas. These would operate similarly to the existing funds.</span></p>\n<p>&#xA0;</p>\n<p><span>New funds in existing cause areas</span></p>\n<p><span>We could add funds in existing cause areas that have the same scope as the current funds. For example, we could add a second fund in global health and development which has the same scope as the fund managed by Elie, but which is managed by someone else.</span></p>\n<p>&#xA0;</p>\n<p><span>Fund manager&#x2019;s discretion</span></p>\n<p><span>We could add funds that give the fund manager wide latitude to recommend a grant to whatever they think is best regardless of cause area.</span></p>\n<p>&#xA0;</p>\n<p><span>Different approaches to existing causes</span></p>\n<p><span>We could add funds that take some different approach to existing cause areas. For example, we could add a fund in global health and development that focuses on high-risk, high-reward projects (e.g. startups, funding evaluations rather than direct interventions), or we could add a long-term future fund that focused on areas other than AI-Safety. </span></p>\n<p>&#xA0;</p>\n<p><span>Funds based on particular tactics</span></p>\n<p><span>We could add funds which are focused on particular tactics instead of cause areas. For example, we could add a fund which donates only to startups or which funds research projects. These funds could operate across a variety of cause areas.</span></p>\n<p>&#xA0;</p>\n<p><span>Funds based on normative disagreements</span></p>\n<p><span>We could add funds which are based on specific normative disagreements. For example, we could have a fund which focuses predominantly on improving (and not necessarily saving) lives or a fund which focuses on reducing suffering.</span></p>\n<p>&#xA0;</p>\n<h2><span><span>Considerations</span></span></h2>\n<p><span>The chicken-and-egg problem for new causes</span></p>\n<p><span>For a fund in a new cause area to succeed it needs both money and high-quality projects to support with that money. This presents different problems for EA Funds than those faced by large funders with an endowment like Open Phil. In Open Phil&#x2019;s case, since it already has the money, it can declare an interest in funding some new area and then use the promise of potential funding to cause people to start new projects. If no projects show up, it can simply redirect the money to other projects.</span></p>\n<p>&#xA0;</p>\n<p><span>However, in EA Funds, the ability of a fund to attract money is partially dependent on the existence of promising projects to fund (since a fund without plausible grantees will have a hard time getting donations). This means that EA Funds may find it difficult to catalyze activity in completely novel areas.</span></p>\n<p>&#xA0;</p>\n<p><span>Clarity</span></p>\n<p><span>It should be relatively easy for donors to figure out what they&#x2019;re supporting if they donate to a fund. For donors willing to research, the fund page should be sufficient to help them understand each fund. </span></p>\n<p>&#xA0;</p>\n<p><span>However, not all donors will carefully read the fund pages and many donors will choose what fund pages to review based on the name and perhaps a short description of each fund. While we hope donors will look at the details of each fund, realistically it may be the case that the name of each fund alone will have a disproportionate effect on whether people choose to support it. </span></p>\n<p>&#xA0;</p>\n<p><span>Fund names should satisfy two goals:</span></p>\n<p>&#xA0;</p>\n<ol>\n<li>\n<p><span>The name should make it clear what the fund is likely to support.</span></p>\n</li>\n<li>\n<p><span>The name should make it clear how the fund is different from the other available funds.</span></p>\n</li>\n</ol>\n<p>&#xA0;</p>\n<p><span>However, some options for adding new funds present greater clarity challenges than others. For example, funds in the same cause area as existing funds will present a particular challenge in choosing names that make it easy to understand how the funds differ. Similarly, funds that operate at the fund manager&#x2019;s discretion will be difficult to name in a way that makes it clear what the fund is likely to support.</span></p>\n<p>&#xA0;</p>\n<p><span>Expanding EA&#x2019;s intellectual horizons</span></p>\n<p><span>Adding funds in areas outside of global health, animal welfare, long-term future, and EA community would help expand the intellectual horizons of EAs and help us find new promising cause areas.</span></p>\n<h2>&#xA0;</h2>\n<h2>&#xA0;</h2>\n<h2><span><span>Question 3: How should we vet new funds?</span></span></h2>\n<p><span>Our current funds represent problem areas that we think are especially promising, have wide community support, and are run by fund managers that we think have strong knowledge and connections in the fund area. We could attempt to ensure that any new funds adhere to similar standards or we could substantially open the platform up and allow anyone (or nearly anyone) to create a fund of their own. </span></p>\n<p>&#xA0;</p>\n<p><span>Below I try to outline a continuum of plausible options for the degree to which we ought to vet new funds. I then outline some considerations that are relevant for deciding where we ought to fall along this continuum.</span></p>\n<p>&#xA0;</p>\n<h2><span><span>Potential options</span></span></h2>\n<p><span>No vetting</span></p>\n<p><span>On one extreme end of the continuum, we could let anyone create a fund which they manage however they want and which anyone can donate to. To add slightly more quality controls we could require certain kinds of reporting and require some standard set of information for the fund page of each fund.</span></p>\n<p>&#xA0;</p>\n<p><span>Democratic vetting</span></p>\n<p><span>We could let anyone create a fund, but only keep funds that receive a certain amount of support from the community (e.g. donations or &#x201C;votes&#x201D; of some kind). We could instead let anyone </span><span>propose</span><span> a fund, but only accept some small number of funds as determined by community support (e.g. pledges to donate).</span></p>\n<p>&#xA0;</p>\n<p><span>Plausibility vetting</span></p>\n<p><span>We could let anyone propose a fund, but then have CEA (or some set of trusted researchers) review the funds and reject any funds which we think are not plausibly a good candidate. </span></p>\n<p>&#xA0;</p>\n<p><span>The precise definition of &#x201C;plausibility&#x201D; in this context is up for grabs, but the goal would be to reject only the funds and fund managers which seem like especially poor options. The process could use some method of democratic vetting to further narrow down the field from among the plausible options.</span></p>\n<p>&#xA0;</p>\n<p><span>&#x201C;Reasonable-person&#x201D; vetting</span></p>\n<p><span>Using the process described above, we could apply a more strict &#x201C;reasonable person&#x201D; standard. The goal would be to only accept funds which a reasonable person might think are better than some benchmark. For example, we could only allow funds which a reasonable person might think are better than AMF or better than the existing funds. Anyone could propose a fund and then this standard would be applied or proposing a fund could be an invite-only process.</span></p>\n<p>&#xA0;</p>\n<p><span>&#x201C;Better than&#x201D; vetting</span></p>\n<p><span>Finally, we could only accept funds that CEA (or some set of trusted researchers) think are better than the existing options for some criterion of betterness. This is different from the reasonable person standard because it requires that we think the fund is <em>actually better</em> than the existing options, not that we could see how someone </span><span>might</span><span><em> think</em> that the fund is better.</span></p>\n<p>&#xA0;</p>\n<p><span>Hybrid options</span></p>\n<p><span>We could also combine multiple approaches to form hybrid options. Some rough ideas for how we might do this are below:</span></p>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><span>Start closed and open up over time</span></p>\n</li>\n<ul>\n<li>\n<p><span>We could start by vetting funds very close for the first few rounds of adding new funds and we could decrease the vetting requirements over time.</span></p>\n</li>\n</ul>\n<li>\n<p><span>Low vetting plus nudges</span></p>\n</li>\n<ul>\n<li>\n<p><span>We could provide very little vetting for creating a fund, but nudge users towards the funds that we think are most promising. For example, the default [allocation page](https://app.effectivealtruism.org/donations/new) could only include highly promising funds and less promising options could be made less immediately obvious.</span></p>\n</li>\n</ul>\n</ul>\n<p>&#xA0;</p>\n<h2><span><span>Considerations</span></span></h2>\n<p><span>Below are some considerations that might factor into the decision of how closely to vet new funds. These are presented in no particular order.</span></p>\n<p>&#xA0;&#xA0;</p>\n<p><span>Inclusion in EA Funds as a nudge</span></p>\n<p><span>User behavior so far suggests that many people choose to split their donation among several funds instead of donating all of their money to a single fund. This suggests that donors see inclusion in EA Funds as a sign of quality and that a fund&#x2019;s inclusion nudges people to donate to causes they might not have given to otherwise. This was also born out in some Skype conversations we had with early users. </span></p>\n<p>&#xA0;</p>\n<p><span>This increases the potential for new funds to cause harm by attracting money that might have been better spent elsewhere.</span></p>\n<p>&#xA0;</p>\n<p><span>Administrative costs</span></p>\n<p><span>Each fund adds some small, but nontrivial administrative cost to CEA. </span></p>\n<p>&#xA0;</p>\n<p><span>For each fund, CEA needs to communicate with the fund manager regularly about the amount of money available, whether they have new grant recommendations, and about posting updates to the website. We also incur administrative costs every time a grant is made as we need the trustees to approve the grant and we need to work with the charity to get them the money. We could probably develop systems to decrease administrative costs if the scale of the project required this, but we likely wouldn&#x2019;t be able to do this in the short term.</span></p>\n<p>&#xA0;</p>\n<p><span>Reputation</span></p>\n<p><span>Lower-quality funds might harm the reputation of EA Funds, CEA, and EA in general.</span></p>\n<p>&#xA0;</p>\n<p><span>Recruiting high-quality fund managers</span></p>\n<p><span>Low-quality funds might make it harder to acquire (and retain) high-quality fund managers as being associated with the project becomes less prestigious.</span></p>\n<p>&#xA0;</p>\n<p><span>Researcher recruitment</span></p>\n<p><span>One source of value from EA Funds is that it might help incentivize talented researchers to do high-quality work on where people ought to donate. Lower barriers to entry in setting up a fund might increase the pipeline of researcher talent that EA Funds helps create.</span></p>\n<p>&#xA0;</p>\n<p><span>Funding externally controversial projects</span></p>\n<p><span>One affordance we&#x2019;d like for EA Funds to have is funding high impact, but externally controversial projects.</span></p>\n<p>&#xA0;</p>\n<p><span>Plausibly, the more funds we have, and the more EA Funds is an open platform, the less the actions of a single fund will negatively affect the platform as a whole. So, we might have more affordance to fund controversial projects by adding more funds.</span></p>\n<p>&#xA0;</p>\n<p><span>New funds and acquiring new users</span></p>\n<p><span>It seems plausible that more funds would make it easier to attract more users for two reasons. First, when someone sets up a fund they will likely reach out to their network to get people to donate which may help us acquire users. Second, the more variety we offer the more likely it is that donors find funds that they strongly resonate with. </span></p>\n<p>&#xA0;</p>\n<p><span>The marketplace of ideas</span></p>\n<p><span>Lower barriers to entry would promote a more open and thriving marketplace of ideas about where people should donate.</span></p>\n<p>&#xA0;</p>\n<p><span>Expertise</span></p>\n<p><span>EA Funds was conceived as a way of making individual&#x2019;s donation decisions easier, by allowing them to draw on the expertise of people or groups who have greater subject-matter expertise and are more up-to-date with the latest research on their Fund&#x2019;s topic, current funding opportunities in the space, and organizational funding constraints. There is a tradeoff between creating fewer new funds that are genuinely expert-led, and a greater number of funds where the average level of expertise is lower.</span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<h1><span><span>Conclusion</span></span></h1>\n<p><span>This post has attempted to describe some of the open questions on EA Funds and the relevant considerations as a way to solicit feedback and new ideas from the EA community. I look forward to a discussion in the comments here and in person for anyone at EA Global: Boston this weekend. </span></p>\n<p>&#xA0;</p>\n<p><span>The next steps for this process are for me to review comments to this post and to discuss the topic with the rest of the CEA team. Afterward, I plan to write a follow-up post that outlines either the option we selected and why or the options we&apos;re currently deciding between. If you have thoughts that you&apos;d prefer not to share here, feel free to email me at kerry@effectivealtruism.org.</span></p>\n<p>&#xA0;</p>\n<p><em><span>Please note that due to EA Global: Boston, CEA staff might be slower to respond to comments than usual.</span></em></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Kerry_Vaughan"}}, {"_id": "Cn3dAZhtqeBXYmQvF", "title": "Considering Considerateness: Why communities of do-gooders should be exceptionally considerate", "postedAt": "2017-05-31T22:41:27.190Z", "htmlBody": "<html><body><p>The CEA research team just published a new paper - <a href=\"https://www.centreforeffectivealtruism.org/blog/considering-considerateness-why-communities-of-do-gooders-should-be/\">Considering Considerateness: Why communities of do-gooders should be exceptionally considerate</a>&#xA0;(<a href=\"https://assets.contentful.com/es8pp29e1wp8/35lEHLDKtiamCQowg0I46O/33abc38b5cedcae3a8cb93a457285a4f/Considering_Considerateness_for_PDF.pdf\">PDF version</a>). The paper&#xA0;is co-authored by Stefan Schubert, Ben Garfinkel, and Owen Cotton-Barratt.&#xA0;</p>\n<h2>Summary</h2>\n<p>When interacting with others you can be considerate of their preferences, for instance by being friendly or reliable. This normally has small positive direct effects. But, by improving your reputation or strengthening aspects of culture that make a community more cooperative, the positive indirect effects can be&#xA0;large.</p>\n<p>We present the case that&#xA0;these indirect effects are further strengthened when you are acting as part of a community of people doing important work. For instance, being considerate can improve the level of trust and collaborativeness among members of the community. It can also improve the reputation of the community. Conversely, failing to be considerate can harm the community, both internally and in its&#xA0;reputation.</p>\n<p>This means that for communities of people striving to do good, such as the effective altruism community, considerateness should be a surprisingly high priority. It could be that, in order to do the most good, they should be considerably more considerate than common sense morality&#xA0;requires.</p></body></html>", "user": {"username": "Stefan_Schubert"}}, {"_id": "a3sScjxdgNTCseBMJ", "title": ".impact is now Rethink Charity", "postedAt": "2017-05-30T20:45:01.645Z", "htmlBody": "<html><body><p><span>I&#x2019;m Tee Barnett &#x2013;&#xA0;and in the last couple of months I&#x2019;ve transitioned into the Executive Director role at .impact. I wanted to reintroduce myself and brief everyone on significant developments over here, including our rebranding, unification of major projects, and staff updates. You can reach me </span><a href=\"http://www.calendly.com/teebarnett\"><span>here</span></a><span> if you&#x2019;d like to chat about it!</span></p>\n<p>&#xA0;</p>\n<p><strong><span>Rebranding</span>&#xA0;</strong></p>\n<p><span>.impact and Students for High-Impact Charity (SHIC) are teaming up to form </span><a href=\"http://www.rtcharity.org\"><span>Rethink Charity</span></a><span>, a rebrand intended to invite curiosity from the wider public about how to do good, and that more accurately articulates our approach to movement building by supporting novel projects. </span></p>\n<p>&#xA0;</p>\n<p><a href=\"http://dotimpact.im\"><span>.impact</span></a><span> is the organization responsible for creating and maintaining </span><a href=\"https://localeanetwork.org/\"><span>LEAN</span></a><span>, the </span><a href=\"http://.org\"><span>EA Hub</span></a><span>, the </span><a href=\"/ea/zw/the_2015_survey_of_effective_altruists_results/\"><span>EA survey</span></a><span>, and a host of </span><a href=\"https://rtcharity.org/all-projects/\"><span>other projects</span></a><span>, and </span><a href=\"http://www.shicschools.org\"><span>SHIC</span></a><span> is the only global network of classes and extracurricular clubs dedicated to bringing effective altruism primarily to high-school students. </span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p><span>You will likely see more .impact-branded materials transition to Rethink Charity in the coming weeks. The branding of our prioritized projects, including SHIC, will remain (more on that below). Rethink Charity will also retain the &#x201C;.impact&#x201D; name as a project that more closely resembles its origins &#x2013; a space for independent, volunteer-led effective altruist projects around the world.</span></p>\n<p>&#xA0;</p>\n<p><strong><span>Prioritized Projects</span>&#xA0;<span>(LEAN, SHIC and P2Ps)</span></strong></p>\n<p>&#xA0;</p>\n<p><span><span>Our newly unified lineup of initiatives facilitate interpersonal engagement from the bottom up. Each of our prioritized projects bring more people into the fold and engage those who are ready to get more involved. The bulk of our resources will go toward these three very strong grassroots initiatives: </span></span></p>\n<p>&#xA0;</p>\n<p><span>- The Local Effective Altruism Network (LEAN)</span></p>\n<p>&#xA0;</p>\n<p><span>- Students for High-Impact Charity (SHIC)</span></p>\n<p>&#xA0;</p>\n<p><span>- Online peer-to-peer (P2P) campaigns and Impact Missions</span></p>\n<p>&#xA0;</p>\n<p><span><span>These projects will interplay and constitute an escalating &#x201C;ladder of engagement&#x201D; designed to make effective altruism more accessible and strengthen community infrastructure. For example, coordinated P2P campaigns inherited from Charity Science will continue to be run by LEAN and SHIC groups. In the past, these fundraisers have </span><a href=\"http://www.charityscience.com/uploads/1/0/7/2/10726656/2.5-yearinternalreviewandplansmovingforward.pdf\"><span>counterfactually raised</span></a><span> ~$200,000. and we see P2Ps and other impact missions (i.e., coordinated effective action) as an integral part of local and student group potential moving forward. In prioritizing these three projects, we&#x2019;re looking to facilitate lifelong engagement with EA from high school until well after university graduation.</span> </span></p>\n<p>&#xA0;</p>\n<p><span>Rethink Charity will also continue to run several other important projects:</span></p>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><span>We will provide tax-deductible international donations to effective charities through our representative legal entities and the facilitation of organizational partnerships &#xA0;&#xA0;</span></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><span>The EA Survey will remain an important annual initiative.</span></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><span>The EA Hub will play an even more prominent role for EA community building by absorbing the .impact Hackpad project registry. </span></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><span>The Effective Altruism Newsletter, local group newsletter, and forthcoming EA mentorship program are among the roster of joint projects we created and maintain, through collaboration with the Centre for Effective Altruism (CEA) and cultivation of a substantial volunteer force. </span></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><span>Rethink Charity and our team of volunteers continue to maintain other platforms and services often used by the effective altruism community, including the </span><a href=\"http://wiki.effectivealtruismhub.com/index.php?title=Effective_Altruism_Wiki\"><span>EA Wiki</span></a><span> and EA Forum. </span></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<p><span>You can learn more about each of these by clicking on their respective links above. </span><span><br></span><span><br></span><strong><span>Staff Update</span></strong></p>\n<p>&#xA0;</p>\n<p><span>With the expansion of major movement building projects under the Rethink Charity umbrella, we&#x2019;ve had to take on more staff and invest further in our volunteer infrastructure in order to have the requisite administrative bandwidth. </span></p>\n<p>&#xA0;</p>\n<p><span><span>Our two most prominent board members, .impact co-founders Tom Ash and Peter Hurford, have been very active in overseeing this transition and rebranding. The Executive Director of SHIC, Baxter Bullock, is now running the project on a full-time basis and has become an integral to the functioning of Rethink Charity operations as well. We brought on Richenda Herzig, formerly a part-time .impact coordinator, for a full-time position as Manager of LEAN. And Catherine Low, a founding member of Effective Altruism New Zealand, is the new part-time Manager of Community, a position largely responsible for orchestrating our considerable intern and volunteer network. Haryshwaran Ilanghovan has stayed on as our Head of Technology for all Rethink Charity projects. Feel free to browse their bios </span><a href=\"https://rtcharity.org/team/\"><span>here</span></a><span>. </span></span></p>\n<p>&#xA0;</p>\n<p><span>I&#x2019;m very bullish about our prospects moving forward. The foundation laid by .impact and the fusion of these major projects into Rethink Charity makes for a very strong EA movement building lineup. </span></p>\n<p>&#xA0;</p>\n<p><span>Tee Barnett</span></p>\n<p><span>Executive Director</span></p>\n<p><a href=\"http://www.rtcharity.org/\"><span>Rethink Charity</span></a><span> (formerly .impact) </span></p>\n<p><span>tee@dotimpact.org</span></p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Tee"}}, {"_id": "8B3bwtoK4K4XD2GE4", "title": "Give if you win (innovation in fundraising)", "postedAt": "2017-05-26T19:36:09.542Z", "htmlBody": "<html><body><p><span>I&#x2019;m an academic Economist at the University of Exeter, working on a project called &#x2018;</span><a href=\"http://giveifyouwin.org\"><span>Give if you win</span></a><span>&#x2019;. Here&apos;s a summary:</span></p>\n<p><span>Millions of employees anticipate end-of-year bonuses and performance-dependent income, particularly in finance and sales. Before these are announced many are uncertain of what size reward, if any, they will get. There is evidence from behavioural economics and psychology that people may be especially generous if asked to commit in advance: &#x2018;if you win a bonus, how much will you donate to charity?&#x2019;, or if asked immediately after they win a bonus. This evidence also broadly justifies pledges like GWWC and the Founder&#x2019;s Pledge. &#xA0;I have been researching this concept at the University of Exeter Business School, and promoting this idea as part of an ESRC-funded impact project called </span><a href=\"http://51.141.6.235/doku.php?id=iifproject:innovations_in_fundraising_outline\"><span>&#x2018;Innovations in fundraising</span></a><span>&#x2019;, partnering with George Howlett of the Centre for Effective Altruism. We give the general pitch and some relevant links on the page </span><a href=\"http://giveifyouwin.org\"><span>giveifyouwin.org</span></a><span>.</span></p>\n<p><span>I would really appreciate it if you could take a look at this page and give some feedback and ideas. I&#x2019;m looking for opportunities to try this out, as well to learn more about potential obstacles and implementation issues.</span></p>\n<p><br><span>By the way, as part of this project, my research assistants and I are gathering information in an &#x2018;Innovations in Fundraising Wiki&#x2019; (</span><a href=\"http://51.141.6.235/doku.php?id=iifwiki:start\"><span>link to alpha version here</span></a><span>), and we are looking for collaborators and volunteers (and very willing to partner and integrate with other platforms). I think these resources could be particularly valuable to Effective Altruists looking to fundraise and to find ways of boosting effective giving. Please let me know if you are interested -- I&#x2019;ll post more on this later. </span></p></body></html>", "user": {"username": "david_reinstein"}}, {"_id": "SJMiToN9m5RTsv23H", "title": "The Giving Game Project's Vision and Strategic Plan", "postedAt": "2017-05-23T23:21:42.879Z", "htmlBody": "<html><body><p><a href=\"https://www.thelifeyoucansave.org/Blog/ID/1357/The-Giving-Game-Projects-Vision-and-Strategic-Plan\">Cross-posted from The Life You Can Save&apos;s blog.</a></p>\n<p>&#xA0;</p>\n<p><span><a href=\"https://www.thelifeyoucansave.org/Giving-Games\">The Giving Game Project</a> has an ambitious goal: we want to provide philanthropy education at a scale that will fundamentally shift the way people learn about, and practice, charitable giving. &#xA0;Why do we think we can achieve this goal, and how are we going to do it?</span></p>\n<p>&#xA0;</p>\n<p><span>Our new &#x201C;</span><a href=\"https://docs.google.com/presentation/d/1-q8I2DLccmd-HIv28O-33aYzd78266NwhS7McBcN8kA/edit?usp=sharing\"><span>Vision and Strategic Plan</span></a><span>&#x201D; answers these questions in detail. It outlines the specific behavioral changes we aim to achieve, the mechanisms we will use to produce them, how our product connects with our target audience, and opportunities for large-scale growth. It also lays out a path toward long-term financial sustainability, even at mass scale. By describing our plans at a granular level, we hope to provide transparency into our thinking and assumptions. </span></p>\n<p>&#xA0;</p>\n<p><span>Read the full plan <a href=\"https://docs.google.com/presentation/d/1-q8I2DLccmd-HIv28O-33aYzd78266NwhS7McBcN8kA/edit?usp=sharing\">here</a>.</span></p>\n<p>&#xA0;</p>\n<p><span>For introductory background on Giving Games, see <a href=\"https://www.thelifeyoucansave.org/Giving-Games/What-is-a-Giving-Game\">here</a>.</span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Jon_Behar"}}, {"_id": "yhs3eqbk6N4prEQC2", "title": "Returns Functions and Funding Gaps", "postedAt": "2017-05-23T08:22:44.935Z", "htmlBody": "<html><body><p>As organisations receive more funding, the value of extra funding changes. This is relevant for donation decisions. People have used various concepts to discuss this feature:</p>\n<ul>\n<li>Room for more funding</li>\n<li>Funding gaps</li>\n<li>Diminishing (marginal) returns&#xA0;</li>\n</ul>\n<p>In this pair of posts I discuss what people might mean by these different terms:</p>\n<ol>\n<li><a href=\"https://www.centreforeffectivealtruism.org/blog/defining-returns-functions-and-funding-gaps/\">Defining returns functions and funding gaps</a> sharpens up the definitions of these terms.</li>\n<li><a href=\"https://www.centreforeffectivealtruism.org/blog/selecting-the-appropriate-model-for-diminishing-returns/\">Selecting the appropriate model for marginal returns </a>analyses the strengths and weaknesses of different models</li>\n</ol>\n<p>The second post is co-authored with Owen Cotton-Barratt. He provided many of the ideas in the posts.</p></body></html>", "user": {"username": "Maxdalton"}}]