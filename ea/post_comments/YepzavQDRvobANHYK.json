[{"_id": "4JPxbvKsvQJdnCiqM", "postedAt": "2024-03-25T08:58:47.220Z", "postId": "YepzavQDRvobANHYK", "htmlBody": "<p>I'm quite tempted to create a course for conceptual AI alignment, especially since agent foundations has been removed from the latest version of the BlueDot Impact course<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"k0y9tuptked\" role=\"doc-noteref\" id=\"fnrefk0y9tuptked\"><sup><a href=\"#fnk0y9tuptked\">[1]</a></sup></span>.<br><br>If I did this, I would probably run it as follows:</p><p>a) Each week would have two sessions. One to discuss the readings and another for people to bounce their takes off others in the cohort. I expect that people trying to learn conceptual alignment would benefit from having extra time to discuss their ideas with informed participants.<br>b) The course would be less introductory, though without assuming knowledge of AGISF. AGISF already serves as a general introduction for those who need it and making progress on conceptual alignment is less of a numbers game, so it would likely make sense to focus on people further along the pipeline, rather than trying to expand the top of the funnel. In terms of the rough target audience, I imagine people who have been browsing Less Wrong or hanging around the AI safety community for years; or maybe someone who found out about it more recently and has been seriously reading up on it for the last couple of months. For this reason, I would want to assume that people already know why we're worried about AI Safety and basic ideas like inner/outer alignment and instrumental convergence.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"2\" data-footnote-id=\"vidi7ctmkj\" role=\"doc-noteref\" id=\"fnrefvidi7ctmkj\"><sup><a href=\"#fnvidi7ctmkj\">[2]</a></sup></span><br>c) I'd probably follow the AGISF in picking one question to focus on every week. I also like how it contextualises each reading.</p><p>Figuring out what to include seems like it'd be a massive challenge, but I agree that one of the best ways to do this would be to just create a curriculum, send it around to people and then additionally collect feedback from people who have gone through the course.</p><p>Anyway, I'd love to hear if anyone has any thoughts on what such a course should look like.<br><br>(The closest current course is the <a href=\"https://forum.effectivealtruism.org/posts/WmnAQ4qTYwCviwDhS/announcing-key-phenomena-in-ai-risk-facilitated-reading\">Key Phenomenon in AI Safety Course</a> that PIBSS ran, but this would assume that people are more technical - in the broader sense where technical includes maths, physics, comp sci, etc - and would be less introductory).</p><ol class=\"footnote-section footnotes\" data-footnote-section=\"\" role=\"doc-endnotes\"><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"1\" data-footnote-id=\"k0y9tuptked\" role=\"doc-endnote\" id=\"fnk0y9tuptked\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"k0y9tuptked\"><sup><strong><a href=\"#fnrefk0y9tuptked\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>This is quite a reasonable decision. Shorter timelines makes agent foundations work less pressing. Additionally, I imagine that most people who complete AGISF would not gain that much value from covering a week on agent foundations, at least not this early in their alignment journeys. Having a week where a substantial part of the cohort feel \"why was I taught this\" is not a very good experience for them.</p></div></li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"2\" data-footnote-id=\"vidi7ctmkj\" role=\"doc-endnote\" id=\"fnvidi7ctmkj\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"vidi7ctmkj\"><sup><strong><a href=\"#fnrefvidi7ctmkj\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Though it wouldn't be too hard to create a document containing assumed knowledge.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "yoYpGrrCtFJwKiwZh", "postedAt": "2024-03-25T10:18:34.405Z", "postId": "YepzavQDRvobANHYK", "htmlBody": "<p>Thanks for engaging!</p><blockquote><p>Two sessions. One to discuss the readings and another for people to bounce their takes off others in the cohort.</p></blockquote><p>Sounds like a fun experiment! I found that just open discussion sometimes leads to less valuable discussion, so in both cases I'd focus on a few specific discussion prompts / trying to help people come to a conclusion on some question. I linked to something about learning activities in the main post, which I think helps with session design. As with anything though, I think trying it out is the only way to know for sure, so feel free to ignore me.</p><blockquote><p>without assuming knowledge of AGISF</p></blockquote><p>I'd be keen to hear specifically what the pre-requisite knowledge is - just in order to inform people if they 'know enough' to take your course. Maybe it's weeks 1-3 of the alignment course? Agree with your assessment that further courses can be more specific, though.</p><blockquote><p>I agree that one of the best ways to do this would be to just create a curriculum, send it around to people and then additionally collect feedback from people who have gone through the course</p></blockquote><p>Sounds right! I would encourage you trying to front-load some of the work before creating a curriculum though. Without knowing how expert you are in agent foundations yourself - I'd suggest trying to take steps that mean your first stab is close enough for giving feedback to seem valuable to the people you ask, and so it's not a huge lift to get from 1st draft to final product and there are no nasty surprises from people who would have done it completely differently.</p><p>I.e. what if you ask 3-5 experts what they think the most important part of agent foundations is, and maybe try to conduct 30 min interviews with them to solicit the story they would tell in a curriculum? You can also ask them their top recommended resources, and why they recommend it. That would be a strong start, I think.</p>", "parentCommentId": "4JPxbvKsvQJdnCiqM", "user": {"username": "j_bernardi"}}, {"_id": "FiFANcA5LKk9ftx4E", "postedAt": "2024-03-25T11:26:08.219Z", "postId": "YepzavQDRvobANHYK", "htmlBody": "<blockquote><p>I found that just open discussion sometimes leads to less valuable discussion, so in both cases I'd focus on a few specific discussion prompts / trying to help people come to a conclusion on some question</p></blockquote><p><br>That's useful feedback. Maybe it'd be best to take some time at the end of the first session of the week to figure out what questions to discuss in the second session? This would also allow people to look things up before the discussion and take some time for reflection.</p><blockquote><p>I'd be keen to hear specifically what the pre-requisite knowledge is - just in order to inform people if they 'know enough' to take your course. Maybe it's weeks 1-3 of the alignment course?</p></blockquote><p>Thoughts on prerequisites off the top of my head:<br>Week 0: Even though it is a theory course, it would likely be useful to have some basic understanding of machine learning, although this would vary depending on the exact content of the course. It might or might not make sense to run a week 0 depending on most people's backgrounds.<br>Week 1 &amp; 2: I'd assume that the participants have at least a basic understanding of inner vs outer alignment, deceptive alignment, instrumental convergence, orthogonality thesis, why we're concerned about powerful optimisers, value lock-in, recursive self-improvement, slow vs. fast take-off, superintelligence, transformative AI, wireheading, though I could quite easily create a document that defines all of these terms. The purpose of this course also wouldn't be to reiterate the basic AI safety argument, although it might cover debates such as the validity of counting arguments for mesa-optimisers or whether RLHF means that we should expect outer alignment to be solved by default.</p><blockquote><p>I.e. what if you ask 3-5 experts what they think the most important part of agent foundations is, and maybe try to conduct 30 min interviews with them to solicit the story they would tell in a curriculum? You can also ask them their top recommended resources, and why they recommend it. That would be a strong start, I think.</p></blockquote><p>That's a great suggestion. I would still be tempted to create a draft curriculum though, even just at the level of week 1 focuses on question x and includes readings on topics a, b and c. I could also lift heavily from the previous <a href=\"https://course.aisafetyfundamentals.com/alignment-2023?session=8\">agent foundations</a> week and other past versions of AISF, <a href=\"https://course.aisafetyfundamentals.com/alignment-201\">alignment 201</a>, <a href=\"https://forum.effectivealtruism.org/posts/WmnAQ4qTYwCviwDhS/announcing-key-phenomena-in-ai-risk-facilitated-reading\">key phenomenon in AI Safety</a>, <a href=\"https://www.lesswrong.com/posts/JsjJuikJsidkyfhyr/mats-ai-safety-strategy-curriculum\">MATS AI Safety Strategy Curriculum</a>, <a href=\"https://intelligence.org/research-guide/\">MIRI's Research Guide</a>, &nbsp;John Wentworth's alignment training program + the highlighted AI Safety Sequences on Less Wrong (in addition to possibly including some material from the AI Safety Bootcamp or Advanced Fellowship that I ran).</p><p>I'd want to first ask them what they would like to see included without them being anchored on my draft, then I'd show them my draft and ask for more specific feedback. Expert time is valuable, so I'd want to get the most out of their time and it is easier to critique a specific artifact.</p>", "parentCommentId": "yoYpGrrCtFJwKiwZh", "user": {"username": "casebash"}}, {"_id": "MKPxTcW6cHMwiBu6P", "postedAt": "2024-03-25T22:28:22.701Z", "postId": "YepzavQDRvobANHYK", "htmlBody": "<p>I was surprised to read this:&nbsp;</p><blockquote><p><strong>In 2020, the going advice for how to learn about AI Safety for the first time was:</strong></p><ol><li><strong>Read everything on the alignment forum. </strong>[...]</li><li><strong>Speak to AI safety researchers.</strong> [...]</li></ol></blockquote><p><br>MIRI, CHAI and 80k all had public reading guides since at least 2017, when I started studying AI Safety.</p><ul><li><a href=\"https://humancompatible.ai/bibliography\">Recommended Materials \u2013 Center for Human-Compatible Artificial Intelligence (humancompatible.ai)</a></li><li><a href=\"https://80000hours.org/articles/ai-safety-syllabus/\">AI safety syllabus - 80,000 Hours (80000hours.org)</a></li><li><a href=\"https://intelligence.org/research-guide/\">Research Guide - Machine Intelligence Research Institute</a></li></ul><p>So seems like at least part of the problem was that these where not well known enough? Which by the way is now a problem for the AI Safety Fundamentals curriculum. When I was giving career advise, most people I talked to, didn't know that the curriculum is publicly available for self studies.&nbsp;</p><p>Despite the existence of these older resources, I still think AI Safety Fundamentals is great.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Linda Linsefors"}}, {"_id": "ASAxnJeEmBck5CNxq", "postedAt": "2024-03-25T23:07:42.417Z", "postId": "YepzavQDRvobANHYK", "htmlBody": "<blockquote><p>Week 0: Even though it is a theory course, it would likely be useful to have some basic understanding of machine learning, although this would vary depending on the exact content of the course. It might or might not make sense to run a week 0 depending on most people's backgrounds.</p></blockquote><p>I would reccomend having a week 0 with some ML and RL basics.&nbsp;<br><br>I did a day 0 ML and RL speed run, at the start of two of my AI Safety workshops at EA hotel in 2019. Where you there for that? It might have been recorded, but I have no idea where it might have ended up. Although obviously some things have happened since then.</p><blockquote><p>Week 1 &amp; 2: I'd assume that the participants have at least a basic understanding of inner vs outer alignment, deceptive alignment, instrumental convergence, orthogonality thesis, why we're concerned about powerful optimisers, value lock-in, recursive self-improvement, slow vs. fast take-off, superintelligence, transformative AI, wireheading, though I could quite easily create a document that defines all of these terms.&nbsp;</p></blockquote><p>Seems very worth creating. Depending on peoples background some people will have an understanding of these with out knowing the terminology. A document explaining each term, and a \"read more\" link to some useful post would be great. Both for people to know if they have the pre-requisite, and to help anyone who <i>almost</i> have the prerequisite to find that one blogpost they (them specifically) should read to be able to follow the course.</p>", "parentCommentId": "FiFANcA5LKk9ftx4E", "user": {"username": "Linda Linsefors"}}, {"_id": "fogAWyx5B5WyPvXnd", "postedAt": "2024-03-26T00:33:02.948Z", "postId": "YepzavQDRvobANHYK", "htmlBody": "<p>I was there for an AI Safety workshop, I can't remember the content though. Do you know what you included?</p>", "parentCommentId": "ASAxnJeEmBck5CNxq", "user": {"username": "casebash"}}, {"_id": "EWucinPq76epE4rL6", "postedAt": "2024-03-26T00:36:40.513Z", "postId": "YepzavQDRvobANHYK", "htmlBody": "<p>I didn't know that CHAI or 80,000 Hours had recommended material.</p><p>The 80,000 Hours syllabus = \"Go read a bunch of textbooks\". This is probably not ideal for a \"getting started' guide.</p>", "parentCommentId": "MKPxTcW6cHMwiBu6P", "user": {"username": "casebash"}}, {"_id": "CaE9WtvFkREaRpwdk", "postedAt": "2024-03-27T14:45:44.670Z", "postId": "YepzavQDRvobANHYK", "htmlBody": "<blockquote>\n<p>The completion rate at BlueDot Impact averaged out at about 75%</p>\n</blockquote>\n<p>How do you define completion?</p>\n", "parentCommentId": null, "user": {"username": "Peter_Hartree"}}, {"_id": "5mMRkFNQMNuJjcDMw", "postedAt": "2024-03-28T22:45:23.137Z", "postId": "YepzavQDRvobANHYK", "htmlBody": "<p>I do think AISF is a real improvement to the field. My apologies for not making this clear enough.</p><p>&nbsp;</p><blockquote><p>The 80,000 Hours syllabus = \"Go read a bunch of textbooks\". This is probably not ideal for a \"getting started' guide.</p></blockquote><p>You mean MIRI's syllabus?&nbsp;</p><p>I don't remember what 80k's one looked like back in the days, but the one that is up not is not just \"Go read a bunch of textbooks\".</p><p>I personally used CHAI's one and found it very useful.</p><p>Also some times you <i>should</i> go read a bunch of text books. Textbooks are great.&nbsp;</p>", "parentCommentId": "EWucinPq76epE4rL6", "user": {"username": "Linda Linsefors"}}]