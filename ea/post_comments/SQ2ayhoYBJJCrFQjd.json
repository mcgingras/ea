[{"_id": "aanRQzGuL7ZZocKob", "postedAt": "2023-01-01T22:48:36.631Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><strong>Title:&nbsp;</strong></p><p>Getting on a different train: can Effective Altruism avoid collapsing into absurdity?</p><p><strong>Author:</strong></p><p>Peter McLaughlin</p><p><strong>URL:&nbsp;</strong></p><p><a href=\"https://forum.effectivealtruism.org/posts/8wWYmHsnqPvQEnapu/getting-on-a-different-train-can-effective-altruism-avoid\">https://forum.effectivealtruism.org/posts/8wWYmHsnqPvQEnapu/getting-on-a-different-train-can-effective-altruism-avoid</a></p><p><strong>Why it's good:</strong></p><p>McLaughlin highlights a problem for people who want to say that scale matters, and also avoid the train to crazy town.</p><blockquote><p>It's not clear how anyone actually gets off the train to crazy town. Once you allow even a little bit of utilitarianism in, the unpalatable consequences follow immediately. The train might be an express service: once the doors close behind you, you can\u2019t get off until the end of the line.</p><p>As Richard Y. Chappell has put it, EAs want&nbsp;<a href=\"https://rychappell.substack.com/p/beneficentrism?s=r\">\u2018utilitarianism minus the controversial bits\u2019</a>. Yet it\u2019s not immediately clear how the models and decision-procedures used by Effective Altruists can consistently avoid any of the problems for utilitarianism: as examples above illustrate, it\u2019s entirely possible that even the simplest utilitarian premises can lead to seriously difficult conclusions.</p></blockquote><p>Tyler Cowen wrote a paper on this problem in 1996, called &nbsp;<a href=\"https://www.jstor.org/stable/2382033\">\u2019What Do We Learn from the Repugnant Conclusion?\u2019</a>. McLaughlin's post opens with an excellent summary.</p><p>The upshot:</p><blockquote><p>For&nbsp;<i>any</i>&nbsp;moral theory with universal domain where utility matters at all, either the marginal value of utility diminishes rapidly (asymptotically) towards zero, or considerations of utility come to swamp all other values.</p></blockquote><p>Uh oh!</p>", "parentCommentId": null, "user": {"username": "Peter_Hartree"}}, {"_id": "PuYdqk6fc4BLWF8gG", "postedAt": "2023-01-01T22:49:34.777Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p>Title: Paul Christiano on how you might get consequentialist behavior from large language models<br>Author: Paul Christiano<br>URL: <a href=\"https://forum.effectivealtruism.org/posts/dgk2eLf8DLxEG6msd/how-would-a-language-model-become-goal-directed?commentId=cbJDeSPtbyy2XNr8E\">https://forum.effectivealtruism.org/posts/dgk2eLf8DLxEG6msd/how-would-a-language-model-become-goal-directed?commentId=cbJDeSPtbyy2XNr8E</a><br>Why it's good: I think lots of people are very wrong about how LLMs might lead to consequentialist behavior, and Paul's comment here is my favorite attempt at answering this question. I think that this question is extremely important.</p>", "parentCommentId": null, "user": {"username": "Buck"}}, {"_id": "jWtQ4sDXhdwmgHMmz", "postedAt": "2023-01-01T23:00:16.010Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><strong>Title:</strong></p>\n<p>EA\u2019s brain-over-body bias, and the embodied value problem in AI alignment</p>\n<p><strong>Author:</strong></p>\n<p>Geoffrey Miller</p>\n<p><strong>URL:</strong></p>\n<p><a href=\"https://forum.effectivealtruism.org/posts/zNS53uu2tLGEJKnk9/ea-s-brain-over-body-bias-and-the-embodied-value-problem-in\">https://forum.effectivealtruism.org/posts/zNS53uu2tLGEJKnk9/ea-s-brain-over-body-bias-and-the-embodied-value-problem-in</a></p>\n<p><strong>Why it's good:</strong></p>\n<p>Embodied cognition is a hot topic in cognitive science. Are AI safety people overlooking this?</p>\n<p>From Geoffrey\u2019s introduction:</p>\n<blockquote>\n<p>Evolutionary biology and evolutionary medicine routinely analyze our bodies\u2019 biological goals, fitness interests, and homeostatic mechanisms in terms of how they promote survival and reproduction. However the EA movement includes some \u2018brain-over-body biases\u2019 that often make our brains\u2019 values more salient than our bodies\u2019 values. This can lead to some distortions, blind spots, and failure modes in thinking about AI alignment. In this essay I\u2019ll explore how AI alignment might benefit from thinking more explicitly and carefully about how to model our embodied values.</p>\n</blockquote>\n<p>Big, if there\u2019s something to it. But the piece received one three word comment...</p>\n", "parentCommentId": null, "user": {"username": "Peter_Hartree"}}, {"_id": "vD7AhYkrEyZz6aR9H", "postedAt": "2023-01-01T23:21:56.609Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><strong>Title:</strong></p><p>Bernard Williams: Ethics and the limits of impartiality</p><p><strong>Author:</strong></p><p>peterhartree</p><p><strong>URL:</strong></p><p><a href=\"/posts/G6EWTrArPDf74sr3S/bernard-williams-ethics-and-the-limits-of-impartiality\">https://forum.effectivealtruism.org/posts/G6EWTrArPDf74sr3S/bernard-williams-ethics-and-the-limits-of-impartiality</a></p><p><strong>Why it's good:</strong></p><p>Derek Parfit saw Bernard Williams as his most important antagonist. Parfit was obsessed with Williams\u2019 \u201c<a href=\"https://annas-archive.org/md5/755bfd8ea38237a4ac40c1130bd3b720\">Internal &amp; External Reasons</a>\u201d paper for several decades.</p><p>My post introduces some of Bernard Williams\u2019 views on metaphilosophy, metaethics and reasons.</p><blockquote><p>What are we doing when we do moral philosophy? How should this self-understanding inform our practice of philosophy, and what we might hope to gain from it?</p></blockquote><p>According to Williams:</p><blockquote><p>Moral philosophy is about making sense of the situation in which we find ourselves, and deciding what to do about it.</p></blockquote><p>Williams wants to push back against a \u201cscientistic\u201d trend in moral philosophy, and against philosophers who exhibit \u201ca Platonic contempt for the the human and the contingent in the face of the universal\u201d. Such philosophers believe that:</p><blockquote><p>if there were an absolute conception of the world, a representation of it which was maximally independent of perspective, that would be better than more perspectival or locally conditioned representations of the world.</p></blockquote><p>And, relatedly:</p><blockquote><p>that offering an absolute conception is the real thing, what really matters in the direction of intellectual authority</p></blockquote><p>Williams thinks there\u2019s another way. It may not give us everything we want, but perhaps it\u2019s all we can have.&nbsp;</p><p>If the post leaves you wanting more, I got into related themes on Twitter last night, <a href=\"https://twitter.com/cxgonzalez/status/1607460683901288448\">in conversation with The Ghost of Jeremy Bentham</a> after <a href=\"https://twitter.com/peterhartree/status/1607455177447215105\">some earlier exegetical mischief</a>. Scroll down and click \u201cShow replies\u201d.</p>", "parentCommentId": null, "user": {"username": "Peter_Hartree"}}, {"_id": "SANExNAr8sHZpXnf3", "postedAt": "2023-01-01T23:48:23.811Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p>My take: perhaps the more principled among us should make room for more messy fudges in our thought. <a href=\"https://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">Cluster thinking</a>, <a href=\"https://sun.pjh.is/holden-karnofsky-on-bounded-commensurability-as-a-way-to-get-ahead-of-the-curve-on-moral-values\">bounded commensurability</a> and <a href=\"https://two-thirds-utilitarian.com/\">two-thirds utilitarianism</a> for the win.</p>", "parentCommentId": "aanRQzGuL7ZZocKob", "user": {"username": "Peter_Hartree"}}, {"_id": "BCioQe22m7YuGoxMk", "postedAt": "2023-01-02T00:27:20.648Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><strong>Title: </strong><a href=\"https://forum.effectivealtruism.org/posts/4rGpNNoHxxNyEHde3/most-problems-fall-within-a-100x-tractability-range-under\">Most problems fall within a 100x tractability range (under certain assumptions)</a>&nbsp;</p><p><strong>Author: </strong>Thomas Kwa</p><p><strong>URL: </strong><a href=\"https://forum.effectivealtruism.org/posts/4rGpNNoHxxNyEHde3/most-problems-fall-within-a-100x-tractability-range-under\">https://forum.effectivealtruism.org/posts/4rGpNNoHxxNyEHde3/most-problems-fall-within-a-100x-tractability-range-under</a>&nbsp;</p><p><strong>Why it's good:</strong> I don't have time to write a long comment, but this is one of the posts where my first reaction was, \"no way,\" and then I went and checked the math and was convinced. Since then, I've often thought of it and brought it up \u2014 I often hear people say or imply, \"sure, the scope of X is huge and it's really neglected, but maybe it's massively more intractable...\" without an explanation for why X is so unusual.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Lizka"}}, {"_id": "MieWPro7cr6n4RNaP", "postedAt": "2023-01-02T01:10:15.942Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><strong>Title: </strong><a href=\"https://forum.effectivealtruism.org/posts/oRx3LeqFdxN2JTANJ/epistemic-legibility\">Epistemic Legibility</a> - \"Tl;dr: being easy to argue with is a virtue, separate from being correct.\"</p><p><strong>Author:</strong> Elizabeth</p><p><strong>URL: </strong><a href=\"https://forum.effectivealtruism.org/posts/oRx3LeqFdxN2JTANJ/epistemic-legibility\">https://forum.effectivealtruism.org/posts/oRx3LeqFdxN2JTANJ/epistemic-legibility</a>&nbsp;</p><p><strong>Why it's good:</strong>&nbsp;</p><p>Things that make texts or discussions more \"epistemically legible\" <a href=\"https://forum.effectivealtruism.org/posts/oRx3LeqFdxN2JTANJ/epistemic-legibility#How_to_be_Legible\">include</a>: making clear what you actually believe, making clear the evidence you're really basing your beliefs on (don't just search for a random hyperlink for a claim \u2014 say why you really believe this, even if it's \"gut feeling\" or \"anecdotal evidence\"), making the logical steps of your argument clear (don't just list assorted evidence and a conclusion \u2014 explain what leads to what and how), use examples, pick a vocabulary that's appropriate to your audience, and write the argument down.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefna3wh30tlhb\"><sup><a href=\"#fnna3wh30tlhb\">[1]</a></sup></span>&nbsp;I also think that summaries or outlines help.</p><p>So why is this important?</p><p>I think a lot of discussions are confused for a bunch of reasons. One of these is that it's hard to understand exactly what other parties are saying and why, both because communicating clearly is hard and because we have a tendency to want to hedge and protect our views \u2014 making it harder for others to see how we might be wrong (e.g. because of <a href=\"https://forum.effectivealtruism.org/topics/impostor-syndrome\">impostor syndrome</a>).&nbsp;</p><p>For instance, I might want to say, \"I think a lot of discussions are confused for a bunch of reasons,\" and walk away \u2014 especially if I find a good hyperlink for \"confused\" or something. But that would make it very hard to argue with me. I didn't explain what \"confused\" really means to me, I didn't list specific reasons, I didn't say which discussions, or approximately how many of them. (So what do you argue with? \"No, I think very few discussions are 'confused'?\") I could, instead, write something more specific, like \"I think that too many posts on the Forum (for my taste) lead to discussions that misinterpret the claims of the related post or are arguing about details or logical connections that aren't actually relevant. This happens for a bunch of reasons, some of which I could list, but I'm focusing on a specific thing here that is one of what I'd guess are the top 10 reasons, and here's how that happens...\" This is still pretty vague, but I think it's better. You can now say, \"Here's my list of 10 reasons that contribute to this phenomenon, and epistemic illegibility doesn't get into the top 10 \u2014 which do you think are less important?\" And I imagine that this leads to a more productive discussion.</p><p>There are downsides and <a href=\"https://forum.effectivealtruism.org/posts/oRx3LeqFdxN2JTANJ/epistemic-legibility#Costs_of_Legibility\">costs</a> to being more specific or epistemically legible like this \u2014 Elizabeth's post acknowledges them, and notes that not everything should necessarily be epistemically legible. For instance, the rewritten claim above is messier and longer than the original one. (Although I don't think this always has to be true.) But on the margin, I think I'd prefer more posts that are messier and even longer if they're also more epistemically legible. And I really like the <a href=\"https://forum.effectivealtruism.org/posts/oRx3LeqFdxN2JTANJ/epistemic-legibility#How_to_be_Legible\">specific suggestions on how to be legible</a>.&nbsp;</p><p>Or, as Elizabeth puts it,&nbsp;</p><blockquote><p>If I hear an epistemically legible argument, I have a lot of options. I can point out places I think the author missed data that impacts their conclusion, or made an illogical leap. I can notice when I know of evidence supporting their conclusions that they didn\u2019t mention. I can see implications of their conclusions that they didn\u2019t spell out. I can synthesize with other things I know, that the author didn\u2019t include.</p><p>If I hear an illegible argument, I have very few options. Perhaps the best case scenario is that it unlocks something I already knew subconsciously but was unable to articulate, or needed permission to admit. This is a huge service! But if I disagree with the argument, or even just find it suspicious, my options are kind of crap. I write a response of equally low legibility, which is unlikely to improve understanding for anyone. Or I could write up a legible case for why I disagree, but that is much more work than responding to a legible original, and often more work than went into the argument I\u2019m responding to, because it\u2019s not obvious what I\u2019m arguing against.&nbsp; I need to argue against many more things to be considered comprehensive. If you believe Y because of X, I can debate X. If you believe Y because \u2026:shrug:\u2026 I have to imagine every possible reason you could do so, counter all of them, and then still leave myself open to something I didn\u2019t think of. Which is exhausting.</p><p>I could also ask questions, but the more legible an argument is, the easier it is to know what questions matter and the most productive way to ask them.&nbsp;</p><p>I could walk away, and I am in fact much more likely to do that with an illegible argument. But that ends up creating a tax on legibility because it makes one easier to argue with, which is the opposite of what I want.</p></blockquote><p>I also love <a href=\"https://forum.effectivealtruism.org/posts/i9RJjun327SnT3vW8/reasoning-transparency\">reasoning transparency</a>, but feel like it gets at the quality in a different way, with a different emphasis. And I've also been using \"<a href=\"https://forum.effectivealtruism.org/posts/gtivkdZKjFRcvfLQC/butterfly-ideas\">butterfly idea</a>\" a lot.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnna3wh30tlhb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefna3wh30tlhb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I like this essay a lot, on this point:<a href=\"http://www.paulgraham.com/words.html\"> Putting Ideas into Words</a>. Excerpt is copied from a different place where I shared the essay, so I don't remember how relevant it is here specifically.&nbsp;</p><blockquote><p>Writing about something, even something you know well, usually shows you that you didn't know it as well as you thought. Putting ideas into words is a severe test. The first words you choose are usually wrong; you have to rewrite sentences over and over to get them exactly right. And your ideas won't just be imprecise, but incomplete too. Half the ideas that end up in an essay will be ones you thought of while you were writing it. Indeed, that's why I write them.</p><p>Once you publish something, the convention is that whatever you wrote was what you thought before you wrote it. These were your ideas, and now you've expressed them. But you know this isn't true. You know that putting your ideas into words changed them. And not just the ideas you published. Presumably there were others that turned out to be too broken to fix, and those you discarded instead.</p><p>[...]</p><p>Putting ideas into words doesn't have to mean writing, of course. You can also do it the old way, by talking. But in my experience, writing is the stricter test. You have to commit to a single, optimal sequence of words. Less can go unsaid when you don't have tone of voice to carry meaning. And you can focus in a way that would seem excessive in conversation.&nbsp;</p><p>[...]</p></blockquote></div></li></ol>", "parentCommentId": null, "user": {"username": "Lizka"}}, {"_id": "MuB8pnYZPux5W4uGR", "postedAt": "2023-01-02T04:46:26.315Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><strong>Title</strong>: Prizes for ML Safety Benchmark Ideas</p>\n<p><strong>Author</strong>: Joshc, Dan H</p>\n<p><strong>URL</strong>: <a href=\"https://forum.effectivealtruism.org/posts/jo7hmLrhy576zEyiL/prizes-for-ml-safety-benchmark-ideas\">https://forum.effectivealtruism.org/posts/jo7hmLrhy576zEyiL/prizes-for-ml-safety-benchmark-ideas</a></p>\n<p><strong>Why it's good</strong>: Benchmarks have been a big driver of progress in AI.  Benchmarks for ML safety could be a great way to drive progress in AI alignment, and get people to switch from capabilities-ish research to safety-ish research.  The structure of the prize looks good: They're offering a lot of money, there are still over 6 months until the submission deadline, and all they're asking for is a brief write-up.  Thinking up benchmarks also seems like the sort of problem that's a good fit for a prize.  My only gripe with the competition is that it doesn't seem widely known, hence posting here.</p>\n", "parentCommentId": null, "user": {"username": "John_Maxwell_IV"}}, {"_id": "JLcAA9tgs8iRxSurq", "postedAt": "2023-01-02T04:46:59.153Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p>I wonder if a good standard rule for prizes is that you want a marketing budget which is at least 10-20% the size of the prize pool, for buying ads on podcasts ML researchers listen to or subreddits they read or whatever.  Another idea is to incentivize people to make submissions publicly, so your contest promotes itself.</p>\n", "parentCommentId": "MuB8pnYZPux5W4uGR", "user": {"username": "John_Maxwell_IV"}}, {"_id": "Fs9kNYekvwibtWCoj", "postedAt": "2023-01-02T08:54:12.612Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p>P.S. If you don't like the Bernard Williams stuff, I'd love to hear your quick thoughts on why.</p>\n<p>He is a divisive figure, especially in Oxford philosophy circles. But Parfit was correct to take him seriously.</p>\n<p>His book \"<a href=\"https://annas-archive.org/md5/6b08a6bfa7ce378084f5efc911e994be\">Ethics &amp; The Limits of Philosophy</a>\" is often recommended as the place to start.</p>\n", "parentCommentId": "vD7AhYkrEyZz6aR9H", "user": {"username": "Peter_Hartree"}}, {"_id": "du3eqyoTBNPezM9c9", "postedAt": "2023-01-02T15:44:34.111Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p>My main hesitation on this would be that I never really figured out how the difference between plausible meta-ethical theories was decision relevant.(I'm not sure if that counts as not liking it though - still interesting!)</p>", "parentCommentId": "Fs9kNYekvwibtWCoj", "user": {"username": "Michelle_Hutchinson"}}, {"_id": "aodzr9Jxys2PbGi7g", "postedAt": "2023-01-03T01:32:36.078Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p>I'll bite on the invitation to nominate my own content. This <a href=\"https://forum.effectivealtruism.org/posts/xdMn6FeQGjrXDPnQj/leveraging-labor-shortages-as-a-pathway-to-career-impact\">short piece of mine</a> spent little time on the front page and didn't seem to capture much attention, either positive or negative. I'm not sure why, but I'd love for the ideas in it to get a second look, especially by people who know more about the topic than I do.</p><p><strong>Title: </strong>Leveraging labor shortages as a pathway to career impact?<i> [note: question mark was added today to better reflect the intended vibe of the post]</i></p><p><strong>Author: </strong>Ian David Moss</p><p><strong>URL: </strong><a href=\"https://forum.effectivealtruism.org/posts/xdMn6FeQGjrXDPnQj/leveraging-labor-shortages-as-a-pathway-to-career-impact\">https://forum.effectivealtruism.org/posts/xdMn6FeQGjrXDPnQj/leveraging-labor-shortages-as-a-pathway-to-career-impact</a></p><p><strong>Why it's good:</strong> I think it surfaces an important and rarely-discussed point that could have significant implications for norms and practices around EA community-building and career guidance if it were determined to be valid.</p>", "parentCommentId": null, "user": {"username": "IanDavidMoss"}}, {"_id": "xwuHvZPR4gBn4mKuE", "postedAt": "2023-01-03T03:40:19.863Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/av7MiEhi983SjoXTe/some-core-assumptions-of-effective-altruism-according-to-me\"><strong>Some core assumptions of effective altruism, according to me</strong></a><strong> &nbsp;by </strong><a href=\"https://forum.effectivealtruism.org/users/peterhartree\"><strong>peterhartree</strong></a><strong> (85 karma)</strong><br>Why it's good: I think people should focus on EA's core assumptions much more than they do and I think this list is pretty accurate (and more accurate than the list it is responding to).</p>", "parentCommentId": null, "user": {"username": "Holly"}}, {"_id": "vXLn9SkJyqNKJW7sy", "postedAt": "2023-01-03T05:35:20.020Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/Cs8qhNakLuLXY4GvE/criticism-of-the-main-framework-in-ai-alignment\"><strong>Criticism of the main framework in AI alignment</strong></a><strong> by </strong><a href=\"https://forum.effectivealtruism.org/users/michele-campolo\"><strong>Michele Campolo</strong></a><strong> (34 karma)</strong><br>Why it's good: Explores an area of AGI alignment that I think is under-discussed, namely the possibility of using AGI for direct moral progress. (Other under-discussed areas with more karma: <a href=\"https://forum.effectivealtruism.org/posts/KqCybin8rtfP3qztq/agi-and-lock-in\">AGI and Lock-In</a>, <a href=\"https://forum.effectivealtruism.org/posts/KmhCzCKdig8NPby2D/robert-long-on-why-you-might-want-to-care-about-artificial\">Robert Long on Why You Might Want To Care About Artificial Sentience</a>, <a href=\"https://forum.effectivealtruism.org/posts/Z7r83zrSXcis6ymKo/dissolving-ai-risk-parameter-uncertainty-in-ai-future\">\u2018Dissolving\u2019 AI Risk \u2013 Parameter Uncertainty in AI Future Forecasting</a>, <a href=\"https://forum.effectivealtruism.org/posts/35bfnGmsyrZkEnkLJ/steering-ai-to-care-for-animals-and-soon\">Steering AI to care for animals, and soon</a>.)</p>", "parentCommentId": null, "user": {"username": "Holly"}}, {"_id": "TaGvsKywThvwbBgr8", "postedAt": "2023-01-03T06:21:09.216Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/bkF4jWM9pbBFxnCLH/observations-of-community-building-in-asia-a\"><strong>Observations of community building in Asia, a \ud83e\uddf5</strong></a><strong> by </strong><a href=\"https://forum.effectivealtruism.org/users/vaidehi_agarwalla\"><strong>Vaidehi Agarwalla</strong></a><strong> (36 karma)</strong><br>Why it's good: Concrete ideas for improving diversity in EA. (Similar posts I particularly appreciated with more karma: <a href=\"https://forum.effectivealtruism.org/posts/4csmTBamMuQy9Zf6Q/top-down-interventions-that-could-increase-participation-and\">Top down interventions that could increase participation and impact of Low and Middle Income Countries in EA</a>, <a href=\"https://forum.effectivealtruism.org/posts/Hyco4iMbL6phJwCH2/ea-career-guide-for-people-from-lmics\">EA career guide for people from LMICs</a>, <a href=\"https://forum.effectivealtruism.org/posts/LCagfA2uS7idsLfoN/a-few-more-relevant-categories-to-think-about-diversity-in\">A few more relevant categories to think about diversity in EA</a>.)</p>", "parentCommentId": null, "user": {"username": "Holly"}}, {"_id": "6iB6FD3Tf92RrtqCE", "postedAt": "2023-01-03T06:39:08.233Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/euLNDcwmmtnXFxcsx/the-possibility-of-microorganism-suffering\"><strong>The Possibility of Microorganism Suffering</strong></a><strong> by </strong><a href=\"https://forum.effectivealtruism.org/users/elias-au-yeung\"><strong>Elias Au-Yeung</strong></a><strong> (46 karma)</strong><br>Why it's good: I think there should be more investigations into potential welfarist priorities that sound super weird. (Similarly, though higher-karma: <a href=\"https://forum.effectivealtruism.org/posts/vbhoFsyQmrntru6Kw/do-brains-contain-many-conscious-subsystems-if-so-should-we\">Do Brains Contain Many Conscious Subsystems? If So, Should We Act Differently?</a>, <a href=\"https://forum.effectivealtruism.org/posts/AexFu2RmYRApNStS5/reducing-nightmares-as-a-cause-area\">Reducing nightmares as a cause area</a>.)</p>", "parentCommentId": null, "user": {"username": "Holly"}}, {"_id": "mJ3K4nrEZHkD2ChJF", "postedAt": "2023-01-03T06:49:24.237Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/BCzDw2WKLjckcK2Ba/paper-summary-the-epistemic-challenge-to-longtermism\"><strong>Paper summary: The Epistemic Challenge to Longtermism (Christian Tarsney)</strong></a><strong> by </strong><a href=\"https://forum.effectivealtruism.org/users/global-priorities-institute\"><strong>Global Priorities Institute</strong></a><strong>, </strong><a href=\"https://forum.effectivealtruism.org/users/ejt\"><strong>EJT</strong></a><strong> (37 karma)</strong><br>Why it's good: An attempt to put numbers on the much-disputed tractability of longtermism (I also really appreciate summaries).</p>", "parentCommentId": null, "user": {"username": "Holly"}}, {"_id": "78kSopGpJLqeGusWr", "postedAt": "2023-01-03T07:02:49.921Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/6fFuPpENfBrjrywLj/space-governance-problem-profile-1\"><strong>Space governance - problem profile</strong></a><strong> by </strong><a href=\"https://forum.effectivealtruism.org/users/finm\"><strong>finm</strong></a><strong>, </strong><a href=\"https://forum.effectivealtruism.org/users/80000_hours\"><strong>80000_Hours</strong></a><strong> (65 karma)</strong><br>Why it's good: A new entry to 80,000 Hours' most pressing world problems that is huge in scale, severely neglected from a longtermist perspective, although admittedly not especially tractable (which is why I'd love to see more posts like <a href=\"https://forum.effectivealtruism.org/posts/x7ynaoXWSy7FQANio/influencing-united-nations-space-governance\">Influencing United Nations Space Governance</a>).</p>", "parentCommentId": null, "user": {"username": "Holly"}}, {"_id": "T8PiCckiiJh8mtPBM", "postedAt": "2023-01-03T07:24:37.856Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/DRaugD8TWj3pqxGRj/consequentialism-and-cluelessness\"><strong>Consequentialism and Cluelessness</strong></a><strong> by </strong><a href=\"https://forum.effectivealtruism.org/users/ryc\"><strong>Richard Y Chappell</strong></a><strong> (27 karma)</strong><br>Why it's good: Raises and defends an important point that I think would release a lot of people from cluelessness-induced paralysis if more widely shared, namely that Option A can still have higher expected value than Option B despite us having no clue what many of the consequences will be, because these invisible consequences speak neither for or against either option. (Another important point that I wish was better known is that <a href=\"https://forum.effectivealtruism.org/posts/DCZhan8phEMRHuewk/person-affecting-intuitions-can-often-be-money-pumped\">Person-affecting intuitions can often be money pumped</a>, although this post was essentially a repeat of a post from 2020.)</p>", "parentCommentId": null, "user": {"username": "Holly"}}, {"_id": "7rdJii6gqfZRuwhXC", "postedAt": "2023-01-03T07:31:58.307Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/LyTWpoQFozDfeQ9z8/some-carl-sagan-quotations\"><strong>Some Carl Sagan quotations</strong></a><strong> by </strong><a href=\"https://forum.effectivealtruism.org/users/finm\"><strong>finm</strong></a><strong> (77 karma)</strong><br>Why it's good: I really appreciate inspiring posts. (Higher-karma posts on this topic that I really appreciated: <a href=\"https://forum.effectivealtruism.org/posts/WRB4Lt4XpP3uSPxhP/open-call-for-ea-stories\">Open call for EA stories, \ud83c\udfa8 Altruist Dreams - a collaborative art piece from EAG SF</a>, <a href=\"https://forum.effectivealtruism.org/posts/oGdCtvuQv4BTuNFoC/good-things-that-happened-in-ea-this-year\">Good things that happened in EA this year</a>, <a href=\"https://forum.effectivealtruism.org/posts/XYdLTKZLQwTr337zM/the-spanish-speaking-effective-altruism-community-is-awesome\">The Spanish-Speaking Effective Altruism community is awesome</a>.)</p>", "parentCommentId": null, "user": {"username": "Holly"}}, {"_id": "6AyLrabqnPM3AgDDk", "postedAt": "2023-01-03T07:54:51.028Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/NRa7ndwZ6kmtJXKqx/ea-aligned-political-activity-in-a-us-congressional-primary\"><strong>EA-Aligned Political Activity in a US Congressional Primary: Concerns and Proposed Changes</strong></a><strong> by </strong><a href=\"https://forum.effectivealtruism.org/users/carolina_ea\"><strong>Carolina_EA</strong></a><strong> (78 karma)</strong><br>Why it's good: I am so, so appreciative when people share detailed, good-faith takes on parts of EA from perspectives that we rarely get such insight into. (Similar posts with at least the same karma, covering disagreement, agreement, praise, indifference, justification, advice: <a href=\"https://forum.effectivealtruism.org/posts/6NnnPvzCzxWpWzAb8/podcast-the-left-and-effective-altruism-with-habiba-islam\">Podcast: The Left and Effective Altruism with Habiba Islam</a>, <a href=\"https://forum.effectivealtruism.org/posts/XTX3MZRqvB7jNAaCM/a-subjective-account-of-what-it-s-like-to-join-an-ea-aligned\">A subjective account of what it's like to join an EA-aligned org without previous EA knowledge</a>, <a href=\"https://forum.effectivealtruism.org/posts/YeudcYiArwWrg77Ng/notes-from-a-pledger\">Notes From a Pledger</a>, <a href=\"https://forum.effectivealtruism.org/posts/vPMo5dRrgubTQGj9g/some-unfun-lessons-i-learned-as-a-junior-grantmaker\">Some unfun lessons I learned as a junior grantmaker</a>, <a href=\"https://forum.effectivealtruism.org/posts/NkPghabDd54nkG3kX/some-observations-from-an-ea-adjacent-charitable-effort\">Some observations from an EA-adjacent (?) charitable effort</a>.)</p>", "parentCommentId": null, "user": {"username": "Holly"}}, {"_id": "xhNi9AeisEhgMBLMf", "postedAt": "2023-01-03T09:17:40.799Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><strong>Title: </strong><a href=\"https://forum.effectivealtruism.org/posts/xnPhkLrfjSjooxnmM/forecasting-newsletter-april-2222\">Forecasting Newsletter: April 2222</a></p><p><strong>Author: </strong>Nuno</p><p><strong>URL: </strong><a href=\"https://forum.effectivealtruism.org/posts/xnPhkLrfjSjooxnmM/forecasting-newsletter-april-2222\">https://forum.effectivealtruism.org/posts/xnPhkLrfjSjooxnmM/forecasting-newsletter-april-2222</a><strong>&nbsp;</strong></p><p><strong>Why it's good:</strong> Incredible density of gags. Some of the in-jokes are so clever that I had to think all day to get them; some are so niche that no one except Nuno and the target could possibly laugh.</p>", "parentCommentId": null, "user": {"username": "technicalities"}}, {"_id": "F9afwmSH6ikbGe3ip", "postedAt": "2023-01-03T09:29:58.120Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p><strong>Title: </strong>The long reflection as the great stagnation&nbsp;</p><p><strong>Author: </strong>Larks</p><p><strong>URL: </strong><a href=\"https://forum.effectivealtruism.org/posts/o5Q8dXfnHTozW9jkY/the-long-reflection-as-the-great-stagnation\">https://forum.effectivealtruism.org/posts/o5Q8dXfnHTozW9jkY/the-long-reflection-as-the-great-stagnation</a><a href=\"https://forum.effectivealtruism.org/posts/xnPhkLrfjSjooxnmM/forecasting-newsletter-april-2222\"><strong>&nbsp;</strong></a></p><p><strong>Why it's good:</strong> Powerful attack on a cherished institution. I don't necessarily agree on the first order, but on the second order people <i>will</i> act up and ruin the Reflection.</p>", "parentCommentId": null, "user": {"username": "technicalities"}}, {"_id": "Smdh4GvJPpxCL5ag3", "postedAt": "2023-01-04T02:14:32.641Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p>Thanks for sharing this; I hadn't read it before and I found it persuasive.</p>", "parentCommentId": "BCioQe22m7YuGoxMk", "user": {"username": "Ben_West"}}, {"_id": "bri2zk9NZuHMPv5Fx", "postedAt": "2023-01-05T14:11:34.890Z", "postId": "SQ2ayhoYBJJCrFQjd", "htmlBody": "<p>Title: <a href=\"https://forum.effectivealtruism.org/posts/5XKAsEBMuxiycTHL7/working-with-the-beef-industry-for-chicken-welfare\">Working with the Beef Industry for Chicken Welfare</a></p><p>Author: <a href=\"https://forum.effectivealtruism.org/users/roberty\"><strong>RobertY</strong></a></p><p><strong>URL: </strong><a href=\"https://forum.effectivealtruism.org/posts/5XKAsEBMuxiycTHL7/working-with-the-beef-industry-for-chicken-welfare\"><strong>https://forum.effectivealtruism.org/posts/5XKAsEBMuxiycTHL7/working-with-the-beef-industry-for-chicken-welfare</strong></a></p><p><strong>Why it's good: Correct focus on a source of immense, totally unnecessary suffering, with outside-the-box thinking to help mitigate the suffering. Thanks, Robert!</strong></p>", "parentCommentId": null, "user": {"username": "MattBall"}}]