[{"_id": "hDWcbyZi5ky5kEGps", "postedAt": "2022-09-20T13:02:54.738Z", "postId": "DftyvLHrfGkKgJDp9", "htmlBody": "<p>Since animals share many similar biological structures with us and evolved similarly, it's relatively possible to make claims about their sentience by analogy to our own. Claims about AI sentience are far harder to verify. One could imagine the possibility of an AI that behaves as if sentient but isn't really sentient. This gives significantly more reason to be wary of just handing everything over to AI systems, even if you are a total hedonistic utilitarian.</p>\n<p>I also agree with others that building a sentient AI with a  positive inner life doesn't seem remotely easy.</p>\n", "parentCommentId": null, "user": {"username": "ThomasWoodside"}}, {"_id": "jFj29FaFdnA9PvQmu", "postedAt": "2022-09-20T17:43:49.739Z", "postId": "DftyvLHrfGkKgJDp9", "htmlBody": "<p>So two questions (please also see my reply to HjalmarWijk for context)::</p><ol><li>Do you on these grounds think that insect suffering (and everything more exotic) is meaningless? Because our last common ancestor with insects hardly have any neurons, and unsurprisingly our neuronal architecture is very different, so there isn't many reasons to expect any isomorphism between our \"mental\" processes.</li><li>&nbsp;Assuming an AI is sentient (in whatever sense you put into this word) but otherwise not meaningfully isomorphic to humans. How do you define \"positive\" inner life in that case?</li></ol>", "parentCommentId": "hDWcbyZi5ky5kEGps", "user": {"username": "Alex P"}}, {"_id": "JQMKxxEieuesKzzMu", "postedAt": "2022-09-20T18:58:50.700Z", "postId": "DftyvLHrfGkKgJDp9", "htmlBody": "<p>In philosophy of mind the theory of functionalism defines mental states as causal structures. So for example, pain is the thing that usually causes withdrawal, avoidance, yelping, etc. and is often caused by e.g. tissue damage. If you see pain as the \"tissue damage signaling\" causal structure, then you could imagine insects also having this as well, even if there is no isomorphism. It's hard to imagine AI systems having this, but you could more easily imagine AI systems having frustration, if you define it as \"inability to attain goals and realization that such goals are not attained\". The idea of an isomorphism is required by the theory of machine functionalism, which essentially states that two feelings are the same if they are basically the same Turing machine running. But humans could be said to be running many Turing machines, and besides no two humans are running the same Turing machine, and comparing states across two Turing machines doesn't really make sense. So I'm not very interested in this idea of strict isomorphism.</p><p>But I'm not fully onboard with functionalism of the more fuzzy/\"squishy\" kind either. I suppose something could have the same causal structures but not really \"feel\" anything. Maybe there is something to mind body materialism: for instance pain is merely a certain kind of neuron firing. In that case, we should have reason to doubt that insects suffer if they don't have those neurons. I certainly am one to doubt that insects suffer, but on the more functionalist flavor of thinking I don't. So I'm pretty agnostic. I'd imagine I might be similarly agnostic towards AI, and as such wouldn't be in favor of handing over the future to them and away from humans, just as I'm not in favor of handing over the future to insects.</p><p>To answer the second question, I think of this in a functionalist way, so if something performs the same causal effects as positive mental states in humans, that's a good reason to think it's positive.</p><p>For more I recommend <a href=\"https://askellio.substack.com/p/ai-consciousness\">Amanda Askell's blog post</a> or Jaegwon Kim's Philosophy of Mind textbook.</p>", "parentCommentId": "jFj29FaFdnA9PvQmu", "user": {"username": "ThomasWoodside"}}, {"_id": "vSLzJ5upyPY3p2ze8", "postedAt": "2022-09-20T20:47:12.278Z", "postId": "DftyvLHrfGkKgJDp9", "htmlBody": "<p>&gt;It's hard to imagine AI systems having this<br><br>Why? As per instrumental convergence, any advanced AI is likely to have self-preservation and a negative reward signal it would receive upon a violation of such drive would be functionally very similar to pain (give or take the bodily component, but I don't think it's required? Otherwise simulate a million human minds in agony is OK, and I assume we agree it's not). Likewise, any system with goal-directed agentic behavior would experience some reward from moving towards its goals, which seems functionally very similar to pleasure (or satisfaction or something along these lines).</p>", "parentCommentId": "JQMKxxEieuesKzzMu", "user": {"username": "Alex P"}}, {"_id": "2izwwGciJQ3iYxGDe", "postedAt": "2022-09-20T20:55:30.536Z", "postId": "DftyvLHrfGkKgJDp9", "htmlBody": "<p>Ok, so here's my take away from the answers so far:</p><p>Most flavors of utilitarianism (except for preference utilitarianism) don't consider any goal-having agent achieving those goals as utility. Instead there assumed to be some metric of similarity between the goals and/or mental states of the agent and those of humans, and the agent's achievement of its goals counts the less toward total utility the lower this similarity metric is, so completely alien agents achieving their alien goals and [non-]experiencing alien non-joy about it don't register as adding utility.</p><p>How exactly this metric should be formulated is disputed and fuzzy, and quite often a lot of this fuzziness and uncertainty is swept under the rug with the word \"sentience\" (or something similar) written on it.</p><p>Additionally, the proportion of EAs who would seriously consider \"all humans replaced by [particular kind of] AIs\" as an acceptable outcome may be not as trivial as I assumed.</p><p>Please let me know if I'm grossly misunderstanding or misrepresenting something, and thank you everyone for your explanations!</p>", "parentCommentId": null, "user": {"username": "Alex P"}}, {"_id": "ofeyPzAHxWtKDqTB6", "postedAt": "2022-09-20T20:57:57.340Z", "postId": "DftyvLHrfGkKgJDp9", "htmlBody": "<p>I just think anguish is more likely than physical pain. I suppose there could be physical pain in a distributed system as a result of certain nodes going down.</p>\n<p>It's actually not obvious to me that simulations of humans could have physical pain. Seems possible, but maybe only other orders of pain like anguish and frustration are possible.</p>\n", "parentCommentId": "vSLzJ5upyPY3p2ze8", "user": {"username": "ThomasWoodside"}}]