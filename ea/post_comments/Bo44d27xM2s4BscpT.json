[{"_id": "C8MyyEMccqQjLHxyX", "postedAt": "2024-03-20T15:11:35.680Z", "postId": "Bo44d27xM2s4BscpT", "htmlBody": "<p><strong>Executive summary:</strong> An AI system estimates a 30-40% probability that a future artificial superintelligence (ASI) would severely harm humanity, which represents a serious existential risk deserving of more attention and proactive effort.</p><p><strong>Key points:</strong></p><ol><li>The AI estimates a 70% probability of successfully developing ASI in the next 50-100 years.</li><li>It assigns a 60% probability that the ASI's values/goals would be misaligned with humanity's wellbeing, which could be catastrophic.</li><li>There is a 50% probability of failing to implement adequate AI safety measures and oversight.</li><li>An unaligned ASI has a 70% probability of overpowering humanity's defenses.</li><li>The 30-40% existential risk estimate has high uncertainty but warrants much more attention and effort to reduce the risk as close to 0% as possible.</li><li>The author finds the AI's estimate valuable, and believes that even a 1% existential risk from ASI is unacceptable.</li></ol><p>&nbsp;</p><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}]