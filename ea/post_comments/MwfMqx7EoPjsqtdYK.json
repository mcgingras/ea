[{"_id": "Ng7Di4Zx72kB7stu9", "postedAt": "2023-06-08T05:58:10.592Z", "postId": "MwfMqx7EoPjsqtdYK", "htmlBody": "<p>I strongly agree with you and, as someone who thinks alignment is extremely hard (not just because of the technical side of the problem, but due the human values side too), I believe that a hard pause and studying how to augment human intelligence is actually the best strategy.</p>", "parentCommentId": null, "user": {"username": "Patricio"}}, {"_id": "tWAJqiDo8f4RqC5KE", "postedAt": "2023-06-08T14:10:42.489Z", "postId": "MwfMqx7EoPjsqtdYK", "htmlBody": "<p><a href=\"https://twitter.com/ch402/status/1666482929772666880\">See also Anthropic's view on this&nbsp;</a></p><blockquote><p>[seeing] a lot of safety research as \"eating marginal probability\" of things going well, progressively addressing harder and harder safety scenarios.</p></blockquote><p>The implicit strat (which Olah may not endorse) is to try to solve easy bits, then move on to harder bits, then note the rate you are progressing at and get a sense of how hard things are that way.&nbsp;</p><p>This would be fine if we could be sure we actually were solving the problems, and also not fooling ourselves about the current difficulty level, and if the relevant research landscape is smooth and not blockable by a single missing piece.&nbsp;</p>", "parentCommentId": null, "user": {"username": "technicalities"}}, {"_id": "5bsMwLzciaepLkZk6", "postedAt": "2023-06-08T20:37:44.758Z", "postId": "MwfMqx7EoPjsqtdYK", "htmlBody": "<p>I agree the implicit strat here doesn\u2019t seem like it\u2019ll make progress on knowing whether the hard problems are real. Lots of the hard problems (generalising well ood, existence of sharp left turns) just don\u2019t seem very related to the easier problems (like making LLMs say nice things), and unless you\u2019re explicitly looking for evidence of hard problems I think you\u2019ll be able to solve the easier problems in ways that won\u2019t generalise (e.g. hammering LLMs with enough human supervision in ways that aren\u2019t scalable, but are sufficient to \u2018align\u2019 it).</p>", "parentCommentId": "tWAJqiDo8f4RqC5KE", "user": {"username": "Aron"}}]