[{"_id": "rKEkHtvtTBkTcHWPu", "postedAt": "2023-10-04T12:38:29.792Z", "postId": "76r25fSByRiNa7Wos", "htmlBody": "<blockquote><p>Identify and validate better methods of eliciting low-probability forecasts</p></blockquote><p><br>I think this is important work, so I\u2019m glad to hear that it\u2019s a priority.</p><p>It\u2019s a two-pronged approach, right? Measuring how reliable forecasters are at working with small probabilities, and using better elicitation measures to reduce the size of any error effect</p><p>I suspect that when&nbsp;<i>measuring how reliable forecasters are at working with small probabilities&nbsp;</i>you\u2019ll find a broad range of reliability. It would be interesting to see how the XPT forecasts change if you&nbsp;<i>exclude&nbsp;</i>those with poor small-probability understanding, or if you weight each response according to the forecaster\u2019s aptitude.</p><p>&nbsp;</p><p>Using comparative judgements seems like a good avenue for exploration. Have you thought about any of the following?</p><ul><li>Using \u201c1-in-x\u201d style probabilities instead of x%. This might be a more \u201cnatural\u201d way of thinking about small probabilities</li><li>Eliciting probabilities by steps: first get respondents to give the order of magnitude (is your prediction between 1-in-10 and 1-in-100 or between 1-in-100 and 1-in-1000 or\u2026), then have them narrow down further. This is still more abstract than the \u201cstruck by lightning\u201d idea, but does not rely on the respondent\u2019s level of lightning-strike knowledge</li><li>Giving respondents sense-checks on the answers they have given before, and an opportunity to amend their answers: \u201cyour estimate of X-risk through Bio is 1% and your estimate of X-risk through natural pandemics is 0.8%, so you think 80% of the x-risk from Bio comes from natural pandemics\u201d</li></ul><p>One more thing:</p><blockquote><p>If we are not sure whether forecasters can tell 0.001% apart from 0.000001% (a magnitude difference of 1,000x), then we should treat a 0.000001% forecast of a catastrophic risk as if it were 0.001% and be much more cautious about potential dangers.</p></blockquote><p>In theory, yes, but I think people are generally much more likely to say 0.001% when their \u201ctrue\u201d probability is 0.000001% than vice versa - maybe because we very rarely think about events of the order of 0.000001%, so 0.001% seems to cover the most unlikely events.</p><p>You might counter that we just need a small proportion of respondents to <i>say 0.00001% when their \u201ctrue\u201d probability is 0.001%</i> &nbsp;to risk undervaluing important risks. But not if we are using medians, as the XPT does.</p><p>I could be wrong on the above, but my take is that understanding the likely&nbsp;<i>direction</i> of errors in the \u201c0.001% vs 0.000001%\u201d scenarios maybe ought to be a priority.</p>", "parentCommentId": null, "user": {"username": "Stan Pinsent"}}, {"_id": "LbYPBLCmQ2uq3ffc8", "postedAt": "2023-10-07T15:05:37.261Z", "postId": "76r25fSByRiNa7Wos", "htmlBody": "<p>Thanks for sharing!</p><p>I think it would be nice to have experts with a more diverse background. From the <a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64abffe3f024747dd0e38d71/1688993798938/XPT.pdf\">report</a>:</p><blockquote><p>The sample drew heavily from the Effective Altruism (EA) community: about 42% of experts and 9% of superforecasters reported that they had attended an EA meetup.</p></blockquote>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}]