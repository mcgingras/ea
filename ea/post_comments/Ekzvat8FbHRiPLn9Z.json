[{"_id": "ixRQDnri9NN4gdMjn", "postedAt": "2018-06-30T18:44:46.755Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>I appreciate this model and feel like it has the potential to be foundational in a rigorous account of group rationality.</p>\n<p>This especially because I find that the higher levels lack a proper feedback mechanism. A common pattern seems to be that we often discuss high-level moral philosophy without making any hard decisions on which philosophy to employ. I even use \u201cso have we fixed morality yet?\u201d As a joke. People laugh, not because I pretend that it\u2019s easy, but because I pretend that making a decision is the point. I suspect that this lack of pressure leads to impoverished thinking.</p>\n<p>Imagine a world where an institution strived to deliver solutions to moral problems as an input to another institution that further carried it out. I\u2019d expect the sense of responsibility to lead to much better thinking. No more belief as attire.</p>\n<p>But then maybe this is already happening in places I haven\u2019t been. Still, I\u2019d love to see more people take responsibility for providing <em>workable</em> answers to philosophy. Even when they\u2019re just chatting at social events.</p>\n", "parentCommentId": null, "user": {"username": "toonalfrink"}}, {"_id": "2nHyJAJuSoGEmTNFD", "postedAt": "2018-07-01T12:33:09.675Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>I really liked this post and the model you've introduced!</p>\n<p>With regards to your pseudomaths, a minor suggestion could be that your product notation is equal to how agentive our actor is. This could allow us to take into account impact that is negative (i.e., harmful processes) by then multiplying the product notation by another factor that takes into account the sign of the action. Then the change in impact could be proportional to the product of these two terms. </p>\n", "parentCommentId": null, "user": {"username": "Eli_Nathan"}}, {"_id": "Cwkma9hjBid9WY45c", "postedAt": "2018-07-01T12:53:47.258Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>I'm happy to hear that it's useful for you. :-) </p>\n<p>Could you clarify what you mean with agentive? \nThe way I see it, at any of the levels from 'Values' to 'Actions', a person's position on the corrigibility scale could be so low to be negative. But it's not an elegant or satisfactory way of modelling it (i.e. different ways of adjusting poorly to evidence could still lead to divergent results from an extremely negative Unilateralist's Curse scenario to just sheer mediocrity)</p>\n", "parentCommentId": "2nHyJAJuSoGEmTNFD", "user": {"username": "remmelt"}}, {"_id": "L7DCT5j263WPeaor4", "postedAt": "2018-07-01T13:16:52.827Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>By agentive I sort of meant &quot;how effectively an agent is able to execute actions in accordance with their goals and values&quot; - which seems to be independent of their values/how aligned they are with doing the most good.</p>\n<p>I think this is a different scenario to the agent causing harm due to negative corrigibility (though I agree with your point about how this could be taken into account with your model).</p>\n<p>It seems possible however that you could incorporate their values/alignment into corrigibility depending on one's meta-ethical stance.</p>\n", "parentCommentId": "Cwkma9hjBid9WY45c", "user": {"username": "Eli_Nathan"}}, {"_id": "YZPxL6FLeHyRC4dmG", "postedAt": "2018-07-01T16:35:55.427Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Good stuff! You might be interested in both OODA loops and Marr's levels of analysis. </p>\n", "parentCommentId": null, "user": {"username": "RomeoStevens"}}, {"_id": "9hHS8vq8WqjBrKins", "postedAt": "2018-07-01T21:29:43.841Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Ah, in this model, I see \u2018effectiveness in executing actions according to values\u2019 a result of lots of directed iteration of improving understanding at lower construal levels over time (reminds of the OODA loop that Romeo mentions above, will also look into the \u2018levels of analysis\u2019 now ). In my view, that doesn\u2019t require an extra factor. </p>\n<p>Which meta-ethical stance do you think this wouldn\u2019t fit into the model? I\u2019m curious to hear your thoughts to see where it fails to work. </p>\n", "parentCommentId": "L7DCT5j263WPeaor4", "user": {"username": "remmelt"}}, {"_id": "AHWeGHppz6uJq7RSu", "postedAt": "2018-07-01T21:38:50.890Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Thanks for the pointers! </p>\n<p>Would you see OODA loops translated to V2ADC as cycling up and down (parts of) the chain as quickly as possible?</p>\n<p>I found this article on Marr\u2019s levels of analysis: <a href=\"http://blog.shakirm.com/2013/04/marrs-levels-of-analysis/\">http://blog.shakirm.com/2013/04/marrs-levels-of-analysis/</a>\nSeems like a useful way of guiding the creation of algorithms (never heard of it before \u2013 I don\u2019t know much about of coding or AI frameworks). </p>\n", "parentCommentId": "YZPxL6FLeHyRC4dmG", "user": {"username": "remmelt"}}, {"_id": "6APE6YA8qeyf7727Z", "postedAt": "2018-07-01T23:39:28.716Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Do you know of a tax-deductible way to support you as a UK donor?</p>\n", "parentCommentId": null, "user": null}, {"_id": "eGWYq44PzLhJzCXbv", "postedAt": "2018-07-02T04:03:41.990Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Good question... I haven\u2019t really thought about it but if it\u2019s a \u00a320,000+ donation perhaps EA Netherlands could register at the HMRC?\n<a href=\"https://www.givingwhatwecan.org/post/2014/06/tax-efficient-giving-guide-uk-donors/\">https://www.givingwhatwecan.org/post/2014/06/tax-efficient-giving-guide-uk-donors/</a></p>\n", "parentCommentId": "6APE6YA8qeyf7727Z", "user": {"username": "remmelt"}}, {"_id": "fKgA8NqhZuNFfP3rW", "postedAt": "2018-07-03T15:40:26.649Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Ah okay - I think I understand you, but this is entering areas where I become more confused and have little knowledge.</p>\n<p>I'm also a bit lost as to what I meant by my latter point, so will think about it some more if possible.</p>\n", "parentCommentId": "9hHS8vq8WqjBrKins", "user": {"username": "Eli_Nathan"}}, {"_id": "QGhEn35zG8ajEg8tR", "postedAt": "2018-07-03T17:30:06.566Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Maybe via EA Grants?</p>\n", "parentCommentId": "6APE6YA8qeyf7727Z", "user": {"username": "Peter_Hurford"}}, {"_id": "eQCC9n7iCPFY6hW4g", "postedAt": "2018-07-03T23:38:20.350Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Excellent work. I hope you'll forgive me taking issue with a smaller point:</p>\n<blockquote>\n<p>Given the uncertainty they are facing, most of OpenPhil's charity recommendations and CEA's community-building policies <em>should be overturned or radically altered in the next few decades</em>. That is, if they actually discover their mistakes. This means it's crucial for them to encourage more people to do local, contained experiments and then integrate their results into more accurate models. (my emphasis)</p>\n</blockquote>\n<p>I'm not so sure that this is true, although it depends on how big an area you imagine will / should be 'overturned'. This also somewhat ties into the discussion about how likely we should expect to be missing a 'cause X'.</p>\n<p>If cause X is another entire cause area, I'd be pretty surprised to see a new one in (say) 10 years which is similar to animals or global health, and even more surprised to see one that supplants long term future. My rationale for this is I see broad funnel where EAs tend to move into the long term future/x-risk/AI, and once there they tend not to leave (I can think of a fair number of people who made the move from (e.g.) global health --&gt; far future, but I'm not aware of anyone who moved from far future --&gt; anything else). There are also people who have been toiling in the long term future vinyard for a long time (e.g. MIRI), and the fact we do not see many people moving elsewhere suggests this is pretty stable attractor.</p>\n<p>There are other reasons for a cause area being a stable attractor besides all reasonable roads lead to it. That said, I'd suggest one can point to general principles which would somewhat favour this (e.g. the scope of the long term future, that the light cone commons, stewarded well, permits mature moral action in the universe to whatever in fact has most value, etc.) I'd say similar points to a lesser degree to apply to the broad landscape of 'on reflection moral commitments', and so the existing cause areas mostly exhaust this moral landscape.</p>\n<p>Naturally, I wouldn't want to bet the farm on what might prove overconfidence, but insofar as it goes it supplies less impetus for lots of exploratory work of this type. At a finer level of granulariy (and so a bit further down your diagram), I see less resilience (e.g. maybe we should tilt the existing global poverty portfolio more one way or the other depending how the cash transfer literature turns out, maybe we should add more 'avoid great power conflict' to the long term future cause area, etc.) Yet I still struggle to see this adding up to radical alteration. </p>\n", "parentCommentId": null, "user": {"username": "Gregory_Lewis"}}, {"_id": "A864oERYj9LN8jsCu", "postedAt": "2018-07-04T06:24:17.564Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>@Peter, any idea how EA Grants could be used as an intermediary here?\n(I did apply myself to EA Grants but I\u2019m not expecting to cover the financial runway of myself or EAN for any longer than 6 months with that)</p>\n", "parentCommentId": "QGhEn35zG8ajEg8tR", "user": {"username": "remmelt"}}, {"_id": "zyYs4QTaH4gbrxb5D", "postedAt": "2018-07-04T06:57:57.519Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>I appreciate you mentioning this! It\u2019s probably not a minor point because if taken seriously, it should make me a lot less worried about people in the community getting stuck in ideologies. </p>\n<p>I admit I haven\u2019t thought this through systematically. Let me mull over your arguments and come back to you here. </p>\n<p>BTW, could you perhaps explain what you meant with the \u201cThere are other causes of an area...\u201d sentence? I\u2019m having trouble understanding that bit. </p>\n<p>And with \u2018on-reflection moral commitments\u2019 do you mean considerations like population ethics and trade-offs between eudaimonia and suffering?</p>\n", "parentCommentId": "eQCC9n7iCPFY6hW4g", "user": {"username": "remmelt"}}, {"_id": "CkrNfYi9smm6gr79F", "postedAt": "2018-07-04T10:14:18.300Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Sorry for being unclear. I've changed the sentence to (hopefully) make it clearer. The idea was there could be other explanations for why people tend to gravitate to future stuff (group think, information cascades, selection effects) besides the balance of reason weighs on its side.</p>\n<p>I do mean considerations like population ethics etc. for the second thing. :)</p>\n", "parentCommentId": "zyYs4QTaH4gbrxb5D", "user": {"username": "Gregory_Lewis"}}, {"_id": "22NGTaihHgBArPbtC", "postedAt": "2018-07-04T14:13:17.549Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>I like the model a lot, thanks for posting!</p>\n<p>One input: I think it could be useful to find a term for it that's easier to memorize (and has a shorter abbreviation).</p>\n", "parentCommentId": null, "user": {"username": "Jonas Vollmer"}}, {"_id": "p6jTb87kXf73L6y7Q", "postedAt": "2018-07-04T14:53:37.222Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>I think you're making some valuable points here (e.g. making sure information is properly implemented into the 'higher levels') but I think your posts would have been a lot better if had skipped all the complicated modelling and difficult language. It strikes me as superfluous and the main result seems to me that it makes your post harder to read without adding any content.</p>\n", "parentCommentId": null, "user": {"username": "Denise_Melchin"}}, {"_id": "DuBA9KMZr85QWobiK", "postedAt": "2018-07-04T15:51:24.320Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Or add a tl;dr</p>\n", "parentCommentId": "p6jTb87kXf73L6y7Q", "user": null}, {"_id": "doDY4dyXQkW55L4Nk", "postedAt": "2018-07-04T15:54:25.540Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Hi Denise, can you give some examples of superfluous language? I tried to explain it as simply as possible (though sometimes jargon and links are needed to avoid having to explain concepts in long paragraphs)  but I\u2019m sure I still made it too complicated in places. </p>\n", "parentCommentId": "p6jTb87kXf73L6y7Q", "user": {"username": "remmelt"}}, {"_id": "QniCf6LxcwGrC4JWf", "postedAt": "2018-07-04T15:56:43.826Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Hmm, I can\u2019t think of a clear alternative to \u2018V2ADC\u2019 yet. Perhaps \u2018decision chain\u2019?</p>\n", "parentCommentId": "22NGTaihHgBArPbtC", "user": {"username": "remmelt"}}, {"_id": "ZrdcjdSPcwMQBtMzs", "postedAt": "2018-07-04T17:06:10.211Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Yeah, that sounds great. Decision chain (abbreviated &quot;DC&quot;).</p>\n", "parentCommentId": "QniCf6LxcwGrC4JWf", "user": {"username": "Jonas Vollmer"}}, {"_id": "wDJ7J8nRHjsnopf9T", "postedAt": "2018-07-04T20:33:05.592Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Hmm, I personally value say five people deeply understanding the model to be able to explore and criticise it over say a hundred people skimming through a tl;dr. This is why I didn\u2019t write one (besides it being hard to summarise anything more than \u2018construal levels matter \u2013 you should consider them in the interactions you have with others\u2019, which I basically do in the first two paragraphs). I might be wrong of course because you\u2019re the second person who suggested this. </p>\n<p>This post might seem deceptively obvious. However, I put a lot of thinking into both refining categories and the connections between them and explaining them in a way that hopefully enables someone to master them intuitively if they take the time to actively engage with the text and diagrams. I probably did make a mistake by outlining both the model and its implications in the same post because it makes it unclear what it\u2019s about and causes discussions here in the comment section to be more diffuse (Owen Cotton-Barratt mentioned this to me). </p>\n<p>If someone prefers to not read the entire post, that\u2019s fine. :-)</p>\n", "parentCommentId": "DuBA9KMZr85QWobiK", "user": {"username": "remmelt"}}, {"_id": "omzxTHMj9bG86ausC", "postedAt": "2018-07-04T21:55:31.829Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>EA UK (the organisation behind EA London) could potentially help for large enough donations, but I'd need to check and it would take a bit of time to administer. I was just wondering if there was an existing mechanism.</p>\n", "parentCommentId": "QGhEn35zG8ajEg8tR", "user": null}, {"_id": "W3zmSPhKQ5uhbPqx9", "postedAt": "2018-07-05T04:42:16.245Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>I was just speculating that maybe EA Grants could help match up individual donors with projects by having the donor make a tax deductible donation to CEA and CEA passing it along as an EA Grant. I don't know if this actually could work and I now see several reasons why maybe it wouldn't.</p>\n", "parentCommentId": "A864oERYj9LN8jsCu", "user": {"username": "Peter_Hurford"}}, {"_id": "hNApt7ucq3zRzqeyz", "postedAt": "2018-07-05T06:47:10.805Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Changed it in the third paragraph. :-)</p>\n", "parentCommentId": "ZrdcjdSPcwMQBtMzs", "user": {"username": "remmelt"}}, {"_id": "euuCq6pzS9muFP8rm", "postedAt": "2018-07-08T19:55:31.516Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>It is still not clear to me how your model is different to what EAs usually call different levels of meta. What is it adding? Using words like 'construal level' complicates the matter further.</p>\n<p>I'm happy to elaborate more via PM if you like.</p>\n", "parentCommentId": "doDY4dyXQkW55L4Nk", "user": {"username": "Denise_Melchin"}}, {"_id": "c5tgp4ZFBkZrZ9Abb", "postedAt": "2018-07-08T21:36:32.847Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Hmm, so here are my thoughts on this:</p>\n<p>1) I think you\u2019re right that the idea of going meta from the object level is an idea that\u2019s known to many EAs. I\u2019d argue though that the categorisations in the diagram are valuable though because I don\u2019t know of any previous article where they\u2019ve all been put together. For veteran EAs, they\u2019ll probably be obvious but I still think it\u2019s useful for them to make the implicit explicit. </p>\n<p>2) The idea of construal levels is useful here because of how thinking in <a href=\"https://wiki.lesswrong.com/wiki/Near/far_thinking\">far vs. near mode</a> affects psychology. E.g. when people think in far mode they</p>\n<ul>\n<li><p>have to ignore details, and tend to be less aware that those nuances actually exist</p>\n</li>\n<li><p>tend to associate other far-mode things with whatever they think of. E.g. Robin Hanson\u2019s point that many sci-fi/futurism books (except, of course, Age of Em) focus on values and broad populations of beings that all look similar, and have blue book covers (i.e. sky, far away)</p>\n</li>\n</ul>\n<p>So this is why I think referring to construal levels adds value. Come to think of it, I should have mentioned this in the post somewhere. Also my understanding of construal level theory is shoddy so would love to hear opinions of someone who\u2019s read more into it. </p>\n<p>BTW, my sister mentioned that I could have made the post a lot more understandable for her if I just started with \u2018Some considerations like X are more concrete and other considerations like Y are more abstract. Here are some considerations in between those.\u2019\nJudging by, that I could have definitely written it more clearly. </p>\n", "parentCommentId": "euuCq6pzS9muFP8rm", "user": {"username": "remmelt"}}, {"_id": "cp7dcPBet6dLQmfst", "postedAt": "2018-07-09T06:27:31.224Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>I've added some interesting links to the post on near vs. far mode thinking, which I found on LessWrong and Overcoming Bias.</p>\n", "parentCommentId": "c5tgp4ZFBkZrZ9Abb", "user": {"username": "remmelt"}}, {"_id": "CfArgKZEGx28rMe2J", "postedAt": "2018-07-10T12:19:26.215Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>First off, I was ambiguous in that paragraph about the level I actually thought decisions should be revised or radically altered. i.e. in say the next 20 years, did I think OpenPhil should revise most of the charities they fund, most of the specific problems they funded or broad focus areas? I think I ended up just expressing a vague sense of \u2018they should change their decisions a lot if they put in much more of the community\u2019s brainpower into analysing data from a granular level upwards\u2019.</p>\n<p>So I appreciate that you actually gave specific reasons for why you'd be surprised to see a new focus area being taken up by people in the EA community in the next 10 years! Your arguments make sense to me and I\u2019m just going to take up your opinion here. </p>\n<p>Interestingly, your interpretation that this is evidence for that there shouldn't be a radical alteration in what causes we focus can be seen both as an outside view and inside view. It's an outside view in the sense that it weights the views of people who've decided to move into the direction of working on the long term future. It's also an inside view in that it doesn't consider roughly what percentage of past cosmopolitan movements where members converged on working on a particular set of problems were seen as wrong by their successors decades later (and perhaps judged to have been blinded by some of the social dynamics you mentioned: groupthink, information cascades and selection effects).</p>\n<p>A historical example where this went wrong is how in the 1920's Bertrand Russell and other contemporary intelligentia had positive views on communism and eugenics, which later failed in practice under Stalin's authoritarian regime and Nazi Germany, respectively. Although I haven't done a survey of other historical movements (has anyone compiled such a list?), I think I still feel slightly more confident than you that we'll radically alter what we'll work on after 20 years if we'd make a concerted effort now to structure the community around enabling a significant portion of our 'members' (say 30%) to work together to gather, analyse and integrate data at each level (whatever that means). </p>\n<p>It does seems that we share some intuitions (e.g. the arguments for valuing future generations similarly to current generations seem solid to me). I've made a quick list on research that could lead to fundamental changes in what we prioritise at various levels. I'd be curious to hear if any of these points has caused you to update any of your other intuitions:</p>\n<p><strong>Worldviews</strong></p>\n<ul>\n<li><p>more neuroscience and qualia research, possibly causing fundamental shifts in our views on how we feel and register experiences</p>\n</li>\n<li><p>research into how different humans trade off suffering and eudaimonia differently</p>\n</li>\n<li><p>a much more nuanced understanding of what psychological needs and cognitive processes lead to moral judgements (e.g. the effect on psychological distance on deontologist vs. consequentialist judgements and scope sensitivity)</p>\n</li>\n</ul>\n<p><strong>Focus areas:</strong></p>\n<p><em>Global poverty</em> </p>\n<ul>\n<li><p>use of better metrics for wellbeing \u2013 e.g. life satisfaction scores and future use of real-time tracking of experiential well-being \u2013 that would result in certain interventions (e.g. in mental health) being ranked higher than others (e.g. malaria)</p>\n</li>\n<li><p>use of better approaches to estimate environmental interactions and indirect effects, like complexity science tools, which could result in more work being done on changing larger systems through leverage points</p>\n</li>\n</ul>\n<p><em>Existential risk</em></p>\n<ul>\n<li><p>more research on how to avoid evolutionary/game theoretical \u201cMoloch\u201d dynamics instead of the current &quot;Maxipok&quot; focus on ensuring that future generations will live and hope that they have more information to assess and deal with problems from there</p>\n</li>\n<li><p>for AI safety specifically, I could see a shift in focus from a single agent produced out of say a lab that presumably gets so powerful to outflank all other agents to analysing systems of more similarly capable agents owned by wealthy individuals and coalitions that interact with each other (e.g. like Robin Hanson's work on Ems) or perhaps more research on how a single agent could be made out of specialised sub-agents representing the interests of various beings. I could also see a shift in focus to assessing and ensuring the welfare of sentient algorithms themselves. </p>\n</li>\n</ul>\n<p><em>Animal welfare</em></p>\n<ul>\n<li><p>more research on assessing sentience, including that of certain insects, plants and colonial ciliates that do more complex information processing, leading to changed views on what species to target</p>\n</li>\n<li><p>shift to working on wild animal welfare and ecosystem design, with more focus on marine ecosystems</p>\n</li>\n</ul>\n<p><em>Community building</em></p>\n<ul>\n<li><p>Some concepts like high-fidelity spreading of ideas and strongly valuing honesty and considerateness seem robust</p>\n</li>\n<li><p>However, you could see changes like emphasising the integration of local data, the use of (shared) decision-making algorithms and a shift away from local events and coffee chats to interactions on online (virtual) platforms</p>\n</li>\n</ul>\n", "parentCommentId": "eQCC9n7iCPFY6hW4g", "user": {"username": "remmelt"}}, {"_id": "hiZka5Yj8od35toFD", "postedAt": "2018-07-10T21:06:14.358Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Fantastic post!  It's a significant upgrade from the &quot;terminal/instrumental values&quot; mental model I was previously using.</p>\n<p>When I first joined EA, I looked at the annual survey of EAs and was surprised to see so much variation in how EAs ranked the importance of the major causes.  I thought that the group would be moving towards a consensus, and that each individual member  would be able to trace their actions up towards their understanding of the most important causes.</p>\n<p>Personally, I tried to build up my own understanding of the cause priority from strong foundations, doing my best to answer meta questions like &quot;do I value all people equally&quot;, &quot;how do I weight animal suffering vs human happiness&quot;.  From there, I worked my way down the V2ADC, trying to meta-analyze the research on causes, eventually coming to an area that I felt confident was the best place to add value.</p>\n<p>I think with a bit more nuance, the EA survey could serve as a good feedback mechanism to see where on the chain we all see ourselves, and to see if the sum of the parts adds up to anything resembling a consistent whole.  Will the EA community end up converging in beliefs and strategy?  Is it an elephant in the room to say that half of the people working on X cause aught to shift to Y cause because the people up the chain are confident that it is a better move for the community?  Even if the exploratory folks at the bottom raised their evidence up the chain, would we have enough corrigibility to pivot?  (Love that word, totally gonna use it more!)</p>\n", "parentCommentId": null, "user": {"username": "Naryan"}}, {"_id": "BmNdJDePp3bz8zpqF", "postedAt": "2018-07-11T05:20:37.353Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>Hi @Naryan,</p>\n<p>I\u2019m glad that this is a more powerful tool for you. </p>\n<p>And kudos for working things from the foundations up! Personally, I still need to take a few hours with a pen and paper to systematically work myself through the decision chain myself. A friend has been nudging me to do that. :-)</p>\n<p>Gregory Lewis makes the argument above that some EAs are moving in the direction of working on long term future work and few are moving back out. I\u2019m inclined to agree with him that they probably have good reasons for that. </p>\n<p>I\u2019d also love to see the results of some far mode \u2014 near mode questions put in the EA Survey or perhaps send out by Spencer Greenberg (not sure if there\u2019s an existing psychological scale to gauge how much people are in each mode when working throughout the day). And of course, how they corellate with cause area preferences. </p>\n<p>Max Dalton explained to me how \u2018corrigiblity\u2019 was one of the most important traits to look for for selecting people you want to work with at EA Global London last year so credit to him. :-) My contribution here is adding the distinction that people often seem more corrigible at some levels than others, especially when they\u2019re new to the community. </p>\n<p>(also, I love that sentence \u2013 \u201cif the exploratory folks at the bottom raised evidence up the chain...\u201d)</p>\n", "parentCommentId": "hiZka5Yj8od35toFD", "user": {"username": "remmelt"}}, {"_id": "tqn86XwaR6wMKHqwu", "postedAt": "2018-07-11T06:14:39.291Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>I agree history generally augurs poorly for those who claim to know (and shape) the future. Although there are contrasting positive examples one can give (e.g. the moral judgements of the early Utilitarians were often ahead of their time re. the moral status of women, sexual minorities, and animals), I'm not aware of a good macrohistorical dataset that could answer this question - reality in any case may prove underpowered.</p>\n<p>Yet whether or not in fact things would change with more democratised decision-making/intelligence gathering/ etc., it remains an open question whether this would be a better approach. Intellectual progress in many areas is no longer an amateur sport (see academia, cf. ongoing professionalisation of many 'bits' of EA, see generally that many important intellectual breakthroughs have historically been made by lone figures or small groups versus more swarm-intelligence-esque methods), and there's a 'clownside' risk of lot of enthusiastic, well-meaning, but inexperienced people making attempts that add epistemic heat rather than light (inter alia). The bar to appreciate 'X is an important issue' may be much lower than 'can contribute usefully to X'. </p>\n<p>A lot seems to turn on whether the relevant problems are more high serial depth (favouring intensive effort) high threshold (favouring potentially-rare ability) or broader and relatively shallower, favouring parallelization. I'd guess relevant 'EA open problems' are a mix, but this makes me hesitant for there to be a general shove in this direction.   </p>\n<p>I have mixed impressions about the items you give below (which I appreciate was meant more as quick illustration than some 'research agenda for the most important open problems in EA'). Some I hold resilient confidence the underlying claim is false, for more I am uncertain yet I suspect progress on answering these questions (/feel we could punt on these for our descendants to figure out in the long reflection). In essence, my forecast is that this work would expectedly tilt the portfolios, but not so much to be (what I would call) a 'cause X' (e.g. I can imagine getting evidence which suggests we should push more of a global health portfolio to mental health - or non-communicable disease - but not something as decisive where we think we should sink the entire portfolio there and withdraw from AMF/SCI/etc.)  </p>\n", "parentCommentId": "CfArgKZEGx28rMe2J", "user": {"username": "Gregory_Lewis"}}, {"_id": "eCpuWuiYD3SeJtu9H", "postedAt": "2018-07-12T18:22:26.700Z", "postId": "Ekzvat8FbHRiPLn9Z", "htmlBody": "<p>For me tl;dr is mainly useful for two things:</p>\n<ul>\n<li>to let me evaluate whether I think the post is interesting enough, or contains enough new information for me to be worth reading</li>\n<li>to jog my memory when I come back to a post a while after I have read it, and am no longer sure what it is about/what its main points are</li>\n</ul>\n", "parentCommentId": "wDJ7J8nRHjsnopf9T", "user": {"username": "ZeitPolizei"}}]