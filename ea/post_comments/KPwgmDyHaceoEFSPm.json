[{"_id": "dTnDBDfoNHNqWZeRP", "postedAt": "2018-06-23T15:29:17.183Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<p>In <a href=\"https://futureoflife.org/2017/07/31/transcript-art-predicting/\">this</a> FLI podcast episode, Andrew Critch suggested handling a potentially dangerous idea like a software update rollout procedure, in which the update is distributed gradually rather than to all customers at once:</p>\n<blockquote>\n<p>... I would tell you the same thing I would tell anyone who discovers a potentially dangerous idea, which is not to write a blog post about it right away.</p>\n</blockquote>\n<blockquote>\n<p>I would say, find three close, trusted individuals that you think reason well about human extinction risk, and ask them to think about the consequences and who to tell next. Make sure you\u2019re fair-minded about it. Make sure that you don\u2019t underestimate the intelligence of other people and assume that they\u2019ll never make this prediction</p>\n</blockquote>\n<blockquote>\n<p>...</p>\n</blockquote>\n<blockquote>\n<p>Then do a rollout procedure. In software engineering, you developed a new feature for your software, but it could crash the whole network. It could wreck a bunch of user experiences, so you just give it to a few users and see what they think, and you slowly roll it out. I think a slow rollout procedure is the same thing you should do with any dangerous idea, any potentially dangerous idea. You might not even know the idea is dangerous. You may have developed something that only seems plausibly likely to be a civilizational scale threat, but if you zoom out and look at the world, and you imagine all the humans coming up with ideas that could be civilizational scale threats. </p>\n</blockquote>\n<blockquote>\n<p>...</p>\n</blockquote>\n<blockquote>\n<p>If you just think you\u2019ve got a small chance of causing human extinction, go ahead, be a little bit worried. Tell your friends to be a little bit worried with you for like a day or three. Then expand your circle a little bit. See if they can see problems with the idea, see dangers with the idea, and slowly expand, roll out the idea into an expanding circle of responsible people until such time as it becomes clear that the idea is not dangerous, or you manage to figure out in what way it\u2019s dangerous and what to do about it, because it\u2019s quite hard to figure out something as complicated as how to manage a human extinction risk all by yourself or even by a team of three or maybe even ten people. You have to expand your circle of trust, but, at the same time, you can do it methodically like a software rollout, until you come up with a good plan for managing it. As for what the plan will be, I don\u2019t know. That\u2019s why I need you guys to do your slow rollout and figure it out.</p>\n</blockquote>\n", "parentCommentId": null, "user": {"username": "ofer"}}, {"_id": "oATNzkj4dkLdAr7ds", "postedAt": "2018-06-23T16:34:14.740Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<p>The relevance of unilateralist's curse dynamics to info hazards is important and worth mentioning here. Even if you independently do a thorough analysis and decide that the info-benefits outweigh the info-hazards of publishing a particular piece of information, that shouldn't be considered sufficient to justify publication. At the very least, you should privately discuss with several others and see if you can reach a consensus.</p>\n", "parentCommentId": null, "user": {"username": "brianwang712"}}, {"_id": "Jrio62o8HoBvKEMDK", "postedAt": "2018-06-23T19:15:00.631Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<p>That is absolutely right, and I am always discussing ideas with friends and advanced specialist before discussing them publicly. But doing this, I discovered two obstacles:</p>\n<p>1) If the idea is really simple, it is likely not new, but in case of a complex idea not much people are able to properly evaluate it. Maybe if Bostrom will spend a few days analysing it, he will say &quot;yes&quot; or &quot;no&quot;, but typically best thinkers are very busy with their own deadlines, and will not have time to evaluate the ideas of random people. So you are limited to your closer friends, who could be biased in favour of you and ignore the info-hazard.</p>\n<p>2) &quot;False negatives&quot;. This is the situation when a person thinks that the idea X is not an informational hazard because it is false. However, the reasons why he thinks that the idea X is false are wrong. In that situation, the info hazard assessment is not happening.</p>\n", "parentCommentId": "dTnDBDfoNHNqWZeRP", "user": {"username": "turchin"}}, {"_id": "YBRo6bgGwJB7CPEbj", "postedAt": "2018-06-23T21:12:57.343Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<p>The unilateralists curse only applies if you expect other people to have the same information as you right?</p>\n<p>You can figure out if they have the same information as you to see if they are concerned about the same things you are. By looking at the mitigation's people are attempting. Altruists  should be attempting mitigations in a unilateralist's curse position, because they should expect someone less cautious than them to unleash the information. Or they want to unleash the information themselves and are mitigating the downsides until they think it is safe.</p>\n<blockquote>\n<p> At the very least, you should privately discuss with several others and see if you can reach a consensus.</p>\n</blockquote>\n<p>I've not had the best luck reaching out to talk to people about my ideas. I expect that the majority of new ideas will come from people not heavily inside the group and thus less influenced by group think. So you might want to think of solutions that take that into consideration.</p>\n", "parentCommentId": "oATNzkj4dkLdAr7ds", "user": {"username": "WillPearson"}}, {"_id": "MAjeRA6zx3hzDTixt", "postedAt": "2018-06-24T02:18:10.408Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<blockquote>\n<p>The unilateralists curse only applies if you expect other people to have the same information as you right?</p>\n</blockquote>\n<p>My understanding is that it applies regardless of whether or not you expect others to have the same information. All it requires is a number of actors making independent decisions, with randomly distributed error, with a unilaterally made decision having potentially negative consequences for all.</p>\n<blockquote>\n<p>You can figure out if they have the same information as you to see if they are concerned about the same things you are. By looking at the mitigation's people are attempting. Altruists should be attempting mitigations in a unilateralist's curse position, because they should expect someone less cautious than them to unleash the information. Or they want to unleash the information themselves and are mitigating the downsides until they think it is safe.</p>\n</blockquote>\n<p>I agree that having dangerous information released by those who are in a position to mitigate the risks is better than having a careless actor releasing that same information \u2013\u2013 but I disagree that this is sufficient reason to preemptively release dangerous information. I think a world where everyone follows the logic of &quot;other people are going to release this information anyway but less carefully, so I might as well release it first&quot; is suboptimal compared to a world where everyone follows a norm of reaching consensus before releasing potentially dangerous information. And there are reasons to believe that this latter world isn't a pipe dream; after all, generally when we're thinking about info hazards, those who have access to the potentially dangerous information generally aren't malicious actors, but rather a finite number of, e.g., biology researchers (for biorisks) who could be receptive to establishing norms of consensus.</p>\n<p>I'm also not sure how the strategy of &quot;preemptively release, but mitigate&quot; would work in practice. Does this mean release potentially dangerous information, but with the most dangerous parts redacted? Release with lots of safety caveats inserted? How does this preclude the further release of the unmitigated info?</p>\n<blockquote>\n<p>I've not had the best luck reaching out to talk to people about my ideas. I expect that the majority of new ideas will come from people not heavily inside the group and thus less influenced by group think. So you might want to think of solutions that take that into consideration.</p>\n</blockquote>\n<p>I'm not sure I'm fully understanding you here. If you're saying that the majority of potentially dangerous ideas will originate in those who don't know what the unilateralist's curse is, then I agree \u2013\u2013 but I think this is just all the more reason to try to spread norms of consensus.</p>\n", "parentCommentId": "YBRo6bgGwJB7CPEbj", "user": {"username": "brianwang712"}}, {"_id": "XadaY5mbirYSiEviz", "postedAt": "2018-06-24T10:21:14.295Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<blockquote>\n<p>I've not had the best luck reaching out to talk to people about my ideas. I expect that the  majority of new ideas will come from people not heavily inside the group and thus less influenced by group think. So you might want to think of solutions that take that into consideration.</p>\n</blockquote>\n<p>Yes, I met the same problem. The best way to find people who are interested and are able to understand the specific problem is to publish the idea openly in a place like this forum, but in that situation, hypothtical bad people also will be able to read the idea.</p>\n<p>Also, info-hazard discussion applies only to &quot;medium level safety reserachers&quot;, as top level ones have enough authority to decide what is the info hazard, and (bio)scientists are not reading our discussions. As result, all fight with infor hazards is applied to small and not very relevant group. </p>\n<p>For example, I was advised not to repost the a scientific study as even reposting it would create the informational hazard in the form of attracting attention to its dangerous applications. However, I see the main problem on the fact that such scinetific research was done and openly published, and our relactance to discuss such events only lower our strategic understanding of the different risks.</p>\n", "parentCommentId": "YBRo6bgGwJB7CPEbj", "user": {"username": "turchin"}}, {"_id": "9wXZ3y7GhpvG2Fqxy", "postedAt": "2018-06-24T14:13:53.140Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<blockquote>\n<p>My understanding is that it applies regardless of whether or not you expect others to have the same information. All it requires is a number of actors making independent decisions, with randomly distributed error, with a unilaterally made decision having potentially negative consequences for all.</p>\n</blockquote>\n<p>Information determines the decisions that can be made. For example you can't spread the knowledge of how to create effective nuclear fusion without the information on how to make it.</p>\n<p>If there is a single person with the knowledge of how to create safe efficient nuclear fusion they cannot expect other people to release it on their behalf. They may expect it to be net positive but they also expect some downsides and are unsure of whether it will be net good or not.  To give a potential downside of nuclear fusion, let us say they are worried about creating excess heat over what the earth can dissipate due to widescale deployment in the world (even if it fixes global warming due to trapping solar energy, it might cause another heat related problem). I forget the technical term for this unfortunately.</p>\n<p>The fusion expert(s) cannot expect other people to release this information for them, for  as far as they know they are the only people making that exact decision.</p>\n<blockquote>\n<p>I'm also not sure how the strategy of &quot;preemptively release, but mitigate&quot; would work in practice. Does this mean release potentially dangerous information, but with the most dangerous parts redacted? Release with lots of safety caveats inserted? How does this preclude the further release of the unmitigated info?</p>\n</blockquote>\n<p>What the researcher can do is try and build consensus/lobby for a collective decision making body on the internal climate heating (ICH) problem.  Planning to release the information when they are satisfied that there is going to be a solution in time for fixing the problem when it occurs. </p>\n<p>If they find a greater than expected number of people lobbying for solutions to the ICH problem, then they can expect they are in a unilateralist's curse scenario. And they may want to hold off on releasing information even when they are satisfied with the way things are going (in case there is some other issue they have not thought of). </p>\n<p>They can look to see what the other people are doing that have been helping with ICH   and see if there other initiatives they are starting, that may or may not be to do with the advent of nuclear fusion.</p>\n<p>I think I am also objecting to the expected payoff being thought of as a fixed quantity. You can either learn more about the world to alter your knowledge of the payoff or try and introduce things/insituttions into the world to alter the expected payoff. Building useful institutions may rely on releasing some knowledge, that is where things become more hairy.</p>\n<blockquote>\n<blockquote>\n<p>I've not had the best luck reaching out to talk to people about my ideas. I expect that the majority of new ideas will come from people not heavily inside the group and thus less influenced by group think. So you might want to think of solutions that take that into consideration.</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<p>I'm not sure I'm fully understanding you here. If you're saying that the majority of potentially dangerous ideas will originate in those who don't know what the unilateralist's curse is, then I agree \u2013\u2013 but I think this is just all the more reason to try to \u2013\u2013 but I think this is just all the more reason to try to spread norms of consensus. </p>\n</blockquote>\n<p>I was suggesting that more norm spreading should be done outwards, keeping it simple and avoiding too much jargon. Is there a presentation of the unilateralist's curse aimed at micro biologists for example? </p>\n<p>Also as the the unilaterlist's curse suggests discussing with other people such that they can undertake the information release, sometimes increases the expectation of a bad out come. How should consensus be reached in those situations?</p>\n<blockquote>\n<p>Increasing the number of agents capable of undertaking the initiative also\nexacerbates the problem: as N grows, the likelihood of someone proceeding\nincorrectly increases monotonically towards 1.7 The magnitude of this effect can be\nquite large even for relatively small number of agents. For example, with the same\nerror assumptions as above, if the true value of the initiative V* = -1 (the initiative is\nundesirable), then the probability of erroneously undertaking the initiative grows\nrapidly with N, passing 50% for just 4 agents.</p>\n</blockquote>\n", "parentCommentId": "MAjeRA6zx3hzDTixt", "user": {"username": "WillPearson"}}, {"_id": "qrzzfrsvDvB8KmqFF", "postedAt": "2018-06-25T03:38:49.024Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<blockquote>\n<p>If there is a single person with the knowledge of how to create safe efficient nuclear fusion they cannot expect other people to release it on their behalf.</p>\n</blockquote>\n<p>Ah right. I suppose the unilateralist's curse is only a problem insofar as there are a number of other actors also capable of releasing the information; if you are a single actor then the curse doesn't really apply. Although one wrinkle might be considering the unilateralist's curse with regards to different actors through time (i.e., erring on the side of caution with the expectation that other actors in the future will gain access to and might release the information), but coordination in this case might be more challenging.</p>\n<blockquote>\n<p>What the researcher can do is try and build consensus/lobby for a collective decision making body on the internal climate heating (ICH) problem. Planning to release the information when they are satisfied that there is going to be a solution in time for fixing the problem when it occurs.</p>\n</blockquote>\n<p>Thanks, this concrete example definitely helps.</p>\n<blockquote>\n<p>I think I am also objecting to the expected payoff being thought of as a fixed quantity. You can either learn more about the world to alter your knowledge of the payoff or try and introduce things/insituttions into the world to alter the expected payoff. Building useful institutions may rely on releasing some knowledge, that is where things become more hairy.</p>\n</blockquote>\n<p>This makes sense. &quot;Release because the expected benefit is above the expected risk&quot; or &quot;not release because the vice versa is true&quot; is a bit of a false dichotomy, and you're right that we should be more thinking about options that could maximize the benefit while minimizing the risk when faced with info hazards.</p>\n<blockquote>\n<p>Also as the the unilaterlist's curse suggests discussing with other people such that they can undertake the information release, sometimes increases the expectation of a bad out come. How should consensus be reached in those situations?</p>\n</blockquote>\n<p>This can certainly be a problem, and is a reason not to go too public when discussing it. Probably it's best to discuss privately with a number of other trusted individuals first, who also understand the unilateralist's curse, and ideally who don't have the means/authority of releasing the information themselves (e.g., if you have a written up blog post you're thinking of posting that might contain info hazards, then maybe you could discuss in vague terms with other individuals first, without sharing the entire post with them?).</p>\n", "parentCommentId": "9wXZ3y7GhpvG2Fqxy", "user": {"username": "brianwang712"}}, {"_id": "ywewbjxpkk2jzAEoB", "postedAt": "2018-06-25T07:25:24.170Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<p>&quot;to prove this argument I would have to present general information which may be regarded as having informational hazard&quot;</p>\n<p>Is there any way to assess the credibility of statements like this (or whether this is actually an argument worth considering in a given specific context)?\nIt seems like you could use this as a general purpose argument for almost everything. </p>\n", "parentCommentId": null, "user": {"username": "Flodorner"}}, {"_id": "sRP3FgtbASHtvES8k", "postedAt": "2018-06-25T09:50:20.774Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<blockquote>\n<p>&quot;to prove this argument I would have to present general information which may be regarded as having informational hazard&quot;</p>\n</blockquote>\n<p>I agree statements of this kind are very annoying, whether or not they're true.</p>\n", "parentCommentId": "ywewbjxpkk2jzAEoB", "user": {"username": "MichaelPlant"}}, {"_id": "7yZ5stHHM4M68s7bT", "postedAt": "2018-06-25T11:47:21.918Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<p>It was in fact a link on the article about how to kill everybody using multiple simultaneous pandemics - this idea may be regarded by someone as an informational hazard, but it was already suggested by some terrorists from Voluntary Human extinction movement. I also discussed with some biologists and other x-risks researchers and we concluded that it is not an infohazard. I can send you a draft. </p>\n", "parentCommentId": "ywewbjxpkk2jzAEoB", "user": {"username": "turchin"}}, {"_id": "5tRaRokJXt89wWYet", "postedAt": "2018-06-25T11:51:54.495Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<p>It would be great to have some kind of a committee for info-hazards assessment, like a group of trusted people who will a) will take responsibility to decide whether the idea should be published or not b) will read all incoming suggestions in timely manner \u0441) their contacts (but may be not all the personalities) will be publicly known.</p>\n", "parentCommentId": null, "user": {"username": "turchin"}}, {"_id": "FiWrpxnycNCvxsLSm", "postedAt": "2018-06-25T19:12:53.517Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<blockquote>\n<p>Ah right. I suppose the unilateralist's curse is only a problem insofar as there are a number of other actors also capable of releasing the information; if you are a single actor then the curse doesn't really apply. Although one wrinkle might be considering the unilateralist's curse with regards to different actors through time (i.e., erring on the side of caution with the expectation that other actors in the future will gain access to and might release the information), but coordination in this case might be more challenging.</p>\n</blockquote>\n<p>Interesting idea. This may be worth trying to develop more fully?</p>\n<blockquote>\n<p>Probably it's best to discuss privately with a number of other trusted individuals first,  who also understand the unilateralist's curse,</p>\n</blockquote>\n<p>I'm still coming at this from a lens of &quot;actionable advice for people not in ea&quot;. It might be that the person doesn't know many other trusted individuals, what should be the advice then? It would probably also be worth giving advice on how to have the conversation as well. The original article gives some advice on what happens if consensus can't be reached (voting/such like). </p>\n<p>As I understand it you shouldn't wait for consensus else you have the unilateralist's curse in reverse. Someone pessimistic about an intervention can block the deployment of an intervention needed to avoid disaster (this seems very possible if you consider crucial considerations flipping signs, rather than just random noise in beliefs in desirability).</p>\n<p>Would you suggest discussion and vote (assuming no other courses of action can be agreed upon)? Do you see the need to correct for status quo bias in any way?</p>\n<p>This seems very important to get right. I'll think about this some more.</p>\n", "parentCommentId": "qrzzfrsvDvB8KmqFF", "user": {"username": "WillPearson"}}, {"_id": "jwAGy6Tck7s4HX3bM", "postedAt": "2018-06-26T09:30:03.873Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<p>Sometimes, when I work on a complex problem, I feel as if I become one of the best specialists in it. Surely, I know three other people who are able to understand my logic, but one of them is dead, another is not replying on my emails and the third one has his own vision, affected by some obvious flaw. So none of them could give me correct advice about the informational hazard. </p>\n", "parentCommentId": "FiWrpxnycNCvxsLSm", "user": {"username": "turchin"}}, {"_id": "EbMiZeuq5tTByxrbt", "postedAt": "2018-06-27T13:39:56.297Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<blockquote>\n<p>Interesting idea. This may be worth trying to develop more fully?</p>\n</blockquote>\n<p>Yeah. I'll have to think about it more.</p>\n<blockquote>\n<p>I'm still coming at this from a lens of &quot;actionable advice for people not in ea&quot;. It might be that the person doesn't know many other trusted individuals, what should be the advice then?</p>\n</blockquote>\n<p>Yeah, for people outside EA I think structures could be set up such that reaching consensus (or at least a majority vote) becomes a standard policy or an established norm. E.g., if a journal is considering a manuscript with potential info hazards, then perhaps it should be standard policy for this manuscript to be referred to some sort of special group consisting of journal editors from a number of different journals to deliberate. I don't think people need to be taught the mathematical modeling behind the unilateralist's curse for these kinds of policies to be set up, as I think people have an intuitive notion of &quot;it only takes one person/group with bad judgment to fuck up the world; decisions this important really need to be discussed in a larger group.&quot;</p>\n<p>One important distinction is that people who are facing info hazards will be in very different situations when they are within EA vs. when they are out of EA. For people within EA, I think it is much more likely to be the case that a random individual has an idea that they'd like to share in a blog post or something, which may have info hazard-y content. In these situations the advice &quot;talk to a few trusted individuals first&quot; seems to be appropriate.</p>\n<p>For people outside of EA, I think those who are in possession of info hazard-y content are much more likely to be embedded in some sort of larger institution (e.g., a research scientist or a journal editor looking to publish something), where perhaps the best leverage is setting up certain policies, rather than trying to teach everyone the unilateralist's curse.</p>\n<blockquote>\n<p>As I understand it you shouldn't wait for consensus else you have the unilateralist's curse in reverse. Someone pessimistic about an intervention can block the deployment of an intervention needed to avoid disaster.</p>\n</blockquote>\n<p>You're right, strict consensus is the wrong prescription. A vote is probably better. I wonder if there's mathematical modeling that you could do that would determine what fraction of votes is optimal, in order to minimize the harms of the standard unilateralist's curse and the curse in reverse? Is it a majority vote? A 2/3s vote? l suspect this will depend on what the &quot;true sign&quot; of releasing the potentially dangerous info is likely to be; the more likely it is to be negative, the higher bar you should be expected to clear before releasing.</p>\n", "parentCommentId": "FiWrpxnycNCvxsLSm", "user": {"username": "brianwang712"}}, {"_id": "LRMq6y73xZZFjHp6v", "postedAt": "2018-06-27T18:14:07.320Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<p>I believe this is something worth exploring. My model is that while most active people thinking about x-risks have some sort of social network links so they can ask others, there may be a long tail of people thinking in isolation, who may at some point just post something dangerous on LessWrong.</p>\n<p>(Also there is a problem of incentives, which are often strongly in favor of publishing. You don't get much credit for not publishing dangerous ideas, if you are not allready part of some established group.)</p>\n", "parentCommentId": "5tRaRokJXt89wWYet", "user": {"username": "Jan_Kulveit"}}, {"_id": "bj3yBnSooBCMw63h4", "postedAt": "2018-06-27T19:50:21.842Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<blockquote>\n<p>For people outside of EA, I think those who are in possession of info hazard-y content are much more likely to be embedded in some sort of larger institution (e.g., a research scientist or a journal editor looking to publish something), where perhaps the best leverage is setting up certain policies, rather than trying to teach everyone the unilateralist's curse.</p>\n</blockquote>\n<p>There is a growing movement of maker's and citizen scientists that are working on new technologies. It might be worth targeting them somewhat (although again probably without the math). I think the approaches for ea/non-ea seem sensible.</p>\n<blockquote>\n<p>You're right, strict consensus is the wrong prescription. A vote is probably better. I wonder if there's mathematical modeling that you could do that would determine what fraction of votes is optimal, in order to minimize the harms of the standard unilateralist's curse and the curse in reverse? Is it a majority vote? A 2/3s vote? l suspect this will depend on what the &quot;true sign&quot; of releasing the potentially dangerous info is likely to be; the more likely it is to be negative, the higher bar you should be expected to clear before releasing.</p>\n</blockquote>\n<p>I also like to weigh the downside of the lack of releasing the information as well. If you don't release information you are making everyone make marginally worse decisions (if you think someone will release it anyway later). For example in the nuclear fusion example, you think that everyone currently building new nuclear fission stations are wasting their time, that people training on how to manage coal plants should be training on something else etc, etc.</p>\n<p>I also have another consideration which is possibly more controversial. I think we need some bias to action, because it seems like we can't go on as we are for too much longer (another 1000 years might be pushing it). The level of resources and coordination towards global problems fielded by the status quo seems insufficient. So it is a default bad outcome. </p>\n<p>With this consideration, going back to the fusion pioneers, they might try and find people to tell so that they could increase the bus factor (the number of people that would have to die to lose the knowledge). They wouldn't want the knowledge to get lost (as it would be needed in the long term) and they would want to make sure that whoever they told understood the import and potential downsides of the technology.</p>\n<p>Edit: \nKnowing the sign of an intervention is hard, even after the fact. Consider the invention and spread of the knowledge about nuclear chain reactions. Without it we would probably be burning a lot more fossil fuels, however with it we have the existential risk associated with it. If that risk never pays out, then it may have been a spur towards greater coordination and peace.</p>\n<p>I'll try and formalise these thoughts at some point, but I am bit work impaired for a while.</p>\n", "parentCommentId": "EbMiZeuq5tTByxrbt", "user": {"username": "WillPearson"}}, {"_id": "hcfZhFpgSCFXqttS9", "postedAt": "2018-06-28T10:08:13.167Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<p>One more problem with the idea that I should consult my friends first before publishing a text is a &quot;friend' bias&quot;: people who are my friends tend to react more positively on the same text than those who are not friends. I personally had a situation when my friends told me that my text is good and non-info-hazardous, but when I presented it to people who didn't know me, their reaction was opposite.</p>\n", "parentCommentId": "bj3yBnSooBCMw63h4", "user": {"username": "turchin"}}, {"_id": "dWJjTEiXY2aNk5Hc6", "postedAt": "2018-07-03T22:45:21.218Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<p>Thanks for writing this. How best to manage hazardous information is fraught, and although I have some work in draft and under review, much remains unclear - as you say, almost anything could have some some downside risk, and never discussing anything seems a poor approach.</p>\n<p>Yet I strongly disagree with the conclusion that the default should be to discuss potentially hazardous (but non-technical) information publicly, and I think your proposals of how to manage these dangers (e.g. talk to one scientist first) generally err too lax. I provide the substance of this disagreement in a child comment.</p>\n<p>I\u2019d strongly endorse a heuristic along the lines of, \u201cTry to avoid coming up with (and don\u2019t publish) things which are novel and potentially dangerous\u201d, with the standard of novelty being a relatively uninformed bad actor rather than an expert (e.g. highlighting/elaborating something dangerous which can be found buried in the scientific literature should be avoided). </p>\n<p>This expressly includes more general information as well as particular technical points (e.g. \u201cNo one seems to be talking about technology X, but here\u2019s why it has really dangerous misuse potential\u201d would \u2018count\u2019, even if a particular \u2018worked example\u2019 wasn\u2019t included).</p>\n<p>I agree it would be good to have direct channels of communication for people considering things like this to get advice on whether projects they have in mind are wise to pursue, and to communicate concerns they have without feeling they need to resort to internet broadcast (cf. Jan Kulveit\u2019s <a href=\"http://effective-altruism.com/ea/1q9/informational_hazards_and_the_costeffectiveness/eq4\">remark</a>). </p>\n<p>To these ends, people with concerns/questions of this nature are warmly welcomed and encouraged to <a href=\"mailto:gjlewis37@gmail.com\">contact me</a> to arrange further discussion.</p>\n", "parentCommentId": null, "user": {"username": "Gregory_Lewis"}}, {"_id": "Fe6zi4v2MTYf9rHSK", "postedAt": "2018-07-03T22:47:53.130Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<p>0: We agree potentially hazardous information should only be disclosed (or potentially discovered) when the benefits of disclosure (or discovery) outweigh the downsides. Heuristics can make principles concrete, and a rule of thumb I try to follow is to have a clear objective in mind for gathering or disclosing such information (and being wary of vague justifications like \u2018improving background knowledge\u2019 or \u2018better epistemic commons\u2019) and incur the least possible information hazard in achieving this. </p>\n<p>A further heuristic which seems right to me is one should disclose information in the way that maximally disadvantages bad actors versus good ones. There are a wide spectrum of approaches that could be taken that lie between \u2018try to forget about it\u2019, and \u2018broadcast publicly\u2019, and I think one of the intermediate options is often best.</p>\n<p>1: I disagree with many of the considerations which push towards more open disclosure and discussion.</p>\n<p>1.1: I don\u2019t think we should be confident there is little downside in disclosing dangers a sophisticated bad actor would likely rediscover themselves. Not all plausible bad actors are sophisticated: a typical criminal or terrorist is no mastermind, and so may not make (to us) relatively straightforward insights, but could still \u2018pick them up\u2019 from elsewhere.</p>\n<p>1.2: Although a big fan of epistemic modesty (and generally a detractor of \u2018EA exceptionalism\u2019), EAs do have an impressive track record in coming up with novel and important ideas. So there is some chance of coming up with something novel and dangerous even without exceptional effort.</p>\n<p>1.3: I emphatically disagree we are at \u2018infohazard saturation\u2019 where the situation re. Infohazards \u2018can\u2019t get any worse\u2019. I also find it unfathomable ever being confident enough in this claim to base strategy upon its assumption (cf. eukaryote\u2019s <a href=\"https://www.lesswrong.com/posts/wSkHFu79xBKaaMmxM/informational-hazards-and-the-cost-effectiveness-of-open#aDBSGby5yZJHodWdA\">comment</a>).</p>\n<p>1.4: There are some benefits to getting out \u2018in front\u2019 of more reckless disclosure by someone else. Yet in cases where one wouldn\u2019t want to disclose it oneself, delaying the downsides of wide disclosure as long as possible seems usually more important, and so rules against bringing this to an end by disclosing yourself save in (rare) cases one knows disclosure is imminent rather than merely possible. </p>\n<p>2: I don\u2019t think there\u2019s a neat distinction between \u2018technical dangerous information\u2019 and \u2018broader ideas about possible risks\u2019, with the latter being generally safe to publicise and discuss.</p>\n<p>2.1: It seems easy to imagine cases where the general idea comprises most of the danger. The conceptual step to a \u2018key insight\u2019 of how something could be dangerously misused \u2018in principle\u2019 might be much harder to make than subsequent steps from this insight to realising this danger \u2018in practice\u2019. In such cases the insight is the key bottleneck for bad actors traversing the risk pipeline, and so comprises a major information hazard.</p>\n<p>2.2: For similar reasons, highlighting a neglected-by-public-discussion part of the risk landscape where one suspects information hazards lie has a considerable downside, as increased attention could prompt investigation which brings these currently dormant hazards to light.</p>\n<p>3: Even if I take the downside risks as weightier than you, one still needs to weigh these against the benefits. I take the benefit of \u2018general (or public) disclosure\u2019 to have little marginal benefit above more limited disclosure targeted to key stakeholders. As the latter approach greatly reduces the downside risks, this is usually the better strategy by the lights of cost/benefit. At least trying targeted disclosure first seems a robustly better strategy than skipping straight to public discussion (<a href=\"http://effective-altruism.com/ea/1q9/informational_hazards_and_the_costeffectiveness/eoy\">cf.</a>).</p>\n<p>3.1: In bio (and I think elsewhere) the set of people who are relevant setting strategy and otherwise contributing to reducing a given risk is usually small and known (e.g. particular academics, parts of the government, civil society, and so on). A particular scientist unwittingly performing research with misuse potential might need to know the risks of their work (likewise some relevant policy and security stakeholders), but the added upside to illustrating these risks in the scientific literature is limited (and the added downsides much greater). The upside of discussing them in the popular/generalist literature (including EA literature not narrowly targeted at those working on biorisk) is limited still further.</p>\n<p>3.2: Information also informs decisions around how to weigh causes relative to one another. Yet less-hazardous information (e.g. the basic motivation given <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/28806130\">here</a> or <a href=\"https://www.effectivealtruism.org/articles/biosecurity-as-an-ea-cause-area-claire-zabel/\">here</a>, and you could throw in social epistemic steers from the prevailing views of EA \u2018cognoscenti\u2019)  is sufficient for most decisions and decision-makers. The cases where this nonetheless might be \u2018worth it\u2019 (e.g. you are a decision maker allocating a large pool of human or monetary capital between cause areas) are few and so targeted disclosure (similar to 3.1 above) looks better.    </p>\n<p>3.3: Beyond the direct cost of potentially giving bad actors good ideas, the benefits of more public discussion may not be very high. There are many ways public discussion could be counter-productive (e.g. alarmism, ill-advised remarks poisoning our relationship with scientific groups, etc.). I\u2019d suggest the examples of cryonics, AI safety, GMOs and other lowlights of public communication of policy and science are relevant cautionary examples.</p>\n<p>4: I also want to supply other more general considerations which point towards a very high degree caution:</p>\n<p>4.1: In addition to the considerations around the unilateralist\u2019s curse <a href=\"http://effective-altruism.com/ea/1q9/informational_hazards_and_the_costeffectiveness/eoz\">offered</a> by Brian Wang (I have written a bit about this in the context of biotechnology <a href=\"https://thebulletin.org/2018/02/horsepox-synthesis-a-case-of-the-unilateralists-curse/\">here</a>) there is also an asymmetry in the sense that it is much easier to disclose previously-secret information than make previously-disclosed information secret. The irreversibility of disclosure warrants further caution in cases of uncertainty like this.</p>\n<p>4.2: I take the examples of analogous fields to also support great caution. As you note, there is a norm in computer security of \u2018don\u2019t publicise a vulnerability until there\u2019s a fix in place\u2019, and initially informing a responsible party to give them the opportunity to to do this pre-publication. Applied to bio, this suggests targeted disclosure to those best placed to mitigate the information hazard, rather than public discussion in the hopes of prompting a fix to be produced. (Not to mention a \u2018fix\u2019 in this area might prove much more challenging than pushing a software update.)</p>\n<p>4.3: More distantly, adversarial work (e.g. red-teaming exercises) is usually done by professionals, with a concrete decision-relevant objective in mind, with exceptional care paid to operational security, and their results are seldom made publicly available. This is for exercises which generate information hazards for a particular group or organisation - similar or greater caution should apply to exercises that one anticipates could generate information hazardous for everyone. </p>\n<p>4.4: Even more distantly, norms of intellectual openness are used more in some areas, and much less in others (compare the research performed in academia to security services). In areas like bio, the fact that a significant proportion of the risk arises from deliberate misuse by malicious actors means security services seem to provide the closer analogy, and \u2018public/open discussion\u2019 is seldom found desirable in these contexts.   </p>\n<p>5: In my work, I try to approach potentially hazardous areas as obliquely as possible, more along the lines of general considerations of the risk landscape or from the perspective of safety-enhancing technologies and countermeasures. I do basically no \u2018red-teamy\u2019 types of research (e.g. brainstorm the nastiest things I can think of, figure out the \u2018best\u2019 ways of defeating existing protections, etc.)</p>\n<p>(Concretely, this would comprise asking questions like, \u201cHow are disease surveillance systems forecast to improve over the medium term, and are there any robustly beneficial characteristics for preventing high-consequence events that can be pushed for?\u201d or \u201cAre there relevant limits which give insight to whether surveillance will be a key plank of the \u2018next-gen biosecurity\u2019 portfolio?\u201d, and not things like, \u201cWhat are the most effective approaches to make pathogen X maximally damaging yet minimally detectable?\u201d)   </p>\n<p>I expect a non-professional doing more red-teamy work would generate less upside (e.g. less well networked to people who may be in a position to mitigate vulnerabilities they discover, less likely to unwittingly duplicate work) and more downside (e.g. less experience with trying to manage info-hazards well) than I. Given I think this work is usually a bad idea for me to do, I think it\u2019s definitely a bad idea for non-professionals to try.</p>\n<p>I therefore hope people working independently on this topic approach \u2018object level\u2019 work here with similar aversion to more \u2018red-teamy\u2019 stuff, or instead focus on improving their capital by gaining credentials/experience/etc. (this has other benefits: a lot of the best levers in biorisk are working with/alongside existing stakeholders rather than striking out on one\u2019s own, and it\u2019s hard to get a role without (e.g.) graduate training in a relevant field). I hope to produce a list of self-contained projects to help direct laudable \u2018EA energy\u2019 to the best ends.</p>\n", "parentCommentId": "dWJjTEiXY2aNk5Hc6", "user": {"username": "Gregory_Lewis"}}, {"_id": "hnhuXDFyY3H2ZcdQN", "postedAt": "2018-07-04T14:17:49.657Z", "postId": "KPwgmDyHaceoEFSPm", "htmlBody": "<p>Hi Gregory, </p>\n<p>A couple of musings generated by your comment.</p>\n<blockquote>\n<p>2: I don\u2019t think there\u2019s a neat distinction between \u2018technical dangerous information\u2019 and \u2018broader ideas about possible risks\u2019, with the latter being generally safe to publicise and discuss.</p>\n</blockquote>\n<p>I have this idea of independent infrastructure, trying to make infrastructure (electricity/water/food/computing) that is on a smaller scale than current infrastructure. This is for a number of reasons, one of which includes mitigating risks, How should I build broad-scale support for my ideas without talking about the risks I am mitigating?</p>\n<blockquote>\n<p>4.1: In addition to the considerations around the unilateralist\u2019s curse offered by Brian Wang (I have written a bit about this in the context of biotechnology here) there is also an asymmetry in the sense that it is much easier to disclose previously-secret information than make previously-disclosed information secret. The irreversibility of disclosure warrants further caution in cases of uncertainty like this.</p>\n</blockquote>\n<p>Although in some scenarios non-disclosure is irreversible as well, as conditions change. Consider if someone had the idea of hacking a computer and had managed to convince the designers of C to create a more secure list indexing and also everyone not to use other insecure languages. Now we would not be fighting the network effect of all the bad C code when trying to get people to code computers securely.</p>\n<p>This irreversibility of non-disclosure seems to only occur if if something is not a huge threat right now, but may become more so as technology develops and gets more widely used and locked in. Not really relevant to the biotech arena. that I can think of immediately at least. But an interesting scenario nonetheless.</p>\n", "parentCommentId": "Fe6zi4v2MTYf9rHSK", "user": {"username": "WillPearson"}}]