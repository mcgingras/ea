[{"_id": "bSW6SxKWPCENnmhvt", "postedAt": "2022-10-19T14:42:41.860Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<p>So what exactly is wrong with x-risk? I feel like it has already been pretty well established?</p>", "parentCommentId": null, "user": {"username": "ThomasFowl"}}, {"_id": "YnkYc2cwawapC9sNZ", "postedAt": "2022-10-19T15:07:13.906Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<p>I think we should avoid using acronyms where possible.&nbsp;</p><p>It makes sense to occasionally use them when you're abbreviating common phrases or names of organisations that would otherwise be long/unwieldy to say or write in full every time. But too often acronyms just needlessly introduce barriers to understanding.</p>", "parentCommentId": null, "user": {"username": "Matt_Sharp"}}, {"_id": "pxPPtgbgvDu2qP7WS", "postedAt": "2022-10-19T15:17:33.529Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<p>Presumably he means because x-risk is short for 'existential risk' and can refer to things other than extinction.&nbsp;</p>", "parentCommentId": "bSW6SxKWPCENnmhvt", "user": {"username": "Sam Glover"}}, {"_id": "wE65xod5pvqkLvqKy", "postedAt": "2022-10-19T16:35:10.523Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<p>Would you be in favor of phasing out s-risk and x-risk?</p>", "parentCommentId": "YnkYc2cwawapC9sNZ", "user": {"username": "Charles_Guthmann"}}, {"_id": "wyoaHDMWzLfkQWbRM", "postedAt": "2022-10-19T16:56:22.017Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<p>for transparency I strong agreement downvoted you for the reason sam said.</p>", "parentCommentId": "bSW6SxKWPCENnmhvt", "user": {"username": "Charles_Guthmann"}}, {"_id": "WPmWEbfpSrX2qbnbr", "postedAt": "2022-10-19T17:19:14.222Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<p>I think x-risk should remain as meaning extinction risk, but \"<a href=\"https://forum.effectivealtruism.org/posts/myp9Y9qJnpEEWhJF9/linch-s-shortform?commentId=bPjpvG8ZCgMA3srsZ\">doom</a>\" better encompasses what people refer to when they use x-risk colloquially. Doom encompasses x-risk (extinction risk), any permanent drastic curtailment of potential (permanent stagnation or totalitarian lock-in), and s-risk (suffering risk; fates worse than extinction).<br><br>EDIT: I fell prey to the original confusion. x-risk was originally existential risk, which encompasses extinction risk and permanent drastic curtailment of future potential. Perhaps x-risk is better as just extinction risk, and ex-risk should be used for existential risk? Doom is still useful as a broader term for common usage (also including s-risk).<br><br>Would be good if OP included a definition of existential risk/x-risk!</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "RnjGtam3XsYWX8KWC", "postedAt": "2022-10-19T17:24:00.079Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<p>Perhaps \"safety from AGI doom\" (or \"safety from ASI doom\", or \"ASI doom-safety\") is better than <a href=\"https://forum.effectivealtruism.org/posts/GEukFgwrrebW7efNz/what-does-it-mean-for-an-agi-to-be-safe-1?commentId=FfAQHxCwzRLZvdo8p\">\"AGI x-safety\"</a>?</p>", "parentCommentId": "WPmWEbfpSrX2qbnbr", "user": {"username": "Greg_Colbourn"}}, {"_id": "GgWqusAjs2PizHg2E", "postedAt": "2022-10-19T17:28:51.953Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<p>I really like this! doom feels right for existential risks and x risk definitely <i>could</i> &nbsp;be used for extinction risk since it also has the phonetic x. However I disagree with the notion that x-risk already means extinction risk, see <a href=\"https://www.fhi.ox.ac.uk/Existential-risk-and-existential-hope.pdf\">this paper</a> for instance, or Sam's comment.&nbsp;</p>", "parentCommentId": "WPmWEbfpSrX2qbnbr", "user": {"username": "Charles_Guthmann"}}, {"_id": "HnPLiMcmicyei32nQ", "postedAt": "2022-10-19T18:05:22.862Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<p>Cool, and thanks for editing the OP. I think the definition of s-risk you give is off though. <a href=\"https://en.wikipedia.org/wiki/Suffering_risks\">S-risks</a> are risks of suffering on an astronomical scale (e.g. a future filled with tortured beings). They are sometimes classed as a sub-category of existential risk, but given they are (much) worse than extinction or a drastic curtailment of potential, they are also often talked about separately.</p>", "parentCommentId": "GgWqusAjs2PizHg2E", "user": {"username": "Greg_Colbourn"}}, {"_id": "K3wmQHXK2NsAw93BS", "postedAt": "2022-10-19T19:03:19.223Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<p>There's a somewhat standard argument I've heard before that we shouldn't separate these out because they're already covered by existential risks. For example, this is already a complaint lobbied against s-risks since they're a subtype of x-risk. Trying to focus on a subtype seems like mostly a bid to allow something to rise to higher importance than it would otherwise be under standard x-risk framings.</p><p>For example, many people try to claim climate change is an extinction risk because fast, significant climate change could cause an extinction event, but that doesn't mean all of life on Earth dies, just that some large percentage of species are wiped out. Existential risk helps keep the lines clear, since climate change is generally not considered an existential risk since plenty of life would be left around after major climate change since that's what's happened in every past instance of major climate change on Earth.</p><p>Not having an acronym for extinction risk seems fine then since I think existential risks matter much more, and we should do what we can as EAs to focus on the highest leverage stuff rather than get distracted by things that might kill lots of species but not wipe out all life, since in comparison this is a lower priority type of risk due to lesser impact.</p>", "parentCommentId": null, "user": {"username": "gworley3"}}, {"_id": "FXpE7Cy98YjowPtvp", "postedAt": "2022-10-19T19:53:06.989Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<p>Wait so something can be an S-risk but not an X-risk?</p><p>Let me know if you agree with the following:</p><p>extinction risk but not existential risk: Something that kills everything on earth but it turns on the future would have been worse if we survived</p><p>S-risk but not existential risk: Factory Farming</p><p>S-risk and existential risk: Stable totalitarianism</p><p>Existential risk but not S-risk or extinction risk: Permanent civilization collapse.</p><p>All three: AI kills us all and then runs horrible simulations</p>", "parentCommentId": "HnPLiMcmicyei32nQ", "user": {"username": "Charles_Guthmann"}}, {"_id": "BGvyybfMLWqYy2cHR", "postedAt": "2022-10-19T20:06:11.358Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<p>Yes, I agree with that, although perhaps not your first example (I would say that is an existential risk), and for the second one (Factory Farming), to be astronomical in size and therefore an s-risk it would have to be \"<a href=\"https://forum.effectivealtruism.org/posts/bfdc3MpsYEfDdvgtP/why-the-expected-numbers-of-farmed-animals-in-the-far-future\">Space colonisation locking in Factory Farming on a cosmic scale</a>\".</p>", "parentCommentId": "FXpE7Cy98YjowPtvp", "user": {"username": "Greg_Colbourn"}}, {"_id": "ymAYTrdCvzTu6sXfJ", "postedAt": "2022-10-19T20:11:55.703Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<p>Interesting... So the first scenario is an x-risk that we would want to increase.&nbsp;</p><p>edit: I had sort of written about this in a separate post but I was told that scenario one is extinction risk and not x-risk. Slightly confused but I agree with you based on the definition of x-risk I used.</p>", "parentCommentId": "BGvyybfMLWqYy2cHR", "user": {"username": "Charles_Guthmann"}}, {"_id": "rSXipK8xoHLjLnmdF", "postedAt": "2022-10-19T20:44:24.372Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<blockquote><p>So the first scenario is an x-risk that we would want to increase.&nbsp;</p></blockquote><p>This is getting into button-pushing territory. \"Would have wanted to increase were we certain of the future being much worse\", maybe. But I don't think we can ever be certain enough.<br><br>Re extinction vs existential, I'd say it is both (extinction being a subset of existential).</p>", "parentCommentId": "ymAYTrdCvzTu6sXfJ", "user": {"username": "Greg_Colbourn"}}, {"_id": "vDxCK5dSMwH5SPq3E", "postedAt": "2022-10-21T06:30:29.387Z", "postId": "8mLm9bnRnEdrSE7ZE", "htmlBody": "<blockquote><p>For example, this is already a complaint lobbied against s-risks since they're a subtype of x-risk.</p></blockquote><p>I don't necessarily think s-risks and extinction risks are strictly subtypes of x-risks (if by subtype you mean subset), although it seems like the community may have a few definitions swirling around for each term.&nbsp;</p><blockquote><p>Trying to focus on a subtype seems like mostly a bid to allow something to rise to higher importance than it would otherwise be under standard x-risk framings</p></blockquote><p>Does giving something an acronym = trying to focus on? It could explicitly help you focus less on it by clarifying via making it easier to communicate, for example. Even if it adds focus, if it also adds clarity (which it totally may not), there is at least the notion of some tradeoff.</p><blockquote><p>rather than get distracted by things that might kill lots of species but not wipe out all life, since in comparison this is a lower priority type of risk due to lesser impact.</p></blockquote><p>This still seems more important than almost anything else that isn't an x-risk to me. So is the implication here that existential risk is the sole term that gets an acronym? I feel ok about letting the global dev and animal welfare communities have acronyms (conditioning on acronyms being useful) even though one might say they are orders of magnitude less important than x-risk reduction.</p>", "parentCommentId": "K3wmQHXK2NsAw93BS", "user": {"username": "Charles_Guthmann"}}]