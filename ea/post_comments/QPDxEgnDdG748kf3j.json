[{"_id": "XxCZDcJrYhi9A7sjD", "postedAt": "2024-01-14T21:53:18.823Z", "postId": "QPDxEgnDdG748kf3j", "htmlBody": "<p>Even if there are risks to using analogies with persuasion, we need analogies in order to persuade people. While a lot of people here are strong abstract thinkers, this is really rare. Most people need something more concrete to latch onto. Uniform disarmament here is a losing strategy; and not justified here as I don't think the analogies are as weak as you think. If you tell me what you consider to be the two weakest analogies above, I'm sure I'd be pretty to steelman at least one of them.<br><br>If we want to improve epistemics, a better strategy would probably be to always try to pair analogies (at least for longer texts/within reason). So identify an analogy to describe how you think about AI, identify an alternate plausible analogy for how you should think about it and then explain why your analogy is better/whereabouts you believe AI lies between the two.</p><blockquote><p>Many proponents of AI risk seem happy to critique analogies when they <i>don't</i> support the desired conclusion, such as <a href=\"https://www.lesswrong.com/posts/RcZeZt8cPk48xxiQ8/anthropomorphic-optimism\">the anthropomorphic analogy</a>.</p></blockquote><p>Of course! Has there ever been a single person in the entire world who has embraced <i>all</i> analogies instead of useful and relevant analogies?</p><p>Maybe you're claiming that AI risk proponents reject analogies in general when someone is using an analogy that supports the opposite conclusion, but accepting the validity of analogies when it supports their conclusion. If this were the case, it would be bad, but I don't actually think this is what is happening. My guess would be that you've seen situations where someone has used an analogy to critique AI safety and then the AI safety person said something along the lines, \"Analogies are often misleading\" and you took this as a rejection of analogies in general as opposed to a reminder to check whether the analogy actually applies.</p>", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "wWwvKrmdpaPragact", "postedAt": "2024-01-14T22:06:36.536Z", "postId": "QPDxEgnDdG748kf3j", "htmlBody": "<blockquote><p>Maybe you're claiming that AI risk proponents reject analogies in general when someone is using an analogy that supports the opposite conclusion, but accepting the validity of analogies when it supports their conclusion. If this were the case, it would be bad, but I don't actually think this is what is happening.</p></blockquote><p>Then perhaps you can reply to the examples I used in the post when arguing that analogies are often used selectively? I named two examples: (1) a preference for an analogy to chimps rather than to golden retrievers when arguing about AI alignment, and (2) a preference for an analogy to human evolution rather than an analogy to within-lifetime learning when arguing about inner misalignment.</p><p>I do think that a major element of my thesis is that many analogies appear to be chosen <i>selectively. </i>While I advocate that we should not merely switch analogies, I think if we are going to use analogies-as-arguments anyway, then we should try to find ones that are the most <i>plausible</i>, and <i>natural. </i>And I don't currently see much reason to prefer to chimp and evolution analogies over their alternatives in that case.</p>", "parentCommentId": "XxCZDcJrYhi9A7sjD", "user": {"username": "Matthew_Barnett"}}, {"_id": "Q2kjtmdbCKADyaFcY", "postedAt": "2024-01-15T07:10:57.209Z", "postId": "QPDxEgnDdG748kf3j", "htmlBody": "<p>I think that neither of those are selective uses of analogies. &nbsp;They do point to similarities between things we have access to and future ASI that you might not think are valid similarities, but that is one thing that makes analogies useful - they can make locating disagreements in people's models very fast, since they're structurally meant to transmit information in a highly compressed fashion.</p>", "parentCommentId": "wWwvKrmdpaPragact", "user": {"username": "T3t"}}, {"_id": "PR97dfQjNjFxGqREA", "postedAt": "2024-01-15T07:24:04.573Z", "postId": "QPDxEgnDdG748kf3j", "htmlBody": "<blockquote><p>\"The analogies establish almost nothing of importance about the behavior and workings of real AIs\"</p></blockquote><p>You seem to be saying that there is some alternative that establishes something about \"real AIs,\" but then you admit these real AIs don't exist yet, and you're discussing \"expectations of the future\" by proxy. I'd like to push back, and say that I think you're not really proposing an alternative, or that to the extent you are, you're not actually defending that alternative clearly.</p><p>&nbsp;</p><p>I agree that arguing by analogy to discuss current LLM behavior is less useful than having a working theory of interpretability and LLM cognition - though we don't have any such theory, as far as I can tell - but I have an even harder time understanding what you're proposing is a superior way of discussing a future situation that isn't amenable to that type of theoretical analysis, because we are trying to figure out where we do and do not share intuitions, and which models are or are not appropriate for describing the future technology. And I'm not seeing a gears level model proposed, and I'm not seeing concrete predictions.</p><p>Yes, arguing by analogy can certainly be slippery and confusing, and I think it would benefit from grounding in concrete predictions. And use of any specific base rates is deeply contentious, since reference classes are always debateable. But at least it's clear what the argument is, since it's an analogy. In opposition to that, arguing by direct appeal to your intuitions, where you claim your views are a \"straightforward extrapolation of current trends\" is being done without reference to your reasoning process. And that reasoning process, because it doesn't have a explicit gears level model, is based on informal human reasoning and therefore, as Lakens argues, deeply rooted in metaphor anyways, seems worse - it's reasoning by analogy with extra steps.</p><p>For example, what does \"straightforward\" convey, when you say \"straightforward extrapolation\"? Well, the intuition the words build on is that moving straight, as opposed to extrapolating exponentially or discontinuously, is better or simpler. Is that mode of prediction easier to justify than reasoning via analogies to other types of minds? I don't know, but it's not obvious, and dismissing one as analogy but seeing the other as \"straightforward\" seems confused.</p>", "parentCommentId": null, "user": {"username": "Davidmanheim"}}, {"_id": "gnj9pwxrc9dg8DYdC", "postedAt": "2024-01-16T18:55:23.889Z", "postId": "QPDxEgnDdG748kf3j", "htmlBody": "<p>Great piece. And quite funny you couldn\u2019t just quote Ross Douthat.</p>\n", "parentCommentId": null, "user": {"username": "Thomaaas"}}, {"_id": "Ya3uGHauCKezTMqeJ", "postedAt": "2024-01-20T05:22:33.980Z", "postId": "QPDxEgnDdG748kf3j", "htmlBody": "<p>I actually thought that the discussion of the chimp analogy was handled pretty well in the podcast. Ajeya brought up that example and then Rob explicitly brought up an alternate mental model of it being a tool (like Google Maps). Discussing multiple possible mental models is exactly want you want to be doing to guard against biases. I agree that it would have be nice to discuss an analogy more like a golden retriever or kid as well, but there's always additional issues that could be discussed.</p><p>I agree Ajeya didn't really provide her reasons for seeing the chimp analogy as useful there, but I think it's valuable as a way of highlighting the AI equivalent of the nature vs. nurture debate. Many people talk about AI's using the analogy of children and they assume that we can produce moral AI's by just treating them well/copying good human parenting strategies. I think the chimp analogy is useful as a way of highlighting that appearance can be decieving.</p>", "parentCommentId": "wWwvKrmdpaPragact", "user": {"username": "casebash"}}, {"_id": "nTCY2CrSvpjaPXLRw", "postedAt": "2024-01-20T20:42:27.924Z", "postId": "QPDxEgnDdG748kf3j", "htmlBody": "<blockquote><p>I actually thought that the discussion of the chimp analogy was handled pretty well in the podcast. Ajeya brought up that example and then Rob explicitly brought up an alternate mental model of it being a tool (like Google Maps)</p></blockquote><p>The tool analogy appeared to have been brought up as a way of strawmanning/weakmanning people who disagree with them. I think the analogy to Google Maps is not actually representative of how most intelligent AI optimists reason about AI as of 2023 (even if Holden Karnofsky used it in 2012, before the deep learning revolution). The full quote was,</p><blockquote><p><strong>Rob Wiblin:</strong> Right. I guess the idea there is that you might think that the chimp is learning that people are to be trusted and it\u2019s all good, but it\u2019s a different mind that thinks differently and draws different conclusions, and it might have particular tendencies that are not obvious to you, particular impulses that are not relatable to you.</p><p>The shrinking number of people who are not troubled by any of this at all, I assume that most of them have a different analogy in mind, which is like a can opener or a toaster. OK, that\u2019s a little bit silly. To be more sympathetic, the analogy that they have in their mind is that this is a tool that we\u2019ve made, that we\u2019ve designed.</p><p><strong>Ajeya Cotra:</strong> Like Google Maps.</p><p><strong>Rob Wiblin:</strong> Like Google Maps. \u201cWe designed it to do the thing that we want. Why do you think it\u2019s going to spin out of control? Tools that we\u2019ve made have never spun out of control and started acting in these bizarre ways before.\u201d If the analogy you have in mind is something like Google Maps, or your phone, or even like a recommendation algorithm, it makes sense that it\u2019s going to seem very counterintuitive in that case to think that it\u2019s going to be dangerous. It\u2019ll be way less intuitive in that case than in the case where you\u2019re thinking about raising a gorilla.</p><p><strong>Ajeya Cotra:</strong> Yeah. I think the real disanalogy between Google Maps and all of this stuff and AI systems is that we are not producing these AI systems in the same way that we produced Google Maps: by some human sitting down, thinking about what it should look like, and then writing code that determines what it should look like.</p></blockquote><p>&nbsp;</p><blockquote><p>Many people talk about AI's using the analogy of children and they assume that we can produce moral AI's by just treating them well/copying good human parenting strategies. I think the chimp analogy is useful as a way of highlighting that appearance can be decieving.</p></blockquote><p>As I said in the post, I think the chimp analogy can be good for conveying the <i>logical possibility</i> of misalignment. Indeed, appearances <i>can</i> be deceiving. I don't see any particularly strong reasons to think appearances actually <i>are</i> deceiving here. What evidence is there that AIs won't actually just be aligned by default given good \"parenting strategies\" i.e. reasonably good training regimes? (And again, I'm not saying AIs <i>will</i> necessarily be aligned by default. I just think this question is uncertain, and I don't think the chimp analogy is actually useful as a mental model of the situation here.)</p>", "parentCommentId": "Ya3uGHauCKezTMqeJ", "user": {"username": "Matthew_Barnett"}}, {"_id": "dMpAhfrFRvdtHWrbb", "postedAt": "2024-01-20T22:16:53.494Z", "postId": "QPDxEgnDdG748kf3j", "htmlBody": "<p>There are lots of people who think about AI as a tool.</p>", "parentCommentId": "nTCY2CrSvpjaPXLRw", "user": {"username": "casebash"}}, {"_id": "ao3XoSwksCtZirQNC", "postedAt": "2024-01-21T04:39:32.951Z", "postId": "QPDxEgnDdG748kf3j", "htmlBody": "<p>A lot of people think about AI in all sorts of inaccurate ways, including those who argue for AI pessimism. \"AI is like Google Maps\" is not at all how most intelligent AI optimists such as Nora Belrose, Quintin Pope, Robin Hanson, and so on, think about AI in 2024. It's a <a href=\"https://slatestarcodex.com/2014/05/12/weak-men-are-superweapons/\">weakman</a>, in a pretty basic sense.</p>", "parentCommentId": "dMpAhfrFRvdtHWrbb", "user": {"username": "Matthew_Barnett"}}, {"_id": "AF9ccyWa86oAwkB5K", "postedAt": "2024-02-07T14:57:56.150Z", "postId": "QPDxEgnDdG748kf3j", "htmlBody": "<p>Interesting post! I think analogies are good for public communication but not for understanding things at a deep level. They're like a good way to quickly template something you haven't thought about at all with something you are familiar with. I think effective mass communication is quite important and we shouldn't let the perfect be the enemy of the good.</p><p>I wouldn't consider my Terminator comparison an analogy in the sense of the other items on this list. Most of the other items have the character of \"why might AI go rogue?\" and then they describe something other than AI that is hard to understand or goes rogue in some sense and assert that AI is like that. But Terminator is just literally about an AI going rogue. It's not so much an analogy as a literal portrayal of the concern. My point wasn't so much that you should proactively tell people that AI risk is like Terminator, but that people are just going to notice this on their own (because it's incredibly obvious), and contradicting them makes no sense.</p>", "parentCommentId": null, "user": {"username": "skluug"}}]