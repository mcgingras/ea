[{"_id": "r3bR38mSN4gz4FdbH", "postedAt": "2023-03-29T14:41:48.774Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I appreciate that you are putting out numbers and explain the current research landscape, but I am missing clear actions.</p>\n<p>The closest you are coming to proposing them is here:</p>\n<blockquote>\n<p>We need a concerted effort that matches the gravity of the challenge. The best ML researchers in the world should be working on this! There should be billion-dollar, large-scale efforts with the scale and ambition of Operation Warp Speed or the moon landing or even OpenAI\u2019s GPT-4 team itself working on this problem.[17] Right now, there\u2019s too much fretting, too much idle talk, and way too little \u201clet\u2019s roll up our sleeves and actually solve this problem.\u201d</p>\n</blockquote>\n<p>But that still isn't an action plan. Say you convince me, most of the EA Forum and half of all university educated professionals in your city that this is a big deal. What, concretely, should we do now?</p>\n", "parentCommentId": null, "user": {"username": "Denise_Melchin"}}, {"_id": "5AWnK7YiRCvYWRFYj", "postedAt": "2023-03-29T14:43:43.606Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>This was enlightening and convincing. Plus a great read!</p>", "parentCommentId": null, "user": {"username": "BenStewart"}}, {"_id": "umQJzmhg3sq5DgEeK", "postedAt": "2023-03-29T14:49:31.134Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>80,000 Hours has a bunch of <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help\">ideas</a> on their AI problem profile.</p><p>(I'm not trying to be facetious. This main purpose of this post to me seems to be motivational: \"I\u2019m just trying to puncture the complacency I feel like many people I encounter have.\" Plus nudging existing alignment researchers towards more empirical work. [Edit: This post could also be concrete career advice if you're someone like Sanjay who read 80,000 Hours' post on the number alignment researchers and was left <a href=\"https://forum.effectivealtruism.org/posts/rZoRGxJzipcQoaPST/how-many-people-are-working-directly-on-reducing-existential?commentId=3mCzf49kS3pCowfqg#comments\">wondering</a> \"...so...is that basically enough, or...? After reading this post, I'm assuming that leopold's answer at least is \"HELL NO.\"])</p>", "parentCommentId": "r3bR38mSN4gz4FdbH", "user": null}, {"_id": "b4Hmsh6KMwJawCsuz", "postedAt": "2023-03-29T14:57:09.079Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>This is some of the finest writing I've seen on AI alignment which both (a) covers technical content , and (b) is accessible to a non-technical audience.&nbsp;</p><p>I particularly liked the fact that the content was opinionated; I think it's easier to engage with content when the author takes a stance rather than just hedges their bets throughout.</p>", "parentCommentId": null, "user": {"username": "Sanjay"}}, {"_id": "m9io3HyykAZKFwH9R", "postedAt": "2023-03-29T15:00:29.889Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I think the suggestion of ELK work along the lines of Collin Burns et al counted as a concrete step that alignment researchers could take.</p><p>There may be other types of influence available for those who are not alignment researchers, which Leopold wasn't precise about. E.g. those working in the financial system may be able to use their influence to encourage more alignment work.</p>", "parentCommentId": "r3bR38mSN4gz4FdbH", "user": {"username": "Sanjay"}}, {"_id": "bX3E77f4XKrKcWp5F", "postedAt": "2023-03-29T15:59:31.501Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>TL;DR: I totally agree with the general spirit of this post, we need people to solve alignment, and we're not on track. Go and work on alignment but before you do, try to engage with the existing research, there are reasons why it exists. There are a lot of things not getting worked on within AI alignment research, and I can almost guarantee you that within six months to a year, you can find things that people haven't worked on.&nbsp;</p><p>So go and find these underexplored areas in a way where you engage with what people have done before you!</p><blockquote><p>There\u2019s no secret elite SEAL team coming to save the day. This is it. We\u2019re not on track.</p><p>If timelines are short and we don\u2019t get our act together, we\u2019re in a lot of trouble. Scalable alignment\u2014aligning superhuman AGI systems\u2014is a real, unsolved problem. It\u2019s quite simple: current alignment techniques rely on human supervision, but as models become superhuman, humans won\u2019t be able to reliably supervise them.</p><p>But my pessimism on the current state of alignment research very much doesn\u2019t mean I\u2019m an Eliezer-style doomer. Quite the opposite, I\u2019m optimistic. I think scalable alignment is a solvable problem\u2014and it\u2019s an ML problem, one we can do real science on as our models get more advanced. But we gotta stop fucking around. We need an effort that matches the gravity of the challenge.<a href=\"https://forum.effectivealtruism.org/posts/5LNxeWFdoynvgZeik/nobody-s-on-the-ball-on-agi-alignment#fnbxq9sjulozt\"><sup>[1]</sup></a></p></blockquote><p>I also agree in that Eliezer's style of doom seems uncalled for and that this is a solvable but difficult problem. My personal p(doom) is something around 20%, and I think this seems quite reasonable.&nbsp;</p><blockquote><p>Barely anyone is going for the throat of solving the core difficulties of scalable alignment. Many of the people who are working on alignment are doing blue-sky theory, pretty disconnected from actual ML models. Most of the rest are doing work that\u2019s vaguely related, hoping it will somehow be useful, or working on techniques that might work now but predictably fail to work for superhuman systems.</p></blockquote><p>Now I do want to give pushback on this claim as I see a lot of people who haven't fully engaged with the more theoretical alignment landscape making this claim. There are only 300 people working on alignment, but those people are actually doing things, and most of them aren't doing blue in the sky theory.</p><p>A note on the ARC claim:</p><blockquote><p><br>But his research now (\u201cheuristic arguments\u201d) is roughly \u201ctrying to solve alignment via galaxy-brained math proofs.\u201d As much as I respect and appreciate Paul,&nbsp;I\u2019m really skeptical of this: basically all deep learning progress has been empirical, often via dumb hacks<a href=\"https://forum.effectivealtruism.org/posts/5LNxeWFdoynvgZeik/nobody-s-on-the-ball-on-agi-alignment#fnguw3v2ovbbo\"><sup>[3]</sup></a>&nbsp;and intuitions, rather than sophisticated theory.&nbsp;My baseline expectation is that aligning deep learning systems&nbsp;will be achieved similarly.<a href=\"https://forum.effectivealtruism.org/posts/5LNxeWFdoynvgZeik/nobody-s-on-the-ball-on-agi-alignment#fnubqf006eg\"><sup>[4]</sup></a>&nbsp;</p></blockquote><p>This is essentially a claim about the methodology of science in that working on existing systems gives more information and breakthroughs compared to working on a blue-sky theory. The current hypothesis for this is that it is just a lot more information-rich to do real-world research. This is, however, not the only way to get real-world feedback loops. Christiano is not working on blue sky theory; he's using real-world feedback loops in a different way; he looks at the real world and looks for information that's already there.&nbsp;</p><p>A discovery of this type is, for example, the tragedy of the commons; whilst we could have created computer simulations to see the process in action, it's 10x easier to look at the world and see the real-time failures. He tells stories and sees where they fail in the future as his <a href=\"https://ai-alignment.com/my-research-methodology-b94f2751cb2c\">research methodology</a>. This gives bits of information on where to do future experiments, like how we would be able to tell that humans would fail to stop overfishing without actually running an experiment on it.</p><p>This is also what John Wentworth does with his research; he looks at the real world as a reference frame which is quite rich in information. Now a good question is why we haven't seen that many empirical predictions from Agent Foundations. I believe it is because alignment is quite hard, and specifically, it is hard to define agency in a satisfactory way due to some really fuzzy problems (<a href=\"https://www.lesswrong.com/s/LWJsgNYE8wzv49yEc\">boundaries,</a> among others) and, therefore, hard to make predictions.&nbsp;</p><p>We don't want to mathematize things too early either, as doing so would put us into a predefined reference frame that it might be hard to escape from. We want to find the right ballpark for agents since if we fail we might base evaluations on something that turns out to be false.&nbsp;</p><p>In general, there's a difference in the types of problems in alignment and empirical ML; the reference class of a \"sharp-left turn\" is different from something empirically verifiable as it is unclearly defined, so a good question is how we should turn one into the other. This question of how we take recursive self-improvement, inner misalignment and agent foundations into empirically verifiable ML experiments is actually something that most of the people I know in AI Alignment are currently actively working on.</p><p><a href=\"https://www.lesswrong.com/posts/cAC4AXiNC5ig6jQnc/understanding-and-controlling-a-maze-solving-policy-network\">This post </a>from Alexander Turner is a great example of doing this as they try \"<a href=\"https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget\">just retargeting the search</a>\"</p><p>Other people are trying other things, such as <a href=\"https://www.lesswrong.com/posts/9fL22eBJMtyCLvL7j/soft-optimization-makes-the-value-target-bigger\">bounding the maximisation in RL into quantilisers</a>. This would, in turn, make AI more \"content\" with not maximising. (<a href=\"https://forum.effectivealtruism.org/posts/YrXZ3pRvFuH8SJaay/reflecting-on-the-last-year-lessons-for-ea-opening-keynote\">fun parallel to how utilitarianism shouldn't be unbounded</a>)</p><p>I could go on with examples, but what I really want to say here is that alignment researchers are doing things; it's just hard to realise why they're doing things when you're not doing alignment research yourself. (If you want to start, book my <a href=\"https://calendly.com/jonas-hallgren\">calendly</a> and I might be able to help you.)</p><p>So what does this mean for an average person? You can make a huge difference by going in and engaging with arguments and coming up with counter-examples, experiments and theories of what is actually going on.&nbsp;</p><p>I just want to say that it's most likely paramount to engage with the existing alignment research landscape before as it's free information and easy to fall into traps if you don't. (a good resource for avoiding some traps is John's <a href=\"https://www.lesswrong.com/s/TLSzP4xP42PPBctgw\">Why Not Just</a> sequence)&nbsp;</p><p>There's a couple of years worth of research there; it is not worth rediscovering from the ground up. Still, this shouldn't stop you, go and do it; you don't need a <a href=\"https://www.lesswrong.com/posts/dhj9dhiwhq3DX6W8z/hero-licensing\">hero licence</a>.</p>", "parentCommentId": null, "user": {"username": "Jonas Hallgren"}}, {"_id": "pYuzoXhBMrbSNoCKY", "postedAt": "2023-03-29T18:12:14.347Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<blockquote><p>It seems plausible that there are \u2265100,000 researchers working on ML/AI in total. That\u2019s a ratio of ~300:1, capabilities researchers:AGI safety&nbsp;researchers.</p></blockquote><p>&nbsp;</p><blockquote><p>Barely anyone is going for the throat of solving the core difficulties of scalable alignment. Many of the people who are working on alignment are doing blue-sky theory, pretty disconnected from actual ML models.</p></blockquote><p>One question I'm always left with is: what is the boundary between being an AGI safety researcher and a capabilities researcher?</p><p>For instance, My friend is getting his PhD in machine learning, he barely knows about EA or LW, and definitely wouldn't call himself a safety researcher. However, when I talk to him, it seems like the vast majority of his work deals with figuring out how ML systems act when put in foreign situations wrt the training data.&nbsp;</p><p>I can't claim to really understand what he is doing but it sounds to me a lot like safety research. And it's not clear to me this is some \"blue-sky theory\". A lot of the work he does is high-level maths proofs, but he also does lots of interfacing with ml systems and testing stuff on them. Is it fair to call my friend a capabilities researcher?</p>", "parentCommentId": null, "user": {"username": "Charles_Guthmann"}}, {"_id": "4pFFAehvYhgwL3TmH", "postedAt": "2023-03-29T21:35:52.431Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Going to say something seemingly-unpopular in a tone that usually gets downvoted but I think needs to be said anyway:</p><p>This stat is why I still have hope: 100,000 capabilities researchers vs 300 alignment researchers.<br><br>Humanity has not tried to solve alignment yet.<br><br>There's no cavalry coming - <i>we</i> are the cavalry.&nbsp;<br><br>I am sympathetic to fears of a new alignment researchers being net negative, and I think plausibly the entire field has, so far, been net negative, but guys, there are <i>100,000 capabilities researchers</i> now! One more is a drop in the bucket.<br><br>If you're still on the sidelines, go post that idea that's been gathering dust in your Google Docs for the last six months. &nbsp;Go fill out that fundraising application.&nbsp;<br><br>We've had enough fire alarms. It's time to <i>act</i>.</p>", "parentCommentId": null, "user": {"username": "EmersonSpartz"}}, {"_id": "Jscne9PiEKtHMCSWR", "postedAt": "2023-03-30T01:38:34.070Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<blockquote><p>ML systems act when put in foreign situations wrt the training data.&nbsp;</p></blockquote><p><br>Could you elaborate on this more? My guess is that they could be working on the ML ethics side of things, which is great, but different than the Safety problem.</p>", "parentCommentId": "pYuzoXhBMrbSNoCKY", "user": {"username": "satpathyakash"}}, {"_id": "erYBxesg7aheE9Dv5", "postedAt": "2023-03-30T03:58:26.000Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I don't remember specifics but he was looking if you could make certain claims on models acting a certain way on data not in the training data based on the shape and characteristics about the training data. I know that's vague sorry, I'll try to ask him and get a better summary.&nbsp;</p>", "parentCommentId": "Jscne9PiEKtHMCSWR", "user": {"username": "Charles_Guthmann"}}, {"_id": "pFdg7xD4sm7nYeRTm", "postedAt": "2023-03-30T09:05:23.930Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>(On phone so rushed reply)</p>\n<p>Thanks for this, it's well written and compelling.</p>\n<p>Who needs to do what differently do you think?</p>\n<p>Do you have object level recommendations for specific audiences?</p>\n<p>For instance a call for more of a specific type of projects to increase the number of people working on AI Safety, their quality of work or coordination etc?</p>\n", "parentCommentId": null, "user": {"username": "Peterslattery"}}, {"_id": "8SGGGi8hZh66mzXFJ", "postedAt": "2023-03-30T09:06:15.491Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<blockquote><p>The challenge isn\u2019t figuring out some complicated, nuanced utility function that \u201crepresents human values\u201d; the challenge is getting AIs to do what it says on the tin\u2014to reliably do whatever a human operator tells them to do.</p></blockquote><p>Why do you think this? I infer for what I've seen written in other posts and comments that this is a common belief but I don't find the reasons why.&nbsp;</p><p>The fact that there are specific really difficult problems with aligning ML systems doesn't mean that the original really difficult problem with finding and specifying the objectives that we want for a superintelligence were solved.</p><p>I hate it because it makes it seems like alignment is a technical problem that can be solved by a single team and as you put it in your <a href=\"https://forum.effectivealtruism.org/posts/Ackzs8Wbk7isDzs2n/want-to-win-the-agi-race-solve-alignment\">other post</a> we should just race and win against the bad guys.&nbsp;</p><p>I could try to envision what type of AI you are thinking of and how would you use it, but I would prefer if you tell me. So, what would you ask your aligned AGI to do and how would it interpret that? And how are you so sure that most alignment researchers would ask it the same things as you?</p>", "parentCommentId": null, "user": {"username": "Patricio"}}, {"_id": "bcrchv6uBGDruKFrM", "postedAt": "2023-03-30T13:23:49.466Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>My gut reaction when reading this comment:</p><p>This comment looks like it's written in an attempt to \"be inspirational\", not an attempt to share a useful insight, or ask a question.</p><p>I hope this doesn't sound unkind. I recognise that there can be value in being inspirational, but it's not what I'm looking for when I'm reading these comments.</p>", "parentCommentId": "4pFFAehvYhgwL3TmH", "user": {"username": "Sanjay"}}, {"_id": "CiJBrkmyDEBGQ7fCN", "postedAt": "2023-03-30T14:49:39.893Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I can't get past the feeling that all the doom will flow through an asymptote of imperfect alignment. How can scalable alignment ever be watertight <i>enough</i> for x-risk to drop to insignificant levels? Especially given the (ML-based) engineering approach suggested. It sounds like formal, verifiable, proofs of existential safety won't ever be an end product of all this. How long do we last in a world like that, where AI capabilities continue improving up to physical limits? Can the acute risk period really be brought to a close this way?</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "rZfp3aCwGR5cqENnz", "postedAt": "2023-03-30T16:46:31.911Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I have only dabbled in ML but this sounds like he may just be testing to see how generalizable models are / evaluating whether they are overfitting or underfitting the training data based on their performance on test data(data that hasn\u2019t been seen by the model and was withheld from the training data). This is often done to tweak the model to improve its performance.</p>\n", "parentCommentId": "pYuzoXhBMrbSNoCKY", "user": {"username": "more better "}}, {"_id": "Ar7zmRAo7kxLjXFFz", "postedAt": "2023-03-30T18:53:56.249Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Great post. What I find most surprising is how small the scalable alignment team at OpenAI is. Though similar teams in DeepMind and Anthropic are probably bigger.</p>\n", "parentCommentId": null, "user": {"username": "Stephen McAleese"}}, {"_id": "BE6JaAbwfkShQw9D5", "postedAt": "2023-03-30T18:57:01.302Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I definitely have very little idea what I\u2019m talking about but I guess part of my confusion is inner alignment seems like a capability of ai? Apologies if I\u2019m just confused.</p>\n", "parentCommentId": "rZfp3aCwGR5cqENnz", "user": {"username": "Charles_Guthmann"}}, {"_id": "tMMHBJLYeNbb4RcwL", "postedAt": "2023-03-30T18:58:03.701Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<blockquote><p>The challenge isn\u2019t figuring out some complicated, nuanced utility function that \u201crepresents human values\u201d; the challenge is getting AIs to do what it says on the tin\u2014to reliably do whatever a human operator tells them to do.</p></blockquote><p>IMO, this implies we need to design AI systems so that they <strong>satisfice</strong> rather than maximize: perform a requested task at a requested performance level <i>but no better than that</i> and with a requested probability <i>but no more likely than that.</i></p>", "parentCommentId": null, "user": {"username": "Jobst Heitzig (vodle.it)"}}, {"_id": "Lt45M2zaRGmPZxoPG", "postedAt": "2023-03-30T19:26:30.320Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Thanks for the feedback. I tried to do both. I think the doomerism levels are so intense right now and need to be balanced out with a bit of inspiration.</p>\n<p>I worry that the doomer levels are so high EAs will be frozen into inaction and non-EAs will take over from here. This is the default outcome, I think.</p>\n", "parentCommentId": "bcrchv6uBGDruKFrM", "user": {"username": "EmersonSpartz"}}, {"_id": "B2oZ2KRmtPwcCo8Pd", "postedAt": "2023-03-30T19:35:37.658Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Your first comment at the top was better, it seems you were inspired. What in the entire universe of possibilities could be wrong with being inspirational?...the entire EA movement is hoping to inspire people to give and act toward the betterment of humankind...before any good idea can be implemented, there must be something to inspire a person to stand up and act. Wow, you're mindset is so off of human reality. Is this an issue of post vs. comments?...who cares if someone adds original material in comments, it's a conversation. Humans are not data in a test tube...the human spirit is another way of saying, \"inspired human\"...when inspired humans think, good things can happen. It is the evil of banality that is so frightening. Uninspired intellect is probably what will kill us all if it's digital.&nbsp;</p>", "parentCommentId": "bcrchv6uBGDruKFrM", "user": {"username": "JeffreyK"}}, {"_id": "hPZG425rPaGYHgQNR", "postedAt": "2023-03-31T00:18:27.173Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Sanjay, I just realized you were the top comment, and now I notice that I feel confused, because your comment directly inspired me to express my views in a tone that was more opinionated and less-hedgy.<br><br>I appreciate - no, I *love* - EA's truth seeking culture but I wish it were more OK to add a bit of Gryffindor to balance out the Ravenclaw.</p>", "parentCommentId": "B2oZ2KRmtPwcCo8Pd", "user": {"username": "EmersonSpartz"}}, {"_id": "sMvyjDGwWvmbcQihb", "postedAt": "2023-04-05T02:22:57.669Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I agree with this post. I've been reading many more papers since first entering this field because I've been increasingly convinced of the value of treating alignment as an engineering problem and pulling insights from the literature. I've also been trying to do more thinking about how to update on the current paradigm from the classic Yud and Bostrom alignment arguments. In this respect, I applaud Quintin Pope for his work.</p><p>This week, I will send a grant proposal to continue my work in alignment. I'd be grateful if you could look at my proposal and provide some critique. It would be great to have an outside view (like yours) to give feedback on it.</p><p>Current short summary: \"This project comprises two main interrelated components: accelerating AI alignment research by integrating large language models (LLMs) into a research system, and conducting direct work on alignment with a focus on interpretability and steering the training process towards aligned AI. The \"accelerating alignment\" agenda aims to impact both conceptual and empirical aspects of alignment research, with the ambitious long-term goal of providing a massive speed-up and unlocking breakthroughs in the field. The project also includes work in interpretability (using LLMs for interpreting models; auto-interpretability), understanding agency in the current deep learning paradigm, and designing a robustly aligned training process. The tools built will integrate seamlessly into the larger alignment ecosystem. The project serves as a testing ground for potentially building an organization focused on using language models to accelerate alignment work.\"</p><p>Please send me a DM if you'd like to give feedback!</p>", "parentCommentId": null, "user": {"username": "jaythibs"}}, {"_id": "HYubSAukirnELtLCp", "postedAt": "2023-04-10T13:01:11.478Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>This comes late, but I appreciate this post and am curating it. I think the core message is an important one, some sections can help people develop intuitions for what the problems are, and the post is written in an accessible way (which is often not the case for AI safety-related posts). As others noted, the post also made a bunch of specific claims that others can disagree with as opposed to saying vague things or hedging a lot, which I also appreciate (see also <a href=\"https://forum.effectivealtruism.org/posts/oRx3LeqFdxN2JTANJ/epistemic-legibility\">epistemic legibility</a>).&nbsp;</p><p>I share Charlie Guthmann's question <a href=\"https://forum.effectivealtruism.org/posts/5LNxeWFdoynvgZeik/nobody-s-on-the-ball-on-agi-alignment?commentId=pYuzoXhBMrbSNoCKY\">here</a>: I get the sense that some work is in a fuzzy grey area between alignment and capabilities, so comparing the amount of work being done on safety vs. capabilities is difficult. I should also note that I don't think all capabilities work can be defended as safety-relevant (see also my own post on <a href=\"https://forum.effectivealtruism.org/posts/f2qojPr8NaMPo2KJC/beware-safety-washing\">safety-washing</a>).&nbsp;</p><p>...</p><p><i>Quick note: I know Leopold \u2014 I don't think this influenced my decision to curate the post, but FYI.&nbsp;</i></p>", "parentCommentId": null, "user": {"username": "Lizka"}}, {"_id": "iMZwhf28xwBfCzsMX", "postedAt": "2023-04-10T15:37:16.362Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<blockquote><p><i>\"...or block deployment when we actually really should deploy, e.g. to beat China.\"</i></p></blockquote><p>What the heck? Keep me out of your \"we\". I'm vehemently against framing this as a nationalist us-them issue. If instead you <i>meant</i> something entirely reasonable like \"e.g. so Xi doesn't become god-emperor,\" <i>then say so</i>. Leave \"China\" out of it, that's where my friends live.</p><p>Otherwise, nice post btw.</p>", "parentCommentId": null, "user": {"username": "Anomalous"}}, {"_id": "yC2NhmXHWo595YfRi", "postedAt": "2023-04-10T18:52:10.360Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I'm honestly curious what motivates people to support this framing. I don't live in an English-speaking country, and for all I know, I might have severely misunderstood broader EA culture. Are people here sincerely feeling nationalistic (a negative term in Norway) about this? I'd be appreciative if somebody volunteered to help me understand.</p>", "parentCommentId": "iMZwhf28xwBfCzsMX", "user": {"username": "Anomalous"}}, {"_id": "rGLna3aQ8AHtmpbfT", "postedAt": "2023-04-10T19:22:52.436Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I can speak for myself: I want AGI, if it is developed, to reflect the best possible values we have currently (i.e. liberal values<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1wqmp1da2i1i\"><sup><a href=\"#fn1wqmp1da2i1i\">[1]</a></sup></span>), and I believe it's likely that an AGI system developed by an organization based in the free world (the US, EU, Taiwan, etc.) would embody better values than one developed by one based in the People's Republic of China. There is a widely held belief in science and technology studies that all technologies have <a href=\"https://montrealethics.ai/embedding-values-in-artificial-intelligence-ai-systems/\">embedded values</a>; the most obvious way values could be embedded in an AI system is through its objective function. It's unclear to me how much these values would differ if the AGI were developed in a free country versus an unfree one, because a lot of the AI systems that the US government uses could also be used for oppressive purposes (and arguably already are used in oppressive ways by the US).</p><p>Holden Karnofsky calls this the \"<a href=\"https://www.cold-takes.com/making-the-best-of-the-most-important-century/\">competition frame</a>\" - in which it matters most <i>who</i> develops AGI. He contrasts this with the \"caution frame\", which focuses more on whether AGI is developed in a rushed way than whether it is misused. Both frames seem valuable to me, but Holden warns that most people will gravitate toward the competition frame by default and neglect the caution one.</p><p>Hope this helps!</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1wqmp1da2i1i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1wqmp1da2i1i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Fwiw I do believe that liberal values can be improved on, especially in that they seldom include animals. But the foundation seems correct to me: centering every individual's right to life, liberty, and the pursuit of happiness.</p></div></li></ol>", "parentCommentId": "yC2NhmXHWo595YfRi", "user": {"username": "evelynciara"}}, {"_id": "SMxxvnzXxJtBxD9iT", "postedAt": "2023-04-10T19:26:06.194Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Like an SLA (service level agreement)!</p>", "parentCommentId": "tMMHBJLYeNbb4RcwL", "user": {"username": "evelynciara"}}, {"_id": "tZEBukcRa5yF39HkN", "postedAt": "2023-04-10T20:55:24.778Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Not exactly. A typical SLA only contains a lower bound, that would still allow for maximization. The program for a satisficer in the sense I meant it would states that the AL system really aims to do no better than requested. So, for example, quantilizers would not qualify since they might still (by chance) choose that action which maximizes return.</p>\n", "parentCommentId": "SMxxvnzXxJtBxD9iT", "user": {"username": "Jobst Heitzig (vodle.it)"}}, {"_id": "KSaZ2NguEF8w93FhX", "postedAt": "2023-04-11T02:45:50.838Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Leopold - thanks for a clear, vivid, candid, and galavanizing post. I agree with about 80% of it.&nbsp;</p><p>However, I don't agree with your central premise that alignment is solvable. We <i>want</i> it to be solvable. We believe that we <i>need</i> it to be solvable (or else, God forbid, we might have to actually <i>stop</i> AI development for a few decades or centuries).&nbsp;</p><p>But that doesn't mean it is solvable. And we have, in my opinion, some pretty compelling reasons to think that it not solvable even in principle, (1) given the diversity, complexity, and ideological nature of many human values (which I've written about in other EA Forum posts, and elsewhere), (2) given the deep game-theoretic conflicts between human individuals, groups, companies, and nation-states (which cannot be waved away by invoking Coherent Extrapolated Volition, or 'dontkilleveryoneism', or any other notion that sweeps people's profoundly divergent interests under the carpet), and (3) given that humans are not the only sentient stakeholder species that AI would need to be aligned with (advanced AI will have implications for every other of the 65,000 vertebrate species on Earth, and most of the 1,000,000+ invertebrate species, one way or another).&nbsp;</p><p>Human individuals aren't aligned with each other. Companies aren't aligned with each other. Nation-states aren't aligned with each other. Other animal species aren't aligned with humans, or with each other. There is no reason to expect that any AI systems could be 'aligned' with the totality of other sentient life on Earth. Our Bayesian prior, based on the simple fact that different sentient beings have different interests, values, goals, and preferences, must be that AI alignment with 'humanity in general', or 'sentient life in general', is simply not possible. Sad, but true.</p><p>I worry that 'AI alignment' as a concept, or narrative, or aspiration, is just promising enough that it encourages the AI industry to charge full steam ahead (in hopes that alignment will be 'solved' before AI advances to much more dangerous capabilities), but it is not delivering nearly enough workable solutions to make their reckless accelerationism safe. We are getting the worst of both worlds -- a credible illusion of a path towards safety, without any actual increase in safety.</p><p>In other words, the assumption that 'alignment is solvable' might be a very dangerous X-risk amplifier, in its own right. It emboldens the AI industry to accelerate. It gives EAs (probably) false hope that some clever technical solution can make humans all aligned with each other, and make machine intelligences aligned with organic intelligences. It gives ordinary citizens, politicians, regulators, and journalists the impression that some very smart people are working very hard on making AI safe, in ways that will probably work. It may be leading China to assume that some clever Americans are already handling all those thorny X-risk issues, such that China doesn't really need to duplicate those ongoing AI safety efforts, and will be able to just copy our alignment solutions once we get them.</p><p>If we take seriously the possibility that alignment might <i>not</i> be solvable, we need to rethink our whole EA strategy for reducing AI X-risk. This might entail EAs putting a much stronger emphasis on slowing or stopping further AI development, at least for a while. We are continually told that 'AI is inevitable', 'the genie is out of the bottle', 'regulation won't work', etc. I think too many of us buy into the over-pessimistic view that there's absolutely nothing we can do to stop AI development, while also buying into the over-optimistic view that alignment is possible -- if we just recruit more talent, work a little more, get a few more grants, think really hard, etc.&nbsp;</p><p>I think we should reverse these optimisms and pessimisms. We need to rediscover some optimism that the 8 billion people on Earth can pause, slow, handicap, or stop AI development by the 100,000 or so AI researchers, devs, and entrepreneurs that are driving us straight into a Great Filter. But we need to rediscover some pessimism about the concept of 'AI alignment' itself.&nbsp;</p><p>In my view, the burden of proof should be on those who think that 'AI alignment with human values in general' is a solvable problem. I have seen no coherent argument that it is solvable. I've just seen people desperate to believe that it is solvable. But that's mostly because the alternative seems so alarming, i.e., the idea that (1) the AI industry is increasingly imposing existential risks on us all, (2) it has a lot of money, power, talent, influence, and hubris, (3) it will not slow down unless we make it slow down, and (4) slowing it down will require EAs to shift to a whole different set of strategies, tactics, priorities, and mind-sets than we had been developing within the 'alignment' paradigm.&nbsp;</p>", "parentCommentId": null, "user": {"username": "geoffreymiller"}}, {"_id": "iaSrpixYecpwiJZy2", "postedAt": "2023-04-11T04:08:55.868Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I agree that the very strong sort of alignment you describe - with the Coherent Extrapolated Volition of humanity, or the collective interest of all sentient beings, or The Form of The Good - is probably impossible and perhaps ill-posed. Insofar as we need this sort of aligned AI for things to go as well as they possibly could, they won't.&nbsp;</p><p>But I don't see why that's the only acceptable target. Aligning a superintelligence with the will of basically any psychologically normal human being (narrower than any realistic target except perhaps a profit-maximizer - in which case yeah, we're doomed) would still be an <i>ok </i>outcome for humans: it certainly doesn't end in paperclips. And alignment with someone even slightly inclined towards impartial benevolence probably goes much better than the status quo, especially for the extremely poor.</p><p>(Animals are at much more risk here, but their current situation is also much worse: I'm extremely uncertain how a far richer world would treat factory farming)</p>", "parentCommentId": "KSaZ2NguEF8w93FhX", "user": {"username": "yefreitor"}}, {"_id": "KKttYaMNeAHHQmPYh", "postedAt": "2023-04-11T10:34:12.224Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Yes, I think we can go further and say that alignment of a superintelligent AGI even with a single individual human may well be impossible. Is such a thing mathematically verifiable as completely watertight, given the orthogonality thesis, basic AI drives and mesaoptimisation? And if it's not watertight, then all the doom flows through the gaps of imperfect, thought to be \"good enough\", alignment. We need a global moratorium on AGI development. This year.</p>", "parentCommentId": "iaSrpixYecpwiJZy2", "user": {"username": "Greg_Colbourn"}}, {"_id": "FGYNkCLYXmuuX6zBY", "postedAt": "2023-04-11T19:18:05.783Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I'm very supportive of this post. Also I will shamelessly share here a sequence I posted in February called \"The Engineer's Interpretability Sequence\". One of the main messages of the sequence could be described as how existing mechanistic interpretability research is not on the ball.&nbsp;</p><p><a href=\"https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7\">https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7</a>&nbsp;</p>", "parentCommentId": null, "user": null}, {"_id": "oqfuuqagGXaxTCPns", "postedAt": "2023-04-11T19:50:25.650Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>AI alignment is a myth; it assumes that humans are a single homogeneous organism and that AI will be one also. Humans have competing desires and interests and so will the AGIs created by them, none of which will have independent autonomous motivations without being programmed to develop them.</p>\n<p>Even on an individual human level alignment is a relative concept. Whether a human engages in utilitarian or deontological reasoning depends on their conception of the self/other (whether they would expect to be treated likewise).</p>\n<p>Regarding LLM specific risk, they are not currently an intelligence threat. Like any tech however they can be deployed by malicious actors in the advancement of arbitrary goals. One reason OpenAI are publicly trialling the models early is to help everyone including researchers learn to navigate their use cases, and develop safeguards to hinder exploitation.</p>\n<p>Addressing external claims, limiting research is a bad strategy given population dynamics; the only secure option for liberal nations is in the direction of knowledge.</p>\n", "parentCommentId": null, "user": {"username": "Richard Baxter"}}, {"_id": "px4xGx8RysHvbwPXd", "postedAt": "2023-04-12T11:13:56.851Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I think humans may indeed find ways to scale up their control over successive generations of AIs for a while, and successive generations of AIs may be able to exert some control over their successors, and so on. However, I don't see how at the end of a long chain of successive generations we could be left with anything that cares much about our little primate goals. Even if individual agents within that system still cared somewhat about humans, I doubt the collective behavior of the society of AIs overall would still care, rather than being driven by its own competitive pressures into weird directions.</p>\n<p>An analogy I often give is to consider our fish ancestors hundreds of millions of years ago. Through evolution, they produced somewhat smarter successors, who produced somewhat smarter successors, and so on. At each point along that chain, the successors weren't that different from the previous generation; each generation might have said that they successfully aligned their successors with their goals, for the most part. But over all those generations, we now care about things dramatically different from what our fish ancestors did (e.g., worshipping Jesus, inclusion of trans athletes, preventing children from hearing certain four-letter words, increasing the power and prestige of one's nation). In the case of AI successors, I expect the divergence may be even more dramatic, because AIs aren't constrained by biology in the way that both fish and humans are. (OTOH, there might be less divergence if people engineer ways to reduce goal drift and if people can act collectively well enough to implement them. Even if the former is technically possible, I'm skeptical that the latter is socially possible in the real world.)</p>\n<p>Some transhumanists are ok with dramatic value drift over time, as long as there's a somewhat continuous chain from ourselves to the very weird agents who will inhabit our region of the cosmos in a million years. But I don't find it very plausible that in a million years, the powerful agents in control of the Milky Way will care that much about what certain humans around the beginning of the third millennium CE valued. Technical alignment work might help make the path from us to them more continuous, but I'm doubtful it will avert human extinction in the long run.</p>\n", "parentCommentId": "iaSrpixYecpwiJZy2", "user": {"username": "Brian_Tomasik"}}, {"_id": "efRNGkkA3fAXt5JBw", "postedAt": "2023-04-13T05:15:01.385Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Anecdata: thanks for curating, I didn\u2019t read this when it first came through and now that I did, it really impacted me.</p>\n<p>Edit:\nComing back after approaching it on LessWrong and now I\u2019m very confused again - seems to have been much less well received. What someone here says is, \u201cgreat balance of technical and generally legible content\u201d over there might be considered \u201cstrawmanning and frustrating\u201d, and I really don\u2019t know what to think.</p>\n", "parentCommentId": "HYubSAukirnELtLCp", "user": {"username": "Phib"}}, {"_id": "YpMLyHZscWuvHMh2W", "postedAt": "2023-04-13T06:22:58.887Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p><i>Copy-pasting here from LW.</i><br><br>Sorry but my rough impression from the post is you seem to be at least as confused about where the difficulties are as average of alignment researchers you think are not on the ball - and the style of somewhat strawmanning everyone &amp; strong words is a bit irritating.<br><br>Maybe I'm getting it wrong, but it seems the model you have for why everyone is not on the ball is something like <i>\"people are approaching it too much from a theory perspective, and promising approach is very close to how empirical ML capabilities research works\" &amp; \"this is a type of problem where you can just throw money at it and attract better ML talent\".</i><br><br>I don't think these two insights are promising.<br><br>Also, again, maybe I'm getting it wrong, but I'm confused how similar you are imagining the current systems to be to the dangerous systems. It seems either the superhuman-level problems (eg not lying in a way no human can recognize) are somewhat continuous with current problems (eg not lying), and in that case it is possible to study them empirically. Or they are not. &nbsp;But different parts of the post seem to point in different directions. (Personally I think the <i>problem</i> is somewhat continuous, but many of the human-in-the-loop solutions are not, and just break down.)<br><br>Also, with what you find promising I'm confused what do you think the 'real science' &nbsp;to aim for is &nbsp;- on one hand it seems you think the closer the thing is to <i>how ML is done in practice</i> the more <i>real science</i> it is. On the other hand, in your view <i>all deep learning progress has been empirical, often via dumb hacks and intuitions </i>(this isn't true imo).&nbsp;</p>", "parentCommentId": null, "user": {"username": "Jan_Kulveit"}}, {"_id": "35GiqyaM5uLAyznGB", "postedAt": "2023-04-13T06:45:06.971Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>In my view this is a bad decision.&nbsp;<br><br>As I <a href=\"https://www.lesswrong.com/posts/uqTJ7mQqRpPejqbfN/nobody-s-on-the-ball-on-agi-alignment?commentId=9TuLBFFSdNWQdy8hF#comments\">wrote on LW&nbsp;</a><br><br><i>Sorry but my rough impression from the post is you seem to be at least as confused about where the difficulties are as average of alignment researchers you think are not on the ball - and the style of somewhat strawmanning everyone &amp; strong words is a bit irritating.</i><br><br>In particular I don't appreciate the epistemic of these moves together<br><br>1. Appeal to seeing thinks from close proximity. <i>Then I got to see things more up close. And here\u2019s the thing: nobody\u2019s actually on the friggin\u2019 ball on this one!</i><br>2. Straw-manning and weakmaning what almost everyone else thinks and is doing<br>3. Use of an emotionally compelling words like <i>'real science'&nbsp;</i> for vaguely defined subjects where the content may be the opposite of what people imagine. Is the empirical alchemy-style ML type of research what's advocated for as the <i>real science</i>?<br>4. What overall sounds more like <i>the aim is to persuade, rather than explain</i><br><br>I think curating this signals this type of bad epistemics is fine, as long as you are strawmanning and misrepresenting others in a legible way and your writing is persuasive. Also there is no need to actually engage with existing arguments, you can just claim <i>seeing things more up close.</i><br><br>Also to what extent are moderator decisions influenced by status and centrality in the community...<br>... if someone new and non-central to the community came up with this brilliant set of ideas how to solve AI safety:<br>1. everyone working on it is not on the ball. why? they are all working on wrong things!<br>2. promising is to do something very close to how empirical ML capabilities research works<br>3. this is a type of problem where you can just throw money at it and attract better ML talent<br>... I doubt this would have a high chance of becoming curated.</p>", "parentCommentId": "HYubSAukirnELtLCp", "user": {"username": "Jan_Kulveit"}}, {"_id": "wPoFRWgjpHLsPYr6o", "postedAt": "2023-04-13T15:15:05.351Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I've been thinking about this very thing for quite some time, and have been thinking up a concrete interventions to help the ML community / industry grasp this. DM me if you're interested to discuss further.</p>", "parentCommentId": "KSaZ2NguEF8w93FhX", "user": {"username": "howdoyousay?"}}, {"_id": "TFpjR5qXj6FLKCFMp", "postedAt": "2023-04-13T16:19:12.403Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Hi Brian, thanks for this reminder about the longtermist perspective on humanity's future. I agree that in a million years, whatever sentient beings that are around may have little interest or respect for the values that humans happen to have now.</p><p>However, one lesson from evolution is that most mutations are harmful, most populations trying to spread into a new habitats fail, and most new species go extinct within about a million years. There's huge survivorship bias in our understanding of natural history.&nbsp;</p><p>I worry that this survivorship bias leads us to radically over-estimate the likely adaptiveness and longevity of any new digital sentiences and any new transhumanist innovations. New autonomous advanced AIs are likely to be extremely fragile, just because most new complex systems that haven't been battle-tested by evolution are extremely fragile.&nbsp;</p><p>For this reason, I think we would be foolish to rush into any radical transhumanism, or any more advanced AI systems, until we have explored human potential further, and until we have been successfully, resiliently multi-planetary, if not multi-stellar. Once we have a foothold in the stars, and humanity has reached some kind of asymptote in what un-augmented humanity can accomplish, then it might make sense to think about the 'next phase of evolution'. Until then, any attempt to push sentient evolution faster will probably result in calamity.</p>", "parentCommentId": "px4xGx8RysHvbwPXd", "user": {"username": "geoffreymiller"}}, {"_id": "E5fLuvsT5YJXMMQGj", "postedAt": "2023-04-14T14:44:00.946Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Thanks. :) I'm personally not one of those transhumanists who welcome the transition to weird posthuman values. I would prefer for space not to be colonized at all in order to avoid astronomically increasing the amount of sentience (and therefore the amount of expected suffering) in our region of the cosmos. I think there could be some common ground, at least in the short run, between suffering-focused people who don't want space colonized in general and existential-risk people who want to radically slow down the pace of AI progress. If it were possible, the Butlerian Jihad solution could be pretty good both for the AI doomers and the negative utilitarians. Unfortunately, it's probably not politically possible (even domestically much less internationally), and I'm unsure whether half measures toward it are net good or bad. For example, maybe slowing AI progress in the US would help China catch up, making a competitive race between the two countries more likely, thereby increasing the chance of catastrophic Cold War-style conflict.</p>\n<p>Interesting point about most mutants not being very successful. That's a main reason I tend to imagine that the first AGIs who try to overpower humans, if any, would plausibly fail.</p>\n<p>I think there's some difference in the case of intelligence at the level of humans and above, versus other animals, in adaptability to new circumstances, because human-level intelligence can figure out problems by reason and doesn't have to wait for evolution to brute-force its way into genetically based solutions. Humans have changed their environments dramatically from the ancestral ones without killing themselves (yet), based on this ability to be flexible using reason. Even the smarter non-human animals display some amount of this ability (cf. the Baldwin effect). (A web search shows that you've written about the Baldwin effect and how being smarter leads to faster evolution, so feel free to correct/critique me.)</p>\n<p>If you mean that posthumans are likely to be fragile at the collective level, because their aggregate dynamics might result in their own extinction, then that's plausible, and it may happen to humans themselves within a century or two if current trends continue.</p>\n", "parentCommentId": "TFpjR5qXj6FLKCFMp", "user": {"username": "Brian_Tomasik"}}, {"_id": "RWYrtKry3cuxcLQnD", "postedAt": "2023-04-14T18:58:18.517Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<blockquote><p>...we have, in my opinion, some pretty compelling reasons to think that it not solvable even in principle, (1) given the diversity, complexity, and ideological nature of many human values... There is no reason to expect that any AI systems could be 'aligned' with the totality of other sentient life on Earth.</p></blockquote><p>One way to decompose the alignment question is into 2 parts:</p><ol><li>Can we aim ASI at all? (e.g. Nate Soares' <a href=\"https://intelligence.org/2023/02/02/what-i-mean-by-alignment-is-in-large-part-about-making-cognition-aimable-at-all/\">What I mean by \u201calignment is in large part about making cognition aimable at all\u201d</a>)</li><li>Can we align it with human values? (the blockquote is an example of this)</li></ol><p>Folks at e.g. MIRI think (1) is the hard problem and (2) isn't as hard; folks like you think the opposite. Then you all talk past each other. (\"You\" isn't aimed at literally you in particular, I'm summarizing what I've seen.) I don't have a clear stance on which is harder; I just wish folks would engage with the best arguments from each side.</p>", "parentCommentId": "KSaZ2NguEF8w93FhX", "user": {"username": "Mo Nastri"}}, {"_id": "cJLQSnMZgiNhAWGCS", "postedAt": "2023-04-14T19:35:05.510Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Mo - you might be right about what MIRI thinks will be hard. I'm not sure; it often seems difficult to understand what they write about these issues, since it's often very abstract and seems not very grounded in specific goals and values that AIs might need to implement. I do think the MIRI-type approach radically under-estimates the difficulty of your point number 2.</p><p>On the other hand, I'm not at all confident that point number 1 will be easy. My hunch is that both 1 and 2 will prove surprisingly hard. Which is a good reason to pause AI research until we make a lot more progress on both issues. (And if we don't make dramatic progress on both issues, the 'pause' should remain in place as long as it takes. Which could be decades or centuries.)</p>", "parentCommentId": "RWYrtKry3cuxcLQnD", "user": {"username": "geoffreymiller"}}, {"_id": "2HjDDAusJuDT88gyt", "postedAt": "2023-04-14T19:37:51.587Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Brian - that all seems reasonable. Much to think about!</p>", "parentCommentId": "E5fLuvsT5YJXMMQGj", "user": {"username": "geoffreymiller"}}, {"_id": "hSM6qzmzuJKTJbHpq", "postedAt": "2023-04-15T00:51:29.855Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>[<i>epistemic status:</i> half-joking]</p><blockquote><p>There\u2019s no secret elite SEAL team coming to save the day.</p></blockquote><p>Are there any organized groups of alignment researchers who serve as a not-so-secret, normal civilian equivalent of a SEAL team trying their best to save the day, while also trying to make no promises of being some kind of elite, hyper-competent super-team?</p>", "parentCommentId": null, "user": {"username": "Evan_Gaensbauer"}}, {"_id": "7w6RbMmjqGtiF3iEb", "postedAt": "2023-04-15T00:55:42.265Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>At this point, I'll hear out the gameplan to align AGI from any kind of normie SEAL team. We're really scraping the bottom of the barrel right now.&nbsp;</p>", "parentCommentId": "hSM6qzmzuJKTJbHpq", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "HsQwozcrDyqKyN8zr", "postedAt": "2023-04-15T01:33:37.695Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I have so far gotten the same impression that making RLHF work as a strategy by iteratively and kind of gradually scaling it in a very operationally secure way seems like maybe the most promising approach. My viewpoint right now still remains as the one you've expressed about how, while as much as the RLHF++ has going for it in a relative sense, in leaves a lot to be desired in an absolute sense in light of the alignment/control problem for AGI.&nbsp;</p><p>Overall, I really appreciate how this post condenses well in detail what is increasingly common knowledge about just how inadequate are the sum total of major approaches being taken to alignment. I've read analyses with the same current conclusion from several other AGI safety/alignment researchers during the last year or two. Yet where I hit a wall is my strong sense that any alternative approaches could just as easily succumb to most if not all of the same major pitfalls you list the RLHF++ approach of having to contend with. In that sense, I also feel most of your points are redundant, To get specific about how your criticisms of RLHF apply to all the other alignment approaches as well...&nbsp;</p><blockquote><p>This currently feels way too much like \u201cimprovise as we go along and cross our fingers\u201d&nbsp;to be Plan A; this should be Plan B or Plan E.</p></blockquote><p>Whether it's an approach inspired by the paradigms established in light of Christiano, Yudkowsky, interpretability research, or elsewise, I've gotten the sense essentially all alignment researchers honestly feel the same way about whatever approach to RLHF they're taking.</p><blockquote><p>\"It might well not work. I expect this to harvest a bunch of low-hanging fruit[...]This really shouldn't be our only plan.</p></blockquote><p>I understand how it feels like, based on how some people tend to talk about RLHF, and sometimes interpretability, they're implying or suggesting that we'll be fine with just this one approach. At the same time, as far as I'm aware, when you get behind any hype, almost everyone admits that whatever particular approach to alignment they're taking may fail to generalize and shouldn't be the only plan.&nbsp;</p><blockquote><p>It rests on pretty unclear empirical assumptions on how crunchtime will go.</p></blockquote><p>I've gotten the sense that the empirical assumption for how crunchtime will go among researchers taking the RLHF approach is, for lack of a better term, kind of a medium-term forecast for the date of the tipping point for AGI, <i>i.e.,</i> probably at least between 2030 and 2040, as opposed to between 2025 and 2030.&nbsp;</p><p>Given this or that certain chain/sequence of logical assumptions about the trajectory or acceleration of capabilities research, there of course is an intuitive case to be made, on rational-theoretic grounds, for acting/operating under the presumption in practice that forecasts of short(er) AGI timelines, <i>e.g., </i>between 1 and 5 years out, are just correct and the most accurate.&nbsp;</p><p>At the same time, such models for timeline and/or trajectory towards AGI anyone could, just as easily, be totally wrong. Those research teams most dedicated to really solving the control problem for transformative/general AI with the shortest timelines are also acting under assumptions derived from models that also severely lacking any empirical basis.&nbsp;</p><p>As far as I'm aware, there is a combined set of several for-profit startups, and non-profit research organizations, that have been trialing state-of-the-art approaches for prediction markets and forecasting methodologies, especially timelines and trajectories of capabilities research for transformative/general AI.&nbsp;</p><p>During the last few years, they've altogether received, at least, a few million dollars to run so many experiments to determine how to achieve more empirically based models for AI timelines or trajectories. While there may potentially be valuable insights for empirical forecasting methods overall, I'm not aware of any results at all vindicating, literally, any theoretical model for capabilities forecasting.&nbsp;</p><blockquote><p>I\u2019m not sure this plan puts us on track to get to a place where we can be confident that scalable alignment is solved. By default, I\u2019d guess we\u2019d end up in a fairly ambiguous situation.</p></blockquote><p>This is yet just another criticism of the RLHF approach that I understand as just as easily applying to any approach to alignment you've mentioned, and even every remotely significant approach to alignment you didn't mention but I've also encountered.</p><p>You also mentioned, for both the relatively cohesive (set of) approach(es) inspired by Christiano's research, or for more idiosyncratic approaches, a la MIRI, you perceive to be a dead end the very abstract, and almost purely mathematical, approach being taken. That's an understandable and sympathetic take. All things being equal, I'd agree with your proposal for what should be done instead:</p><blockquote><p>We need a concerted effort that matches the gravity of the challenge. The best ML researchers in the world should be working on this! There should be billion-dollar, large-scale efforts with the scale and ambition of Operation Warp Speed or the moon landing or even OpenAI\u2019s GPT-4 team itself working on this problem.</p></blockquote><p>Unfortunately, all things are not equal. The societies we live in will. for foreseeable future. keep operating on a set of very unfortunate incentive structures.&nbsp;</p><p>To so strongly invest into ML-based approaches to alignment research, in practice, often entails working in some capacity of advancing capabilities research even more, especially in industry and the private sector. That's a major reason why, regardless of whatever ways they might be superior, ML-based approaches to alignment are often eschewed.&nbsp;</p><p><i>I.e.,</i> most conscientious alignment researchers don't feel like their field is ready to pivot so fully to ML-based approaches to alignment, without in the process increasing whatever existential risk super-human AGI might pose to humanity, as opposed to decreasing such risk. As harsh as I'm maybe being, I also think the most novel and valuable propositions in this post are your own you've downplayed:</p><blockquote><p>For example, I\u2019m really excited about work like <a href=\"https://arxiv.org/abs/2212.03827\"><u>this recent paper</u></a>&nbsp;(<a href=\"https://arxiv.org/abs/2212.03827\"><u>paper</u></a>, <a href=\"https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without\"><u>blog post on broader vision</u></a>), which prototypes a method to detect \u201cwhether a model is being honest\u201d via unsupervised methods. More than just this specific result, I\u2019m excited about the style:</p><ul><li>Use conceptual thinking to identify methods that might plausibly scale to superhuman methods (here: unsupervised methods, which don\u2019t rely on human supervision)</li><li>Empirically test this with current models.</li></ul><p>I think there\u2019s a lot more to do in this vein\u2014carefully thinking about empirical setups that are analogous to the core difficulties of scalable alignment, and then empirically testing and iterating on relevant ML methods.</p></blockquote><p>My one recommendation is that you don't dwell any longer on so many things in AI alignment as a field most alignment researchers already acknowledge, and get down your proposals for taking an evidence-based approach to expanding the robustness of alignment of unsupervised systems. That's as exciting a new research direction I've heard of in the last year too!</p>", "parentCommentId": null, "user": {"username": "Evan_Gaensbauer"}}, {"_id": "9uebc2nAyghXoHG5L", "postedAt": "2023-04-15T16:42:31.143Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Loved the language in the post! To the point without having to use unnecessary jargon.<br><br>There are two things I'd like you to elaborate on if possible:<br><br>&gt; \"the challenge is getting AIs to do what it says on the tin\u2014to reliably do whatever a human operator tells them to do.\"<br><br>If I understand correctly you imply that there is still a human operator to a superhuman AGI, do you think this is the way that alignment will work out? What I see is that humans have flaws, do we really want to give a \"genie\" / extremely powerful tool to humans that even already struggle with the powerful tools that they have? At least right now these powerful tools are in the hands of the more responsible few, but if it becomes more widely accessible that's very different.<br><br>What do you think of going the direction of developing a \"Guardian AI\", which would still solve the alignment problem using the tools of ML, but involving humans giving up control of the alignment?<br><br>The second one is more practical, which action do you think one should take. I've of course read the recommendations that other people have put out there so far, but would be curious to hear your take on this.&nbsp;<br>&nbsp;</p>", "parentCommentId": null, "user": {"username": "Riccardo"}}, {"_id": "LwvCPLSiRWah5Qz2e", "postedAt": "2023-04-15T23:12:01.780Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<blockquote><p>(Importantly, from my understanding, this isn\u2019t OpenAI being evil or anything like that\u2014OpenAI would love to hire more alignment researchers, but there just aren\u2019t many great researchers out there focusing on this problem.)</p></blockquote><p>Thank you emphasizing you're not implying OpenAI is evil only because some practices at OpenAI may be inadequate. I feel like I shouldn't have to thank you for that, though I do, just to emphasize how backwards the thinking and discourse in the AI safety/alignment community often is when a pall of fear and paranoia is cast on all AI capabilities researchers.&nbsp;</p><p>During a recent conversation about AI alignment with a few others, when I expressed a casual opinion about how AGI labs have some particularly mistaken practice, I too felt a need to clarify I didn't mean to imply that Sundar Pichai or Sam Altman are evil because of it.&nbsp;</p><p>I don't even remember right now what that point of criticism I made was. I don't remember if that conversation was last week or the week before. It hasn't stuck in my mind because it didn't feel that important. It was an offhand comment about a relatively minor mistake AGI labs are making, tangential to the main argument I was trying to make.&nbsp;</p><p>Yet it's telling that I felt a need to clarify I wasn't implying OpenAI or DeepMind is evil, during even just a private conversation. It's telling that you've felt a need to do that in this post. It's a sign of a serious problem in the mindset of at least a minority of the AI safety/alignment community.&nbsp;</p><p>Another outstanding feature of this post is how you've mustered the effort to explain at all why you consider different approaches to alignment to be inadequate. This distinguishes your post from others like it from the the last year. Others who've tried to get across the same point you're making have, instead of explaining their disagreements, have generally alleged almost everyone else in entire field of AI alignment are literally insane.&nbsp;</p><p>That's not helpful for a few reasons. Such a claim is probably not true. It'd be harder to make a more intellectually lazy or unconvincing argument. It counts as someone making a bold, senseless attempt to, arguably, dehumanize hundreds of their peers.&nbsp;</p><p>This isn't just a negligible error from somebody recognized as part of a hyperbolic fringe in AI safety/alignment community. It's direly counterproductive when it comes from leading rationalists, like Eliezer Yudkowsky and Oliver Habryka, who wield great influence in their own right, and are taken very seriously by hundreds of other people. Thank you for writing this post as corrective to that kind of mistake a lot of your close allies have been making too.</p>", "parentCommentId": null, "user": {"username": "Evan_Gaensbauer"}}, {"_id": "ojEktwyY5nijnDt2f", "postedAt": "2023-04-15T23:43:43.862Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<blockquote><p>As others noted, the post also made a bunch of specific claims that others can disagree with as opposed to saying vague things or hedging a lot, which I also appreciate (see also <a href=\"https://forum.effectivealtruism.org/posts/oRx3LeqFdxN2JTANJ/epistemic-legibility\">epistemic legibility</a>).&nbsp;</p></blockquote><p>Thank you for acknowledging this and emphasizing the specific claims being made. I'm guessing you didn't mean to cast aspersions through a euphemism. I'd respect you not being as explicit about it if that is part of what you meant here.&nbsp;</p><p>For my part, though, I think you're understating how much of a problem those other posts are, so I feel obliged to emphasize how the vagueness and hedging in some of those other posts has, wittingly or not, serving to spread hazardous misinformation. To be specific, here's an excerpt from <a href=\"https://forum.effectivealtruism.org/posts/5LNxeWFdoynvgZeik/nobody-s-on-the-ball-on-agi-alignment?commentId=LwvCPLSiRWah5Qz2e\">this other comment I made</a> raising the same concern:</p><blockquote><p>Others who've tried to get across the same point [Leopold is] making have, instead of explaining their disagreements, have generally alleged almost everyone else in entire field of AI alignment are literally insane.<br>[...]<br>It counts as someone making a bold, senseless attempt to, arguably, dehumanize hundreds of their peers.&nbsp;<br><br>This isn't just a negligible error from somebody recognized as part of a hyperbolic fringe in AI safety/alignment community. It's direly counterproductive when it comes from leading rationalists, like Eliezer Yudkowsky and Oliver Habryka, who wield great influence in their own right, and are taken very seriously by hundreds of other people.</p></blockquote>", "parentCommentId": "HYubSAukirnELtLCp", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "RMyLMHexpehDJFM3S", "postedAt": "2023-04-16T00:09:57.595Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<blockquote><p>There's no cavalry coming - <i>we</i> are the cavalry.&nbsp;</p></blockquote><p>It's ambiguous who this \"we\" is. It obscures the fact there are overlapping and distinct communities among AI alignment as an umbrella movement. There have also been increasing concerns that a couple of those communities serving as nodes in that network, namely rationality and effective altruism, are becoming more trouble than they're worth. This has been coming from effective altruists and rationalists themselves.</p><p>I'm aware of, and have been part of, increasingly frequent conversations that AI safety and alignment, as a movement/community/whatever, shouldn't just \"divorce\" from EA or rationality, but can and should become more autonomous and independent from them.&nbsp;</p><p>What that implies for 'the cavalry' is, first, that much of the standing calvary is more trouble than it's worth. It might be prudent to discard and dismiss much of the existing cavalry.&nbsp;</p><p>Second, the AI safety/alignment community gaining more control over its own trajectory may provide an opportunity to rebuild the cavalry, for the better. AI alignment as a field could become more attractive to those who find it offputting, at this point, understandably, because of its association with EA and the rationality community.&nbsp;</p><p>AI safety and AI alignment, freed of the baggage EA and rationality, could bring in fresh ranks to the cavalry to replace those standing ranks still causing so many problems.</p>", "parentCommentId": "4pFFAehvYhgwL3TmH", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "mDQpwBjyQhieoYhq5", "postedAt": "2023-04-16T00:21:39.682Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<blockquote><p>I worry that the doomer levels are so high EAs will be frozen into inaction and non-EAs will take over from here. This is the default outcome, I think.</p></blockquote><p>On one hand, as I got at in <a href=\"https://forum.effectivealtruism.org/posts/5LNxeWFdoynvgZeik/nobody-s-on-the-ball-on-agi-alignment?commentId=RMyLMHexpehDJFM3S\">this comment</a>, I'm more ambivalent than you about whether it'd be worse for non-EAs to take more control over the trajectory on AI alignment.&nbsp;</p><p>On the other hand, one reason why I'm ambivalent about effective altruists (or rationalists) retaining that level is control is that I'm afraid that the doomer-ism may become an endemic or terminal disease for the EA community. AI alignment might be refreshed by many of those effective altruists currently staffing the field being replaced. So, thank you for pointing that out too. I expressed a similar sentiment in <a href=\"https://forum.effectivealtruism.org/posts/5LNxeWFdoynvgZeik/nobody-s-on-the-ball-on-agi-alignment?commentId=LwvCPLSiRWah5Qz2e\">this comment</a>, though I was more specific because I felt it was important to explain just how bad the doomer-ism has been getting.</p><blockquote><p>Others who've tried to get across the same point [Leopold is] making have, instead of explaining their disagreements, have generally alleged almost everyone else in entire field of AI alignment are literally insane.&nbsp;</p><p>That's not helpful for a few reasons. Such a claim is probably not true. It'd be harder to make a more intellectually lazy or unconvincing argument. It counts as someone making a bold, senseless attempt to, arguably, dehumanize hundreds of their peers.&nbsp;</p><p>This isn't just a negligible error from somebody recognized as part of a hyperbolic fringe in AI safety/alignment community. It's direly counterproductive when it comes from leading rationalists, like Eliezer Yudkowsky and Oliver Habryka, who wield great influence in their own right, and are taken very seriously by hundreds of other people.</p></blockquote>", "parentCommentId": "Lt45M2zaRGmPZxoPG", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "pF8nRjGKRqrmjoFaB", "postedAt": "2023-04-17T03:16:48.585Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I'm late to this, but I'm surprised that this post doesn't acknowledge the approach of inverse reinforcement learning (IRL) which <a href=\"https://80000hours.org/podcast/episodes/stuart-russell-human-compatible-ai/#top\">Stuart Russell discussed on the 80,000 Hours podcast</a> and which also featured in his book <a href=\"https://en.wikipedia.org/wiki/Human_Compatible\">Human Compatible</a>.&nbsp;</p><p>I'm no AI expert, but this approach seems to me like it avoids the \"as these models become superhuman, humans won\u2019t be able to reliably supervise their outputs\" problem, as a superhuman AI using IRL doesn't have to be supervised, it just observes us and through doing so better understands our values.&nbsp;</p><p>I'm generally surprised at the lack of discussion of IRL in the community. When one of the world leaders in AI says a particular approach in AI alignment is our best hope, shouldn't we listen to them?</p>", "parentCommentId": null, "user": {"username": "jackmalde"}}, {"_id": "XdnQerbwBnxbwChmX", "postedAt": "2023-04-17T11:24:39.765Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>How can we make IRL 100% watertight? Humans make mistakes and do bad things. We can't risk that happening even once with a superintelligent AI. You can't do trial and error if you're dead after the first wrong try. Or the SAI could execute a million requests of it safely, but then the million-and-first initiates an unstoppable chain of actions that leads to a sterile planet. The way I see it is that all the doom flows through the tiniest gap in imperfect alignment once you reach a certain power level. Can IRL ever lead to mathematically verifiable 100% perfect alignment?</p>", "parentCommentId": "pF8nRjGKRqrmjoFaB", "user": {"username": "Greg_Colbourn"}}, {"_id": "CaKyehK5eZcJFfJL4", "postedAt": "2023-04-17T13:23:26.124Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>This is exactly the discussion I want! I\u2019m mostly just surprised no one seems to be talking about IRL.</p>\n<p>I don\u2019t have firm answers (when it comes to technical AI alignment I\u2019m a bit of a noob) but when I listened to the podcast with Stuart Russell I remember him saying that we need to build in a degree of uncertainty into the AI so they essentially have to ask for permission before they do things, or something like that. Maybe this means IRL starts to become problematic in much the same way as other reinforcement learning approaches as in some way we do \u201csupervise\u201d the AI, but it certainly seems like easier supervision compared to the other approaches.</p>\n<p>Also as you say the AI could learn from bad people. This just seems an inherent risk of all possible alignment approaches though!</p>\n", "parentCommentId": "XdnQerbwBnxbwChmX", "user": {"username": "jackmalde"}}, {"_id": "inBxDgkQuxvadwt9t", "postedAt": "2023-04-17T14:48:20.881Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I guess a counter to the \"asking for permission\" as a solution thing is: how do you stop the AI from manipulating or deceiving people into giving it permission? Or acting in unsafe ways to minimise it's uncertainty (or even keep it's uncertainty within certain bounds). It's like the alignment problem just shifts elsewhere (also, <a href=\"https://www.youtube.com/watch?v=bJLcIBixGj8&amp;ab_channel=RobertMiles\">mesaoptimization</a>, or inner alignment, isn't really addressed by IRL).<br><br>Re learning from bad people, I think a bigger problem is instilling any human-like motivation into <a href=\"https://knowyourmeme.com/memes/shoggoth-with-smiley-face-artificial-intelligence\">them</a> at all.</p>", "parentCommentId": "CaKyehK5eZcJFfJL4", "user": {"username": "Greg_Colbourn"}}, {"_id": "8zT9ivKWZMoEjfmbL", "postedAt": "2023-04-17T16:46:49.491Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>You're making me want to listen to the podcast episode again. From a quick look at the <a href=\"https://80000hours.org/podcast/episodes/stuart-russell-human-compatible-ai/#principles-for-beneficial-machines-002925\">transcript</a>, Russell thinks the three principles of AI should be:</p><ol><li>The machine\u2019s only objective is to maximize the realization of human preferences.</li><li>The machine is initially uncertain about what those preferences are.</li><li>The ultimate source of information about human preferences is human behavior.</li></ol><p>It certainly seems such an IRL-based AI would be <i>more</i> open to being told what to do than a traditional RL-based AI.</p><p>RL-based AI generally doesn't want to obey requests or have its goal be changed, because this hinders/prevents it from achieving its original goal. IRL-based AI literally has the goal of realising human preferences, so it would need to have a pretty good reason (from its point of view) <i>not</i> to obey someone's request.</p><p>Certainly early on, IRL-based AI would obey any request you make provided you have baked in a high enough degree of uncertainty into the AI (principle 2). After a while, the AI becomes more confident about human preferences and so may well start to manipulate or deceive people when it thinks they are not acting in their best interest. This sounds really concerning, but in theory it might be good if you have given the AI enough time to learn.</p><p>For example, after a sufficient amount of time learning about human preferences, an AI may say something like \"I'm going to throw your cigarettes away because I have learnt people really value health and cigarettes are really bad for health\". The person might say \"no don't do that I really want a ciggie right now\". If the AI ultimately knows that the person really shouldn't smoke for their own wellbeing, it may well want to manipulate or deceive the person into throwing away their cigarettes e.g. through giving an impassioned speech about the dangers of smoking.</p><p>This sounds concerning but, provided the AI has had enough time to properly learn about human preferences, the AI should, in theory, do the manipulation in a minimally-harmful way. It may for example learn that humans really don't like being tricked, so it will try to change the human's mind just by giving the person the objective facts of how bad smoking is, rather than more devious means. The most important thing seems to be that the IRL-based AI has <i>sufficient uncertainty</i> baked into them for a <i>sufficient amount of time</i>, so that they only start pushing back on human requests when they are <i>sufficiently confident</i> they are doing the right thing.</p><p>I'm far from certain that IRL-based AI is watertight (my biggest concern remains the AI learning from irrational/bad people), but on my current level of (very limited) knowledge it does seem the most sensible approach.</p>", "parentCommentId": "inBxDgkQuxvadwt9t", "user": {"username": "jackmalde"}}, {"_id": "LpgoXv4QHXEBMysXa", "postedAt": "2023-04-17T17:38:05.245Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Interesting about the <a href=\"https://en.wikipedia.org/wiki/Dual_process_theory\">\"System 2\" vs \"System 1\"</a> preference fulfilment (your cigarettes example). But all of this is still just focused on <a href=\"https://www.lesswrong.com/posts/poyshiMEhJsAuifKt/outer-vs-inner-misalignment-three-framings-1#:~:text=Roughly%20speaking%2C%20the%20outer%20alignment,in%20accordance%20with%20human%20preferences.\">outer alignment</a>. How does the inner <a href=\"https://knowyourmeme.com/memes/shoggoth-with-smiley-face-artificial-intelligence\">shoggoth</a> get prevented from mesaoptimising on an arbitrary goal?</p>", "parentCommentId": "8zT9ivKWZMoEjfmbL", "user": {"username": "Greg_Colbourn"}}, {"_id": "ZRRRQkqFh7mvLa7fX", "postedAt": "2023-04-17T18:59:59.357Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I\u2019m afraid I\u2019m not well read on the problem of inner alignment and why optimizing on an arbitrary goal is a realistic worry. Can you explain why this might happen / provide an good, simple resource that I can read?</p>\n", "parentCommentId": "LpgoXv4QHXEBMysXa", "user": {"username": "jackmalde"}}, {"_id": "wbmKTbgwY4B8KqK36", "postedAt": "2023-04-17T19:25:53.928Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>The LW <a href=\"https://www.lesswrong.com/tag/mesa-optimization\">wiki entry</a> is good. Also the Rob Miles video I link to above explains it well with visuals and examples. I think there are 3 core parts to the AI x-risk argument: the orthogonality thesis (Copernican revolution applied to mind-space; why outer alignment is hard), Basic AI Drives (convergent instrumental goals leading to power seeking), and Mesaoptimizers (why inner alignment is hard).</p>", "parentCommentId": "ZRRRQkqFh7mvLa7fX", "user": {"username": "Greg_Colbourn"}}, {"_id": "9wG6F8AJ5Dchtz5Pc", "postedAt": "2023-04-17T23:20:07.791Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Thanks. I watched Robert Miles' video which was very helpful. Especially the part where he explains why an AI might want to act in accordance with its base objective in a training environment only to then pursue its mesa objective in the real world.</p><p>I'm quite uncertain at this point, but I have a vague feeling that Russell's second principle (The machine is initially uncertain about what those preferences are) is very important here. It is a vague feeling though...</p>", "parentCommentId": "wbmKTbgwY4B8KqK36", "user": {"username": "jackmalde"}}, {"_id": "46iW78EQK6E2RuLgt", "postedAt": "2023-04-18T02:41:42.958Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Thanks for the explanation! Though I think I've been misunderstood.</p><p>I think I strongly prefer if e.g. Sam Altman, Demis Hassabis, or Elon Musk ends up with majority influence over how AGI gets applied (assuming we're alive), over leading candidates in China (especially Xi Jinping). But to state that preference as \"I hope China doesn't develop AI before the US!\" seems ... <i>unusually </i>imprecise and harmful. Especially when nationalistic framings like that are already very likely to fuel otherisation and lack of cross-cultural understanding.</p><p>It's like saying \"Russia is an evil country for attacking Ukraine,\" when you could just say \"Putin\" or \"the Russian government\" or any other way of phrasing what you mean with less likelihood of spilling over to hatred of <i>Russians</i> in general.</p>", "parentCommentId": "rGLna3aQ8AHtmpbfT", "user": {"username": "Anomalous"}}, {"_id": "dzK9xySkfsgaMs9Np", "postedAt": "2023-07-19T13:19:02.550Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>Regarding the analogy you use where humans etc not being aligned with each other implying that human-machine alignment is equally hard: Humans are in competition with other humans. Nation-states are in competition with other nation-states. However AI algorithms are created by humans as a tool (at least, for now that seems to be the intention). Not to say this is an argument to think alignment is possible but I do think this is a flawed analogy.</p>", "parentCommentId": "KSaZ2NguEF8w93FhX", "user": {"username": "Andreas Netteland"}}, {"_id": "LyaegGF9EboBYhkLx", "postedAt": "2023-09-19T22:00:59.678Z", "postId": "5LNxeWFdoynvgZeik", "htmlBody": "<p>I'm no expert on this at all but this is very interesting. I wonder if the key to alignment is some kind of inductive alignment where rather than designing a superintelligent system T_N in a vacuum you design a series of increasingly intelligent systems T_0..T_N where the alignment is inbuilt at each level and a human aligns the basic T_0. i.e. you build T_1 as a small advancement of T_0 such that it can be aligned by T_0 and so on.</p>\n", "parentCommentId": null, "user": {"username": "Merallis"}}]