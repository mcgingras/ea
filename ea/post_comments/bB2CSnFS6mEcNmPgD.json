[{"_id": "wmCDKwk2RpqLeeojm", "postedAt": "2023-05-01T21:10:01.647Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>I agree that if you have to slow down all AI progress or none of it, you should slow it all down. But fortunately, you don't-- you can almost have the best of both worlds.</p><p>Insofar as AI x-risk looks like LLMs while awesome stuff like medicine (and robotics and autonomous vehicles and more) doesn't look like LLMs, caution on LLMs doesn't delay other awesome stuff.* So when you talk about slowing AI progress, make it clear that you only mean AI <i>on the path to dangerous capabilities</i>.</p><p>*That's not exactly true: e.g. maybe an LLM can automate medical research, or recursively bootstrap itself to godhood and then solve medicine. But \"caution with LLMs\" doesn't conflict with \"progress on medicine now.\"</p>", "parentCommentId": null, "user": {"username": "zsp"}}, {"_id": "BCwLx6f6NrpoN6yLG", "postedAt": "2023-05-01T22:57:51.497Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>I appreciate that this post acknowledges that there are costs to caution. I think it could've gone a bit further in emphasizing how these costs, while large in an absolute sense, are small <i>relative to the risks</i>.</p><p>The formal way to do this would be a cost-benefit analysis on longtermist grounds (perhaps with various discount rates for future lives). But I think there's also a way to do this in less formal/wonky language, without requiring any longtermist assumptions.</p><p>If you have a technology where half of experts believe there's a ~10% chance of extinction, the benefits need to be <i>enormous</i> for them to outweigh the costs of caution. I like Tristan Harris's <a href=\"https://twitter.com/tristanharris/status/1635357118030286848\">airplane analogy</a>:</p><blockquote><p>Imagine: would you board an airplane if 50% of airplane engineers who built it said there was a 10% chance that everybody on board dies?</p></blockquote><p>Here's another frame (that I've been finding useful with folks who don't follow the technical AI risk scene much): History is full of examples of people saying that they are going to solve everyone's problems. There are many failed messiah stories. In the case of AGI, it's true that <i>aligned and responsibly developed </i>AI could do a lot of good. But when you have people saying \"the risks are overblown-- we're smart and responsible enough to solve everything\", I think it's pretty reasonable to be skeptical (on priors alone).</p><p>Finally, one thing that sometimes gets missed in this discussion is that <strong>most advocates of pause still want to get to AGI eventually</strong>. Slowing down for a few years or decades <i>is </i>costly, and advocates of slowdown should recognize this. But <i>the costs are substantially lower than the risks. </i>I think both of these messages get missed in discussions about slowdown.</p>", "parentCommentId": null, "user": {"username": "Akash"}}, {"_id": "dBy2eYKxXxmcsNDMn", "postedAt": "2023-05-01T23:03:18.837Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>I think more powerful (aligned) LLMs would lead to more awesome stuff, so caution on LLMs does delay other awesome stuff.</p><p>I agree with the point that \"there's value that can be gained from figuring out how to apply systems at current capabilities levels\" (<a href=\"https://www.lesswrong.com/posts/P98i7kAN2uWuy7mhD/ai-summer-harvest\">AI summer harvest</a>), but I wouldn't go as far as \"you can almost have the best of both worlds.\" It seems more like \"we can probably do a lot of good with existing AI, so even though there are costs of caution, those costs are worth paying, and at least we can make some progress applying AI to pressing world problems while we figure out alignment/governance.\" (My version isn't catchy though, oops).</p>", "parentCommentId": "wmCDKwk2RpqLeeojm", "user": {"username": "Akash"}}, {"_id": "Ti8bcNYrQDzzyN4kJ", "postedAt": "2023-05-01T23:08:07.841Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>Sure.</p><p>Often when people talk about awesome stuff they're not referring to LLMs. In this case, there's no need to slow down the awesome stuff they're talking about.</p>", "parentCommentId": "dBy2eYKxXxmcsNDMn", "user": {"username": "zsp"}}, {"_id": "w8Due6k2rovfvv4W8", "postedAt": "2023-05-01T23:16:35.787Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>Lots of awesome stuff requires AGI or superintelligence. People think LLMs (or stuff LLMs invent) will lead to AGI or superintelligence.</p>\n<p>So wouldn\u2019t slowing down LLM progress slow down the awesome stuff?</p>\n", "parentCommentId": "Ti8bcNYrQDzzyN4kJ", "user": {"username": "Akash"}}, {"_id": "NiAaHjfDHSK7xeD9o", "postedAt": "2023-05-02T00:04:51.129Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>Yeah, that awesome stuff.</p><p>My impression is that most people who buy \"LLMs --&gt; superintelligence\" favor caution despite caution slowing awesome stuff.</p><p>But this thread seems unproductive.</p>", "parentCommentId": "w8Due6k2rovfvv4W8", "user": {"username": "zsp"}}, {"_id": "ECvyzw3HZMJzJfo7g", "postedAt": "2023-05-02T01:15:55.696Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>Let me make the contrarian point here that you don't have to build AGI to get these benefits eventually. An alternative, much safer approach would be to stop AGI entirely and try to inflate human/biological intelligence with drugs or other biotech. Stopping AGI is unlikely to happen and this biological route would take a lot longer but it's worth bringing up in any argument about the risks vs. reward of AI.</p>", "parentCommentId": null, "user": {"username": "RedStateBlueState"}}, {"_id": "XJFakYFJYndHx3xN3", "postedAt": "2023-05-02T09:53:20.548Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<blockquote><p>Imagine: would you board an airplane if 50% of airplane engineers who built it said there was a 10% chance that everybody on board dies?</p></blockquote><p>In the context of the OP, the thought experiment would need to be extended.</p><p>\"Would you risk a 10% chance of a deadly crash to go to [random country]\" -&gt; ~100% of people reply no.</p><p>\"Would you risk a 10% of a deadly crash to go to a Utopia without material scarcity, conflict, disease?\" -&gt; One would expect a much more mixed response.</p><p>The main ethical problem is that in the scenario of global AI progress, <i>everyone</i> is forced to board the plane, irrespective of their preferences.</p>", "parentCommentId": "BCwLx6f6NrpoN6yLG", "user": {"username": "matthias_samwald"}}, {"_id": "C5hKygmShXJ7MRhGF", "postedAt": "2023-05-02T10:14:28.171Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>I think that just as the risks of AGI are overstated, so too are the potential benefits. Don't get me wrong, i expect it would still be revolutionary and incredible, just not <i>magical</i>.&nbsp;</p><p>Going from tens of thousands of biomedical researchers to hundreds of millions would definitely greatly speed up &nbsp;medical research... but I think you would run into diminishing returns, as the limiting bottleneck is often not the number of researchers. For example, coming up with the covid vaccine took barely any time at all, but it took years to get it out due to the need for human trials and to actually build and distribute the thing.&nbsp;</p><p>I still think there would be a massive boost, but perhaps not a \"jump in forward a century\" one. It's hard to predict exactly what the shortcomings of AGI will be, but there has never been a technology that lacked shortcomings, and I don't think AGI will be the exception.&nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "xj4CTBEvrwpPqPeXW", "postedAt": "2023-05-02T12:58:50.513Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>I agree with you more than with Akash/Tristan Harris here, but note that death and Utopia are not the only possible outcomes! It's more like \"Would you risk a 10% of a deadly crash<i> for a chance</i> to go to a Utopia without material scarcity, conflict, disease\"</p>", "parentCommentId": "XJFakYFJYndHx3xN3", "user": {"username": "Linch"}}, {"_id": "G63jgnKFZXwrpJrr3", "postedAt": "2023-05-02T15:26:45.987Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<blockquote><p>Insofar as AI x-risk looks like LLMs while awesome stuff like medicine (and robotics and autonomous vehicles and more) doesn't look like LLMs, caution on LLMs doesn't delay other awesome stuff.* So when you talk about slowing AI progress, make it clear that you only mean AI <i>on the path to dangerous capabilities</i>.</p></blockquote><p>AI biologists seem <i>extremely</i> dangerous to me - something \"merely\" as good at viral genomes as GPT-4 is at language would already be an existential threat to human civilization, if not necessarily homo sapiens.&nbsp;</p>", "parentCommentId": "wmCDKwk2RpqLeeojm", "user": {"username": "yefreitor"}}, {"_id": "yLqi6HiPnCubgFJww", "postedAt": "2023-05-02T18:59:50.555Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>I agree that bottlenecks like the ones you mention will slow things down. I think that's compatible with this being a \"jump in forward a century\" thing though.</p><p>Let's consider the case of a cure for cancer. First of all, even if it takes \"years to get it out due to the need for human trials and to actually build and distribute the thing\" AGI could still bring the cure forward from 2200 to 2040 (assuming we get AGI in 2035).</p><p>Second, the excess top-quality labour from AGI could help us route-around the bottlenecks you mentioned:</p><ul><li><strong>Human trials</strong>: AGI might develop ultra-high-reliability ways to verify that drugs work without human trials. That could either lead to a change in regulatory requirements or to people buying the AGI-designed drugs sooner in countries where that's legal.</li><li><strong>Manufacturing and distributing the drug</strong>: Imagine if we'd had 100 million of the most competent humans working (remotely) full time on optimising every step of the manufacturing+distribution process for COVID? They could have:<ul><li>Planned out how to use all the US' available manufacturing and transportation infrastructure maximally efficiently</li><li>Give real-time instructions to all the humans working in those industries so that they were more productive and better coordinated.</li><li>Recruit and train of new human workers (again instructing them in real-time) to increase the available labour.</li><li>More speculatively, it might not take long for AGI to design robots that could do the physical labour needed to manufacture and distribute the vaccines.&nbsp;</li></ul></li></ul>", "parentCommentId": "C5hKygmShXJ7MRhGF", "user": {"username": "Tom_Davidson"}}, {"_id": "iEcSctFamJWrck3jf", "postedAt": "2023-05-03T20:44:04.869Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>I'm confused, we make this caution compromise all the time - for example, medicine trial ethics. Can we go faster? Sure, but the risks are higher. Yes, that can mean that some people will not get a treatment that is developed a few years too late.</p>\n<p>Another closer example is gain of function research. The point is, we could do a lot, but we chose not to - AI should be no different.</p>\n<p>Seems to me that this post is a little detached from real world caution considerations, even if it isn't making an incorrect point.</p>\n", "parentCommentId": null, "user": {"username": "Ariel G."}}, {"_id": "AJ6L7BQE3dADXSv3o", "postedAt": "2023-05-04T14:01:25.433Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>As I have <a href=\"https://forum.effectivealtruism.org/posts/32wmwfYELKSEfckYv/we-don-t-need-agi-for-an-amazing-future\">argued here</a> in more detail, we don't need AGI for an amazing future, including curing cancer. We don't have to decide between \"all in for AGI\" and \"full-stop in developing AI\". There's a middle ground, and I think it's the best option we have.</p>", "parentCommentId": null, "user": {"username": "Karl von Wendt"}}, {"_id": "D99Gg5tdHX2ERnMEn", "postedAt": "2023-05-04T14:01:59.494Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>I fully agree, <a href=\"https://forum.effectivealtruism.org/posts/32wmwfYELKSEfckYv/we-don-t-need-agi-for-an-amazing-future\">see this post</a>.</p>", "parentCommentId": "ECvyzw3HZMJzJfo7g", "user": {"username": "Karl von Wendt"}}, {"_id": "jksa43pM9wkeYMkGL", "postedAt": "2023-05-06T15:10:24.299Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>I thought about this a moderate amount as my wife was dying. This is definitely personal for me. But I still think the potential risks are too great and that the overall balance favors caution, even if people like my wife won't be able to benefit. Cost-benefit analysis is often very sad like that.</p>\n", "parentCommentId": null, "user": {"username": "Peter_Hurford"}}, {"_id": "LbEfu76GmgXwnGv5b", "postedAt": "2023-05-07T12:35:02.765Z", "postId": "bB2CSnFS6mEcNmPgD", "htmlBody": "<p>One thing I, as a lay person do not understand about the benefits of AI: Why do we think the corporations that would own these systems would care for the poor and sick? We already have the cure for many of the worst diseases in the world, but we do not cure them because the people who suffer them do not have money to pay for the cure. The same goes for poverty, we have enough food and wealth already for everyone to live dignified and fulfilling lives but we do not share the resources because some people want more than what is required. Why would this distribution problem not continue under AGI? Why would not AGI be focused on marginal life extension for the rich, and bigger houses, space travel etc. for the middle classes rather than getting more resources to the poorest? I would love to be convinced that this is not likely to continue happening under very capable AI - I hope that I have missed convincing writing on how AGI within the current economic system would differ significantly from how other increases in productivity and technical progress get distributed (or that AGI will radically transform the economic system to cater more to the marginalized).&nbsp;<br><br>In other words, it seems to me that to solve poverty and global health, power needs to shift. And power structures do not seem to be something you can fix with science or technology.</p>", "parentCommentId": null, "user": {"username": "Ulrik Horn"}}]