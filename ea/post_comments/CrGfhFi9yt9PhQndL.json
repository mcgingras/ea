[{"_id": "fjuGtMxvMx4od3ufW", "postedAt": "2022-11-16T00:41:29.176Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": "<p>Do you know if there is a version that isn't paywalled?&nbsp;</p>", "parentCommentId": null, "user": {"username": "Lauren Maria"}}, {"_id": "SXsa6vt3fktac4WFb", "postedAt": "2022-11-16T00:50:57.827Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": "<p><a href=\"https://archive.ph/CGjTz\">https://archive.ph/CGjTz</a></p>\n", "parentCommentId": "fjuGtMxvMx4od3ufW", "user": {"username": "lincolnq"}}, {"_id": "caZbYpCkGGKozbRpD", "postedAt": "2022-11-16T00:56:04.148Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": "<p>Thank you!</p>", "parentCommentId": "SXsa6vt3fktac4WFb", "user": {"username": "Lauren Maria"}}, {"_id": "yGbNAShTAauaaATwg", "postedAt": "2022-11-16T01:02:16.822Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": "<p>Many of these concerns resonated with me.</p><p>A relative outsider, my understanding of EA formed around its online content, which emphasises utilitarianism and longtermism. Whenever speaking to EA's in person, I'm often surprised that these perspectives are more weakly held by community members (and leaders?) than I expected. I think there are messaging issues here. Part of the issue might be that longtermist causes are more interesting to write and talk about. We should be careful to allocate attention to &nbsp;cause areas proportional to their significance.</p><p>Too much of the ecosystem feels dependent on a few grantmakers / re-granters. It concentrates too much power in relatively few people's hands. (At the same time, this seems to be a very hard problem to solve. No particular initiatives come to my mind.)</p><p>I see EA's concerns with reputational risk and optics as flaws with its overly utilitarian perspective. Manipulating the narrative has short-term reputational benefits and hidden long-term costs.</p><p>At the same time, I am sceptical of EA's ability to adequately address these issues. Such concerns have been previously raised without significant change. It feels like many of these issues have arisen due to the centralisation of power and the over-weighting of community leaders' opinions, yet simultaneously the community is sufficiently de-centralised that it's difficult to coordinate such a change.</p>", "parentCommentId": null, "user": {"username": "Austin_Zhang"}}, {"_id": "mdyoHMiSsQphbbvsy", "postedAt": "2022-11-16T01:35:37.276Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": "<p>Imagine thinking this is a good outcome of the \"keep your mouth shut\" strategy CEA recommends regarding media:</p><blockquote><p>Effective altruism is not a cult. As one EA advised his peers in a forum post about how to talk to journalists: \u201cDon\u2019t ever say \u2018People sometimes think EA is a cult, but it\u2019s not.\u2019 If you say something like that, the journalist will likely think this is a catchy line and print it in the article. This will give readers the impression that EA is not quite a cult, but perhaps almost.\u201d</p><p>\u2026</p><p>Effective altruism treats public engagement as yet another dire risk. Bostrom has written about \u201cinformation hazards\u201d when talking about instructions for assembling lethal weaponry, but some effective altruists now use such parlance to connote bad press. EAs speak of avoiding \u201creputational risks\u201d to their movement and of making sure their \u201coptics\u201d are good. In its annual report in 2020, the Centre for Effective Altruism logged all 137 \u201cPR cases\u201d it handled that year: \u201cWe learned about 78% of interviews before they took place. The earlier we learn of an interview, the more proactive help we can give on mitigating risks.\u201d It also noted the PR team\u2019s progress in monitoring \u201crisky actors\u201d: not people whose activities might increase the existential risks to humanity, but those who might harm the movement\u2019s standing.</p></blockquote><p>Terrible look, to be honest.</p>", "parentCommentId": null, "user": {"username": "peppersghost"}}, {"_id": "4CyYiArAvPG3tyu6x", "postedAt": "2022-11-16T01:55:43.343Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": "<p>Thank you for sharing! &nbsp;A one sentence thought on one of the paragraphs towards the end outlined by a former EA member...</p><blockquote><p>Initially, they appeared to achieve their goal: MacAskill offered to talk to Cremer. She presented him with structural reforms they could make to the community. Among other things, Cremer wanted whistleblowers to have more protection and for there to be more transparency around funding and decisions about whom to invite to conferences. MacAskill responded that he wanted to support more \u201ccritical work\u201d. Subsequently, the movement established a criticism contest. Yet when it came to specifics such as the mechanisms for raising and distributing money, he seemed to think the current process was sufficiently rigorous. MacAskill disputes this characterisation and told me he was in favour of \u201cincreasing donor diversity\u201d.</p></blockquote><p>I could understand the pursuit for accomplishing critical work and achieve EA objectives, but a structure to safeguard the work and EA brand is vital as well.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Miguel"}}, {"_id": "4qoT8PuX7eS8v3NgA", "postedAt": "2022-11-16T02:52:57.035Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": "<p>That's interesting, I've had the exact opposite experience. I was attracted to EA for similar reasons that Zoe and Ben mention in the article, such as global poverty and health, but then found that everyone I was meeting in the EA community was working on longtermist stuff (AI alignment and safety mostly). We have discussed that perhaps since my club was at a university, it's possible that most of the university students in the club at the time were just more career aligned with longtermist stuff. I don't know how accurate that is though.&nbsp;</p>", "parentCommentId": "yGbNAShTAauaaATwg", "user": {"username": "Lauren Maria"}}, {"_id": "mgtKWLtoP7epAPBEE", "postedAt": "2022-11-16T03:19:57.389Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": "<p>I found this quote to be particularly salient:\n\u201cHaving a handful of wealthy donors and their advisers dictate the evolution of an entire field is bad epistemics at best and corruption at worst\u201d</p>\n", "parentCommentId": null, "user": {"username": "Marcus Rademacher"}}, {"_id": "ptKyrdq3qETScnHpS", "postedAt": "2022-11-16T05:22:25.961Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": "<p>Sadly, I agree with many of the points in this article.&nbsp;</p><blockquote><p>\u201cJust as the astrologer promises us that \u2018struggle is in our future\u2019 and can therefore never be refuted, so too can the longtermist simply claim that there are a staggering number of people in the future, thus rendering any counter argument mute,\u201d he wrote in a post on the Effective Altruism forum. This matters, Chugg told me, because \u201cYou\u2019re starting to pull numbers out of hats, and comparing them to saving living kids from malaria.\u201d</p></blockquote><p>I've been thinking this for a long time but not been able to put together something so succinct. Personally, I will carry on championing my interpretation of EA that is to look at charity like your investments and get the best bang for your buck. Wether I'll use the term 'EA' to describe myself will depend on the next few months - if the general understanding of EA is speculative longtermism, cultish behavior, and 'ends justify the means' then I'd rather not bring it up.&nbsp;</p><p>Maybe EA will split in two, one group carrying on like they are now with a focus on longtermism and another that focuses solely on real impacts that can be seen and measured within our lifetimes. Maybe it doesn't matter as long as you and I keep it in our minds when we make our donations to charity funding malaria nets saving real lives today, no matter how small that impact might be compared to SBF and the future trillions of humans at risk of AI going rogue on Mars.&nbsp;</p><p>Edit: Not to say longtermism doesn't have its place, I just feel too much time is spent on these things that may never happen while real people face real issues today (or may face in the near future, like pandemic preparedness).&nbsp;</p>", "parentCommentId": null, "user": {"username": "EddSH1994"}}, {"_id": "sto2qXGWAj5BhmnQQ", "postedAt": "2022-11-16T11:43:32.800Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": "<p>Other than seemingly conflating EA with utilitarianism sometimes, I thought this was quite a good piece which raises some important pain points in the movement.</p>\n<p>Let's aim to become more transparent, less secretive, more decentralized and put into place whistle blower protections.</p>\n", "parentCommentId": null, "user": {"username": "Jeroen_W"}}, {"_id": "oJQnh5gzBiksyDyzB", "postedAt": "2022-11-16T11:56:32.105Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": "<p>I thought this was a relatively balanced piece actually as far as criticisms go. The author is clearly not a fan, but I feel like she resisted the temptation to straw-man a lot more than most critics - good on her (...or good on The Economist if this is their general style?).</p><p>I think this phrasing is unfortunate though:</p><blockquote><p>Cremer and Kemp were told that they and their institutions might lose funding because of it, and were advised not to publish at all.</p></blockquote><p>I imagine this will be interpreted by most readers as a threat from funders. Whereas my understanding was that this was a case of other community members looking out for Cremer and Kemp, telling them they were worried that this might happen. From their <a href=\"https://forum.effectivealtruism.org/posts/gx7BEkoRbctjkyTme/democratising-risk-or-how-ea-deals-with-critics-1\">post</a>:</p><blockquote><p>These individuals\u2014often senior scholars within the field\u2014told us in private that they were concerned that any critique of central figures in EA would result in an inability to secure funding from EA sources, such as OpenPhilanthropy. We don't know if these concerns are warranted.</p></blockquote><p>(By the way, the free link has an extra comma at the end which needs removing for the link to work.)</p>", "parentCommentId": null, "user": null}, {"_id": "DaPvbf3q5TTWjN4ee", "postedAt": "2022-11-16T20:40:44.812Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": "<p>Isn't it somewhat ironic though that you're caring what the Economist journalists think, and implicitly connoting that that forum post shouldn't have been made because it gave bad PR?</p>", "parentCommentId": "mdyoHMiSsQphbbvsy", "user": {"username": "DonyChristie"}}, {"_id": "8ksZjdSJStJAQfs4X", "postedAt": "2022-11-16T21:16:54.354Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": "<p>I just find it funny how posting something like that in a public forum will, of course, make it seen by journalists sooner or later, anyway.</p><p>It's the second bit that concerns me more because I think it's essentially a correct description of how CEA, and EAs in general (largely because of CEA's influence), view public engagement. Any interaction outside the community is seen mainly as something that should be handled through a lens of risk mitigation. The way it's phrased makes it sound like the CEA stopped 78% of 137 virus outbreaks.</p><p>Like I wrote elsewhere, I think the danger with the \"don't talk to media\" approach is that you get very few views into a movement, mostly from leadership, and if one of those rare appearances takes a wrong turn, there is not a plurality of other views and appearances out there to balance it.</p><p>For example, if the only people who \"should\" give interviews are EA leadership philosophers that are deeply into longtermism, that will make it seem like the entire EA movement is all about longtermism. This is not true.</p>", "parentCommentId": "DaPvbf3q5TTWjN4ee", "user": {"username": "peppersghost"}}, {"_id": "JkKdHfbdTLBcfBMpJ", "postedAt": "2023-05-04T02:44:03.618Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": "<p>Thanks for clarifying this! I really had interpreted it as a threat from funders.</p>", "parentCommentId": "oJQnh5gzBiksyDyzB", "user": {"username": "Diego Oliveira"}}, {"_id": "C7tvtEP5ejDRyHZkA", "postedAt": "2022-11-16T03:30:48.357Z", "postId": "CrGfhFi9yt9PhQndL", "htmlBody": null, "parentCommentId": null, "user": null}]