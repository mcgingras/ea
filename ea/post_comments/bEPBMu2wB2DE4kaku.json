[{"_id": "rCvFcxZJoJQomg788", "postedAt": "2018-04-12T07:19:30.317Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>Bravo! </p>\n<p>FWIW I am one of the people doing something similar to what you advocate: I work in biorisk for comparative advantage reasons, although I think AI risk is a bigger deal. </p>\n<p>That said, this sort of trading might be easier within broad cause areas than between them. My impression is received wisdom among the far future EAs is that both AI and bio are both 'big deals': AI might be (even more) important, yet bio (even more) neglected. For this reason even though I suspect most (myself included) would recommend a 'pluripotent far future EA' to look into AI first, it wouldn't take much to tilt the scales the other way (e.g. disposition, comparative advantage, and other things you cite). It also means individuals may not suffer a motivation hit if they are merely doing a very good thing rather than the very best thing by their lights. I think a similar thing applies to means that further a particular cause (whether to strike out on ones own versus looking for a role in an existing group, operations versus research, etc.)</p>\n<p>When the issue is between cause areas, one needs to grapple with decisive considerations open chasms which are hard to cross with talent arbitrage. In the far future case, the usual story around astronomical waste etc. implies (<em>pace</em> <a href=\"http://reducing-suffering.org/why-charities-dont-differ-astronomically-in-cost-effectiveness/\">Tomasik</a>) that work on the far future is hugely more valuable than work in another cause area like animal welfare. Thus even if one is comparatively advantaged in animal welfare, one may still think their marginal effect is much greater in the far future cause area.</p>\n<p>As you say, this could still be fertile ground for moral trade, and I also worry about more cynical reasons that explain this hasn't happened (cf. fairly limited donation trading so far). Nonetheless, I'd like to offer a few less cynical reasons that draw the balance of my credence.</p>\n<p>As you say, although Allison and Bettina should think, &quot;This is great, by doing this I get to have a better version of me do work on the cause I think is most important!&quot; They might mutually recognise their cognitive foibles will mean they will struggle with their commitment to a cause they both consider objectively less important, and this term might outweigh their comparative advantage. </p>\n<p>It also may be the case that developing considerable sympathy to a cause area may not be enough. Both intra- and outside EA, I generally salute well-intentioned efforts to make the world better: I wish folks working on animal welfare, global poverty, or (developed world) public health every success. Yet when I was doing the latter, despite finding it intrinsically valuable, I struggled considerably with motivation. I imagine the same would apply if I traded places with an 'animal-EA' for comparative advantage reasons. </p>\n<p>It would been (prudentially) better if I could 'hack' my beliefs to find this work more intrinsically valuable. Yet people are (rightly) chary to try and hack prudentially useful beliefs (cf. Pascal's wager, where Pascal anticipated the 'I can't just change my belief in God' point, and recommended atheists go to church and other things which would encourage religious faith to take root), given it may have spillover into other domains where they take epistemic accuracy is very important. If cause area decisions mostly rely on these (which I hope they do), there may not be much opportunity to hack away this motivational bracken to provide fertile ground for moral trade. 'Attitude hacking' (e.g. I really like research, but I'd be better at ops, so I try to make myself more motivated by operations work) lacks this downside, and so looks much more promising.</p>\n<p>Further, a better <em>ex ante</em> strategy across the EA community might be not to settle for moral trade, but instead discuss the merits of the different cause areas. Both Allison and Bettina take the balance of reason on their side, and so might hope either a) they get their counterpart to join them, or b) they realise they are mistaken and so migrate to something more important. Perhaps this implies an idealistic view of how likely people are to change their minds about these matters. Yet the track record of quite a lot people changing their minds about what cause areas are the most important (I am one example) gives some cause for hope.</p>\n", "parentCommentId": null, "user": {"username": "Gregory_Lewis"}}, {"_id": "X6RTh5FYQbDHJovDN", "postedAt": "2018-04-12T10:16:26.458Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<blockquote>\n<p>However, we can also err by thinking about a too narrow reference class</p>\n</blockquote>\n<p>Just to pick up on this, a worry I've had for a while - which I'm don't think I'm going to do a very job explaining here - is that the reference class people use is &quot;current EAs&quot; not &quot;current and future EAs&quot;. To explain, when I started to get involved in EA back in 2015, 80k's advice, in caricature, was that EAs should become software developers or management consultants and earn to give, whereas research roles, such as becoming a philosopher or historian, are low priority. Now the advice has, again in caricature, swung the other way: management consultancy looks very unpromising, and people are being recommended to do research. There's even occassion discussion (see MacAskill's 80k podcast) that, on the margin, philosophers might be useful. If you'd taken 80k's advice seriously and gone in consultancy, it seems you would have done the wrong thing. (Objection, imagining Wiblin's voice: but what about personal fit? We talked about that. Reply: if personal fit does all the work - i.e. &quot;just do the thing that has greatest personal fit&quot; -  then there's no point making more substantive recommendations)</p>\n<p>I'm concerned that people will funnel themselves into jobs that are high-priority now, in which they have a small comparative advice to other EAs, rather than jobs in which they will later have a much bigger comparative advantage to other EAs. At the present time, the conversation is about EA needing more operations roles. Suppose two EAs, C and D, are thinking about what to do. C realises he's 50% better than D at ops and 75% better at research, so C goes into Ops because that's higher priority. D goes into research. Time passes the movement grows. E now joins. E is better than C at Ops. The problem is that C has taken an ops role and it's much harder for C to transition to research. C only has a comparative advantage at ops in the first time period, thereafter he doesn't. Overall, it looks like C should just have gone into research, not ops. </p>\n<p>In short, our comparative advantage is not fixed, but will change over time simply based on who else shows up. Hence we should think about comparative advantage over our lifetimes rather than the shorter term. This likely changes things.</p>\n", "parentCommentId": null, "user": {"username": "MichaelPlant"}}, {"_id": "L5B6wdNTwxSx7wmBX", "postedAt": "2018-04-12T12:32:32.702Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>It seems to me like you're in favor of unilateral talent trading, that is, that someone should work on a cause he thinks isn't critical but he has a comparative advantage there, because he believes that this will induce other people to work on his preferred causes. I disagree with this. When someone works on a cause, this also increases the amount of attention and perceived value it is given in the EA community as a whole. As such I expect the primary effect of unilateral talent trading would be to increase the cliquishness of the EA community -- people working on what's popular in the EA community rather than what's right. Also, what's commonly considered as EA priorities could differ significantly from the actual average opinion, and unilateral trading would unrightly shift the latter in the direction of the former, especially as the former is more easily gamed by advertising etc.. On the whole, I discourage working on a cause you don't think is important unless you are confident this won't decrease the total amount of attention given to your preferred cause. That is, only accept explicit bilateral trades with favorable terms.</p>\n", "parentCommentId": null, "user": {"username": "itaibn"}}, {"_id": "iLfYRXoq6uvSdrKgQ", "postedAt": "2018-04-12T18:58:00.066Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>I completely agree. I considered making the point in the post itself, but I didn't because I'm not sure about the practical implications myself!</p>\n", "parentCommentId": "X6RTh5FYQbDHJovDN", "user": {"username": "Denise_Melchin"}}, {"_id": "ku83oXbcx5PnNy6uL", "postedAt": "2018-04-12T19:22:39.604Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>Interesting article. I see some practical issues though.</p>\n<p>Finding a symmetrical trade partners would be very hard. If Allison has a degree from Oxford and Bettina from community college, the trade would not be fair.</p>\n<blockquote>\n<p>A more easily implementable solution is to search for a donor willing to offset a cause area switch, i.e. make a donation to the cause area the talent will be leaving. </p>\n</blockquote>\n<p>Would such a donation be made monthly or would it be one-time donation when the person does a switch? If it\u2019s monthly, what happens when the donor changes her mind or doesn\u2019t have funds anymore? The person who made the switch is left in an awkward career situation. If it\u2019s a one time donation, what motivates the person who switched to stay in her job?</p>\n<p>Maybe for compensation Allison could ask MIRI to pay her a salary AND donate some money to THL every month. Or she could simply ask MIRI pay her more and then donate the money herself. From MIRI's perspective that's probably similar to hiring a non-EA but this is the best way I see to avoid coordination problems.</p>\n", "parentCommentId": null, "user": {"username": "saulius"}}, {"_id": "5btbMnrpQ5npu6pQc", "postedAt": "2018-04-12T20:46:53.343Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>This is a good point, although talent across time is comparatively harder to estimate. So &quot;act according to present-time comparative advantage&quot; might be a passable approximation in most cases.</p>\n<p>We also need to consider the interim period when thinking about trades across time. If C takes the ops job, then in the period between C taking the job and E joining the movement, we get better ops coverage. It's not immediately obvious to me how this plays out, might need a little bit of modelling.</p>\n", "parentCommentId": "X6RTh5FYQbDHJovDN", "user": {"username": "Michael_PJ"}}, {"_id": "kdN4Sogep7C2occGy", "postedAt": "2018-04-13T02:34:44.699Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>I'm worried that you're mis-applying the concept of comparative advantage here. In particular, if agents A and B both have the same values and are pursuing altruistic ends, comparative advantage should not play a role---both agents should just do whatever they have an absolute advantage at (taking into account marginal effects, but in a large population this should often not matter).</p>\n<p>For example: suppose that EA has a &quot;shortage of operations people&quot; but person A determines that they would have higher impact doing direct research rather than doing ops. Then in fact the best thing is for person A to work on direct research, even if there are already many other people doing research and few people doing ops. (Of course, person A could be mistaken about which choice has higher impact, but that is different from the trade considerations that comparative advantage is based on.)</p>\n<p>I agree with the heuristic &quot;if a type of work seems to have few people working on it, all else equal you should update towards that work being more neglected and hence higher impact&quot; but the justification for that again doesn't require any considerations of trading with other people . In general, if A and B can trade in a mutually beneficial way, then either A and B have different values or one of them was making a mistake.</p>\n", "parentCommentId": null, "user": {"username": "jsteinhardt"}}, {"_id": "2GGAGvRkoCRTTXA9S", "postedAt": "2018-04-13T05:29:07.138Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p><a href=\"http://effective-altruism.com/ea/10s/should_you_switch_away_from_earning_to_give_some/89n\">This poll</a> is an interesting case study in comparative advantage.  It seems that around half of EAs would actually find it easier to work a nonprofit making $40K than earn to give with a salary of $160K and donate $80K.  I'm guessing it has something to do with sensitivity to the endowment effect/loss aversion.</p>\n", "parentCommentId": null, "user": {"username": "John_Maxwell_IV"}}, {"_id": "uXELykXz9xc2z6tHh", "postedAt": "2018-04-13T05:30:38.960Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>Before operations it was AI strategy researchers, and before AI strategy researchers it was web developers.  At various times it has been EtG, technical AI safety, movement-building, etc.  We can't predict talent shortages precisely in advance, so if you're a person with a broad skillset, I do think it might make sense to act as flexible human capital and address whatever is currently most needed.</p>\n", "parentCommentId": "X6RTh5FYQbDHJovDN", "user": {"username": "John_Maxwell_IV"}}, {"_id": "nMdkfKQG58x9Dmh3y", "postedAt": "2018-04-13T13:06:00.217Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>You could try to model by estimating how (i) the talent needs and (ii) the talent availability will be distributed if we further scale the community. </p>\n<p>(i) If you assume that the EA community grows, you may think that the percentage of different skillsets that we need in the community will be different. E.g. you might believe that if the community grows by a factor of 10, we don't need 10x as many people thinking about movement building strategy (the problems size increases not linearly with the number of people) or entrepreneurial skills (as the average org will be larger and more established), but an increase by a factor of say 2-5 might be sufficient. On the other hand, you'd quite likely need ~10x as many ops people. </p>\n<p>(ii) For the talent distribution, one could model this using one of the following assumptions:</p>\n<p>1) Linearly scale the current talent distribution (i.e. assume that the distribution of skillsets in the future community would be the same as today).</p>\n<p>2) Assume that the future talent distribution will become more similar to a relevant reference class (e.g. talent distribution for graduates from top unis)</p>\n<p>A few conclusions e.g. I'd get from this</p>\n<ul>\n<li><p>weak point against skills building in start-ups - if you're great at this, start stuff now</p>\n</li>\n<li><p>weak point in favour of building management skills, especially with assumption 1), but less so with assumption 2)</p>\n</li>\n<li><p>weak point against specialising in areas where EA would really benefit from having just 2-3 experts but unlikely need many more (e.g. history, psychology, institutional decision making, nanotech, geoengineering) if you're also a good fit for sth else, as we might just find them along the way</p>\n</li>\n<li><p>esp. if 2), weak points against working on biorisk (or investing substantially in skills building in bio) if you might be an equal fit for technical AI safety, as the maths/computer science : biologists ratio at most unis is more 1 : 1 (see <a href=\"https://www.hesa.ac.uk/news/11-01-2018/sfr247-higher-education-student-statistics/subjects)\">https://www.hesa.ac.uk/news/11-01-2018/sfr247-higher-education-student-statistics/subjects)</a>, but we probably want to have 5-10x as many people working on AI rather than biorisk.\n[The naive view using current talent distribution might suggest that you should work on bio rather than AI if you're an equal fit, as the current AI : bio talent ratio seems to be &gt; 10: 1]</p>\n</li>\n</ul>\n<p>All of this is less relevant if you believe in high discount rates on work done now rather than in 5-10 years. </p>\n", "parentCommentId": "ZS7xYR6ud8vP5bfzD", "user": {"username": "Sebastian_Oehm"}}, {"_id": "262W5wddQ4zcT6iKk", "postedAt": "2018-04-13T23:14:10.970Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>I think I'd go the other way and suggest people focus more on personal fit: i.e. do the thing in which you have greatest comparative advantage relative to the world as a whole, not just to the EA world. </p>\n", "parentCommentId": "uXELykXz9xc2z6tHh", "user": {"username": "MichaelPlant"}}, {"_id": "cgo5iv8NuEgfteJxP", "postedAt": "2018-04-13T23:18:07.441Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>I agree it's really complicated, but merits some thinking. The one practical implication I take is &quot;if 80k says I should be doing X, there's almost no chance X will be the best thing I could do by the time I'm in a position to do it&quot;</p>\n", "parentCommentId": "iLfYRXoq6uvSdrKgQ", "user": {"username": "MichaelPlant"}}, {"_id": "ZeyBBEcJCsDweLxbw", "postedAt": "2018-04-14T12:19:40.985Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>I really like that idea. It might also be useful to check whether this model would have predicted past changes of career recommendations.</p>\n", "parentCommentId": "nMdkfKQG58x9Dmh3y", "user": {"username": "JanBrauner"}}, {"_id": "XHri77JmFxHuXgToq", "postedAt": "2018-04-14T14:43:08.038Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>Thanks for the very useful link. I think this means if you are one of those people who are okay with donating 50%, and if you donate to one of the smaller organizations that is funding constrained, it really would be high impact.</p>\n", "parentCommentId": "2GGAGvRkoCRTTXA9S", "user": {"username": "Denkenberger"}}, {"_id": "hFCTzHD84hz9X4b4F", "postedAt": "2018-04-16T01:01:09.822Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<blockquote>\n<p>We can do the same for trading talent. People thinking about working in another cause area can ask around whether there\u2019s someone considering switching to a cause area preferred by them. However, trading places in this scenario brings major practical challenges, so it is likely not viable in most cases.</p>\n</blockquote>\n<p>One difficulty with this is that it's hard to go back on the trade if the other person decides to stop cooperating. If you're doing a moral trade of say, being vegetarian in order to get someone to donate more to a poverty charity, you can just stop being vegetarian if the person stops donating. (You should want to do this so that the trades actually maintain validity as trades, rather than means of hijacking people into doing things that fulfill the other person's values.) However, if Allison focuses on biorisk to get Bettina to do animal welfare work, either one is likely to end up with only weakly fungible career capital and therefore be unable to pivot back to their own priorities if the other pulls out. This is particularly bad if fungibility is asymmetrical -- say, if one person cultivated operations experience that can be used many places, while the other built up deep domain knowledge in an area they don't prioritize. It therefore seems important that people considering doing this kind of thing aim not only for having tradable priorities but also similar costs to withdrawing from the trade.</p>\n", "parentCommentId": null, "user": {"username": "Roxanne_Heston"}}, {"_id": "drA3n9PpLyqcZpGiq", "postedAt": "2018-04-16T01:58:51.694Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>I suspect that the motivation hacking you describe is significantly harder for researchers than for, say, operations, HR, software developers, etc. To take your language, I do not think that the cause area beliefs are generally 'prudentially useful' for these roles, whereas in research a large part of your job may on justifying, developing, and improving the accuracy of those exact beliefs.</p>\n<p>Indeed, my gut says that most people who would be good fits for these many critical and under-staffed supporting roles don't need to have a particularly strong or well-reasoned opinion on which cause area is 'best' in order to do their job extremely well. At which point I expect factors like 'does the organisation need the particular skills I have', and even straightforward issues like geographical location, to dominate cause prioritisation.</p>\n<p>I speculate that the only reason this fact hasn't permeated into these discussions is that many of the most active participants, including yourself and Denise, are in fact researchers or potential researchers and so naturally view the world through that lens.</p>\n", "parentCommentId": "rCvFcxZJoJQomg788", "user": {"username": "AGB"}}, {"_id": "898sz5XXjkueYDinY", "postedAt": "2018-04-16T02:01:56.053Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>I agree with your last paragraph, but indeed think that you are being unreasonably idealistic :)</p>\n", "parentCommentId": "rCvFcxZJoJQomg788", "user": {"username": "AGB"}}, {"_id": "8a3KbgLSpuxrYnQMw", "postedAt": "2018-04-16T13:00:47.140Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<blockquote>\n<p>There is a fair number of people who don\u2019t work in their top pick cause area or even cause areas they are much less convinced of than their peers, but currently they don\u2019t advertise this fact.</p>\n</blockquote>\n<p>I think this is likely to be correct. However, I seriously wonder if the distribution is uniform; i.e. are there as much people working on international development while it's not their top pick as on AI Safety? I would say not.</p>\n<p>The next question is whether we should update towards the causes where everyone who works in it is convinced it's top priority, or whether there are other explanations for this hypothesis. I'm not sure how to approach this problem.</p>\n", "parentCommentId": null, "user": {"username": "SiebeRozendal"}}, {"_id": "9RSFi9sKDAzXy56ei", "postedAt": "2018-04-17T00:48:32.763Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<blockquote>\n<p> If we think of the community as needing one ops person and one research person, the marginal value in each area drops to zero once that role is filled.</p>\n</blockquote>\n<p>Yes, but these effects <em>only</em> show up when the number of jobs is small. In particular: If there are already 99 ops people and we are looking at having 99 vs. 100 ops people, the marginal value isn't going to drop to zero. Going from 99 to 100 ops people means that mission-critical ops tasks will be done slightly better, and that some non-critical tasks will get done that wouldn't have otherwise. Going from 100 to 101 will have a similar effect.</p>\n<p>In contrast, in the traditional comparative advantage setting, there remain gains-from-coordination/gains-from-trade even when the total pool of jobs/goods is quite large.</p>\n<p>The fact that gains-from-coordination only show up in the small-N regime here, whereas they show up even in the large-N regime traditionally, seems like a crucial difference that makes it inappropriate to apply standard intuition about comparative advantage in the present setting.</p>\n<p>If we want to analyze this more from first principles, we could pick one of the standard justifications for considering comparative advantage and I could try to show why it breaks down here. The one I'm most familiar with is the one by David Ricardo (<a href=\"https://en.wikipedia.org/wiki/Comparative_advantage#Ricardo's_example)\">https://en.wikipedia.org/wiki/Comparative_advantage#Ricardo's_example)</a>.</p>\n", "parentCommentId": "cYbj8Xk6ybXP3wK3X", "user": {"username": "jsteinhardt"}}, {"_id": "oeLpSfK23pfBycXEL", "postedAt": "2018-04-20T06:22:00.000Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>I agree with the &quot;in short&quot; section. I'm less sure about exactly how it changes things. It seems reasonable to think more about your comparative advantage compared to the world as a whole (taking that as a proxy for the future composition of the community), or maybe just try to think more about which types of talent will be hardest to attract in the long-term. I don't think much the changes in advice about etg and consulting were due to this exact mistake.</p>\n<p>One small thing we'll do to help with this is ask people to project the biggest talent shortages at longer time horizons in our next talent survey.</p>\n", "parentCommentId": "X6RTh5FYQbDHJovDN", "user": {"username": "Benjamin_Todd"}}, {"_id": "KKwYNBvNEh99x3MMo", "postedAt": "2018-04-20T06:24:03.447Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>That seems very strong - you're saying all our recommendations are wrong, even though we're already trying to take account of this effect.</p>\n", "parentCommentId": "cgo5iv8NuEgfteJxP", "user": {"username": "Benjamin_Todd"}}, {"_id": "hXddxNqbkR4rptsYa", "postedAt": "2018-04-20T12:17:35.270Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>I'd hesitate to extrapolate my experience across to operational roles for the reasons you say. That said, my impression was operations folks place a similar emphasis on these things as I. Tanya Singh (one my colleagues) gave a <a href=\"https://www.youtube.com/watch?v=sK4lgWx4gjE\">talk</a> on 'x risk/EA ops'. From the Q&amp;A (with apologies to Roxanne and Tanya for my poor transcription):</p>\n<blockquote>\n<blockquote>\n<p>One common retort we get about people who are interested in operations is maybe they don't need to be value-aligned. Surely we can just hire someone who has operations skills but doesn't also buy into the cause. How true do you think this claim is?</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<p>I am by no means an expert, but I have a very strong opinion. I think it is extremely important to be values aligned to the cause, because in my narrow slice of personal experience that has led to me being happy, being content, and that's made a big difference as to how I approach work. I'm not sure you can be a crucial piece of a big puzzle or a tightly knit group if you don't buy into the values that everyone is trying to push towards. So I think it's very very important. </p>\n</blockquote>\n", "parentCommentId": "drA3n9PpLyqcZpGiq", "user": {"username": "Gregory_Lewis"}}, {"_id": "HEvSYgvpKuNpTiYH3", "postedAt": "2018-09-08T15:49:09.838Z", "postId": "bEPBMu2wB2DE4kaku", "htmlBody": "<p>See our new article about this topic:\n<a href=\"https://80000hours.org/articles/comparative-advantage/\">https://80000hours.org/articles/comparative-advantage/</a></p>\n", "parentCommentId": null, "user": {"username": "Benjamin_Todd"}}]