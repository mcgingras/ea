[{"_id": "g4GZHdXBYCq7zDvhz", "postedAt": "2023-06-06T20:50:44.045Z", "postId": "Pe69bG6uGxLzThYZp", "htmlBody": "<p>I only skimmed the post, but it was unusually usefwl for me even so. I hadn't grokked risk from simply runaway replication of LLMs. Despite studying both evolution &amp; AI, I'd just never thought along this dimension. I always assumed the AI had to be <i>smart</i> in order to be dangerous, but this is a concrete alternative.</p><ol><li>People will continue to iteratively experiment with and improve recursive LLMs, both via fine-tuning and architecture search.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcctieuwyrd\"><sup><a href=\"#fncctieuwyrd\">[1]</a></sup></span></li><li>People will try to automate the architecture search part as soon as their networks seem barely sufficient for the task.</li><li>Many of the subtasks in these systems explicitly involve AIs \"calling\" a copy of themselves to do a subtask.</li><li>OK, I updated: risk is less straightforward than I thought. While the AIs do call copies of themselves, rLLMs can't really undergo a runaway replication cascade unless they can call themselves as \"<a href=\"https://en.wikipedia.org/wiki/Daemon_(computing)\">daemons</a>\" in separate threads (so that the control loop doesn't have to wait for the output before continuing). And I currently don't see an obvious profit motive to do so.</li></ol><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncctieuwyrd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcctieuwyrd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Genetic evolution is a central example of what I mean by \"architecture search\". DNA only encodes the architecture of the brain with little control over <i>what</i> it learns specifically, so genes are selected for how much they contribute to the phenotype's <i>ability </i>to learn.</p><p>While rLLMs will at first be selected for something like profitability, that may not remain the dominant selection criterion for very long. Even narrow agents are likely to have the ability to copy themselves, especially if it involves persuasion. And given that they delegate tasks to themselves &amp; other AIs, it seems very plausible that failure modes include copying itself when it shouldn't, even if they have no internal drive to do so. And once AIs enter the realm of <i>self-replication</i>, their proliferation rate is unlikely to remain dependent on humans at all.</p><p>All this speculation is moot, however, if somebody just tells the AI to maximise copies of itself. That seems likely to happen soon after it's feasible to do so.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Elua"}}, {"_id": "M5QLSJqWXQ3x95jHr", "postedAt": "2023-06-07T09:51:07.933Z", "postId": "Pe69bG6uGxLzThYZp", "htmlBody": "<blockquote><p>OK, I updated: risk is less straightforward than I thought. While the AIs do call copies of themselves, rLLMs can't really undergo a runaway replication cascade unless they can call themselves as \"<a href=\"https://en.wikipedia.org/wiki/Daemon_(computing)\">daemons</a>\" in separate threads (so that the control loop doesn't have to wait for the output before continuing). And I currently don't see an obvious profit motive to do so.</p></blockquote><p>I'm not sure if I understand your point correctly. The LLMs wouldn't have to be replicated, because different copies of the self-replicating agent could access the same LLM in parallel, just like many human users can access ChatGPT at the same time. At the later stage, when the LLM operators try to block access to their LLMs or even take them offline, the agent would have to find a way to replicate at least one LLM just once and run it on sufficiently powerful hardware to use it as (part of) its \"brain\".</p>", "parentCommentId": "g4GZHdXBYCq7zDvhz", "user": {"username": "Karl von Wendt"}}, {"_id": "dEW3cjhe6k5v5ewtm", "postedAt": "2023-06-20T22:05:01.423Z", "postId": "Pe69bG6uGxLzThYZp", "htmlBody": "<p>Karl - I like that you've been able to develop a plausible scenario for a global catastrophic risk that's based mostly on side-effects of evolutionary self-replication, rather than direct power-seeking.</p><p>This seems to be a relatively neglected failure mode for AI. When everybody was concerned about nanotechnology back in the 1990s, the '<a href=\"https://en.wikipedia.org/wiki/Gray_goo\">grey goo</a> scenario' was a major worry (in which self-replicating nanotech turns everything on Earth into copies of itself.) &nbsp;Your story explores a kind of AI version of the grey goo catastrophe.</p>", "parentCommentId": null, "user": {"username": "geoffreymiller"}}]