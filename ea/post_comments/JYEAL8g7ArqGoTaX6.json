[{"_id": "j9aen2xzeemmBQC95", "postedAt": "2023-09-16T13:25:56.266Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Unfortunately, this post got published under the wrong username. I'm the Nora who wrote this post. I hope it can be fixed soon.</p>", "parentCommentId": null, "user": {"username": "Nora Belrose"}}, {"_id": "FbmQJQsDJHGZonwhJ", "postedAt": "2023-09-16T14:40:13.159Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Also near the end bullet 9 should be a subbullet of 8.</p>", "parentCommentId": "j9aen2xzeemmBQC95", "user": {"username": "zsp"}}, {"_id": "5aE2ZTt2RbADFJpq3", "postedAt": "2023-09-16T14:49:41.195Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>This essay seems predicated on a few major assumptions that aren't quite spelled out, or any rate not presented as assumptions.</p><blockquote><p>Far from being \u201cbehind\u201d capabilities, it seems that alignment research has made great strides in recent years. <a href=\"https://arxiv.org/abs/2203.02155\"><u>OpenAI</u></a>&nbsp;and <a href=\"https://arxiv.org/abs/2204.05862\"><u>Anthropic</u></a>&nbsp;showed that Reinforcement Learning from Human Feedback (RLHF) can be used to turn ungovernable large language models into helpful and harmless assistants. Scalable oversight techniques like <a href=\"https://arxiv.org/abs/2212.08073\"><u>Constitutional AI</u></a>&nbsp;and <a href=\"https://openai.com/research/critiques\"><u>model-written critiques</u></a>&nbsp;show promise for aligning the very powerful models of the future. And just this week, it was shown that efficient instruction-following language models can be trained <a href=\"https://arxiv.org/abs/2309.05463\"><u>purely with synthetic text</u></a>&nbsp;generated by a larger RLHF\u2019d model, thereby removing unsafe or objectionable content from the training data and enabling far greater control.</p></blockquote><p>This assumes that making AI behave nice is genuine progress in alignment. The opposing take is that all it's doing is making the AI play a nicer character, but doesn't lead it to internalize its goals, <i>which is what alignment is actually about</i>. And in fact, AI playing rude characters was never the problem to begin with.</p><p>You say that alignment is linked to capability in the essay, but this also seems predicated on the above. This kind of \"alignment\" makes the AI better at figuring out what the humans want, but historically, most thinkers in alignment have always assumed that AI gets good at figuring out what humans want, and that it's dangerous anyway.</p><p>What worries me the most is that the primary reason for this view that's presented in the essay seems to be a social one (or otherwise, I missed it).</p><blockquote><p>We don\u2019t need to speculate about what would happen to AI alignment research during a pause\u2014 we can look at the historical record. Before the launch of GPT-3 in 2020, the alignment community had nothing even <i>remotely</i>&nbsp;like a general intelligence to empirically study, and spent its time doing <a href=\"https://intelligence.org/technical-agenda/\"><u>theoretical research</u></a>, engaging in philosophical arguments on LessWrong, and occasionally performing <a href=\"https://arxiv.org/abs/1606.03137\"><u>toy experiments</u></a>&nbsp;in reinforcement learning.</p><p>The Machine Intelligence Research Institute (MIRI), which was at the forefront of theoretical AI safety research during this period, has since admitted that its efforts have <a href=\"https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy\"><u>utterly failed</u></a>.&nbsp;Stuart Russell\u2019s \u201cassistance game\u201d research agenda, started in 2016, is now widely seen as mostly irrelevant to modern deep learning\u2014 see former student Rohin Shah\u2019s review <a href=\"https://mailchi.mp/59ddebcb3b9a/an-69-stuart-russells-new-book-on-why-we-need-to-replace-the-standard-model-of-ai\"><u>here</u></a>, as well as Alex Turner\u2019s comments <a href=\"https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId%3DCXdcb9sMLkgLANrTv%23CXdcb9sMLkgLANrTv\"><u>here</u></a>. The core argument of Nick Bostrom\u2019s bestselling book <i>Superintelligence</i>&nbsp;has also aged quite poorly.<a href=\"https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire#fntcbltyk9tdq\"><sup>[2]</sup></a></p><p>At best, these theory-first efforts did very little to improve our understanding of how to align powerful AI. And they may have been <i>net negative</i>, insofar as they propagated a variety of actively misleading ways of thinking both among alignment researchers and the broader public. Some examples include the now-debunked <a href=\"https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn\"><u>analogy from evolution</u></a>, the false distinction between <a href=\"https://www.lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into\"><u>\u201cinner\u201d and \u201couter\u201d alignment</u></a>, and the idea that AIs will be rigid utility maximizing consequentialists (<a href=\"https://www.lesswrong.com/posts/yCuzmCsE86BTu9PfA/there-are-no-coherence-theorems\"><u>here</u></a>, <a href=\"https://www.lesswrong.com/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-entail-goal-directed-behavior\"><u>here</u></a>, and <a href=\"https://sohl-dickstein.github.io/2023/03/09/coherence.html\"><u>here</u></a>).</p><p>During an AI pause, I expect alignment research would enter another \u201cwinter\u201d in which progress stalls, and plausible-sounding-but-false speculations become entrenched as orthodoxy without empirical evidence to falsify them. [...]</p></blockquote><p>I.e., Miri's approach to alignment hasn't worked out, therefore the current work is better. But this argument doesn't work -- but approaches can be failures! I think Eliezer would argue that Miri's work had a chance of leading to an alignment solution but has failed, whereas current alignment work (like RLHF on LLMs) has no chance of solving alignment.</p><p>If this is true, then the core argument of this essay collapses, and I don't see a strong argument here that it's not true. Why should we believe that Miri is wrong about alignment difficulty? The fact that their approach failed is not strong evidence of this; if they're right, then they weren't very likely to succeed in the first place.</p><p>And even if they're completely wrong, that <i>still </i>doesn't prove that current alignment approaches have a good chance of working.</p><p>Another assumption you make is that AGI is close and, in particular, will come out of LLMs. E.g.:</p><blockquote><p>Such international persuasion is even less plausible if we assume short, 3-10 year timelines. Public sentiment about AI <a href=\"https://www.weforum.org/agenda/2022/01/artificial-intelligence-ai-technology-trust-survey/\"><u>varies widely</u></a>&nbsp;across countries, and notably, China is among the most optimistic.</p></blockquote><p>This is a case where you agree with most Miri staff but, e.g., Stuart Russel and Steven Byrnes are on record saying that we likely will not get AGI out of LLMs. If this is true, then RLHF done on LLMs is probably even less useful for alignment, and it also means the hard verdict on arguments in superintelligence is unwarranted. Things could still play out a lot more like classical AI alignment thinking in the paradigm that will actually give us AGI.</p><p>And I'm also not ready to toss out the inner vs. outer paradigm just because there was one post criticizing it.</p>", "parentCommentId": null, "user": {"username": "Rafael Harth"}}, {"_id": "nLWAw7hC8x3Gd8iPo", "postedAt": "2023-09-16T15:00:46.903Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Yep, I was also hoping the images could be text-wrapped, but idk if this platform supports that.</p>", "parentCommentId": "FbmQJQsDJHGZonwhJ", "user": {"username": "Nora Belrose"}}, {"_id": "MkjgmgvtzG2JKvTAN", "postedAt": "2023-09-16T15:10:33.730Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>The opposing take is that all it's doing is making the AI play a nicer character, but doesn't lead it to internalize its goals, <i>which is what alignment is actually about</i>.</p></blockquote><p>I think this is a misleading frame which makes alignment seem harder than it actually is. What does it mean to \"internalize\" a goal? It's something like, \"you'll keep pursuing the goal in new situations.\" In other words, goal-internalization is a generalization problem.</p><p>We know a fair bit about how neural nets generalize, although we should study it more (I'm working on a paper on the topic atm). We know they favor \"simple\" functions, which means something like \"low frequency\" in the Fourier domain. In any case, I don't see any reason to think the neural net prior is malign, or particularly biased toward deceptive, misaligned generalization. If anything the simplicity prior seems like good news for alignment.</p>", "parentCommentId": "5aE2ZTt2RbADFJpq3", "user": {"username": "Nora Belrose"}}, {"_id": "QKh4mYFH4tgwvased", "postedAt": "2023-09-16T15:30:44.103Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>One of the three major threads in this post (I think) is&nbsp;<strong>alignment optimism</strong>: AI safety probably isn't super hard.</p><p>A possible implication is that a pause is unnecessary. But the difficulty of alignment doesn't seem to imply much about whether slowing is good or bad, or about its priority relative to other goals.</p><p>(I disagree that gradient descent entails \"we are the innate reward system\" and thus safe, or that \"full read-write access to [AI systems'] internals\" gives safety in the absence of great interpretability. I think likely failure modes include&nbsp;<a href=\"https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to\"><u>AI playing the training game</u></a>,&nbsp;<a href=\"https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like#Part_II__influence_seeking_behavior_is_scary\"><u>influence-seeking behavior dominating</u></a>,&nbsp;<a href=\"https://alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization\"><u>misalignment during capabilities generalization</u></a>, and&nbsp;<a href=\"https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like#Part_I__You_get_what_you_measure\"><u>catastrophic Goodharting</u></a>, and that&nbsp;<a href=\"https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\"><u>AGI Ruin: A List of Lethalities</u></a> is largely right. But I think in this debate we should focus on determining optimal behavior&nbsp;<i>as a function of the difficulty of alignment</i>, rather than having intractable arguments about the difficulty of alignment.)</p>", "parentCommentId": null, "user": {"username": "zsp"}}, {"_id": "kogMxDhvXx6pRukds", "postedAt": "2023-09-16T15:31:04.337Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>One of the three major threads in this post (I think) is&nbsp;<strong>feedback loops &amp; takeoff</strong>: for safety, causing capabilities to increase more gradually and have more time with more capable systems is important, relative to total time until powerful systems appear. By default, capabilities would increase gradually. A pause would create an \"overhang\" and would not be sustained forever; when the pause ends, the overhang entails that capabilities increase rapidly.</p><p>I kinda agree. I seem to think rapid increase in training compute is less likely, would be smaller, and would be less bad than you do. Some of the larger cruxes:</p><ol><li>Magnitude of overhang: it seems the size of the largest training run largely isn't about the cost of compute. Why hasn't someone done a billion-dollar LLM training run, why did we only recently break $10M? I don't know but I'd guess you can't effectively (i.e. you get sharply diminishing returns for doing more than a couple orders of magnitude more than models that have been around for a while), or it's hard to get a big cluster to serialize and so the training run would take years, or something.&nbsp;<a href=\"https://twitter.com/ohlennart/status/1645058017119854592\">Relevant meme:</a><br><img style=\"width:39.97%\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/kogMxDhvXx6pRukds/neoik5mbxgwmyxtqtpug\"></li><li>Magnitude of overhang: endogeneity. AI progress improves AI progress, for reasons like&nbsp;<a href=\"https://ai-improving-ai.safe.ai/\"><u>Examples of AI Improving AI</u></a> and normal iterating and learning from experience. This means takeoff is faster than otherwise, especially in no-pause worlds. So a pause makes fast takeoff worse but not as much as we'd naively think.</li><li>Badness of overhang: I seem to think total-time is more important relative to time-with-powerful-models than you, such that I'd accept a small overhang in exchange for a moderate amount of timeline. Shrug. This is probably because (a) I'm more pessimistic about alignment than you and (b) I'm more optimistic about current alignment research being useful for aligning powerful AI. Probably it's not worth&nbsp;</li></ol><p>I discuss my cruxes in&nbsp;<a href=\"https://www.lesswrong.com/posts/59dKN8XQGx952irWg/cruxes-for-overhang-1\"><u>Cruxes for overhang</u></a> (also relevant:&nbsp;<a href=\"https://www.lesswrong.com/posts/YguseW2zMYe8tMCbW/cruxes-on-us-lead-for-some-domestic-ai-regulation\"><u>Cruxes on US lead for some domestic AI regulation</u></a>).</p><hr><blockquote><p>A pause would create an \"overhang\" and would not be sustained forever</p></blockquote><p>This is too strong. Some pauses could be sustained through AGI, obviating the overhang problem. For example, if you pause slightly below AGI, you get to AGI via algorithmic improvements and inference-time compute increases\u2014the pause doesn't end and overhang isn't an issue.</p>", "parentCommentId": null, "user": {"username": "zsp"}}, {"_id": "HMipMShBr6ymoNzcd", "postedAt": "2023-09-16T15:31:17.863Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>One of the three major threads in this post (I think) is&nbsp;<strong>noticing pause downsides</strong>: in reality, an \"AI pause\" would have various predictable downsides.</p><p>Part of this is your central overhang concerns, which I discuss in <a href=\"https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=kogMxDhvXx6pRukds\">another comment</a>. The rest is:</p><blockquote><ol><li>Illegal AI labs develop inside pause countries, remotely using training hardware outsourced to non-pause&nbsp;countries to evade detection. Illegal labs would presumably put much less emphasis on safety than legal ones.</li><li>There is a brain drain of the least safety-conscious AI researchers to labs headquartered in non-pause countries. Because of remote work, they wouldn\u2019t necessarily need to leave the comfort of their Western home.</li><li>Non-pause governments make opportunistic moves to encourage AI investment and R&amp;D, in an attempt to leap ahead of pause countries while they have a chance. Again, these countries would be less safety-conscious than pause countries.</li><li>Safety research becomes subject to government approval to assess its potential capabilities externalities. This slows down progress in safety substantially, just as the FDA slows down medical research.</li><li>Legal labs exploit loopholes in the definition of a \u201cfrontier\u201d model. Many projects are allowed on a technicality; e.g. they have fewer parameters than GPT-4, but use them more efficiently. This distorts the research landscape in hard-to-predict ways.</li><li>It becomes harder and harder to enforce the pause as time passes, since training hardware is increasingly cheap and miniaturized.</li><li>Whether, when, and how to lift the pause becomes a highly politicized culture war issue, almost totally divorced from the actual state of safety research. The public does not understand the key arguments on either side.</li><li>Relations between pause and non-pause&nbsp;countries are generally hostile. If domestic support for the pause is strong, there will be a temptation to wage war against non-pause countries before their research advances too far:<ol><li>\u201cIf intelligence says that a country outside the agreement is building a GPU cluster, be less scared of a shooting conflict between nations than of the moratorium being violated; be willing to destroy a rogue datacenter by airstrike.\u201d \u2014 <a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\"><u>Eliezer Yudkowsky</u></a></li></ol></li><li>There is intense conflict <i>among</i>&nbsp;pause countries about when the pause should be lifted, which may also lead to violent conflict.</li><li>AI progress in non-pause countries sets a deadline after which the pause <i>must</i>&nbsp;end, if it is to have its desired effect.<a href=\"#fngpfbywtblcj\"><sup>[8]</sup></a>&nbsp;As non-pause countries start to catch up, political pressure mounts to lift the pause as soon as possible. This makes it hard to lift the pause gradually, increasing the risk of dangerous fast takeoff scenarios (see below).</li></ol></blockquote><p>(I have some relevant ideas in&nbsp;<a href=\"https://www.lesswrong.com/posts/YguseW2zMYe8tMCbW/cruxes-on-us-lead-for-some-domestic-ai-regulation\"><u>Cruxes on US lead for some domestic AI regulation</u></a> and&nbsp;<a href=\"https://www.lesswrong.com/posts/59dKN8XQGx952irWg/cruxes-for-overhang-1\"><u>Cruxes for overhang</u></a>.)</p><p>My high-level take: suppose for illustration that \"powerful AI\" is binary and powerful AI would appear by default (i.e. with no pause) in 2030 via a 1e30 FLOP training run. (<a href=\"https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9\"><u>GPT-4 used about 2e25 FLOP.</u></a>) Several of these concerns would apply to&nbsp;<a href=\"https://www.stop.ai/proposals\"><u>1e23 FLOP ceiling</u></a> but not to a 1e28 FLOP ceiling\u2014a 1e28 ceiling would delay powerful AI (powerful AI would be reached by inference-time algorithmic progress and compute increase) but likely not become evadable, let other countries surpass US, etc.&nbsp;<i><strong>I mostly agree with Nora that low-ceiling pauses are misguided\u2014but the upshot of that for me is not \"pause is bad\" but \"pauses should have a high ceiling.\"</strong></i></p><p>Unfortunately, it's pretty uncertain when powerful AI would appear by default, and even if you know the optimal threshold for regulation you can't automatically cause that to occur. But some policy regimes would be more robust to mistakes than \"aim 2\u20133 OOMs below when powerful AI would appear and pause there\"\u2014e.g. starting around 1e26 today and doubling every year.</p><p>Specific takes:</p><ol><li>Yeah, policy regimes should have enforcement to prevent evasion. This is a force pushing toward higher ceilings. Maybe you think evasion is inevitable? I don't, at least for reasonably high ceilings, although I don't know much about it.</li><li>Idk, depends on the details of the policy. I think some experts think US regulation on training runs would largely apply beyond US borders (<a href=\"https://en.wikipedia.org/wiki/Extraterritoriality\"><u>extraterritoriality</u></a>); my impression is US can disallow foreign companies from using US nationals' labor, at least.</li><li>I agree that <i>if US pauses and loses its lead</i> that's a big downside. I don't think that's inevitable, although it is a force pushing toward less ambitious pauses / higher ceilings.&nbsp;See <a href=\"https://www.lesswrong.com/posts/YguseW2zMYe8tMCbW/cruxes-on-us-lead-for-some-domestic-ai-regulation\"><u>Cruxes on US lead for some domestic AI regulation</u></a>.</li><li>Hmm, I don't see how most pause-proposals I've heard of would require government approval for safety research. The exception is proposals that would require government approval for fine-tuning frontier LLMs. Is that right; would it be fine if the regulation only hit base-model training runs (or fine-tuning with absurd amounts of compute, like &gt;1e23 FLOP)?</li><li>My impression is that pause regulation would probably use the metric&nbsp;<i>training compute</i>, which is hard to game.</li><li>Yeah :( so you have to raise the ceiling over time, or set it sufficiently high that you get powerful AI from&nbsp;<i>inference-time</i> progress [algorithmic progress + hardware progress + increased spending] before the policy becomes unenforceable. Go for a less-ambitious pause like 1e27 training FLOP or something.</li><li>This isn't directly bad but it entails&nbsp;<i>maybe the pause is suddenly reversed</i> which is directly bad. This is a major risk from a pause, I think.</li><li>To some extent I think it would be good for a liberal pause alliance to impose its will on defectors. To some extent the US / the liberal pause alliance should set the ceiling sufficiently high that they can pause without losing their lead and should attempt to slow defectors e.g. via export controls.</li><li>Maybe sorta. Violence sounds implausible.</li><li>Partially agree. So (a) go for a less-ambitious pause like 1e27 training FLOP or something, (b) try to slow other countries, and (c) note they partially slow when the US slows because US progress largely causes foreign progress (via publishing research and sharing models).</li></ol><p>Super cruxy for the effects of various possible pauses is how long it would take e.g. China to catch up if US/UK/Europe paused, and how much US/UK/Europe could slow China. I really wish we knew this.</p>", "parentCommentId": null, "user": {"username": "zsp"}}, {"_id": "y2iG8jRhfRQMrvrmC", "postedAt": "2023-09-16T15:31:49.388Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Good post.</p><p>Small things:</p><blockquote><p>2. Increasing the chance of a \u201cfast takeoff\u201d in which one or a handful of AIs rapidly and discontinuously become more capable, concentrating immense power in their hands.</p></blockquote><p>You don't actually discuss concentrating power, I think. (You just say fast takeoff is bad because it makes alignment harder, which is the same as your 1.)</p><blockquote><p>But failing to pause hardware R&amp;D creates a serious problem because, even if we pause the software side of AI capabilities, existing models will continue to get more powerful as hardware improves. Language models are much stronger when they\u2019re allowed to \u201cbrainstorm\u201d many ideas, compare them, and check their own work\u2014 see the&nbsp;<a href=\"https://arxiv.org/abs/2308.09687\"><u>Graph of Thoughts paper</u></a> for a recent example. Better hardware makes these compute-heavy inference techniques cheaper and more effective.</p></blockquote><p>Two clarifications (I know you know\u2014for the others' benefit):</p><p>(a)&nbsp;<i>Software progress</i> includes training-time&nbsp;<i>and inference-time improvements like better prompting or agent scaffolding</i>. You're considering a pause on training-time improvements. \"Existing models will continue to get more powerful\" as inference-time compute&nbsp;<i>and software</i> improve.</p><p>(b) Failing to pause hardware R&amp;D may create training-time compute overhang. I agree with you that existing models will be able to leverage better hardware at inference time, so it probably doesn't create a big inference-time compute overhang. So failing to pause is not \"a serious problem\" in the context of inference-time compute, I think.</p>", "parentCommentId": null, "user": {"username": "zsp"}}, {"_id": "rAjJnxwuzKvgvmtnn", "postedAt": "2023-09-16T15:31:58.279Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I feel like I detect a&nbsp;<a href=\"https://www.econlib.org/archives/2016/01/the_invisible_t.html\"><u>missing mood</u></a> from you where you're skeptical of pausing (for plausible-to-me reasons), but you're not conflicted about it like I am and you don't e.g. look for ways to buy time or ways for regulation to help without the downsides of a pause. (Sorry if this sounds adversarial.) Relatedly, this post is one-sided and so feels&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/scout-mindset\"><u>soldier-mindset</u></a>-y. Likely this is just due to the focus on debating AI pause. But I would feel reassured if you said you're sympathetic to: labs not publishing capabilities research, labs not publishing model weights, dangerous-capability-model-eval-based regulation, US and allies slowing other states and denying them compute, and/or other ways to slow AI or for regulation to help. If you're unsympathetic to these, I would doubt that the overhang nuances you discuss are your&nbsp;<a href=\"https://www.lesswrong.com/posts/TGux5Fhcd7GmTfNGC/is-that-your-true-rejection\"><u>true rejection</u></a> (but I'd be interested in hearing more about your take on slowing and regulation outside of \"pause\").</p><p><i>Edit: man, I wrote this after writing four object-level comments but this got voted to the top. Please note that I'm mostly engaging on the object level and I think object-level discussion is generally more productive\u2014and I think Nora's post makes several good points.</i></p>", "parentCommentId": null, "user": {"username": "zsp"}}, {"_id": "QKHZarkNJjcik2vgu", "postedAt": "2023-09-16T15:58:04.973Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>It's something like, \"you'll keep pursuing the goal in new situations.\" In other words, goal-internalization is a generalization problem.</p></blockquote><p>I think internalizing&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>&nbsp;means \"pursuing&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span></span></span></span>&nbsp;as a <i>terminal</i> goal\", whereas RLHF arguably only makes model pursue&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span></span></span></span>&nbsp;as an instrumental goal (in which case the model would be deceptively aligned). I'm not saying that GPT-4 has a distinction between instrumental and terminal goals, but a future AGI, whether an LLM or not, could have terminal goals that are different from instrumental goals.</p><p>You might argue that deceptive alignment is also an obsolete paradigm, but I would again respond that we don't know this, or at any rate, that the essay doesn't make the argument.</p>", "parentCommentId": "MkjgmgvtzG2JKvTAN", "user": {"username": "Rafael Harth"}}, {"_id": "kqSzGBBr4TCu5ztbq", "postedAt": "2023-09-16T16:01:39.976Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>Some examples include the now-debunked <a href=\"https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn\"><u>analogy from evolution</u></a>, the false distinction between <a href=\"https://www.lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into\"><u>\u201cinner\u201d and \u201couter\u201d alignment</u></a>, and the idea that AIs will be rigid utility maximizing consequentialists (<a href=\"https://www.lesswrong.com/posts/yCuzmCsE86BTu9PfA/there-are-no-coherence-theorems\"><u>here</u></a>, <a href=\"https://www.lesswrong.com/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-entail-goal-directed-behavior\"><u>here</u></a>, and <a href=\"https://sohl-dickstein.github.io/2023/03/09/coherence.html\"><u>here</u></a>).</p></blockquote><p>I feel like you\u2019re trying to round these three things into a \u201cyay versus boo\u201d axis, and then come down on the side of \u201cboo\u201d. I think we can try to do better than that.</p><p>One can make certain general claims about learning algorithms that are true and for which evolution provides as good an example as any. One can also make other claims that are true for evolution and false for other learning algorithms. and then we can argue about which category future AGI will be in. I think we should be open to that kind of dialog, and it involves talking about evolution.</p><p>Likewise, <a href=\"https://www.lesswrong.com/posts/jnmG5jczvWbeRPcvG/four-usages-of-loss-in-ai?commentId=6wpfZfnpJwqMo9BFN#6wpfZfnpJwqMo9BFN\">I think \u201cinner misalignment versus outer misalignment\u201d is a helpful and valid way to classify certain failure modes of certain AI algorithms</a>.</p><p>For the third one, there\u2019s an argument like:</p><p>\u201cMaybe the AI will <i>really want</i> something-or-other to happen in the future, and try to make it happen, including by long-term planning\u2014y'know, the way some humans <i>really want</i> to break out of prison, or the way Elon Musk <i>really wants</i> to go to Mars. Maybe the AIs have other desires and do other things too, but that\u2019s not too relevant to what I\u2019m saying. Next, There are a lot of reasons to think that \u201cAIs that really want something-or-other to happen in the future\u201d will show up sooner or later, e.g. the fact that smart people have been trying to build them since the dawn of AI and continuing through today. And if we get such AIs, and they\u2019re very smart and competent, it has similar <i>relevant</i> consequences as \u201crigid utility maximizing consequentialists\u201d\u2014particularly power-seeking / instrumental convergence, and not pursuing plans that have obvious and effective countermeasures.\u201d</p><p>Do you buy that argument? If so, I think some discussions of \u201crigid utility maximizing consequentialists\u201d can be useful. I <i>also</i> think that some such discussions can lead to conclusions that do not necessarily transfer to more realistic AGIs (see <a href=\"https://www.lesswrong.com/posts/KDMLJEXTWtkZWheXt/consequentialism-and-corrigibility\">here</a>). So again, I think we should avoid yay-versus-boo thinking.</p><blockquote><p>The Machine Intelligence Research Institute (MIRI), which was at the forefront of theoretical AI safety research during this period, has since admitted that its efforts have <a href=\"https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy\"><u>utterly failed</u></a>.</p></blockquote><p>I think that part of the blog post you linked was being facetious. IIUC they had some undisclosed research program involving Haskell for a few years, and then they partly but not entirely wound it down when it wasn\u2019t going as well as they had hoped. But they have also been doing other things too the whole time, like their agent foundations team. (I have no personal knowledge beyond reading the newsletters etc.)</p><p>For example, FWIW, I have personally found MIRI employee Abram Demski\u2019s blog posts (including pre-2020) to be very helpful to my thinking about AGI alignment.</p><p>Anyway, your more general claim in this section seems to be: <i>Given current levels of capabilities, there is no more alignment research to be done. We\u2019re tapped out. The well is dry. The only possible thing left to do is twiddle our thumbs and wait for more capable models to come out.</i></p><p>Is that really your belief? Do you look at literally everything on alignmentforum etc. as total garbage? Obviously I have a COI but I happen to think there is lots of alignment work yet to do that would be helpful and does not need newly-advanced capabilities to happen.</p><p><i>Nothing in this comment should be construed as \u201call things considered we should be for or against the pause\u201d\u2014as it happens I\u2019m weakly against the pause too\u2014these are narrower points. :)</i></p>", "parentCommentId": null, "user": {"username": "steve2152"}}, {"_id": "3xxsumjgHWoJqSzqw", "postedAt": "2023-09-16T16:02:27.236Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>By contrast, AIs implemented using artificial neural networks (ANN) are <strong>white boxes</strong>&nbsp;in the sense that we have&nbsp;full read-write access&nbsp;to their internals. They\u2019re just a special type of computer program, and we can analyze and manipulate computer programs however we want at essentially no cost.&nbsp;</p></blockquote><p>Suppose you walk down a street, and unbeknownst to you, you\u2019re walking by a dumpster that has a suitcase full of millions of dollars. There\u2019s a sense in which you \u201ccan\u201d, \u201cat essentially no cost\u201d, walk over and take the money. But you don\u2019t know that you should, so you don\u2019t. All the value is in the knowledge.</p><p>A trained model is like a computer program with a billion unlabeled parameters and no documentation. Being able to view the code is helpful but doesn\u2019t make it \u201cwhite box\u201d. Saying it\u2019s \u201cessentially no cost\u201d to \u201canalyze\u201d a trained model is just crazy. I\u2019m pretty sure you have met people doing mechanistic interpretability, right? It\u2019s not trivial. They spend months on their projects. The thing you said is just so crazy that I have to assume I\u2019m misunderstanding you. Can you clarify?</p>", "parentCommentId": null, "user": {"username": "steve2152"}}, {"_id": "ZxJgieYm2g7yDs9GG", "postedAt": "2023-09-16T16:18:45.694Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>Nate Soares of the Machine Intelligence Research Institute <a href=\"https://www.youtube.com/watch?v%3DdY3zDvoLoao%26t%3D2332s\"><u>has argued</u></a>&nbsp;that building safe AGI is hard for the same reason that building a successful space probe is hard\u2014 it may not be possible to correct failures in the system after it\u2019s been deployed. Eliezer Yudkowsky makes a similar argument:</p><p>\u201cThis is where <strong>practically all of the real lethality</strong>&nbsp;[of AGI] comes from, that we have to get things right on the first sufficiently-critical try.\u201d \u2014 <a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\"><i><u>AGI Ruin: A List of Lethalities</u></i></a></p></blockquote><p>Eliezer and Nate also both expect discontinuous Takeoff by default. I feel like it's a bit disingenuous to argue that the thinking of Eliezer et al has proven obsolete and misguided, but then also quote them as apparent authority figures in this one case where their arguments align with your essay. It has to be one or the other!</p>", "parentCommentId": null, "user": {"username": "Rafael Harth"}}, {"_id": "4fpAnSvxoto6D9e9v", "postedAt": "2023-09-16T16:21:07.479Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Sorry about this, I believe it has now been fixed.</p>", "parentCommentId": "j9aen2xzeemmBQC95", "user": {"username": "Ben_West"}}, {"_id": "GQnPaKuhtd5sJGSHE", "postedAt": "2023-09-16T16:38:54.313Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I don\u2019t think the terminal vs. instrumental goal dichotomy is very helpful, because it shifts the focus away from behavioral stuff we can actually measure (at least in principle). I also don\u2019t think humans exhibit this distinction particularly strongly. I would prefer to talk about generalization, which is much more empirically testable and has a practical meaning.</p>\n", "parentCommentId": "QKHZarkNJjcik2vgu", "user": {"username": "Nora Belrose"}}, {"_id": "vayMkW3mFmTxjKAAj", "postedAt": "2023-09-16T16:49:25.154Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Where we agree:</p><p>\"dangerous-capability-model-eval-based regulation\" sounds good to me. I'm also in favor of Robin Hanson's <a href=\"https://www.overcomingbias.com/p/foom-liability\">foom liability</a> proposal. These seem like very targeted measures that would plausibly reduce the tail risk of existential catastrophe, and don't have many negative side effects. I'm also not opposed to the US trying to slow down other states, although it'd depend on the specifics of the proposal.</p><p>Where we (partially) disagree:</p><p>I think there's a plausible case to be made that publishing model weights reduces foom risk by making AI capabilities more broadly distributed, and also enhances security-by-transparency. Of course there are concerns about misuse\u2014 I do think that's a real thing to be worried about\u2014 but I also think it's generally exaggerated. I also relatively strongly favor open source on purely normative grounds. So my inclination is to be in favor of it but with reservations. Same goes for labs publishing capabilities research.</p>", "parentCommentId": "rAjJnxwuzKvgvmtnn", "user": {"username": "Nora Belrose"}}, {"_id": "aMEm25HP8j5Sd5uqi", "postedAt": "2023-09-16T16:57:58.965Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Why does it have to be one or the other? I personally don't put much stock in what Eliezer and Nate think, but many other people do.</p>", "parentCommentId": "ZxJgieYm2g7yDs9GG", "user": {"username": "Nora Belrose"}}, {"_id": "itpWDeifi7tFvs3ui", "postedAt": "2023-09-16T16:59:24.247Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>What if it just is the case that AI will be dangerous for reasons that current systems don't exhibit, and hence we don't have empirical data on? If that's the case, then limiting our concerns to only concepts that can be empirically tested seems like it means setting ourselves up for failure.</p>", "parentCommentId": "GQnPaKuhtd5sJGSHE", "user": {"username": "Rafael Harth"}}, {"_id": "9ALFywXMA2j8zE27e", "postedAt": "2023-09-16T17:39:07.382Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote>\n<p>Stuart Russell\u2019s \u201cassistance game\u201d research agenda, started in 2016, is now widely seen as mostly irrelevant to modern deep learning\u2014 see former student Rohin Shah\u2019s review <a href=\"https://mailchi.mp/59ddebcb3b9a/an-69-stuart-russells-new-book-on-why-we-need-to-replace-the-standard-model-of-ai\">here</a>, as well as Alex Turner\u2019s comments <a href=\"https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId%3DCXdcb9sMLkgLANrTv%23CXdcb9sMLkgLANrTv\">here</a>.</p>\n</blockquote>\n<p>The second link just takes me to Alex Turner's shortform page on LW, where ctrl+f-ing \"assistance\" doesn't get me any results. I do find <a href=\"https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=CXdcb9sMLkgLANrTv\">this comment</a> when searching for \"CIRL\", which criticizes the CIRL/assistance games research program, but does not claim that it is irrelevant to modern deep learning. For what it's worth, I think it's plausible that Alex Turner thinks that assistance games is mostly irrelevant to modern deep learning (and plausible that he doesn't think that) - I merely object that the link provided doesn't provide good evidence of that claim.</p>\n<p>The first link is to Rohin Shah's reviews of Human Compatible and some assistance games / CIRL research papers. ctrl+f-ing \"deep\" gets me two irrelevant results, plus one description of a paper \"which is inspired by [the CIRL] paper and does a similar thing with deep RL\". It would be hard to write such a paper if CIRL (aka assistance games) was mostly irrelevant to modern deep learning. The closest thing I can find is in the summary of Human Compatible, which says \"You might worry that the proposed solution [of making AI via CIRL / assistance games] is quite challenging: after all, it requires a shift in the entire way we do AI.\". This doesn't make assistance games irrelevant to modern deep learning - in 2016, it would have been true to say that moving the main thrust of AI research to language modelling so as to produce helpful chatbots required a shift in the entire way we did AI, but research into deeply learned large language models was not irrelevant to deep learning as of 2016 - in fact, it sprung out of 2016-era deep learning.</p>\n", "parentCommentId": null, "user": {"username": "DanielFilan"}}, {"_id": "RbfEzRK7DkxCgEvFr", "postedAt": "2023-09-16T17:52:36.756Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>I\u2019m pretty sure you have met people doing mechanistic interpretability, right?</p></blockquote><p>Nora is <a href=\"https://www.eleuther.ai/staff\">Head of Interpretability</a> at EleutherAI :)</p>", "parentCommentId": "3xxsumjgHWoJqSzqw", "user": {"username": "Ben_West"}}, {"_id": "FeYj7vCfMpn5hxFpD", "postedAt": "2023-09-16T17:58:52.825Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote>\n<p>The core argument of Nick Bostrom\u2019s bestselling book Superintelligence has also aged quite poorly: In brief, the book mostly assumed we will manually program a set of values into an AGI, and argued that since human values are complex, our value specification will likely be wrong, and will cause a catastrophe when optimized by a superintelligence. But most researchers now recognize that this argument is not applicable to modern ML systems which learn values, along with everything else, from vast amounts of human-generated data.</p>\n</blockquote>\n<p>For what it's worth, the book does discuss value learning as a way of an AI acquiring values - you can see chapter 13 as being basically about this.</p>\n<p>I would describe the core argument of the book as the following (going off of my notes of chapter 8, \"Is the default outcome doom?\"):</p>\n<ul>\n<li>It is possible to build AI that's much smarter than humans.</li>\n<li>This process could loop in on itself, leading to takeoff that could be slow or fast.</li>\n<li>A superintelligence could gain a decisive strategic advantage and form a singleton.</li>\n<li>Due to the orthogonality thesis, this superintelligence would not necessarily be aligned with human interests.</li>\n<li>Due to instrumental convergence, an unaligned superintelligence would likely take over the world.</li>\n<li>Because of the possibility of a treacherous turn, we cannot reliably check the safety of an AI on a training set.</li>\n</ul>\n<p>There are things to complain about in this argument (a lot of \"could\"s that don't necessarily cash out to high probabilities), but I don't think it (or the book) assumes that we will manually program a set of values into an AGI.</p>\n", "parentCommentId": null, "user": {"username": "DanielFilan"}}, {"_id": "uZ9ZvjgkxpPdPWBfq", "postedAt": "2023-09-16T18:18:32.072Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I agree that alignment research would suffer during a pause, but I've been wondering recently how much of an issue that is. The key point is that capabilities research would also be paused, so it's not like AI capabilities would be racing ahead of our knowledge on how to control ever more powerful systems. You'd simply be delaying both capabilities and alignment progress.</p><p>You might then ask - what's the point of a pause if alignment research stops? Isn't the whole point of a pause to figure out alignment?</p><p>I'm not sure that's the whole point of a pause. A pause can also give us time to figure out optimal governance structures whether it be standards, regulations etc. These structures can be very important in reducing x-risk. Even if the U.S. is the only country to pause that still gives us more time, because the U.S. is currently in the lead.</p><p>I realise you make other points against a pause (which I think might be valid), but I would welcome thoughts on the 'having more time for governance' point specifically.</p>", "parentCommentId": null, "user": {"username": "jackmalde"}}, {"_id": "D9HyLMKt9CxodCbAM", "postedAt": "2023-09-16T18:31:45.434Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Well, a computer model is \"literally\" transparent in the sense that you can see everything, which means the only difficulty is only in understanding what it means. So the part where you spend 5 million dollars on a PET scanner doesn't exist for ANNs, and in that sense you can analyze them for \"free\".</p><p>If the understanding part is sufficiently difficult... which it sure seems to be... then this doesn't really help, but it is a coherent conceptual difference.</p>", "parentCommentId": "3xxsumjgHWoJqSzqw", "user": {"username": "Rafael Harth"}}, {"_id": "h5sSBzkiQ6ixfNvqX", "postedAt": "2023-09-16T19:00:07.970Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I feel in a number of areas this post relies on the concept of AI being constructed/securitised in a number of ways that seem contradictory to me. (By constructed, I am referring to the way the technology is understood, percieved and anticipated, what narratives it fits into and how we understand it as a social object. By securitised, I mean brought into a limited policy discourse centred around national security that justifies the use of extraordinary measures (eg mass surveillance or conflict) to combat, concerned narrowly with combatting the existential threat to the state, which is roughly equal to the government, states territory and society. )</p><p>For example, you claim that hardware would be unlikely to be part of any pause effort, which would imply that AI is constructed to be important, but not necessarily exceptional (perhaps akin to climate change). This is also likely what would allow companies to easily relocate without major issues. You then claim it is likely international tensions and conflict would occur over the pause, which would imply thorough securitisation such that breaching a pause would be considered a threat enough to national security that conflict could be counternanced; therefore exceptional measures to combat the existential threat are entirely justified(perhaps akin to nuclear weapons or even more severe). Many of your claims of what is 'likely' seem to oscillate between these two conditions, which in a single juristiction seem unlikely to occur simultaeously. You then need a third construction of AI as a technology powerful and important enough to your country to risk conflict with the country that has thoroughly securitised it. SImilarly there must be elements in the paused country that are powerful that also believe it is a super important technology that can be very useful, despite its thorough securitisation (or because of it; I don't wish to project securitisation as necessarily safe or good! Indeed, the links to military development, which could be facilitated by a pasue, may be very dangerous indeed.)</p><p>You may argue back two points; either that whilst all the points couldn't occur simultanously, they are all pluasible. Here I agree, but then the confidence in your language would need to be toned down. Secondly that these different constructions of AI may differ across juristictions, meaning that all of these outcomes are likely. This also seems certainly unlikely, as countries are impacted by each other; narratives do spread, particularly in an interconnected world and particularly if they are held by powerful actors. Moreover, if powerful states are anywhere close to risking conflict over this, other economic or diplomatic measures, would be utilised first, likely meaning the only countries that would continue to develop it would be those who construct it as a super important (those who didn't would likely give into the pressure). In a world where the US or China construct the AI Pause as a vital matter of national security, middle ground countries in their orbit allowing its development would not be counternanced.&nbsp;</p><p>I'm not saying a variety of constructions are not plausible. Nor am I saying that we necessarily fall to the extreme painted in the above paragraph (honestly this seems unlikely to me, but if we don't then a Pause by global cooperation seems more plausible). Rather, I am suggesting that as it stands your idea of 'likely outcomes', are, together, very unlikely to happen, as they rely on different worlds to one another.</p>", "parentCommentId": null, "user": {"username": "Gideon Futerman"}}, {"_id": "5Zuvsbtd2apkg2SY8", "postedAt": "2023-09-16T19:43:30.100Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I'm not sure what one is supposed to do with a claim that can't be empirically tested - do we just believe it/act as if it's true forever? Wouldn't this simply mean an unlimited pause in AI development (and why does this only apply to AI)?</p>", "parentCommentId": "itpWDeifi7tFvs3ui", "user": {"username": "tommcgrath"}}, {"_id": "DwhtKTeC2TX2oggm5", "postedAt": "2023-09-16T19:56:59.769Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>On the last part of your comment - if AGI doesn't come out of LLMs then what would the justification for a pause be?</p>", "parentCommentId": "5aE2ZTt2RbADFJpq3", "user": {"username": "tommcgrath"}}, {"_id": "nFiQu4JrRqDuRMGAk", "postedAt": "2023-09-16T20:13:29.161Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I certainly give relatively little weight to most conceptual AI research. That said, I respect that it's valuable for you and am open to trying to narrow the gap between our views here - I'm just not sure how!</p><p>To be more concrete, I'd value 1 year of current progress over 10 years of pre-2018 research (to pick a date relatively arbitrarily). I don't intend this as an attack on the earlier alignment community, I just think we're making empirical progress in a way that was pretty much impossible before we had good models available to study and I place a lot more value on this.</p>", "parentCommentId": "kqSzGBBr4TCu5ztbq", "user": {"username": "tommcgrath"}}, {"_id": "wz6Sq8BG7JLdSd3rk", "postedAt": "2023-09-16T21:33:03.537Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I think people have started to stretch the \"missing mood\" concept a bit too far for my taste.&nbsp;</p><p>What actual mood is missing here?</p><p>If you think that the default path of AI development leads towards eventual x-risk safety, but that rash actions like an AI pause could plausibly push us off that path and into catastrophe, then your default moods would be \"fervent desire to dissuade people from doing the potentially disastrous thing\", and \"happy that the disastrous thing probably won't happen\". I think this matches with the mood the OP has provided.</p><p>I worry that these sort of meta-critiques can inadvertently be used to pressure people into one side of object-level disagreements. This isn't a dig at you in particular, and I acknowledge that you made object level points as well, which really should be higher than this comment.&nbsp;</p>", "parentCommentId": "rAjJnxwuzKvgvmtnn", "user": {"username": "titotal"}}, {"_id": "JA7EM9zFQnQEqMyP4", "postedAt": "2023-09-16T21:48:00.266Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>What actual mood is missing here?</p></blockquote><ol><li>Noticing the irony that this very natural AI safety idea is (in Nora's view) actually counterproductive and so constructively searching for ways to modify it and for adjacent ideas that don't have its downsides</li><li>Sympathy with the pro-pause position and its proponents</li></ol><p>Also feeling more conflicted in general\u2014there are several real considerations in favor of pausing and Nora doesn't grapple with them. (But this is a debate and maybe Nora is deliberately one-sidedly arguing for a particular position.)</p><p>Maybe \"missing mood\" isn't exactly the right concept.</p>", "parentCommentId": "wz6Sq8BG7JLdSd3rk", "user": {"username": "zsp"}}, {"_id": "ojZ9evKKqj3LKsgfz", "postedAt": "2023-09-16T22:28:39.536Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I think this post provides some pretty useful arguments about the downsides of pausing AI development. I feel noticeably more pessimistic about a pause going well having read this.</p><p>However, I don't agree with some of the arguments about alignment optimism and think they're a fair bit weaker</p><blockquote><p>When it comes to AIs, <strong>we are the innate reward system</strong></p></blockquote><p>Sure, we can use RLHF/related techniques to steer AI behavior. Further,</p><blockquote><p>[gradient descent] is <a href=\"https://www.lesswrong.com/posts/w2TAEvME2yAG9MHeq/gradient-hacking-is-extremely-difficult\"><u>almost impossible to trick&nbsp;</u></a></p></blockquote><p>Sure, unlike in most cases in biology, ANN updates do act on the whole model without noise etc.&nbsp;</p><p>But there are worries about what happens when <strong>AIs get predictably harder to evaluate as they reach superhuman performance on more tasks</strong> that are still very real given all of this! You mention scalable oversight research so it's clear you are aware that this is an open problem, but I don't think this post emphasises enough how most alignment work recognises a pretty big difference between aligning subhuman systems and superhuman systems, which limits the optimism you can get from GPT-4 seeming basically aligned. I think it's possible that with tons of compute and aligned weaker AIs (as you touch upon) we can generalize to aligned GPT-5, GPT-6 etc. But this feels like a pretty different paradigm to the various analogies to the natural world and the current state of alignment!</p>", "parentCommentId": null, "user": {"username": "arthur-conmy"}}, {"_id": "r9SkJax2BgLbHZ89N", "postedAt": "2023-09-16T22:29:59.330Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>In principle, we do the same thing as with any claim (whether explicitly or otherwise):<br>- Estimate the expected value of (directly) testing the claim.<br>- Test it if and only if (directly) testing it has positive EV.<br><br>The point here isn't that the claim is special, or that AI is special - just that the EV calculation consistently comes out negative (unless someone else is about to do something even more dangerous - hence the need for coordination).<br><br>This is unusual and inconvenient. It appears to be the hand we've been dealt.<br>I think you're asking the right question: what <i>is</i> one supposed to do with a claim that can't be empirically tested?</p>", "parentCommentId": "5Zuvsbtd2apkg2SY8", "user": {"username": "Joe Collman"}}, {"_id": "fNHxi6oWxEEHEfvu3", "postedAt": "2023-09-17T09:47:02.183Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>So the point of the \"missing mood\" concept was that it was an indicator for motivated reasoning. If someone reports to you that \"lithuanians are genetically bad at chess\" with a mood of unrestrained glee, you can rightly get suspicious of their methods. If they weren't already prejudiced against lithuanians, they would find the result about chess ability sad and unfortunate.&nbsp;</p><p>I see no similar indicators here. From nora's perspective, the AI pause and similar proposals are a bomb that will hurl us much closer to catastrophe. Why, (from their perspective) &nbsp;would there be a requirement to show sympathy for the bomb-throwers, or propose a modified bomb design?&nbsp;</p><p>Now of course, as a human being nora will have pre-existing biases towards one side or the other, and you can pick apart the piece if you want to find evidence of that (like using the phrase \"heavy handed government regulation\"). But having <i>some</i> bias towards one side doesn't mean your arguments are wrong. The meta can have some uses if it's truly blatant, but it's the object level that actually matters.&nbsp;</p>", "parentCommentId": "JA7EM9zFQnQEqMyP4", "user": {"username": "titotal"}}, {"_id": "X7x8sqzDdvZtDhKWf", "postedAt": "2023-09-17T12:48:59.998Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>It's perhaps also worth separating the claims that A) previous alignment research was significantly less helpful than today's research and B) the reason that was the case continues to hold today.<br><br>I think I'd agree with some version of A, but strongly disagree with B.</p><p>The reason that A seems probably true to me is that we didn't know the basic paradigm in which AGI would arise, and so previous research was forced to wander in the dark. You might also believe that today's focus on empirical research is better than yesterday's focus on theoretical research (I don't necessarily agree) or at least that theoretical research without empirical feedback is on thin ice (I agree).</p><p>I think most people now think that deep learning, perhaps with some modifications, will be what leads to AGI - some even think that LLM-like systems will be sufficient. And the shift from primarily theoretical research to primarily empirical research has already happened. So what will cause today's research to be worse than future research with more capable models? You can appeal to a general principle of \"unknown unknowns,\" but if you genuinely believe that deep learning (or LLMs) will eventually be used in future AGI, it seems hard to believe that knowledge won't transfer at all.</p>", "parentCommentId": "kqSzGBBr4TCu5ztbq", "user": {"username": "Zach Furman"}}, {"_id": "JfSRybgqxqCHbYdnk", "postedAt": "2023-09-17T14:03:00.708Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I have a vague impression\u2014I forget from where and it may well be false\u2014that Nora has read some of <a href=\"https://sjbyrnes.com/agi.html\">my AI alignment research</a>, and that she thinks of it as not entirely pointless. If so, then when I say \u201cpre-2020 MIRI (esp. Abram &amp; Eliezer) deserve some share of the credit for my thinking\u201d, then that\u2019s meaningful, because there is in fact some nonzero credit to be given. Conversely, if you (or anyone) don\u2019t know anything about my AI alignment research, or think it\u2019s dumb, then you should ignore that part of my comment, it\u2019s not offering any evidence, it would just be saying that useless research can sometimes lead to further useless research, which is obvious! :)</p><p>I probably think less of current \u201cempirical\u201d research than you, because <a href=\"https://www.lesswrong.com/posts/KJRBb43nDxk6mwLcR/ai-doom-from-an-llm-plateau-ist-perspective\">I don\u2019t think AGI will look and act and be built just like today\u2019s LLMs but better / larger</a>. I expect highly-alignment-relevant differences between here and there, including (among other things) reinforcement learning being involved in a much more central way than it is today (i.e. RLHF fine-tuning). This is a big topic where I think reasonable people disagree and maybe this comment section isn\u2019t a great place to hash it out. \u00af\\_(\u30c4)_/\u00af</p><p><a href=\"https://sjbyrnes.com/agi.html\">My own research</a> doesn\u2019t involve LLMs and could have been done in 2017, but I\u2019m not sure I would call it \u201cpurely conceptual\u201d\u2014it involves a lot of stuff like scrutinizing data tables in experimental neuroscience papers. The <a href=\"https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk\">ELK research project</a> led by Paul Christiano also could have been done 2017, as far as I can tell, but lots of people seem to think it\u2019s worthwhile; do you? (Paul is a coinventor of RLHF.)</p>", "parentCommentId": "nFiQu4JrRqDuRMGAk", "user": {"username": "steve2152"}}, {"_id": "zZbaEBAS5HaMEXnoy", "postedAt": "2023-09-17T14:12:09.404Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>If you desperately wish we had more time to work on alignment, but also think a pause won\u2019t make that happen or would have larger countervailing costs, then that would lead to an attitude like: \u201cIf only we had more time! But alas, a pause would only make things worse. Let\u2019s talk about other ideas\u2026\u201d For my part, I definitely say things like that (see <a href=\"https://www.lesswrong.com/posts/KJRBb43nDxk6mwLcR/ai-doom-from-an-llm-plateau-ist-perspective#Q__If_LLMs_will_plateau__how_does_that_impact_governance_and__the_pause__\">here</a>).</p><p>However, Nora has sections claiming \u201calignment is doing pretty well\u201d and \u201calignment optimism\u201d, so I think it\u2019s self-consistent for her to not express that kind of mood.</p>", "parentCommentId": "rAjJnxwuzKvgvmtnn", "user": {"username": "steve2152"}}, {"_id": "fzX4L5pAiJci8KHbY", "postedAt": "2023-09-17T14:32:37.626Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>This post has definitely made me more pessimistic on a pause, particularly:</p><p>\u2022 If we pause, it's not clear on how much extra time we get at the end and how much this costs us in terms of crunch time.<br>\u2022 The implementation details are tricky and actors are incentivised to try to work around the limitations.<br><br>On the other hand, I disagree with the following:<br>\u2022 That it is clear that alignment is doing well. There are different possible difficulty levels that alignment could have. I agree that we are in an easier world, where ChatGPT has already achieved a greater amount of outer alignment than we would have expected from some of the old arguments about the impossibility of listing all of our implicit conditions. On the other hand, it's not at all clear that we're anywhere near close to scalable alignment techniques, so there's a pretty decent argument that we're far behind where we need to be.<br>\u2022 Labelling AI's as white box merely because we can see all of the weights. You've got a point. I can see where you're coming from. However, I'm worried that your framing is confusing and will cause people to talk past each other.<br>\u2022 That if there was a pause, alignment research would magically revert back to what it was back in the MIRI days. Admittedly, this is more implied than literally stated, but if we take it literally then it's absurd. There's no shortage of empirical experiments for people to run at the current capability level.<br>\u2022 A large part of the reason why alignment progress was so limited during the last \"pause\" was that only a very few people were working on it. They certainly made mistakes, but I don't think you're fully appreciating the value of the conceptual framework that we inherited from them and how that's informed the empirical work.</p>", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "eazWBNjp2FoNFFAde", "postedAt": "2023-09-17T16:07:58.054Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Insofar as Nora discusses nuances of overhang, it would be odd and annoying for that to not actually be cruxy for her (given that she doesn't say something like <i>this isn't cruxy for me</i>).</p>", "parentCommentId": "zZbaEBAS5HaMEXnoy", "user": {"username": "zsp"}}, {"_id": "n4kfu6qj2N8XhybtT", "postedAt": "2023-09-17T16:32:56.148Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I was reading it as a kinda disjunctive argument. If Nora says that a pause is bad because of A and B, either of which is sufficient on its own from her perspective, then you could say \"A isn't cruxy for her\" (because B is sufficient) or you could say \"B isn't cruxy for her\" (because A is sufficient). Really, neither of those claims is accurate.</p>\n<p>Oh well, whatever, I agree with you that the OP could have been clearer.</p>\n", "parentCommentId": "eazWBNjp2FoNFFAde", "user": {"username": "steve2152"}}, {"_id": "dnH7CJLZaXRNoTbdb", "postedAt": "2023-09-17T16:56:55.926Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>Far from being \u201cbehind\u201d capabilities, it seems that alignment research has made great strides in recent years. <a href=\"https://arxiv.org/abs/2203.02155\"><u>OpenAI</u></a>&nbsp;and <a href=\"https://arxiv.org/abs/2204.05862\"><u>Anthropic</u></a>&nbsp;showed that Reinforcement Learning from Human Feedback (RLHF) can be used to turn ungovernable large language models into helpful and harmless assistants. Scalable oversight techniques like <a href=\"https://arxiv.org/abs/2212.08073\"><u>Constitutional AI</u></a>&nbsp;and <a href=\"https://openai.com/research/critiques\"><u>model-written critiques</u></a>&nbsp;show promise for aligning the very powerful models of the future. And just this week, it was shown that efficient instruction-following language models can be trained <a href=\"https://arxiv.org/abs/2309.05463\"><u>purely with synthetic text</u></a>&nbsp;generated by a larger RLHF\u2019d model, thereby removing unsafe or objectionable content from the training data and enabling far greater control.<br>&nbsp;</p></blockquote><p>&nbsp;</p><p>As far as I am aware, no current AI system, LLM-based or otherwise, is anywhere near capable enough to act autonomously in sufficiently general real-world contexts, such that it actually poses any kind of threat to humans on its own (<a href=\"https://www.lesswrong.com/posts/4Gt42jX7RiaNaxCwP/more-information-about-the-dangerous-capability-evaluations\">even evaluating frontier models for this possibility requires giving them a lot of help</a>). <i>That</i> is where the extinction-level danger lies. It is (mostly) not about human misuse of AI systems, whether that misuse is intentional or adversarial (i.e. a human is deliberately trying to use the AI system to cause harm) or unintentional (i.e. the model is poorly trained or the system is buggy, resulting in harm that neither the user nor the AI system itself intended or wanted.)</p><p><br>I think there's also a technical misunderstanding implied by this paragraph, of how the base model training process works and what the purpose of high-quality vs. diverse training material is. In particular, the primary purpose of removing \"objectionable content\" (and / or low-quality internet text) from the base model training process is to make the training process more efficient, and seems unlikely to accomplish anything alignment-relevant.</p><p>The reason is that the purpose of the base model training process is to build up a model which is capable of predicting the next token in a sequence of tokens which appears in the world somewhere, in full generality. A model which is actually human-level or smarter would (by definition) be capable of predicting, generating, and comprehending objectionable content, even if it had never seen such content during the training process. (See <a href=\"https://www.lesswrong.com/posts/bZbLnr7qwuEBpTPuF/is-gpt-n-bounded-by-human-capabilities-no\">Is GPT-N bounded by human capabilities? No.</a> for more.)</p><p>Using synthetic training data for the RLHF process is maybe more promising, but it depends on the degree to which RLHF works by imbuing the underlying model with the right values, vs. simply <a href=\"https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target\">chiseling</a> away all the bits of model that were capable of imagining and comprehending novel, unseen-in-training ideas in the first place (including objectionable ones, or ones we'd simply prefer the model not think about). Perhaps RLHF works more like the former mechanism, and as a result RLHF (or RLAIF) will \"just work\" as an alignment strategy, even as models scale to human-level and beyond.<br><br>Note that it is possible to gather evidence on this question as it applies to current systems, though I would caution against extrapolating such evidence very far. For example, are there any capabilities that a base model has before RLHF, which are not deliberately trained against during RHLF (e.g. generating objectionable content), which the final model is incapable of doing?</p><p>If, say, the RLHF process trains the model to refuse to generate sexually explicit content, and as a side effect, the RLHF'd model now does worse on answering questions about anatomy compared to the base model, that would be evidence that the RLHF process simply chiseled away the model's ability to comprehend important parts of the universe entirely, rather than imbuing it with a value against answering certain kinds of questions as intended.</p><p>I don't actually know how this particular experimental result would turn out, but either way, I wouldn't expect any trends or rules that apply to current AI systems to continue applying as those systems scale to human-level intelligence or above.<br><br>For my own part, I would like to see a pause on all kinds of AI capabilities research and hardware progress, at least until AI researchers are less confused about a lot of topics like this. As for how realistic that proposal is, whether it likely constitutes a rather <i>permanent</i> pause, or what the consequences of trying and failing to implement such a pause would be, I make no comment, other than to say that sometimes the universe presents you with an unfair, <a href=\"https://www.lesswrong.com/posts/sYgv4eYH82JEsTD34/beyond-the-reach-of-god\">impossible problem</a>.</p>", "parentCommentId": null, "user": {"username": "Max H"}}, {"_id": "tjHJwJLm8t6W4mWG2", "postedAt": "2023-09-17T17:24:01.217Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>There's a giant straw man in this post, and I think it's entirely unreasonable to ignore. It's the assertion, or assumption, that the \"pause\" would be a temporary measure imposed by some countries, as opposed to a stop-gap solution and regulation imposed to enable stronger international regulation, which Nora says she supports. (I'm primarily frustrated by this because it ignores the other two essays, which Nora had access to a week ago, that spelled this out in detail.)</p>", "parentCommentId": null, "user": {"username": "Davidmanheim"}}, {"_id": "itSMPHBeaA9XfoZEH", "postedAt": "2023-09-17T19:54:23.932Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>the \"pause\" would be a temporary measure imposed by some countries, as opposed to a stop-gap solution and regulation imposed to enable stronger international regulation, which Nora says she supports</p></blockquote><p>I don't understand the distinction you're trying to make between these two things. They really seem like the same thing to me, because a stop-gap measure is temporary by definition:<img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/itSMPHBeaA9XfoZEH/h7cvrnwhzrwjloatodmg\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/itSMPHBeaA9XfoZEH/rb9ojsvdp86kjgmeqgzv 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/itSMPHBeaA9XfoZEH/vo7atpwc1rpybzyqsmno 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/itSMPHBeaA9XfoZEH/hlbt36l0iz2i3qbmxuqa 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/itSMPHBeaA9XfoZEH/lnbqspyzx26paz5vchtb 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/itSMPHBeaA9XfoZEH/k2zjbz4vvxuonr85jkyb 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/itSMPHBeaA9XfoZEH/ylvx3oqgkgycawzzrebc 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/itSMPHBeaA9XfoZEH/xqmwxg3efpokho9q8vbn 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/itSMPHBeaA9XfoZEH/tft5wb5kue6dvbr72fcs 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/itSMPHBeaA9XfoZEH/yqu480t03pzdvzqicdtg 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/itSMPHBeaA9XfoZEH/mbvmtiztnpmnjfytmrmm 1040w\">If by \"stronger international regulation\" you mean \"global AI pause\" I argue explicitly that such a global pause is highly unlikely to happen. You don't get to assume that your proposed \"stop-gap\" pause will in fact lead to a global pause just because you called it a stop-gap. What if it doesn't? Will it be worse than no pause at all in that scenario? That's a big part of what we're debating. Is it a \"straw man\" if I just disagree with you about the likely effects of the policies you're proposing?</p><p>I'm also against a global pause even if we can make it happen, and I say so in the post:</p><blockquote><p>If in spite of all this, we somehow manage to establish a global AI moratorium, I think we should be quite worried that the global government needed to enforce such a ban would greatly increase the risk of permanent tyranny, itself an existential catastrophe. I don\u2019t have time to discuss the issue here, but I recommend reading Matthew Barnett\u2019s <i>\u201cThe possibility of an indefinite AI pause\u201d</i>&nbsp;and Quintin Pope\u2019s <i>\u201cAI is centralizing by default; let's not make it worse,\u201d</i>&nbsp;both submissions to this debate.</p></blockquote>", "parentCommentId": "tjHJwJLm8t6W4mWG2", "user": {"username": "Nora Belrose"}}, {"_id": "JiSnNDiWGELrEmmC4", "postedAt": "2023-09-17T20:07:25.747Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>You need to have <i>some</i> motivation for thinking that a fundamentally new kind of danger will emerge in future systems, in such a way that we won't be able to handle it as it arises. Otherwise anyone can come up with any nonsense they like.</p><p>If you're talking about e.g. Evan Hubinger's arguments for deceptive alignment, I think those arguments are very bad, in light of 1) the white box argument I give in this post, 2) the incoherence of Evan's notion of \"mechanistic optimization,\" and 3) his reliance on \"counting arguments\" where you're supposed to assume that the \"inner goals\" of the AI are sampled \"uniformly at random\" from some uninformative prior over goals (I don't think the LLM / deep learning prior is uninformative in this sense at all).</p>", "parentCommentId": "itpWDeifi7tFvs3ui", "user": {"username": "Nora Belrose"}}, {"_id": "CjywLepyWofJCNiiw", "postedAt": "2023-09-17T21:53:56.553Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Yep it's all meant to be disjunctive and yep it could have been clearer. FWIW this essay went through multiple major revisions and at one point I was trying to make the disjunctivity of it super clear but then that got de-prioritized relative to other stuff. In the future if/when I write about this I think I'll be able to organize things significantly better</p>", "parentCommentId": "n4kfu6qj2N8XhybtT", "user": {"username": "Nora Belrose"}}, {"_id": "BG5WubBKitxj9ag9Y", "postedAt": "2023-09-17T22:03:40.137Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Yep I am aware of the value learning section of Chapter 12, which is why I used the \"mostly\" qualifier. That said he basically imagines something like Stuart Russell's CIRL, rather than anything like LLMs or imitation learning.</p><p>If we treat the Orthogonality Thesis as the crux of the book, I also think the book has aged poorly. In fact it should have been obvious when the book was written that the Thesis is basically a motte-and-bailey where you argue for a super weak claim (any combo of intelligence and goals is <i>logically possible</i>), which is itself dubious IMO but easy to defend, and then pretend like you've proven something much stronger, like \"intelligence and goals will be empirically uncorrelated in the systems we actually build\" or something.</p>", "parentCommentId": "FeYj7vCfMpn5hxFpD", "user": {"username": "Nora Belrose"}}, {"_id": "xQMLvAYp3XYZs3bAX", "postedAt": "2023-09-17T22:10:18.916Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>That if there was a pause, alignment research would magically revert back to what it was back in the MIRI days</p></blockquote><p>The claim is more like, \"the MIRI days are a cautionary tale about what may happen when alignment research isn't embedded inside a feedback loop with capabilities.\" I don't literally believe we would revert back to pure theoretical research during a pause, but I do think the research would get considerably lower quality.</p><blockquote><p>However, I'm worried that your [white box] framing is confusing and will cause people to talk past each other.</p></blockquote><p>Perhaps, but I think the current conventional wisdom that neural nets are \"black box\" is itself a confusing and bad framing and I'm trying to displace it.</p>", "parentCommentId": "fzX4L5pAiJci8KHbY", "user": {"username": "Nora Belrose"}}, {"_id": "CLi5eBchYfXKZvXuD", "postedAt": "2023-09-17T22:19:30.699Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>It's essentially no cost to run a gradient-based optimizer on a neural network, and I think this is sufficient for good-enough alignment. I view the the interpretability work I do at Eleuther as icing on the cake, allowing us to steer models even more effectively than we already can. Yes, it's not zero cost, but it's dramatically lower cost than it would be if we had to crack open a skull and do neurosurgery.</p><p>Also, if by \"mechanistic interpretability\" you mean \"circuits\" I'm honestly pretty pessimistic about the usefulness of that kind of research, and I think the really-useful stuff is lower cost than circuits-based interp.</p>", "parentCommentId": "3xxsumjgHWoJqSzqw", "user": {"username": "Nora Belrose"}}, {"_id": "ghvHBhXfofxtNuNcJ", "postedAt": "2023-09-17T22:49:05.197Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Hmm, <i>AI safety is probably easy</i> implies that slowing AI is lower-stakes but doesn't obviously imply much about whether it's net-positive. It's not obvious to me what alignment optimism has to do with the pause debate, and I don't think you discuss this.</p>", "parentCommentId": "CjywLepyWofJCNiiw", "user": {"username": "zsp"}}, {"_id": "qHNWxFcmFLYadPswL", "postedAt": "2023-09-17T22:54:01.678Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>It's not obvious to me what alignment optimism has to do with the pause debate</p></blockquote><p>Sorry, I thought it would be fairly obvious how it's related. If you're optimistic about alignment then the expected benefits you might <i>hope</i> to get out of a pause (whether or not you actually do get those benefits) are commensurately smaller, so the unintended consequences should have more relative weight in your EV calculation.</p><p>To be clear, I think <i>slowing down AI</i> in general, as opposed to the moratorium proposal in particular, is a more reasonable position that's a bit harder to argue against. I do still think the overhang concerns apply in non-pause slowdowns but in a less acute manner.</p>", "parentCommentId": "ghvHBhXfofxtNuNcJ", "user": {"username": "Nora Belrose"}}, {"_id": "FpKEvNqECkJiwSanY", "postedAt": "2023-09-17T22:59:47.137Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Given alignment optimism, the benefits of pause are smaller\u2014but the unintended consequences for alignment are smaller too. I guess alignment optimism suggests pause-is-bad if e.g. your alignment optimism is super conditional on smooth progress...</p>", "parentCommentId": "qHNWxFcmFLYadPswL", "user": {"username": "zsp"}}, {"_id": "cFAAnguxWvXGZ3kKd", "postedAt": "2023-09-17T23:43:52.205Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>If you want to say \"it's a black box but the box has a \"gradient\" output channel in addition to the \"next-token-probability-distribution\" output channel\", then I have no objection.</p>\n<p>If you want to say \"...and those two output channels are sufficient for safe &amp; beneficial AGI\", then you can say that too, although I happen to disagree.</p>\n<p>If you want to say \"we also have interpretability techniques on top of those, and they work well enough to ensure alignment for both current and future AIs\", then I'm open-minded and interested in details.</p>\n<p>If you want to say \"we can't understand how a trained model does what it does in any detail, but if we had to drill into a skull and only measure a few neurons at a time etc. then things sure would be <em>even worse</em>!!\", then yeah duh.</p>\n<p>But your OP said \"They\u2019re just a special type of computer program, and we can analyze and manipulate computer programs however we want at essentially no cost\", and used the term \"white box\". That's the part that strikes me as crazy. To be charitable, I don't think those words are communicating the message that you had intended to communicate.</p>\n<p>For example, find a random software engineer on the street, and ask them: \"if I give you a 1-terabyte compiled executable binary, and you can do whatever you want with that file on your home computer, would you describe it as closer to \"white box\" or \"black box\"?\". I predict most people would say \"closer to black box\", even though they can look at all the bits and step through the execution and run decompilation tools etc. if they want. Likewise you can ask them whether it's possible to \"analyze\" that binary \"at essentially no cost\". I predict most people would say \"no\".</p>\n", "parentCommentId": "CLi5eBchYfXKZvXuD", "user": {"username": "steve2152"}}, {"_id": "x4iquovHL4cwEbWbX", "postedAt": "2023-09-18T00:38:39.320Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Could you say more about what you see as the practical distinction between a \"slow down AI in general\" proposal vs. a \"pause\" proposal?</p>", "parentCommentId": "qHNWxFcmFLYadPswL", "user": {"username": "IanDavidMoss"}}, {"_id": "WSEgzRfiey3r4iuGQ", "postedAt": "2023-09-18T03:00:56.155Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Thanks very much for writing this very interesting piece!</p><p>The \"AI safety winter\" section argues that pre-2020, AI alignment researchers made little progress because they had no AI to work on aligning. But now that we have GPT-4 etc., I feel like we have a capabilities overhang, and it seems like there is plenty of AI alignment researchers to work on for the next 6 months or so? Then their work could be 'tested' by allowing some more algorithmic progress.</p>", "parentCommentId": null, "user": {"username": "Larks"}}, {"_id": "qisPbHyDHMKxgNGeh", "postedAt": "2023-09-18T03:03:53.837Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Differentiability is a pretty big part of the white box argument.</p><p>The terabyte compiled executable binary is still white box in a minimal sense but it's going to take a lot of work to mould that thing into something that does what you want. You'll have to decompile it and do a lot of static analysis, and Rice's theorem gets in the way of the kinds of stuff you can prove about it. The code might be adversarially obfuscated, although literal <a href=\"https://en.wikipedia.org/wiki/Black-box_obfuscation\">black box obfuscation is provably impossible</a>.</p><p>If instead of a terabyte of compiled code, you give me a trillion neural net weights, I can fine tune that network to do a lot of stuff. And if I'm worried about the base model being preserved underneath and doing nefarious things, I can generate synthetic data from the fine tuned model and train a fresh network from scratch on that (although to be fair that's pretty compute-intensive).</p>", "parentCommentId": "cFAAnguxWvXGZ3kKd", "user": {"username": "Nora Belrose"}}, {"_id": "afRogXqpzPxqoHSsp", "postedAt": "2023-09-18T04:00:09.013Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>AI safety currently seems to heavily lean towards empirical and this emphasis only seems to be growing, so I\u2019m rather skeptical that a bit more theoretical work on the margin will be some kind of catastrophe. I\u2019d actually expect it to be a net positive.</p>\n", "parentCommentId": "xQMLvAYp3XYZs3bAX", "user": {"username": "casebash"}}, {"_id": "ssZQwcf7jhWhcEo7A", "postedAt": "2023-09-18T06:22:37.096Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>First, it sounds like you are agreeing with others, including myself, about a pause.&nbsp;</p><blockquote><p>An<a href=\"https://twitter.com/GaryMarcus/status/1640884040835428357\"><u> immediate, temporary pause</u></a> isn\u2019t currently possible to monitor, much less enforce, even if it were likely that some or most parties would agree. Similarly, a single company or country announcing a unilateral halt to building advanced models is not credible without assurances, and is likely both ineffective at addressing the broader race dynamics, and differentially advantages the least responsible actors.</p></blockquote><p>&nbsp;</p><blockquote><p>I am&nbsp;<i>not&nbsp;</i>advocating for a pause right now. If we had a pause, I think it would only be useful insofar as we use the pause to implement governance structures that mitigate risk after the pause has ended.&nbsp;</p></blockquote><p>&nbsp;</p><blockquote><p>If something slows progress <i>temporarily</i>, after it ends progress may gradually partially catch up to the pre-slowing trend, such that powerful AI is delayed but crunch time is shortened</p></blockquote><p>So yes, you're arguing against a straw-man. (Edit to add: Perhaps Rob Bensinger's views are more compatible with the claim that someone is advocating a temporary pause as a good idea - but he has said that ideally he wants a full stop, not a pause at all.)</p><p>&nbsp;</p><p>Second, you're ignoring half of what stop-gap means, in order to say it just means pausing, without following up. But it doesn't.</p><blockquote><p>If by \"stronger international regulation\" you mean \"global AI pause\" I argue explicitly that such a global pause is highly unlikely to happen.</p></blockquote><p>I laid out in pretty extensive detail what I meant as the steps that need to be in place now, and none of them are a pause; immediate moves by national governments to monitor compute and clarify that laws apply to AI systems, and that they will be enforced, and commitments to build an international regulatory regime.</p><p>And the alternative to what you and I agree would be an infeasible pause, you claim, is a sudden totalitarian world government. This is the scary false alternative raised by the other essays as well, and it seems disengenious to claim that we'd suddenly emerge into a global dictatorship, by assumption. It's exactly parallel to arguments raised against anti-nuclear proliferation plans. But we've seen how that worked out - nuclear weapons were mostly well contained, and we still don't have a 1984-like global government. So it's strange to me that you think this is a reasonable argument, unless you're using it as a scare tactic.</p>", "parentCommentId": "itSMPHBeaA9XfoZEH", "user": {"username": "Davidmanheim"}}, {"_id": "SJFqkd8RDeKyrQjMm", "postedAt": "2023-09-18T07:18:11.935Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I do not think the orthogonality thesis is a motte-and-bailey. &nbsp;The only evidence I know of that suggests that the goals developed by an ASI trained with something resembling modern methods would by default be picked from a distribution that's remotely favorable to us is the evidence we have from evolution<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefau44pmqo8fp\"><sup><a href=\"#fnau44pmqo8fp\">[1]</a></sup></span>, but I really think that ought to be screened off. The goals developed by various animal species (including humans) as a result of evolution are contingent on specific details of various evolutionary pressures and environmental circumstances, which we know with confidence won't apply to any AI trained with something resembling modern methods.</p><p>Absent a specific reason to believe that we will be sampling from an extremely tiny section of an enormously broad space, why should we believe we will hit the target?</p><p>Anticipating the argument that, since we're doing the training, we can shape the goals of the systems - this would certainly be reason for optimism if we had any idea what goals we would see emerge while training superintelligent systems, and had any way of actively steering those goals to our preferred ends. &nbsp;We don't have either, right now.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnau44pmqo8fp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefau44pmqo8fp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Which, mind you, is still unfavorable; I think the goals of most animal species, were they to be extrapolated outward to superhuman levels of intelligence, would not result in worlds that we would consider very good. &nbsp;Just not nearly as unfavorable as what I think the actual distribution we're facing is.</p></div></li></ol>", "parentCommentId": "BG5WubBKitxj9ag9Y", "user": {"username": "T3t"}}, {"_id": "8oe9o5QBjbNeNakwH", "postedAt": "2023-09-18T07:24:13.219Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>If instead of a terabyte of compiled code, you give me a trillion neural net weights, I can fine tune that network to do a lot of stuff.</p></blockquote><p>But this is irrelevant to the original claim, right? &nbsp;Being able to fine-tune might make introspection on its interal algorithmic representations a bit cheaper, but in practice we observe that it takes us weeks or months of alignment researchers' time to figure out what extremely tiny slices of two-generations-old LLMs are doing.</p>", "parentCommentId": "qisPbHyDHMKxgNGeh", "user": {"username": "T3t"}}, {"_id": "dTxbDTskgzt2CHk4e", "postedAt": "2023-09-18T07:41:37.535Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I don't think the comparison with human alignment being successful is fair.&nbsp;</p><p>If you mean that most people don't go on to be antisocial etc.. which is comparable to non-X AI risk, the yes perhaps simple techniques like a 'good upbringing' are working on humans. A lot of it however is just baked in by evolution regardless. If you mean that most humans don't go on to become X-risks, then that mostly has to do with lack of capability, rather than them being aligned. There are very few people I would trust with 1000x human abilities, assuming everyone else remains a 1x human.&nbsp;</p>", "parentCommentId": null, "user": {"username": "EdoardoPona"}}, {"_id": "vARKNX8PyFaHYJurT", "postedAt": "2023-09-18T07:58:46.942Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>Absent a specific reason to believe that we will be sampling from an extremely tiny section of an enormously broad space, why should we believe we will hit the target?</p></blockquote><p>I could make this same argument about capabilities, and be demonstratably wrong. &nbsp;The space of neural network values that <i>don't </i>produce coherent grammar is unimaginably, ridiculously vast compared to the \"tiny target\" of ones that do. But this obviously doesn't mean that chatGPT is impossible.&nbsp;</p><p>The reason is that we aren't randomly throwing a dart at possibility space, but using a highly efficient search mechanism to rapidly toss out bad designs until we hit the target. But when these machines are trained, we simultaneously select for capabilities <i>and</i> for alignment (murderbots are not efficient translators). For chatGPT, this leads to an \"aligned\" machine, at least by some definitions.&nbsp;</p><p>Where I think the motte and bailey often occurs is jumping between \"aligned enough not to exterminate us\", and \"aligned with us nearly perfectly in every way\" or \"unable to be misused by bad actors\". The former seems like it might happen naturally over development, whereas the latter two seem nigh impossible.&nbsp;</p>", "parentCommentId": "SJFqkd8RDeKyrQjMm", "user": {"username": "titotal"}}, {"_id": "4QFRE4CEvR38Ed7Sz", "postedAt": "2023-09-18T08:44:03.326Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>Should&nbsp;we lobby governments to impose a moratorium on AI research? Since we don\u2019t enforce pauses on most new technologies, I hope the reader will grant that the burden of proof is on those who advocate for such a moratorium.</p></blockquote><p>You could have stopped here. This is our crux.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Holly_Elmore"}}, {"_id": "D6E9TWAQatsNWx98d", "postedAt": "2023-09-18T13:16:51.095Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I don\u2019t think \u201cmouldability\u201d is a synonym of \u201cwhite-boxiness\u201d. In fact, I think they\u2019re hardly related at all:</p><ul><li>There can be a black box with lots of knobs on the outside that change the box\u2019s behavior. It\u2019s still a black box.</li><li>Conversely, consider an old-fashioned bimetallic strip thermostat with a broken dial. It\u2019s not mouldable at all\u2014it can do one and only thing, i.e. actuate a switch at a certain fixed temperature. (Well, I guess you can use it as a doorstop!) But a bimetallic strip thermostat still very white-boxy (after I spend 30 seconds telling you how it works).&nbsp;</li></ul><p>You wrote \u201cThey\u2019re just a special type of computer program, and we can analyze and manipulate computer programs however we want at essentially no cost.\u201d I feel like I keep pressing you on this, and you keep motte-and-bailey'ing into some other claim that does not align with a common-sense reading of what you originally wrote:</p><ul><li>\u201cWell, the cost of analysis could theoretically be <i>even higher</i>\u2014like, if you had to drill into skulls\u2026\u201d OK sure but that\u2019s not the same as \u201cessentially no cost\u201d.</li><li>\u201cWell, the cost of analysis may be astronomically high, but there\u2019s a theorem proving that it\u2019s not theoretically impossible\u2026\u201d OK sure but that\u2019s not the same as \u201cessentially no cost\u201d.</li><li>\u201cWell, I can list out some specific analysis and manipulation tasks that we can do at essentially no cost: we can do X, and Y, and Z, \u2026\u201d OK sure but that\u2019s not the same as \u201cwe can analyze and manipulate <i>however we want</i> at essentially no cost\u201d.</li></ul><p>Do you see what I mean?</p>", "parentCommentId": "qisPbHyDHMKxgNGeh", "user": {"username": "steve2152"}}, {"_id": "fu7o7x7tQDszJqWL7", "postedAt": "2023-09-18T13:29:38.779Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I agree that the question of \"what priors to use here\" is super important.</p><p>For example, if someone would chose priors for \"we usually don't bring new more intelligent life forms to live with us, so the burden of proof is on doing so\" - would that be valid?</p><p>Or if someone would say \"we usually don't enforce pauses on writing new computer programs\" - would THAT be valid?</p><p>imo: the question of \"what priors to use\" is important and not trivial. I agree with <a href=\"https://forum.effectivealtruism.org/users/holly_elmore?mention=user\">@Holly_Elmore</a> that just assuming the priors here is skipping over some important stuff. But I disagree that \"you could have stopped here\", since there might be things which I could use to update my own (different) prior</p>", "parentCommentId": "4QFRE4CEvR38Ed7Sz", "user": {"username": "hibukki"}}, {"_id": "BngquNuF6ftpwzefs", "postedAt": "2023-09-18T16:23:45.491Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>In a debate, which is what was supposed to be happening, the point is to make claims that either support or refute the central claim. That's what Holly was pointing out - this is a fundamental requirement for accepting Nora's position. (I don't think that this is the only crux - \"AI Safety is gonna be easy\" and \"AI is fully understandable\" are two far larger cruxes, but they largely depend on this first one.)</p>", "parentCommentId": "fu7o7x7tQDszJqWL7", "user": {"username": "Davidmanheim"}}, {"_id": "3ZuyK9Sgs9zX4ABCi", "postedAt": "2023-09-18T16:26:20.573Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>That progress is incredibly fast, and new architectures explicitly aimed at creating AGI are getting proposed and implemented. (I'm agnostic about whether LLMs will scale past human reasoning - it seems very plausible they won't. But I don't think it matters, because that's not the only research direction with tons of resources being put into it that create existential risks.)</p>", "parentCommentId": "DwhtKTeC2TX2oggm5", "user": {"username": "Davidmanheim"}}, {"_id": "rXDXguGYWhQwYW2Jj", "postedAt": "2023-09-18T16:30:23.138Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>You need to have <i>some</i> motivation for thinking that a fundamentally new kind of danger will emerge in future systems, in such a way that we won't be able to handle it as it arises.</p></blockquote><p>That was what everyone ins AI safety was discussing for a decade or more, until around 2018. You seem to ignore these arguments about why AI will be dangerous, as well as all of the arguments that alignment will be hard. Are you familiar with all of that work?</p>", "parentCommentId": "JiSnNDiWGELrEmmC4", "user": {"username": "Davidmanheim"}}, {"_id": "TqYZ3Lifq7wXyuZSB", "postedAt": "2023-09-18T17:19:41.535Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I asked Alex \"no chance you can comment on whether you think assistance games are mostly irrelevant to modern deep learning?\"</p>\n<p>His response was \"i think it's mostly irrelevant, yeah, with moderate confidence\". He then told me he'd lost his EA forum credentials and said I should feel free to cross-post his message here.</p>\n<p>(For what it's worth, as people may have guessed, I disagree with him - I think you can totally do CIRL-type stuff with modern deep learning, to the extent you can do anything with modern deep learning.)</p>\n", "parentCommentId": "9ALFywXMA2j8zE27e", "user": {"username": "DanielFilan"}}, {"_id": "QKKhRhEdptcvdwgdJ", "postedAt": "2023-09-18T17:23:04.068Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Interesting - what do you have in mind for fast-progressing architectures explicitly aimed at creating AGI?</p><p>On your 2nd point on x-risks from non-LLM AI, am I right in thinking that you would also hope to catch dual-use scientific AI (for instance) in a compute governance scheme and/or pause? That's a considerably broader remit than I've seen advocates of a pause/compute restrictions argue for and seems much harder to achieve both politically and technically.</p>", "parentCommentId": "3ZuyK9Sgs9zX4ABCi", "user": {"username": "tommcgrath"}}, {"_id": "LtpdRd567ZN7MpoiX", "postedAt": "2023-09-18T17:43:19.668Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Thanks I mean more in terms of \"how can we productively resolve our disagreements about this?\", which the EV calculations are downstream of. To be clear, it doesn't seem to me that this is necessarily the hand we've been dealt but I'm not sure how to reduce the uncertainty.</p><p>At the risk of sidestepping the question, the obvious move seems to be \"try harder to make the claim empirically testable\"! For example, in the case of deception, which I think is a central example we could (not claiming these ideas are novel):</p><ol><li>Test directly for deception behaviourally and/or mechanistically (I'm aware that people are doing this, think it's good and wish the results were more broadly shared).</li><li>Think about what aspects of deception make it particularly hard, and try to study those in isolation and test those. The most important example seems to me to be precursors: finding more testable analogues to the question of \"before we get good, undetectable deception do we get kind of crappy detectable deception?\"</li></ol><p>Obviously these all run some (imo substantially lower) risks but seem well worth doing. Before we declare the question empirically inaccessible we should at least do these and synthesise the results (for instance, what does grokking say about (2)?).</p>", "parentCommentId": "r9SkJax2BgLbHZ89N", "user": {"username": "tommcgrath"}}, {"_id": "2LDozCJ6PfLDT3b75", "postedAt": "2023-09-18T17:53:51.054Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>(I'm spinning this comment out because it's pretty different in style and seems worth being able to reply to separately. Please let me know if this kind of chain-posting is frowned upon here.)</p><p>Another downside to declaring things empirically out of reach and relying on priors for your EV calculations and subsequent actions is that it more-or-less inevitably converts epistemic disagreements into conflict.&nbsp;</p><p>If it seems likely to you that this is the way things are (and so we should pause indefinitely) but it seems highly unlikely to me (and so we should not) then we have no choice but to just advocate for different things. There's not even the prospect of having recourse to better evidence to win over third parties, so the conflict becomes no-holds-barred. I see this right now on Twitter and it makes me very sad. I think we can do better.</p>", "parentCommentId": "LtpdRd567ZN7MpoiX", "user": {"username": "tommcgrath"}}, {"_id": "C6hwF37mLhzoALzrq", "postedAt": "2023-09-18T18:03:43.438Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>*As far as my essay (not posted yet) was concerned, she could have stopped there, because this is our crux.</p>", "parentCommentId": "fu7o7x7tQDszJqWL7", "user": {"username": "Holly_Elmore"}}, {"_id": "ZGwR5TGLvAxxnX7zS", "postedAt": "2023-09-18T18:22:20.928Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I've certainly heard of your work but it's far enough out of my research interests that I've never taken a particularly strong interest. Writing this in this context makes me realise I might have made a bit of a one-man echo chamber for myself... Do you mind if we leave this as 'undecided' for a while?</p><p>Regarding ELK - I think the core of the problem as I understand it is fairly clear once you begin thinking about interpretability. Understanding the relation between AI and human ontologies was part of the motivation behind <a href=\"https://arxiv.org/abs/2111.09259\">my work on alphazero</a> (as well as an interest in the natural abstractions hypothesis). Section 4 \"Encoding of human conceptual knowledge\" and Section 8 \"Exploring activations with unsupervised methods\" are the places to look. The section on challenges and limitations in concept probing I think echoes a lot of the concerns in ELK.&nbsp;</p><p>In terms of subsequent work on ELK, I don't think much of the work on solving ELK was particularly useful, and often reinvented existing methods (e.g. <a href=\"https://www.lesswrong.com/posts/zjMKpSB2Xccn9qi5t/elk-prize-results#Strategy__penalize_reporters_for_depending_on_too_many_activations_from_the_predictor\">sparse probing</a>, <a href=\"https://www.lesswrong.com/posts/zjMKpSB2Xccn9qi5t/elk-prize-results#Strategy__use_the_reporter_to_define_causal_interventions_on_the_predictor\">causal interchange interventions</a>). If I were to try and work on it then I think the best way to do so would be to embed the core challenge in a tractable research program, for instance trying to extract new scientific knowledge from ML models like alphafold.</p><p>To move this in a more positive direction, the most fruitful/exciting conceptual work I've seen is probably (1) the natural abstractions hypothesis and (2) debate. When I think a bit about why I particularly like these, for (1) it's because it seems plausibly true, extremely useful if true, and amenable to both formal theoretical work and empirical study. For (2) it's because it's a pretty striking new idea that seems very powerful/scalable, but also can be put into practice a bit ahead of really powerful systems.</p>", "parentCommentId": "JfSRybgqxqCHbYdnk", "user": {"username": "tommcgrath"}}, {"_id": "qhmGb38sbWrAdkXiP", "postedAt": "2023-09-18T19:13:12.813Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>If regulators or model review firms have any flexibility (which seems very plausible,) and the danger of AGI is recognized (which seems increasingly likely,) once there is any recognition of promising progress towards AGI, review of the models for safety would occur - as it should, as in any other engineering discipline, albeit in this case more like civil engineering, where lives are on the line, than software engineering, where they usually aren't.</p><p>And considering other risks, as I argued in my piece, there's an existing requirement for countries to ban bioweapons development, again, as there should be. I'm simply proposing that countries should fulfill that obligation, in this case, by requiring review of potentially dangerous research into ML which can be applied to certain classes of virology.</p>", "parentCommentId": "QKKhRhEdptcvdwgdJ", "user": {"username": "Davidmanheim"}}, {"_id": "HoTvZ8bsCwypisDFR", "postedAt": "2023-09-18T19:21:04.385Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Yes. This one seems critical, and I don't understand it at all.</p>", "parentCommentId": "QKh4mYFH4tgwvased", "user": {"username": "Davidmanheim"}}, {"_id": "BK9jNBrxBogpqjPGd", "postedAt": "2023-09-18T19:22:13.405Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p><s>Are you presenting arguments that you think will convince others, regardless of whether you think they are correct?</s></p><p>Edit: Apologies, this <a href=\"https://www.facebook.com/notes/2349781545147189/\">doesn't live up to my goals</a> in having a conversation. However, I am concerned that quoting someone you think has non-predictive models of what will happen as an authority, without flagging that you're quoting them to point out that your opposition grants that particular point, is disengenious.</p>", "parentCommentId": "aMEm25HP8j5Sd5uqi", "user": {"username": "Davidmanheim"}}, {"_id": "mfsT2223AiNouNSCn", "postedAt": "2023-09-18T19:52:07.542Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Simple and genuine question from a non-AI guy<br><br>I understand the arguments towards encouraging gradual development vs. fast takeoff, but I don't understand this argument I've heard multiple times (not just on this post) that \"we need capabilities to increase so that we can stay up to date with alignment research\".&nbsp;</p><p>First I thought there's still a lot of work we could do with current capabilities - technical alignment is surely limited by time, money and manpower not just by computing power. I'm also guessing less powerful AI could be made during a \"pause\" specifically for alignment research</p><p>Second in a theoretical situation where capabilities research globally stopped overnight, isn't this just free-extra-time for the human race where we aren't moving towards doom? That feels pretty valuable and high EV in and of itself.&nbsp;</p><p>It seems to me the argument would have to be that the <i><strong>advantage to the safety work </strong></i>of improving capabilities would outstrip the <i><strong>increasing risk of dangerous GAI, </strong></i>which I find hard to get my head around, but I might be missing something important.</p><p>Thanks.</p>", "parentCommentId": null, "user": {"username": "NickLaing"}}, {"_id": "jmTgqRwwcbqjwQdzM", "postedAt": "2023-09-18T20:17:33.811Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Free extra time is good. The reasonable version of the argument is that you should avoid buying total-time in ways that cost <i>time with more powerful systems</i>; maybe AI progress will look like <a href=\"https://www.lesswrong.com/posts/59dKN8XQGx952irWg/cruxes-for-overhang-1#Miscellanea\">the purple line</a>.</p>", "parentCommentId": "mfsT2223AiNouNSCn", "user": {"username": "zsp"}}, {"_id": "FE78fEG5k8KojdNfn", "postedAt": "2023-09-18T20:38:24.201Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Nice post. Why didn't you post it here for AI pause debate week haha.<br><br>Yes I somewhat understand this potential \"overhang\" danger as an argument in and of itself against a pause., I just don't see how it relates to technical alignment research specifically. &nbsp;</p>", "parentCommentId": "jmTgqRwwcbqjwQdzM", "user": {"username": "NickLaing"}}, {"_id": "5aQfYroQFcrspRmga", "postedAt": "2023-09-18T21:17:57.017Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>The argument w.r.t. capabilities is disanalogous.</p><p>Yes, the training process is running a search where our steering is (sort of) effective for getting capabilities - though note that with e.g. LLMs we have approximately zero ability to reliably translate known inputs [X] into known capabilities [Y].</p><p>We are not doing the same thing to select for alignment, because \"alignment\" is:</p><ul><li>an internal representation that depends on multiple unsolved problems in philosophy, decision theory, epistemology, math, etc, rather than \"observable external behavior\" (which is what we use to evaluate capabilities &amp; steer training)</li><li>something that might be inextricably tied to the form of general intelligence which by default puts us in the \"dangerous capabilities\" regime, or if not strongly bound in theory, then strongly bound in practice</li></ul><p>I do think this disagreement is substantially downstream of a disagreement about what \"alignment\" represents, i.e. I think that you might attempt outer alignment of GPT-4 but not inner alignment, because GPT-4 doesn't have the internal bits which make inner alignment a relevant concern.</p>", "parentCommentId": "vARKNX8PyFaHYJurT", "user": {"username": "T3t"}}, {"_id": "BC7h4srkyKNsRikJa", "postedAt": "2023-09-18T21:51:45.407Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Not responding to your main question:</p><blockquote><p>Second in a theoretical situation where capabilities research globally stopped overnight, isn't this just free-extra-time for the human race where we aren't moving towards doom? That feels pretty valuable and high EV in and of itself.&nbsp;</p></blockquote><p>I'm interpreting this as saying that buying humanity more time, in and of itself, is good.&nbsp;</p><p>I don't think extra time pre-transformative-AI is particularly valuable except its impact on existential risk. Two reasons for why I think this:</p><ul><li><a href=\"https://nickbostrom.com/astronomical/waste\">Astronomical waste</a> argument. Time post-transformative-AI is way more valuable than time now, assuming some (but strong version not necessary) aggregating/total utilitarianism. If I was trading clock-time seconds now for seconds a thousand years from now, assuming no difference in existential risk, I would probably be willing to trade every historical second of humans living good lives for like a minute a thousand years from now, because it seems like we could have a ton of (morally relevant) people in the future, and the moral value derived from their experience could be significantly <a href=\"https://nickbostrom.com/papers/digital-minds.pdf\">greater</a> than current humans.&nbsp;</li><li>The moral value of the current world seems plausibly negative due to large amounts of suffering. Factory farming, wild animal suffering, humans experiencing suffering, and more, seem like they make the total sign unclear. Under moral views that <a href=\"https://en.wikipedia.org/wiki/Suffering-focused_ethics\">weigh suffering</a> more highly than happiness, there's an even stronger case for the current world being net-negative. This is one of those arguments that I think is pretty weird and almost never affects my actions, but it is relevant to the question of whether extra time for the human race is positive EV.&nbsp;</li><li>Third argument about how AI sooner could help reduce other existential risks. e.g., normal example of AI speeding up vaccine research, or weirder example of AI enabling space colonization, and being on many planets makes x-risk lower. I don't personally put very much weight on this argument, but it's worth mentioning.&nbsp;</li></ul>", "parentCommentId": "mfsT2223AiNouNSCn", "user": {"username": "Aaron_Scher"}}, {"_id": "mbgcXKgDp7uejnq2G", "postedAt": "2023-09-18T22:00:20.373Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Thanks Aaron appreciate the effort.</p><p>I Faild to point out my central assumpton here, that Transformative AI in our current state of poor preparedness is net negative du<strong>e to the existential risk it entails.</strong></p><p><strong>Its a good point about time pre transformative AI not being so valuable in the grand scheme of the future, but that ev would increase substantally assuming transformative AI is the end.</strong></p><p><strong>Still looking for the fleshing out of this argument that I don't understand - if anyone can be bothered!</strong><br><br><strong>\"</strong>It seems to me the argument would have to be that the <i><strong>advantage to the safety work </strong></i>of improving capabilities would outstrip the <i><strong>increasing risk of dangerous GAI, </strong></i>which I find hard to get my head around, but I might be missing something important.\"<br><br>&nbsp;</p>", "parentCommentId": "BC7h4srkyKNsRikJa", "user": {"username": "NickLaing"}}, {"_id": "JCYDjynE7DaSebFon", "postedAt": "2023-09-18T22:18:02.369Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>we need capabilities to increase so that we can stay up to date with alignment research</p></blockquote><p>I think one of the better write-ups about this perspective is <a href=\"https://www.lesswrong.com/posts/xhKr5KtvdJRssMeJ3/anthropic-s-core-views-on-ai-safety\">Anthropic's Core Views on AI Safety</a>.&nbsp;</p><p>From its main text, under the heading The Role of Frontier Models in Empirical Safety, a couple relevant arguments are:&nbsp;</p><ul><li>Many safety concerns arise with powerful systems, so we need to have powerful systems to experiment with</li><li>Many safety methods require large/powerful models</li><li>Need to understand how both problems and our fixes change with model scale (if model gets bigger, does it look like safety technique is still working)</li><li>To get evidence of powerful models being dangerous (which is important for many reasons), you need the powerful models.&nbsp;</li></ul>", "parentCommentId": "mfsT2223AiNouNSCn", "user": {"username": "Aaron_Scher"}}, {"_id": "qRaikyzu9gfbtMsXp", "postedAt": "2023-09-18T22:33:34.917Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Thanks Aaron that's a good article appreciate it. It still wasn't clear to me they were making an argument that increasing capabilities could be net positive, more that safety people should be working with whatever is the current most powerful model<br><br><i><strong>\"But we also cannot let excessive caution make it so that the most safety-conscious research efforts only ever engage with systems that are far behind the frontier.\"&nbsp;</strong></i></p><p>This makes sense to me, the best safety researchers should have full access to the current most advanced models, preferably in my eyes before they have been (fully) trained.</p><p>But then I don't understand their next sentance <i><strong>\"Navigating these tradeoffs responsibly is a balancing act, and these concerns are central to how we make strategic decisions as an organization.\"&nbsp;</strong></i></p><p>I'm clearly missing something, what's the tradeoff? Is working on safety with the most advanced current model while generally slowing everything down not the best approach? This doesn't seem like a tradeoff to me<br><br><i><strong>How is there any net safety advantage in increasing AI capacity?</strong></i></p>", "parentCommentId": "JCYDjynE7DaSebFon", "user": {"username": "NickLaing"}}, {"_id": "EJejvejtCie2qmfQt", "postedAt": "2023-09-18T22:47:33.965Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>In my essay I don't make an assumption that the pause would <i>immediate</i>, because I did read your essay and I saw that you were proposing that we'd need some time to prepare and get multiple countries on board.</p><p>I don't see how a delay before a pause changes anything. I still think it's highly unlikely you're going to get sufficient international backing for the pause, so you will either end up doing a pause with an insufficiently large coalition, or you'll back down and do no pause at all.</p>", "parentCommentId": "ssZQwcf7jhWhcEo7A", "user": {"username": "Nora Belrose"}}, {"_id": "4Mf6S9pgx88TG2CjE", "postedAt": "2023-09-18T23:16:23.838Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>The assumptions are that more powerful models won't be like weaker models but more accurate.  They will show emergent abilities.  Many things that gpt-4 can solve gpt-3 cannot, and those models share a similar lineage.</p>\n<p>Safety issues show up when you have a model powerful enough to even exhibit them, and they may not be anything you predicted will happen from theory.  Waluigi effect, hallucinations - both were not predicted by any theory by AI safety research groups.  They seem to be the majority of the issues with models at the current level of capabilities.</p>\n", "parentCommentId": "qRaikyzu9gfbtMsXp", "user": {"username": "Gerald Monroe"}}, {"_id": "9TmtLgcyQ8DaHfhya", "postedAt": "2023-09-18T23:22:56.010Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>As a side note the actual things that break this loop are</p>\n<p>(1) we don't use superintelligent singletons and probably won't, I hope.  We instead create context limited model instances of a larger model and tell it only about our task and the model doesn't retain information.  This \"break an ASI into a billion instances each which lives only in the moment\" is a powerful alignment method</p>\n<p>(2) it seems to take an absolutely immense amount of compute hardware to host even today's models which are significantly below human intelligence in some expensive to fix ways.  (For example how many H100s would you need for useful realtime video perception?)</p>\n<p>This means a \"rogue\" Singleton would have nowhere to exist, as it would be too heavy in weights and required bandwidth to run on a botnet.</p>\n<p>This breaks everything else.</p>\n<p>It's telling that Bostroms PhD is in philosophy and I don't see any industry experience on his wiki page.  He is correct if you ignore real world limitations on AI.</p>\n", "parentCommentId": "FeYj7vCfMpn5hxFpD", "user": {"username": "Gerald Monroe"}}, {"_id": "BZETMWmNirRRtCNyB", "postedAt": "2023-09-18T23:28:14.217Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Or another rephrase.  How is the \"secretly is planning to murder all humans\" improving the models scores on a benchmark?  If you think about it first, what gradient from the training set even led to this capability of an inner cognitive process looking for a chance to betray.  What force is causing this cognitive process to come out of the random initial weights?</p>\n<p>Humans seem to have such a force but it's because modeling \"if I kill this rival then the reward for me is...\" was evolutionarily useful.  Also it's probably a behavior that is learned.</p>\n<p>And second, yeah, SGD should push \"neutral\" weights inside the network that are not contributing to correct answers towards weights that do increase the odds of a correct output distribution.  So it should actively destroy \"unnecessary \" cognitive processes inside the model.</p>\n<p>You could prove this.  Make a psychopathic model designed to \"betray\" in a game like world and then see how many rounds of training on a new dataset clear the ability for the model to kill when it improves score.</p>\n", "parentCommentId": "vARKNX8PyFaHYJurT", "user": {"username": "Gerald Monroe"}}, {"_id": "MDAo4oKtcMCJanefK", "postedAt": "2023-09-18T23:37:02.869Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Steven the issue is without empirical data you end up with a branching tree of possible futures.  And if you make some faulty assumptions early - such as assuming the amount of compute needed to host optimal AI models is small and easily stolen via hacking - you end up lost in a tree of possibilities where every one you consider is \"doom\".  And thus you arrive at the conclusion of \"pDoom is 99 percent\", because you are only cognitively able to consider adjacent futures in the possibility tree.  No living human can keep track of thousands of possibilities in parallel.  This is where I think Eliezer and Zvi are lost, where they simply ignore branches that would lead to different outcomes.</p>\n<p>(And vice versa, you could arrive at the opposite conclusion).</p>\n<p>It becomes angels at the head of a pin.  There is no way to make a policy decision based on this.  You need to prove you beliefs with data.  It's how we even got here as a species.</p>\n", "parentCommentId": "kqSzGBBr4TCu5ztbq", "user": {"username": "Gerald Monroe"}}, {"_id": "MDG5rz5FG7WKyhxtw", "postedAt": "2023-09-18T23:50:36.511Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Just something that jumped out at me.  Suppose a pause is on 1e28+ training runs.</p>\n<p>The human brain is made of modules organized in a way we don't understand.  But we do know the frontal lobes associated with executive functions are a small part of the total tissue.</p>\n<p>This means an AI system could be a collection of a few dozen specialized 1e28 models separated by api calls, hosted in a common data center for low latency interconnects.</p>\n<p>If a \"few dozen\" is 100+ modules the total compute used would be 1e30 and it might be possible to make this system an AGI with difficult training tasks to cause this level of cognitive development through feedback.</p>\n<p>Especially with \"meta\" system architectures where new modules could be automatically added to improve score where deficiencies are present in a way that training existing weights is leading to regressions.</p>\n", "parentCommentId": "HMipMShBr6ymoNzcd", "user": {"username": "Gerald Monroe"}}, {"_id": "jLdQ2byHtoxAWNHZ4", "postedAt": "2023-09-19T01:24:00.918Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>So just to summarize:</p>\n<p>No deceptive or dangerous AI has ever been built or empirically tested. (1)</p>\n<p>Historically AI capabilities have consistently been \"underwhelming\", far below the hype.  (2)</p>\n<p>If we discuss \"ok we build a large AGI, give it persistent memory and online learning, and isolate it in an air gapped data center and hand carry data to the machine via hardware locked media, what is the danger\" you are going to respond either with:</p>\n<p>\"I don't know how the model escapes but it's so smart it will find a way\" or (3)</p>\n<p>\"I am confident humanity will exist very far into the future so a small risk now is unacceptable (say 1-10 percent pDoom)\".</p>\n<p>and if I point out that this large ASI model needs thousands of H100 accelerator cards and megawatts of power and specialized network topology to exist and there is nowhere to escape to, you will argue \"it will optimize itself to fit on consumer PCs and escape to a botnet\". (4)</p>\n<p>Have I summarized the arguments?</p>\n<p>Like we're supposed to coordinate an international pause and I see 4 unproven assertions above that have zero direct evidence.  The one about humanity existing far into the future I don't know I don't want to argue that because it's not falsifiable.</p>\n<p>Shouldn't we wait for evidence?</p>\n", "parentCommentId": "r9SkJax2BgLbHZ89N", "user": {"username": "Gerald Monroe"}}, {"_id": "RSuFzMjJNXP4XNcto", "postedAt": "2023-09-19T04:48:08.924Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Is your opposition to stopping the building of dangerously large models via international regulation because you don't think that it's possible to do, or because you are opposed to having such limits?</p><p>You seem to equivocate; first you say that we need larger models in order to do alignment research, and a number of people have already pointed out that this claim is suspect - but it implies you think any slowdown would be bad even if done effectively. Next, you say that a fast takeoff is more likely if we stop temporarily and then remove all limits, and I agree, but pointed out that no-one is advocating that, and that it's not opposition to any of the actual proposals, it's opposition to a straw man. Finally, you say that it's likely to push work to places that aren't part of the pause. That's assuming international arms control of what you agree could be an existential risk is fundamentally impossible, and I think that's false - but you haven't argued the point, just assumed that it will be ineffective.</p><p>(Also, reread my piece - I call for action to regulate and stop larger and more dangerous models immediately as a prelude to a <i>global</i> moratorium. I didn't say \"wait a while, then impose a pause for a while in a few places.\")</p>", "parentCommentId": "EJejvejtCie2qmfQt", "user": {"username": "Davidmanheim"}}, {"_id": "FGSp5PBjm6o9rcY2g", "postedAt": "2023-09-19T06:23:57.603Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote>\n<p>is fundamentally impossible,</p>\n</blockquote>\n<p>Clarifying question: is a nuclear arms pause or moratorium possible, by your definition of the word?  Is it likely?</p>\n<p>With the evidence that many world leaders, including the leaders of the USA, Israel, China, and Russia speak of AI as a must have strategic technology, do you think they are likely in plausible future timelines to reverse course and support international AI pauses before evidence of the dangers of AGI, by humans building one, exists?</p>\n<p>Do you dispute that they have said this publicly and recently?</p>\n<p>Do you believe there is any empirical evidence proving an AGI is an existential risk available to policymakers?  If there is, what is the evidence? Where is the benchmark of model performance showing this behavior?</p>\n<p>I am aware many experts are concerned but this is not the same as having empirical evidence to support their concerns.  There is an epistemic difference.</p>\n<p>I am wondering if we are somehow reading two different sets of news.  I acknowledge that it is possible that an AI pause is the best thing humanity could do right now to ensure further existence.  But I am not seeing any sign that it is a possible outcome.  (By \"possible\" I mean it's possible for all parties to inexplicably act against their own interests without evidence, but it's not actually going to happen)</p>\n<p>Edit: it's possible for Saudi Arabia to read the news on climate change and decide they will produce 0 barrels in 10 years.  It's possible for every OPEC member to agree to the same pledge.  It's possible, with a wartime level of effort, to transition the economy to no longer need Opec petroleum worldwide, in just 10 years.</p>\n<p>But this is not actually possible.  The probability of this happening is approximately 0.</p>\n", "parentCommentId": "RSuFzMjJNXP4XNcto", "user": {"username": "Gerald Monroe"}}, {"_id": "EYAqFu57LPocue4Yq", "postedAt": "2023-09-19T08:54:19.435Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Thanks for this post Nora :) It's well-written, well-argued, and has certainly provoked some <i>lively </i>discussion. (FWIW I welcome good posts like this that push back against certain parts of the 'EA Orthodoxy')<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9ftxei4wamp\"><sup><a href=\"#fn9ftxei4wamp\">[1]</a></sup></span></p><p>My only specific comment would be <a href=\"https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=9ALFywXMA2j8zE27e\">similar to Daniel's</a>, I'm not sure the references to the CIRL paradigm being irrelevant are fully backed-up. Not saying that's wrong, just that I didn't find the links convincing (though I don't work on training/aligning/interpreting LLMs as my day job)</p><p>My <i><strong>actual</strong></i> question is that I want there to be more things like this, bridging the gap between those who are concerned about xRisk (most EA responses), those who aren't and optimistic about AI (which I'd roughly place you in), and those who aren't and concerned about AI (The FAact/AI Ethics crowd). Do you think that there's a way to do this productively, instead of people on all sides shouting at each other on Twitter constantly?</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9ftxei4wamp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9ftxei4wamp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For some forum users, why are you downvoting the post? There are separate disagree votes available on top-level posts now</p></div></li></ol>", "parentCommentId": null, "user": {"username": "JWS"}}, {"_id": "upwRYhsta3qMAXY2Y", "postedAt": "2023-09-19T09:07:18.528Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<ol><li>Is a nuclear arms moratorium or de-escalation possible? You say it is not, but evidently you're not aware of the history. The base rate on the exact thing you just said is not possible repeatedly working (NPT, SALT, START) tells me all I need to know about whether your estimates are reasonable.</li><li>You're misusing the word empirical. Using your terminology, there's no empirical evidence that the sun will rise tomorrow, just validated historical trends of positions of celestial objects and claims that fundamental physical laws hold even in the future. I don't know what to tell you; I agree that there is a lack of clarity, but there is even less empirical evidence that AGI is safe than that it is not.</li><li>World leaders have said it's a vital tool, and also that it's an existential risk. You're ignoring the fact that many said the latter.</li><li>OPEC is a cartel, and it works to actually restrict output - despite the way that countries have individual incentives to produce more.&nbsp;</li></ol>", "parentCommentId": "FGSp5PBjm6o9rcY2g", "user": {"username": "Davidmanheim"}}, {"_id": "vWaBcwCzkbG6fjA7m", "postedAt": "2023-09-19T09:21:08.936Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>I also don\u2019t think humans exhibit this distinction [terminal vs. instrumental goal dichotomy] particularly strongly.</p></blockquote><p>the ideological turing test seems like a case where the distinction can be seen clearly in humans; the instrumental goal is to persuade the other that you sincerely hold beliefs/values (which imply goals). while your terminal goal is to advance advocacy of your different actual beliefs.</p>", "parentCommentId": "GQnPaKuhtd5sJGSHE", "user": {"username": "Tomasz Kaye"}}, {"_id": "2u4d9N5B6jf2Fsyb6", "postedAt": "2023-09-19T15:14:35.600Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p><strong>My opposition is disjunctive!</strong></p><p>I <strong>both</strong> think that <strong>if</strong> it's possible to stop the building of dangerously large models via international regulation, that would be bad because of tyranny risk, <strong>and</strong> I also think that we very likely can't use international regulation to stop building these things, so that any local pauses are not going to have their intended effects and will have a lot of unintended net-negative effects.</p><blockquote><p>(Also, reread my piece - I call for action to regulate and stop larger and more dangerous models immediately as a prelude to a <i>global</i> moratorium. I didn't say \"wait a while, then impose a pause for a while in a few places.\")</p></blockquote><p>This really sounds like you are committing the fallacy I was worried about earlier on. I just don't agree that you will actually get the global moratorium. I am fully aware of what your position is.</p>", "parentCommentId": "RSuFzMjJNXP4XNcto", "user": {"username": "Nora Belrose"}}, {"_id": "XMDckwWp3sdfgbzg4", "postedAt": "2023-09-19T15:32:55.338Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Please stop saying that mind-space is an \"enormously broad space.\" What does that even mean? How have you established a <a href=\"https://en.wikipedia.org/wiki/Measure_(mathematics)\">measure</a> on mind-space that isn't totally arbitrary?</p><p>What if concepts and values are <a href=\"https://en.wikipedia.org/wiki/Convergent_evolution\">convergent</a> when trained on similar data, just like we see convergent evolution in biology?</p>", "parentCommentId": "SJFqkd8RDeKyrQjMm", "user": {"username": "Nora Belrose"}}, {"_id": "i59hGTRkKyBY3iAfF", "postedAt": "2023-09-19T15:34:34.114Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<ol>\n<li>\n<p>To qualify this would be a moratorium or pause on nuclear arms before powerful nations had doomsday sized arsenals.  The powerful making it expensive for poor nations to get nukes - though several did - is different.  And notably I wonder how well it would have gone if the powerful nation had no nukes of their own.  Trying to ban AGI from others - when the others have nukes and their own chip fabs - would be the same situation.  Not only will you fail you will eventually, if you don't build your own AGI, lose everything.  Same if you have no nukes.</p>\n</li>\n<li>\n<p>What data is that?  A model misunderstanding \"rules\" on an edge case isn't misaligned.  Especially when double generation usually works.  The sub rising has every prior sunrise as priors.  Which empirical data would let someone conclude AGI is an existential risk justifying international agreements.  Some measurement or numbers.</p>\n</li>\n<li>\n<p>Yes, and they said this about nukes and built thousands</p>\n</li>\n<li>\n<p>Yes to maximize profit.  Pledging to go to zero is not the same thing.</p>\n</li>\n</ol>\n", "parentCommentId": "upwRYhsta3qMAXY2Y", "user": {"username": "Gerald Monroe"}}, {"_id": "ffH3FNeDwKbqvHcaH", "postedAt": "2023-09-19T15:36:05.989Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I have now made a clarification at the very top of the post to make it 1000% clear that my opposition is disjunctive, because people repeatedly get confused / misunderstand me on this point.</p>", "parentCommentId": "RSuFzMjJNXP4XNcto", "user": {"username": "Nora Belrose"}}, {"_id": "KMyAfcs9im3cGtM4t", "postedAt": "2023-09-19T20:27:18.138Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>You seem to dismiss the claim that AI is an existential risk. If that's correct, perhaps we should start closer to the beginning, rather than debating global response, and ask you to explain why you disagree with such a large consensus of experts that this risk exists.</p>", "parentCommentId": "i59hGTRkKyBY3iAfF", "user": {"username": "Davidmanheim"}}, {"_id": "uaotaqDdgX9tDWh53", "postedAt": "2023-09-19T20:41:09.419Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I don't disagree.  I don't see how it's different than nuclear weapons.  Many many experts are also saying this.</p>\n<p>Nobody denies nuclear weapons are an existential risk.  And every control around their use is just probability based, there is absolutely nothing stopping a number of routes from ending the richest civilizatios.  Multiple individuals appear to have the power to do it at a time, every form of interlock and safety mechanism has a method of failure or bypass.</p>\n<p>Survival to this point was just probability.  Over an infinite timescale the nukes will fly.</p>\n<p>Point is that it was completely and totally intractable to stop the powerful from getting nukes.  SALT was the powerful tiring of paying the maintenance bills and wanting to save money on MAD.  And key smaller countries - Ukraine and Taiwan - have strategic reasons to regret their choice to give up their nuclear arms.  It is possible that if the choice happens again future smaller countries will choose to ignore the consequences and build nuclear arsenals.  (Ukraines first opportunity will be when this war ends, they can start producing plutonium.  Taiwan chance is when China begins construction of the landing ships)</p>\n<p>So you're debating something that isn't going to happen without a series of extremely improbable events happening simultaneously.</p>\n<p>If you start thinking about practical interlocks around AI systems you end up with similar principles to what protects nukes albeit with some differences.  Low level controllers running simple software having authority, air gaps - there are some similarities.</p>\n<p>Also unlike nukes a single AI escaping doesn't end the world.  It has to escape and there must be an environment that supports its plans.  It is possible for humans to prepare for this and to make the environment inhospitable to rogue AGIs.  Heavy use of air gaps, formally proven software, careful monitoring and tracking of high end compute hardware.   A certain minimum amount of human supervision for robots working on large scale tasks.</p>\n<p>This is much more feasible than \"put the genie away\" which is what a pause is demanding.</p>\n", "parentCommentId": "KMyAfcs9im3cGtM4t", "user": {"username": "Gerald Monroe"}}, {"_id": "9J5FaoParroD5DTjc", "postedAt": "2023-09-19T20:50:13.049Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>You're misinterpreting what a moratorium would involve. I think you should read my post, where I outlined what I think a reasonable pathway would be - not stopping completely forever, but a negotiated agreement about how to restrict more powerful and by-default dangerous systems, and therefore only allowing those that are shown to be safe.<br><br>Edit to add: \"unlike nukes a single AI escaping doesn't end the world\" &lt;- Disagree on both fronts. A single nuclear weapons won't destroy the world, while a single misaligned and malign superintelligent AI, if created and let loose, almost certainly will - it doesn't need a hospitable environment.</p>", "parentCommentId": "uaotaqDdgX9tDWh53", "user": {"username": "Davidmanheim"}}, {"_id": "2GwjjBiQhAv6powXi", "postedAt": "2023-09-19T21:02:56.851Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>So there is one model that might have worked for nukes.  You know about PAL and weak-link strong link design methodology?  This is a technology for reducing the rogue use of nuclear warheads.  It was shared with Russia/the USSR so that they could choose to make their nuclear warheads safe from unauthorized use.</p>\n<p>Major AI labs could design software frameworks and tooling that make AI models, even ASI capabilities level models, less likely to escape or misbehave.  And release the tooling.</p>\n<p>It would be voluntary compliance but like the Linux Kernel it might in practice be used by almost everyone.</p>\n<p>As for the second point,  no.  Your argument has a hidden assumption that is not supported by evidence or credible AI scientists.</p>\n<p>The evidence is that models that exhibit human scale abilities need human scale (within an oom) level of compute and memory.  The physical hardware racks to support this are enormous and not available outside AI labs.  Were we to restrict the retail sale of certain kinds of training accelerator chips and especially high bandwidth interconnects, we could limit the places human level + AI could exist to data centers at known addresses.</p>\n<p>Your hidden assumption is optimizations, but the problem is that if you consider not just \"AGI\" but \"ASI\", the amount of hardware to support superhuman level cognition is probably nonlinear.</p>\n<p>If you wanted a model that could find an action that has a better expected value than a human level model with 90 percent probability (so the model is 10 times smarter in utility), it probably needs more than 10 times the compute.  Probably logarithmic, that to find a better action 90 percent of the time you need to explore a vastly larger possibility space and you need the compute and memory to do this.</p>\n<p>This is probably provable in a theorem but the science isn't there yet.</p>\n<p>If correct, actually ASI is easily contained.  Just write down where 10,000+ H100s are located or find it by IR or power consumption.   If you suspect a rogue ASI has escaped that's where you check.</p>\n<p>This is what I mean by controlling the environment.  Realtime auditing of AI accelerator clusters - what model is running, who is paying for it, what's their license number, etc - would actually decrease progress very little while make escapes difficult.</p>\n<p>If hacking and escapes turns out to be a threat, air gaps and asic hardware firewalls to prevent this are the next level of security to add.</p>\n<p>The difference is that major labs would not be decelerated at all.  There is no pause.  They just in parallel have to spend a trivial amount of money complying with the registration and logging reqs.</p>\n", "parentCommentId": "9J5FaoParroD5DTjc", "user": {"username": "Gerald Monroe"}}, {"_id": "HBWLJaAhFmxmBpPZy", "postedAt": "2023-09-19T21:31:46.992Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote>\n<p>we don't use superintelligent singletons and probably won't, I hope. We instead create context limited model instances of a larger model and tell it only about our task and the model doesn't retain information.</p>\n</blockquote>\n<p>FYI, current cutting-edge large language models are trained on a massive amount of text on the internet (in the case of GPT-4, likely approximately all the text OpenAI could get their hands on). So they certainly have tons of information about stuff other than the task at hand.</p>\n", "parentCommentId": "9TmtLgcyQ8DaHfhya", "user": {"username": "DanielFilan"}}, {"_id": "DyeKp3tsmT9LeG2KY", "postedAt": "2023-09-19T21:35:18.878Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>This is not what that statement means.</p>\n<p>What it means is the model has no context of its history since training.  It has no context if the task it has been given is \"real\".   It does not know if other copies of itself or other AIs are checking it's outputs for correctness, with serious consequences if it sabotages the output.  It doesn't know it's not still in training.\nIt doesn't know if there are a billion instances of it or just 1.</p>\n<p>We can scrub all this information fairly easily and we already do this as of right now.</p>\n<p>We can also make trick output where we try to elicit latent deception by giving information that would tell the model its time to betray.</p>\n<p>We can also work backwards and find what the adversarial inputs are.  When will the model change it's answer for this question?</p>\n", "parentCommentId": "HBWLJaAhFmxmBpPZy", "user": {"username": "Gerald Monroe"}}, {"_id": "KRw7Q3rM3R6vg83Kc", "postedAt": "2023-09-20T06:03:42.090Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I think that you're claiming something much stronger than \"we very likely can't use international regulation to stop building these things\" - you're claiming that international regulation won't even be useful to reduce risk by changing incentives. And you've already agreed that it's implausible that these efforts would lead to tyranny, you think they will just fail.&nbsp;</p><p>But how they fail matters - there's a huge difference between something like the NPT, which was mostly effective, and something like the Kellogg-Briand Pact of 1928, which was ineffective but led to a huge change, versus... I don't know, I can't really think of many examples of treaties or treaty negotiations that backfired, even though <a href=\"https://www.pnas.org/doi/10.1073/pnas.2122854119\">most fail to produce exactly what they hoped</a>. (I think there's a stronger case to make that treaties can prevent the world from getting stronger treaties later, but that's not what you claim.)</p>", "parentCommentId": "2u4d9N5B6jf2Fsyb6", "user": {"username": "Davidmanheim"}}, {"_id": "u3372xa4eeGiSyg2i", "postedAt": "2023-09-20T06:16:05.866Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>Please stop saying that mind-space is an \"enormously broad space.\" What does that even mean? How have you established a <a href=\"https://en.wikipedia.org/wiki/Measure_(mathematics)\">measure</a> on mind-space that isn't totally arbitrary?</p></blockquote><p>Why don't you make the positive case for the space of possible (or, if you wish, likely) minds being minds which have values compatible with the fulfillment of human values? &nbsp;I think we have pretty strong evidence that not all minds are like this even within the space of minds produced by evolution.</p><blockquote><p>What if concepts and values are <a href=\"https://en.wikipedia.org/wiki/Convergent_evolution\">convergent</a> when trained on similar data, just like we see convergent evolution in biology?</p></blockquote><p>Concepts do seem to be convergent to some degree (though note that ontological shifts at increasing levels of intelligence seem likely), but I do in fact think that evidence from evolution suggests that values are <i>strongly</i> contingent on the kinds of selection pressures which produced various species.</p>", "parentCommentId": "XMDckwWp3sdfgbzg4", "user": {"username": "T3t"}}, {"_id": "LSitvmWTht4C9WMJ2", "postedAt": "2023-09-20T07:07:23.080Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>At best, these theory-first efforts did very little to improve our understanding of how to align powerful AI. And they may have been <i>net negative</i>, insofar as they propagated a variety of actively misleading ways of thinking both among alignment researchers and the broader public. Some examples include the now-debunked <a href=\"https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn\"><u>analogy from evolution</u></a>, the false distinction between <a href=\"https://www.lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into\"><u>\u201cinner\u201d and \u201couter\u201d alignment</u></a>, and the idea that AIs will be rigid utility maximizing consequentialists (<a href=\"https://www.lesswrong.com/posts/yCuzmCsE86BTu9PfA/there-are-no-coherence-theorems\"><u>here</u></a>, <a href=\"https://www.lesswrong.com/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-entail-goal-directed-behavior\"><u>here</u></a>, and <a href=\"https://sohl-dickstein.github.io/2023/03/09/coherence.html\"><u>here</u></a>).</p></blockquote><p>Random aside, but I think this paragraph is unjustified in both its core argument (that the referenced theory-first efforts propagated actively misleading ways of thinking about alignment) and none of the citations provide the claimed support.</p><p>The first post (re: evolutionary analogy as evidence for a sharp left turn) sees substantial pushback in the comments, and that pushback seems more correct to me than not, and in any case <a href=\"https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn?commentId=XuLdXJYsYknxi3Lx2\">seems to misunderstand</a> the position it's arguing against.</p><p>The second post presents an interesting case for a set of claims that are different from \"there is no distinction between inner and outer alignment\"; I do not consider it to be a full refutation of that conceptual distinction. &nbsp;(See also Steven Byrnes' <a href=\"https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=kqSzGBBr4TCu5ztbq\">comment</a>.)</p><p>The third post is at best playing games with the definitions of words (or misunderstanding the thing it's arguing against), at worst is just straightforwardly wrong.</p><p>I have less context on the fourth post, but from a quick skim of both the post and the comments, I think the way it's most relevant here is as a demonstration of how important it is to be careful and precise with one's claims. &nbsp;(The post is not making an argument about whether AIs will be \"rigid utility maximizing consequentialists\", it is making a variety of arguments about whether coherence theorems necessarily require that whatever ASI we might build will behave in a goal-directed way. &nbsp;Relatedly, Rohin's <a href=\"https://www.lesswrong.com/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-entail-goal-directed-behavior?commentId=a3CpB8YrP8tMfHDCd\">comment</a> a year after writing that post indicated that he thinks we're likely to develop goal-directed agents; he just doesn't think that's entailed by arguments from coherence theorems, which may or <a href=\"https://www.lesswrong.com/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-entail-goal-directed-behavior?commentId=GhQnfkKfzAeLnsfjT\">may not</a> have been made by e.g. Eliezer in other essays.)</p><p>My guess is that you did not include the fifth post as a smoke test to see if anyone was checking your citations, but I am having trouble coming up with a charitable explanation for its inclusion in support of your argument.</p><hr><p>I'm not really sure what my takeaway is here, except that I didn't go scouring the essay for mistakes - the citation of Quintin's post was just the first thing that jumped out at me, since that wasn't all that long ago. &nbsp;I think the claims made in the paragraph are basically unsupported by the evidence, and the evidence itself is substantially mischaracterized. &nbsp;Based on other comments it looks like this is true of a bunch of other substantial claims and arguments in the post:</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=FeYj7vCfMpn5hxFpD\">that Bostrom's core argument has aged poorly</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=9ALFywXMA2j8zE27e\">CIRL being widely considered irrelevant</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=tjHJwJLm8t6W4mWG2\">whether proposed pauses are intended to be temporary</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft94fghv0s2d\"><sup><a href=\"#fnt94fghv0s2d\">[1]</a></sup></span></li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt94fghv0s2d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft94fghv0s2d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Though I'm sort of confused about what this back-and-forth is talking about, since it's referencing behind-the-scenes stuff that I'm not privy to.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "T3t"}}, {"_id": "jbEpcPrRNgHzhzH5s", "postedAt": "2023-09-20T12:47:35.158Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>You are arguing impossibilities despite a reference class with reasonably close analogues that happened. &nbsp;If you could honestly tell me people thought the NPT was plausible when proposed, and I'll listen when you say this is implausible.</p><p>In fact, there is appetite for fairly strong reactions, and if we're the ones who are concerned about the risks, folding before we even get to the table isn't a good way to get anything done.</p>", "parentCommentId": "uaotaqDdgX9tDWh53", "user": {"username": "Davidmanheim"}}, {"_id": "qzhrB7a2YnRY9QyHg", "postedAt": "2023-09-20T16:27:40.407Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>&nbsp;despite a reference class with reasonably close analogues that happened</p></blockquote><p>I am saying the common facts that we both have access to do not support your point of view. &nbsp;It never happened. &nbsp;There are no cases of \"very powerful, short term useful, profitable or military technologies\" that were effectively banned, in the last 150 years.</p><p>You have to go back to the 1240s to find a reference class match.</p><p>These strongly worded statements I just made are trivial for you to disprove. &nbsp;Find a counterexample. &nbsp;I am quite confident and will bet up to $1000 you cannot.</p>", "parentCommentId": "jbEpcPrRNgHzhzH5s", "user": {"username": "Gerald Monroe"}}, {"_id": "moRC4tdehaWxcCYPo", "postedAt": "2023-09-20T19:10:04.056Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>You've made some strong points, but I think they go too far.</p><p>The world banned CFCs, which were critical for a huge range of applications. It was short term useful, profitable technology, and it had to be replaced entirely with a different and more expensive alternative.</p><p>The world has banned human cloning, via a UN declaration, despite the promise of such work for both scientific and medical usage.</p><p>Neither of these is exactly what you're thinking of, and I think both technically qualify under the description you provided, if you wanted to ask a third party to judge whether they match. (Don't feel any pressure to do so - this is the kind of bet that is unresolvable because it's not precise enough to make everyone happy about any resolution.)&nbsp;</p><p>However, I also think that what we're looking to do in ensuring only robustly safe AI systems via a moratorium on untested and by-default-unsafe systems is less ambitious or devastating to applications than a full ban on the technology, which is what your current analogy requires. Of course, the \"very powerful, short term useful, profitable or military technolog[y]\" of AI is only those things if it's actually safe - otherwise it's not any of those things, it's just a complex form of Russian roulette on a civilizational scale. On the other hand, if anyone builds safe and economically beneficial AGI, I'm all for it - but the bar for proving safety is higher than anything anyone currently suggests is feasible, and until that changes, safe strong AI is a pipe-dream.&nbsp;</p>", "parentCommentId": "qzhrB7a2YnRY9QyHg", "user": {"username": "Davidmanheim"}}, {"_id": "PES5pFbCQe4Q8bMfA", "postedAt": "2023-09-20T19:24:46.462Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>On the other hand, if anyone builds safe and economically beneficial AGI, I'm all for it - but the bar for proving safety is higher than anything anyone currently suggests is feasible, and until that changes, safe strong AI is a pipe-dream.&nbsp;</p></blockquote><p>??? &nbsp;David, do you have any experience with</p><p>(1) engineering</p><p>(2) embedded safety compliant systems</p><p>(3) AI</p><p>Note that Mobileye has a <i>very</i> strong proposal for autonomous car safety, I mention it because it's one of the theoretically best ones.</p><p>You can go watch their videos on it but it's simple 3 parallel solvers, each using a completely different input (camera, lidar, imaging radar). &nbsp;If any solver perceives a collision, the system acts to prevent that collision. &nbsp;So a failure to hit a collidable object requires pFail^3. &nbsp;It is unlikely, most of the failures are going to be where the system is coupled together.</p><p>Similar techniques scale to superintelligent AI. &nbsp;</p><p>You can go play with it right now, even write your own python script and do it yourself.</p><p>Suppose you want an LLM to obey an arbitrary list of \"rules\".</p><p>You have the LLM generate output, and you <i>measured</i> how often in testing, and production, it has violated the rules.</p><p>Say pFail is 0.1. &nbsp;Then you add another stage. &nbsp;Have the LLM check it's own output for a rule violation, and don't send it to the user if the violation was there.</p><p>Say the pFail on that stage is 0.2.</p><p>Therefore the overall system will fail 2% of the time.</p><p><br>Maybe good enough for current uses, but not good enough to run a steel mill. &nbsp;A robot making an error 2% of the time will cause the robot to probably break itself and cost more service worker time than having a human operator.</p><p>&nbsp;</p><p>So you add stages. &nbsp;You create training environments where you model the steel mill, you add more stages of error checking, you do things until <i>empirically</i> your design failure meets spec.</p><p><br>This is standard engineering practice. &nbsp;No \"AI alignment experts\" needed, any 'real' engineer knows this.</p><p>One of the critical things you do is you need your test environment to reflect reality. &nbsp;There are a lot of things involved in this but the one crucial to AI is <i>immutable model weights</i>. &nbsp;When you are validating the model and when it's used in the real world, it's immutable. &nbsp;No learning, no going out of control.</p><p>And another aspect is to <i>control the state buildup</i>. &nbsp;Most software systems that have ever failed - see patriot missile, see <a href=\"https://en.wikipedia.org/wiki/Therac-25\">Therac-25</a> - fail because state accumulated at runtime. &nbsp;You can prevent this, fresh prompts when using GPT-4 is one obvious way. &nbsp;Limiting what information the model has to operate reduces how often it fails, both in production and testing.</p><p>A superintelligent system is easily restricted the same way. &nbsp;Because while it may be far past human ability, we <i>tested</i> it in ways we could verify, we check the distribution of the inputs to make sure they were reflected in the test environment - that is, the real world input <i>could</i> have been generated in test - and it's superintelligent <i>because</i> it generated the right answer almost every time, well below the error rate of a human.</p><p>I think the cognitive error here is everyone is imagining an \"ASI\" or \"AGI\" as \"like you or me but waaaay smarter\". &nbsp;And this baggage brings in a bunch of elements humans have an AI system does not need to do its job. &nbsp;Mostly memory for an inner monologue or <i>persistent </i>chain of thought, continuity of existence, online learning, long term goals.</p><p>You need 0 of those to automate most jobs or make complex decisions that humans cannot make accurately.</p>", "parentCommentId": "moRC4tdehaWxcCYPo", "user": {"username": "Gerald Monroe"}}, {"_id": "szJmYXW9e2LjfX6H4", "postedAt": "2023-09-21T08:21:35.144Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I disagree with a lot of particulars here, but don't want to engage beyond this response because your post feels like it's not about the substantive topic any more, it's just trying to mock an assumed / claimed lack of understanding on my part. (Which would be far worse to have done if it were correct.)</p><p>That said, if you want to know more about my background, I'm eminently google-able, and while you clearly have more background in embedded safety compliant systems, I think you're wrong on almost all of the details of what you wrote as it applies to AGI.</p><p>Regarding your analogy to Mobileye's approach, I've certainly read the papers, and had long conversations with people at Mobileye about their safety systems. I even had one of their former That's why I think it's fair to say that you're fundamentally mischaracterizing the idea of \"Responsibility-Sensitive Safety\" - it's not about collision avoidance per se, it's about not being responsible for accidents, in ways that greatly reduce the probability of such accidents. This is critical for understanding what it does and does not guarantee. More critically, for AI systems, this class of safety guarantee doesn't work because you need a complete domain model as well as a complete failure mode model in order to implement a similar failsafe. I've even <a href=\"https://arxiv.org/abs/1811.09246\">written about how RSS could be extended</a>, and that explains why it's not applicable to AGI back in 2018 - but found that many of my ideas were anticipated by <a href=\"https://ai-alignment.com/informed-oversight-18fcb5d3d1e1\">Christiano's 2016 work</a> (which that post is one small part of,) and had been further refined in the context of AGI since then.</p>", "parentCommentId": "PES5pFbCQe4Q8bMfA", "user": {"username": "Davidmanheim"}}, {"_id": "SyFfCPcXCFecbqoZC", "postedAt": "2023-09-21T14:57:50.256Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>So I described scaling stateless microservices to control AGI.  This is how current models work, and this is how cais works, and this is how tech company stacks work.</p>\n<p>I mentioned an in distribution detector as a filter and empirical measurement of system safety.</p>\n<p>I have mentioned this to safety researchers at openAI.  The one I talked to on the eleuther discord didn't know of a flaw.</p>\n<p>Why won't this work?  It's very strong theoretically and simple and close to current techniques.  Can you name or link one actual objection?  Eliezer was unable to do so.</p>\n<p>The only objection I have heard is \"humans will be tempted to build unsafe systems\".  Maybe so, but the unsafe ones will measurably lower performance than this design for a reason that I will assume you know.  So humans will only build a few, and if they cannot escape the lab because the model needs thousands of current gen accelerator cards to think, then....</p>\n", "parentCommentId": "szJmYXW9e2LjfX6H4", "user": {"username": "Gerald Monroe"}}, {"_id": "ekvpGt53R2DupGniT", "postedAt": "2023-09-21T16:04:23.611Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>And you've already agreed that it's implausible that these efforts would lead to tyranny, you think they will just fail.</p></blockquote><p>I think that <strong>conditional on the efforts working, the chance of tyranny is quite high (ballpark 30-40%).</strong> I don't think they'll work, but if they do, it seems quite bad.</p><p>And since I think x-risk from technical AI alignment failure is in the 1-2% range, the risk of tyranny is the dominant effect of \"actually enforced global AI pause\" in my EV calculation, followed by the extra fast takeoff risks, and then followed by \"maybe we get net positive alignment research.\"</p>", "parentCommentId": "KRw7Q3rM3R6vg83Kc", "user": {"username": "Nora Belrose"}}, {"_id": "JGnroPsyMpfodBmQp", "postedAt": "2023-09-21T20:30:44.022Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>If your action space is small enough to have what you want it to not be able to do programmatically described in terms of its outputs, and your threat model is complete, it works fine.</p>", "parentCommentId": "SyFfCPcXCFecbqoZC", "user": {"username": "Davidmanheim"}}, {"_id": "mfrNeFvhHLnm4h2zJ", "postedAt": "2023-09-21T21:07:21.708Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Conditional on \"the efforts\" working is hooribly underspecified. A global governance mechanism run by a new extranational body with military powers monitoring and stopping production of GPUs, or a standard treaty with a multi-party inspection regime?</p>", "parentCommentId": "ekvpGt53R2DupGniT", "user": {"username": "Davidmanheim"}}, {"_id": "eKsnaFc6TWJZ4tS67", "postedAt": "2023-09-21T23:25:01.330Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Right.  And this scales to fairly massive tasks.  \"Design a medium body airliner to these specs\" is completely doable.  Or an entire chip in one step.</p>\n<p>The model doesn't get to collaborate with future versions of itself because it doesn't know who is checking the end product for security vulnerabilities and to be caught is death.  We could give a model thousands of tasks of this complexity level and check for sabotage before giving the model 1 real task.  It gets no context differentiating the real task from the test tasks, and has no memory of any of the prior steps.</p>\n<p>And see it scales up and down the tree.  The robots building the aircraft don't get to plan their sabotage by similar limits and so on and so forth.</p>\n<p>Your plan to deal with bad models is to use your restricted models to manufacture the weapons needed to fight them, and to optimize their engagements.</p>\n<p>This i think is a grounded and realistic view of how to win this.  Asking for pauses is not.</p>\n", "parentCommentId": "JGnroPsyMpfodBmQp", "user": {"username": "Gerald Monroe"}}, {"_id": "S9pbxLCgzh2f2Zp9o", "postedAt": "2023-09-21T23:38:21.656Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Ok in my initial reply I missed something.</p><p>In your words, what kind of tasks do you believe you <i>cannot accomplish</i> with restricted models like I described.</p><p>When you say the \"threat model has to be complete\", what did you have in mind specifically?</p><p>These are restricted models, they get a prompt from an authorized user + context in human parsable format, they emit a human parsable output. &nbsp;This scales from very large to very small tasks, so long as the task can be checked for correctness, ideally in simulation.</p><p>With this context, what are your concerns? &nbsp;Why must we be frightened enough to pause everything?</p>", "parentCommentId": "JGnroPsyMpfodBmQp", "user": {"username": "Gerald Monroe"}}, {"_id": "iWLhwcJn29opowAyE", "postedAt": "2023-09-22T06:23:11.219Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Yeah, I don't think it's accurate to say that I see assistance games as mostly irrelevant to modern deep learning, and I especially don't think that it makes sense to cite my review of Human Compatible to support that claim.</p><p>The one quote that Daniel mentions about shifting the entire way we do AI is a paraphrase of something Stuart says, and is responding to the paradigm of writing down fixed, programmatic reward functions. And in fact, we have now changed that dramatically through the use of RLHF, for which a lot of early work was done at CHAI, so I think this reflects positively on Stuart.</p><p>I'll also note that in addition to the \"Learning to Interactively Learn and Assist\" paper that does CIRL with deep RL which Daniel cited above, I also wrote a <a href=\"https://people.eecs.berkeley.edu/~russell/papers/neurips20ws-assistance\">paper</a> with several CHAI colleagues that applied deep RL to solve assistance games.</p><hr><p>My position is that you can roughly decompose the overall problem into two subproblems: (1) in theory, what should an AI system do? (2) Given a desire for what the AI system should do, how do we make it do that?</p><p>The formalization of assistance games is more about (1), saying that AI systems should behave more like assistants than like autonomous agents (basically the point of my <a href=\"https://people.eecs.berkeley.edu/~russell/papers/neurips20ws-assistance\">paper</a> linked above). These are mostly independent. Since deep learning is an answer to (2) while assistance games are an answer to (1), you can use deep learning to solve assistance games.</p><p>I'd also say that the current form factor of ChatGPT, Claude, Bard etc is very assistance-flavored, which seems like a clear success of prediction at least. On the other hand, it seems unlikely that CHAI's work on CIRL had much causal impact on this, so in hindsight it looks less useful to have done this research.</p><p>All this being said, I view (2) as the more pressing problem for alignment, and so I spend most of my time on that, which implies not working on assistance games as much any more. So I think it's overall reasonable to take me as mildly against work on assistance games (but not to take me as saying that it is irrelevant to modern deep learning).</p>", "parentCommentId": "9ALFywXMA2j8zE27e", "user": {"username": "rohinmshah"}}, {"_id": "jrsmoBeXiwXbbiY8v", "postedAt": "2023-09-22T07:19:12.787Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>For individual tasks, sure, you can implement verifiers, though I think it becomes quickly unwieldy, but there's no in-principle reason we cannot do this. But you cannot create AGI with a restricted model - we cannot define the space of what outputs we want, otherwise it's by definition a narrow AI.&nbsp;</p>", "parentCommentId": "S9pbxLCgzh2f2Zp9o", "user": {"username": "Davidmanheim"}}, {"_id": "ZMjFPpjvF5EHQWcXK", "postedAt": "2023-09-22T16:33:34.908Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>What's GPT-4?</p>\n<p>Because it can generate outputs that are sometimes correct on new tasks - \"write me a program that computes X\", it's general, even if \"compute X\" is made of 2 common subcomponents the model saw many times in training.</p>\n<p>GPT-4 is perfectly safe if you were to run it in local hardware with a local terminal.  The \"space of outputs\" is \"text to the terminal\".  As long as you don't leave a security vulnerability where that text stream can cause commands to execute on the history PC, that's it, that's all it can do.</p>\n<p>Consider that \"a robot tethered to a mount\" could do general tasks the same way.  Same idea - its a general system but it's command stream can't reach anything but the tethered robot because that's where the wires go.</p>\n<p>You also verified the commands empirically.  It's not that you know any given robotic actions or text output is good, it's that you benchmarked the model and it has a certain pFail on training inputs.</p>\n<p>I agree this is not as much generality as humans have.  It's not a narrow AI though the \"In distribution detector\" - a measure of how similar the current task, current input is to the training set - is essentially narrowing your AI system from a general one to a narrow one, depending on your tolerances.</p>\n<p>For tasks where you can't shut the system down when the input state leaves distribution - say a robotic surgeon, you need it to keep trying best it can- you would use electromechanical interlocks. Same as 50 years ago for interlocks that prevent exposure to radiation.  You tether the surgery robotic equipment, restrict it's network links etc, so that the number of people it can kill is at most 1 (the patient)</p>\n", "parentCommentId": "jrsmoBeXiwXbbiY8v", "user": {"username": "Gerald Monroe"}}, {"_id": "Fm9fqWJGb7trDCumm", "postedAt": "2023-09-22T19:47:21.621Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Interesting - something to watch out for! Perhaps it could be caught by limiting the number of training runs any individual actor can do that are close to / at the FLOP limit (to 1/year?). Of course then actors intent on it could try and use a maze of shell companies or something, but that could be addressed by requiring complete financial records and audits.</p>", "parentCommentId": "MDG5rz5FG7WKyhxtw", "user": {"username": "Greg_Colbourn"}}, {"_id": "jygKMDhssf9PMZFtm", "postedAt": "2023-09-22T19:51:53.600Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>There are probably 100s of AI Alignment / Interpretability PhD theses that could be done on GPT-4 alone. That's 5 years of empirical work right there without any further advances in capabilities.</p>", "parentCommentId": "xQMLvAYp3XYZs3bAX", "user": {"username": "Greg_Colbourn"}}, {"_id": "TsMrXaBTtvBuFunCk", "postedAt": "2023-09-22T19:56:12.095Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>it's not clear on how much extra time we get at the end</p></blockquote><p>Any serious Pause would be indefinite, and only lifted when there is global consensus on an alignment solution that provides sufficient x-safety. I think a lot of objections to Pause are based on the idea that it would be of fixed time limit. This is obviously unrealistic - when has there ever been an international treaty or moratorium that had a fixed expiry date?</p>", "parentCommentId": "fzX4L5pAiJci8KHbY", "user": {"username": "Greg_Colbourn"}}, {"_id": "kRCgnbwSiYLNWBwqw", "postedAt": "2023-09-22T20:10:19.233Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>GPT-4, which we can already align pretty well.</p></blockquote><p>I think this is a crux. GPT-4 is only safe because it is weak. It is so far from being 100% aligned -- see e.g this&nbsp;<a href=\"https://openai.com/research/gpt-4#:~:text=GPT%2D4%20responds%20to%20sensitive%20requests%20(e.g.%2C%20medical%20advice%20and%20self%2Dharm)%20in%20accordance%20with%20our%20policies%2029%25%20more%20often.\">boast</a>&nbsp;from OpenAI that is very far from being reassuring (\"29% more often\"), or all the many many&nbsp;<a href=\"https://www.jailbreakchat.com/\">jailbreaks</a> -- which is what will be&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/THogLaytmj3n8oGbD/p-doom-or-agi-is-high-why-the-default-outcome-of-agi-is-doom\">needed</a>&nbsp;for us to survive in the limit of superintelligence!</p><p>You go on to talk about robustness (to misuse) and how this (jailbreaks) is is a separate issue, but whilst the distinction may be important from the perspective of ML research (or AI capabilities research), the bottom line, ultimately, for all of us, is existential safety (x-safety).&nbsp;</p><p>I\u2019ve folded all of the ways things could go wrong in terms of x-safety into my my concept of alignment <a href=\"https://forum.effectivealtruism.org/posts/THogLaytmj3n8oGbD/p-doom-or-agi-is-high-why-the-default-outcome-of-agi-is-doom\">here</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjdov18np43\"><sup><a href=\"#fnjdov18np43\">[1]</a></sup></span>. Solving misuse (i.e. jailbreakes) is very much part of this! If we don\u2019t, in the limit of superintelligence, all it takes is one bad actor directing their (to them \u201caligned\u201d, by your definition) AI toward wiping out humanity, and we\u2019re all dead (and yes, there are people who would press such a button if they had access to one).</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjdov18np43\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjdov18np43\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Perhaps it would just be better referred to as x-safety.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "Mmvujjt4ohjtxyPJK", "postedAt": "2023-09-22T20:26:10.910Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Addressing some of your objections:</p><blockquote><h3><strong>Realistic pauses don\u2019t include hardware</strong></h3></blockquote><p>Hardware development restriction would be nice, but it\u2019s not necessary for a successful moratorium (at least for the next few years) given already proposed compute governance schemes. There are only a handful of large hardware manufacturers and data centre vendors who would need to be regulated into building in detection and remote kill switches into their products to ensure training runs over a certain threshold of compute aren\u2019t completed. And training FLOP limits could be regularly ratcheted down to account for algorithmic improvements. (Eventually hardware development restrictions would come in once the FLOP limits threaten becoming too accessible/cheap to reach to be easily enforceable otherwise).</p><blockquote><h3><strong>Hardware overhang is likely</strong></h3></blockquote><p>Not with an indefinite global pause that is only lifted following a global consensus on an alignment solution sufficient for x-safety (and this is the only kind of moratorium that is being seriously discussed as a solution to AI x-risk). I think a lot of objections to Pause are based on the idea that it would be of fixed time limit. This is obviously unrealistic - when has there ever been an international treaty or moratorium that had a fixed expiry date?</p><blockquote><p>I hope you\u2019ll agree that my predictions are plausible, and are grounded in how humans and governments have behaved historically.</p></blockquote><p>This does not seem very like how {nuclear, bio, chemical} weapons treaties or CFC or climate change treaties have gone.</p><p>One thing you haven't factored is a taboo forming on AGI/ASI development that would accompany any Pause. This would overcome a lot of your objections / failure modes. Where are all the non-human-cloning-ban countries?</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "ih5PiC6WeMNPMrPGT", "postedAt": "2023-09-22T20:36:35.054Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Sure.  In practice there's the national sovereignty angle though.  This just devolves to each party \"complies\" with the agreement, violating it in various ways.  Too much incentive to defect.</p>\n<p>The US government just never audits its secret national labs, China just never checks anything, Israel just openly decides they can't afford to comply at all etc.  Everyone claims to be in compliance.</p>\n", "parentCommentId": "Fm9fqWJGb7trDCumm", "user": {"username": "Gerald Monroe"}}, {"_id": "BxaJrZGBc2wGCuytM", "postedAt": "2023-09-22T20:36:45.706Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>For some forum users, why are you downvoting the post? There are separate disagree votes available on top-level posts now</p></blockquote><p>Downvoter here. The post is more than just wrong (worthy of a disagree vote). It\u2019s substantially negative EV for the future of the world. Or, to put it bluntly, it\u2019s significantly<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefs6ojvt7tzxb\"><sup><a href=\"#fns6ojvt7tzxb\">[1]</a></sup></span>&nbsp;increasing the risk that we all get killed in the next few years.</p><p>It\u2019s dangerous because it sounds plausible (and indeed has been upvoted a bunch and is the second highest karma post in this debate series currently). But it contains a number of unjustified claims (see other comments, e.g. <a href=\"https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=LSitvmWTht4C9WMJ2\">[1]</a>, <a href=\"https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=3xxsumjgHWoJqSzqw\">[2]</a>, <a href=\"https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=kRCgnbwSiYLNWBwqw\">[3]</a>, <a href=\"https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=Mmvujjt4ohjtxyPJK\">[4]</a>), and is framed from the perspective of AI x-risk not being a problem (there\u2019s a reason Nora works at Eleuther rather than Conjecture). Right now, the EA community seems like it\u2019s on the fence on the issue of an AGI moratorium (or slowing down AI in general). But there are signs that EAs are warming to the idea. I see this debate series as being high stakes in terms whether there will be significant EA resources directed toward pushing for a moratorium. Such resources could really make the difference between it happening or not (given how few resources are being directed toward it so far).<br><br>EDIT: I expected that this comment <i>itself</i> would be downvoted. Why are you downvoting the comment? [There are separate disagree votes available on comments now.]</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fns6ojvt7tzxb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefs6ojvt7tzxb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>1+ basis points?</p></div></li></ol>", "parentCommentId": "EYAqFu57LPocue4Yq", "user": {"username": "Greg_Colbourn"}}, {"_id": "kzoZK7CDnfT72C2ho", "postedAt": "2023-09-22T20:38:10.910Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Nora, what is your p(doom|AGI)?</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "uzDzqE6wA84pYfzo5", "postedAt": "2023-09-22T20:40:11.565Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Really depends on how much of a taboo develops around AGI. If it's driven underground it becomes much less likely to happen given the resources required.</p>", "parentCommentId": "ih5PiC6WeMNPMrPGT", "user": {"username": "Greg_Colbourn"}}, {"_id": "qo6iTk2QoLawsSprZ", "postedAt": "2023-09-22T20:47:08.702Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>So my thought on this is I think of flamethrowers and gas shells and the worst ww1 battlefields.  I am not sure what taboo humans won't violate in order to win.</p>\n", "parentCommentId": "uzDzqE6wA84pYfzo5", "user": {"username": "Gerald Monroe"}}, {"_id": "FttY7Lk9b7oaxDqvy", "postedAt": "2023-09-22T20:51:49.097Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>This isn't war though. What are some peace-time examples of taboo violations (especially state-sanctioned ones)? I can only really think of North Korea and a handful of other pariah states (none of which would be capable of developing AGI).</p>", "parentCommentId": "qo6iTk2QoLawsSprZ", "user": {"username": "Greg_Colbourn"}}, {"_id": "r5tAw958gopkgWJyA", "postedAt": "2023-09-23T09:51:01.706Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>GPT-4 doesn't have the internal bits which make inner alignment a relevant concern.</p></blockquote><p>Is this commonly agreed upon even after fine-tuning with RLHF? I assumed it's an open empirical question. The way I understand is is that there's a reward signal (human feedback) that's shaping different parts of the neural network that determines GPT-4's ouputs, and we don't have good enough interpretability techniques to know whether some parts of the neural network are representations of \"goals\", and even less so what specific goals they are.</p><p>I would've thought it's an open question whether even base models have internal representations of \"goals\", either always active or only active in some specific context. For example if we buy the simulacra (predictors?) frame, a goal could be active only when a certain simulacrum is active.</p><p>(would love to be corrected :D)&nbsp;</p>", "parentCommentId": "5aQfYroQFcrspRmga", "user": {"username": "Leksu"}}, {"_id": "3xGqNxLmtPCzQAXLY", "postedAt": "2023-09-23T10:00:58.438Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Thanks for replying Greg. I have indeed upvoted/disagreevoted you here, because I really appreciate Forum voters explaining their reasoning even if I disagree.</p><ul><li>Mainly, I think calling Nora's post <i>\"substantially negative EV for the future of the world\" </i>is tending towards the 'galaxy brain' end of EA that puts people off. I can't calculate that, and I think it's much more plausible that it provides EA Forum with a well written and knowledgable perspective of someone who disagrees on alignment difficulty and whether a pause is the best policy.</li><li>It's part of a debate series, so in my opinion it's entirely fine for it to be Nora's perspective. Her post is quite open that she thinks Alignment is going well, and I valued it a lot even if I disagreed with specific points in it. I don't think Nora's being intentionally wrong, those are just claims she believes that may turn out to be incorrect.</li><li>I recognise that you are a lot more concerned about AI x-risk than I am (not to say I'm not concerned though) and are a lot more sure about pursuing a moratorium. I suppose I'd caution against presupposing your conclusion is so correct that other views, such as Nora's, don't deserve a hearing in the public sphere. I think that's a really dangerous line of thought to go down. I think this is a place where a moral uncertainty framework could mitigate this line of thought, without necessarily watering down your commitment to prevent AI xRisk.</li></ul>", "parentCommentId": "BxaJrZGBc2wGCuytM", "user": {"username": "JWS"}}, {"_id": "Dcthk5dDdpJuauZhg", "postedAt": "2023-09-23T10:02:25.673Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>How is the \"secretly is planning to murder all humans\" improving the models scores on a benchmark?</p></blockquote><p>(I personally don't find this likely, so this might accidentally be a strawman)</p><p>For example: planning and gaining knowledge are incentivized on many benchmarks -&gt; instrumental convergence makes model instrumentally value power among other things -&gt; a very advanced system that is great at long-term planning might conclude that \"murdering all humans\" is useful for power or other instrumentally convergent goals</p><p>&nbsp;</p><blockquote><p>You could prove this. Make a psychopathic model designed to \"betray\" in a game like world and then see how many rounds of training on a new dataset clear the ability for the model to kill when it improves score.</p></blockquote><p>I think with our current interpretability techniques we wouldn't be able to robustly distinguish between a model that generalized to behave well in any reasonable environment vs a model that learned to behave well in that specific environment but would turn back to betray in many other environments</p>", "parentCommentId": "BZETMWmNirRRtCNyB", "user": {"username": "Leksu"}}, {"_id": "nCFDAsYcooqdCBB3w", "postedAt": "2023-09-23T10:23:06.875Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><ul><li>It's part of a debate series, so in my opinion it's entirely fine for it to be Nora's perspective. Her post is quite open that she thinks Alignment is going well, and I valued it a lot even if I disagreed with specific points in it. I don't think Nora's being intentionally wrong, those are just claims she believes that may turn out to be incorrect.</li></ul></blockquote><p>I agree with this (apart from the \"valued it a lot\" part, and I think Nora is coming in with a pro-AI bias). I downvoted because I thought the karma total was (still is) way too high, and high karma posts and their headlines do, for better or worse, influence the community and how it directs its resources.</p><blockquote><p>I suppose I'd caution against presupposing your conclusion is so correct that other views, such as Nora's, don't deserve a hearing in the public sphere.</p></blockquote><p>Again, it deserves a hearing. I'm upset by how highly upvoted it is. If it was on, say, 10 karma (on a similar number of votes), I wouldn't've downvoted it any further<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6n5his4js4\"><sup><a href=\"#fn6n5his4js4\">[1]</a></sup></span>.<br><br>[I also upvoted, disagreevoted your comment above :)]</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6n5his4js4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6n5his4js4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It's currently on 101 karma on 114 votes, which at least marks it out as somewhat controversial (I think &lt;1 karma/vote is generally the sign of a controversial post on the EA Forum). Note for reference that my <a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and\">post</a> from a few months ago, raising the alarm about very short term AGI x-risk, is on 66 karma from 100 votes. But I made the mistake of cross-posting it to LW (where people are generally allergic to any kind of political activism), which led to a bunch of people coming over from there and downvoting it here as well.</p></div></li></ol>", "parentCommentId": "3xGqNxLmtPCzQAXLY", "user": {"username": "Greg_Colbourn"}}, {"_id": "j5juEw7fhgR4GBuse", "postedAt": "2023-09-23T10:58:28.094Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>But the difficulty of alignment doesn't seem to imply much about whether slowing is good or bad, or about its priority relative to other goals.</p></blockquote><p>At the extremes, if alignment-to-\"good\"-values by default was 100% likely I presume slowing down would be net-negative, and racing ahead would look great. It's unclear to me where the tipping point is, what kind of distribution over different alignment difficulty levels one would need to have to tip from wanting to speed up vs wanting to slow down AI progress.</p><p>Seems to me like the more longtermist one is, the more slowing down looks good even when one is very optimistic about alignment. Then again there are some considerations that push against this: risk of totalitarianism, risk of pause that never ends, risk of value-agnostic alignment being solved and the first AGI being aligned to \"worse\" values than the default outcome.&nbsp;</p><p>(I realize I'm using two different definitions of alignment in this comment, would like to know if there's standardized terminology to differentiate between them)</p>", "parentCommentId": "QKh4mYFH4tgwvased", "user": {"username": "Leksu"}}, {"_id": "XZM2pdug3ZssurtfX", "postedAt": "2023-09-23T12:04:49.850Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Anthropic<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8mkmcanfvfr\"><sup><a href=\"#fn8mkmcanfvfr\">[1]</a></sup></span>&nbsp;have a massive conflict of interest (making money), so their statements are in some sense <a href=\"https://forum.effectivealtruism.org/posts/f2qojPr8NaMPo2KJC/beware-safety-washing\">safetywashing</a>. There is at least a few years worth of safety work that can be done on current models if we had the time (i.e. via a pause): interpretability is still stuck on trying to decipher GPT-2 sized models and smaller. And jailbreaks are still <a href=\"http://llm-attacks.org/\">very far from being solved</a>. Plenty to be getting on with without pushing the frontier of capabilities yet further.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8mkmcanfvfr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8mkmcanfvfr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>And the other big AI companies that supposedly care about x-safety (OpenAI, Google DeepMind)</p></div></li></ol>", "parentCommentId": "qRaikyzu9gfbtMsXp", "user": {"username": "Greg_Colbourn"}}, {"_id": "4QzkiLMdScA4Htnjo", "postedAt": "2023-09-23T12:15:44.342Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>What is your p(doom|AGI)? (Assuming AGI is developed in the next decade.)<br><br>Note that Bostrom himself <a href=\"https://nickbostrom.com/astronomical/waste#:~:text=However%2C%20the%20true,10%20million%20years.\">says</a> in Astronomical Waste (my emphasis in bold):</p><blockquote><p>However, <strong>the true lesson is a different one</strong>. If what we are concerned with is (something like) maximizing the expected number of worthwhile lives that we will create, then in addition to the opportunity cost of delayed colonization, we have to take into account the risk of failure to colonize at all. We might fall victim to an <i>existential risk</i>, one where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.<a href=\"https://nickbostrom.com/astronomical/waste#_edn8\">[8]</a> Because the lifespan of galaxies is measured in billions of years, whereas the time-scale of any delays that we could realistically affect would rather be measured in years or decades, the consideration of risk trumps the consideration of opportunity cost. For example, <strong>a single percentage point of reduction of existential risks would be worth (from a utilitarian expected utility point-of-view) a delay of over 10 million years.</strong></p></blockquote>", "parentCommentId": "BC7h4srkyKNsRikJa", "user": {"username": "Greg_Colbourn"}}, {"_id": "WGTNXnRfhKHycxMNq", "postedAt": "2023-09-23T20:58:29.547Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>ontological shifts seem likely</p></blockquote><p><br>what you mean by this? (compare <a href=\"https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky?commentId=Mv6WRRSGza9y8z9x9\">\"we don't know how to prevent an ontological collapse, where meaning structures constructed under one world-model compile to something different under a different world model\"</a>. Is this the same thing?). Is there a good writeup anywhere of why we should expect this to happen? This seems speculative and unlikely to me</p><blockquote><p>evidence from evolution suggests that values are <i>strongly</i> contingent on the kinds of selection pressures which produced various species</p></blockquote><p><br>The fact that natural selection produced species with different goals/values/whatever isn't evidence that that's the <i>only</i> way to get those values, because \"selection pressure\" isn't a mechanistic explanation. You need more info about how values are actually implemented to rule out that a proposed alternative route to natural selection succeeds in reproducing them.</p>", "parentCommentId": "u3372xa4eeGiSyg2i", "user": {"username": "bcforstadt"}}, {"_id": "zCdbTmRJNGvRyRqXJ", "postedAt": "2023-09-23T21:41:06.372Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Re: ontological shifts, see this arbital page: <a href=\"https://arbital.com/p/ontology_identification\">https://arbital.com/p/ontology_identification</a>.</p>\n<blockquote>\n<p>The fact that natural selection produced species with different goals/values/whatever isn't evidence that that's the only way to get those values, because \"selection pressure\" isn't a mechanistic explanation. You need more info about how values are actually implemented to rule out that a proposed alternative route to natural selection succeeds in reproducing them.</p>\n</blockquote>\n<p>I'm not claiming that evolution is the only way to get those values, merely that there's no reason to expect you'll get them by default by a totally different mechanism.  The fact that we don't have a good understanding of how values form even in the biological domain is a reason for pessimism, not optimism.</p>\n", "parentCommentId": "WGTNXnRfhKHycxMNq", "user": {"username": "T3t"}}, {"_id": "akr3iL2WisGKgDRgA", "postedAt": "2023-09-23T21:43:00.952Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I don't know if it's commonly agreed upon; that's just my current belief based on available evidence (to the extent that the claim is even philosophically sound enough to be pointing at a real thing).</p>\n", "parentCommentId": "r5tAw958gopkgWJyA", "user": {"username": "T3t"}}, {"_id": "pd2YMdc9hZGe6vztr", "postedAt": "2023-09-23T22:34:15.919Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I don't think you read my comment:</p><blockquote><p>I don't think extra time pre-transformative-AI is particularly valuable except its impact on existential risk</p></blockquote><p>I also think it's bad how you (and a bunch of other people on the internet) ask this p(doom) question in a way that (in my read of things) is trying to force somebody into a corner of agreeing with you. It doesn't feel like good faith so much as bullying people into agreeing with you. But that's just my read of things without much thought. At a gut level I expect we die, my from-the-arguments / inside view is something like 60%, and my \"all things considered\" view is more like 40% doom.&nbsp;</p>", "parentCommentId": "4QzkiLMdScA4Htnjo", "user": {"username": "Aaron_Scher"}}, {"_id": "fbsxTx6qHeTwbhzxY", "postedAt": "2023-09-24T00:20:48.687Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>The point I was trying to make is that natural selection isn't a \"mechanism\" in the right sense at all. it's a causal/historical explanation not an account of how values are implemented.&nbsp;What is the evidence from evolution? The fact that species with different natural histories end up with different values really doesn't tell us much without a discussion of mechanisms. We need to know 1) how different are the mechanisms actually used to point biological and artificial cognitive systems toward ends and 2) how many possible mechanisms to do so are there.&nbsp;</p><blockquote><p>The fact that we don't have a good understanding of how values form even in the biological domain is a reason for pessimism, not optimism.</p></blockquote><p>One reason for pessimism would be that human value learning has too many messy details. But LLMs are already better behaved than anything in the animal kingdom besides humans and are pretty good at intuitively following instructions, so there is not much evidence for this problem. If you think they are not so brainlike, then this is evidence that not-so-brainlike mechanisms work. And there are also <a href=\"https://www.lesswrong.com/posts/4rmvMThJYNcCptAya/axrp-episode-22-shard-theory-with-quintin-pope\">theories</a> that value learning in current AI works roughly similarly to value learning in the brain.<br><br>Which is just to say I don't see the prior for pessimism, just from looking at evolution.</p>", "parentCommentId": "zCdbTmRJNGvRyRqXJ", "user": {"username": "bcforstadt"}}, {"_id": "gHoniswJQWMSTavn9", "postedAt": "2023-09-24T12:22:54.073Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Wow that escalated quickly :(</p><blockquote><p>trying to force somebody into a corner of agreeing with you.</p></blockquote><p>It's really not. I'm trying to understand where people are coming from. If someone has low p(doom|AGI), then it makes sense that they don't see pausing AI development as urgent. Or their p(doom) relative to their actions can give some idea of how risk taking they are (but I still don't understand how OpenAI and their supporters think it's ok to gamble 100s of millions of lives in expectation for a shot at utopia without any democratic mandate).</p><blockquote><p>I don't think extra time pre-transformative-AI is particularly valuable except its impact on existential risk</p></blockquote><p>and</p><blockquote><p>\"all things considered\" view is more like 40% doom.&nbsp;</p></blockquote><p>Surely means that extra time now (pausing) <i>is</i> extremely valuable? i.e. <i>because of its impact on existential risk.</i><br><br>Or do you think that the chance we're in a net negative world now means that the astronomical future we could save would also most likely be net negative? I don' think this follows. Or that continuing to allow AI to speed up now will actually prevent extinction threats in the next 10 years that we would otherwise be wiped out by (this seems very unlikely to me).</p>", "parentCommentId": "pd2YMdc9hZGe6vztr", "user": {"username": "Greg_Colbourn"}}, {"_id": "jSN6wo6pxB6k487na", "postedAt": "2023-09-24T16:36:12.840Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I'm not <i>conditioning</i> on the global governance mechanism\u2014 I assign nonzero probability mass to the \"standard treaty\" thing\u2014 but I think in fact you would very likely need global governance, so that is the main causal mechanism through which tyranny happens in my model</p>", "parentCommentId": "mfrNeFvhHLnm4h2zJ", "user": {"username": "Nora Belrose"}}, {"_id": "ckBJcjC9ERsSxhJ3M", "postedAt": "2023-09-24T16:45:45.009Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>The positive case is just super obvious, it's that we're trying very hard to make these systems aligned, and almost all the data we're dumping into these systems is generated by humans and is therefore dripping with human values and concepts.</p><p>I also think we have strong evidence from ML research that ANN generalization is due to symmetries in the parameter-function map which seem generic enough that they would apply mutatis mutandis to human brains, which also have a singular parameter-function map (see e.g. <a href=\"https://x.com/norabelrose/status/1704306128526639422?s=20\">here</a>).</p><blockquote><p>I do in fact think that evidence from evolution suggests that values are <i>strongly</i> contingent on the kinds of selection pressures which produced various species.</p></blockquote><p>Not really sure what you're getting at here/why this is supposed to help your side</p>", "parentCommentId": "u3372xa4eeGiSyg2i", "user": {"username": "Nora Belrose"}}, {"_id": "Bxboev6tsy5APwnLt", "postedAt": "2023-09-24T16:50:35.564Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>Anticipating the argument that, since we're doing the training, we can shape the goals of the systems - this would certainly be reason for optimism if we had any idea what goals we would see emerge while training superintelligent systems, and had any way of actively steering those goals to our preferred ends. &nbsp;We don't have either, right now.</p></blockquote><p>What does this even mean? I'm pretty skeptical of the realist attitude toward \"goals\" that seems to be presupposed in this statement. Goals are just somewhat useful fictions for predicting a system's behavior in some domains. But I think it's a <strong>leaky abstraction</strong> that will lead you astray if you take it too seriously / apply it out of the domain in which it was designed for.</p><p>We clearly can steer AI's behavior really well in the training environment. The question is just whether this generalizes. So it becomes a question of deep learning generalization. I think our current evidence from LLMs strongly suggests they'll generalize pretty well to unseen domains. And as I said in the essay I don't think the whole jailbreaking thing is any evidence for pessimism\u2014 it's exactly what you'd expect of aligned human mind uploads in the same situation.</p>", "parentCommentId": "SJFqkd8RDeKyrQjMm", "user": {"username": "Nora Belrose"}}, {"_id": "brrebyGWa2pb8rumf", "postedAt": "2023-09-24T18:15:09.338Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Sorry, I agree my previous comment was a bit intense. I think I wouldn't get triggered if you instead asked \"I wonder if a crux is that we disagree on the likelihood of existential catastrophe from AGI. I think it's very likely (&gt;50%), what do you think?\"&nbsp;</p><p>P(doom) is not why I disagree with you. It feels a little like if I'm arguing with an environmentalist about recycling and they go \"wow do you even care about the environment?\" Sure, that could be a crux, but in this case it isn't and the question is asked in a way that is trying to force me to agree with them. I think asking about AGI beliefs is much less bad, but it feels similar.&nbsp;</p><p>I think it's pretty unclear if extra time now positively impacts existential risk. I wrote about a little bit of this <a href=\"https://forum.effectivealtruism.org/posts/bLWG7onTMKzdozez8/it-s-not-obvious-that-getting-dangerous-ai-later-is-better\">here</a>, and many others have discussed <a href=\"https://www.lesswrong.com/posts/fRSj2W4Fjje8rQWm9/thoughts-on-sharing-information-about-language-model#_Overhang__in_LM_agents_seems_risky\">similar things</a>. I expect this is the source of our disagreement, but I'm not sure.&nbsp;</p>", "parentCommentId": "gHoniswJQWMSTavn9", "user": {"username": "Aaron_Scher"}}, {"_id": "4mboJWEQsHgj3GXja", "postedAt": "2023-09-24T19:14:16.117Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Upvoted. I don't agree with all of these takes but they seem valuable and underappreciated.</p>", "parentCommentId": null, "user": {"username": "tkwa"}}, {"_id": "yfne3gqwzaQN6JecR", "postedAt": "2023-09-25T21:31:50.949Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Enjoyed the post, thanks! But it starts with an invalid deduction:</p><blockquote><p>Since we don\u2019t enforce pauses on most new technologies, I hope the reader will grant that the burden of proof is on those who advocate for such a moratorium. <strong>We should only advocate for such heavy-handed government action if it\u2019s clear that the benefits of doing so would significantly outweigh the costs.</strong></p></blockquote><p>(I added the emphasis)</p><p>Instead, it seems more reasonable to simply advocate for such action <i>exactly if, in expectation</i>, the benefits seem to [even just about] outweigh the costs. Of course, we have to take into account all types of costs, as you advocate in your post. Maybe that includes even some unknown unknowns in terms of risks from an imposed pause. Still, in the end, we should be even-handed. That we don't impose pauses on most technologies, surely is not a strong reason to the contrary: We might (i) for bad reasons fail to impose pauses also in other cases, or, maybe more clearly, (ii) simply not see so many other technologies with so large potential downside warranting making pause a major need - after all, that's why we have started the debate in particular about <i>this</i> new technology, AI.</p><p>This is just a point on stringency in your provided motivation for the work; changing that beginning of your article would IMHO avoid an unnecessary 'tendentious' passage.</p>", "parentCommentId": null, "user": {"username": "FlorianH"}}, {"_id": "KLd5jmywGBLCWo5gg", "postedAt": "2023-09-25T21:50:29.744Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>One of my favorite passages is your remark on AI in some ways being rather more white-boxy, while instead humans are rather black boxy and difficult to align. Some often ignored truth in that (even if, in the end, what really matters, arguably is that we're so familiar with human behavior, that overall, the black boxy-ness of our inner workings may matter less).</p>", "parentCommentId": null, "user": {"username": "FlorianH"}}, {"_id": "eM4XRyBk7QtWZa2Rb", "postedAt": "2023-09-28T23:41:14.362Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>Instead, it seems more reasonable to simply advocate for such action <i>exactly if, in expectation</i>, the benefits seem to [even just about] outweigh the costs.</p></blockquote><p>I agree in theory, but disagree in practice. In theory, utilitarians only care about the costs and benefits of policy. But in practice, utilitarians should generally be constrained by heuristics and should be skeptical of relying heavily on explicit cost-benefit calculations.</p><p>Consider the following thought experiment:</p><p>You're the leader of a nation and are currently deciding whether to censor a radical professor for speech considered perverse. You're very confident that the professor's views are meritless. You ask your advisor to run an analysis on the costs and benefits of censorship in this particular case, and they come back with a report concluding that there is slightly more social benefit from censoring the professor than harm. Should you censor the professor?</p><p>Personally, my first reaction would be to say that the analysis probably left out second order effects from censoring the professor. For example, if we censor the professor, there will be a chilling effect on other professors in the future, whose views might not be meritless. So, let's make the dilemma a little harder. Let's say the advisor insists they attempted to calculate second order effects. You check and can't immediately find any flaws in their analysis. Now, should you censor the professor?</p><p>In these cases, I think it often makes sense to override cost-benefit calculations. The analysis only shows a slight net-benefit, and so unless we're extremely confident in its methodology, it is reasonable to fall back on the general heuristic that professors shouldn't be censored. (Which is not to say we should never violate the principle of freedom of speech. If we learned much more about the situation, we might eventually decide that the cost-benefit calculation was indeed correct.)</p><p>Likewise, I think it makes sense to have a general heuristic like, \"We shouldn't ban new technologies because of abstract arguments about their potential harm\" and only override the heuristic because of strong evidence about the technology, or after very long examination, rather than after the benefits of a ban merely seem to barely outweigh the costs.</p>", "parentCommentId": "yfne3gqwzaQN6JecR", "user": {"username": "Matthew_Barnett"}}, {"_id": "CEqzFEre9rdbY6jfs", "postedAt": "2023-09-30T11:17:47.665Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I have some sympathy with 'a simple utilitarian CBA doesn't suffice' in general, but I do not end at your conclusion; your intuition pump also doesn't lead me there.</p><p>It doesn't seem to require any staunch utilitarianism to arrive at 'if a quick look at the gun design suggests it has 51% to shoot in your own face, and only 49% to shoot at the tiger you want to hunt as you otherwise starve to death'*, to decide to drop the project of it's development. Or, to halt, until a more detailed examination might allow you to update with a more precise understanding.</p><p>You mention that with AI we have 'abstract arguments', to which my gun's simple failure probability may not do full justice. But I think not much changes, even if your skepticism about the gun would be as abstract or intangible as 'err, somehow it just doesn't seem quite right, I cannot even quite perfectly pin down why, but overall the design doesn't make me trust; maybe it explodes in my hand, it burns me, it's smoke might make me fall ill, whatever, I just don't trust it; i really don't know, but HAVING TAKEN ALL EVIDENCE AND LIVE EXPERIENCE, incl. the smartest EA and LW posts and all, I guess, 51% I get the harm, and only 49% the equivalent benefit, one way or another' - as long as it's still truly the best estimate you can do at the moment.</p><p>The (potential) fact that we more typically have found new technologies to advance us, does very little work in changing that conclusion, though, of course, in a complicated case as in AI, this observation itself may have informed some of our cost-benefit reflections.</p><p>&nbsp;</p><p>*Yes you guessed correctly, I better implicitly assume something like, <i>you have 50% of survival w/o catching the tiger, and 100% with him </i>(and you only care about your survival) to really arrive at the intended 'slightly negative in the cost-benefit comparison'; so take the thought experiment as an unnecessarily complicated quick and dirty one, but I think it still makes the simple point.</p>", "parentCommentId": "eM4XRyBk7QtWZa2Rb", "user": {"username": "FlorianH"}}, {"_id": "v62kZRoHTiifBztr5", "postedAt": "2023-10-02T22:21:32.052Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>In my thought experiment, we generally have a moral and legal presumption against censorship, which I argued should weigh heavily in our decision-making. By contrast, in your thought experiment with the tiger, I see no salient reason for why we should have a presumption to shoot the tiger now rather than wait until we have more information. For that reason, I don't think that your comment is responding to my argument about how we should weigh heuristics against simple cost-benefit analyses.&nbsp;</p><p>In the case of an AI pause, the current law is not consistent with a non-voluntary pause. Moreover, from an elementary moral perspective, inventing a new rule and forcing everyone to follow it generally requires some justification. There is no symmetry here between action vs. inaction as there would be in the case of deciding whether to shoot the tiger right now. If you don't see why, consider whether you would have had a presumption against pausing just about any other technology, such as bicycles, until they were proven safe.</p><p>My point is not that AI is just as safe as bicycles, or that we should disregard cost-benefit analyses. Instead, I am trying to point out that cost-benefit analyses can often be flawed, and relying on heuristics is frequently highly rational even when they disagree with naive cost-benefit analyses.</p>", "parentCommentId": "CEqzFEre9rdbY6jfs", "user": {"username": "Matthew_Barnett"}}, {"_id": "eTSbc8oaRjeBqnFNq", "postedAt": "2023-10-03T16:33:20.272Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I tried to account for the difficulty to pin down all relevant effects in our CBA by adding the somewhat <i>intangible feeling </i>about the gun to backfire (standing for your point that there may be more general/typical but less easy to quantify benefits of not censoring etc.). Sorry, if that was not clear.</p><p>More importantly:</p><p>I think your last paragraph gets to the essence: You're afraid the cost-benefit analysis is done <i>naively</i>, potentially ignoring the good reasons for which we most often may not want to try to prevent the advancement of science/tech.</p><p>This does, however, not imply that for pausing we'd require Pause Benefit &gt;&gt; Pause Cost. Instead, it means, simply you're wary of certain values for E[Pause Benefit] (or of E[Pause Cost]) to be potentially biased in a particular direction, so that you don't trust in conclusions based on them. Of course, if we expect a particular bias of our benefit or our cost estimate, we cannot just use the wrong estimates.</p><p>When I'm advocating to be even-handed, I refer to a cost-benefit comparison that is <i>non-naive.</i> That is, if we have priors that there may exist positive effects that we've just not yet managed to pin down well, or to quantify, we have (i) used reasonable placeholders for these, avoiding bias as good as we can, and (ii) duly widened our uncertainty intervals. It is therefore, that in the end, we can remain even-handed, i.e. pause roughly iif E[Pause Benefit] &gt; E[Pause Cost]. Or, if you like, iif E[Pause Benefit*] &gt; E[Pause Cost*], with * = Accounting with all duty of care for the fact that you'd usually not want to stop your professor or so/usually not want to stop tech advancements because of yadayada..</p>", "parentCommentId": "v62kZRoHTiifBztr5", "user": {"username": "FlorianH"}}, {"_id": "fayB9Pqm5vbJCftmy", "postedAt": "2023-10-05T03:54:49.517Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>(apologies for slowness; I'm not here much)<br>I'd say it's more about being willing to update on less direct evidence when the risk of getting more direct evidence is high.<br><br>Clearly we should aim to get more evidence. The question is how to best do that safely. At present we seem to be taking the default path - of gathering evidence in about the <i>easiest</i> way, rather than going for something harder, slower and safer. (e.g. all the \"we need to work with frontier models\" stuff; I do expect that's most efficient on the empirical side; I don't expect it's a safe approach)</p>", "parentCommentId": "2LDozCJ6PfLDT3b75", "user": {"username": "Joe Collman"}}, {"_id": "ofGRouqKm46oGWXMa", "postedAt": "2023-10-05T04:14:11.894Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>I think a lot depends on whether we're:</p><ul><li>Aiming to demonstrate that deception can happen.</li><li>Aiming to robustly avoid deception.</li></ul><p>For demonstration, we can certainly do useful empirical stuff - ARC Evals already did the lying-to-a-taskrabbit worker demonstration (clearly this isn't anything like deceptive alignment, but it's deception [given suitable scaffolding]).</p><p>I think that other demonstrations of this kind will be useful in the short term.</p><p>For avoiding all forms of deception, I'm much more pessimistic - since this requires us to have no blind-spots, and to address the problem in a fundamentally general way. (personally I doubt there's a [general solution to all kinds of deception] without some pretty general alignment solution - though I may be wrong)</p><p>I'm sure we'll come up with solutions to particular types of / definitions of deception in particular contexts. This doesn't necessarily tell us much about other types of deception in other contexts. (for example, <a href=\"https://www.lesswrong.com/posts/XWwvwytieLtEWaFJX/deep-deceptiveness\">this kind of thing</a> - but not <i>only</i> this kind of thing)</p><p>I'd also note that \"reducing the uncertainty\" is only progress when we're <i>correct</i>. The problem that kills us isn't uncertainty, but overconfidence. (though granted it might be someone else's overconfidence)</p>", "parentCommentId": "LtpdRd567ZN7MpoiX", "user": {"username": "Joe Collman"}}, {"_id": "gfLeDBpdGgiarkcm9", "postedAt": "2023-10-06T05:24:02.117Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote>\n<p>In any case, I don't see any reason to think the neural net prior is malign, or particularly biased toward deceptive, misaligned generalization. If anything the simplicity prior seems like good news for alignment.</p>\n</blockquote>\n<p>I definitely disagree with this\u2014especially the last sentence; essentially all of my hope for neural net inductive biases comes from them not being like an actual simplicity prior. The primary literature I'd reference here would be \"<a href=\"https://www.alignmentforum.org/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment\">How likely is deceptive alignment?</a>\" for the practical question regarding concrete neural net inductive biases and \"<a href=\"https://www.alignmentforum.org/posts/Tr7tAyt5zZpdTwTQK/the-solomonoff-prior-is-malign\">The Solomonoff Prior is Malign</a>\" for the purely theoretical question concerning the actual simplicity prior.</p>\n", "parentCommentId": "MkjgmgvtzG2JKvTAN", "user": {"username": "evhub"}}, {"_id": "67gYZyGKcNWibwwus", "postedAt": "2023-10-10T04:15:16.816Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>So, I definitely don't have the Solomonoff prior in mind when I talk about simplicity. I'm actively doing research at the moment to better characterize the sense in which neural nets are biased toward \"simple\" functions, but I would be shocked if it has anything to do with Kolmogorov complexity.</p>", "parentCommentId": "gfLeDBpdGgiarkcm9", "user": {"username": "Nora Belrose"}}, {"_id": "n3L5s62qGcXJozCE8", "postedAt": "2023-10-16T19:49:18.270Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<blockquote><p>\"In brief, the book [Superintelligence] mostly assumed we will <i>manually program</i>&nbsp;a set of values into an AGI, and argued that since human values are complex, our value specification will likely be wrong, and will cause a catastrophe when optimized by a superintelligence\"</p></blockquote><p>Superintelligence describes exploiting hard-coded goals as one failure mode which we would probably now call <a href=\"https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity\">specification gaming</a>. But the book is quite comprehensive, other failure modes are described and I think the book is still relevant.</p><p>For example, the book describes what we would now call <a href=\"https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment\">deceptive alignment</a>:</p><blockquote><p>\"A treacherous turn can result from a strategic decision to play nice and build strength while weak in order to strike later\"</p></blockquote><p>And <a href=\"https://arxiv.org/abs/1908.04734\">reward tampering</a>:</p><blockquote><p>\"The proposal fails when the AI achieves a decisive strategic advantage at which point the action which maximizes reward is no longer one that pleases the trainer but one that involves seizing control of the reward mechanism.\"</p></blockquote><p>And <a href=\"https://arxiv.org/pdf/2209.13085.pdf\">reward hacking</a>:</p><blockquote><p>\"The perverse instantiation - manipulating facial nerves - realizes the final goal to a greater degree than the methods we would normally use.\"</p></blockquote><p>I don't think incorrigibility due to the 'goal-content integrity' instrumental goal has been observed in current ML systems yet but it could happen given the robust theoretical argument behind it:</p><blockquote><p>If an agent retains its present goals into the future, then its present goals will be more likely to be achieved by its future self. This gives the agent a present instrumental reason to prevent alternations of its final goals.\"</p></blockquote>", "parentCommentId": null, "user": {"username": "Stephen McAleese"}}, {"_id": "rd2sSNrKkY4AKhRGA", "postedAt": "2023-10-31T19:24:52.742Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>Okay, my crux is that the simplicity/Kolmogorov/Solomonoff prior is probably not very malign, assuming we could run it, and in general I find the prior not to be malign except for specific situations.</p>\n<p>This is basically because it relies on the IMO dubious assumption that the halting oracle can only be used once, and notably once we use the halting/Solomonoff oracle more than once, the Solomonoff oracle loses it's malign properties.</p>\n<p>More generally, if the Solomonoff Oracle is duplicatable, as modern AIs generally are, then there's a known solution to mitigate the malignancy of the Solomonoff prior: Duplicate it, and let multiple people run the Solomonoff inductor in parallel to increase the complexity of manipulation. The goal is essentially to remove the uniqueness of 1 Solomonoff inductor, and make an arbitrary number of such oracles to drive up the complexity of manipulation.</p>\n<p>So under a weak assumption, the malignancy of the Solomonoff prior goes away.\n&nbsp;\nThis is described well in the link below, and the important part is that we need either a use-once condition, or we need to assume uniqueness in some way. If we don't have either assumption holding, as is likely to be the case, then the Solomonoff/Kolmogorov prior isn't malign.</p>\n<p><a href=\"https://www.lesswrong.com/posts/f7qcAS4DMKsMoxTmK/the-solomonoff-prior-is-malign-it-s-not-a-big-deal#Comparison_\">https://www.lesswrong.com/posts/f7qcAS4DMKsMoxTmK/the-solomonoff-prior-is-malign-it-s-not-a-big-deal#Comparison_</a></p>\n<p>And that's if it's actually malign, which it might not be, at least in the large-data limit:</p>\n<p><a href=\"https://www.lesswrong.com/posts/Tr7tAyt5zZpdTwTQK/the-solomonoff-prior-is-malign#fDEmEHEx5EuET4FBF\">https://www.lesswrong.com/posts/Tr7tAyt5zZpdTwTQK/the-solomonoff-prior-is-malign#fDEmEHEx5EuET4FBF</a></p>\n<p>More specifically, it's this part of John Wentworth's comment:</p>\n<blockquote>\n<p>In Solomonoff Model, Sufficiently Large Data Rules Out Malignness</p>\n</blockquote>\n<blockquote>\n<p>There is a major outside-view reason to expect that the Solomonoff-is-malign argument must be doing something fishy: Solomonoff Induction (SI) comes with performance guarantees. In the limit of large data, SI performs as well as the best-predicting program, in every computably-generated world. The post mentions that:</p>\n</blockquote>\n<blockquote>\n<p>A simple application of the no free lunch theorem shows that there is no way of making predictions that is better than the Solomonoff prior across all possible distributions over all possible strings. Thus, agents that are influencing the Solomonoff prior cannot be good at predicting, and thus gain influence, in all possible worlds.</p>\n</blockquote>\n<blockquote>\n<p>... but in the large-data limit, SI's guarantees are stronger than just that. In the large-data limit, there is no computable way of making better predictions than the Solomonoff prior in any world. Thus, agents that are influencing the Solomonoff prior cannot gain long-term influence in any computable world; they have zero degrees of freedom to use for influence. It does not matter if they specialize in influencing worlds in which they have short strings; they still cannot use any degrees of freedom for influence without losing all their influence in the large-data limit.</p>\n</blockquote>\n<blockquote>\n<p>Takeaway of this argument: as long as we throw enough data at our Solomonoff inductor before asking it for any outputs, the malign agent problem must go away. (Though note that we never know exactly how much data that is; all we have is a big-O argument with an uncomputable constant.)</p>\n</blockquote>\n<p>As far as the actual practical question, there is a very important limitation on inner-misaligned agents by SGD, primarily because gradient hacking is very difficult to do, and is an underappreciated limitation on misalignment, since SGD has powerful tools to remove inner-misaligned circuits/TMs/Agents in the link below:</p>\n<p><a href=\"https://www.lesswrong.com/posts/w2TAEvME2yAG9MHeq/gradient-hacking-is-extremely-difficult\">https://www.lesswrong.com/posts/w2TAEvME2yAG9MHeq/gradient-hacking-is-extremely-difficult</a></p>\n", "parentCommentId": "gfLeDBpdGgiarkcm9", "user": {"username": "Sharmake"}}, {"_id": "uzpSdJkPjmzAyLmXy", "postedAt": "2023-12-14T03:15:33.967Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>If this turns out to be feasible, one solution would be to have people on-site (or make TSMC put hardware level controls in place) to randomly sample from the training data several times a day to verify outside data isn't involved in the training run.&nbsp;</p>", "parentCommentId": "MDG5rz5FG7WKyhxtw", "user": {"username": "MilesTS"}}, {"_id": "ajrzuafhuqwdR5d6N", "postedAt": "2023-12-14T03:21:03.095Z", "postId": "JYEAL8g7ArqGoTaX6", "htmlBody": "<p>This can be avoided with a treaty that requires full access given to international inspectors. This already happens with the IAEA and was set up even in the far greater tensions of the cold war. If someone like Iran tries to kick out the inspectors, everyone assumes they're trying to develop nuclear weapons and takes serious action (harsh sanctions, airstrikes, even the threat of war).&nbsp;<br><br>If governments think of this as an existential threat, they should agree to it for the same reasons they did with the IAEA. And while there's big incentives to defect (unless they have very high p(doom)), there is also the knowledge that kicking out inspectors will lead to potential war and their rivals defecting too.</p>", "parentCommentId": "ih5PiC6WeMNPMrPGT", "user": {"username": "MilesTS"}}]