[{"_id": "fSKDSxdgjSHmmzPwY", "postedAt": "2017-10-29T19:31:42.506Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>I think your conclusion is worth being a post on it's own, and would potentially get read by more people in a shorter format. </p>\n<p>It may also be the people that you'd want to read to the end wouldn't read a post as in depth as this.</p>\n", "parentCommentId": null, "user": {"username": "DavidNash"}}, {"_id": "fHm3kFB9ip72yqorQ", "postedAt": "2017-10-29T20:27:30.287Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Another reason to not have too much modesty within society is that it makes expert opinion very appealing to subvert. I wrote a bit about that <a href=\"https://www.lesserwrong.com/posts/2itvkQZMzobPeeeht/immodesty-deceit-and-motivated-reasoning\">here</a>. </p>\n<p>Note that I don't think that my views about the things that I believe subverted/unmoored would be necessarily correct, but that the first order of business would be to try and build a set of experts with better incentives.</p>\n", "parentCommentId": null, "user": {"username": "WillPearson"}}, {"_id": "cubpmCn7XJE5FQYEq", "postedAt": "2017-10-29T22:43:21.579Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Thank so much for the clear and eloquent post.  I think a lot of the issues related to lack of expertise and expert bias are stronger than I think you do, and I think it's both rare and not inordinately difficult to adjust for common biases such that in certain cases a less-informed individual can often beat the expert consensus (because few enough of the experts are doing this, for now). But it was useful to read this detailed and compelling explanation of your view. </p>\n<p>The following point seems essential, and I think underemphasized: </p>\n<blockquote>\n<p>Modesty can lead to double-counting, or even groupthink. Suppose in the original example Beatrice does what I suggest and revise their credences to be 0.6, but Adam doesn\u2019t. Now Charlie forms his own view (say 0.4 as well) and does the same procedure as Beatrice, so Charlie now holds a credence of 0.6 as well. The average should be lower: (0.8+0.4+0.4)/3, not (0.8+0.6+0.4)/3, but the results are distorted by using one-and-a-half helpings of Adam\u2019s credence. With larger cases one can imagine people wrongly deferring to hold consensus around a view they should think is implausible, and in general the nigh-intractable challenge from trying to infer cases of double counting from the patterns of \u2018all things considered\u2019 evidence.</p>\n</blockquote>\n<blockquote>\n<p>One can rectify this by distinguishing \u2018credence by my lights\u2019 versus \u2018credence all things considered\u2019. So one can say \u201cWell, by my lights the credence of P is 0.8, but my actual credence is 0.6, once I account for the views of my epistemic peers etc.\u201d Ironically, one\u2019s personal \u2018inside view\u2019 of the evidence is usually the most helpful credence to publicly report (as it helps others modestly aggregate), whilst ones all things considered modest view usually for private consumption.</p>\n</blockquote>\n<p>I rarely see any effort to distinguish between the two outside the rationalist/EA communities, which is one reason I think both over-modesty and overconfident backlash against it are common.</p>\n<p>My experience is that most reasonable, intelligent people I know have never explicitly thought of the distinction between the two types of credence. I think many of them have an intuition that something would be lost if they stated their &quot;all things considered&quot; credence only, even though it feels &quot;truer&quot; and &quot;more likely to be right,&quot; though they haven't formally articulated the problem.  And knowing that other people rarely make this distinction, it's hard for everyone know how to update based on others' views without double-counting, as you note. </p>\n<p>It seems like it's intuitive for people to state either their inside view, or their all-things-considered view, but not both. To me, stating &quot;both&quot;&gt;&quot;inside view only&quot;&gt;&quot;outside view only&quot;, but I worry that calls for more modest views tend to leak nuance and end up pushing for people to publicly state &quot;outside view only&quot; rather than &quot;both&quot;</p>\n<p>Also, I've generally heard people call the  &quot;credence by my lights&quot; and &quot;credence all things considered&quot; one's &quot;impressions&quot; and &quot;beliefs,&quot; respectively, which I prefer because they are less clunky. Just fyi. </p>\n<p>(views my own, not my employer's) </p>\n", "parentCommentId": null, "user": {"username": "ClaireZabel"}}, {"_id": "ta7if8XSFPXHxNNmv", "postedAt": "2017-10-29T23:05:42.404Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>I agree that this distinction is important and should be used more frequently. I also think good terminology is very important. Clunky terms are unlikely to be used. </p>\n<p>Something along the lines of &quot;impressions&quot; or &quot;seemings&quot; may be good for &quot;credence by my lights&quot; (cf optical illusions, where the way certain matter of facts seem or appear to you differs from your beliefs about them). Another possibility is &quot;private signal&quot;.</p>\n<p>I don't think inside vs outside view is a good terminology. E.g., I may have a credence by my lights about X partly because I believe that X falls in a certain reference class. Such reasoning is normally called &quot;outside-view&quot;-reasoning, yet it doesn't involve deference to epistemic peers. </p>\n", "parentCommentId": "cubpmCn7XJE5FQYEq", "user": {"username": "Stefan_Schubert"}}, {"_id": "3pA7kFxvSy2PbGyoE", "postedAt": "2017-10-29T23:36:40.190Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>I just thought I'd note that this appears similar to the 'herding' phenomenon in political polling, which reduces aggregate accuracy: <a href=\"http://www.aapor.org/Education-Resources/Election-Polling-Resources/Herding.aspx\">http://www.aapor.org/Education-Resources/Election-Polling-Resources/Herding.aspx</a></p>\n", "parentCommentId": "cubpmCn7XJE5FQYEq", "user": {"username": "Robert_Wiblin"}}, {"_id": "WzFy28F2AxgzDoYNp", "postedAt": "2017-10-29T23:51:52.068Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Hi Greg, thanks for this post, it was very good. I thought it would help future discussion to separate these claims, which leave your argument ambiguous:</p>\n<ol>\n<li>You should give equal weight to your own credences and those of epistemic peers on all propositions for which you and they are epistemic peers.</li>\n<li>Claims about the nature of the community of epistemic peers and our ability to reliably identify them. </li>\n</ol>\n<p>In places, you seem to identify modesty with 1, in others with the conjunction of 1 and a subset of claims in 2. 1 doesn't seem sufficient on its own for modesty, for if 1 is true but I have no epistemic peers or can't reliably identify them, then I should pay lots of attention to my own inside view of an issue. Similarly, if EAs have no epistemic peers or superiors, then they should ignore everyone else. This is compatible with conciliationism but seems immodest. The relevant claim in 2 seems to be that for most people, including EAs, with beliefs about practically important propostions, there are epistemic peers and superiors who can be reliably identified. </p>\n<p>This noted, I wonder how different the conjunction of 1 and 2 is to epistemic chauvinism. It seems to me that I could accept 1 and 2, but demote people from my epistemic peer group with respect to a proposition p if they disagree with me about p. If I have read all of the object-level arguments on p and someone else has as well and we disagree on p, then demotion seems appropriate at least in some cases. To give an example, I've read and thought less about vagueness less than lots of much cleverer philosophers who hold a view called supervaluationism, which I believe to be extremely implausible. I believe I can explain why they are wrong with the object-level arguments about vagueness. I received the evidence that they disagree. Very good, I reply, they are not my epistemic peers with respect to this question for object level reasons x, y, and z. (Note that my reasons for demoting them are the object-level reasons; they are not that I believe that supervaluationism is false. Generally, the fact that I believe p is usually not my reason to believe that p.) This is entirely compatible with the view that I should be modest with respect to my epistemic peers. </p>\n<p>In this spirit, I find Scott Sumner's quote deeply strange. If he thinks that &quot;there is no objective reason to favor my view over Krugman's&quot;, then he shouldn't believe his view over Krugman's (even though he (Sumner) does). If I were in Sumner's shoes after reasoning about p and reading the object level reasons about p, then I would EITHER become agnostic or demote krugman from my epistemic peer group.</p>\n", "parentCommentId": null, "user": null}, {"_id": "F5bgaqWAhJetLPsWa", "postedAt": "2017-10-30T00:12:22.075Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>This post is one of the best things I've read on this forum. I upvoted it, but didn't feel that was sufficient appreciation for you writing something this thorough in your spare time!</p>\n", "parentCommentId": null, "user": {"username": "Robert_Wiblin"}}, {"_id": "xFcTmD4FSSmvxt2fu", "postedAt": "2017-10-30T00:27:44.347Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>I thought I'd offer up more object-level examples to try to push against your view. AI risk is a case in which EAs disagree with the consensus among numerous AI researchers and other intelligent people.  In my view, a lot of the arguments I've heard from AI researchers have been very weak and haven't shifted my credence all that much. But modesty here seems to push me toward the consensus to a greater extent than the object-level reasons warrant. </p>\n<p>With respect to the question of AI risk, it seems to me that I should demote these people from my epistemic peer group because they disagree with me on the subject of AI risk. If you accept this, then its hard to see what difference there is between immodesty and modesty</p>\n", "parentCommentId": "WzFy28F2AxgzDoYNp", "user": null}, {"_id": "CXhP4DDgGCWR6B4Zq", "postedAt": "2017-10-30T00:45:33.586Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>As one data point, I did not have this association with &quot;impressions&quot; vs. &quot;beliefs&quot;, even though I do in fact distinguish between these two kinds of credences and often report both (usually with a long clunky explanation since I don't know of good terminology for it).</p>\n", "parentCommentId": "cubpmCn7XJE5FQYEq", "user": {"username": "rohinmshah"}}, {"_id": "3NNjDhNsmE3BD7QrB", "postedAt": "2017-10-30T00:52:40.490Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>I'm not sure where I picked it up, though I'm pretty sure it was somewhere in the rationalist community. </p>\n<p>E.g. from <a href=\"http://lesswrong.com/lw/b1r/what_epistemic_hygiene_norms_should_there_be/\">What epistemic hygiene norms should there be?</a>:</p>\n<blockquote>\n<p>Explicitly separate \u201cindividual impressions\u201d (impressions based only on evidence you've verified yourself) from \u201cbeliefs\u201d (which include evidence from others\u2019 impressions)</p>\n</blockquote>\n", "parentCommentId": "CXhP4DDgGCWR6B4Zq", "user": {"username": "ClaireZabel"}}, {"_id": "WQ9Z5o8b3taSwxGAp", "postedAt": "2017-10-30T01:00:23.246Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>The comments on <a href=\"http://www.overcomingbias.com/2008/04/naming-beliefs.html\">naming beliefs</a> by Hal Finney (2008) appears to be how the consensus around the impressions/beliefs distinction began to form (the commenters include such movers and shakers as Eliezer and Anna Salamon).</p>\n<p>Also, <a href=\"https://meteuphoric.wordpress.com/2017/09/23/impression-track-records/\">impression track records</a> by Katja (September 2017) recent blog post/article circulated in the rationalist community that revived the terminology.</p>\n", "parentCommentId": "CXhP4DDgGCWR6B4Zq", "user": {"username": "vipulnaik"}}, {"_id": "rppTrXxk4c25iwdBw", "postedAt": "2017-10-30T01:14:06.597Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>The dichotomy I see the most at MIRI is 'one's inside-view model' v. 'one's belief', where the latter tries to take into account things like model uncertainty, outside-view debiasing for addressing things like the planning fallacy, and deference to epistemic peers. Nate draws this distinction a lot.</p>\n", "parentCommentId": "cubpmCn7XJE5FQYEq", "user": {"username": "RobBensinger"}}, {"_id": "6nTZMqAivoKbkM7Ac", "postedAt": "2017-10-30T01:22:30.743Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>I guess you could make a trichotomy:</p>\n<p>a) Your inside-view model.</p>\n<p>b) Your all-things-considered private signal, where you've added outside-view reasoning, taken model uncertainty into account, etc.</p>\n<p>c) Your all-things-considered belief, which also takes the views of your epistemic peers into account.</p>\n", "parentCommentId": "rppTrXxk4c25iwdBw", "user": {"username": "Stefan_Schubert"}}, {"_id": "9BudHkbzimsDbQdye", "postedAt": "2017-10-30T03:14:14.912Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>I don't have much to contribute to the normative social epistemology questions raised here, since this is a huge debate within philosophy. People interested in a general summary might read the Philosophy Compass <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1747-9991.2009.00237.x/abstract\">review</a> or the SEP <a href=\"https://plato.stanford.edu/entries/epistemology-social/#PeeDis\">article</a>.</p>\n<p>But I did want to question the claim about the descriptive social epistemology of the EA movement which is made i.e. that:</p>\n<blockquote>\n<p>What occurs instead is agreement approaching fawning obeisance to a small set of people the community anoints as \u2018thought leaders\u2019, and so centralizing on one particular eccentric and overconfident view.</p>\n</blockquote>\n<p>I'm not sure this is useful as a general characterisation of the EA community, though certainly at times people are too confident, too deferential etc. What beliefs might be the beneficiaries of this fawning obeisance? There doesn't seem to me to be sufficient uncontroversial agreement about much (even utilitarianism has a number of prominent 'thought leaders' pushing against it saying that we ought to be opening ourselves up to alternatives). </p>\n<p>The general characterisation seems in tension with the common idea that EA is highly combative and confrontational (it would be strange though not impossible if we had a constant disagreement and attempted argumentative one-upmanship, combined with excessive deference to certain thought leaders). Instead what I see is occasional excessive deference to people respected within certain cliques, by members of those circles, but not 'centralization' on any one particular view. Perhaps all Greg has in mind is these kinds of cases where people defer too much to people they shouldn't (perhaps due to a lack of actual experts in EA rather than due to their own vice). But then it's not clear to me what the typical EA-rationalist who has not and probably shouldn't make a deep study of many-worlds, free will, or meta-ethics should do to avoid this problem.</p>\n", "parentCommentId": null, "user": {"username": "David_Moss"}}, {"_id": "mYyvzQPdAE3JG4tyC", "postedAt": "2017-10-30T07:39:53.307Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>The difference in many object level claims, like the probability that there will be an intelligence explosion and so on, is not very much between EAs and AI researchers. This survey demonstrated it: <a href=\"https://arxiv.org/abs/1705.08807\">https://arxiv.org/abs/1705.08807</a></p>\n<p>AI researchers are just more likely to have an attitude that anything less than ~10% likely to occur should be ignored, or existential risks are not orders of magnitude more important than other things, or similar kinds of judgement calls.</p>\n<p>The one major technical issue where EAs might be systematically different from AI researchers would be the validity of current research in addressing the problem.</p>\n", "parentCommentId": "xFcTmD4FSSmvxt2fu", "user": {"username": "kbog"}}, {"_id": "76k89CvcxqT8tWHNz", "postedAt": "2017-10-30T07:53:30.690Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<blockquote>\n<p>(even utilitarianism has a number of prominent 'thought leaders' pushing against it saying that we ought to be opening ourselves up to alternatives).</p>\n</blockquote>\n<p>Also, EA selects for utilitarians in the first place. So you can't say that we're being irrational just because we're disproportionately utilitarian.</p>\n", "parentCommentId": "9BudHkbzimsDbQdye", "user": {"username": "kbog"}}, {"_id": "ZBGDtR4XhA5zJKmye", "postedAt": "2017-10-30T09:23:34.615Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Is there any data on how likely EAs think that explosive progress after HLMI will happen? I would have thought it more than 10%? </p>\n<p>I would also have expected more debate about explosive progress, more than just the recent Hanson-Yudkowski flair up, if there was as much doubt in the community as that survey suggests.</p>\n", "parentCommentId": "mYyvzQPdAE3JG4tyC", "user": {"username": "WillPearson"}}, {"_id": "pS2k9y6Mpsx6rXtQD", "postedAt": "2017-10-30T16:20:52.493Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Gregory, thanks for writing this up. Your writing style is charming and I really enjoy reading the many deft turns of phrase. </p>\n<p>Moving on to the substance, I think I share JH's worries. What seems missing from your account is why people have the credences they have. Wouldn't it be easiest just to go and assess the object level reasons people have for their credences? For instance, with your Beatrice and Adam example, one (better?) way to make progress on finding out whether it's an oak or not is ask them for their reasons, rather than ask them to state their credences and take those on trust. If Beatrice says &quot;I am tree expert but I've left my glasses at home so can't see the leaves&quot; (or something) whereas Adam gives a terrible explanation (&quot;I decided every fifth tree I see must be an oak tree&quot;), that would tell us quite a lot. </p>\n<p>Perhaps, we should defer to others either when we don't know what their reasons are but need to make a decision quickly, or we think they have the same access to object levels reasons as we do (potential example: two philosophers who've read everything but still disagree).</p>\n", "parentCommentId": "WzFy28F2AxgzDoYNp", "user": {"username": "MichaelPlant"}}, {"_id": "DpJKupxsMeYos2Twp", "postedAt": "2017-10-30T18:46:48.490Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Thanks for your generous reply, Claire. I agree the 'double counting' issue remains challenging, although my thought was that most people, at least in the wider world, are currently pretty immodest, the downsides are not too large in what I take to be common applications where you are trying to weigh up large groups of people/experts. I agree there's a risk of degrading norms if people mistakenly switch to offering 'outside view' credences publicly.</p>\n<p>I regret I hadn't seen the 'impressions' versus 'beliefs' distinction being used before. 'Impression' works very well for 'credence by my lights' (I had toyed with using the term 'image'), but I'm not sure 'belief' translates quite so well for those who haven't seen the way the term is used in the rationalist community. I guess this might just be hard, as there does seem to be a good word (or two) I can find which captures modesty (&quot;being modest, my credence is X&quot;, &quot;modestly, I think it's Y&quot;, maybe?)</p>\n", "parentCommentId": "cubpmCn7XJE5FQYEq", "user": {"username": "Gregory_Lewis"}}, {"_id": "trWqXWkByqL7c5y4K", "postedAt": "2017-10-30T19:14:21.382Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Hello John (and Michael - never quite how to manage these sorts of 'two to one' replies)</p>\n<p>I would reject epistemic chauvinism. In the cases where you disagree on P with your epistemic peer, and you take some set of object reasons x, y, and z to support P, the right approach is to downgrade your confidence in the strength of these reasons rather than demote them from epistemic peerhood. I'd want to support that using some set of considerations about [2]: among others, the reference class where you demote people from peerhood (or superiority) on disagreement goes predictably much worse in the 'truly modest' one where you downgrade your confidence in the reasons that lead you to disagree (consider a typical crackpot who thinks the real numbers have the same cardinality as the natural for whatever reason, and then infers from disagreement mathematicians are all fools)</p>\n<p>For the supervaluation case, I don't know whether it is the majority view on vagueness, but pretend it was a consensus. I'd say the right thing in such a situation is to be a supervaluationist yourself, even if it appears to you it is false. Indicting apparent peers/superiors for object level disagreement involves retrenchment, and so seems to go poorly. </p>\n<p>In the AI case, I'd say you'd have to weigh up (which is tricky) degrees of expertise re. AI. I don't see it as a cost for my view to update towards the more sceptical AI researchers even if you don't think the object level reasons warrant it, as in plausible reference classes the strategy of going with the experts beats going with the non-expert opinion.</p>\n<p>In essence, the challenge modesty would make is, &quot;Why do you back yourself to have the right grasp on the object level reasons?&quot; Returning to a supervaluation consensus, it seems one needs to offer a story as to why the object level reasons that convincingly refute the view are not appreciated by the philosophers who specialise in the subject. It could be the case they're all going systemically wrong (and so you should demote them), but it seems more likely that you have mistaken the object level balance of reason. Using the former as an assumption looks overconfident.</p>\n<p>What I take Sumner to be saying is to take the agnosticism you suggest he should, maybe something like this:</p>\n<blockquote>\n<p>My impression is that my theory is right, but I don't believe its more likely my impression is more likely to be right than Paul Krugman's (or others). So if you put a gun to my head and I had to give my best guess on economics, I would take an intermediate view, and not follow the theory I espouse. In my day to day work, though, I use this impression to argue in support of this view, so it can contribute to our mutual knowledge.</p>\n</blockquote>\n<p>Of course, maybe you can investigate the object level reasons, per Michael's example. In the Adam and Beatrice case, Oliver could start talking to them about the reasons, and maybe find one of them isn't an epistemic peer to the other (or to him). Yet in cases where Oliver forms his own view about the object level considerations, he should still be modest across the impressions of Adam, Beatrice, and himself, for parallel reasons to the original case where he was an outsider (suppose we imagine Penelope who is an outsider to <em>this</em> conversation, etc.)</p>\n", "parentCommentId": "WzFy28F2AxgzDoYNp", "user": {"username": "Gregory_Lewis"}}, {"_id": "qexstBf8aGZuZEPZK", "postedAt": "2017-10-30T22:20:07.801Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Concur that the distinction between &quot;credence by lights&quot; and &quot;credence all things considered&quot; seems very helpful, possibly deserving of it's own post.</p>\n", "parentCommentId": "cubpmCn7XJE5FQYEq", "user": {"username": "Michael_PJ"}}, {"_id": "qoECpzWL5EPqf5SXv", "postedAt": "2017-10-30T22:36:58.050Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Great post!</p>\n<p>I think the question of &quot;how do we make epistemic progress&quot; needs a bit more attention.</p>\n<p>Continuing the analogy with the EMH (which I like), I think the usual answer is that there <em>are</em> some $20 bills on the floor, and that some individuals are either specially placed to see that, or have a higher risk tolerance and so are willing to take the risk that it's a prank.</p>\n<p>This suggests similar cases in epistemics: some people really are better situated (the experts), but perhaps we should also consider the class of &quot;epistemic risk takers&quot;, who hold riskier beliefs in the attempt to get &quot;ahead of the curve&quot;. However, as with startup founders, we should take such people with more than a pinch of salt. We may want to &quot;fund&quot; the ecosystem as a whole, because on average the one successful epistemic entrepreneur pays for the rest, but any individual is still likely to be worse than the consensus.</p>\n<p>So that suggests that we should encourage people to think riskily, but usually discount their &quot;risky&quot; beliefs when making practical decisions until they have proven themselves. And this actually seems to reflect our behaviour: people <em>are</em> much more epistemically modest when the money is on the table. Being more explicit about which beliefs are &quot;speculative&quot; seems like it would be an improvement, though.</p>\n<p>Finally, we also have to ask how people become experts. Again, in the economic analogy, people end up well situated to start businesses often through luck, sometimes through canny positioning, and sometimes through irrational pursuit of an idea. Similarly, to become an expert in X one has to invest a lot of time and effort, but we may want people to speculate in this domain too, and become experts in unlikely things so that we can get good credences on topics that may, with low probability, turn out to be important.</p>\n<p>(Meta: I was confused about whether to comment on LW2 or here. Cross-posting comments seems silly... or is it?)</p>\n", "parentCommentId": null, "user": {"username": "Michael_PJ"}}, {"_id": "99PZp65NuKtBKwSE6", "postedAt": "2017-10-31T04:04:06.593Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>This was a really good read! In addition to being super well-timed.</p>\n<p>I don't think there's a disagreement here about ideal in-principle reasoning. I\u2019m guessing that the disagreement is about several different points:</p>\n<ul>\n<li><p>In reality, how generally difficult is it to spot important institutions and authorities failing in large ways? Where we might ask subquestions for particular kinds of groups; e.g., maybe you and the anti-modest will turn out to agree about how dysfunctional US national politics is on average, while disagreeing about how dysfunctional academia is on average in the US.</p>\n</li>\n<li><p>In reality, how generally difficult is it to evaluate your own level of object-level accuracy in some domain, the strength of object-level considerations in that domain, your general competence or rationality or meta-rationality, etc.? To what extent should we update strongly on various kinds of data about our reasoning ability, vs. distrusting the data source and penalizing the evidence? (Or looking for ways to not have to gather or analyze data like that at all, e.g., prioritizing finding epistemic norms or policies that work relatively OK without such data.)</p>\n</li>\n<li><p>How strong are various biases, either in general or in our environs? It sounds like you think that arrogance, overconfidence, and excess reliance on inside-view arguments are much bigger problems for core EAs than underconfidence or neglect of inside-view arguments, while Eliezer thinks the opposite.</p>\n</li>\n<li><p>What are the most important and useful debiasing interventions? It sounds like you think these mostly look like attempts to reduce overconfidence in inside views, self-aggrandizing biases, and the like, while Eliezer thinks that it's too easy to overcorrect if you organize your epistemology around that goal. I think the anti-modesty view here is that we should mostly address those biases (and other biases) through more local interventions that are sensitive to the individual's state and situation, rather than through rules akin to &quot;be less confident&quot; or &quot;be more confident&quot;.</p>\n</li>\n<li><p>What's the track record for more modesty-like views versus less modesty-like views overall?</p>\n</li>\n<li><p>What's the track record for critics of modesty in particular? I would say that Eliezer and his social circle have a really strong epistemic track record, and that this is good evidence that modesty is a bad idea; but I gather you want to use that track record as Exhibit A in the case for modesty being a good idea. So I assume it would help to discuss the object-level disagreements underlying these diverging generalizations.</p>\n</li>\n</ul>\n<p>Does that match your sense of the disagreement?</p>\n", "parentCommentId": null, "user": {"username": "RobBensinger"}}, {"_id": "hxygnK4mzPPjwzTRK", "postedAt": "2017-10-31T17:51:16.797Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Thanks for your helpful reply. I think your bullet points do track the main sources of disagreement, but I venture an even crisper summary:</p>\n<p>I think the Eliezer-style 'immodest' view comprises two key claims:</p>\n<p>1) There are a reasonably large number of cases that due to inadequate equiilbria or similar that who we might take to be expert classes are in fact going to be sufficiently poorly optimised for the truth that the views a reasonable rationalist or similar could be expected to do better.</p>\n<p>2) We can reliably identify these cases.</p>\n<p>If they're both true we can license ourselves to 'pick fights' where we make confident bets against expert consensus (or lack thereof) in the knowledge we are more likely than not to be right. If not, then it seems modesty is the better approach: it might be worth acting 'as if' our contra-expert impression is right and doing further work (because we might discover something important), but nonetheless defer to the expert consensus.</p>\n<p>It seems the best vindication of the immodesty view as Eliezer defends is a track record of such cases on his behalf or the wider rationalist community. You correctly anticipate I would definitely include the track record here as highly adverse. For two reasons:</p>\n<p>First, when domain experts look at the 'answer according to the rationalist community re. X', they're usually very unimpressed, even if they're sympathetic to the view themselves. I'm pretty Atheist, but I find the 'answer' to the theism question per LW or similar woefully rudimentary compared to state of the art discussion in the field. I see similar experts on animal consciousness, quantum mechanics, free will, and so on similarly be deeply unimpressed with the sophistication of argument offered.</p>\n<p>Unfortunately, many of these questions tend to be the sort where a convincing adjudication is far off (i.e. it seems unlikely to discover convincing proof of physicalism any time soon). So what we observe is both compatible with 'the rationalist community is right and this field is diseased (and so gets it wrong)' and 'the rationalist community is greatly over confident and the field ia on the right track'. That said, I take the number of fields which the rationalist community takes to be sufficiently diseased that it takes itself to do better as implausible on priors.</p>\n<p>The best thing would be a clear track record to judge - single cases, either way, don't give much to go one, as neither modesty nor immodesty would claim they should expect to win every single time. I see the rationalist community having one big win (re. AI), yet little else. That Eliezer's book offers two pretty weak examples (e.g. BoJ, where he got the argument from a recognised authority, and an n=1 medical intervention), and reports one case against (e.g. a big bet of Taubes) doesn't lead me to upgrade my pretty autumnal view of the track record. </p>\n", "parentCommentId": "99PZp65NuKtBKwSE6", "user": {"username": "Gregory_Lewis"}}, {"_id": "s2EDvx93h2gthyTjQ", "postedAt": "2017-10-31T18:41:17.893Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<blockquote><p>I would say that Eliezer and his social circle have a really strong epistemic track record, and that this is good evidence that modesty is a bad idea; but I gather you want to use that track record as Exhibit A in the case for modesty being a good idea.</p></blockquote><p>Really? My sense is that the opposite is the case. Eliezer himself acknowledges that he has an \"amazing bet-losing capability\" and my sense is that he tends to bet against scientific consensus (while Caplan, who almost always takes the consensus view, <a href=\"http://econlog.econlib.org/archives/2016/05/why_bryan_capla_1.html\">has won</a> all his bets). Carl Shulman <a href=\"https://www.lesserwrong.com/posts/QSHwKqyY4GAXKi9tX/a-personal-history-of-involvement-with-effective-altruism#vi4D6cgPxEve9ByAY\">notes</a> that Eliezer's approach \"has lead [him] astray repeatedly, but I haven't seen as many successes.\"</p>", "parentCommentId": "99PZp65NuKtBKwSE6", "user": {"username": "Pablo_Stafforini"}}, {"_id": "xjkREmtvoHPsrwo2u", "postedAt": "2017-10-31T19:55:26.311Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<blockquote>\n<p>and Carl Shulman notes that his approach &quot;has lead [him] astray repeatedly, but I haven't seen as many successes.&quot;</p>\n</blockquote>\n<p>That quote may not convey my view, so I'll add to this. I think Eliezer has had a number of striking successes, but in that comment I was saying that it seemed to me he was overshooting more than undershooting with the base rate for dysfunctionality in institutions/fields, and that he should update accordingly and check more carefully for the good reasons that institutional practice or popular academic views often (but far from always) indicate. That doesn't mean one can't look closely and form much better estimates of the likelihood of good invisible reasons, or that the base rate of dysfunction is anywhere near zero. E.g. I think he has discharged the burden of due diligence wrt MWI.</p>\n<p>If many physicists say X, and many others say Y and Z which seem in conflict with X, then at a high rate there will be some good arguments for X, Y, and Z. If you first see good arguments for X, you should check to see what physicists who buy Y and Z are saying, and whether they (and physicists who buy X) say they have knowledge that you don't understand. </p>\n<p>In the case of MWI, the physicists say they don't have key obscure missing arguments (they are public and not esoteric), and that you can sort interpretations into ones that accept the unobserved parts of the wave function in QM as real (MWI, etc), ones that add new physics to pick out part of the wavefunction to be our world, and ones like shut-up-and-calculate that amount to 'don't talk about whether parts of the wave function we don't see are real.'</p>\n<p>Physicists working on quantum foundations are mostly mutually aware of one another's arguments, and you can read or listen to them for their explanations of why they respond differently to that evidence, and look to the general success of those habits of mind. E.g. the past success of <a href=\"https://philpapers.org/surveys/linear_most_with.pl?A=main:Science:scientific%20realism\">scientific realism</a> and Copernican moves: distant lands on Earth that were previously unseen by particular communities turned out to be real, other Sun-like stars and planets were found, biological evolution, etc. Finding out that many of the interpretations amount to MWI under another name, or just refusing to answer the question of whether MWI is true, reduces the level of disagreement to be explained, as does the finding that realist/multiverse interpretations have tended to gain ground with time and to do better among among those who engage with quantum foundations and cosmology. </p>\n<p>In terms of modesty, I would say that generally 'trying to answer the question about external reality' is a good epistemic marker for questions about external reality, as is Copernicanism/not giving humans a special place in physics or drastically penalizing theories on which the world is big/human nature looks different (consistently with past evidence). Regarding new physics for objective collapse, I would also note the failure to show it experimentally and the general opposition to it. That seems sufficient to favor the realist side of the debate among physicists.</p>\n<p>In contrast, I hadn't seen anything like such due diligence regarding nutrition, or precedent in common law.</p>\n<p>Regarding the OP thesis, you could summarize my stance as that assigning 'epistemic peer' or 'epistemic superior/inferior' status in the context of some question of fact requires a lot of information and understanding when we are not assumed to already have reliable fine-grained knowledge of epistemic status. That often involves descending into the object-level: e.g. if the class of 'scientific realist arguments' has a good track record, then you will need to learn enough about a given question and the debate on it to know if that systemic factor is actually at play in the debate before you can know whether to apply that track record in assessing epistemic status.</p>\n", "parentCommentId": "s2EDvx93h2gthyTjQ", "user": {"username": "CarlShulman"}}, {"_id": "dt4xpT7gspbTEyreR", "postedAt": "2017-10-31T20:59:50.130Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<blockquote>\n<p>In that comment I was saying that it seemed to me he was overshooting more than undershooting with the base rate for dysfunctionality in institutions/fields, and that he should update accordingly and check more carefully for the good reasons that institutional practice or popular academic views often (but far from always) indicate. That doesn't mean one can't look closely and form much better estimates of the likelihood of good invisible reasons, or that the base rate of dysfunction is anywhere near zero.</p>\n</blockquote>\n<p>I offered that quote to cast doubt on Rob's assertion that Eliezer has &quot;a really strong epistemic track record, and that this is good evidence that modesty is a bad idea.&quot;  I didn't mean to deny that Eliezer had some successes, or that one shouldn't &quot;look closely and form much better estimates of the likelihood of good invisible reasons&quot; or that &quot;the base rate of dysfunction is anywhere near zero&quot;, and I didn't offer the quote to dispute those claims.</p>\n<p>Readers can read the <a href=\"https://www.lesserwrong.com/posts/QSHwKqyY4GAXKi9tX/a-personal-history-of-involvement-with-effective-altruism#vi4D6cgPxEve9ByAY\">original comment</a> and judge for themselves whether the quote was in fact pulled out of context.</p>\n", "parentCommentId": "xjkREmtvoHPsrwo2u", "user": {"username": "Pablo_Stafforini"}}, {"_id": "TqXgYXYuoiAQpGRzi", "postedAt": "2017-10-31T21:18:57.998Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Please take my comment as explaining my own views, lest they be misunderstood, not condemning your citation of me.</p>\n", "parentCommentId": "dt4xpT7gspbTEyreR", "user": {"username": "CarlShulman"}}, {"_id": "gtMgsbqY48x4cujCL", "postedAt": "2017-10-31T21:28:24.957Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Okay, thank you for the clarification.</p>\n<p>[In the original version, your comment said that the quote was pulled out of context, hence my interpretation.]</p>\n", "parentCommentId": "TqXgYXYuoiAQpGRzi", "user": {"username": "Pablo_Stafforini"}}, {"_id": "zstpKX9HWozg5aj9F", "postedAt": "2017-11-01T17:19:39.694Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Hi Greg, \nSo, your view is that it's ok to demote people from my peer group when I not only disagree with them about p but also when I have an explanation of why they would be biased that doesn't apply to me. And on your view their verdict on p could never be evidence of their bias. This last seems wrong in many cases. </p>\n<p>Consider some obvious truth P (e.g. if a, then a; if a or b, then a and b can't both not be true; it's wrong to torture people for fun etc.). Myself and some other equally intelligent person have been thinking about P for an equal amount of time. I learn that she believes that not-P. It seems entirely appropriate for me to demote them in this case. If you deny this, suppose now we are deciding on some proposition Q and I knew only that they had got P wrong. As you would agree, their past performance (on P) is pro tanto reason to demote with respect to Q. How can it then not also be pro tanto reason to demote with respect to P? [aside: the second example of an obvious truth I gave is denied by supervaluationists]. In short, how could epistemic peerhood not be in part determined by performance on the object level reasons?</p>\n<p>In some of these cases, it also seems that in order to justifiably demote, one doesn't need to offer an account of why the other party is biased that is independent of the object-level reasons. </p>\n<p>A separate point, it seems like today and historically there are and have been pockets of severe epistemic error. e.g. in the 19th century, almost all of the world's most intelligent philosophers thought that idealism is true; a large chunk of political philosophers believe that public reason is true; I'm sure there are lots of examples outside philosophy. </p>\n<p>In this context, selective epistemic exceptionalism seems appropriate for a community that has taken lots of steps to debias. There's still very good reason to be aware of what the rest of the epistemic community thinks and why they think it, and this is a (weaker) form of modesty. </p>\n", "parentCommentId": "trWqXWkByqL7c5y4K", "user": null}, {"_id": "gC2DtoLQKTbec2odK", "postedAt": "2017-11-01T21:27:56.091Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Thanks for drawing our attention to that early Overcoming Bias post. But please note that it was written by Hal Finney, not Robin Hanson. It took me a few minutes to realize this, so it seemed worth highlighting lest others fail to appreciate it.</p>\n<p>Incidentally, I've been re-reading Finney's posts over the past couple of days and have been very impressed. What a shame that such a fine thinker is no longer with us.</p>\n<p>ETA: Though one hopes this is <a href=\"https://www.alcor.org/blog/hal-finney-becomes-alcors-128th-patient/\">temporary</a>.</p>\n", "parentCommentId": "WQ9Z5o8b3taSwxGAp", "user": {"username": "Pablo_Stafforini"}}, {"_id": "A9r2QkseqvukxPcoi", "postedAt": "2017-11-01T23:09:37.059Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Thanks for writing this\u2013as basically everyone else has said, it's really beautifully written.</p>\n<p>I share others' (cf. Claire Zabel's comment) gratitude for the distinction you make between publicly reporting one's inside view while privately acting on one's outside view. This seems to raise a serious question about what is public and what is private. For instance, donation decisions may seem like a very private decision (unless declared publicly), but as an organization starts to grow, people will interpret that as a signal of people's views, which can lead to double-counting. I think this is actually something worth worrying about: while I think the most vocal EAs lean too far toward immodesty in expression of attitudes, EAs writ large do seem to act to a serious degree based on others' actions (at least in animal advocacy). The methodological individualism of economics and other fields that guide EAs may cause people to systematically overestimate how private certain decisions are.</p>\n<p>Another worry I have is that people may systematically confuse expert consensus as having a wider scope for the following reason: experts who study Y may pronounce an opinion not on Y but on 'Y given X' even though they have not studied X. Economists, for instance, will often make explicit or just-shy-of-explicit claims about whether a policy is good or not, but the goodness of policies typically depends on empirical facts that most economists are equipped to consider and normative claims that economists may not be equipped to consider. It strikes me that we need to have a fine scalpel to see that we should accept economists' consensus on the direction and magnitude of policies' effects but look to political philosophers or ethicists for judgments of those effects.</p>\n", "parentCommentId": null, "user": {"username": "zdgroff"}}, {"_id": "4EvMdEEafekDogRYW", "postedAt": "2017-11-02T21:04:05.534Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<blockquote>\n<p>Unfortunately, many of these questions tend to be the sort where a convincing adjudication is far off (i.e. it seems unlikely to discover convincing proof of physicalism any time soon).</p>\n</blockquote>\n<p>I think a convincing object-level argument could be given; you could potentially show on object-level grounds why the specific arguments or conclusions of various rationalists are off-base, thereby at least settling the issue (or certain sub-issues) to the satisfaction of people who take the relevant kinds of inside-view arguments sufficiently seriously in the first place. I'd be particularly interested to hear reasons you (or experts you defer to) reject the relevant arguments against gods, philosophical zombies, or objective collapse / non-realism views in QM.</p>\n<p>If you mean that a convincing expert-consensus argument is likely to be far off, though, then I agree about that. As a start, experts' views and toolkits in general can be slow to change, particularly in areas like philosophy.</p>\n<p>I assume one part of the model Eliezer is working with here is that it can take many decades for new conceptual discoveries to come to be widely understood, accepted, and used in a given field, and even longer for these ideas to spill over into other fields. E.g., some but not all philosophers have a deep understanding of Shannon, Solomonoff, and Jaynes' accounts of inductive inference, even though many of the key insights have been around for over fifty years at this point. When ideas spread slowly, consensus across all fields won't instantly snap into a new state that's maximally consistent with all of the world's newest developments, and there can be low-hanging fruit for the philosophers who do help import those ideas into old discussions.</p>\n<p>This is why Eliezer doesn't claim uniqueness for his arguments in philosophy; e.g., Gary Drescher used the same methodology and background ideas to arrive largely at the same conclusions largely independently, as far as I know.</p>\n<p>I'd consider the big advances in decision theory from Wei Dai and Eliezer to be a key example of this, and another good example of independent discovery of similar ideas by people working with similar methodologies and importing similar ideas into a relatively old and entrenched field. (Though Wei Dai and Eliezer were actively talking to each and sharing large numbers of ideas, so the independence is much weaker.)</p>\n<p>You can find most of the relevant component ideas circulating before that, too; but they were scattered across multiple fields in a way that made them less likely to get spontaneously combined by specialists busy hashing out the standard sub-sub-arguments within old paradigms.</p>\n", "parentCommentId": "hxygnK4mzPPjwzTRK", "user": {"username": "RobBensinger"}}, {"_id": "4XzLk4BqoAfFYK9bC", "postedAt": "2017-11-02T23:59:46.897Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>I agree such an object level demonstration would be good evidence (although of course one-sided, for reasons Pablo ably articulates elsewhere). I regret I can't provide this. On many of these topics (QM, p-zombies) I don't pretend any great knowledge; for others (e.g. Theism), I can't exactly find the 'rationalist case for Atheism' crisply presented. </p>\n<p>I am naturally hesitant to infer from the (inarguable) point that diffusion of knowledge and ideas within and across fields takes time that he best explanation for disagreement is that rationalists are just ahead of the curve. I enjoyed the small parts of Drescher I read, but I assume many reasonable philosophers are aware of his work and yet are not persuaded. Many things touted in philosophy (and elsewhere) as paradigm shifting insights transpire to be misguided, and betting on some based on your personal assent on the object level looks unlikely to go well.</p>\n<p>I consider the decision theory work a case-in-point. The view that the F- U- T- DT is this great advance on decision theoretic state of the art is a view that is very tightly circumscribed to the rationalist community itself. Of course, many decision theorists are simply ignorant of it given it is expounded outside the academic press. Yet others are not: there were academic decision theorists who attend some MIRI workshops, others who have been shown versions (via Chalmers, I understand), and a few who have looked at MIRI's stuff on Arxiv and similar. Yet the prevailing view of these seems to be at best lukewarm, and at worst scathing. </p>\n<p>This seems challenging to reconcile with a model of rationalists just getting to the great insights early before everyone else catches up. It could be the decision theorist community is so diseased so they cannot appreciate the technical breakthrough MIRI-style decision theory promises. Yet I find the alternative hypothesis where it is the rationalist community which is diseased and diving down a decision theory dead end without the benefit of much interaction with decision theory experts to correct them somewhat more compelling.</p>\n", "parentCommentId": "4EvMdEEafekDogRYW", "user": {"username": "Gregory_Lewis"}}, {"_id": "Ku7jan23tjN2L2dRT", "postedAt": "2017-11-03T00:19:01.808Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Minor point: epistemic peer judgements are independent of whether you disagree with them or not. I'm happy to indict people who are epistemically unvirtuous even if they happen to agree with me. </p>\n<p>I generally think one should not use object level disagreement to judge peerhood, given the risk of entrenchment (i.e. everyone else thinks I'm wrong, so I conclude everyone else is wrong and an idiot). </p>\n<p>For 'obvious truths' like P, there's usually a lot of tacit peer agreement in background knowledge. So the disagreement with <em>you and these other people</em> provides some evidence for demotion, rather than disagreeing with you alone. I find it hard to disentangle intuitions where one removes this rider, and in these cases I'm not so sure about whether steadfastness + demotion is the appropriate response. Demoting supervaluationaists as peers re. supervaluationism because they disagree with you about it, for example, seems a bad idea.</p>\n<p>In any case, almost by definition it would be extraordinarily rare people we think prima facie are epistemic peers disagree on something sufficiently obvious. In real world cases where its some contentious topic where reasonable people disagree, one should not demote people based on their disagreement with you (or, perhaps, in these cases the evidence for demotion is sufficiently trivial that it is heuristically better ignored). </p>\n<p>Modest accounts shouldn't be surprised by expert error. Yet being able to determine these instances ex post gives little steer as to what to do ex ante. Random renegade schools of thought assuredly have an even poorer track record. If it was the case the EA/rationalist community had a good track record of out performing expert classes in their field, that would be a good reason for epistemic exceptionalism. Yet I don't see it. </p>\n", "parentCommentId": "zstpKX9HWozg5aj9F", "user": {"username": "Gregory_Lewis"}}, {"_id": "h9owPBwnxfaPvRQHT", "postedAt": "2017-11-04T00:38:07.877Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>To be clear, I'm not saying that the story I told above (&quot;here are some cool ideas that I claim haven't sufficiently saturated the philosophy community to cause all the low-hanging fruit to get grabbed, or to produce fieldwide knowledge and acceptance in the cases where it has been grabbed&quot;) should persuade arbitrary readers that people like Eliezer or Gary Drescher are on the right track; plenty of false turns and wrong solutions can also claim to be importing neglected ideas, or combining ideas in neglected ways. I'm just gesturing at one reason why I think it's possible at all to reach confident correct beliefs about lots of controversial claims in philosophy, in spite of the fact that philosophy is a large and competitive field whose nominal purpose is to answer these kinds of questions.</p>\n<p>I'm also implicitly making a claim about there being similarities between many of the domains you're pointing to that help make it not just a coincidence that one (relatively) new methodology and set of ideas can put you ahead of the curve on multiple issues simultaneously (plus produce multiple discovery and convergence). A framework that's unusually useful for answering questions related to naturalism, determinism, and reflective reasoning can simultaneously have implications for how we should (and shouldn't) be thinking about experience, agency, volition, decision theory, and AI, among other topics. To some extent, all of these cases can be thought of as applications of a particular naturalist/reductionist toolkit (containing concepts and formalisms that aren't widely known among philosophers who endorse naturalism) to new domains.</p>\n<p>I'm curious what criticisms you've heard of MIRI's work on decision theory. Is there anything relevant you can link to?</p>\n", "parentCommentId": "4XzLk4BqoAfFYK9bC", "user": {"username": "RobBensinger"}}, {"_id": "5E4fAAqRqwNfv9vSL", "postedAt": "2017-11-04T08:22:29.733Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>I don't think the account of the relative novelty of the 'LW approach' to philosophy makes a good fit for the available facts; &quot;relatively&quot; new is, I suggest, a pretty relative term.</p>\n<p>You can find similar reduction-esque sensibilities among the logicial positivists around a century ago, and a very similar approach from Quine about half a century ago. In the case of the logical positivists, they enjoyed a heyday amongst the philosophical community, but gradually fell from favour due to shortcomings other philosophers identified; I suggest Quine is a sufficiently 'big name' in philosophy that his approach was at least widely appreciated by the relevant academic communities. </p>\n<p>This is challenging to reconcile with an account of &quot;Rationality's philosophical framework allows one to get to confidently get to the right answer across a range of hard philosophical problems, and the lack of assent of domain experts is best explained by not being aware of it&quot;. Closely analogous approaches have been tried a very long time ago, and haven't been found extraordinarily persuasive (even if we subset to naturalists). It doesn't help that when the 'LW-answer' is expounded (e.g. in the sequences) the argument offered isn't particularly sophisticated (and often turns out to be <a href=\"http://lesswrong.com/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/\">recapitulating</a> extant literature), nor does it usually deign to address objections raised by dissenting camps.</p>\n<p>I suggest a better fit for this data is the rationality approach looks particularly persuasive to people without subject matter expertise.</p>\n<p>Re. decision theory. Beyond the general social epistemiological steers (i.e. the absence of good decision theorists raving about the breakthrough represented by MIRI style decision theory, despite many of them having come into contact with this work one way or another), remarks I've heard often target 'technical quality': Chalmers <a href=\"https://www.lesserwrong.com/posts/nQ3uGYeD7wdinPrKz/david-chalmers-on-lesswrong-and-the-rationalist-community\">noted</a> in a past AMA disappointment this theory had not been made rigorous (maybe things have changed since), and I know one decision theorist's view is that the work also isn't rigorous and a bit sloppy (on Carl's advice, I'm trying to contact more). Not being a decision theorist myself, I haven't delved into the object level considerations.  </p>\n", "parentCommentId": "h9owPBwnxfaPvRQHT", "user": {"username": "Gregory_Lewis"}}, {"_id": "AF74qMcj5FhE2qmjx", "postedAt": "2017-11-04T16:53:28.954Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>The &quot;<a href=\"https://intelligence.org/2017/03/18/new-paper-cheating-death-in-damascus/\">Cheating Death in Damascus</a>&quot; and &quot;<a href=\"https://intelligence.org/2017/10/22/fdt/\">Functional Decision Theory</a>&quot; papers came out in March and October, so I recommend sharing those, possibly along with the &quot;<a href=\"https://intelligence.org/2017/04/07/decisions-are-for-making-bad-outcomes-inconsistent/\">Decisions Are For Making Bad Outcomes Inconsistent</a>&quot; conversation notes. I think these are much better introductions than e.g. Eliezer's old &quot;Timeless Decision Theory&quot; paper.</p>\n<p>Quineans and logical positivists have some vague attitudes in common with people like Drescher, but the analogy seems loose to me. If you want to ask why other philosophers didn't grab all the low-hanging fruit in areas like decision theory or persuade all their peers in areas like philosophy of mind (which is an interesting set of questions from where I'm standing, and one I'd like to see examined more too), I think a more relevant group to look at will be technically minded philosophers who think in terms of Bayesian epistemology (and information-theoretic models of evidence, etc.) and software analogies. In particular, analogies that are more detailed than just &quot;the mind is like software&quot;, though computationalism is an important start. A more specific question might be: &quot;Why didn't E.T. Jaynes' work sweep the philosophical community?&quot;</p>\n", "parentCommentId": "5E4fAAqRqwNfv9vSL", "user": {"username": "RobBensinger"}}, {"_id": "gwYPkNRYGg84LjEQB", "postedAt": "2017-11-09T00:18:59.199Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>To support a claim that this applies in &quot;virtually all&quot; cases, I'd want to see more engagement with pragmatic problems applying modesty, including:</p>\n<ul>\n<li>Identifying experts is far from free epistemically.</li>\n<li>Epistemic majoritarianism in practice assumes that no one else is an epistemic majoritarian. Your first guess should be that nearly everyone else is iff you are, in which you should expect information cascades due to the occasional overconfident person. If other people are not majoritarians because they're too stupid to notice the considerations for it, then it seems a bit silly to defer to them. On the other hand, if they're not majoritarians because they're <em>smarter</em> than you are... well, you mention this, but this objection seems to me to be obviously fatal and the only thing left is to explain <em>why</em> the wisdom of the majority disagrees with the epistemically modest.</li>\n<li>The vast majority of information available about other people's opinions does not differentiate clearly between their impressions and their beliefs after adjusting for their knowledge about others' beliefs.</li>\n<li>People lie to maintain socially desirable opinions.</li>\n<li>Control over others' opinions is a valuable social commodity, and apparent expertise gives one some control.</li>\n</ul>\n<p>In particular, the last two factors (different sorts of dishonesty) are much bigger deals if most uninformed people copy the opinions of apparently informed people instead of saying &quot;I have no idea&quot;.</p>\n<p>Overall, I agree that when you have a verified-independent, verified-honest opinion from a peer, one should weight it equally to one's own, and defer to one's verified epistemic superiors - but this has little to do with real life, in which we rarely have that opportunity!</p>\n", "parentCommentId": null, "user": {"username": "BenHoffman"}}, {"_id": "qFGz372GfpTkh28vQ", "postedAt": "2018-02-27T02:16:14.649Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Apropos of which, SEP published an <a href=\"https://plato.stanford.edu/entries/disagreement/\">article</a> on disagreement last week, which provides an (even more) up to date survey of philosophical discussion in this area. </p>\n", "parentCommentId": "9BudHkbzimsDbQdye", "user": {"username": "Gregory_Lewis"}}, {"_id": "YBPd7iEmLd6A6HXyv", "postedAt": "2018-06-29T22:20:42.645Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>To add to the list of references in this thread, Brian Tomasik talks about this in &quot;Gains from Trade through Compromise&quot; in the section <a href=\"https://foundational-research.org/gains-from-trade-through-compromise/#Epistemic_prisoners_dilemma\">&quot;Epistemic prisoner's dilemma&quot;</a>.</p>\n", "parentCommentId": "CXhP4DDgGCWR6B4Zq", "user": {"username": "riceissa"}}, {"_id": "9S933PbcFMwbgPsAs", "postedAt": "2019-11-19T13:13:15.228Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Great article. I&apos;m very late to the party in reading it &amp; commenting, but I hope not too late to be of use!</p><p>I have three further reasons for epistemic immodesty in some circumstances. They all involve experts, or those who follow their advice, being overconfident about the experts&apos; relevant knowledge. (Though I note your comments about debunking experts; none of these arguments show an amateur is better than some other, probably small, set of experts who have taken these considerations into account.)</p><p>HIDDEN PREFERENCES</p><p>You mention that expert views aren&apos;t relevant in matters of taste, i.e. preference. However, expert views are often based on non-explicit preferences, which some experts may even be unaware of themselves.</p><p>To start with a clear situation where preferences are involved: If I&apos;m looking for a house to buy and trying to decide which one to choose, I may well consult experts in the field, such as an estate agent (realtor), a mortgage advisor, and an architect (if it may need building work). They may advise that I can&apos;t afford a house more than $x, or it will cost $y to do up, etc. But even with all their expert advice, this won&apos;t necessarily settle the matter of which house to buy, because I also have to *like* the house in question, want to live in that area, etc. So my decision involves both expert factual opinion and my personal preference; and I am the sole expert on the latter.</p><p>Now to take a less clear situation, currently topical in the UK: Brexit. Despite years of debate about this, which often includes discussion of experts and whether they should be trusted, I don&apos;t think I&apos;ve heard anyone state clearly that it too mixes expert opinion and preference. Most economists say Brexit will harm the economy, and most voters opposed to Brexit assume this simply entails Brexit is a bad thing. But of course the issue is not only about money - various other considerations are involved (e.g. self-determination) - and the trade-off between these is a matter of preference. Some people with unusual preferences may have coherent reasons to oppose Brexit (e.g. I spoke to someone who voted based on the fact that animal welfare is taken more seriously in the UK than most other EU countries, a consideration she regarded as more important than the economy). So this is an example of a &apos;semi-hidden&apos; preference - one where many people assume expert opinion is a silver bullet - perhaps including the experts themselves - and overlook the element of preference.</p><p>A different example is government guidelines on alcohol consumption. In the UK men are advised by experts to drink no more than (I think) 14 units per week. However, this advice is based on a trade-off between health and pleasure: if you really enjoy alcohol you may be happy to exchange a risk of significantly reduced health or longevity for drinking much more than 14 units. This trade-off is a preference, which the experts have made for you. (And AFAIK the trade-off they chose is arbitrary, not even based on research into say average preferences.)</p><p>Other topics may include preferences so hidden that even the experts are hardly aware of them. An example in EA would be the use of DALYs and QALYs (disability/quality-adjusted life years) as human welfare metrics in assessing charities &amp; interventions. Some who work with these metrics may overlook, or perhaps be unaware of, their shortcomings. DALYs and QALYs as currently defined assume that no condition is worse than death - which is inconsistent with the existence of suicide and euthanasia. When ordinary people are surveyed, their views on this vary widely - some taking the (perhaps religious) position that nothing is worse than death, and suicide/euthanasia should never be allowed, whereas others have no problem with the idea of suicide/euthanasia to escape prolonged untreatable agony, for example. So the mere use of these units involves tacitly taking a position on this, i.e. a hidden preference. A resulting expert view that X charity or intervention is better than Y is therefore partly objective and partly subjective; the expert themself may overlook this fact, or even (when involving technical philosophical issues) be unaware of it.</p><p>Other unstated assumptions are widespread in EA, e.g. that saving lives is a good thing (even though the world may be overpopulated), or that the prevention of merely potential future humans by mass extinction is a bad thing (even though contraception is fine).</p><p>In such cases, a non-expert who identifies such a hidden preference that they don&apos;t share may well have good reason to disregard the expert opinion.</p><p>SHAKY FOUNDATIONS</p><p>Relatedly, there is the issue of core assumptions that are largely unquestioned within a scientific field. A classic example is induction: physics assumes that just because in the past things seem to have behaved in a regular fashion, they will continue to do so. This is the basis of the belief in physical laws (and other laws of nature). Philosophers have long questioned this assumption; there really may be no reason to assume the sun will rise tomorrow, or that the speed of light was the same yesterday or a million years ago; which undermines all kinds of experiments and models. I expect many physicists are only dimly aware of this, know little of the arguments involved, and perhaps regard it as a quasi-theological debate not worth serious attention.</p><p>As with DALYs and QALYs, core assumptions like induction are often shaky, and the shakiness is often only taken seriously (or even known about) by those outside the field, e.g. philosophers. Indeed, articles of faith are often left unquestioned by true believers, lest they turn out to be an Achilles heel, and (mixing more metaphors) the whole edifice is built on sand. To question foundational beliefs may be heresy.</p><p>So an amateur outsider may well be more aware of such problems than an expert in the field; and may therefore be justified in using them to dismiss expert opinion, or at least, to take it with a big pinch of salt.</p><p>NARROW EXPERTISE</p><p>Many experts are only expert in an extremely narrow field, yet may be assumed to have a broader range of expertise (and some experts may also believe this themselves).</p><p>Apologies, but the clearest example I can think of is myself! At one time I was one of just a handful of world experts in an extremely narrow field - the music notation software industry. (As I owned a company in this field.) My knowledge was extremely in-depth - I had spent years coding this kind of software, knew endless obscure feature requirements, knew all about the market, wrote manuals and brochures, etc. Yet in other respects I knew less than many amateurs. I had never used (and hardly even seen) any music notation software other than my own company&apos;s. I knew even less about other types of music software (e.g. sequencers), used by millions of people, often my own customers. So I was a world expert in a very narrow field, yet an ignoramus both in aspects of my own field, and in very close fields.</p><p>The same is presumably true elsewhere. Amateurs may know as much as a world expert who is only slightly outside their very narrow field, or even on topics within their specialism. And at least occasionally, experts are unaware of their ignorance on these things. That is, they may make the same false assumptions as others do about the breadth &amp; depth of their expertise.</p><p>(An example: the book The Oxford Companion to the Mind is an encyclopedia edited by the eminent psychologist Richard Gregory. Some of the entries in the original edition are by Gregory himself, despite dealing with philosophy of mind &amp; metaphysics, topics evidently outside his expertise. They are amateurish, making confusions that would embarrass a philosophy undergraduate. Even the blurb on the cover jacket casually conflated &apos;brain&apos; and &apos;mind&apos; in ways only an ignoramus would do. When I was a philosophy student I was so astonished by this I almost wrote a letter to Gregory suggesting he get someone with domain expertise to rewrite his entries.)</p>", "parentCommentId": null, "user": {"username": "bfinn"}}, {"_id": "a6nqpsotK2CriCYth", "postedAt": "2019-11-19T16:09:15.653Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>On a separate small point, I think your probability estimate for ESP is too low, for two reasons:</p><p>Firstly, it is a taboo topic (like UFOs and the Loch Ness monster), which scientists are therefore far more likely to dismiss from a position of ignorance, or with weakish arguments (e.g. &apos;it lacks an explanatory mechanism&apos;, &apos;much of the research methodology is flawed&apos;, or &apos;some of the research has been on fraudsters&apos; - hardly disproof). Few skeptics have domain expertise, i.e. of having conducted or investigated research in the area.</p><p>Secondly, ESP covers quite a range of rather distinct phenomena. Only one has to be right for ESP to be true. And I&apos;m not sure that all would require completely novel scientific principles (e.g. unknown physical forces); and the fact that our understanding of physics has gaps, and our understanding of consciousness certainly does, may well leave room for some form of ESP to be compatible with current science (not that that is essential).</p>", "parentCommentId": null, "user": {"username": "bfinn"}}, {"_id": "4zFi8WALnJdx3xGtb", "postedAt": "2020-10-11T19:27:11.541Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<blockquote><p>when domain experts look at the 'answer according to the rationalist community re. X', they're usually very unimpressed, even if they're sympathetic to the view themselves. I'm pretty Atheist, but I find the 'answer' to the theism question per LW or similar woefully rudimentary compared to state of the art discussion in the field. I see similar experts on animal consciousness, quantum mechanics, free will, and so on similarly be deeply unimpressed with the sophistication of argument offered.</p></blockquote><p>I would love to see better evidence about this. Eg it doesn't match my experience of talking to physicists.</p>", "parentCommentId": "hxygnK4mzPPjwzTRK", "user": {"username": "Buck"}}, {"_id": "CmfSCytYj5fQmirmX", "postedAt": "2020-10-13T19:06:41.078Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<blockquote>I&apos;m pretty Atheist, but I find the &apos;answer&apos; to the theism question per LW or similar woefully rudimentary compared to state of the art discussion in the field.</blockquote><p>This will be a pertinent critique if the aim of LessWrong is to be a skeptics forum, created to make the most canonical debunkings (serving a societal purpose akin to <a href=\"https://www.snopes.com/\">Snopes</a>). It seems much less relevant if you are trying to understand the world, unless you maybe have a very strong intuition or evidence that sophistication is highly correlated with truth.</p>", "parentCommentId": "hxygnK4mzPPjwzTRK", "user": {"username": "Linch"}}, {"_id": "hCkyLHvW67vG2LkrP", "postedAt": "2023-01-22T20:36:44.316Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>I'm pretty late to the party (perhaps even so late that people forgot that there was a party), but just in case someone is still reading this, I'll leave my 2 cents on this post.&nbsp;</p><p><strong>[Context:</strong> A few days ago, <a href=\"https://forum.effectivealtruism.org/posts/9J3o3cjGSHbHZoWQN/a-new-heuristic-to-update-on-the-credences-of-others\">I released a post </a>that distils a paper by Kenny Easwaran and others, in which they propose a rule for updating on the credences of others. In a (tiny) nutshell, this rule, \"Upco\", asks you to update on someones credence in proposition A by multiplying your odds with their odds.]</p><p>&nbsp;1. Using Upco suggests some version of strong epistemic modesty: whenever the product of all the odds of your peers that you have learned is larger than your own odds, then your credence should be dominated by those of others; and if we grant that this is virtually always the case, then strong epistemic modesty follows.&nbsp;</p><p>2. While I agree with some version of strong epistemic modesty, I strongly disagree with what I take to be the method of updating on the credence of peers that is proposed in this post: taking some kind of linear average (from hereon referred to as LA). Here's a summary of reasons why I think Upco is a better updating rule, copied from my post:&nbsp;</p><blockquote><p>Unfortunately, the LA has some undesirable properties (see section 4 of the paper):</p><ul><li>Applied in the way sketched above, LA is non-commutative, meaning that LA is sensitive to the order in which you update on the credences of others, and it seems like this should be completely irrelevant to your subsequent beliefs.&nbsp;<ul><li>This can be avoided by taking the \u201ccumulative average\u201d of the credences of the people you update on, i.e. each time you learn someone's credence in A you average again over all the credences you have ever learned regarding this proposition. However, now the LA has lost its initial appeal; for each proposition you have some credence in, rationality seems to require you to keep track of everyone you have updated on and the weights you assigned to them. This seems clearly intractable once the number of propositions and learned credences grows large.&nbsp;</li><li>See&nbsp;<a href=\"https://www.cambridge.org/core/journals/episteme/article/abs/commutativity-of-evidence-a-problem-for-conciliatory-views-of-peer-disagreement/660AA593D8EF336A8CBD350608DA2677\"><u>Gardiner (2013)</u></a> for more on this.</li></ul></li><li>Relatedly, LA is also sensitive to whether you update on multiple peers at once or sequentially.</li><li>Also, LA does not commute with Bayesian Updating. There are cases where it matters whether you&nbsp;<i>first</i> update on someone's credence (e.g. regarding the bias of a coin) using the LA and&nbsp;<i>then&nbsp;</i>on \u201cnon-psychological\u201d evidence (e.g. the outcome of a coin-flip you observed) using Bayesian Updating or the reverse.&nbsp;</li><li>Moreover, LA does&nbsp;<i>not</i> preserve \u2018judgments of independence\u2019. That is, if two peers judge two propositions A and B to be independent, i.e.&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(AB)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"= P(A)P(B)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q(A B) = Q(A)Q(B)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>, then after updating on each other's credences, independence is not always preserved. This seems intuitively undesirable: if you think that the outcome of (say) a coin flip and a die roll are independent and I think the same - why should updating on my credences change your mind about that?</li><li>LA does <i>not </i>exhibit what the authors call \u201csynergy\u201d. That is, suppose&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(A) = p\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q(A) = q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span></span></span></span>. Then it is necessarily the case that&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(A)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q^+(A)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;are in the interval&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"[p,q]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span></span></span></span>&nbsp;if they are both applying LA. In other words, using the LA never allows you to update beyond the credence of the most confident person you\u2019ve updated on (or yourself if you are more confident than everybody else).&nbsp;<br><br><ul><li>At first sight, this might seem like a feature rather than a bug. However, this means that the credence of someone less confident than you can&nbsp;<i>never&nbsp;</i>be positive evidence regarding the issue at hand. Suppose you are 95% sure that A is true. Now, for any credence smaller than 95% LA would demand that you update downwards. Even if someone is perfectly rational, has a 94.9% credence in A and has evidence completely independent from yours, LA tells you that their credence is disconfirming evidence.&nbsp;</li></ul></li></ul><p><br>Perhaps most importantly, since Bayesian Updating does&nbsp;<i>not&nbsp;</i>have these properties, LA does&nbsp;<i>not&nbsp;</i>generally produce the same results. Thus, insofar as we regard Bayesian updating as the normative ideal, we should expect LA to be at best an imperfect heuristic and perhaps not even that.&nbsp;</p><p>In sum, LA has a whole host of undesirable properties. It seems like we therefore would want an alternative rule that avoids these pitfalls while retaining the simplicity of LA.&nbsp;&nbsp;&nbsp;</p><p>The EaGlHiVe aims to show that such a rule exists. They call this rule \u201cUpco\u201d, standing for \u201cupdating on the credences of others\u201d. Upco is a simple rule that avoids many of the problems of LA: preservation of independence, commutativity, synergy, etc. Moreover, Upco produces the same results as Bayesian Updating under some conditions.</p></blockquote>", "parentCommentId": null, "user": {"username": "Aaron__Maiwald"}}, {"_id": "YNfvtLB7PsmPsT9ft", "postedAt": "2023-02-05T01:31:26.277Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Somehow I missed your reply originally; I've updated my comment to correct the author name of the post.</p>\n", "parentCommentId": "gC2DtoLQKTbec2odK", "user": {"username": "vipulnaik"}}, {"_id": "fH5tckbrZXz5M9BTb", "postedAt": "2023-02-05T01:35:22.861Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Thanks! By the way, &nbsp;I found your original comment helpful for writing about the history of the concept of an <a href=\"https://forum.effectivealtruism.org/topics/independent-impression\">independent impression</a>.</p>", "parentCommentId": "YNfvtLB7PsmPsT9ft", "user": {"username": "Pablo_Stafforini"}}, {"_id": "6bCTD79KogRcs6wyY", "postedAt": "2023-05-12T16:24:18.834Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Can you give 5 examples of cases where rationalist/EAs should defer more to experts?</p>", "parentCommentId": null, "user": {"username": "tailcalled"}}, {"_id": "nmR63aMDjweypEDPv", "postedAt": "2023-06-04T20:05:34.238Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p>Although you are right that modesty (or deference) often outperforms one's own personal judgment, this isn't always the case. Results below are based on Monte Carlo simulations I haven't published yet.</p><p>Take the case of a crowd estimating a cow's weight. The members of the crowd announce their guesses sequentially. They adopt a uniform rule of D% deference, so that each person's guess is a weighted average of a sample from a Normal distribution centered on the cow's true weight, and of the current crowd average guess:</p><p>Guess_i = D*(Crowd average) + (1-D)*(Direct observation)</p><p>Under this rule, as deference increases, the crowd converges more slowly on the cow's true weight: deference is bad for group epistemics. This isn't the same thing as an information cascade, because the crowd <i>will</i> converge eventually unless they are completely deferent.&nbsp;</p><p>Furthermore, the benefits of deference for individual guess accuracy are maximized at about 78% deference except potentially on very long timescales. Beyond this point, the group converges so slowly that it undermines, though doesn't fully cancel out, the individual benefit of adopting the group average.</p><p>Finally, when faced with a choice of whether to make a guess according to the group's rule for deference or whether to be 100% deferent and simply guess the current group average, you will actually do better to make a partially deferent guess than a 100% deferent guess if the group is more than about 78% deferent. Below that point, it's better for individual accuracy to 100% defer, which suggests a Prisoner's Game Dilemma model in which individuals 'defect' on the project of obtaining high crowd accuracy by deferring to the crowd rather than contributing their own independent guess, leading to very high and deeply suboptimal but not maximally bad levels of deference. &nbsp;</p><p>These results depend on specific modeling assumptions.</p><ul><li>We might penalize inaccuracy according to its square. This makes deference less useful for individual accuracy, putting the optimal level closer to 65% rather than 78%.</li><li>We can also imagine that instead of using the crowd average, the deferent portion of the guess is sampled from a Normal distribution about the crowd average. In this case, the optimal level of deference is closer to 45%, and beyond about 78% deference, it's better to ignore the crowd entirely and just do pure direct observation.</li><li>I haven't simulated this yet, but I am curious to know what happens if we assume a fixed 1% of guesses are purely independent.</li><li>We are evaluating the whole timeline of guesses and observations. What if we are thinking about a person joining a mature debate, where the crowd average has had more time to converge?</li><li>It assumes that making and sharing observations is cost-free. In reality, of course, if every scientist had to redo all the experiments in their field (i.e. never deferred to previous results), science could not progress, and this is true everywhere else as well.</li><li>Whether or not the cost of further observations is linked to the current group accuracy. If we imagine a scenario where individual accuracy is a heavy driver of the costs of individual observations, then we might want to prioritize deference to keep costs down and permit more or faster guesses. If instead it is crowd accuracy that controls the costs of observations, then we might want to focus on independent observations.&nbsp;</li></ul><p>Overall, I think we need to refocus the debate on epistemic modesty around tradeoffs and modeling assumptions in order to help people make the best choice given their goals:</p><ul><li>How much to defer seems to depend on a few key factors:<ul><li>The cost of making independent observations vs. deferring, and whether or not these costs are linked to current group or individual accuracy</li><li>How inaccuracy is penalized&nbsp;</li><li>How deferent we think the group is</li><li>Whether we are prioritizing our own individual accuracy or the speed with which the group converges on the truth</li></ul></li><li><strong>Basically all the problems with deference can be eliminated if we are able to track the difference between independent observations and deferent guesses.</strong></li></ul><p>My main takeaways are that:</p><ul><li>Intuition is only good enough to be dangerous for thinking in the abstract about deference</li><li>Real-world empirical information is crucial for making the right choice of modeling assumptions to decide on how much to defer</li><li>Deference is a common and important topic in the rationalist and EA communities on a number of subjects, and should motivate us to try and take a lot more guesses</li><li>It is probably worth trying to figure out how to better track the difference between deferent guesses and independent observations in our discourse</li></ul>", "parentCommentId": null, "user": {"username": "AllAmericanBreakfast"}}, {"_id": "onkAhngHC2qHK967b", "postedAt": "2023-10-05T11:24:34.645Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": "<p><a href=\"https://royalsocietypublishing.org/doi/full/10.1098/rstb.2020.0052\">Some evidence that people tend to underuse social information</a>, suggesting they're not by default epistemically modest:</p><blockquote><p><br>Social information is immensely valuable. Yet we waste it. The information we get from observing other humans and from communicating with them is a cheap and reliable informational resource. It is considered the backbone of human cultural evolution. Theories and models focused on the evolution of social learning show the great adaptive benefits of evolving cognitive tools to process it. In spite of this, human adults in the experimental literature use social information quite inefficiently: they do not take it sufficiently into account. A comprehensive review of the literature on five experimental tasks documented 45 studies showing social information waste, and four studies showing social information being over-used. These studies cover \u2018egocentric discounting\u2019 phenomena as studied by social psychology, but also include experimental social learning studies. Social information waste means that human adults fail to give social information its optimal weight. Both proximal explanations and accounts derived from evolutionary theory leave crucial aspects of the phenomenon unaccounted for: egocentric discounting is a pervasive effect that no single unifying explanation fully captures. Cultural evolutionary theory's insistence on the power and benefits of social influence is to be balanced against this phenomenon.</p></blockquote><p>There is a discussion on \"the producer-scrounger dilemma for information use\" of potential interest:</p><blockquote><p>Social information is only useful when others also gather information asocially. Cultural evolutionary models contain a possible explanation of egocentric discounting. Rogers' influential model [<a href=\"https://royalsocietypublishing.org/doi/full/10.1098/rstb.2020.0052#RSTB20200052C81\"><strong>81</strong></a>] showed that social learning may not provide any advantage over individual learning when the environment changes. The advantage of using social learning depends on the frequency of social learners in the population: if those are too numerous, social learning is useless. When there are mostly individual learners, copying is effective, because it saves the costs of individual exploration, and because the probability of copying a correct behaviour is high. However, when there are mostly social learners, the risk of copying an outdated behaviour increases and individual learners are advantaged. This means the advantages of social learning are inversely frequency-dependent: the more other people learn socially, the less efficient it is to learn from them. The same logic is reflected, on a smaller scale, in models of information cascades, where social learning can (with a small probability) become detrimental for an individual when too many other individuals resort to it. More generally, a broad range of models converge upon the view that social information use can be likened, in terms of evolutionary game theory, to a producer\u2013scrounger dynamic [<a href=\"https://royalsocietypublishing.org/doi/full/10.1098/rstb.2020.0052#RSTB20200052C37\"><strong>37</strong></a>,<a href=\"https://royalsocietypublishing.org/doi/full/10.1098/rstb.2020.0052#RSTB20200052C77\"><strong>77</strong></a>,<a href=\"https://royalsocietypublishing.org/doi/full/10.1098/rstb.2020.0052#RSTB20200052C82\"><strong>82</strong></a>]. At equilibrium, these games typically yield a mixed population of producers (individual learners) and scroungers (social learners), where neither type does better than the other [<a href=\"https://royalsocietypublishing.org/doi/full/10.1098/rstb.2020.0052#RSTB20200052C83\"><strong>83</strong></a>,<a href=\"https://royalsocietypublishing.org/doi/full/10.1098/rstb.2020.0052#RSTB20200052C84\"><strong>84</strong></a>]. Egocentric discounting might emerge from a producer\u2013scrounger dilemma, as a response to the devaluation of social information which may occur when too many other agents rely on social learning.</p></blockquote><p>Note that this seems to assume that people don't use the \"credence by my lights\" vs. \"credence all things considered\"-distinction discussed in the comments.</p>", "parentCommentId": null, "user": {"username": "Stefan_Schubert"}}, {"_id": "dseXGczQgrriQWhRR", "postedAt": "2023-05-30T22:21:37.512Z", "postId": "WKPd79PESRGZHQ5GY", "htmlBody": null, "parentCommentId": null, "user": null}]