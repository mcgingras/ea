[{"_id": "JZySL8P4vkcAbR79L", "postedAt": "2022-11-04T19:54:09.472Z", "postId": "FnviTNXcjG2zaYXQY", "htmlBody": "<p>Sounds like a good plan! I think there are other challenges with alignment that this wouldn\u2019t solve (e.g. inner misalignment), but this would help. If you haven\u2019t seen it, you might be interested in <a href=\"https://arxiv.org/abs/2008.02275\">https://arxiv.org/abs/2008.02275</a></p>\n", "parentCommentId": null, "user": {"username": "Aidan O'Gara"}}, {"_id": "tPRonLpZYHaWmdFvp", "postedAt": "2022-11-06T03:22:18.963Z", "postId": "FnviTNXcjG2zaYXQY", "htmlBody": "<p>Thank you, this is super helpful! I appreciate it.&nbsp;</p><p>Yes, good point, if inner misalignment would emerge from an ML system, then any data source that was used for training would be ignored by the system anyways.</p><p>Depends on if you think alignment is a problem for the humanities or for engineering.&nbsp;</p>", "parentCommentId": "JZySL8P4vkcAbR79L", "user": {"username": "oliver_siegel"}}]