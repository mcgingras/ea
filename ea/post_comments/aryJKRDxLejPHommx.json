[{"_id": "jCYroTGzsLnWMiQoN", "postedAt": "2022-11-13T18:07:02.441Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<p>Although EA risk attitudes may have played a role in FTX's demise, I think to the extent that is true it is due to the peculiar nature of finance rather than EA advice being wrong in most instances. Specifically, impact in most areas (e.g., media, innovations, charitable impact) is heavily right-tailed but financial exchanges have a major left-tailed risk of collapse. As human expectations of success are heavily formed and <a href=\"https://www.aeaweb.org/articles?id=10.1257/jep.36.3.223\">biased by our most recent similar experiences</a>, this will cause people to not take enough risk when the value is in the right tail (as median&lt;mean) and take on too much when there are major failures in the left tail (as median&gt;mean).</p><p>If this is true, we may need to consider which specific situations have these left-tailed properties and to be cautious about discouraging too much risk taking in those domains. However, &nbsp;I suspect that this situation may be very rare and has few implications for what EAs should do going forwards.</p><p>NOTE: I published something similar on another thread but feel it is even more relevant here.</p>", "parentCommentId": null, "user": {"username": "Joseph Richardson"}}, {"_id": "zep6vNydnXXdZCsw7", "postedAt": "2022-11-13T20:07:28.753Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<p>Excellent post, and I agree with much of it. (In fact, I was planning to write something similar about the perils of expected value thinking.) I agree that SBF seems to have been misguided more by expected value thinking than by utilitarianism per se.&nbsp;</p><p>In particular, I think there's been a very naive over-reliance in both EA and the LessWrong rationalist community on the Tversky &amp; Kahneman 'heuristics and biases' program of research on 'cognitive biases'. That's the program that convinced many smart hyper-systematizers that 'loss aversion' and 'risk aversion' are irrational biases that should be overcome by 'debiasing'.&nbsp;</p><p>Much of what SBF said in the interviews you quoted seems inspired by that 'cognitive biases' view that (1) expected utility theory is a valid normative model for decision making, (2) humans should strive to overcome their biases and conform more to expected utility theory.&nbsp;</p><p>I understand the appeal of that thinking. I took Amos Tversky's decision-making class at Stanford back in the late 1980s. I worked a fair amount on judgment and decision-making, and game theory, back in the day. However from the late 1980s onwards, the cognitive biases research has been challenged repeatedly and incisively by other behavioral sciences researchers, including the ecological rationality field (e.g. Gerd Gigerenzer, Ralph Hertwig, Peter Todd), the evolutionary biology work on animal behavior (e.g. risk-sensitive foraging theory), and the evolutionary psychology field.&nbsp;</p><p>All of those fields converged on a view that loss aversion and risk aversion are NOT always irrational. In fact, for mortal animals that face existential risks to their survival and reproduction prospects, they are very often appropriate. This is the problem of the 'lower boundary' of ruination and disaster that the OP here mentioned. When animals -- including humans -- are under selection to live a long time, they do not evolve to maximize expected utility (e.g. calorie intake per hour of foraging). Instead, the evolve to minimize likelihood of catastrophic risk (e.g. starvation during a cold night). The result: loss aversion and, often, risk aversion. (Of course, risk-seeking often makes sense in many domains of animal behavior such as male-male competition for mates. But loss-seeking almost never makes sense.)</p><p>So, I think EAs should spend a lot more time re-thinking our emphasis on expected utility maximization, and our contempt for 'cognitive biases' -- which often evolved as adaptive solutions to real-life dangers of catastrophic failure, not just as 'failures of rationality', as often portrayed in the Rationalist community. We should also be extremely wary of trying to 'debias' people, without understanding the evolutionary origins and adaptive functions of our decision-making 'biases'.&nbsp;</p><p>A good start would be to read the many great books about decision making by <a href=\"https://en.wikipedia.org/wiki/Gerd_Gigerenzer\">Gerd Gigerenzer</a> (including his critiques of Daniel Kahneman's research and expected utility theory), and to learn a bit more about <a href=\"https://en.wikipedia.org/wiki/Optimal_foraging_theory\">optimal foraging theory</a>.&nbsp;</p><p>PS I'm especially concerned that AI safety research relies on expected value thinking about the benefits and costs of developing transformational AI. As if a huge potential upside from AI (prosperity, longevity, etc) can counter-balance the existential risks of AI. &nbsp;That kind of reasoning strikes me as more orders of magnitude more dangerous than anything SBF did.</p>", "parentCommentId": null, "user": {"username": "geoffreymiller"}}, {"_id": "fosR2MeoEacCXEjNw", "postedAt": "2022-11-13T20:11:05.298Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<p>One thing that he didn't use in his EV calculations is meta-level impact of failure on the popularity of EA and utilitarianism. &nbsp;Even &nbsp;relatively small failure in money could have almost infinite negative utility if topics like x-risks prevention become very unpopular.</p>", "parentCommentId": null, "user": {"username": "turchin"}}, {"_id": "k9Dacmy3rirArgvAe", "postedAt": "2022-11-13T20:14:28.789Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<p>I disagree, and I view Joseph Richardson's comment as why it's limited to finance rather than indicating a systemic problem:</p>\n<blockquote>\n<p>Although EA risk attitudes may have played a role in FTX's demise, I think to the extent that is true it is due to the peculiar nature of finance rather than EA advice being wrong in most instances. Specifically, impact in most areas (e.g., media, innovations, charitable impact) is heavily right-tailed but financial exchanges have a major left-tailed risk of collapse. As human expectations of success are heavily formed and biased by our most recent similar experiences, this will cause people to not take enough risk when the value is in the right tail (as median&lt;mean) and take on too much when there are major failures in the left tail (as median&gt;mean).</p>\n</blockquote>\n<blockquote>\n<p>If this is true, we may need to consider which specific situations have these left-tailed properties and to be cautious about discouraging too much risk taking in those domains. However,  I suspect that this situation may be very rare and has few implications for what EAs should do going forwards.</p>\n</blockquote>\n", "parentCommentId": "zep6vNydnXXdZCsw7", "user": {"username": "Sharmake"}}, {"_id": "TDwXwmqDjqJdzcicR", "postedAt": "2022-11-13T20:21:57.700Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<p>Great post.</p><p>It's embarrassing that EA has been so far reluctant to discuss the plainly obvious fact that SBF's risk-taking is linked to his EA philosophy. It's incredibly obvious that it was. Everyone outside of EA already knows this.</p><p>The reasoning is fairly straightforward: a double or nothing coin-flip has high value in expectation, even when you scale it to the billions of dollars. So risky decisions can still have high expected value. This is true even if the risk includes, for example, the situation we are currently in.</p><p>Because if that risk were small enough, or if the rewards for getting away with it were high enough, then SBFs gambles could have plausibly had high value in expectation. Consider also the fact that EV reasoning leads to practically unbounded upsides.</p><p>Put all this together and that leads us to fanaticism. As it happens, a good definition of 'fanaticism' can be found in SBF's blog (linked in the OP):</p><blockquote><p>The argument, roughly goes: when computing expected impact of causes, mine is 10^30 times higher than any other, so nothing else matters. &nbsp;For instance, there are 10^58 future humans, so increasing the odds that they exist by even .0001% is still worth 10^44 times more important that anything that impacts current humans.&nbsp;</p></blockquote><p>So it seems like SBF was at least aware of fanaticism. And that's no surprise. We've known this for a while. Because there has been talk on this forum for years about fanaticism. SBF surely was privy to some of these online discussions, as well as some offline discussions too. So perhaps he took fanaticism to heart. If he did, that would be unsurprising.</p><p>Because many prominent EAs have promoted fanaticism. Consider for example this forum post from the Global Priorities Institute called <a href=\"https://forum.effectivealtruism.org/posts/B9WsjWv7xPJcmpNuM/in-defence-of-fanaticism\">In Defense of Fanaticism</a>. Here's the abstract:</p><blockquote><p>Consider a decision between: 1) a certainty of a moderately good outcome, such as one additional life saved; 2) a lottery which probably gives a worse outcome, but has a <i>tiny</i> probability of some vastly better outcome (perhaps trillions of additional blissful lives created). Which is morally better? By expected value theory (with a plausible axiology), no matter how tiny its probability of the better outcome, (2) will be better than (1) as long as that better outcome is good enough. But this seems <i>fanatical</i>. So you may be tempted to abandon expected value theory.</p><p>But not so fast \u2014 denying all such fanatical verdicts brings serious problems. For one, you must reject either that moral betterness is transitive or even a weak principle of tradeoffs. For two, you must accept that judgements are either: inconsistent over structurally-identical pairs of lotteries; or absurdly sensitive to small probability differences. For three, you must accept that the practical judgements of agents like us are sensitive to our beliefs about far-off events that are unaffected by our actions. And, for four, you may also be forced to accept judgements which you know you would reject if you simply learned more. Better to accept fanaticism than these implications.</p></blockquote><p>And this blog posts corresponds to a paper &nbsp;from the Global Priorities Institute, which can be found <a href=\"https://globalprioritiesinstitute.org/hayden-wilkinson-in-defence-of-fanaticism/\">here</a>. Notice that the math in defence of fanaticism, here, is pretty rigorous. Now compare that to recent defences of utilitarian-minded EV reasoning (like, for example, <a href=\"https://forum.effectivealtruism.org/posts/nvus8kuGxyacyfXeg/naive-vs-prudent-utilitarianism\">this post</a>).&nbsp;</p><p>There is an asymmetry here. The defences of fanaticism are quite rigorous, mathematically speaking, whereas recent defences of utilitarianism + EV theory involve no math at all; the defence of fanaticism engages with its critics arguments head-on, whereas recent defences of utilitarianism do not engage with any critical arguments at all. So what line of reasoning would be more convincing to SBF? A hand wave, or a rigorous mathematical argument?</p><p>Since SBF is clearly a quantitatively minded person, it's quite plausible that the mathematical rigour was more appealing to him. And, unfortunately, that is quite plausibly why we are in the situation we are currently in.<br><br>To anyone who disagrees: I respectfully ask that you disagree with me mathematically. We know the quantity of money that SBF was working with, because we can estimate his expected earnings. We can estimate the risks he was working with. We can estimate the downsides of those risks (like bad PR and so on), too. We can give all these things a range to account for uncertainty, too. Once we have collected all these numbers, we can plug them into our EV calculus. So if you think that SBF made decisions with negative expected utility, please show me why by showing me your numbers.</p><p>But until then, I think it's best we admit that SBF's behaviour is linked to his utilitarian-minded EV reasoning.</p><p>If we fail to own up to this, then we are being dogmatic.</p>", "parentCommentId": null, "user": null}, {"_id": "CCysLsNvE8L3FX6xS", "postedAt": "2022-11-13T20:50:19.665Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<blockquote><p>In the absence of a push to be more ambitious, there's a pretty good chance that SBF would have felt content working at Jane Street Capital and donating a chunk of money to charity, and would only have left it to pursue direct work.</p></blockquote><p>I'd be curious to get takes on this from people who know SBF better. In my (limited) impression from working with him, he seemed both extremely ambitious and hard to influence; I'm doubtful that EA ambition culture had a big effect on him.&nbsp;</p><p>That said, I think agree with the general takeaway of tempering the pro-ambition framing . I think it's important to cultivate \"humility / thoughtfulness / integrity\" as EA ideals, and in particular as virtues that ought to be highly prioritized when one has a lot of influence.</p>", "parentCommentId": null, "user": {"username": "arthrowaway"}}, {"_id": "YSbPKd6etCr75W8tq", "postedAt": "2022-11-13T21:55:30.701Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<p>If this issue is limited to finance, why do you think that animals of most species studied so far seem to show loss aversion, and often show risk aversion?&nbsp;</p><p>Why would these 'cognitive biases' have evolved so widely?</p>", "parentCommentId": "k9Dacmy3rirArgvAe", "user": {"username": "geoffreymiller"}}, {"_id": "xcgnZcKbCE9xBLP5D", "postedAt": "2022-11-13T22:19:11.617Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<p>I have the answer, and it is right in my quote.</p>\n<p>Also, we are severely misaligned with evolution, to the point that in certain areas, we (according to evolution), are inner misaligned and outer misaligned, thus our goals can be arbitrarily different goals than what evolution has as it's goal.</p>\n<p>It's a textbook inner alignment and outer alignment failure.</p>\n", "parentCommentId": "YSbPKd6etCr75W8tq", "user": {"username": "Sharmake"}}, {"_id": "fWmdDSyizDueDHJH3", "postedAt": "2022-11-13T22:19:57.804Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<p>I don\u2019t think \u201crisk aversion\u201d was labelled  as a cognitive bias by anyone in the economics orbit. It just flows from diminishing marginal utility of income.  But please let me know if you have some references for this.</p>\n", "parentCommentId": "zep6vNydnXXdZCsw7", "user": {"username": "david_reinstein"}}, {"_id": "Epna7cjdrzx68ByHW", "postedAt": "2022-11-13T22:24:59.568Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<blockquote>\n<p>I suspect that altruistic impact is less linear in money, and that there are a lot of other details about the way things play out, that affect altruistic impact. For instance, I suspect that FTX could have had a significant positive impact if it had quit with SBF making enough to earmark a billion dollars for charity. That would have been enough money to champion the values and start a pattern of altruism that ultimately could have been continued by other donors (ironically, Nick Beckstead makes the point that individual funders may have relatively few good grants to make and that's why Future Fund experimented with delegating grantmaking to a larger number of regrantors; I think a similar point applies at the foundation level as well).</p>\n</blockquote>\n<blockquote>\n<p>This would obviously have been better than what ultimately transpired, but I suspect it would have been better even in properly done expected value calculations. This is a tricky point to justify and I won't attempt to do it here.</p>\n</blockquote>\n<p>This doesn\u2019t seem so hard to argue to me. Diminishing marginal returns to the amount invested in these innovative exploratory long termist research and impact projects. The fact that they found it hard to scale quickly and felt \u2018talent and vetting constrained\u2019 offers evidence of this.</p>\n", "parentCommentId": null, "user": {"username": "david_reinstein"}}, {"_id": "TJtGN9vyLP537NDEx", "postedAt": "2022-11-13T22:25:47.140Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<p>Sorry, but I don't understand your reply.&nbsp;</p><p>Are you saying that humans show too much loss aversion and risk aversion, and these 'biases' are maladaptive (unaligned with evolution)? Or that humans don't show enough loss aversion and risk aversion, compared to what evolution would have favored?</p><p>'Inner alignment' and 'outer alignment' aren't very helpful bits of AI safety jargon in this context, IMHO.&nbsp;</p>", "parentCommentId": "xcgnZcKbCE9xBLP5D", "user": {"username": "geoffreymiller"}}, {"_id": "nBbh9QKy4ZbweSfzJ", "postedAt": "2022-11-13T22:32:39.640Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<p>I don't know about economics, but 'risk aversion' is standardly treated as a 'cognitive bias' in psychology, e.g. <a href=\"https://en.wikipedia.org/wiki/Risk_aversion_(psychology)\">here</a></p><p>And the interviews with SBF (in the OP) seem to hint that he viewed risk aversion as more-or-less irrational, from the perspective of expected value theory.</p><p>I agree with your point that risk aversion regarding income is not 'irrational' given diminishing marginal utility of income.</p>", "parentCommentId": "fWmdDSyizDueDHJH3", "user": {"username": "geoffreymiller"}}, {"_id": "bCk8ugrqNf34xW3Fv", "postedAt": "2022-11-13T22:50:59.660Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<blockquote>\n<p>Are you saying that humans show too much loss aversion and risk aversion, and these 'biases' are maladaptive (unaligned with evolution)? Or that humans don't show enough loss aversion and risk aversion, compared to what evolution would have favored?</p>\n</blockquote>\n<p>Yes, in both cases.</p>\n<p>The basic issue is ignoring heavy tails to the right is going to give you too much risk-averseness, while heavy tails to the left will give you too much risk-seeking.</p>\n<p>An example of a heavy tail to the left is finance, where riskiness blows you up, but doesn't give you that much more to donate. Thus, SBF too much risk, and took too much wrong-way risk in particular.</p>\n<p>An example of a heavy tail to the right is job performance, where the worst is a mediocre job performance, while the best can be amazing. This, there is likely not enough risk aversion.</p>\n<p>Link here:</p>\n<p><a href=\"https://forum.effectivealtruism.org/posts/ntLmCbHE2XKhfbzaX/how-much-does-performance-differ-between-people\">https://forum.effectivealtruism.org/posts/ntLmCbHE2XKhfbzaX/how-much-does-performance-differ-between-people</a></p>\n<p>And we need to be clear: The world we are living in with complicated societies and newfangled phones now would be totally against evolution's values, so that's why I brought up the misalignment talk from AI safety.</p>\n", "parentCommentId": "TJtGN9vyLP537NDEx", "user": {"username": "Sharmake"}}, {"_id": "zDpmPtzbsA3QL8Sfp", "postedAt": "2022-11-19T21:16:10.497Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<p>Professional gambler here. I haven't really studied the formal theory behind the Kelly Criterion, but I'm certainly aware of the practical import. It doesn't rely on having a logarithmic utility function for money; it makes a much stronger claim, which is that it maximizes long-term results, and I believe it has been formally proven to do so.</p><p>Overbetting Kelly results in a much higher risk of ruin, which reduces long-term results <i>even if</i> your utility function for money is linear, as SBF claims.&nbsp;</p>", "parentCommentId": null, "user": {"username": "creedofhubris"}}, {"_id": "CBgqyp2CGu4HbpZNT", "postedAt": "2022-11-20T03:49:35.093Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<p>I see there seems to be some disagreement on this point... let me quote the conclusion of Kelly's original paper:<br><br>\"The gambler introduced here follows an essentially different criterion from the classical gambler. At every bet he maximizes the expected value of the logarithm of his capital. The reason has nothing to do with the value function which he attached to his money, but merely with the fact that it is the logarithm which is additive in repeated bets and to which the law of large numbers applies. \"<br><br>https://archive.org/details/bstj35-4-917/page/n9/mode/2up?view=theater</p><p>&nbsp;</p><p><br>&nbsp;</p>", "parentCommentId": "zDpmPtzbsA3QL8Sfp", "user": {"username": "creedofhubris"}}, {"_id": "sboDP6QRnRvPM9YYL", "postedAt": "2022-11-20T07:03:36.742Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<p>Good point! My understanding is that SBF's argument was that the right thing to average wasn't serial rounds of oneself (where the money to play with would be determined by past rounds), but parallel-universe versions of oneself (i.e., of 100 parallel universes with SBF trying his strategy, what % would lead to him being super-rich?).</p>\n", "parentCommentId": "CBgqyp2CGu4HbpZNT", "user": {"username": "vipulnaik"}}, {"_id": "ugHtC2eDH4PTNkAHb", "postedAt": "2022-12-13T04:14:53.480Z", "postId": "aryJKRDxLejPHommx", "htmlBody": "<p><a href=\"https://www.theguardian.com/us-news/2022/dec/12/former-ftx-ceo-sam-bankman-fried-arrested-in-the-bahamas-local-authorities-say\">https://www.theguardian.com/us-news/2022/dec/12/former-ftx-ceo-sam-bankman-fried-arrested-in-the-bahamas-local-authorities-say</a></p>\n", "parentCommentId": null, "user": {"username": "Deborah W.A. Foulkes"}}]