[{"_id": "MBp3dN3XBjSZboHHn", "postedAt": "2024-01-03T13:19:56.899Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>Thanks for posting this, I'm very excited to see the discussion it generates! One note: in the Acute Malnutrition Treatment section under Treatment effects, the sheet linked on \"relatively steep discount\" is currently private.&nbsp;</p>", "parentCommentId": null, "user": {"username": "MHR"}}, {"_id": "LWGkkgbyYttyhFaDG", "postedAt": "2024-01-03T15:39:16.925Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>[btw, this is a common problem when using spreadsheets rather than when modeling in a software development environment - the software space has a lot of experience in working in (partially-) open source settings]</p>", "parentCommentId": "MBp3dN3XBjSZboHHn", "user": {"username": "edoarad"}}, {"_id": "CZg3FJW9xNXywg7tz", "postedAt": "2024-01-03T16:26:03.209Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>I have a very rough draft based on work with <a href=\"https://forum.effectivealtruism.org/users/michael-latowicki-1?mention=user\">@Michael Latowicki</a> and Charity Entrepreneurship (<a href=\"https://forum.effectivealtruism.org/users/filip_murar?mention=user\">@Filip_Murar</a> and <a href=\"https://forum.effectivealtruism.org/users/weeatquince_duplicate0-37104097316182916?mention=user\">@weeatquince</a>) where we tried various tools for modeling uncertainty for use in their CEAs. Due to personal issues, I'm not sure when I'll finish it, so <a href=\"https://docs.google.com/document/d/1xeNBYrB7G8bl2LhtHBK0yIdERFpK36tkSpuImElIaVU/edit\">here it is</a> (very unpolished and not finished).</p><p>Mainly, we list various tools we found with some comments on them and discuss the pros and cons of spreadsheet vs software solutions. The latter is mostly coherent so I'll paste it here:</p><h2>Spreadsheets vs programming languages</h2><p>The tools we found for probabilistic calculation come in two flavours: either the calculation is expressed in a spreadsheet, or it is written as text in some programming language that is customized by a dedicated interpreter or library for this purpose.</p><p>The spreadsheet-based solutions seem to have one salient advantage: they do not scare non-programmers away. This is, we think, a barrier-to-entry advantage, rather than a long-term productivity advantage. The kind of people who build cost-effectiveness model are not incapable of being productive in a simple, probability-dedicated programming language as well as with a probabilistic library within a general purpose language.&nbsp;</p><h3>The error-proneness of spreadsheets</h3><p>We strongly suspect that using spreadsheets to organize formulae is more error-prone than using programming languages. We are not alone in this suspicion. Audits of spreadsheets generally&nbsp;<a href=\"https://arxiv.org/pdf/0802.3457.pdf\"><u>find</u></a> that something between a large fraction and&nbsp;<a href=\"https://arxiv.org/abs/0805.4224\"><u>nearly all spreadsheets</u></a> contain errors.&nbsp;</p><p>Some pitfalls that make spreadsheets more error-prone than programming languages:</p><ul><li>In a spreadsheet, mistyping a cell address does not necessarily result in an error message. By contrast, mistyping a variable name in a programming language typically does.</li><li>In a spreadsheet, unintentionally leaving a cell empty is equivalent to setting it to zero, which often does not result in an error. In a programming language, using a variable without declaring it does trigger an error.</li><li>Since spreadsheet formulae typically reference values by cell address, erroneous references are not salient to the reader. If your formula is \u201cpower(1+interest_rate, -years_left)\u201d, but it is encoded as \u201cpower(1+M7, $A$13)\u201d then you may easily fail to notice that the \u201c13\u201d should actually be \u201c12\u201d.</li><li>Formulas are not automatically updated to account for added data, for example in sums of columns.</li><li>Copy-and-pasting formulae often results in erroneous cell references because the software adjusts treats cell addresses as relative by default and the spreadsheet author often fails to consider whether addresses should be relative or absolute.</li></ul><p>There are&nbsp;<a href=\"https://arxiv.org/abs/0805.4224\"><u>other</u></a> causes of errors in spreadsheets, but you get the point. Errors occur in every kind of programming environment, of course, but spreadsheets sport their own additional pitfalls, on top of those that exist in every programming language.</p><p>We are far from certain that writing cost-effectiveness analyses in an ordinary programming language would reduce the error rate compared to spreadsheets - quantitative estimates of the error rate in both&nbsp;<a href=\"https://mba.tuck.dartmouth.edu/spreadsheet/product_pubs_files/errors.pdf\"><u>spreadsheets</u></a> and in&nbsp;<a href=\"https://www.johnsymons.net/wp-content/uploads/2018/09/Understanding-error-rates-.pdf\"><u>non-spreadsheet</u></a> programs find error rates on the same order of magnitude. The mix of problems that are typically approached using these two types of tools is different though, and we have not found an apples-to-apples study of those error rates.</p><p>It is apparently not easy to root out spreadsheet errors. In \u201cSpreadsheet errors: What we know. What we think we can do\u201d, Professor of IT management Ray Panko summarizes his findings:</p><blockquote><p>Unfortunately, only one approach to error reduction has been demonstrated to be effective. This is code inspection, in which a&nbsp;<i>group</i> of spreadsheet developers checks a spreadsheet cell-by-cell to discover errors. Even this exhausting and expensive process will catch only about 80% of all errors. Other things can be done to reduce errors, but given the tenacity of spreadsheet error in the face of even cell-by-cell code inspection, we should be careful about expecting too much from most error-reducing approach.</p></blockquote>", "parentCommentId": null, "user": {"username": "edoarad"}}, {"_id": "w4iuAhCXeTSAnzNcM", "postedAt": "2024-01-03T16:26:32.389Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>Hi there - thanks so much for catching this! Our malnutrition CEA is not yet public because it's still a work-in-progress. I've removed the hyperlink accordingly. Thanks again!</p>", "parentCommentId": "MBp3dN3XBjSZboHHn", "user": {"username": "GiveWell"}}, {"_id": "oQAFYzdLgTiEKckix", "postedAt": "2024-01-03T21:27:34.597Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>I made a similar comment on Noah's original submission: I think the optimizer's curse will not be a serious problem, if two conditions hold.</p>\n<ol>\n<li>We don't care about learning the actual cost-effectiveness of the interventions we select; we only care about ranking them correctly.</li>\n<li>The distribution of true cost-effectiveness for interventions has a thicker tail than the distribution of errors in our estimates.</li>\n</ol>\n<p>1 is a normative question, and it's my understanding of what GiveWell's goals are (since it ties most directly to the question of whether to fund the intervention). We want to select the best interventions: the cost-effectiveness number is just a means to that end.</p>\n<p>2 is an empirical question. It's well known that the distribution of true cost-effectiveness is fat tailed in many domains, but it's not really well known what the distribution of errors in estimates is. Sometimes people slap a lognormal assumption into their Monte Carlo simulations and run with it, but I don't think that's principled at all, and it's more likely to be a product of normal variables. GiveWell has the tools to estimate this, though.</p>\n<p>What happens if both 1 and 2 are true? Then our decision problem is simply to identify the top N interventions. The main risk is that uncertainty might cause us to label an intervention as being in the top N when it's actually not. However, since true cost effectiveness has a fat tail, <em>the top N interventions are miles better than the rest.</em> For a lemon intervention to sneak into our top N interventions, it would have to have a <em>very</em> high error. But condition 2 implies that the error variance is too small to generate errors large enough to knock a lemon intervention into the top! So when you estimate an intervention as \"top\", the prior probability of an error large enough to support a lemon getting that estimate is low, so the probability that this really is a top intervention is high. Thus, optimizer's curse is not a problem.</p>\n<p>This is an argument sketch, not a proof by any means. Obviously, 2 could fall apart in reality. I have done simulations with some other cost-effectiveness data that suggest it holds there, but I haven't written that up, and it's not the same as GiveWell's set of interventions. The point of this sketch is to suggest that GiveWell should look more into condition 2. It could shed light on whether the optimizer's curse is really important in this setting.</p>\n", "parentCommentId": null, "user": {"username": "therealslimkt"}}, {"_id": "YBQ5AAWSz93sfJyJv", "postedAt": "2024-01-03T22:58:26.814Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>I've often thought that more quantification of the uncertainty could be useful in communicating to donors as well. E.g. \"our 50% confidence interval for AMF is blah, and that confidence interval for deworming blah, so you can see we have much less confidence in it\". So I think this is a step in the right direction, thanks for sharing, setting it out in your usual thoughtful manner.</p>", "parentCommentId": null, "user": {"username": "Sanjay"}}, {"_id": "MaBwa7A26qwSvtQCt", "postedAt": "2024-01-03T23:01:51.669Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>Sorry for asking about a minor detail, but Figure 3 in <a href=\"https://forum.effectivealtruism.org/posts/NWmBkMe3yF4GQNnai/how-we-plan-to-approach-uncertainty-in-our-cost#3_2_1_Treatment_effects\">section 3.2.1</a> shows an internal validity adjustment of 90% for ITNs (top row of figure). I thought this was <a href=\"https://docs.google.com/spreadsheets/d/18ROI6dRdKsNfXg5gIyBa1_7eYOjowfbw5n65zkrLnvc/edit#gid=1364064522&amp;range=A48:I48\">95%</a>? Am I misunderstanding how you're thinking about the adjustment in this document?</p>", "parentCommentId": null, "user": {"username": "Sanjay"}}, {"_id": "2v9cLJzrbxW3WYqkm", "postedAt": "2024-01-04T02:23:11.695Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>I've personally found Guesstimate less error-prone and easier to verify and I'd guess easier to use in general than Google Sheets/Excel. Node names+acronyms and the displayed arrows between nodes are helpful.</p>\n<p>I'd also imagine Guesstimate would beat programming languages on these, too, with fewer places for error per variable or operation.</p>\n<p>However, Guesstimate is often not flexible enough, or takes a lot of nodes to do some simple things (e.g. sampling randomly from one of multiple variables). It can get very slow to edit with many (like 100 or more) nodes. It can also be more tedious for simple operations over many variables at a time, like a sum, IIRC.</p>\n<p>(Of the options you've listed, I've only used Guesstimate and Google Sheets (without the probability stuff). I was also a deep learning engineer for ~2 years.)</p>\n", "parentCommentId": "CZg3FJW9xNXywg7tz", "user": {"username": "MichaelStJules"}}, {"_id": "NZrDPiFevD96GKNYP", "postedAt": "2024-01-04T10:48:56.615Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>To add, we at Charity Entrepreneurship have been experimenting with using <a href=\"https://usedagger.com/\">Dagger</a>/<a href=\"https://usecarlo.com/\">Carlo</a> for our cost-effectiveness analyses of new charity ideas. We've put together this (very rough, work-in-progress) <a href=\"https://docs.google.com/document/d/1xoMFWDT4p1_r4CYxUbXrluzRRF0cGbp74w-7Ia7w23E/edit\">guide</a> on how to use Dagger \u2013 sharing in case others find it helpful.</p>", "parentCommentId": "CZg3FJW9xNXywg7tz", "user": {"username": "Filip_Murar"}}, {"_id": "cndYNDnBKmPnd95nB", "postedAt": "2024-01-04T13:35:46.911Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>Something I've wondered is whether GiveWell has looked at whether its methods are robust against <a href=\"https://en.wikipedia.org/wiki/Pascal%27s_mugging\">\"Pascal's mugging\"</a> type situations, where a very high estimate of expected value of an intervention leads to it being chosen even when it seems very implausible a priori. The deworming case seems to fit this mould to me somewhat - an RCT finding a high expected impact despite no clear large near term health benefits and no reason to think there's another mechanism to getting income improvements (as I understand it) does seem a bit like the hypothetical mugger promising to give a high reward despite limited reason to expect it to be true (though not as extreme as in the philosophical thought experiments).<br><br>Actually, doing a bit of searching turned up that Pascal's mugging has been discussed in an old 2011 post on the GiveWell blog <a href=\"https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">here</a>, but only abstractly and not in the context of any real decisions. The post seems to argue that past some point, based on Bayesian reasoning, \"the greater [the 'explicit expected-value' estimate] is, the lower the expected value of Action A\". So by that logic, it's potentially the case that had the deworming RCT turned up a higher, even harder to believe estimate of the effect on income, a good evaluation could have given a lower estimate of expected value. Discounting the RCT expected value by a constant factor that is independent of the RCT result doesn't capture this. (But I've not gone through the maths of the post to tell how general the result is.)<br><br>The post goes on to say 'The point at which a threat or proposal starts to be called \u201cPascal\u2019s Mugging\u201d can be thought of as the point at which the claimed value of Action A is wildly outside the prior set by life experience (which may cause the feeling that common sense is being violated)'. Maybe it's not common sense being violated in the case of deworming, but it does seem quite hard to think of a good explanation for the results (for an amateur reader like me anyway). Has any analysis been done on whether the deworming trial results should be considered past this point? It seems to me that that would require coming up with a prior estimate and checking that the posterior expectation does behave sensibly as hypothetical RCT results go beyond what seems plausible a priori. Of course, thinking may have evolved a lot since that post, but it seems to pick up on some key points to me.<br><br>It looks like &gt;$10M were given by GiveWell to deworming programs in 2023, and from what I can tell it looks like a large proportion of funds given to the \"All Grants\" fund went to this cause area, so it does seem quite important to get the reasoning here correct. Since learning about the issues with the deworming studies, I've wondered whether donations to this cause can currently make sense - as an academic, my life experience tells me not to take big actions based on results from individual published studies! And this acts as a barrier to feeling comfortable with donating to the \"All Grants\" fund for me, even though I'd like to handover more of the decision-making to GiveWell otherwise.</p>", "parentCommentId": null, "user": {"username": "Pagw"}}, {"_id": "Kn6aCmmgQDpmPqncX", "postedAt": "2024-01-05T01:47:58.625Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>From the post:</p>\n<blockquote>\n<p>We don\u2019t always try to convert the answers to these questions to the same \"currency\" as our cost-effectiveness estimates, because we think entertaining multiple perspectives ultimately makes our decision-making more robust. We\u2019ve previously written about this here, and we think these arguments still ring true. In particular, we think cluster-style thinking (Figure 6) handles unknown-unknowns in a more robust way, as we find that expert opinion is often a good predictor of \u201cwhich way the arguments I haven\u2019t thought of yet will point.\u201d</p>\n</blockquote>\n<p><a href=\"https://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">This</a> is the blog post being referenced. Its about exactly the problem you describe.</p>\n", "parentCommentId": "cndYNDnBKmPnd95nB", "user": {"username": "therealslimkt"}}, {"_id": "oZrW85Mwwjkwur6de", "postedAt": "2024-01-05T07:37:52.149Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>Hmm it's not very clear to me that it would be effective at addressing the problem - it seems a bit abstract as described. And addressing Pascal's mugging issues seems like it potentially requires modifying how cost effectiveness estimates are done ie modifying one component of the \"cluster\" rather than it just being a cluster vs sequence thinking matter. It would be good to hear more about how this kind of thinking is influencing decisions about giving grants in actual cases like deworming if it is being used.</p>\n", "parentCommentId": "Kn6aCmmgQDpmPqncX", "user": {"username": "Pagw"}}, {"_id": "hhowSjfy2PsCgBDvF", "postedAt": "2024-01-05T07:59:34.953Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>Isn't 1 addressed by Noah's submission? That you will rank noisily-estimated interventions higher.</p>\n", "parentCommentId": "oQAFYzdLgTiEKckix", "user": {"username": "jooke"}}, {"_id": "LFosciP6NxzpSvnZi", "postedAt": "2024-01-05T08:00:59.558Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>I agree with your point 2. To be Bayesian: if your prior is much more uncertain than you likelihood, the likelihood dominates the posterior.</p>\n", "parentCommentId": "oQAFYzdLgTiEKckix", "user": {"username": "jooke"}}, {"_id": "fWKeo7mpeykeKbfrq", "postedAt": "2024-01-05T08:02:51.574Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>Could you expand on this please? Isn't this going to be roughly equivalent to \"we kept our GitHub repo private\"?</p>\n", "parentCommentId": "LWGkkgbyYttyhFaDG", "user": {"username": "jooke"}}, {"_id": "BzKKTLudZpkqa4Yxa", "postedAt": "2024-01-05T08:06:10.130Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>Pascal's mugging should be addressed by a prior which is more sceptical of extreme estimates.</p>\n<p>GiveWell are approximating that process here:</p>\n<blockquote>\n<p>We\u2019re reluctant to take this estimate at face value because (i) this result has not been replicated elsewhere and (ii) it seems implausibly large given the more muted effects on intermediate outcomes (e.g., years of schooling).</p>\n</blockquote>\n", "parentCommentId": "cndYNDnBKmPnd95nB", "user": {"username": "jooke"}}, {"_id": "zxNYis7RrKARavtB7", "postedAt": "2024-01-05T08:10:52.581Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>I don't think this evaluation is especially useful, because it only presents one side of the argument. Why spreadsheets are bad, not their advantages or how errors typically occur in programming languages.</p>\n<p>The bottom line you present (quoted below) is in fact not very action relevant. It's not strong enough to even support that the switching costs are worth it IMO.</p>\n<blockquote>\n<p>We are far from certain that writing cost-effectiveness analyses in an ordinary programming language would reduce the error rate compared to spreadsheets - quantitative estimates of the error rate in both spreadsheets and in non-spreadsheet programs find error rates on the same order of magnitude. The mix of problems that are typically approached using these two types of tools is different though, and we have not found an apples-to-apples study of those error rates.</p>\n</blockquote>\n", "parentCommentId": "CZg3FJW9xNXywg7tz", "user": {"username": "jooke"}}, {"_id": "n95X64F9XpcuETm5x", "postedAt": "2024-01-05T08:29:49.940Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>If 2 holds, the risk of noise causing interventions to be re-ranked is small, because the noise distribution is more compressed than the true gap between interventions.</p>\n", "parentCommentId": "hhowSjfy2PsCgBDvF", "user": {"username": "therealslimkt"}}, {"_id": "8J2wfjBhTMzcsaff7", "postedAt": "2024-01-05T08:49:00.949Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>It's a potential solution, but I think it requires the prior to decrease quickly enough with increasing cost effectiveness, and this isn't guaranteed. So I'm wondering is there any analysis to show that the methods being used are actually robust to this problem e.g. exploring sensitivity to how answers would look if the deworming RCT results had been higher or lower and that they change sensibly?&nbsp;<br><br>A document that looks to give more info on the method used for deworming looks to be <a href=\"https://docs.google.com/document/d/1TLbeSws1PcIQ6RpzG-iGZYfTLc98cmm0K4zWNhqaQoQ/edit#heading=h.1gijujhxfwgv\">here</a>, so perhaps that can be built on - but from a quick look it doesn't seem to say exactly what shape is being used for the priors in all cases, though they look quite Gaussian from the plots.</p>", "parentCommentId": "BzKKTLudZpkqa4Yxa", "user": {"username": "Pagw"}}, {"_id": "FLDtLXFzmMHz5mDiM", "postedAt": "2024-01-05T09:30:30.872Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>The main point is that access management is more natively associated with the structure of the model in software settings. Say, you are less likely to release a model without its prerequisites.</p>\n<p>But I agree that this could also be messed up in software environments, and that it's mainly an issue of UI and culture. I guess I generally argue for a modeling environment that is \"modeling-first\" rather than something like \"explainable-results-first\".</p>\n", "parentCommentId": "fWKeo7mpeykeKbfrq", "user": {"username": "edoarad"}}, {"_id": "CmHm4tYTCDis6tHoM", "postedAt": "2024-01-05T09:34:08.188Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>Totally agree with the need for a more balanced and careful analysis!</p>\n", "parentCommentId": "zxNYis7RrKARavtB7", "user": {"username": "edoarad"}}, {"_id": "WpgC2RaxMiNjrqGNm", "postedAt": "2024-01-05T10:33:59.950Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>I agree.</p>\n<p>Reflecting, in the everything-is-Gaussian case a prior doesn't help much. Here, your posterior mean is <a href=\"https://en.m.wikipedia.org/wiki/Conjugate_prior#When_likelihood_function_is_a_continuous_distribution\">a weighted average of prior and likelihood</a>, with the weights depending only on the variance of the two distributions. So if the likelihood mean increases but with constant variance then your posterior mean increases linearly. You'd probably need a bias term or something in your model (if you're doing this formally).</p>\n<p>This might actually be an argument in favour of GiveWell's current approach, assuming they'd discount more as the study estimate becomes increasinly implausible.</p>\n", "parentCommentId": "8J2wfjBhTMzcsaff7", "user": {"username": "jooke"}}, {"_id": "dqQJe7GZGYfCEHJEp", "postedAt": "2024-01-05T14:08:21.561Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>A lognormal prior (and a normal likelihood function) might be a good starting point when adjusting for the statistical uncertainty in an effect size estimate. The resulting posterior cannot be calculated in closed form, but I have a simple website that calculates it using numerical methods. <a href=\"https://bayesupdate.com/?data={%22prior%22:%20{%22family%22:%20%22lognormal%22,%20%22normal%22:%20{%22param1%22:%20null,%20%22param2%22:%20null},%20%22lognormal%22:%20{%22param1%22:%201.0,%20%22param2%22:%200.5},%20%22beta%22:%20{%22param1%22:%20null,%20%22param2%22:%20null},%20%22uniform%22:%20{%22param1%22:%20null,%20%22param2%22:%20null},%20%22diff_log_betas%22:%20{%22param1%22:%20null,%20%22param2%22:%20null,%20%22param3%22:%20null,%20%22param4%22:%20null}},%20%22likelihood%22:%20{%22family%22:%20%22normal%22,%20%22normal%22:%20{%22param1%22:%2010.0,%20%22param2%22:%203.0},%20%22normal_95_ci_bool%22:%20false,%20%22normal_95_ci%22:%20{%22param1%22:%20null,%20%22param2%22:%20null},%20%22lognormal%22:%20{%22param1%22:%20null,%20%22param2%22:%20null},%20%22beta%22:%20{%22param1%22:%20null,%20%22param2%22:%20null},%20%22uniform%22:%20{%22param1%22:%20null,%20%22param2%22:%20null},%20%22binomial%22:%20{%22param1%22:%20null,%20%22param2%22:%20null}},%20%22graphrange%22:%20{%22param1%22:%20null,%20%22param2%22:%20null},%20%22custompercentiles%22:%20%22%22}\">Here's an example</a>.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dqQJe7GZGYfCEHJEp/jaxsn72covcxvnkgtsuq\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dqQJe7GZGYfCEHJEp/ove31y4pza6qxpcb4bza 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dqQJe7GZGYfCEHJEp/mwlikbepvy50dnrzerar 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dqQJe7GZGYfCEHJEp/v7ngbyc4mwuxoopvlhse 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dqQJe7GZGYfCEHJEp/lcqrlthbwgzxdgyfpov4 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dqQJe7GZGYfCEHJEp/arqvdznnpjn7famvlqed 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dqQJe7GZGYfCEHJEp/e090ohnxaxseaipzoqdv 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dqQJe7GZGYfCEHJEp/gsdvbozihjaytppglhyw 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dqQJe7GZGYfCEHJEp/boqx0yxtu2hvspqrfkyg 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dqQJe7GZGYfCEHJEp/p3eddclaukmbtjmlybpx 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dqQJe7GZGYfCEHJEp/itpaeg2zwkm2r9i9szfa 1302w\"></figure><hr><p>Worth noting that adjusting for the statistical uncertainty in an effect size estimate is quite different from adjusting for the totality of our uncertainty in a cost-effectiveness estimate. For doing the latter, it's unclear to me what likelihood function would be appropriate. I'd love to know if there are practical methods for choosing the likelihood function in these cases.</p><hr><p>GiveWell does seem to be using mostly normal priors in the document you linked. I don't have time to read the whole document and think carefully about what prior would be most appropriate. For its length (3,600 words including footnotes) the document doesn't appear to give much reasoning for the choices of distribution families.&nbsp;</p>", "parentCommentId": "8J2wfjBhTMzcsaff7", "user": {"username": "usedagger.com"}}, {"_id": "uMu5epF2JkvgWAwwK", "postedAt": "2024-01-05T14:15:28.479Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<blockquote>\n<p>exploring sensitivity to how answers would look if the deworming RCT results had been higher or lower and that they change sensibly?</p>\n</blockquote>\n<p>Do you just mean that the change in the posterior expectation is in the correct direction? In that case, we know the answer from theory: yes, for any prior and a wide range of likelihood functions.</p>\n<p><a href=\"https://github.com/tadamcz/value-of-information/blob/master/assets/andrews1972.pdf\">Andrews et al. 1972</a> (Lemma 1) shows that when the signal <code>B</code> is normally distributed, with mean <code>T</code>, then, for any prior distribution over <code>T</code>, <code>E[T|B=b]</code> is increasing in <code>b</code>.</p>\n<p>This was generalised by <a href=\"https://github.com/tadamcz/value-of-information/blob/master/assets/ma1999.pdf\">Ma 1999</a> (Corollary 1.3) to any likelihood function arising from a <code>B</code> that (i) has <code>T</code> as a location parameter, and (ii) is strongly unimodally distributed.</p>\n", "parentCommentId": "8J2wfjBhTMzcsaff7", "user": {"username": "usedagger.com"}}, {"_id": "C8M57n6xigCzC5Ha9", "postedAt": "2024-01-05T21:58:15.709Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>I guess it depends on what the \"correct direction\" is thought to be. From the reasoning quoted in my first post, it could be the case that as the study result becomes larger the posterior expectation should actually reduce. It's not inconceivable that as we saw the estimate go to infinity, we should start reasoning that the study is so ridiculous as to be uninformative and so not the posterior update becomes smaller. But I don't know. What you say seems to suggest that Bayesian reasoning could only do that for rather specific choices of likelihood functions, which is interesting.</p>", "parentCommentId": "uMu5epF2JkvgWAwwK", "user": {"username": "Pagw"}}, {"_id": "ppcyuAsp2XaxvC49k", "postedAt": "2024-01-17T21:44:25.269Z", "postId": "NWmBkMe3yF4GQNnai", "htmlBody": "<p>Hi Sanjay - thanks for the close read! You're right that Figure 3 should read 95%, not 90% - we're working on correcting the figure and will update the post ASAP. Thanks again!</p>\n", "parentCommentId": "MaBwa7A26qwSvtQCt", "user": {"username": "GiveWell"}}]