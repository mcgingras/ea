[{"_id": "b7mNL2asfwfafZQrb", "postedAt": "2023-02-20T17:19:34.307Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>Very clear - makes a point that I've been struggling to think about and explain to people. Thanks for writing this.</p>\n", "parentCommentId": null, "user": {"username": "lincolnq"}}, {"_id": "yZuPAt8fnQeW68hPf", "postedAt": "2023-02-20T19:06:01.826Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>Thank you!</p>\n", "parentCommentId": "b7mNL2asfwfafZQrb", "user": {"username": "rgb"}}, {"_id": "YQraQqANuAuYwbPDH", "postedAt": "2023-02-23T22:51:57.852Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<blockquote><p>We trust human self-reports about consciousness, which makes them an indispensable tool for understanding the basis of human consciousness (\u201cI just saw a square flash on the screen\u201d; \u201cI felt that pinprick\u201d).</p></blockquote><p>&nbsp;</p><p>I want to clarify that these are <i>examples</i> of self-reports about consciousness and not <i>evidence</i> of consciousness in humans. A p-zombie would be able to report these stimuli without subjective experience of them.&nbsp;</p><p>They are \"indispensable tools for understanding\" insofar as we already have a high credence in human consciousness.</p>", "parentCommentId": null, "user": {"username": "Rocket"}}, {"_id": "knCButw6hcoRG8kbX", "postedAt": "2023-02-23T22:55:38.842Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<blockquote><p>a very close connection between an entity\u2019s capacity to model its own mental states, and consciousness itself.</p></blockquote><p>&nbsp;</p><p>The 80k episode with David Chalmers includes some discussion of meta-consciousness and the relationship between awareness and awareness of awareness (of awareness of awareness...). &nbsp;Would recommend to anyone interested in hearing more!&nbsp;</p><p>They make the interesting analogy that we might learn more about God by studying how people think about God than by investigating God itself. Similarly we might learn more about consciousness by investigating how people think about it...</p>", "parentCommentId": null, "user": {"username": "Rocket"}}, {"_id": "ZTDKH9HFWD5bFKib2", "postedAt": "2023-02-23T23:46:49.087Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>This is true for literally all empirical evidence if you accept the possibility of a P-Zombie. The only possible falsification for consciousness can come from the internal subject itself, nothing else will do. But for everyone apart from <i>you</i>, it's self-reports, 3rd party observation, or nothing.</p><p><strong>Edit:</strong> What I mean here is that these self-reports are evidence - if they're not then there's no evidence for any minds apart from your own. And therefore we also ought to take AI self-reports as evidence. Not as serious as we take human self-reports at this stage, but evidence nonetheless.</p>", "parentCommentId": "YQraQqANuAuYwbPDH", "user": {"username": "JWS"}}, {"_id": "B5ZhM5qTeTMjgAMjH", "postedAt": "2023-02-24T01:18:22.470Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>Agree, that's a great pointer! For those interested, <a href=\"https://philpapers.org/archive/CHATMO-32.pdf\">here</a> is the paper and <a href=\"https://80000hours.org/podcast/episodes/david-chalmers-nature-ethics-consciousness/\">here</a> is the podcast episode.</p>\n<p>[Edited to add a nit-pick: the term 'meta-consciousness' is not used, it's the 'meta-problem of consciousness', which is the problem of explaining why people think and talk the way they do about consciousness]</p>\n", "parentCommentId": "knCButw6hcoRG8kbX", "user": {"username": "rgb"}}, {"_id": "qRH3fMqZ2wZNqTCuZ", "postedAt": "2023-02-24T02:07:29.781Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>Thanks for the comment. A couple replies:</p>\n<blockquote>\n<p>I want to clarify that these are examples of self-reports about consciousness and not evidence of consciousness in humans.</p>\n</blockquote>\n<p>Self-report is evidence of consciousness in Bayesian sense (and in common parlance): in a wide range of scenarios, if a human says they are conscious of something, you should have a higher credence than if they do not say they are. And in the scientific sense: it's commonly and appropriately taken as evidence in scientific practice; here is Chalmers's <a href=\"https://consc.net/papers/scicon.html\">\"How Can We Construct a Science of Consciousness?\"</a> on the practice of using self-reports to gather data about people's conscious experiences:</p>\n<blockquote>\n<p>Of course our access to this data depends on our making certain assumptions: in particular, the assumption that other subjects really are having conscious experiences, and that by and large their verbal reports reflect these conscious experiences. We cannot directly test this assumption; instead, it serves as a sort of background assumption for research in the field. But this situation is present throughout other areas of science. When physicists use perception to gather information about the external world, for example, they rely on the assumption that the external world exists, and that perception reflects the state of the external world. They cannot directly test this assumption; instead, it serves as a sort of background assumption for the whole field. Still, it seems a reasonable assumption to make, and it makes the science of physics possible. <em>The same goes for our assumptions about the conscious experiences and verbal reports of others. These seem to be reasonable assumptions to make, and they make the science of consciousness possible .</em></p>\n</blockquote>\n<p>It's suppose it's true that self-reports can't budge someone from the hypothesis that other actual people are p-zombies, but few people (if any) think that. From the <a href=\"https://plato.stanford.edu/entries/zombies/\">SEP</a>:</p>\n<blockquote>\n<p>Few people, if any, think zombies actually exist. But many hold that they are at least conceivable, and some that they are possible....The usual assumption is that none of us is actually a zombie, and that zombies cannot exist in our world. The central question, however, is not whether zombies can exist in our world, but whether they, or a whole zombie world (which is sometimes a more appropriate idea to work with), are possible in some broader sense.</p>\n</blockquote>\n<p>So  yeah: my take is that no one, including  anti-physicalists who discuss p-zombies like Chalmers, really thinks that we can't use self-report as evidence, and correctly so.</p>\n", "parentCommentId": "YQraQqANuAuYwbPDH", "user": {"username": "rgb"}}, {"_id": "ib3BvWSxBCMkQ6oL7", "postedAt": "2023-02-24T05:17:29.690Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p><strong>Post summary (feel free to suggest edits!):</strong><br>Argues that statements by large language models that seem to report their internal life (eg. \u2018I feel scared because I don\u2019t know what to do\u2019), isn't straightforward evidence either for or against the sentience of that model. As an analogy, parrots are probably sentient and very likely feel pain. But when they say \u2018I feel pain\u2019, that doesn\u2019t mean they are in pain.</p><p><br>It might be possible to train systems to more accurately report if they are sentient, via removing any other incentives for saying conscious-sounding things, and training them to report their own mental states. However, this could advance dangerous capabilities like situational awareness, and training on self-reflection might also be what ends up<i> making&nbsp;</i>a system sentient.<br><br>(This will appear in this week's forum summary. If you'd like to see more summaries of top EA and LW forum posts, check out the <a href=\"https://forum.effectivealtruism.org/s/W4fhpuN26naxGCBbN\">Weekly Summaries</a> series.)</p>", "parentCommentId": null, "user": {"username": "GreyArea"}}, {"_id": "bPM88W5yKGF5tW49K", "postedAt": "2023-02-24T07:28:48.615Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>I like it! I think one thing the post itself could have been clearer on is that reports <em>could</em> be indirect evidence for sentience, in that they are evidence of certain capabilities that are themselves evidence of sentience. To give an example (though it\u2019s still abstract), the ability of LLMs to fluently mimic human speech \u2014&gt; evidence for capability C\u2014&gt; evidence for sentience. You can imagine the same thing for parrots: ability to say \u201cI\u2019m in pain\u201d\u2014&gt; evidence of learning and memory \u2014&gt; evidence of sentience. But what they aren\u2019t are <em>reports</em> of sentience.</p>\n<p>so maybe at the beginning: aren\u2019t \u201cstrong evidence\u201d or \u201cstraightforward evidence\u201d</p>\n", "parentCommentId": "ib3BvWSxBCMkQ6oL7", "user": {"username": "rgb"}}, {"_id": "Bn5N3i4tLNi7YsLSf", "postedAt": "2023-02-25T02:14:36.296Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>Fixed, thanks!</p>", "parentCommentId": "bPM88W5yKGF5tW49K", "user": {"username": "GreyArea"}}, {"_id": "4DrbHeHwNRLn6uuRu", "postedAt": "2023-02-25T04:48:03.136Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>Thanks for following up and thanks for the references! Definitely agree these statements are evidence; I should have been more precise and said that they're weak evidence / not likely to move your credences in the existence/prevalence of human consciousness.</p>", "parentCommentId": "qRH3fMqZ2wZNqTCuZ", "user": {"username": "Rocket"}}, {"_id": "JzPvBwuXHXZ35SrzB", "postedAt": "2023-02-26T14:54:27.794Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>Thanks for writing this!</p><p>In my view, everything is sentient in expectation, in the sense that everything has a positive expected moral weight for the reasons described by Brian Tomasik <a href=\"https://reducing-suffering.org/is-there-suffering-in-fundamental-physics/\">here</a>. So I think the relevant question is not whether LLMs are sentient (they are in expectation), but rather:</p><ul><li>What is the expected moral weight of LLMs (as a function of their properties, such as number of parameters)?</li><li>What can we do (if anything) to increase the wellbeing of LLMs?</li></ul><p>These are obviously super hard questions, but they are so important and neglected that research on them may well be cost-effective.</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "BAi6mjYWDi2uLMKSW", "postedAt": "2023-02-27T22:33:22.264Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>The Brian Tomasik post you link to considers the view that fundamental physical operations may have moral weight (call this view \"Physics Sentience\").&nbsp;</p><p>[Edit: see Tomasik's comment <a href=\"https://forum.effectivealtruism.org/posts/P4ut25NhfsFEMEeLJ/what-to-think-when-a-language-model-tells-you-it-s-sentient?commentId=a3fLBsDuEk3XAPgDr\">below</a>. What I say below is true of a different sort of Physics Sentience view like <a href=\"https://plato.stanford.edu/entries/panpsychism/#CombProb\">constitutive micropsychism</a>, but not necessarily of Brian's own view, which has somewhat different motivations and implications]</p><p>But even if true, <strong>[many versions of] </strong>Physics Sentience <strong>[but not necessarily Tomasik's] </strong>doesn't have straightforward implications about what high-level systems, like organisms and AI systems, also comprise a sentient subject of experience. Consider: a human being touching a stove is experiencing pain on Physics Sentience; but a pan touching a stove is not experiencing pain. On Physics Sentience, the pan is made up of sentient matter, but this doesn't mean that the pan qua pan is also a moral patient, another subject of experience that will suffer if it touches the stove.</p><p>To apply this to the LLMs case:&nbsp;</p><p>-Physics Sentience will hold that the hardware on which LLMs run is sentient - after all, it's a bunch of fundamental physical operations.</p><p>-But Physics Sentience will also hold that the hardware on which a giant lookup table is running is sentient, to the same extent and for the same reason.</p><p>-Physics Sentience is silent on whether there's a difference between (1) and (2), in the way that there's a difference between the human and the pan.</p><p>The same thing holds for other panpsychist views of consciousness, fwiw. Panpsychist views that hold that fundamental matter is consciousness don't tell us anything, themselves, about what animals or AI systems are sentient. It just says they are made of conscious (or proto-conscious) matter.</p>", "parentCommentId": "JzPvBwuXHXZ35SrzB", "user": {"username": "rgb"}}, {"_id": "2kmMn2CNrjJByyrye", "postedAt": "2023-02-28T11:11:27.330Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>Thanks for the clarification!</p><p>I linked to Brian Tomasik's post to provide useful context, but I wanted to point to a more general argument: we do not understand sentience/consciousness well enough to claim LLMs (or whatever) have null expected moral weight.</p>", "parentCommentId": "BAi6mjYWDi2uLMKSW", "user": {"username": "vascoamaralgrilo"}}, {"_id": "Db27gyKqM6DzZZAgT", "postedAt": "2023-02-28T20:51:06.510Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>Ah, thanks! Well, even if it wasn't appropriately directed at your claim, I  appreciate the opportunity to rant about how panpsychism (and related views) don't entail AI sentience :)</p>\n", "parentCommentId": "2kmMn2CNrjJByyrye", "user": {"username": "rgb"}}, {"_id": "a3fLBsDuEk3XAPgDr", "postedAt": "2023-03-08T04:14:11.640Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>Unlike the version of panpsychism that has become fashionable in philosophy in recent years, my version of panpsychism is based on the fuzziness of the concept of consciousness. My view is involves attributing consciousness to all physical systems (including higher-level ones like organisms and AIs) to the degree they show various properties that we think are important for consciousness, such as perhaps a global workspace, higher-order reflection, learning and memory, intelligence, etc. I'm a panpsychist because I think at least some attributes of consciousness can be seen even in fundamental physics to a non-zero degree. However, I personally would attribute much more consciousness to an LLM than to a rock that has equal mass as the machines running the LLM. I think it's less obvious whether an LLM is more sentient than a collection of computers doing an equal number of more banal computations, such as database queries or video-game graphics.</p>\n", "parentCommentId": "Db27gyKqM6DzZZAgT", "user": {"username": "Brian_Tomasik"}}, {"_id": "kiABjjwFjXFvcwmqr", "postedAt": "2023-03-17T17:14:02.069Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>Hi Brian! Thanks for your reply. I think you're quite right to distinguish between your flavor of panpsychism and the flavor I was saying doesn't entail much about LLMs. I'm going to update my comment above to make that clearer, and sorry for running together your view with those others.</p>\n", "parentCommentId": "a3fLBsDuEk3XAPgDr", "user": {"username": "rgb"}}, {"_id": "2QEmkQ9GrKWoQQAZq", "postedAt": "2023-03-17T17:35:19.364Z", "postId": "P4ut25NhfsFEMEeLJ", "htmlBody": "<p>No worries. :) The update looks good.</p>\n", "parentCommentId": "kiABjjwFjXFvcwmqr", "user": {"username": "Brian_Tomasik"}}]