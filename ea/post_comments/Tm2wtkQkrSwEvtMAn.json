[{"_id": "XvYnvgzggGvEoxmvG", "postedAt": "2023-03-04T00:18:30.818Z", "postId": "Tm2wtkQkrSwEvtMAn", "htmlBody": "<blockquote><p>An important idea here is that A and B might have something of value to offer each other, despite the <strong>presence </strong>of a (physically) causal communication channel.&nbsp;&nbsp;</p></blockquote><p>Should this read \"absence of a (physically) causal communication channel\"? I'm confused by this sentence as stated</p>", "parentCommentId": null, "user": {"username": "elephantower"}}, {"_id": "hKkKQYwsum2EmPrHu", "postedAt": "2023-03-04T17:55:00.249Z", "postId": "Tm2wtkQkrSwEvtMAn", "htmlBody": "<p>I\u2019ve mostly arrived at similar conclusions through <a href=\"https://longtermrisk.org/msr\">Evidential Cooperation in Large Worlds</a> (ECL; see, e.g., Lukas\u2019s commentary). ECL adds a few extra steps that make the implications a lot clearer to me (though not to the extent of them actually being clear). I\u2019d be curious how they transfer to acausal normalcy!</p><p><strong>Set of cooperators.</strong> When it comes acausal normalcy, what is the set of all cooperation partners? I imagine it\u2019s everyone who honestly tries to figure out acausal normalcy and tries to adhere to it, so that people who try to exploit it are automatically expelled from the set of cooperation partners? That\u2019s how it works with ECL. There could be correlations between beings who are wont to cooperate vs. defect (be it out of ignorance), so this could rule out some moral norms.</p><p><strong>Circular convincingness.</strong> In the ECL paper, Caspar cites some more and less formal studies of the efficacy of acausal cooperation on earth. They paint a bit of a mixed picture. The upshot was something like that acausal cooperation can\u2019t be trusted with few participants. There were some arguments that it works in elections (a few million participants), but I don\u2019t remember the details. So the step that the universe is big (possibly infinite) was critical to convince every actor that acausal cooperation is worth it and thus convince them that others out there will also consider it worth it.</p><p>Is this covered by Payor\u2019s Lemma? I\u2019m still trying to wrap my head around it, especially which of the assumptions are ones it proves and which are assumptions I need to make to apply it\u2026 It also looks a bit funky to me. Is the character missing from the typeface or is there meant to be a box there?</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hKkKQYwsum2EmPrHu/ufvopv5vtkhiwhyxwxf0\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hKkKQYwsum2EmPrHu/w4szlxxjdhtddbljncxi 93w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hKkKQYwsum2EmPrHu/ryhzm50sygomoxtyyp0f 173w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hKkKQYwsum2EmPrHu/yopuhefvce1gwwibpeb8 253w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hKkKQYwsum2EmPrHu/pyz8b6t4hce2vgbxcw4x 333w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hKkKQYwsum2EmPrHu/boh5t9biyupaqsdxgxax 413w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hKkKQYwsum2EmPrHu/il5sizraq9moh4btywyf 493w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hKkKQYwsum2EmPrHu/l8tzutefhwehos1rmhmb 573w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hKkKQYwsum2EmPrHu/paq2pklweje0z1b0lbuf 653w\"></figure><p><strong>Power.</strong> Another problem that I feel quite uncertain about is power. In the extremes, someone powerless will be happy to acausally cooperate rather than be crushed, and someone all-powerful who knows it (i.e. there\u2019s no one else equally or more powerful) has probably no interest in acausal cooperation. The second one of these beings is probably impossible, but there seems to be a graduation where the more powerful a being is, the less interested it is in cooperation. This is alleviated by the uncertainty even the powerful beings will have about their relative rank in the power hierarchy, and norms around pride and honor and such where beings may punish others for defections even at some cost to themselves. I don\u2019t see why these alleviating factors should precisely cancel out the greater power. I would rather expect there to be complex tradeoffs.&nbsp;</p><p>That all makes me inclined to think that the moral norms of more powerful beings should be weighed more (a bit less than proportionally more) than the moral norms of less powerful beings. These powerful beings are of course superintelligences and grabby aliens but also our ancestors. (Are there already writings on this? I\u2019m only aware of a fairly short Brian Tomasik essay that came to a slightly different conclusion.)</p><p><strong>Convergent drives.</strong> Self-preservation, value preservation, and resource acquisition may be convergent drives of powerful beings because they would otherwise perhaps not be powerful. Self-preservation implies existence in the first place, so that it may be optimal to help these powerful beings to come into existence (but which ones?). Value preservation is a difficult one since whatever powerful being ends up existing might\u2019ve gotten there only because its previous versions still value-drifted. Resource acquisition may push toward instilling some indexical uncertainty in AIs so that they can figure out whether we\u2019re in a simulation, and whether we can control or trade with higher levels or other branches. I feel highly unsure about these implications, but I think it would be important to get them right.</p><p><strong>Idealization.</strong> But the consideration of our ancestors brings me to my next point of confusion, which is how to idealize norms. You mention the the distinction between instrumental and terminal goals. I think it\u2019s an important one. E.g., maybe monogamy limited the spread of STDs among other benefits; some of these are less important now with tests, vaccinations, treatments, condoms, etc. So if our ancestors valued monogamy instrumentally, we don\u2019t need to continue upholding it to cooperate with them even though they have a lot of power over our very existence. But if they valued it terminally, we might have to! Perhaps I really have to bite bullet on this one (if I understand you correctly) and turn it into a tradeoff between the interests of powerful people in the past with weird norms and the interests of less powerful people today\u2026</p><p><strong>Positive and negative reinforement.</strong> When it comes to the particular values, especially reinforcement learning stands out to me. When a being is much smaller than its part of the universe, it\u2019ll probably need to learn, so my guess is that reinforcement learning is very common among all sorts of beings. Buck once argued that any kind of stimulation, positive or negative, expends energy, so that we\u2019re calibrated such that most beings who survive and reproduce will spend most the time close the energy-conserving neutral state. That sounds pretty universal (though perhaps there are civilizations where only resources other than energy are scarce), so that the typical utilitarian values of increasing happiness and reducing suffering may be very common too.</p><p>(When I was new to EA, I read a lot of Wikipedia and SEP articles on various moral philosophies. My impression was that reducing suffering is \u201cnot dispreferred\u201d by all of them (or to the extent that some of them might\u2019ve been explicitly pro suffering, it was in specific, exceptional instances such as retribution) whereas all other norms were either dispreferred by some moral philosophies or really specific. My tentative guess at the time was that the maximally morally cooperative actions are those that reduced suffering while not running into any complicated tradeoffs against other values.)</p><p><strong>Unique optimal ethics.</strong> An interesting observation is that ECL implies that there is one optimal compromise morality \u2013 that anyone who deviates from it leaves gains from moral trade on the table. So my own values should inform my actions only to a very minor degree. (I\u2019m one sample among countless samples when it comes to inferring the distribution of values across the universe.) Instead my actions should be informed by the distribution of all values around the universe to the extent that I can infer it; by the distribution of relevant resources around the universe; and by the resources I have causally at my disposal, which could put me in a &nbsp;comparatively advantageous position to benefit some set of values. I wonder, is there one optimal acausal normalcy too?&nbsp;</p>", "parentCommentId": null, "user": {"username": "Telofy"}}, {"_id": "kdnrFuYCgCNtnwSmk", "postedAt": "2023-03-11T12:35:24.076Z", "postId": "Tm2wtkQkrSwEvtMAn", "htmlBody": "<p>Isn't an acausal norm equivalent to a goal-directed norm? If not, then what's the difference?</p>", "parentCommentId": null, "user": {"username": "Vynn"}}, {"_id": "ZHndXn4tmBABHZwry", "postedAt": "2023-03-12T10:56:35.483Z", "postId": "Tm2wtkQkrSwEvtMAn", "htmlBody": "<p>Very interesting post, thanks for writing this!</p><blockquote><p><strong>1. Simulations are not the most efficient way for A and B to reach their agreement.</strong> Rather, writing out arguments or formal proofs about each other is much more computationally efficient, because nested arguments naturally avoid stack overflows in a way that nested simulations do not.&nbsp; In short, each of A and B can write out an argument about each other that self-validates without an infinite recursion.&nbsp; There are several ways to do this, such as using L\u00f6b's Theorem-like constructions (as in this<a href=\"https://acritch.com/papers/parametric-bounded-lob.pdf,\">&nbsp;<u>2019 JSL paper</u></a>), or even more simply and efficiently using Payor's Lemma (as in this<a href=\"https://www.lesswrong.com/posts/2WpPRrqrFQa6n2x3W/modal-fixpoint-cooperation-without-loeb-s-theorem\">&nbsp;<u>2023 LessWrong post</u></a>).</p></blockquote><p>I'm wondering to what extent this is the exact same as <a href=\"https://longtermrisk.org/msr\">Evidential Cooperation in Large Worlds</a>, with which you don't need simulations because you cooperate only with the agents that are <a href=\"https://www.lesswrong.com/posts/HLXiJgqxuMpwamdar/conditions-for-superrationality-motivated-cooperation-in-a\">decision-entangled</a> with you (i.e., those you can prove will cooperate if you cooperate). While not needing simulation is an advantage, &nbsp;the big limitation of <a href=\"https://longtermrisk.org/msr\">Evidential Cooperation in Large Worlds</a> is that the sample of agents you can cooperate with is fairly small (since they need to be decision-entangled with you).<br><br>The whole point of nesting simulations -- and classic <a href=\"https://www.lesswrong.com/tag/acausal-trade\">acausal trade</a>-- is to create some form of artificial/\"indirect\" decision-entanglement with agents who would otherwise not be entangled with you (by creating a channel of \"communication\" that makes the players able to see what the other is actually playing so you can start implementing a tit-for-tat strategy). Without those simulations, you're limited to the agents you can prove will necessarily cooperate if you cooperate (without any way to verify/coordinate via mutual simulation). (Although one might argue that you can hardly simulate agents you can't prove anything about or are not (close to) decision-entangled with, anyway.)<br><br>So is your idea basically <a href=\"https://longtermrisk.org/msr\">Evidential Cooperation in Large Worlds</a> explained in another way or is it something in between that and classic acausal trade?&nbsp;</p>", "parentCommentId": null, "user": {"username": "Jim Buhler"}}]