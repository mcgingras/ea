[{"_id": "ucqPQ5sgMsj5WMpsT", "title": "Jeff Bezos announces donation plans (in response to question)", "postedAt": "2022-11-14T14:31:53.922Z", "htmlBody": "<p>For a change, some good&nbsp; <a href=\"https://www.google.com/amp/s/amp.cnn.com/cnn/2022/11/14/business/jeff-bezos-charity/index.html\">billionaire philanthropy news</a></p>\n<p>Caveats:</p>\n<ul>\n<li>It seems to have been in response to the question \"Do you plan to give away the majority of your wealth in your lifetime?\"; I don't know whether he encouraged them to ask this</li>\n<li>I suspect this is not entirely 'news'; iirc he has made noises like this in the past</li>\n</ul>\n<blockquote>\n<p>Amazon founder Jeff Bezos plans to give away the majority of his $124 billion net worth during his lifetime, telling CNN in an exclusive interview he will devote the bulk of his wealth to fighting climate change and supporting people who can unify humanity in the face of deep social and political divisions.</p>\n</blockquote>\n<p>This seems potentially promising:&nbsp; he seems to prioritize effectiveness.</p>\n<blockquote>\n<p>\u201cThe hard part is figuring out how to do it in a levered way,\u201d he said, implying that even as he gives away his billions, he is still looking to maximize his return. \u201cIt\u2019s not easy. Building Amazon was not easy. It took a lot of hard work, a bunch of very smart teammates, hard-working teammates, and I\u2019m finding \u2014 and I think Lauren is finding the same thing \u2014 that charity, philanthropy, is very similar.\u201d<br>\n\u201cThere are a bunch of ways that I think you could do ineffective things, too,\u201d he added. \u201cSo you have to think about it carefully and you have to have brilliant people on the team.\u201d</p>\n<p>Bezos\u2019 methodical approach to giving stands in sharp contrast to that of his ex-wife, the philanthropist MacKenzie Scott, who recently <a href=\"https://www.cnn.com/2022/03/23/tech/mackenzie-scott-donations\">gave away nearly $4 billion to 465 organizations</a> in the span of less than a year.</p>\n</blockquote>\n<p>In terms of specifics, the Earth Fund seems relatively good, to me:</p>\n<blockquote>\n<p>Bezos has committed $10 billion over 10 years, or about 8% of his current net worth, to the Bezos Earth Fund, which S\u00e1nchez co-chairs. Among its priorities are reducing the carbon footprint of construction-grade cement and steel; pushing financial regulators to consider climate-related risks; advancing data and mapping technologies to monitor carbon emissions; and building natural, plant-based carbon sinks on a large scale.</p>\n</blockquote>\n<p>I\u2019m less enthusiastic about the \u201cBezos Courage and Civility Award\u201d which seems celebrity-driven (as much as I love Dolly Parton) and perhaps less likely to target global priorities/effectiveness.</p>\n", "user": {"username": "david_reinstein"}}, {"_id": "sEpWkCvvJfoEbhnsd", "title": "The FTX crisis highlights a deeper cultural problem within EA - we don't sufficiently value good governance", "postedAt": "2022-11-14T12:43:56.822Z", "htmlBody": "<h2>Introduction</h2><p>In this piece, I will explain why I don't think the collapse of FTX and resulting fallout for Future Fund and EA community in general is a one-off or 'black swan' event as some have argued on this forum. Rather, I think that what happened was part of a broader pattern of failures and oversights that have been persistent within EA and EA-adjacent organisations since the beginning of the movement.</p><p>As a disclaimer, I do not have any inside knowledge or special expertise about FTX or any of the other organisations I will mention in this post. I speak simply as a long-standing and concerned member of the EA community.</p><h2>Weak Norms of Governance</h2><p>The essential point I want to make in this post is that the EA community has not been very successful in fostering norms of transparency, accountability, and institutionalisation of decision-making. Many EA organisations began as <i>ad hoc</i> collections of like-minded individuals with very ambitions goals but relatively little career experience. This has often led to inadequate organisational structures and procedures being established for proper management of personal, financial oversight, external auditing, or accountability to stakeholders. Let me illustrate my point with some major examples I am aware of from EA and EA-adjacent organisations:</p><ol><li>Weak governance structures and financial oversight at the <a href=\"https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si\">Singularity Institute</a>, leading to the theft of over $100,000 in 2009.</li><li>Inadequate record keeping, rapid executive turnover, and insufficient board oversight at the <a href=\"https://www.centreforeffectivealtruism.org/our-mistakes\">Centre for Effective Altruism</a> over the period 2016-2019.</li><li>Inadequate financial record keeping at <a href=\"https://80000hours.org/about/credibility/evaluations/mistakes/#accounting-behind\">80,000 Hours</a> during 2018.</li><li>Insufficient oversight, unhealthy power dynamics, and other harmful practices reported at <a href=\"https://www.lesswrong.com/posts/MnFqyPLqbiKL8nSR7/my-experience-at-and-around-miri-and-cfar-inspired-by-zoe\">MIRI/CFAR</a> during 2015-2017.</li><li>Similar problems reported at the EA-adjacent organisation <a href=\"https://medium.com/@zoecurzi/my-experience-with-leverage-research-17e96a8e540b#3b11\">Leverage Research</a> during 2017-2019.</li><li>'Loose norms around board of directors and conflicts of interests between funding orgs and grantees' at <a href=\"https://forum.effectivealtruism.org/posts/efGNMe6uB87qXozXJ/ny-times-on-the-ftx-implosion-s-impact-on-ea\">FTX and the Future Fund</a> from 2021-2022.</li></ol><p>While these specific issues are somewhat diverse, I think what they have in common is an insufficient emphasis on principles of good organisational governance. This ranges from the most basic such as clear objectives and good record keeping, to more complex issues such as external auditing, good systems of accountability, transparency of the organisation to its stakeholders, avoiding conflicts of interest, and ensuring that systems exist to protect participants in asymmetric power relationships. I believe that these aspects of good governance and robust institution building have not been very highly valued in the broader EA community. In my experience, EAs like to talk about philosophy, outreach, career choice, and other nerdy stuff. Discussing best practise of organisational governance and systems of accountability doesn't seem very high status or 'sexy' in the EA space. There has been some discussion of such issues on this forum (e.g. <a href=\"https://forum.effectivealtruism.org/posts/5ZznqbRthKCbAB9Fk/some-benefits-and-risks-of-failure-transparency\">this thoughtful post</a>), but overall EA culture seems to have failed to properly absorb these lessons.</p><p>EA projects are often run by small groups of<a href=\"https://forum.effectivealtruism.org/posts/ThdR8FzcfA8wckTJi/ea-survey-2020-demographics\"> young idealistic people who have similar educational and social backgrounds</a>, who often socialise together, and (in many cases) participate in romantic relationships with one another - The case of Sam Bankman-Fried and Caroline Ellison is certainly not the only such example in the EA community. The EA culture seems to be heavily influenced by <a href=\"https://80000hours.org/2015/08/why-is-80000-hours-in-y-combinator-as-a-non-profit-and-whats-it-like/\">start-up culture and entrepreneurialism</a>, with a focus on moving quickly and relying on finding highly-skilled and highly-aligned people and then providing them funding and space to work with minimal oversight. A great deal of reliance is placed on personal relationships and trust in the well-meaning of fellow EAs. Of course reliance on trust is not bad in itself and is hardly unique to EA, however I think in the context of the EA community this has led to a relative disinterest in building sound and robust institutional structures.</p><h2>Responding to Rebuttals</h2><p>At this point I want to acknowledge some obvious rejoinders. First, I realise that governance has progressively improved at many of the older EA organisations over time. Nevertheless, based on my personal experience and reading of various organisational reports, as well as the obvious recent case of FTX, problems of weak governance and the lack of priority these receives in the EA community is still a major issue. Second, it is also true that many social movements or groups experience similar problems, and as I have no data on the issue I make no claim as to whether they are more or less common in EA compared to comparable movements or communities. Nevertheless, I think governance norms are still serious issues for the EA community, even if they are also issues for other groups or movements.</p><h2>The Need for Better Norms</h2><p>So what, specifically, am I proposing? Again, I want to emphasise the point of this post is not to critique specific organisations like CEA or 80k, or argue about what they should change in any particular way. Rather, my goal is to encourage people in the EA community to internalise and better implement some of the core values of<a href=\"https://www.bpe.al/en/12-principles-good-governance#\"> good governance</a> and institutional design. Some of these include:</p><ol><li>Accountability: Who is in charge? Who reports to whom, about what, and how often? Who is accountable for particular decisions?</li><li>Consideration of stakeholders: Who is affected by the actions and choices of an organisation or project? How are their interests incorporated into decision-making? Is leadership adequately accountable to stakeholders?</li><li>Avoidance of conflicts of interest: Are conflicts of interests present due to personal, organisational, or financial ties? What procedures exist for identifying and reporting such conflicts? Are stakeholders adequately informed about actual or perceived conflicts of interests?</li><li>Decision-making procedures: What formal procedures exist for arriving at important decisions? How is stakeholder feedback sought and incorporated? What, how, and where are records of decision processes kept? Who makes which types of decisions, and how are they held accountable for them?</li><li>Power dynamics: What procedures exist for protecting parties in asymmetric power relationships? Are there adequate opportunities for anonymous complaints or concerns to be raised? How are high-status individuals held accountable in the event of wrongdoing?</li></ol><p>Some readers may see these principles as rather stuffy or irrelevant to much EA practise, but I think this attitude is precisely the problem. More consistent and considered application of these principles would, I believe, have significantly reduced the severity of many of the problems in EA organisations mentioned previously. While its not necessary for every local group or every small project to have elaborate institutional formalisms or extensive documentation, practising the principles of good governance is in my view valuable for everyone, and should be something we regularly consider and discuss as EAs. I am <i>not </i>saying we should forget the values of dynamism or tight-knit communities, or that we should all turn into bureaucrats. I <i>am </i>saying that as a community, I don't think we take good governance seriously enough or spend enough time thinking about it. Also it should go without saying that the more power and responsibility a person or organisation acquires, and the more money they have stewardship over, the more important these principles become.</p><p>My overall message, therefore, is that good governance matters, strong institutions are important, and relying extensively on informal interpersonal bonds is often insufficient and prone to problems. I hope the EA community can continue to learn from its mistakes and seek to better internalise and actualise the values of good governance.</p>", "user": {"username": "Fods12"}}, {"_id": "iCYxXmFri5QSj9K6J", "title": "Lessons from taking group members to an EAGx conference", "postedAt": "2022-11-14T12:03:47.381Z", "htmlBody": "<figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668427594/mirroredImages/iCYxXmFri5QSj9K6J/ceq3spclz935ube4uv41.jpg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5330dfa26f57b42d10086b5517be6eec20884e577d1e24ad.jpeg/w_310 310w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5330dfa26f57b42d10086b5517be6eec20884e577d1e24ad.jpeg/w_620 620w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5330dfa26f57b42d10086b5517be6eec20884e577d1e24ad.jpeg/w_930 930w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5330dfa26f57b42d10086b5517be6eec20884e577d1e24ad.jpeg/w_1240 1240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5330dfa26f57b42d10086b5517be6eec20884e577d1e24ad.jpeg/w_1550 1550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5330dfa26f57b42d10086b5517be6eec20884e577d1e24ad.jpeg/w_1860 1860w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5330dfa26f57b42d10086b5517be6eec20884e577d1e24ad.jpeg/w_2170 2170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5330dfa26f57b42d10086b5517be6eec20884e577d1e24ad.jpeg/w_2480 2480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5330dfa26f57b42d10086b5517be6eec20884e577d1e24ad.jpeg/w_2790 2790w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5330dfa26f57b42d10086b5517be6eec20884e577d1e24ad.jpeg/w_3016 3016w\"></figure><p>We (Irene and Jelle) run the EA Eindhoven university group and went to&nbsp;<a href=\"https://www.eaglobal.org/events/eagxrotterdam-2022/\"><strong><u>EAGxRotterdam</u></strong></a> (4-6 November 2022) with 16 of our members.</p><p>Our members told us they really enjoyed the conference and have plans for how they want to pursue their EA journeys. We are excited that almost all Dutch universities have&nbsp;<a href=\"https://effectiefaltruisme.nl/sub-groups/\"><u>EA groups</u></a> now and felt EAGxRotterdam was the capstone of this year in which the Dutch EA community has really taken off.</p><p>These are some of the lessons we learned and best practices we discovered in our preparation for the conference as well as what we did during the conference itself. We hope other group organizers can benefit from these lessons too. We of course hope to visit many more EA conferences in the future and grow as community builders, which means this guide is a work in progress. We are also excited to learn about the best practices of other community builders and welcome their suggestions. Feel free to place comments!</p><h1><strong>Some things to keep in mind when reading this</strong></h1><p>Circumstance-specific things that were the case for us:</p><ul><li>Our group is only a few months old, our members all became engaged with EA only recently (most of them through our Introduction Fellowship) and our two organizers were the only people who had ever attended an EA conference before. This is why we put quite a lot of effort into encouraging members to apply and helping them prepare.</li><li>Rotterdam is only a 1-hour train ride from Eindhoven, so it was relatively easy for us to convince members to attend. Some of our members stayed with friends or traveled back and forth every day.</li><li>The conference was on the weekend between two exam weeks at our university and a few of our members cited this as a reason for not applying. One of our members got accepted but never showed up to the conference in order to work on a class assignment. We really regret these things but do not know what we could have done about them.</li><li>Because this conference in the Netherlands was such a rare event, we also advised some members to apply even though we were not 100% sure if they were engaged enough in EA. We would probably be stricter with our advice to them about this for conferences that are further away.<ul><li>For members who had not done an Introduction Fellowship (or equivalent), we made it clear that the conference was not going to be useful if they did not do some kind of preparation. We agreed with them that they would go over the&nbsp;<a href=\"https://forum.effectivealtruism.org/handbook\"><u>EA Handbook</u></a> and scheduled a 1-on-1 to discuss the preparation. In the end, all people who were interested were willing to do this preparation and carried it out. We spoke to them after to conference and they told us they found the conference interesting and returned with new ideas.</li></ul></li><li>We had 3 people show up at our collective application night, which is not a lot, but they all applied. We had approximately 10 people in total show up at our preparation evenings (we hosted 2 in a student caf\u00e9 on our campus).</li><li>We were both volunteers at EAGxRotterdam, had a lot of 1-on-1s scheduled and Jelle was also a speaker. Irene also met a community builder at the conference who was mainly there to guide his members, but in our case, that was not our only priority.</li></ul><h1><strong>Encourage and help members to apply</strong></h1><ul><li>Plan other programs (especially the Introduction Fellowship) so that they finish in time to still have space for promoting the conference and giving members the time to apply.<ul><li>Pitch the conference during your programs and events</li></ul></li><li>Host a collective application night<ul><li>Guide for&nbsp;<a href=\"https://docs.google.com/document/d/1VpH72G81G3inWmenq8oa4cRfu5vADOZmDrY6pv2KRDo/edit?usp=sharing\"><u>Collective application night</u></a> by the EAGxRotterdam team that we used</li></ul></li><li>Focus on members you think would benefit most from the conference<ul><li>Members who have already done at least an Introduction Fellowship (or equivalent)</li><li>Have 1-on-1s with these members</li></ul></li><li>Make sure members know that the conference is free for students and that they can receive travel compensation if that is necessary for them. For some of our members, this was also not clear from the webpage of EAGxRotterdam, and they only applied after we repeated this message.</li></ul><h1><strong>Help members prepare</strong></h1><ul><li>Make a group chat.</li><li>Share information about claiming the conference ticket, preparation advice, what the schedule of an EA conference generally looks like (so they have some idea even if Swapcard has not been published yet), and ways to coordinate travel and accommodation options<ul><li>The guide we made for EAGxRotterdam:&nbsp;<a href=\"https://docs.google.com/document/d/1VPlZljo3GS7J12_xiWk2XN4xixc3KrmP09ybu3pZvXQ/edit#heading=h.73tsyiabr3x4\"><u>20221020 EAGxRotterdam info for EA Eindhoven members</u></a></li></ul></li><li>Recommend the following EA Forum posts<ul><li><a href=\"https://forum.effectivealtruism.org/posts/5hKDjrGocGcreH3DC/how-to-get-the-maximum-value-out-of-effective-altruism\"><u>How to Get the Maximum Value Out of Effective Altruism Conferences</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/pKbTjdopzSEApSQfc/doing-1-on-1s-better-eag-tips-part-ii\"><u>Doing 1-on-1s Better - EAG Tips Part II</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/sxKJckCiZQyux4kCx/ea-global-tips-networking-with-others-in-mind\"><u>EA Global Tips: Networking with others in mind</u></a></li></ul></li><li>Host a preparation event (or multiple)<ul><li>We did this at Hubble Community Caf\u00e9, a student caf\u00e9 on our campus where we also host our regular socials. We did not use a classroom in a university building because we were looking for a more casual (\u201cgezellige\u201d) setting.</li><li>Do this 1 or 2 weeks before the conference<ul><li>Make sure Swapcard has already been published, so try to figure out when that will happen, and then plan the event at least a few days later to accommodate for delays.</li></ul></li><li>Make sure members did and know everything from the information document shared earlier (e.g., claim their ticket, book their travel and accommodation)<ul><li>Look for possibilities to connect members to book a room together or travel together.</li></ul></li><li>Ask members what they are most excited about in EA and use that to let them define their goals for the conference. What do they want to learn and what (kind of) people do they want to meet?</li><li>Give them recommendations of talks and people you might know (from your personal knowledge/network)<ul><li>Recommend the \u201cFirst-timers at EAGx\u201d session to first-timers</li><li>Also, recommend sessions like&nbsp; \u201cLessons from early EA careers\u201d to inexperienced EAs.</li></ul></li><li>Go over Swapcard with them<ul><li>Show how they can sign up for sessions</li><li>Show how they can filter for people in the Attendee tab or in the Attendee Data Sheet</li><li>Show how they can edit their availability for sessions</li><li>Explain that in the app, they cannot send a connection request to someone if they opened that person\u2019s profile from a chat, but only if they click that person\u2019s profile from the Attendee tab (this is a bug that sometimes confuses people into thinking other people are not available for chats).</li></ul></li><li>Explain how they can schedule 1-on-1s.<ul><li><strong>Most people are afraid to do this</strong>, so give them courage, show them how and make sure they have sent some requests&nbsp;<i>during</i> the preparation event</li><li>Recommend them to send people a message that goes something like: \u201cHi [name]! I am [name] and [what they do in their daily life]. I saw that you [what they do/are interested in/knowledgeable about]. I am working on/interested in [question you have that they can help you with]. Would you be interested in having a chat about this? Feel free to send me a meeting request. I look forward to hearing from you! Kind regards, [name].\u201d</li></ul></li><li>Make sure the members going to the conference know each other at least somewhat so that they have people to hang out with during the conference</li></ul></li></ul><h1><strong>During the conference</strong></h1><ul><li>If you meet members during the conference, ask them how they are doing and give them advice.</li><li>If they seem overwhelmed (which is of course totally understandable) make sure they know it is okay to rest and make sure they know where the nap room and chill areas are.</li><li>If you have some time off (or have a very uneventful volunteer shift), send members a message to let them know where you are: \u201cI\u2019ll be on a volunteer shift at the reception until [time], but I\u2019m not that busy, so feel free to drop by for a chat about how you\u2019re doing and what you\u2019ve learned this weekend, or about anything else!\u201d</li><li>Take a group picture (at the picture wall) to use for outreach materials<ul><li>We found that a good moment for this is after the closing talk/somewhere during dinnertime on the last day of the event.<ul><li>Some people are not there yet on Friday</li><li>During the conference itself, people are busy with their own schedules&nbsp;</li></ul></li><li>Make sure to ask the people in the picture for permission to use this picture in outreach materials (this is at least what the&nbsp;<a href=\"https://autoriteitpersoonsgegevens.nl/nl/onderwerpen/foto-en-film/beeldmateriaal\"><u>Dutch AVG law</u></a> requires).</li></ul></li><li>Make sure members have somewhere to go in the evenings (if they want to)<ul><li>Direct them to the Signal channel of the conference or the list of unofficial/unaffiliated events</li><li>Invite them to go with you when you go out<ul><li>This can be a bit hard if you are invited to a gathering of people you are affiliated with but your members are not (e.g., EA community builders), so then it is especially important to make sure they do not feel left out.</li></ul></li></ul></li></ul><h1><strong>Follow up afterward</strong></h1><ul><li>Encourage members to work out their notes and come up with actionable steps for the following weeks to pursue what they are most excited about.</li><li>Follow up with a 1-on-1 after a few weeks to give them advice and look at how you can support them.</li><li>Since people who have gone to an EA conference (hopefully) have a sense of the directions in EA they want to explore for their future careers, we feel they would most benefit from working on in-depth projects, with our coaching and advice. This is why we are exploring offering EA \u201cdevelopment projects\u201d to our members (or letting them choose their own), similar to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bnxipKeqwg2yWwn2n/from-practical-projects-to-self-development-a-shift-in-focus\"><u>what Per Ivar Friborg is doing at EA NTNU</u></a>. This is a&nbsp;<a href=\"https://airtable.com/shrzr1Dv8ugToJUuy\"><u>list</u></a> of projects we have come up with so far for our group.</li></ul><p><i>Note: This post is also available as a Google Doc </i><a href=\"https://docs.google.com/document/d/1egSHBBeBYqNyHasv3x5Kyz_1fEioXJ3kEv19ZaVkyu8/edit?usp=sharing\"><i>here</i></a><i>. Another resource we found helpful is this:&nbsp;</i><a href=\"https://docs.google.com/document/d/1W9A4z89916gDaT2WbDFuOCnYzIzBKGCPGSx7AiV7Mmc/edit#heading=h.b7v1iu467id2\"><i><u>Resource on EA Global and EAGx for EA Group Organizers 2022 [shared]</u></i></a></p>", "user": {"username": "Irene Hulsen"}}, {"_id": "7PqmnrBhSX4yCyMCk", "title": "Effective Peer Support Network in FTX crisis (Update)", "postedAt": "2022-11-14T12:09:16.810Z", "htmlBody": "<p><strong>Are you going through a rough patch due to the FTX crisis?</strong><br><strong>Or do you want to help EA peers who are?&nbsp;</strong></p><p>Following up on our post from last Friday, <a href=\"https://forum.effectivealtruism.org/posts/mRmehL6Yuu9SozjNs/give-and-get-mental-health-support-during-these-hard-times\">'Get and Give Mental Health Support During These Hard Times'</a>, we [Rethink Wellbeing and the Mental Health Navigator] set up a support network to react to the many people in our community affected by the FTX crisis. The number of people who have joined is growing! These are the two modes of action:<br>&nbsp;</p><ul><li><a href=\"https://docs.google.com/spreadsheets/d/1ztWUq7ZN21nlgU74O51zPfrDLi9WZy-9SvPhkpeWk1A/edit#gid=0\"><strong>(1) In this table, </strong></a>you can find<strong> </strong>experienced supporters<strong>.</strong><br><br><ul><li>These supporters want to <strong>help (for free</strong>), and you can just contact them. The community health team, as well as a growing number of coaches and therapists that are informed about EA and the FTX crisis, are already listed.&nbsp;</li><li><strong>If you have experience in supporting others</strong> and would like to dedicate &gt;=1 hour in the next few weeks to support one or more EA peers that are having a hard time, you can take <strong>1 min to add your details to the table</strong>. We likely need more support to take care of this situation. Consultants might be helpful, too.<br><br>&nbsp;</li></ul></li></ul><ul><li><strong>(2) You can </strong><a href=\"https://join.slack.com/t/effectivepeersupport/shared_invite/zt-1mocp8wch-IEmFfrXwsSIRRxx7eBwO8A\"><strong>join our new Peer Support Network Slack here</strong></a>.<br><br>People can share and discuss their issues, get together in groups and 1:1, as well as get support from the trained helpers as well as peers:<br><br><ul><li>It enables you to <strong>chat with a trained helper anonymously\u2014if</strong> you use a Nickname and an email address that doesn't contain your name.</li><li>You can create <strong>closed sub-channels here for a small group</strong> of people with similar issues that want to support each other more closely.</li><li>One can <strong>tackle specific topics in sub-channels</strong>, e.g., dealing with loss, future worries, or personal crises.<br>&nbsp;</li></ul></li></ul><p><strong>Feel free to send this to anyone who might need or wish to provide help!</strong><br><br><br>Rethink Wellbeing was supposed to receive FTX funding to implement <a href=\"https://forum.effectivealtruism.org/posts/ihGLzqnnau5aBim3N/effective-peer-support-to-help-you-flourish-growing-your\">Effective Peer Support</a>. Finding a funding source (for USD 25 to 32,5 k), we could also provide a simple first version of mental health support groups and 1:1s to EAs affected by the FTX crisis quickly. Let us know if you'd like to make that possible. Around 300 EAs have signed up to participate in Effective Peer Support already, mainly after our post in October. Let us know if you'd like to learn more.</p><p><br>&nbsp;</p>", "user": null}, {"_id": "9tSt63dcZadfhc2yP", "title": "How can we make EA more accessible?", "postedAt": "2022-11-14T09:55:54.007Z", "htmlBody": "<p>Hi all!</p><p>My name is Zofia and I had the pleasure to lead a meetup about accessibility within EA during the amazing EAGxRotterdam.</p><p>Based on the outcomes of that meet-up, as well as my own experiences within EA, I observed that, although the EA community is very inclusive, many of the resources and events are still not fully accessible to people with various disabilities. I would like to bring this topic to the attention of the CEA and start addressing it. I'd like to start working on a report on that topic to get a broader pool of experiences and issues that need to be addressed. If you have anything on that topic I invite you to fill in this short form:&nbsp;</p><p><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfsWg4QjMd91MjhN6tc-objzhrX-yxyI_a9wQRdVbk7Hzovcg/viewform?usp=sf_link\">https://docs.google.com/forms/d/e/1FAIpQLSfsWg4QjMd91MjhN6tc-objzhrX-yxyI_a9wQRdVbk7Hzovcg/viewform?usp=sf_link</a>.&nbsp;</p><p>You can also just respond to that post. I will of course share the report once it's finished.<br>Thanks in advance!</p>", "user": {"username": "Zofia Staszewska"}}, {"_id": "swn2yYoRxDXsKJAZd", "title": "Last Call for Eastern European Community Leaders to Apply for a Retreat [Nov 24 - Nov 27]", "postedAt": "2022-11-14T07:54:54.030Z", "htmlBody": "<p>The team at EA Slovakia received funding from EA Czechia to organize a retreat for community builders from Eastern Europe. &nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668412805/mirroredImages/swn2yYoRxDXsKJAZd/p1qdbykpcmcyligqsrzj.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8636dbb3bfef8add150d9f1ae2edf9cfd9448f9fa63de4c4.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8636dbb3bfef8add150d9f1ae2edf9cfd9448f9fa63de4c4.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8636dbb3bfef8add150d9f1ae2edf9cfd9448f9fa63de4c4.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8636dbb3bfef8add150d9f1ae2edf9cfd9448f9fa63de4c4.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8636dbb3bfef8add150d9f1ae2edf9cfd9448f9fa63de4c4.png/w_1000 1000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8636dbb3bfef8add150d9f1ae2edf9cfd9448f9fa63de4c4.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8636dbb3bfef8add150d9f1ae2edf9cfd9448f9fa63de4c4.png/w_1400 1400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8636dbb3bfef8add150d9f1ae2edf9cfd9448f9fa63de4c4.png/w_1600 1600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8636dbb3bfef8add150d9f1ae2edf9cfd9448f9fa63de4c4.png/w_1800 1800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8636dbb3bfef8add150d9f1ae2edf9cfd9448f9fa63de4c4.png/w_1920 1920w\"></figure><p>The retreat will start in 10 days and we have some last-minute spots after some people opted out. <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSd6_vGPzJ7NUVgQOEZj_k5oLVdj_AkJ-CmPqwQkPQPA-V6H8g/viewform\">Apply here</a>.</p><p>The main retreat will start on Friday 25. November 16:00 at <a href=\"https://www.divokavoda.sk/en/accommodation/\">Divoka Voda</a>, Bratislava; and end on Sunday, 27. November, 14:00. We will have pre-retreat activities starting on Thursday, 24. November 18:00, until the official start of the retreat. Participation is free, we offer travel support if needed.</p><p>We think this retreat is a wonderful opportunity for people who want to start new communities, as well as for those who can offer advice, or think about the Eastern European region as a whole.&nbsp;</p><p>Currently, we have about 30 people from most countries. One notable exception is EA Estonia which decided to not apply.</p><p>The retreat is being prepared by a core team of 4 people, with the occasional support of 3 additional senior people. Even though we started preparations almost 3 months ago, the last 10 days will be much more hectic, as expected. But we are mostly in good shape and focusing on finalizing the program.</p><p>We want to focus on:</p><ul><li>Current State of EA in Eastern Europe. Presentation of existing community-building efforts. Mainly EA Czechia, EA Poland, and EA Hungary, but all the other countries as well (Slovakia, Bulgaria, Romania, Serbia, Russia, Ukraine, Turkey, Greece, Georgia, Latvia, and Bosnia)</li><li>Deep dive into building strategies for individual communities and the region. We want to focus on actually building the strategies and preparing drafts. As well as a discussion of strategic topics, such as comparative advantage and the theory of change.&nbsp;</li><li>Developing national organization - workshop about practical issues related to community-building. Includes topics like funding, co-founders, core team, community-building funnel, organizational structure, NGO boards, working with volunteers, employing people, running community events, etc. We expect communities to be in a variety of states with diverse needs, asking different questions. The aim of this time is to come up with practical action items for leaders of each community that once executed will move the community forward.</li><li>Exploration of different perspectives and building of our identity. Here we want to focus on EA Narrative and the values we want to spread in Eastern Europe. Most of us already learned that what works in Oxford or Bay Area, doesn't necessarily work in our part of the world. We want to think from first principles about what messages we want to share.&nbsp;</li><li>Preparation of projects. Lastly, the ideal output of this retreat is the creation or improvement of several functional EA communities that are attracting the right crowd of talented and ambitious people and redirecting them to more impactful career directions. At the same time, our experience tells us that national groups are great spaces for project incubation. For example, Effective Thesis is one such project incubated by EA Czechia. During this workshop, we want to start with brainstorming projects and end up with a project plan draft for each individual or community.</li></ul><p>We are aware this program is ambitious for a short retreat. If you have some useful feedback or have experience running similar retreats, we would love to chat. You can comment here, DM me, or contact us at info@efektivnyaltruizmus.sk</p>", "user": {"username": "MatejVrzala"}}, {"_id": "LYqkptuAiPQcmmGbs", "title": "AI Safety Microgrant Round", "postedAt": "2022-11-14T04:25:17.266Z", "htmlBody": "<p>We are pleased to announce an AI Safety Microgrants Round, which will provide micro-grants to field-building projects and other initiatives that can be done with less.&nbsp;</p><p>We believe there are projects and individuals in the AI Safety space who lack funding but have high agency and potential. We think individuals helping to fund projects will be particularly important given recent changes in the availability of funding. For this reason, we decided to experiment with a small micro-grant round as a test and demonstration of this concept.</p><p>To keep the evaluation simple, we\u2019re focusing on field-building projects rather than projects that would require complex evaluation (we expect that most technical projects would be much more difficult to evaluate).</p><p>We are offering microgrants <strong>up to $2,000 USD</strong> with the <strong>total size of this round being $6,000 USD</strong> (we know that this is a tiny round, but we are running this as a proof-of-concept). One possible way this could pan out would be two grants of $2000 and two of $1000, although we aren\u2019t wedded and we are fine with request for less. We want to fund grant requests of this size where EA funds possibly has a bit too much overhead.</p><p>The process is as follows:</p><ol><li>Fill out this form at&nbsp;<a href=\"https://microgrant.ai\"><u>microgrant.ai</u></a> (&lt;15 min).</li><li>Shortlisted applications receive a follow-up call within two weeks after the applications close&nbsp;</li><li>We may also send an email with follow-up questions if we need more information. We would expect a reply within a few days so that we could confirm the grant within a week</li></ol><p><strong>To inspire applicants, here are some examples of projects where a microgrant would have been helpful:</strong></p><ol><li>Chris recently received a grant through an Australian-based organisation to hire two facilitators at $1,000 each for running a local version of the AGI safety fundamentals course. Intro fellowships have proven to be a great way of engaging people and we would be excited about funding this if it would help kickstart a new local AI safety group rather than just being an isolated project.</li><li>We might have funded something like the<a href=\"https://forum.effectivealtruism.org/posts/c5SeLNpnHNNif6Doz/announcing-the-ai-safety-nudge-competition-to-help-beat\">&nbsp;<u>AI safety nudge competition</u></a> to help people overcome their procrastination, and are excited about the potential to motivate a large number of people to accelerate their AI safety journeys and about experimenting with a potentially scalable intervention.</li><li>The<a href=\"https://forum.effectivealtruism.org/posts/QrwnajRpteBZhQZnu/sydney-ai-safety-fellowship\">&nbsp;<u>Sydney AI Safety Fellowship</u></a> was originally going to be funded with three people each throwing in $2000, which would have been enough for a coworking space, weekly lunches and some socials for a few participants. We would be excited about funding a similar project if they were likely to attract good candidates - especially in communities and countries where this is currently nonexistent.&nbsp;</li></ol><p>We are open to smaller grant applications, but in a lot of cases, small grants don't make much of a counterfactual difference, however here are two examples of the kinds of grants we would be excited about:</p><ol><li>One of us thinks there should be a logo for AI safety just like EA has the light bulb. This may be considered for a microgrant.</li><li>One of us recently granted $200 to half subsidise ten maths lessons. This allowed the grantee to build up evidence to subsequently get a grant to half-subsidise their lessons. We are excited about grants that assist someone in testing their fit. One of us had also granted $100 to someone in a low and middle-income country to get them a device for self-study so they could upskill themselves.</li></ol><p>In contrast, here are the kinds of grants we wouldn't be very excited about:</p><ul><li>Any project with significant downside risk</li><li>$2000 towards a project which requires $8,000 which hasn't been obtained yet</li><li>Funding to add one more fellow to a program that already has ten fellows. These kind of grants could be impactful, but allowing these kind of grants would like greatly increase the number of applications we would have to evaluate. Either at least 25% of the funding should be coming from the micro-grant round or the grant should make a qualitative difference.</li></ul><p><strong>Next Steps</strong></p><p>Fill out the form at Microgrant.ai by December 1, 2022. If we\u2019re interested in your project/idea, we will get back to you in a few days after the application closes. We are aiming to schedule calls with shortlisted applicants within two weeks of applications closing. Best of luck! We also encourage you to consider whether you should be submitting a grant application to EA Funds. You can email hello@microgrant.ai if you have questions.</p><p><strong>Fine Print</strong></p><p>This grant round is being funded by Chris Leong,&nbsp; Damola Morenikeji and David Kristoffersson. Thanks to A_Donor, Yanni Kyriacos, Evan Gaensbauer and Brendon Wong for their advice and assistance in this project.</p><p>We may be unable to provide grants to some applicants if there are sanctions applicable to their country.</p>", "user": {"username": "casebash"}}, {"_id": "efGNMe6uB87qXozXJ", "title": "NY Times on the FTX implosion's impact on EA", "postedAt": "2022-11-14T03:51:38.432Z", "htmlBody": "<p>The impact of the FTX scandal on EA is starting to hit the news. The coverage in this NY Times article seems fair to me. I also think that the FTX Future Fund leadership decision to jointly resign both was the right thing to do, and comes across that way in the article. Will MacAskill, I think, is continuing to show leadership interfacing with the media - it's a big transition from his book tour not long ago to giving quotes to the press about FTX's implosion.</p><p>The article focuses on the impact this has had on EA:</p><blockquote><p>[The collapse of FTX] has also dealt a significant blow to the corner of philanthropy known as effective altruism, a philosophy that advocates applying data and evidence to doing the most good for the many and that is deeply tied to Mr. Bankman-Fried, one of its leading proponents and donors. Now nonprofits are scrambling to replace millions in grant commitments from Mr. Bankman-Fried\u2019s charitable vehicles, and members of the effective altruism community are asking themselves whether they might have helped burnish his reputation...</p><p>... For a relatively young movement that was already wrestling over its growth and focus, such a high-profile scandal implicating one of the group\u2019s most famous proponents represents a significant setback.</p></blockquote><p>The article mentions the FTX Future Fund joint resignation, focusing on the grants that will not be able to be honored and what those might have helped.</p><p>The article talks about Will MacAskill inspiring SBF to switch his career plans to pursue earning to give, but doesn't try to blame the fraud on utilitarianism or on EA. This is my opinion, but I'm just confused by people's eagerness to blame this on utilitarianism or the EA movement. The common-sense American lens to view these sorts of outcomes is a framework of personal responsibility. If SBF committed fraud, that is indicative of a problem with his personal character, not the moral philosophy he claims to subscribe to.</p><blockquote><p>His connection to the movement in fact predates the vast fortune he won and lost in the cryptocurrency field. Over lunch a decade ago while he was still in college, Mr. Bankman-Fried told Mr. MacAskill, the philosopher, that he wanted to work on animal-welfare issues. Mr. MacAskill suggested the young man could do more good earning large sums of money and donating the bulk of it to good causes instead.</p><p>Mr. Bankman-Fried went into finance with the stated intention of making a fortune that he could then give away. In an <a href=\"https://www.nytimes.com/2022/10/08/business/effective-altruism-elon-musk.html\">interview with The New York Times</a> last month about effective altruism, Mr. Bankman-Fried said he planned to give away a vast majority of his fortune in the next 10 to 20 years to effective altruist causes. He did not respond to a request for comment for this article.</p></blockquote><p>Contrary to my expectation, the article was pretty straightforward in describing the global health/longtermism aspects of EA:</p><blockquote><p>Effective altruism focuses on the question of how individuals can do as much good as possible with the money and time available to them. Historically, the community focused on low-cost medical interventions, such as insecticide-treated bed nets to prevent mosquitoes from giving people malaria.</p><p>More recently many members of the movement have focused on issues that could have a greater impact on the future, like pandemic prevention and nuclear nonproliferation as well as preventing artificial intelligence from running amok and sending people to distant planets to increase our chances of survival as a species.</p></blockquote><p>Probably the most critical aspect of the article was this:</p><blockquote><p><a href=\"https://www.urban.org/author/benjamin-soskis\">Benjamin Soskis</a>, senior research associate in the Center on Nonprofits and Philanthropy at the Urban Institute, said that the issues raised by Mr. Bankman-Fried\u2019s reversal of fortune acted as a \u201cdistorted fun-house mirror of a lot of the problems with contemporary philanthropy,\u201d in which very young donors control increasingly enormous fortunes.</p><p>\u201cThey gain legitimation from their status as philanthropists, and there\u2019s a huge amount of incentive to allow them to call the shots and gain prominence as long as the money is flowing,\u201d Mr. Soskis said.</p></blockquote><p>But even this focuses on the same problems of the purchasing of status with philanthropy that we ourselves are wrestling with right now.</p><p><i>Edit: See Aaron Chang's comment for what he sees as the most glaring issue pointed out by the article - \"loose norms around board of directors and conflicts of interests between funding orgs and grantees\"</i></p><p>I expect that other articles may take a harder look at EA. But I was heartened to see that in this case at least, the author, Nicholas Kulish, seems to be treating EA like what I understand it to be -- a lot of people earnestly trying to figure out how to make the world a better place, and trying to find the most ethical way to navigate a disaster.</p>", "user": {"username": "AllAmericanBreakfast"}}, {"_id": "fzJN8ANwuWF3gbvXF", "title": "Estimating the probability that FTX Future Fund grant money gets clawed back", "postedAt": "2022-11-14T03:33:05.246Z", "htmlBody": "<p>It would be great to have more people involved in estimating the probability that FTX Future Fund grants end up getting clawed back (i.e., that projects that received such grants are asked to give the money back). This is a very important question for grant recipients right now, with a lot of murkiness around it - your forecasting help is appreciated!</p><p>If you have an opinion on the topic, would like to help make the forecasts more accurate, or are just interested in the question, here are three forecasting tournament pages related to the question:</p><p>&nbsp;</p><figure class=\"media\"><div data-oembed-url=\"https://manifold.markets/SpencerGreenberg/will-projects-which-received-grants\">\n\t\t\t\t<div class=\"manifold-preview\">\n\t\t\t\t\t<iframe src=\"https://manifold.markets/SpencerGreenberg/will-projects-which-received-grants\">\n\t\t\t\t</iframe></div>\n\t\t\t</div></figure><figure class=\"media\"><div data-oembed-url=\"https://manifold.markets/EliezerYudkowsky/will-5-of-an-ftx-grant-be-clawed-ba\">\n\t\t\t\t<div class=\"manifold-preview\">\n\t\t\t\t\t<iframe src=\"https://manifold.markets/EliezerYudkowsky/will-5-of-an-ftx-grant-be-clawed-ba\">\n\t\t\t\t</iframe></div>\n\t\t\t</div></figure><figure class=\"media\"><div data-oembed-url=\"https://manifold.markets/NathanpmYoung/will-there-be-more-than-5-accounts\">\n\t\t\t\t<div class=\"manifold-preview\">\n\t\t\t\t\t<iframe src=\"https://manifold.markets/NathanpmYoung/will-there-be-more-than-5-accounts\">\n\t\t\t\t</iframe></div>\n\t\t\t</div></figure>", "user": {"username": "spencerg"}}, {"_id": "7q8uEwksmP5wK6kqA", "title": "How the FTX crash damaged the Altruistic Agency", "postedAt": "2022-11-13T21:38:42.584Z", "htmlBody": "<h2>Introduction</h2><p>First, I\u2019d like to express my deepest sympathies to everyone affected by the FTX breakdown. A lot of people have lost their life savings and are suffering terribly, something I do not wish to diminish in any way. Many are affected in far worse ways than I can even imagine, and I hope as many of those as possible will be able to find the support they need to get through these challenging times.</p><p>I got confirmation this week on Wednesday (Nov 9) that payouts from the FTX Future Fund had stopped. Since I had an outstanding grant with them, this was of great concern to me personally. The days following that, and what is still unravelling minute by minute, it seems, was the complete meltdown of FTX and all related entities, including the Future Fund. You are all following these events in other places, so I won\u2019t go into that much, but I wanted to offer a perspective from a Future Fund grantee and the specific ways something like this can do damage.</p><p>This post is also a call for support since all funding for my next year of operations has suddenly evaporated entirely. Specifically, if you or someone you know is a grantmaker or donor to EA meta/infrastructure/operations projects and would be interested in funding a new organisation with a good track record, please get in touch. I\u2019d be happy to share much more details and data on the proven value so far, my grant application to the Future Fund, and anything else that might be of interest.</p><p>I will also mention the benefits of being vigilant about organisational structure and how it can save your organisation in the long run, even though it might be an upfront and ongoing cost of time and money.</p><h2>Background</h2><p>I <a href=\"https://forum.effectivealtruism.org/posts/zwyETHzPL6A8kRJ3h/altruistic-agency-free-tech-expertise-for-effective\">founded</a> the <a href=\"https://altruistic.agency/\">Altruistic Agency</a> in January this year. The idea was to apply my knowledge and experience from 15+ years as a full-stack developer to help organisations and others in the EA community with tech expertise. I hypothesised that the kind of work I had done mostly in a commercial context for most of my professional life is also highly valuable to nonprofits/charities and others doing high-impact work within EA cause areas. I have always been fond of meta projects, and this seemed like something that could greatly increase productivity (like technology does), especially by saving people lots of time that they could instead spend on their core mission.</p><p>Thanks to a grant from the EA Infrastructure Fund, I was able to test this hypothesis full-time and spent the first half of this year providing free tech support to many EA organisations and individuals. During that time, I worked with around 45 of them in areas such as existential risk, animal advocacy, climate, legal research, mental health, and effective fundraising. The work ranged from small tasks, such as improving email deliverability, fixing website bugs, and making software recommendations, to larger tasks, such as building websites, doing security audits, and software integration.</p><p>The response from the community was overwhelmingly positive, and the data from the January\u2013June pilot phase of the Altruistic Agency indicates high value. I solved issues (on median) in a fifth of the time it would have taken organisations themselves. The data indicates just one person doing this work for six months saved EA organisations at least 900 hours of work. Additionally, many respondents said they learned a lot from working with me, both about their own systems and setups and about tech in general. Not least, increased awareness of security issues in code and systems, which will only become more crucial over time, and can be significantly harmful if not dealt with properly.</p><p>Insights from the pilot phase told me two important things: First, that this was a worthwhile project that provided a lot of value and should be continued. I have been over capacity essentially since the start, meaning there is a lot of demand. Second, that I could be far more ambitious. Most notably, working with many different organisations and projects, I started to notice the outlines of ideas for various infrastructure projects that could help many people simultaneously. This includes things such as donation platforms, job boards, and databases that each organisation does their own version of, often helped by volunteers with limited time.</p><p>Capacity issues were at times challenging, as the demand turned out to be very high, and I was combining the client work with the organisational work, which sometimes caused delays and interrupted planning \u2013 all of which were insights during this pilot phase that informed the plan for how the agency was to be improved. Towards the end of the pilot phase, I decided the next phase would be about fixing these things, mainly by raising more funds to hire at least a couple of people to help with technical work and operations.</p><h2>Scaling</h2><p>To scale up the Altruistic Agency and aim for a more ambitious version of it, I started looking into turning it into a proper nonprofit organisation. I have run it under my own (sole proprietorship) company until now, which isn\u2019t ideal in terms of taxation, bookkeeping, auditing, and so on. Setting up a dedicated legal entity would allow a complete separation and also make it much easier to accept donations and other funding and eventually employ people. The formal nonprofit status would also benefit the project in various ways, for example, in terms of accountability and assurance, as well as financially, since many services offer nonprofit discount pricing and such.</p><p>At the time, I lived in Amsterdam, but decided to set up the organisation in Brussels and consequently move there. The main reasons were firstly that it\u2019s the de facto centre of power for institutions such as the EU and NATO, with an enormous secondary layer of institutions related to European governance and an even larger tertiary layer of NGOs, lobbyists, journalists etc. Secondly, that there seems to be plenty of room for development of the EA community in Brussels, which I would love to be part of. Simply put, Brussels is a vastly connected place for a nonprofit in Europe.</p><p>The EA Infrastructure Fund agreed with the soundness of my plans and made a second grant to cover the costs of setting up this organisation and that of my own time while working on it, simultaneously also initiating some of the mentioned infrastructure projects.</p><p>Working with a legal consultant, in turn, working with a notary (a legal requirement), we decided on a public interest foundation. This is one of the kinds of nonprofit forms available in Belgium. It\u2019s similar to a private foundation in its governance but with additional provisions that declare the organisation has a philanthropic purpose by law. It\u2019s more expensive to set up because it\u2019s more paperwork and requires a Royal Decree of approval, meaning it also takes a lot more time. That we decided to go for this form might turn out to have been one of the most consequential decisions I made for the organisation.</p><h2>FTX</h2><p>While working with the legal consultant during July\u2013September to get everything in order, I also spent a lot of time on additional fundraising opportunities. One of the people I had worked with on a client project was very enthusiastic about the Altruistic Agency and turned out to be a regrantor for the FTX Future Fund. This person suggested I start working on a proposal for them to raise funds mainly for hiring people. I got to work on this, which involved writing and rewriting a lot of my thinking about the agency\u2019s purpose, its priorities, and figuring out what exact funding model it would use going forward. I got a lot of input on this both from the regrantor and from many other long-time EA technologists, for which I am immensely thankful.</p><p>Working backwards from the vision of the Altruistic Agency I have in mind for the coming years, I eventually came up with a proposal that struck a good balance between ambitious and intentional. The next phase of the organisation would be to hire a full-time developer and a full-time Director of Operations as soon as possible, with me acting as the Executive Director. This would allow the client work and other technical development to run non-stop, regardless of my own schedule from week to week. With the Director of Operations, I would spend much more time working out better processes for all parts of the work and much more time on research and publication about the technical needs of the EA community and its cause areas.</p><p>Eventually, I want the Altruistic Agency to work as a broad technical agency and a kind of incubator for high-impact technical projects. We would help any organisation develop MVPs and other things that help them get things off the ground, and even more importantly, develop software and services as public goods for the benefit of the entire EA community and more broadly anyone working within high-impact cause areas.</p><p>Until this week, I thought this next phase of the Altruistic Agency would take place during 2023 and that I had funding for it. That I would no longer have to juggle every single aspect of the organisation myself. That I would soon have an employment contract and salary from my newly formed organisation, thus also fulfilling the requirements for residency in Belgium. Now, none of that is the case anymore, and I need to get back to fundraising, just to solve my own income situation, if nothing else.</p><p>A small bit of irony in all this: I mentioned before that setting up a public interest foundation takes a lot more time. All the necessary paperwork was submitted by the notary some time ago, and as soon as I got word back that it had been approved, I would be able to set up a bank account, finish the due diligence with the Future Fund and receive the funds. But because we went for the slightly more complicated form of organisation, the process did not finish soon enough for any of that to happen. By going for increased legitimacy, the Altruistic Agency also accidentally avoided this whole world of trouble. So it goes. This has reminded me the that it is up to ourselves what level of scrutiny, accountability, transparency, and safeguarding we want to put our organisations under. I hope this is something many EA organisations revisit in light of the current situation, especially grantmaking organisations.</p><h2>Support</h2><p>Again, if my work resonates with you and you have any way of pointing funders or funding my way, please do so. You can email me directly at markus@altruistic.agency. I don\u2019t know yet how many other Future Fund grantees are in my situation or how much funds were \u201ccommitted\u201d (they weren\u2019t really, it turned out) to approved grants. I can\u2019t imagine there is a straightforward way for funding organisations within EA alone to pick up all of that, especially as most of them have other funding bars and criteria.</p><p>Already from the start, I\u2019ve been aware that the EA funding landscape is rather small and homogenous compared to the bigger nonprofit sector, and that any EA organisation should aim to diversify their funding. This FTX crisis just shows again that relying on a small number of very large donors is fragile, which is a lesson both for nonprofits themselves, and for the grantmaking organisations they get funding from.</p><p>I think I ultimately dodged a bullet. Not having signed any contracts or received funds from any entity related to FTX seems like the safe place to be right now. It would have tainted my organisation legally and morally. Starting a new organisation is always uncertain, especially in terms of funding prospects, so I know not to put too much faith in opportunities until they are fully resolved. But I\u2019d rather be set back a year and once again full of uncertainty, than find out my organisation was built on resources from potentially criminal activities. We are all trying to act morally and should always reject any benefit from immoral actions.</p>", "user": {"username": "peppersghost"}}, {"_id": "rMbgpKhK5M2pE4s25", "title": "In Defence of Temporal Discounting in Longtermist Ethics", "postedAt": "2022-11-13T21:30:40.522Z", "htmlBody": "<h1>Introduction</h1><p>My thoughts on temporal discounting in longtermist ethics.</p><p>&nbsp;</p><h1>Two Senses of Normative Ethics</h1><p>There are two basic senses of a normative moral system:<br>1. Criterion of judgment: \"what is right/good; wrong/evil?\"<br>2. Decision procedure: \"how should I act? What actions should I take?\"&nbsp;</p><p>We can \u2014 and I argue we should \u2014 distinguish between these two senses.</p><p><br>Consider consequentialism:</p><h3>Criterion of Judgment</h3><p>Using consequentialism as a criterion of judgment, we can evaluate the actual <i>ex post</i> consequences of actions (perhaps over a given timeframe: e.g. to date, or all of time [if you assume omniscience]) to decide whether an action was right/wrong.</p><p>&nbsp;</p><h3>Decision Procedure</h3><p>However, when deciding what action to take, we cannot know what the actual consequences will be.&nbsp;</p><p>For a decision procedure to be useful at all \u2014 for it to even qualify as a decision procedure \u2014 it must be <i>actionable</i>. That is, it must be possible \u2014 not only in principle, but also in practice \u2014 to act in accordance with the decision procedure. That is, the decision procedure must be:</p><ul><li>Directly evaluable or</li><li>Approximately evaluable or&nbsp;</li><li>Robustly estimable or</li><li>Etc.</li></ul><p>We should have a way of determining what course of action the procedure actually recommends in a given scenario. As such, the procedure must be something we can evaluate/approximate/estimate <i>ex ante</i>, before we know the actual consequences of our actions.</p><p>Because of <a href=\"https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities\">coherence arguments</a>, I propose that a sensible decision procedure for consequentialists is the \"ex ante expected consequences<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefygdklhck0ld\"><sup><a href=\"#fnygdklhck0ld\">[1]</a></sup></span>&nbsp;of (policies over) actions\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrbztbxu98bk\"><sup><a href=\"#fnrbztbxu98bk\">[2]</a></sup></span>.</p><hr><h1>Against Discounting</h1><p>When considering how we should value the longterm future, I feel like MacAskill/Ord conflate/elide between the two senses<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefled6guqtp0e\"><sup><a href=\"#fnled6guqtp0e\">[3]</a></sup></span>.</p><p>They make a compelling argument that we shouldn't discount the interests (wellbeing/preferences) of future people:</p><p>Assigning a fixed discount rate per year (e.g. 1%) to future people and extrapolating back in time, you get a conclusion like the interests of Ancient Egyptian royalty (e.g. Cleopatra) outweigh the interests of everyone alive today, and this seems obviously lacking.</p><p>&nbsp;</p><h1>Temporal Discounting and the Two Senses of Normative Ethics</h1><p>I agree that from a perspective of \"morality as criterion of judgment\", we should not discount the interests of future people. Plausibly, in a consequentialist-criterion-of-judgment-framework almost all the value of any action is determined by its impact on far future people.</p><p><br>However, it does not follow that in a consequentialist-decision-procedure-framework almost all the value of any action is determined by its impact on far future people.</p><p>Rather, it seems to me that this is quite unlikely to be the case.</p><p><br>It is difficult for us to evaluate the effect of our actions on future people (What future people will counterfactually [depending on which actions we take] exist? What are their interests? What will be the effects of our actions on those interests? Etc.), and the further out they are, the greater our uncertainty of the effect of our actions.</p><p><a href=\"https://80000hours.org/podcast/episodes/alan-hajek-probability-expected-value/#relevance-to-objective-consequentialism-030947\">Alan H\u00e1jek makes the case for the difficulty of objective consequentialism as a decision procedure in his interview with Robert Wiblin</a>.<br>&nbsp;</p><p>To a first approximation<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1uta9pfajjn\"><sup><a href=\"#fn1uta9pfajjn\">[4]</a></sup></span>, the uncertainty of the aggregate moral value of a particular action grows exponentially the further out in time we consider in our evaluation window<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv5yjch71y8\"><sup><a href=\"#fnv5yjch71y8\">[5]</a></sup></span>.</p><p><br>As such, I think a temporal discount rate does make sense within a consequentialist-decision-procedure-framework. At least if you agree that the relevant consideration when deciding what action to take is \"ex ante expected consequences\".</p><p><br>The conclusion of the above is that while the interests of Cleopatra in her time do not in actuality outweigh the interests of everyone alive today, Cleopatra should not have considered the people alive today in her moral decision making.</p><p><br>And I endorse that conclusion? (If it's a bullet, then it must be the easiest bullet I've ever bitten.) I don't think Cleopatra could have usefully evaluated the consequences of her actions on people alive today. The current global geopolitical macrostate is probably something Cleopatra's accessible world models could not readily conceive.</p><p>In her decision making, Cleopatra should have considered the interests of her direct subjects and those in the near future; they are the only people for whom she could usefully reason about.</p><p>&nbsp;</p><p>To summarise this argument in a less nuanced but more memetically fit form:</p><blockquote><p>We should care about the interests of our children and grandchildren; and leave the interests of our great grandchildren to our grandchildren; they are better positioned to evaluate and act upon them.</p></blockquote><p>&nbsp;</p><h1>Conclusions/Summary</h1><ul><li>We shouldn't discount the interests of future people within a consequentialist-criterion-of-judgment-framework<ul><li>Actual, existent people don't <i>intrinsically </i>matter any more or less based on their temporal location</li><li>Cleopatra's interests do not outweigh the interests of the eight billion people alive today</li></ul></li><li>We should discount the interests of future people within a consequentialist-decision-procedure-framework<ul><li>Due to our uncertainty about:<ul><li>The counterfactual existence of such people</li><li>Their interests</li><li>The effects of our actions upon them</li><li>Etc.</li></ul></li><li>This uncertainty grows (exponentially? hyperbolically?) the more distant they are from us in time</li></ul></li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnygdklhck0ld\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefygdklhck0ld\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Subject to bounded computing constraints. Evaluating the full ex ante expected consequences may be computationally intractable. So some sort of \"best effort\" estimate of said consequences may be needed.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrbztbxu98bk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrbztbxu98bk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>More sophisticated consequentialists may want higher order abstractions in their decision procedures (policies for selecting policies for ... for selecting actions).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnled6guqtp0e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefled6guqtp0e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note, I'm going of my vague recollections of their writings, so this may be somewhat inaccurate.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1uta9pfajjn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1uta9pfajjn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>There is a maximum level of uncertainty: maximum entropy, and so beyond some certain horizon in time, we have roughly constant (maximum) uncertainty about future people, and wouldn't further discount people that come into existence after that horizon based on temporal displacement.&nbsp;</p><p>As such a proper temporal discounting due to uncertainty may not be exponential but perhaps <a href=\"https://en.wikipedia.org/wiki/Hyperbolic_discounting\">hyperbolic </a>or similar?</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv5yjch71y8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv5yjch71y8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I suspect the growth rate of said uncertainty is probably significantly higher than 1% (at least before the maximum entropy time horizon).</p></div></li></ol>", "user": {"username": "Dragon God"}}, {"_id": "Y4f6H5juwkMEHwBmv", "title": "X-risk Mitigation Does Actually Require Longtermism ", "postedAt": "2022-11-13T19:40:00.770Z", "htmlBody": "<h1>Introduction</h1>\n<p>Myself \u2014 and many others \u2014 have argued that longtermism is not needed to argue for x-risk mitigation. That all such actions can be adequately justified within a neartermist framework, and given the poor reception/large inferential distance/inaccessibility of longtermist arguments, we might be better served arguing for x-risk mitigation within a strictly near termist framework.</p>\n<p>But this is not fully accurate.</p>\n<p>Averting extinction makes sense in near termist ethical frameworks (8 billion people dying is very bad), but extinction is not the only category of existential risk, and it's the only one that can readily be justified within neartermist frameworks.</p>\n<h1>Longtermism and Existential Risks</h1>\n<p>Excluding extinction, all the other existential risks \u2014 the very concept of an \"existential risk\" itself \u2014 implicitly rely on longtermism.</p>\n<p>Toby Ord defined an existential catastrophe as an event that permanently curtails the <em>longterm potential</em> of humanity/human civilisation.</p>\n<p>A few classes of existential catastrophe other than extinction:</p>\n<ul>\n<li>Value lockin</li>\n<li>Irreversible technological regression</li>\n<li>Any discrete event that prevents us from reaching technological maturity</li>\n<li>Any discrete event that leads to Bostrom's \"Astronomical Waste\"</li>\n</ul>\n<p>(I would also add \"technological stagnation\" to the list. It's not a discrete event [so Ord didn't consider it as a catastrophe], but it has the same effect of curtailing the long term potential of human civilisation.)</p>\n<p>We cannot even conceive of an existential catastrophe without a framework of \"longterm potential of human civilisation\", concepts of \"technological maturity\", \"astronomical waste\", etc.</p>\n<p>All of these are concepts that are defined only within a longtermist framework.</p>\n<p>Thus, existential risk mitigation is inherently a longtermist prospect.</p>\n<h1>Caveats</h1>\n<p>While extinction risks aren't the only existential risks, they are the one that has attracted the supermajority of attention and funding.</p>\n<p>Excluding extinction risk mitigation, other longtermist projects looks like:</p>\n<ul>\n<li>Grand strategy for humanity</li>\n<li>Promoting more adequate/resilient institutions</li>\n<li>Better mechanisms for coordination and cooperation</li>\n<li>Governance of advanced/speculative technologies</li>\n<li>Space settlement and colonisation</li>\n<li>Etc.</li>\n</ul>\n<p>Some of these actions may not have that large an effect on near term extinction risks.</p>\n<p>Maybe there's an argument that we should argue for taking actions to mitigate near term extinction risks separately from other more inherently longtermist actions.</p>\n", "user": {"username": "Dragon God"}}, {"_id": "QTCntnqznK9RnGvAz", "title": "The Most Cost-Effective Ways to Treat Depression Globally", "postedAt": "2022-11-13T18:34:58.270Z", "htmlBody": "<p>Globally, 280 million people are living with depressive disorders. In low-income countries, approximately 85% receive no treatment.</p><p>StrongMinds provides free group therapy to women and adolescents in Uganda and Zambia.</p><p>The Happier Lives Institute recently completed a cost-effectiveness analysis concluding that StrongMinds\u2019 depression treatment is one of the world's most cost-effective interventions for increasing wellbeing. It is 9x more cost-effective than direct cash transfers for improving the life of an individual, as well as their entire household.</p><p>Join us together with the Happier Lives Institute for a discussion about the Happier Lives Institute report and how it disrupts prevailing thinking in the world of charitable giving.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1668364578/mirroredImages/QTCntnqznK9RnGvAz/sms0a0dncrck8f5jh9kx.png\"> When: 14th of November, 12:00-12:45 Swedish time (UTC+1)<br><img src=\"http://res.cloudinary.com/cea/image/upload/v1668364578/mirroredImages/QTCntnqznK9RnGvAz/t1g9xlwzozm5wbuyn5el.png\"> Where: Online via Zoom<br><img src=\"http://res.cloudinary.com/cea/image/upload/v1668364578/mirroredImages/QTCntnqznK9RnGvAz/vzzl2upycf2v9xov5u9r.png\"> How: 30 minutes presentation followed by 15 minutes Q&amp;A.</p><p>The event is free and open to the public. Welcome!<br>_______</p><p>The Swedish Network for Global Mental Health is a non-profit association for individuals interested in mental health in a global context. From researchers and clinicians to policy makers and civil society.</p>", "user": {"username": "BarryGrimes"}}, {"_id": "7qd68BsKhHnBEGtB5", "title": "Best Courses for an Undergraduate Economics Major to Get a Job as an Undergrad Research Assistant?", "postedAt": "2022-11-13T18:16:16.610Z", "htmlBody": "<p>Hello, I am an undergraduate junior majoring in economics and minoring in mathematics. I want to be an undergraduate research assistant during the summer or part-time during the upcoming semesters. I want to focus on the fields of extreme poverty, global health, and foreign affairs.</p><p>&nbsp;</p><p>Fall 2022 is my first semester as a junior in economics at a 4-year college. I just graduated from my community college in August, so my completed economics courses are limited.</p><p>&nbsp;</p><p>Here are all the economic and mathematics courses I will have completed in my academic career by the end of this semester: Intro to Macroeconomics, Intro to Microeconomics, Statistical Methods, Calculus I, Calculus II, Calculus III, Differential Equations, Numerical Analysis, Sustainable Development Goals (United Nations),</p><p>&nbsp;</p><p>These are the courses I can take in the Spring 2023 semester:</p><ul><li>Intermediate Microeconomic Theory</li><li>Intermediate Macroeconomic Theory</li><li>Introduction to econometrics</li><li>Economic Development</li><li>Money &amp; Banking</li><li>Behavioral Economics</li><li>Probability and Statistics</li><li>Linear Algebra<br>&nbsp;</li></ul><p>I can only take 3 courses next semester due to my (unrelated) work and long commute. Which 3 courses should an undergraduate junior majoring in economics and minoring in mathematics took to best prepare them for an undergraduate economics research assistance position? And what 3 courses will best stand out to employers? Especially in said fields of interest in extreme poverty, global health, and foreign affairs.<br>&nbsp;</p><p>Thank you!<br><br><span>(I see the other EA undergraduates (even those academically a year below me) have a leg up due to their Ivy League or prestigious universities having more internship and part-time opportunities available within their institutions. I've applied to nearby internships, research, and part-time opportunities at Columbia, NYU, Council on Foreign Relations, etc., and don't even get a rejection email back. I can see why though: It's a safer bet to choose applicants from said prestigious universities than a recent community college graduate. Due to this, I'm considering leaving Rutgers-Newark to at least Rutgers-New Brunswick in the Fall of 2023 and applying to prestigious universities for the Fall of 2023 semester. If you can't beat them, join them.</span><br><br><span>I would really appreciate the advice as I'm trying my best to get into EA careers and causes (mentioned in my bio), I know 80k says these are easier to attain if you go to a top school, but due to my financial and personal ordeals, I didn't apply after high school or community college. But I'm still trying to remain optimistic and plan to dedicate my career to the most good I can do!)</span></p>", "user": {"username": "Jeffrey Arana"}}, {"_id": "SnNC6iRCwgTvGq94k", "title": "Heroic - a free resouce of personal development and practical wisdom", "postedAt": "2022-11-13T18:02:54.590Z", "htmlBody": "<p>(The original post attempted to link to the horrible FTX situation. This was done in good faith - we wanted to channel the compassion to something useful. However, based on feedback this came across as headline-grabbing and it seemed too much like a promotion. We substantially rewrote it. We're sorry for any upset we may have caused some of you. Sincerely.).</p><p>We've seen many people benefit from personal development and practical wisdom. Concretely, we've seen this among our clients (we're <a href=\"https://forum.effectivealtruism.org/posts/DwiJBvnjxjptQePSs/coaching-reduce-struggle-and-develop-talent\">coaches</a> for EAs), friends, and in our own personal lives. However, not everyone has access to good ways of benefitting from personal development and practical wisdom. This could be due to a lack of awareness or financial means. While we think coaching is particularly promising (see <a href=\"https://forum.effectivealtruism.org/posts/DwiJBvnjxjptQePSs/coaching-reduce-struggle-and-develop-talent\">here</a> and <a href=\"https://forum.effectivealtruism.org/posts/aGTuQjvHrFqhgSuxZ/underinvestment-at-the-top-what-i-discovered-coaching-a\">here</a>) we'd like to introduce a different (and free way) of pursuing personal development and practical wisdom. Concretely, we'd like to introduce a service called optimize.me (part of Heroic) as we've found that ~10 people within EA have benefitted non-trivially from it. We'll share a brief background on Optimize.me (Heroic) and then continue with a curated list of high EV recommendations.</p><h1>&nbsp;</h1><h1>Background</h1><p><a href=\"https://www.heroic.us/optimize\"><u>Heroic</u></a>, founded by the exemplary Brian Johnson, seeks to create a more virtuous and flourishing society. Concretely, they seek to cultivate heroes and pursue the audacious vision of positive psychology - 51% of the world\u2019s population flourishing by 2051. At some point during the last year, they decided to make part of their service (optimize.me) free and we'd like to enable others to use that to add value to their lives.&nbsp;<br>&nbsp;</p><h1>How might optimize.me add value to your life?</h1><p>There are three sources of value that we and ~ 10 others can personally attest to:</p><ol><li>Masterclasses: Long-form videos (1-2 hours) on essential domains of human life (e.g., conquering digital addiction, sleep, and productivity) that tend to be neglected in our current society and/or where the framing of these domains are highly suboptimal compared to what they could be.</li><li>Philosopher\u2019s notes: 6-page PDFs and associated 10-20 minutes videos on hundreds of books&nbsp;on personal development and practical wisdom.</li><li>+1\u2019s: Two-minute videos and associated notes providing wisdom in an engaging and bite-sized way.<br><br>&nbsp;</li></ol><p>Please bear in mind</p><ul><li>The epistemics of the videos aren\u2019t peer-reviewed science but rather relatively nuanced personal experiences, largely drawing on hundreds of books and creating connections between ideas, thinkers, and practitioners.</li><li><a href=\"https://en.wikipedia.org/wiki/Don%27t_throw_the_baby_out_with_the_bathwater\"><u>Don\u2019t throw the baby out with the bathwater</u></a>. Brian\u2019s content has substantially improved the lives of many people, including truth-seeking EAs. So while you might experience him as annoying, overly \"American\" or bombastic, you might be able to distill valuable pieces of information.</li></ul><h2>Masterclasses</h2><ul><li>Masterpiece days 101: It provides a motivating framing on how to think about your entire day as a potential masterpiece. Importantly, it introduces concrete cornerstone habits - e.g., deep work, shut-down complete, and digital sunset.</li><li>Greatest Year Ever: How to set ambitious yearly goals, embody your ideal self, and a generally useful framework for how to view and seize your precious time.</li><li>Conquering digital addiction 101: Shows the harm of digital consumption, perhaps a blindspot eating away at your well-being and productivity, and also provides helpful models and tools to cultivate healthy digital behavior.</li><li>Movement 101: Moving is an essential part of healthy and productive humans but is often neglected. This class points out the negative consequences of under-investing in movement and provides concrete practical advice on how to integrate more of it - beyond just going to the gym.</li><li>Goal-setting 101: Thorough and practical lessons on how to best set ambitious goals, avoid second-guessing, and use some best practices to reach them.</li><li>Anti-fragile 101: Key ideas on how to constructively respond to challenges and approach life, namely by seeing obstacles as opportunities to grow (\u201cBring it on!\u201d). The class also links to growth mindset and highlights the importance of applying insights and big ideas, especially when it\u2019s difficult.</li></ul><p><br>&nbsp;</p><h2>Philosopher Notes</h2><ul><li><a href=\"https://www.heroic.us/optimize/pn/deep-work-cal-newport\"><u>Deep Work by Cal Newport (Focus)</u></a></li><li><a href=\"https://www.heroic.us/optimize/pn/atomic-habits-james-clear\"><u>Atomics Habit by James Clear</u></a> (Behaviour Change)</li><li><a href=\"https://www.heroic.us/optimize/pn/flourish-martin-seligman\"><u>Flourish by Martin Seligman</u></a> (Positive Psychology)</li><li><a href=\"https://www.heroic.us/optimize/pn/motivation-and-personality-abraham-maslow\"><u>Motivation and Personality by Abraham Maslow&nbsp;</u></a>(Psychology of Personal Development)</li><li><a href=\"https://www.heroic.us/optimize/pn/peak-anders-ericsson-robert-pool\"><u>Peak</u></a> by<a href=\"https://www.heroic.us/optimize/authors/anders-ericsson\">&nbsp;<u>Anders Ericsson</u></a> and<a href=\"https://www.heroic.us/optimize/authors/robert-pool\">&nbsp;<u>Robert Pool</u></a> (Peak Performance)</li><li><a href=\"https://www.heroic.us/optimize/pn/mans-search-for-meaning-viktor-frankl\"><u>Man's Search For Meaning by Victor Frankl</u></a> (Practical Wisdom)<br>&nbsp;</li></ul><h2>+1\u2019s</h2><ul><li><a href=\"https://www.heroic.us/optimize/plus-one/destiny-math\"><u>What Must YOU be?</u></a></li><li><a href=\"https://www.heroic.us/optimize/plus-one/poison-into-medicine\"><u>Poison into medicine</u></a></li><li><a href=\"https://www.heroic.us/optimize/plus-one/on-humility\"><u>On humility</u></a></li><li><a href=\"https://www.heroic.us/optimize/plus-one/eudaimonology\"><u>Eudaimonology</u></a></li><li><a href=\"https://www.heroic.us/optimize/plus-one/raise-the-basement\"><u>Raise the Basement</u></a></li></ul><p>&nbsp;</p><h2>How to integrate these ideas</h2><p>In the following, we briefly outline a form of how to engage with the Heroic content and how to start integrating ideas into your life.&nbsp;</p><p>Engaging with Heroic content is straightforward: create an account and access the content for free via the Website or mobile app. We suggest engaging with (1) the +1\u2019s daily and (2) longer formats on the weekend or unhurried evenings.&nbsp; The wisdom nuggets in the form of +1\u2019s can fit well into a lunch break or morning routine. On Sundays, for instance, you can watch and process a Masterclass. Certainly, you know best how to fit educational and inspiring content into your schedule.&nbsp;</p><p>The key move, though, is pick up ideas that resonate and start applying them (as opposed to mindlessly consuming the content).</p><p><a href=\"https://www.heroic.us/optimize/plus-one/warriors-vs-librarians\"><i><u>Warriors vs. Librarians</u></i></a></p><p>When you have identified an idea or set of ideas that inspire you, let's bring them and their associated benefits to life. Rather than just consuming ideas and collecting them (like a librarian), the most important move is to take action and apply them in practice (like an applied philosopher). To deepen the understanding, we suggest bringing ideas into your life via the following pathways:</p><p><strong>Self-facilitated:</strong>&nbsp;</p><ol><li>Journal on the idea:&nbsp;<ol><li>Elaborate in your own words: What is this idea about?</li><li>How does this link to your knowledge or previous experience?</li><li>What area or current pain point does it address?</li></ol></li><li>Set up an experiment:&nbsp;<ol><li>In which way are you going to apply this idea?</li><li>How long does the experiment run?</li></ol></li><li>Monitor and review:&nbsp;<ol><li>How do you keep track of your intention to apply this idea? Consider using a simple spreadsheet.</li><li>How did your experiment go?</li><li>What are practical takeaways?</li></ol></li></ol><p><strong>Facilitated:</strong> You may feel inspired to discuss ideas with someone that helps you on your personal journey, for instance, a coach, mentor or therapist. Ask them to facilitate understanding the idea better and putting it into practice.</p><p><strong>Social:</strong> You may feel inspired to share and discuss ideas with friends and peers that are interested in wisdom and personal development. While intellectual engagement is encouraged, your aim should be to translate the idea into a practical mini-experiment.</p><h2>Conflict of interest</h2><p>Sebastian ($316) and Henning are symbolic founding investors in Heroic, which raised money through crowdfunding in 2021. On the one hand, that means you should probably assume that we're biased. On the other hand, it might signal that we actually think this is a valuable service and we'd like more people to benefit from it.&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "SebastianSchmidt"}}, {"_id": "nqN63CNyBKCgdNeNp", "title": "A positive perspective on the future of the movement", "postedAt": "2022-11-13T18:39:03.013Z", "htmlBody": "<p>We wanted to quickly share another perspective on recent events. We don\u2019t intend to interpret or respond to the current situation (and we think it\u2019s extremely important that others do so - we're only adding to the conversation), but rather offer an outlook that focuses on the past and future of the movement.</p><p>From an outside perspective, what happened to the movement in the past year?&nbsp;</p><ol><li>We received insane access to money.&nbsp;</li><li>We've gradually realized we are no longer funding-constrained (at least for longtermist projects).</li><li>We encouraged each other to be more ambitious and adopt a \u201cmoonshot\u201d mindset.</li></ol><p>And what happened to the movement now?</p><ol><li>We lost this access.</li><li>We\u2019re back to being funding-constrained. This means we\u2019re likely to tune back up back the emphasis on fundraising we had before this influx of funding.&nbsp;</li><li><strong>But we still have the ambitious plans we made. And we could still have the ambition that developed these plans.</strong> Compared to before, when we fundraise and donate now, we know what we can do. We\u2019ve gathered many concrete plans and ideas for mega projects - so now we know what we can aim for. Internally, we gained motivation from having a better sense of what our donations and fundraising can do, and externally, we can use these plans to excite potential donors.&nbsp;</li></ol><p>Looking a few months ahead, the movement is also likely to gain better mechanisms for self-checking, which is important for our future resilience.</p><p>The next few weeks might be quite shaky regarding PR. But remember that whatever negative PR we receive soon - there is no other movement that is spreading ideas and tools for seriously helping others the most. Therefore, it is up to us to recover as a movement from these waves. Whatever people say about the&nbsp;<i>infrastructure&nbsp;</i>of the movement - we need to remind ourselves that we're here for its&nbsp;<i>ideas</i>.&nbsp;</p><p>The fog will clear in several weeks and we\u2019ll probably find ourselves in a new EA landscape, quite different from how it was before this influx of funding - not that different regarding our finances, but with a better understanding of what we can do when we\u2019re ambitious.</p>", "user": {"username": "GidonKadosh"}}, {"_id": "aryJKRDxLejPHommx", "title": "SBF, extreme risk-taking, expected value, and effective altruism", "postedAt": "2022-11-13T17:44:46.310Z", "htmlBody": "<p><em>NOTE: I have some indirect associations with SBF and his companies,\nthough probably less so than many of the others who've been posting\nand commenting on the forum. I don't expect anything I write here to\nmeaningfully affect how things play out in the <em>future</em> for me, so I\ndon't think this creates a conflict of interest, but feel free to\ndiscount what I say.</em></p>\n<p><em>NOTE 2: I'm publishing this post without having spent the level of\neffort polishing and refining it that I normally try to spend. This\nis due to the time-sensitive nature of the subject matter and because\nI expect to get more value from being corrected in the comments on\nthe post than from refining the post myself. If errors are pointed\nout, I will try to correct them, but may not always be able to make\ntimely corrections, so if you're reading the post, please also check\nthe comments to check for flaws identified by comments.</em></p>\n<p><em>NOTE 3: Byrne Hobart's post <a href=\"https://www.thediff.co/p/money-credit-trust-ftx\">Money, Credit, Trust, and\nFTX</a> makes fairly\noverlapping points albeit with different emphases and a lot more\nelaboration (and less focus on the effective altruism angle).</em></p>\n<p>The collapse of Sam Bankman-Fried (SBF) and his companies FTX and\nAlameda Research is the topic du jour on the Effective Altruism Forum,\nand there have been <a href=\"https://forum.effectivealtruism.org/topics/ftx-crisis?sortedBy=new\">several posts on the\nForum</a>\ndiscussing what happened and what we can learn from it. The post <a href=\"https://forum.effectivealtruism.org/posts/j7sDfXKEMeT2SRvLG/ftx-faq\">FTX\nFAQ</a>\nprovides a good summary of what we know as of the time I'm writing\nthis post. I'm also funding work on a <a href=\"https://timelines.issarice.com/wiki/Timeline_of_FTX_collapse\">timeline of FTX\ncollapse</a>\n(still a work in progress, but with enough coverage already to be useful if\nyou are starting with very little knowledge).</p>\n<p>Based on information so far, fraud and deception on the part of SBF\n(and/or others in FTX and/or Alameda Research) likely happened and\nwere likely key to the way things played out and the extent of damage\ncaused. The trigger seems to be the <a href=\"https://twitter.com/LucasNuzzi/status/1590122590206824448\">big loan that FTX provided to\nAlameda Research to bail it\nout</a>, using\ncustomer funds for the purpose. If FTX hadn't bailed out Alameda, it's\nquite likely that the spectacular death of FTX we saw (with depositors\nlosing all their money as well) wouldn't have happened. But it's also\nplausible that without the loan, the situation with Alameda Research\nwas dire enough that Alameda Research, and then FTX, would have died\ndue to the lack of funds. Hopefully that would have been a more\ngraceful death with less pain to depositors. That is a very important\ndifference. Nonetheless, I suspect that by the time of the bailout, we\nwere already at a kind of endgame.</p>\n<p>In this post, I try to step back a bit from the endgame, and even get\naway from the specifics of FTX and Alameda Research (that I know very\nlittle about) and in fact even get away from the specifics of SBF's\nbusiness practices (where again I know very little). Rather, I talk\nabout SBF's overall philosophy around risk and expected value, <em>as he\nhas articulated himself, and has been approvingly amplified by several\nEA websites and groups</em>.  I think the philosophy was key to the\noverall way things played out. And I also discuss the relationship\nbetween the philosophy and the ideas of effective altruism, both in\nthe abstract and as specifically championed by many leaders in\neffective altruism (including the team at 80,000 Hours). My goal is to\nencourage people to reassess the philosophy and make appropriate\nupdates.</p>\n<p>I make two claims:</p>\n<ul>\n<li>\n<p>Claim 1: SBF engages in extreme risk-taking that is a crude\napproximation to the idea of expected value maximization as\nperceived by him.</p>\n</li>\n<li>\n<p>Claim 2: At least part of the motivation for SBF's risk-taking comes\nfrom ideas in effective altruism, and in particular specific points\nmade by EA leaders including people affiliated with 80,000\nHours. While personality probably accounts for a lot of SBF's\ndecisions, the role of EA ideas as a catalyst cannot be dismissed\nbased on the evidence.</p>\n</li>\n</ul>\n<p>Here are a few things I am <em>not</em> claiming (some of these are discussed\nin a little more detail toward the end of the post, though I don't\nelaborate extensively on them):</p>\n<ul>\n<li>\n<p>I'm not claiming that the EA philosophy and community create more\nincentives for deception and dishonesty than most groups do; I\nactually think the opposite is true. Rather, I'm focusing on the\nencouragement that the EA philosophy and community provide for\nrisk-taking, and the expected value framework that naively\nencourages this.</p>\n</li>\n<li>\n<p>I'm not claiming that the basic arguments about risk-taking,\nexpected value, and effective altruism are completely wrong or that\nevents with SBF have fully invalidated them. I think the basic logic\nof many of these is still fairly sound, but also that the events\nwith SBF should lead us to <em>update in the other direction</em> and to\n<em>add more nuance to our thinking about these topics</em>. I try to\narticulate this a little bit in this post, but nowhere near enough,\nand further articulation would require a separate post (perhaps by\nanother person).</p>\n</li>\n<li>\n<p>I'm not claiming that the downside of making pro-risk-taking\narguments should have been fully obvious in hindsight -- the\ncollapse of SBF / FTX is new information and whatever model we have\nof the world after it should be at least somewhat different than the\nmodel we had of the world prior to it. I do think that at least some\naspects of these points should have been given more attention in the\npast, even with the more limited information available then.</p>\n</li>\n</ul>\n<p>And to be clear, these are some things I'm <em>not</em> really covering in\nthis post:</p>\n<ul>\n<li>I'm not covering the fraught topic of whether EA leaders or people\nshould have been able to predict what specifically happened with SBF\nand FTX, and whether they had prior indications of his\ncharacter. That topic has been discussed in several other threads.</li>\n</ul>\n<h2>Claim 1 justification: SBF engages in extreme risk-taking</h2>\n<p>I won't really provide much direct justification for Claim 1; I'll\nnote in passing that a lot of commentary both on the EA Forum (such as\n<a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=GoDd83K7ipktDtWWs\">Kerry Vaughan's\nsummary</a>)\nand in external press coverage (see for instance\n<a href=\"https://www.axios.com/2022/11/11/sam-bankman-fried-risk-taking-fail\">Axios</a>). The\njustification for Claim 2 provided below is more detailed and also\nimplicitly provides further justification for Claim 1.</p>\n<p>See also Byrne Hobart's post <a href=\"https://www.thediff.co/p/money-credit-trust-ftx\">Money, Credit, Trust, and\nFTX</a>, that goes into\nsome of the math involving expected value and the historical context\nof FTX and Alameda Research.</p>\n<h2>Claim 2 justification: At least part of the motivation for SBF's extreme risk-taking comes from effective altruist ideas</h2>\n<h3>SBF's articulation in a fireside chat with Stanford EA</h3>\n<p>In a <a href=\"https://www.youtube.com/watch?v=--tV8U3BbJk&amp;t=3107s\">fireside chat with Stanford EA, SBF gives advice to students\nbased on his own\nexperience</a>. At\nfirst listening, everything he says sounds quite reasonable (in\ngeneral, SBF's public persona feels very reasonable -- something that\nfalsely causes people to feel reassured!).</p>\n<p>Here is the transcript from YouTube, lightly edited by me for\nsentencification and removal of \"um\"s and uh\"s; you can watch the\nvideo or read the original transcript on YouTube by clicking \"Show\ntranscript\" in the options under \"...\" below the video. I have\nhighlighted the portions most relevant to my points, but have not\nelided any other stuff within those segments of the video.</p>\n<blockquote>\n<p>Moderator (51:52): I think you basically answered this already but\nwhat concrete advice do you have for students for how they should be\nspending their time, how to be more ambitious, and how to better\noptimize for their goals figuring out what their goals might be or\nought to be?  Maybe what would you do differently if you were a\nstudent or a first year now at MIT or Stanford? And yeah any last\nwords that you'd want to leave the audience with before wrapping up.</p>\n</blockquote>\n<blockquote>\n<p>SBF (52:20): I'd go to back to 2010 drop out and buy a lot of\nbitcoin but but but seriously i think there's something a little bit\ntrue there although that's not exactly what I think I could have\npredicted um which is that in 2012 um I had a friend at MIT who was\nsort of bored one day I think some some guy I don't remember who\ngave one free bitcoin to every MIT student around then i think there\nwas like I don't know I was like well like five dollars at the time\nor something. Anyway, one of my friends, Gary, got bored and built\nsome Bitcoin arbitrage bots for the nascent crypto exchanges that\nwere around back in the early 2010s and made some money doing it;\nnot a lot, he sort of saturated the market.  There wasn't a lot of\nvolume but that was pretty cool. I never really checked it out that\nmuch. He was kind of tempted to got distracted he stopped doing this\nthere's like it wasn't big enough field to make much and then\nneither of us thought about crypto again for five years. And then I\ncalled him up and we founded Alameda together. Certainly in\nretrospect it's hard to argue that like it wouldn't have been\ncorrect for us to just drop everything and do that back in 2012.</p>\n</blockquote>\n<blockquote>\n<p>SBF contd (53:45): And obviously there's a lot of stupid retroactive\nretrospective thinking there where like we couldn't have known what\nwould happen but i actually think at the time we should have done it.\nWe shouldn't have been able to predict how well it would have gone but\ndiving into something that seems exciting and giving it your all and\nseeing how far it will go -- I think it's just like an incredibly good\nstrategy in life and it's way better than you know sort of sticking\naround for another few years not doing much or just sort of like\nfollowing the status quo. If you see a great opportunity I sort of\nthink take it whatever it is. If it seems way better than whatever\nelse you'd be doing by some sort of like <strong>weird expected value\ncalculation</strong> that seems like it can't possibly be right but kind of\nfeels cool i think it is <strong>probably right in expectation</strong> and yeah\n<strong>it'll probably fail that's okay most things do</strong> you try another\nthing. I don't know that that could be in a lot of different ways\nright like that could be some earning to give startup that could be\njumping in some EA organization that could be taking charge of running\nStanford EA that could be working you know diving into some biorisk\nresearch ir some other wacky thing like i don't know but there are a\nlot of awesome things to do out there and, you know, try them see how\nit goes! Try things that seem like they'll either be the right thing\nfor you to do for the time being and teach you a lot or like the\nupside if they go extremely well is extremely high and like if the\nthing you're doing is neither of those, keep your eyes\nopen for something else!</p>\n</blockquote>\n<p>And earlier in the talk, SBF says:</p>\n<blockquote>\n<p>SBF (44:05): um i think there's been a lot of very very bad messaging\nover the years on that. I think there's a lot of messaging that is all\na funding bottleneck and then a pretty sharp turn towards it's all a\ntalent bottleneck. I think they're both wrong um my sense is that both\nmatter. I think like as i've sort of spent more time trying to find\nthings to fund. I found more things to fund and and don't currently\nfeel like strongly under constrained on funding and over constrained\non talent. I think both are very much limiting factors. And there are\nways to really scale up the amount of of good that you can give to.\nSo what are some ways to do it? First of all, I think it's sort of\nlike a little awkward but it's just true um and probably not worth\nlike you know trying to to sort of ignore that like on the funding\nside it it's probably going to be very top-heavy. It's a property of\nhow the world works today that like the distribution of how much you\ncan make over various things is not it's not like a normal\ndistribution like the tail is way fatter. And it just has a pretty\nstraightforward implication, I think, which is that like if earning to\ngive is what you're thinking of doing and to be very clear I do think\nthat can be incredibly valuable and i don't think that we are\nunconstrained on funding, <strong>I think you should be thinking big</strong>. I\nthink <strong>you should be thinking in expected value terms what's the\nthing you can do that will make the most [money]</strong>. And I want to flag\nthere that <strong>if you think that the odds that you will achieve that\ntarget through the path are above 30 percent</strong>, you're almost\ncertainly not being ambitious enough! <strong>It is almost certainly going\nto be the case that there is a risk-reward trade-off here that the\nthings that make the most in expected value terms are things that will\nprobably fail</strong> and that if you're playing this correctly you should\nbe it's very likely you should be pursuing a path where you think that\n<strong>the median amount that you end up being able to donate is zero</strong> or\nvery close to it like it's sort of brutal and weird that's that's how\nthe math works um i it not always but but I think more often than not\nthat like um this is like super top heavy. You should be looking for\nthings that have extremely high upside and willing to accept that they\nmight fail, <strong>willing to accept that they will probably fail</strong> and to\nacknowledge that <strong>we're trying to maximize our collective total\nimpact and expected value on the world and you know there's no special\nvirtue associated with having at least some impact like this stuff is\nlinear</strong>. Expected values: I think are pretty brutal but\nthey are what they are! If your vision for what you're gonna do seems\nvery likely to work you should think about how to make that vision\nmore ambitious such you know obviously maximizing for how much it will\nwork given that but like probably you're not being ambitious enough if\nit seems like it'll probably work. Although it should seem like it\ncould plausibly work or otherwise probably it's a mistake.</p>\n</blockquote>\n<h3>SBF's articulation in a podcast with 80,000 Hours</h3>\n<p>In a <a href=\"https://80000hours.org/podcast/episodes/sam-bankman-fried-high-risk-approach-to-crypto-and-doing-good/\">podcast with 80,000\nHours</a>\n(full transcript available on page), SBF makes the same points about\nexpected value, but goes into a little more detail. Here are the first\nthree paras from 80,000 Hours' summary on top (emphases mine):</p>\n<blockquote>\n<p>If you were offered a 100% chance of $1 million to keep yourself, or\na 10% chance of $15 million \u2014 it makes total sense to play it\nsafe. You\u2019d be devastated if you lost, and barely happier if you\nwon.</p>\n</blockquote>\n<blockquote>\n<p>But if you were offered a 100% chance of donating $1 billion, or a\n10% chance of donating $15 billion, you should just go with whatever\nhas the highest expected value \u2014 that is, probability multiplied by\nthe goodness of the outcome \u2014 and so swing for the fences.</p>\n</blockquote>\n<blockquote>\n<p><strong>This is the totally rational but rarely seen high-risk approach to\nphilanthropy championed by today\u2019s guest, Sam Bankman-Fried</strong>. Sam\nfounded the cryptocurrency trading platform FTX, which has grown his\nwealth from around $1 million to $20,000 million.</p>\n</blockquote>\n<p>And more in the full transcript:</p>\n<blockquote>\n<p>Rob Wiblin: Yeah. Let\u2019s back up a bit, and help to set the scene for\nlisteners. What motivated you to take such a high-risk, high-return\napproach to doing good as starting your own crypto trading firm? And\nthen also just saying, \u201cWe don\u2019t like the exchanges we\u2019re operating\non. I\u2019m going to start my own crypto exchange and try to compete\nthere.\u201d</p>\n</blockquote>\n<blockquote>\n<p>Sam Bankman-Fried: This probably won\u2019t be super shocking to you, but\nwhen you think about things from \u2014 taking a step back \u2014</p>\n</blockquote>\n<blockquote>\n<p>Rob Wiblin: Expected value?</p>\n</blockquote>\n<blockquote>\n<p>Sam Bankman-Fried: If your goal is to have impact on the world \u2014 and\nin particular if your goal is to maximize the amount of impact that\nyou have on the world \u2014 that has pretty strong implications for what\nyou end up doing. <strong>Among other things, if you really are trying to\nmaximize your impact, then at what point do you start hitting\ndecreasing marginal returns? Well, in terms of doing good, there\u2019s\nno such thing: more good is more good</strong>. It\u2019s not like you did some\ngood, so good doesn\u2019t matter anymore. But how about money? Are you\nable to donate so much that money doesn\u2019t matter anymore? And the\nanswer is, I don\u2019t exactly know. But you\u2019re thinking about the scale\nof the world there, right? At what point are you out of ways for the\nworld to spend money to change?</p>\n</blockquote>\n<blockquote>\n<p>Sam Bankman-Fried: There\u2019s eight billion people. Government budgets\nrun in the tens of trillions per year. It\u2019s a really massive\nscale. You take one disease, and that\u2019s a billion a year to help\nmitigate the effects of one tropical disease. So it\u2019s unclear\nexactly what the answer is, but it\u2019s at least billions per year\nprobably, so <strong>at least 100 billion overall before you risk running\nout of good things to do with money</strong>. I think that\u2019s actually a\nreally powerful fact. <strong>That means that you should be pretty\naggressive with what you\u2019re doing, and really trying to hit home\nruns rather than just have some impact \u2014 because the upside is just\nabsolutely enormous</strong>.</p>\n</blockquote>\n<blockquote>\n<p>Rob Wiblin: Yeah. Our instincts about how much risk to take on are\ntrained on the fact that in day-to-day life, the upside for us as\nindividuals is super limited. Even if you become a millionaire,\nthere\u2019s just only so much incrementally better that your life is\ngoing to be \u2014 and getting wiped out is very bad by contrast.</p>\n</blockquote>\n<blockquote>\n<p>Rob Wiblin: But when it comes to doing good, you don\u2019t hit declining\nreturns like that at all. Or not really on the scale of the amount\nof money that any one person can make. So you kind of want to just\nbe risk neutral. As an individual, to make a bet where it\u2019s like,\n\u201cI\u2019m going to gamble my $10 billion and either get $20 billion or\n$0, with equal probability\u201d would be madness. But from an altruistic\npoint of view, it\u2019s not so crazy. Maybe that\u2019s an even bet, but you\nshould be much more open to making radical gambles like that.</p>\n</blockquote>\n<blockquote>\n<p>Sam Bankman-Fried: Completely agree. I think that\u2019s just a big piece\nof it. Your strategy is very different if you\u2019re optimizing for\nmaking at least a million dollars, versus if you\u2019re optimizing for\njust the linear amount that you make. One piece of that is that\nAlameda was a successful trading firm. Why bother with FTX? And the\nanswer is, there was a big opportunity there that I wanted to go\nafter and see what we could do there. It\u2019s not like Alameda was\ndoing well and so what\u2019s the point, because it\u2019s already doing well?\nNo. <strong>There\u2019s well, and then there\u2019s better than well \u2014 there\u2019s no\nreason to stop at just doing well</strong>.</p>\n</blockquote>\n<h3>The expected value argument and its connection with effective altruism</h3>\n<p>It's folk wisdom that personal (selfish) utility for individuals tends\nto be <em>less than linear</em> in the money they have, an idea that is also\nwidely known as the <a href=\"https://www.economicshelp.org/blog/12309/concepts/diminishing-marginal-utility-of-income-and-wealth/\">diminishing marginal utility of\nmoney</a>. One\ncommon (though probably inaccurate) approximation is that utility to\nindividuals is approximately <a href=\"https://economics.stackexchange.com/questions/23952/is-the-utility-of-money-actually-logarithmic\">logarithmic in\nmoney</a>. This\nis the motivation for the <a href=\"https://en.wikipedia.org/wiki/Kelly_criterion\">Kelly\ncriterion</a>, a widely\nreferenced criterion for how to diversify one's portfolio in order to\nmaximize the expected value of the <em>logarithm</em> of wealth. These general\nideas are well-known in economics and among a lot of intellectuals\nincluding many in the effective altruist movement.</p>\n<p>The \"altruistic\" twist here is that for individuals interested in\naltruistic impact, utility is much closer to being linear in money\nthan logarithmic. Or, we don't quite see diminishing marginal utility\nof money for altruistic purposes, at least at the amounts of money\nthat most people can make. That's because <em>the problems of the world\nare huge and can absorb huge amounts of money</em> (this is true for most\nbig problems, ranging from climate change to AI safety to global\nhealth and development to animal welfare). So basically doubling your\nwealth that you intend to allocate to charity should approximately\ndouble your impact.</p>\n<p>The basic idea is covered in a <a href=\"https://rationalaltruist.com/2013/02/28/risk-aversion-and-investment-for-altruists/\">post by Paul\nChristiano</a>\n(also <a href=\"https://80000hours.org/2015/10/common-investing-mistakes-in-the-effective-altruism-community/\">cited by 80,000\nHours</a>)\nbut he's only looking at financial investments. In contrast, SBF\npreaches and practices defining one's whole life / earning-to-give\ntrajectory around risky high-expected-value bets. See also the <a href=\"https://forum.effectivealtruism.org/topics/risk-aversion\">risk\naversion topic on the EA\nForum</a>.</p>\n<p>For more on SBF's articulation of the math and the thinking behind\nthis, see his <a href=\"https://twitter.com/SBF_FTX/status/1337250686870831107\">tweet\nthread</a> where\nhe compares the Kelly criterion (maximizing expected log wealth) with\nhis own approach that is based on closer to linear returns.</p>\n<h3>Endorsements of \"thinking big\" and more-than-normal risk-taking in effective altruism</h3>\n<p>Some of the enthusiastic agreement and encouragement of SBF's views\ncan be seen in the 80,000 Hours podcast, where the interviewer, Robert\nWiblin, agrees with and even repeats and summarizes several of SBF's\nexpected value claims. For instance, quoting from the preceding\nexcerpt of the 80,000 Hours podcast:</p>\n<blockquote>\n<p>Rob Wiblin: But when it comes to doing good, you don\u2019t hit declining\nreturns like that at all. Or not really on the scale of the amount\nof money that any one person can make. So you kind of want to just\nbe risk neutral. As an individual, to make a bet where it\u2019s like,\n\u201cI\u2019m going to gamble my $10 billion and either get $20 billion or\n$0, with equal probability\u201d would be madness. But from an altruistic\npoint of view, it\u2019s not so crazy. Maybe that\u2019s an even bet, but you\nshould be much more open to making radical gambles like that.</p>\n</blockquote>\n<p>The 80,000 Hours post <a href=\"https://80000hours.org/articles/be-more-ambitious/\">Be more\nambitious</a> makes\nfairly similar arguments about the importance of being more ambitious\nand the value of focusing on upside, and the way these become <em>more</em>\nimportant if you want to do good rather than if you are just\ninterested in your personal well-being. SBF is also cited as a case\nstudy! There are also cautionary notes later in the post about limiting\ndownsides, but the final note is still around encouraging more rather than less ambition:</p>\n<blockquote>\n<p>We advise people who are overconfident, as well as people who are\nunderconfident. But if your aim is to have an impact,\nunderconfidence seems like the bigger danger. It\u2019s better to aim a\nlittle too high than too low.</p>\n</blockquote>\n<blockquote>\n<p>But ambitious people do not need to be irrational. You don\u2019t need to\nconvince yourself that success is guaranteed. To be worth betting\non, you just need to believe that:</p>\n</blockquote>\n<blockquote>\n<p>Success is possible</p>\n</blockquote>\n<blockquote>\n<p>Your downsides are limited</p>\n</blockquote>\n<blockquote>\n<p>The expected value of pursuing the path is high</p>\n</blockquote>\n<blockquote>\n<p>If you\u2019ve found a path that might be amazing, make a backup plan and\ngive it a go. It may not work out, but it might be the best thing\nyou ever decide to do.</p>\n</blockquote>\n<p>Moreover, even the discussion of backup plans only talks of <em>personal</em>\nbackup plans, rather than backup plans to mitigate the potential\nimpact on others one does business with (for instance, customers,\nemployees, investors) or on charities and foundations that might start\ndepending on one's donation plans; emphases in the below excerpt are\nmine:</p>\n<blockquote>\n<p>Even if you can\u2019t easily estimate how likely risks are to\nmaterialise, you can often do a lot to limit them, freeing yourself\nup to focus on upsides.</p>\n</blockquote>\n<blockquote>\n<p>Over time, you can aim to set up your life to make yourself more\nable to take risks. Some of the most important steps you can take\ninclude:</p>\n</blockquote>\n<blockquote>\n<p>Building up your financial security. If you\u2019re at constant risk of\nfailing to make your rent, that\u2019s a serious downside you can\u2019t\ndiscount.  Looking after your physical and mental health and\nimportant relationships, so that your lifestyle is sustainable.\nBuilding valuable career capital that gives you backup options,\ne.g. through building skills or finding good mentors.  When\ncomparing different career paths, here are some tips:</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>Consider \u2018downside scenarios\u2019 for each of the paths you\u2019re\nconsidering. What might happen in the worst 10% of scenarios?</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"2\">\n<li>Look for risks that are really serious. It\u2019s easy to have a vague\nsense that you might \u2018fail\u2019 by embarking on an ambitious path, but\nwhat would failure actually be like? <strong>The risks to be most\nconcerned about are those that could prevent you from trying again,\nor that could make <em>your</em> life a lot worse</strong>. You might find that\nwhen you think about what would actually happen if you failed, your\nlife would still be fine. For example, if you apply for a grant for\nan ambitious project and don\u2019t get it, you will have just lost a bit\nof time.</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"3\">\n<li>If you identify a serious risk of pursuing some option, see if\nyou can modify the option to reduce that risk. Many entrepreneurs\nlike Bill Gates are famous for dropping out of college, which makes\nthem look like risk-takers. But besides the security provided by his\nupper middle-class background, Gates also made sure he had the\noption to return to Harvard if his startup failed. By modifying the\noption, starting Microsoft didn\u2019t involve much risk at all. Often\nthe most useful step you can do here is to have a good backup plan,\nand this is part of our planning process.</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"4\">\n<li>If you can\u2019t modify the path to reduce the risk to an acceptable\nlevel, eliminate that option and try something else.</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"5\">\n<li>Check with your gut. If <em>you</em> feel uneasy about embarking on a\npath even after taking the steps above, there may be a risk you\nhaven\u2019t realised yet. Negative emotions can be a sign to keep\ninvestigating to figure out what\u2019s behind them.</li>\n</ol>\n</blockquote>\n<p>Will MacAskill, a key figure in effective altruism, was an early influence on SBF and pushed him in the earning-to-give direction. This is confirmed by SBF in both the 80,000 Hours podcast and the Stanford EA interview; it's also described in\n<a href=\"https://www.newyorker.com/magazine/2022/08/15/the-reluctant-prophet-of-effective-altruism\">this New Yorker\narticle</a>:</p>\n<blockquote>\n<p>He had recently become vegan and was in the market for a righteous\npath. MacAskill pitched him on earning to give.</p>\n</blockquote>\n<p>MacAskill has also made the general argument that if your goals are\naltruistic, you should be much more ruthless in your pursuit of scale\nand take on more risk. The <a href=\"https://youtu.be/YkdI8ztqWZc?t=10140\">video I could most readily find was a deep\ndive with Ali Abdaal</a> and was\ntalking about altruistic impact through \"direct work\" rather than\ndonations, but elsewhere in the video he does suggest a kind of\nexchange rate between the two depending on one's direct impact in\ncomparison to the value of donations.</p>\n<blockquote>\n<p>Will MacAskill (2:49:00): This also is a difference between if you're\ntrying to optimize for impact versus income so yeah you might think\nlike okay got a couple of million in the bank now i'm just going to be\nhappy with that like i can just seek that out like additional money's\nnot worth that much more. Because you've got like it is three million\nYouTube subscribers?</p>\n</blockquote>\n<blockquote>\n<p>Ali Abdaal: About that</p>\n</blockquote>\n<blockquote>\n<p>Will MacAskill: Okay, yeah, so you're like if I had six million I'd\nhave a bit more money but it's not going to be a huge difference in my\nwell-being, [so] I'm not particularly motivated to grow the\nnumbers.</p>\n</blockquote>\n<blockquote>\n<p>Ali Abdaal: except i don't have like an impact goal</p>\n</blockquote>\n<blockquote>\n<p>Will MacAskill: Exactly! But now if you're having impacts yeah how much better the six\nmillion subscribers than three million.</p>\n</blockquote>\n<blockquote>\n<p>Ali Abdaal: yeah way better</p>\n</blockquote>\n<blockquote>\n<p>Will MacAskill: Probably about twice as good like maybe not exactly\nlike but like to first approximation yeah and so having altruistic\nimpact in mind gives like much stronger arguments for scaling.</p>\n</blockquote>\n<h3>Linearity on the low end: the lower bound of zero impact and non-consideration of <em>negative</em> impact by losing money</h3>\n<p>In the above discussion, my focus when talking about the\nclose-to-linear altruistic returns of money was on the upside/positive\nside: you can scale up giving since the world's problems are so\nbig. However, there's another direction where this is important as\nwell: the direction toward zero (and beyond?).</p>\n<p>One sometimes-implicit and sometimes-explicit idea in SBF's discourse\nis the idea that utility is close enough to linear in money, and as an\nimportant corollary, there's a lower bound at zero. The worst-case\noutcome here is making nothing, in which case you make no donations\nand therefore have effectively zero impact. So risk-taking has very\nhigh upside but only a limited downside -- in the worst case, you're\nwiped out, you declare bankruptcy, maybe you even die penniless.</p>\n<p>From a <em>selfish</em> perspective, this is a pretty bad outcome (and\nindeed, a logarithmic model of utility would give an infinite\n<em>negative</em> utility to having no money). So from a selfish perspective,\nthere's a big downside to being wiped out, and this is part of what\nmotivates risk-aversion.</p>\n<p>From an altruistic perspective, however, getting to zero money is a\nbad outcome but only to the extent that it represents the absence of\ngood outcomes. So it's an outcome that you try to avoid, but not all\nthat desperately.</p>\n<p>Moreover, this simple framework was developed mostly in connection\nwith people managing their own savings, rather than running complex\ncompanies that manage <em>other people's</em> investments and assets. So it\ndoesn't even begin to grapple with the idea of going <em>negative</em> and\nthe utility implications of that. Of course, personal wealth can be\nnegative when one puts money on a credit card or takes a student loan,\nbut these are relatively small amounts and people generally start\nthinking of altruism when they're no longer in significant debt. My\nguess is that SBF (consciously or subconsciously) rounds up \"going\nnegative\" to zero because ultimately it just means he's able to donate\nzero money.</p>\n<h3>Startup risk and the kicking in of caution</h3>\n<p>A lot of what SBF said about risk-taking makes a lot of sense in the\ncontext of somebody trying a startup idea (having earmarked some sort\nof safety net that they won't touch, and then using other funds from\nthemselves or outside investors that are explicitly understood to be\nfor the purpose). What also tends to happen is that once the startup\nstarts succeeding and real people start depending on it for real\nstuff, it starts moving in a more conservative direction -- reducing\nthe riskiness of its actions. There are probably four factors that\npush in that direction:</p>\n<ol>\n<li>\n<p><strong>The founders/owners now have more to lose from a purely selfish\nperspective</strong>; this essentially comes from the \"diminishing marginal\nutility of money\" idea albeit it may or may not be seen in purely\nfinancial terms. For instance, after a company grows from\nnear-nothing to being worth a few million, and the founders have\nshares worth a decent chunk of that, they are at risk of losing\nthat money if they tank the startup.</p>\n</li>\n<li>\n<p><strong>The founders/owners have a desire to succeed and to not mess\nthings up (e.g., because they now feel more passionately about the\nthing they're building, they feel attached to its success, or to\navoid embarrassment)</strong>. Messing up an already-big company feels\nmore embarrassing, and can be more guilt-inducing as well to the\nextent that one sees the pain caused to others.</p>\n</li>\n<li>\n<p><strong>The founders/owners have needed to involve other stakeholders,\nwho also can lose out if things go bad</strong>. This includes investors,\nemployees, customers, partners, etc. Some of them may have\nincentives to take more risk (particularly investors who want to\nget big payouts from a diversified portfolio) but others benefit\nfrom greater stability and less risk. Moreover, since different\nstakeholders see the riskiness of various actions differently, and\nsome level of agreement is needed, the overall direction will be\ntoward less risk.</p>\n</li>\n<li>\n<p><strong>Third parties may put more pressure of various sorts</strong>; this\nincludes regulators, hackers, a hostile press, or various other\nactors. In the face of this pressure, more caution and care may be\nneeded.</p>\n</li>\n</ol>\n<p>A <a href=\"https://danluu.com/wat/\">great post by Dan Luu</a> talks about how\nGoogle and Microsoft ultimately got serious about security after\nembarrassing incidents. He writes:</p>\n<blockquote>\n<p>Google didn't go from adding z to the end of names to having the\nworld's best security because someone gave a rousing speech or wrote\na convincing essay. They did it after getting embarrassed a few\ntimes, which gave people who wanted to do things \u201cright\u201d the\nleverage to fix fundamental process issues. It's the same story at\nalmost every company I know of that has good practices. Microsoft\nwas a joke in the security world for years, until multiple\ndisastrously bad exploits forced them to get serious about\nsecurity. This makes it sound simple, but if you talk to people who\nwere there at the time, the change was brutal. Despite a mandate\nfrom the top, there was vicious political pushback from people whose\nposition was that the company got to where it was in 2003 without\nwasting time on practices like security. Why change what's worked?</p>\n</blockquote>\n<p>So what was special about the SBF situation where they were able to\nget to such a huge scale <em>without</em> these sorts of things kicking in?\nLet's go through the four points:</p>\n<ol>\n<li>\n<p><strong>The founders/owners now have more to lose from a purely selfish\nperspective</strong>: I think that although this was true, it probably\nwasn't as true in SBF's <em>perception</em> because his mental model was\nthat of altruistic impact and linear utility. So making what he\nconsidered a positive-EV bet when the company was worth $5 billion\nmay not have felt that different from making a positive-EV bet when\nthe company was worth $5 million. So at least the absence of this\nparticular mechanism was tied to the altruistic endgame of the\nmoney.</p>\n</li>\n<li>\n<p><strong>The founders/owners have a desire to succeed and to not mess\nthings up (e.g., because they now feel more passionately about the\nthing they're building, they feel attached to its success, or to\navoid embarrassment)</strong>: My guess is that while SBF obviously had a\ndesire to succeed and not mess things up, he didn't actually feel\nthat passionate about the value of the work he was doing and saw it\nas a gamble to make money; as long as it was EV-positive, he was\nwilling to take big risks even after amassing a lot of wealth. I\nbelieve that this stemmed very directly from his EA-influenced\nthinking about risk and value.</p>\n</li>\n<li>\n<p><strong>The founders/owners have needed to involve other stakeholders,\nwho also can lose out if things go bad</strong>: The failure of this\nmechanism doesn't seem directly tied to SBF's EA connection, but\nmay be more of a feature of the business: they were able to get to\na fairly large scale without having a lot of different\nstakeholders, and were also able to preserve a fair amount of\nsecrecy despite the openness of the blockchain.</p>\n</li>\n<li>\n<p><strong>Third parties may put more pressure of various sorts</strong>: This\ndidn't happen ... until it did, and then everything collapsed. The\nfailure here in the wider world seems mostly unrelated to EA and\nmay have more to do with the novelty of the space and therefore the\nlack of relevant critical expertise; however, the failure to notice\nthis within EA was likely due to EA's positive impression of SBF\nand his expected value-maximizing ideals.</p>\n</li>\n</ol>\n<h2>Further thoughts (without extensive justification)</h2>\n<h3>Tentative thoughts on where I think SBF went wrong in his thinking</h3>\n<p>I had listened to SBF's fireside chat shortly after it had come\nout. His thoughts on taking risk had been interesting to me, insofar\nas it differed from my own philosophy on risk, but I didn't consider\nhim <em>wrong</em> per se. If anything, listening to him made me update my\npriors slightly <em>toward</em> taking more risk. I couldn't find anything\nvery categorically wrong in what he said.</p>\n<p>Upon further reflection, I actually think that what he said was\ndirectionally incorrect in several ways, and what ended up happening\nto him is directional evidence that I should be updating <em>away</em> from\nthe direction of his advice. In particular, I suspect that these are\nsome areas where he's wrong:</p>\n<ul>\n<li>\n<p>Even viewed altruistically, going to zero or negative money has\npretty sharp negative utility, particularly the way it ended up\nhappening to FTX (with a loss of customer deposits and attendant\nsuffering of many people). But the drop would have plausibly been\nbad even in tamer scenarios (for instance, if Alameda had openly\ngone bankrupt and FTX had died out with the collapse of the FTT).</p>\n</li>\n<li>\n<p>I suspect that altruistic impact is less than linear in money, and\nthat there are a lot of other details about the way things play out,\nthat affect altruistic impact. For instance, I suspect that FTX\ncould have had a significant positive impact if it had quit with SBF\nmaking enough to earmark a billion dollars for charity. That would\nhave been enough money to champion the values and start a pattern of\naltruism that ultimately could have been continued by other donors\n(ironically, <a href=\"https://youtu.be/n4_TeUInMpU\">Nick Beckstead makes the point that individual funders\nmay have relatively few good grants to\nmake</a> and that's why Future Fund\nexperimented with delegating grantmaking to a larger number of\nregrantors; I think a similar point applies at the foundation level\nas well).</p>\n<p>This would obviously have been better than what ultimately\ntranspired, but I suspect it would have been better even in properly\ndone expected value calculations. This is a tricky point to justify\nand I won't attempt to do it here.</p>\n</li>\n<li>\n<p>SBF fails to account for optimism bias ... if you think you've got a\n30% chance of success, you probably have much less. I suspect he was\nultimately a victim of this optimism bias.</p>\n</li>\n</ul>\n<p>I could be wrong about several of these points; I'm also not\nretreading the familiar ground of how what ended up happening (and was\nlikely a direct result of SBF's actions) was terrible and unethical\netc. I'm making the point that even the original risk-taking was\nprobably wrong from a perspective of maximizing altruistic impact.</p>\n<h3>Luxurious lifestyle?</h3>\n<p><a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=neLHt7r7naRSB6YCh\">Oliver Habryka's\ncomment</a>\nseems valuable:</p>\n<blockquote>\n<p>Yep, I was and continue to be confused about this. I did tell a\nbunch of people that I think promoting SBF publicly was bad, and\ne.g. sent a number of messages when some news article that people\nwere promoting (or maybe 80k interview?)  was saying that \"Sam\nsleeps on a bean bag\" and \"Sam drives a Corolla\" when I was quite\nconfident that they knew that Sam was living in one of the most\nexpensive and lavish properties in the Bahamas and was definitely\nnot living a very frugal livestyle.</p>\n</blockquote>\n<p>I think this is important as part of the general point that SBF was\nsuccessful at cultivating a certain kind of image in the media that\ndidn't reflect his reality. However, I don't think that the fact (that\nhis actual lifestyle was an order of magnitude more luxurious than his\npublic persona might indicate) undercuts the general claim that his\nmain goal was to make a bunch of money to donate. Even this moderately\nmore luxurious lifestyle was still well within his means, and if\nmaintaining that lifestyle were his goal, exiting after making a few\nhundred million dollars would probably have been a selfishly smart\nthing to do.</p>\n<h3>Noble lies?</h3>\n<p><a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=NbevNWixq3bJMEW7b\">A comment by Oliver Habryka</a> covers more relevant attributes of\nSBF that, if true, could be part of the reason for FTX's ultimate\ncollapse:</p>\n<blockquote>\n<p>I definitely would have put Sam into the \"un-lawuful oathbreaker\"\ncategory and have warned many people I have been working with that\nSam has a reputation for dishonesty and that we should limit our\nengagement with him (and more broadly I have been complaining about\nan erosion of honesty norms among EA leadership to many of the\ncurrent leadership, in which I often brought up Sam as one of the\nsources of my concern directly).</p>\n</blockquote>\n<blockquote>\n<p>I definitely had many conversations with people in \"EA leadership\"\n(which is not an amazingly well-defined category) where people told\nme that I should not trust him. To be clear, nobody I talked to\nexpected wide-scale fraud, and I don't think this included literally\neveryone, but almost everyone I talked to told me that I should\nassume that Sam lies substantially more than population-level\nbaseline (while also being substantially more strategic about his\nlying than almost everyone else).</p>\n</blockquote>\n<blockquote>\n<p>I do want to add to this that in addition to Sam having a reputation\nfor dishonesty, he also had a reputation for being vindictive, and\nalmost everyone who told me about their concerns about Sam did so\nwhile seeming quite visibly afraid of retribution from Sam if they\nwere to be identified as the source of the reputation, and I was\nnever given details without also being asked for confidentiality.</p>\n</blockquote>\n<p>These claims about dishonesty, if true, could help explain why SBF\nfailed to get the right sort of feedback and checks and balances that\ncould have prevented him from making risky moves.</p>\n<p>In a <a href=\"https://en.wikipedia.org/wiki/The_Inventor:_Out_for_Blood_in_Silicon_Valley\">documentary on Theranos founder Elizabeth\nHolmes</a>,\npsychology professor Dan Ariely said that his experiments found that\npeople are much more likely to lie a little bit when the effect of the\nlying will send money to charity rather than when they pocket the\nmoney from their lies. He also claimed that lie detector tests have\nmore trouble catching lies spoken by people who are lying to send more\nmoney to charity. He claimed that people feel less conflicted about\nwhat they consider noble lies than what they consider self-serving\nlies, and therefore can lie more convincingly when they think it's for\nthe greater good. I have not checked the research myself, but you can\nsee a summary of the argument in <a href=\"https://www.businessinsider.com/theranos-elizabeth-holmes-the-inventor-hbo-psychology-of-lying-dan-ariely-2019-3\">this Business Insider\narticle</a>. I\nhave also heard that Ariely himself got into trouble for <a href=\"https://www.buzzfeednews.com/article/stephaniemlee/dan-ariely-honesty-study-retraction\">allegedly\nfake data in another\nexperiment</a>,\nbut the general point he made seems plausible even if you don't put\nmuch weight on his experimental evidence.</p>\n<p>To the extent that SBF engaged in dishonesty without having any of the\n\"tells\" that dishonest people have, his altruistic endgame might be\npart of the reason for it. <em>However</em>, I don't see this as being\nheavily connected with <em>effective altruism</em> as a philosophy,\ncommunity, or social movement. That's because in general, <em>unlike with\nrisk-taking</em>, the EA philosophy and community has <em>not</em> encouraged\nlying. If anything, it has, at least in explicit statements, put a\ngreater premium on integrity than most people do.</p>\n<h3>A counterfactual thought experiment</h3>\n<p>One standard way of evaluating whether A causes B is to think about a\nworld where A hadn't happened and ask whether B was likely to have\nhappened in that world.</p>\n<p>Here's a thought experiment -- what would have happened if the\nexisting EA philosophy and community hadn't had this strong bent\ntoward risk-taking and thinking big, and/or hadn't been pushing\nearning to give? It's definitely pretty unclear, but I think:</p>\n<ol>\n<li>\n<p>In the absence of an earning-to-give push, there's a pretty good\nchance that SBF would have gone on to do some form of direct work\ninitially (e.g., working on animal welfare as was his original\nintent, or getting into some work in the longtermist space).</p>\n</li>\n<li>\n<p>In the absence of a push to be more ambitious, there's a pretty\ngood chance that SBF would have felt content working at Jane Street\nCapital and donating a chunk of money to charity, and would only\nhave left it to pursue direct work. You can take a look at his <a href=\"https://measuringshadowsblog.blogspot.com/\">old\nblog</a> -- reads just\nlike an ordinary earning-to-giver.</p>\n</li>\n<li>\n<p>If he hadn't internalized the utility-is-linear-in-money argument\nthat is common in EA circles (and that pushed him to continue to\ntake similar risks despite amassing large sums of money), it's\nlikely that SBF would have exited after Alameda Research's initial\ntrading success and then used that to make donations.</p>\n</li>\n</ol>\n", "user": {"username": "vipulnaik"}}, {"_id": "L4S2NCysoJxgCBuB6", "title": "Announcing Nonlinear Emergency Funding", "postedAt": "2022-11-13T18:02:33.918Z", "htmlBody": "<p><i>[Applications are now closed. Please </i><a href=\"https://forum.effectivealtruism.org/posts/HPdWWetJbv4z8eJEe/open-phil-is-seeking-applications-from-grantees-impacted-by\"><i>apply to Open Phil</i></a><i> or </i><a href=\"https://forum.effectivealtruism.org/posts/buEazpmcKJhM5KRGx/sff-speculation-grants-as-an-expedited-funding-source\"><i>SFF</i></a><i> or other funders]</i></p><p>Like most of you, we at&nbsp;<a href=\"http://www.nonlinear.org\"><u>Nonlinear</u></a> are horrified and saddened by recent events concerning FTX.&nbsp;<br><br>Some of you counting on Future Fund grants are suddenly finding yourselves facing an existential financial crisis, so, inspired by the&nbsp;<a href=\"https://fastgrants.org/\"><u>Covid Fast Grants</u></a> program, we\u2019re trying something similar for EA. If you are a Future Fund grantee and &lt;$10,000 of bridge funding would be of substantial help to you, fill out this&nbsp;<a href=\"https://tally.so/r/w4arzd\"><u>short form</u></a> (&lt;10 mins) and we\u2019ll get back to you ASAP.&nbsp;</p><p><br>We have a small budget, so if you\u2019re a funder and would like to help, please reach out:&nbsp;<a href=\"mailto:katwoods@nonlinear.org\"><u>katwoods@nonlinear.org</u></a></p><p>[Edit: This funding will be coming from non-FTX funds, our own personal money, or the personal money of the earning-to-givers who've stepped up to help. Of note, we are undecided about the ethics and legalities of spending Future Fund money, but that is not relevant for this fund, since it will be coming from non-FTX sources.]</p>", "user": {"username": "katherinesavoie"}}, {"_id": "arA65LFet5K9KDeMF", "title": "Will AI Worldview Prize Funding Be Replaced?", "postedAt": "2022-11-13T17:10:55.447Z", "htmlBody": "<p><br>UPDATE: QUESTION HAS BEEN ANSWERED - https://forum.effectivealtruism.org/posts/3kaojgsu6qy2n8TdC/pre-announcing-the-2023-open-philanthropy-ai-worldviews</p><p>This question is regarding the Future Fund\u2019s <a href=\"https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize\">AI Worldview Prize</a>, and how the FTX collapse will affect this - in particular, how likely it is another funder will step in to award participants prizes.</p><p>I recently spent a couple weeks researching AI Risk arguments from first principles and was planning to spend the next five weeks continuing to do research and writing a submission for this contest.</p><p>Considering the large prize pool (up to 1.5 million dollars) and obvious importance of this issue to EA and longtermism, I am sure there are many others in this situation as well.</p><p>It seems possible that in order to not disrupt the work of researchers like myself, and to replace rewards for prize participants who have already submitted, another large funder such as Open Philanthropy may want to step in to partially or fully place this prize.</p><p>My question is:</p><p>How likely is something like this is to occur, so that I can plan accordingly?</p><p>Of course an official answer would be ideal, but any thoughts or input is appreciated. Thanks!</p>", "user": {"username": "Jordan Arel"}}, {"_id": "o8B9kCkwteSqZg9zc", "title": "Thoughts on legal concerns surrounding the FTX situation", "postedAt": "2022-11-13T17:01:03.096Z", "htmlBody": "<p><strong>Edited to add</strong>: I've posted a clarifying remark in the comments section <a href=\"https://forum.effectivealtruism.org/posts/o8B9kCkwteSqZg9zc/thoughts-on-legal-concerns-surrounding-the-ftx-situation?commentId=FKQpFSgzsknhTryud\">here</a>.&nbsp;</p><p>As Open Phil\u2019s managing counsel, and as a member of the EA community, I\u2019ve received an onslaught of questions relating to the FTX fiasco in the last few days. Unfortunately, I\u2019ve been unable to answer most of them either because I don\u2019t have the relevant legal expertise, or because I can\u2019t provide legal advice to non-clients (and Open Philanthropy is my only client in this situation), or because the facts I\u2019d need to answer the questions just aren\u2019t available yet.&nbsp;</p><p>The biggest topic of concern is something along the lines of: if I received FTX-related<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefia3c965i7ud\"><sup><a href=\"#fnia3c965i7ud\">[1]</a></sup></span>&nbsp;grant money, what am I supposed to do? Will it be clawed back? This post aims to provide legal context on this topic; it doesn\u2019t address ethical or other practical perspectives on the topic.</p><p>Before diving into that, I want to offer this context: this is the first few days of what is going to be a multi-year legal process. It will be drawn out and tedious. We cannot will the information we want into existence, so we can\u2019t be as strategic as we\u2019d like.&nbsp;</p><p>But there\u2019s an upside to that. Emotions are high and many people are probably not in a great mental place to make big decisions. This externally-imposed waiting period can be put to good use as a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BesfLENShzSMeb7Xi/community-support-given-ftx-situation\"><u>time to process</u></a>.&nbsp;</p><p>I understand that for some people who received FTX-related grant money, waiting doesn\u2019t feel like an option; people need to know whether they can pay rent, or if their organization still exists. I hope some of the information below will provide a little more context for individual situations and decisions.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref06ytsb8zkdvm\"><sup><a href=\"#fn06ytsb8zkdvm\">[2]</a></sup></span></p><p>I also committed to putting out an explainer on clawbacks. That is&nbsp;<a href=\"https://docs.google.com/document/d/1XLEnM9FBzH9XKNw_wrB5snz4Iru95dqKaM_GTlbgBrA/edit?usp=sharing\"><u>here</u></a>, though I think the information in this post is more useful.&nbsp;&nbsp;</p><h1>Bankruptcy and Clawbacks&nbsp;</h1><h2>Background</h2><p><i>The information in this section is based on publicly available information and general conversations with bankruptcy lawyers. I do not have access to any nonpublic information about this case. None of this should be taken as legal advice to you.</i></p><p>FTX filed for bankruptcy on Friday (November 11th, 2022). More specifically, Alameda Research Ltd. filed a voluntary petition for bankruptcy under Chapter 11 of the Bankruptcy Code, by filing a standard form in the United States Bankruptcy Court for the District of Delaware. The filing includes 134 \u201cdebtor entities\u201d (listed as Annex I); it looks like this covers the full FTX corporate group.</p><p>This means that the full FTX group is now under the protection of the bankruptcy court, and over the coming months, all of the assets in the debtor group will be administered for the benefit of FTX\u2019s creditors. By filing under Chapter 11 (instead of Chapter 7), FTX has preserved the option of emerging out of the bankruptcy proceeding and continuing to operate in some capacity. You can read a useful explainer on the bankruptcy process&nbsp;<a href=\"https://www.uscourts.gov/services-forms/bankruptcy/bankruptcy-basics/chapter-11-bankruptcy-basics\"><u>here</u></a>.&nbsp;</p><p>The rules in the Bankruptcy Code are ultimately trying to ensure a&nbsp;<i>fair</i> outcome for creditors. This includes capturing certain payment transactions that occurred in the past. Basically, the debtor can reach back in time to undo deals it made and recoup monies it paid; this money comes back into the estate through clawbacks and gets redistributed to creditors according to the bankruptcy rules.&nbsp;</p><h2>Clawbacks</h2><p>There are two main types of clawback processes. The first and most common (called a \u201cpreference claim\u201d) target transactions that happened in the 90 days prior to the bankruptcy filing.&nbsp;<strong>Essentially, if you received money from an FTX entity in the debtor group anytime on or after approximately August 11, 2022, the bankruptcy process will probably ask you, at some point, to pay all or part of that money back.</strong> It\u2019s almost impossible to say right now whether any specific grant or contract will be subject to clawback \u2013 there just isn\u2019t enough information on the court docket \u2013 and if your transaction is captured, you will eventually receive formal notice from the bankruptcy court and will have an opportunity to raise a defense, negotiate a settlement, or litigate. You can read a legal explainer about preference claims&nbsp;<a href=\"https://blogs.orrick.com/distressed-download/2015/02/13/decoding-the-code-preferences-under-section-547-of-the-bankruptcy-code/\"><u>here</u></a>.&nbsp;</p><p>The second clawback process (called a \u201cfraudulent conveyance claim\u201d) targets transactions that happened up to two years prior to the bankruptcy filing. The root of these claims is an allegation that the debtor moved assets out of their organization&nbsp;<i>for the purpose of</i> frustrating future creditor claims. These types of claims are more complicated to prove, less commonly brought, and more individualized to the specific transaction.&nbsp;</p><p><strong>It is way too early to tell</strong> whether and how fraudulent conveyance claims might be used in the FTX bankruptcy proceeding. But they would tend to target larger transactions or transfers that seem irregular, even if the recipient (e.g., grantees receiving funds) did nothing wrong and had no knowledge of any impropriety. Again, if your transaction is captured in a fraudulent conveyance claim (essentially if you last received money from an FTX entity more than 90 days ago), you will receive formal notice and have an opportunity to make your own case. I don\u2019t currently have a good explainer for fraudulent conveyances, but the topic is somewhat covered in the explainer our external counsel helped put together, which is&nbsp;<a href=\"https://docs.google.com/document/d/1XLEnM9FBzH9XKNw_wrB5snz4Iru95dqKaM_GTlbgBrA/edit?usp=sharing\"><u>here</u></a>.&nbsp;</p><h2><br>Being a Creditor</h2><p>Finally, to the extent you have a binding agreement entitling you to receive funds, you may qualify as an unsecured creditor in the bankruptcy action. This means that&nbsp;<strong>you could have a claim for payment</strong> against the debtor\u2019s estate. If you are listed as a potential creditor, you\u2019ll receive official notice from the court about the process for getting your claim on file; even if you don\u2019t receive notice, if you think you have a claim, you will be able to file one proactively. It\u2019s not at all clear whether there will be any assets left over for unsecured creditors, especially ones that are not depositors on the exchanges.&nbsp;&nbsp;</p><h2>Timing</h2><p><strong>I want to emphasize that this process is likely to take months to unfold.</strong> I expect that FTX will file more documents with the court in the coming days; those documents should include at least a partial list of assets, debts, and potential creditors (including those captured in the lookback periods). Many court filings are publicly accessible, and you can look them all up yourself as they are disclosed on the court\u2019s docket, but I will try to make the more substantive or useful filings available for you. I will update with a link when the folder is ready.&nbsp;</p><h1>Looking Forward</h1><p>I intend to share thoughts on another big area of concern - communications with FTX and FTXF-related people, and document preservation, in a future post.&nbsp;</p><p>For further resources surrounding bankruptcy proceedings and clawbacks:&nbsp;</p><p>I\u2019ll do my best to provide updates as more relevant information is disclosed, and to maintain a current database of the most useful filings in the bankruptcy action&nbsp;<a href=\"https://drive.google.com/drive/folders/1LBAwkHyEgU4CJBkH_iS-NWJW1dff6hkO?usp=sharing\"><u>here</u></a>.&nbsp;</p><p>As major developments unfold in the bankruptcy case, I currently plan to provide general updates to the community via the EA Forum. As part of that effort, I also plan to share explainers prepared by bankruptcy lawyers and other experts. This is a large, complex case and there will be a lot of people watching, reporting and giving takes, which I will aim to share.</p><p>Finally, I am working to organize additional legal support options for the FTX grant recipient individuals and organizations that are most impacted as they navigate this process.&nbsp;&nbsp;</p><p><br><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnia3c965i7ud\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefia3c965i7ud\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I say FTX-related and not FTXF because my impression is that grant money came from many different entities, and not just FTX Foundation.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn06ytsb8zkdvm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref06ytsb8zkdvm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In case this is helpful additional information: my husband is an FTXF grantee who received his money more than 90 days ago, and we\u2019ve personally decided to halt committing any more of that money for now, though that has very little impact on our personal financial situation so is not a hard decision.&nbsp;</p></div></li></ol>", "user": {"username": "Molly"}}, {"_id": "huHNg2HSxhXLqzzkb", "title": "Tainted donations: a historical perspective from Rhodri Davies", "postedAt": "2022-11-13T16:37:29.873Z", "htmlBody": "<p>I think Rhodri Davies produces interesting content that is largely underappreciated in EA. He's particularly strong at providing a historical perspective on philanthropy issues.</p><p>The <a href=\"https://www.philanthropisms.com/1862997/10835981\">podcast I linked to</a> in this post covers tainted donations -- donations from a source that is considered \"bad\" in some sense. Typically the key question is whether you should accept or return such funds. You may prefer his <a href=\"https://twitter.com/Philliteracy/status/1463986748678156289?s=20\">twitter thread</a> covering essentially the same content.</p><p>This content was produced before it was known that the FTX money was (likely?) from fraudulent activity, and it wasn't designed for an EA audience, but in the current circumstances, EAs might find it to be interesting context.</p><p>Those of you who love content in audio format might favour this podcast. However I think its content is of variable interestingness. Here are a few of my opinions:</p><ul><li>Rhodri talks about St Augustine's views on tainted donations -- this is not very interesting in my view, since it tells us little more than \"people were thinking about this topic as early as the 5th century\"</li><li>Similar comments apply to The Venerable Bede</li><li>Rhodri's coverage of St Aquinas is a bit more interesting. It includes a distinction between \"ill-gotten gains\" (money from quite bad sources) and \"filthy lucre\" (actually criminal sources, I think). I still didn't feel like Aquinas had provided us with a clear rationale for this distinction, or even a clear view on whether \"alms\"/charity from ill-gotten gains was permissible, but at least it felt like there was a bit more interesting content.</li><li>I found the story about the Paris Guild of Prostitutes interesting. They asked to make a financial contribution to the building of the Notre Dame cathedral, which permitted them to have their trade depicted on the stained glass windows. They suggested that instead of a depiction of their trade being on the stained glass windows (!) there should instead be a depiction of the Virgin (!!)</li><li>George Bernard Shaw's argument was worth hearing. He claimed that \"practically all the spare money in the country consists of a mass of rent, interest and profit, every penny of which is bound up with crime, drink, prostitution, disease, and all the evil fruits of poverty\" and \"all our money is tainted\".</li></ul><p>Given that the interestingness of the content was, in my view, variable, I ended up stopping listening the podcast partway through and switched to the <a href=\"https://twitter.com/Philliteracy/status/1463986748678156289?s=20\">twitter thread</a> instead, having noticed that the two seemed to overlap very heavily.</p>", "user": {"username": "Sanjay"}}, {"_id": "uQqZG7nWXTJJQ3oAd", "title": "An Outsider's View on the FTX Crash", "postedAt": "2022-11-13T15:25:12.498Z", "htmlBody": "<p>I'm writing this post to help understand the community the impact of the FTX Crash on the image of SBF and EA.&nbsp;</p><p>The following information comes from a meeting of an amateur investor group a relative is in. Said relative is not involved in EA at all. In the meeting, the FTX Crash and SBF's actions were a topic of discussion. The focus is not on EA, but on the economic implications of what happened. However, SBF's EA commitments are mentioned, which is why I think it might be valuable to post those parts, if only slightly so.</p><p>English is not the language used in the meeting, I translated it myself, so mistakes are possible. I did try to ensure the way in which things are said is reflected in the translation. To be clear: I don't endorse anything said here, nor do I agree or disagree with anything said here and I can't say what parts of it are true and which aren't.</p><blockquote><p>The person who is responsible might end up in jail. That's what would happen if this happened in the bank sector.</p></blockquote><blockquote><p>What he did is a pure scam. What he has accomplished, cannot be done without being smart or having a moral compass.&nbsp;</p><p>He invested a lot in media appearance. He hired YouTubers where he pushed hard on his image. An image he used to become the popular as well. He often pushed that he would give away millions, billions and be the most philantropic.</p><p>He knew that by not being transparent he could scam people. He used the money of his clients to invest in his own coin so he became rich himself.&nbsp;</p></blockquote><p>Personally, I'm under the impression that they believe that he used EA as a PR stunt to enrich himself and that his end goal was not to donate. I don't think that is true, but if many people believe it, I do think it can be especially harmful to the image of EA.</p>", "user": {"username": "Jeroen De Ryck"}}, {"_id": "eoLwR3y2gcZ8wgECc", "title": "Hubris and coldness within EA (my experience)", "postedAt": "2022-11-13T14:54:47.474Z", "htmlBody": "<p>Hi all.</p>\n<p>Like a lot of people that have had a connection to EA I am appalled by the close connection between the FTX scandal and EA. But not surprised.</p>\n<p>The EA community events I attended totally killed my passion for EA. I attended an EA global conference in London and left feeling really really sad. Before the conference I was told I was not important enough or not worth the time to get career advice. One person I'd met before at local EA events made it clear that he didn't want to waste time talking to me (this was in the guide btw to make it clear if you don't think someone is worth your time). Well it certainly made me unconfident and uncomfortable to approach anyone else. I found the whole thing miserable. Everyone went out to take photo for the conference and I didn't bother. I don't want to be part of a community that I didn't feel happy in.</p>\n<p>On a less personal level, I overheard some unpleasant conversations about how EA should only be reserved for the intellectual elite (whatever the fuck that is) and how diversity didn't really matter. How they were annoyed that women got talks just for being women.</p>\n<p>Honestly, the whole place just reeked of hubris - everyone was so sure they were right, people had no interest in you as a person. I have never experienced more unfriendly, self-important, uncompassionate people in my life (I am 31 now). It was of course the last time I was ever involved with anything EA related.</p>\n<p>Maybe you read this and can dismiss it with yeah but issues are too important to waste time with petty small talk or showing interest in others. Or your subjective experience doesn't matter. Or we talk about rationality and complex ideas here , not personal opinions.</p>\n<p>But that is the whole point I'm trying to make. When you take away the human element, when you're so focused on grandiose ideas and certain of your perfect rationality, you end up dismissing the fast thinking necessary to make good ethical decisions. Anyone that values human kindness would run a mile from someone that doesn't have the respect to listen to someone talking to them and makes clear that their video game is valued above that person. Similarly to the long history of Musk's contempt for ordinary people.</p>\n<p>EA just seems so focused on being ethical that it forgot how to be nice. In my opinion, a new more inclusive organisation with a focus on making a positive impact needs to be created - with a better name.</p>\n", "user": {"username": "James Gough"}}, {"_id": "DB9ggzc5u9RMBosoz", "title": "Wrong lessons from the FTX catastrophe", "postedAt": "2022-11-14T00:39:30.179Z", "htmlBody": "<p>There remains a large amount of uncertainty about what exactly happened inside FTX and Alameda. I do not offer any new takes on what occurred. However, there are the inklings of some \"lessons\" from the situation that are incorrect regardless of how the details flesh out. Pointing these out now may seem in poor taste while many are still coming to terms with what happened, but it important to do so before they become the canonical lessons.</p><h3>1. Ambition was a mistake</h3><p>There have been a number of calls for EA to \"go back to bed nets.\" It is notable that this refrain conflates the alleged illegal and unethical behavior from FTX/Alameda with the philosophical position of longtermism. Rather than evaluating the two issues as distinct, the call seems to assume both were born out of the same runaway ambition.&nbsp;</p><p>Logically, this is obviously not the case. Longtermism grew in popularity during SBFs rise, and Future Fund did focus on longtermist projects, but FTX/Alameda situation has no bearing on the truth of longtermism and associated project's prioritization.&nbsp;</p><p>To the extent that both becoming incredibly rich and affecting the longterm trajectory of humanity are \"ambitious\" goals, ambition is not the problem. Committing financial crimes is a problem. Longtermism has problems, like knowing how to act given uncertainty about the future. But an enlightened understanding of ambition accommodates these problems: We should be ambitious in our goals while understanding our limitations in solving them.&nbsp;</p><p>There is an uncomfortable reality that SBF symbolized a new level of ambition for EA. That sense of ambition should be retained. His malfeasance should not be.&nbsp;</p><p>This is not to say that there may be lessons to learn about transparency, overconfidence, centralized power, trusting leaders, etc from these events. But all of these are distinct from a lesson about ambition, which depends more on vague allusions to Icarus than argument.&nbsp;</p><h3>2. No more earning to give</h3><p>I am not sure that this is being learned as a \"lesson\" or if this situation simply \"leaves a bad taste\" in EA's mouths about earning to give. The alleged actions of the FTX/Alameda team in no way suggest that earning money to donate to effective causes is a poor career path.</p><p>Certain employees of FTX/Alameda seem to have been doing distinctly unethical work, such as grossly mismanaging client funds. One of the only arguments for why one might be open to working a possibly unethical job like trading crypto currency &nbsp;is because an ethically motivated actor would do that job in a <i><strong>more ethical way</strong></i> than the next person would (from <a href=\"https://80000hours.org/articles/harmful-career/\">Todd and MacAskill</a>). Earning to give never asked EAs to pursue unethical work, and encouraged them to pursue any line of work in an ethically upstanding way.&nbsp;</p><p>(I &nbsp;emphasize Todd and MacAskill conclude in their 2017 post: \"We believe that in the vast majority of cases, it\u2019s a mistake to pursue a career in which the direct effects of the work are seriously harmful\").&nbsp;</p><p>Earning to give in the way it has always been advised-by doing work that is basically ethical-continues to be a highly promising route to impact. This is especially true when total EA assets have significantly depreciated.&nbsp;</p><p>There is a risk that EAs do not continue to pursue earning to give, thinking either that it is icky post-FTX or that someone else has it covered. This is a poor strategy. It is imperative that some EAs who are well suited for founding companies shift their careers into entrepreneurship as soon as possible.&nbsp;</p><h3>3. General FUD from nefarious actors</h3><p>As the EA community reals from the FTX/Alameda blow up, a number of actors with histories of hating EA have chimed in with threads about how this catastrophe is in line with X thing they already believed or knew was going on within EA.&nbsp;</p><p>I believe, at least in many cases, this is being done in straightforward bad faith. In some cases, it seems like a deliberate effort to sow division within the EA movement. This is the sort of speculative and poorly founded claim that is very unpopular on the Forums, but the possibility this may be happening should be taken seriously. When the same accounts are liking threads taking glee in this disaster or mocking the appearances and personal writing of FTX/Alameda employees, it seems unlikely that they are simply interested in helping EA.&nbsp;</p><p>EAs, in a reasonable aim to avoid tribalism, have taken these additional allegations very seriously. After all-one could, in theory learn productive lessons from someone trying to antagonizing them. Yet this is largely a mistake. Tribalism makes for poor reasoning, but so does naively engaging with a dishonest interlocutor who is trying to manipulate you.</p><p>There are legitimate questions here, like the nature of the early Alameda split. I encourage EAs to have conversations internally or to sympathetic counter-parties, who are actually working on solving the newly revealed problems with EA, rather than those celebrating EA's alleged downfall.&nbsp;</p><h3>4. Most hot takes on ethics&nbsp;</h3><p>Events are not evidence to the truth of philosophical positions. There was already a critique of expected value reasoning and utilitarianism could drive people to do certain immoral actions or take too large of risks. The FTX/Alameda catastrophe gives no more evidence against utilitarianism than the famous utilitarian hospital, and no more evidence lying to the hypothetical murder at the door gives evidence against deontology.&nbsp;</p><p>Those frustrated or appalled by the situation should reflect on their ethical framework, but they should not make hasty and poorly founded jumps to \"<a href=\"https://twitter.com/RYChappell/status/1590773440113709057\">rule utilitarianism</a>\" or \"<a href=\"https://marginalrevolution.com/marginalrevolution/2022/11/is-the-ea-movement-dead.html\">virtue ethics</a>\" or newly invented blends of ethical systems. I appreciate the work of <a href=\"https://forum.effectivealtruism.org/posts/nvus8kuGxyacyfXeg/naive-vs-prudent-utilitarianism\">Richard Chappell</a> and others in tempering these impulses. As Richard <a href=\"https://twitter.com/RYChappell/status/1591366642352926720\">reminds</a> us: \"Remember, folks: If your conception of utilitarianism renders it *predictably* harmful, then you're thinking about it wrong.\" Deep ethical reflection is worth doing, but it tends to be better done in a reflective place rather than a reactionary one.&nbsp;</p><p>There is a separate set of critiques, such as how closely one should try to adhere to an ethical system. For example, Eliezer Yudkowsky <a href=\"https://twitter.com/ESYudkowsky/status/1497157447219232768\">recommends</a> we \"Go three-quarters of the way from deontology to utilitarianism and then stop . . . &nbsp;Stay there at least until you have become a god.\" There are also outstanding questions to whether EA or utilitarianism could have encouraged SBF's actions. These lines of inquiry seem sensible in response to recent events, but they are distinct from analyzing the ground level truth of ethical systems. And while these should be investigated, they should be investigated thoughtfully as they are large and important questions, not hastily without even the facts properly established.&nbsp;</p><p>Finally, it's worth pointing out that ethical uncertainty - the position that Will MacAskill invented and advocates for - looks reasonably good.&nbsp;</p><p>In general, I hope that any lessons taken from recent events come after due reflection and many deep breaths. And I hope everyone is doing OK and wish everyone who was negatively affected outside and inside the EA community my best. &nbsp;<br>&nbsp;</p>", "user": {"username": "burner"}}, {"_id": "G9RHEcHMLguGJY7uP", "title": "You *should* have capacity for more transparency and accountability", "postedAt": "2022-11-13T11:52:39.735Z", "htmlBody": "<p>There are frequent discussions on the EA forums regarding how transparent EA orgs should be.</p>\n<p>Transparency is expensive and opportunity costs are high - the money and time costs that could be devoted to <em>very quickly</em> publishing information about the activities of orgs alongside conflicts of interest is money and time that could be spent on object level impact.</p>\n<p>The chances of better transparency detecting bad behaviour is low - obviously, most people do not break common sense ethical rules.</p>\n<p>But many EAs have argued that high degrees of transparency are worth it anyway, because other than making it likelier that bad behaviour is caught, it enforces accountability and *disincentivises bad behaviour in the first place *.</p>\n<p>Obviously, the costs of letting bad behaviour continue are high, both through potential direct harm and through reputational risks to EA.</p>\n<p>I think the FTX saga should significantly update orgs towards spending more money and time on becoming highly transparent and accountable to the community, even if it means spending less money and time on effective causes in the short term.</p>\n<p>(I\u2019m thinking about a post where this was discussed a while ago, and someone commented \u201cjust hire more people\u201d or something along those lines and everyone got annoyed at them. Does anyone have a link to this post?)</p>\n", "user": {"username": "freedomandutility"}}, {"_id": "4bDRdeRHH57G34fv9", "title": "In favour of \u2018personal policies\u2019", "postedAt": "2022-11-13T10:15:25.866Z", "htmlBody": "<p>I\u2019m pretty sad this week and isolating due to having covid. I thought I\u2019d try to have something positive come out of that, and write up a blog post I\u2019ve had in the back of my mind for a while.&nbsp;</p><p>I wanted to share my positive experience with setting \u2018policies\u2019 for myself. They\u2019re basically heuristics I use to avoid having to make as many decisions (particular about things that are somewhat stressful). I got the idea, and suggestions for ways to implement it, from Brenton Mayer (thank you!).</p><h2>What are personal policies?</h2><p>There are various classes of decisions I make periodically, for which I\u2019d like to have an answer in advance rather than deciding in individual cases. Those are the kinds of cases in which I try to make a \u2018policy decision\u2019 going forward.&nbsp;</p><p>This is the kind of thing we do all the time for particular types of actions. For example, someone might decide to be a vegetarian. From then, they no longer consider in each individual instance whether they should eat some dish with meat in, they\u2019ve made a blanket rule not to do so in any individual case.&nbsp;</p><p>There are a number of things like \u2018being a vegetarian\u2019 which we\u2019re used to. We\u2019re less likely to make up our own of these \u2018rules I plan to live by\u2019. A way we might frame it when we do is as \u2018getting into a habit\u2019. I sometimes prefer the framing of \u2018policy\u2019 in that it\u2019s instantaneous (whereas something can\u2019t really be a habit until you\u2019ve done it a few times) and it sounds like a clear decision you\u2019re acting on.&nbsp;</p><p>A way I like to think of this is:&nbsp;<i>For tricky repeating decisions, make them only once.</i>&nbsp;</p><p>Having said that, for long run policies, it\u2019s likely you\u2019ll want to have periodic re-examinations of them to check you still endorse them. I keep a list of my policies, both to make them easier to remember and to come back and re-evaluate them.</p><h2>Use cases and benefits</h2><h3>Make faster decisions</h3><p>Having a policy for some type of decision means you don\u2019t have to spend time making a decision in each specific case.&nbsp;</p><p>One of my friends has the policy of always running for a train if there\u2019s one she wants to be on which looks about to leave. This is the kind of situation where we\u2019re often unsure what to do - is it impossible to make the train however fast I am? Do I have plenty of time because it\u2019s not about to leave? Time spent dithering increases the chance you miss the train even if you then choose to run for it. So having made the decision in advance means over the long run you\u2019ll catch more trains than you otherwise would have. (And given the downside is basically some extra cardio, this seems easily worth it.)</p><h3>Make better decisions</h3><p><strong>More thoughtful decisions:&nbsp;</strong>Even aside from cases where you don\u2019t have enough time to think much about a decision (like catching a train), it\u2019s worth putting more time into a blanket policy than an individual decision. Rather than half-thinking through a type of decision a number of times, you might act better if you make a careful/thorough decision once and then act on that repeatedly (When the decisions are relevantly similar.).</p><p><strong>More objective decisions:&nbsp;</strong>In some cases, individual decision points are emotionally laden in a way that will bias your decision. In cases like that, you might make a more objective decision in advance than you would in the moment. For example, you might find it hard not to donate to a charity that\u2019s raising money on the street if you have to decide whether to do that on the spur of the moment. You might feel your decision will be more objective if you think beforehand about the circumstances under which you do and don\u2019t want to donate to charities when you pass fundraisers for them (eg yes for particular interventions, no for others).</p><p><strong>Help from others:&nbsp;</strong>I often find it easier to make decisions in discussion with another person. Typically people aren\u2019t that keen on getting phone calls at random times about small decisions you\u2019re making right now. Whereas they\u2019re often quite on board with helping you figure out some ongoing policy.&nbsp;</p><p>For policies where the decisions are going to feel hard in the moment, I sometimes figure out with a second person their take on why the policy decision is the right one. The idea is to find a framing that allows the decision I endorse to be easier in the moment. Sometimes that\u2019s just writing down that the second person agrees with the policy. Sometimes it\u2019s something more specific. For example, I find it pretty hard to send rejection emails. A piece of language I have in a doc for myself on this is \u201cIf the whole point is to help applicants do good, it seems like it\u2019s breaking the implicit contract with them to do things that reduce the good I\u2019m / they\u2019re doing\u201d.</p><p><strong>Being able to rely on your future self:&nbsp;</strong>Sometimes having a policy about decisions I\u2019ll need to make in future helps me make better decisions now. I recently realised I should have the policy of \u2018if I\u2019m going to do some minor medical procedure I expect to hurt (eg smear test), I should take plenty of pain meds in advance, including codeine\u2019. I\u2019ve typically ended up putting off booking things I think might hurt because it\u2019s aversive to book them knowing that will trigger them happening. And I\u2019ve prevaricated on how much to take pain meds on the day because if I take codeine I\u2019ll be less clear headed for work afterwards. Having made the decision in advance that I\u2019ll take extra pain meds, I\u2019m expecting to find it easier to book them on time. That seems worth it to me.</p><h3>Reduce stress</h3><p>Probably the biggest benefit I get from having policies is reducing stress in decision making. I have a tendency to want to optimise everything I do, and therefore to angst over every decision. It\u2019s often decidedly less stressful overall to make a hard decision once and then stick to it. That\u2019s so even in some cases where the initial decision of \u2018I\u2019ll do x in every instance\u2019 is harder to make than \u2018I\u2019ll do x just this once\u2019.</p><p>An example that\u2019s been particularly useful for me was thinking about how much and when to work. A year or so ago I ended up caught in a negative cycle around how much I felt I should work. I had a picture in my head that being a good mother meant spending my whole weekend with my son. But then I kept not getting as much work done as I wanted to during the week, and therefore deciding to make late notice childcare arrangements at the weekend I didn\u2019t feel good about. I felt like I was always doing a bad job as a mother and also not helping the world as much as I wanted.&nbsp;</p><p>The way that situation eventually got fixed was that I sat down with a friend/colleague to make a weekly plan I actually endorsed. It felt really aversive to do that, because I basically had the views that \u2018good mothers don\u2019t work at weekends\u2019 and also \u2018I ought to work more than 5 days a week\u2019. I didn\u2019t want to look at the fact that I was going to fail at at least one of those. But doing it alongside Brenton made that feel less bad.&nbsp;</p><p>I ended up making a plan that involved working typically 5 and a half days a week, with flexibility to go up to 6 and down to 5. That included realising that I needed my views to be coherent, and thinking about what the concrete details of each week should look like and how to make that work in a way that my husband, son and I all felt good about. Having made a specific plan makes me feel less guilty to Leo about working on Saturday and less guilty to the world about not working on Sunday, because I know I made a long term policy decision that took everyone into consideration.</p><p>This \u2018reducing stress\u2019 benefit to setting policies will probably be biggest for people who tend to feel anxiety and/or be perfectionist. In some ways the idea is similar to the <a href=\"https://www.worry-tree.com/blog/how-to-use-worry-time\">worry time technique</a>, in which a person allots a time per day as \u2018worry time\u2019, and pushes off worries at other times to then. Rather than continuously asking yourself something like \u2018should I work harder today\u2019, you make one decision \u2018I\u2019ll work x hours per day\u2019 and don\u2019t think further about it until your next policy re-evaluation point.</p><h2>Examples of policies</h2><p>I thought it would be useful to share a few more of the policies I have or have had to give some colour on the types of situations in which this can be useful. I\u2019d rather not get into debates about whether individual of them seem reasonable. I think people will have pretty different intuitions about the object level policies, and a reason for having them is that I think the decisions are non-straight forward and stressful to make.</p><p><strong>Gratitude journaling:</strong> Filling in a gratitude journal can feel like procrastinating, or being selfish, when there\u2019s a lot going on. I realised that I was fairly unreliable at filling it in, and worse at doing so when I wasn\u2019t feeling that good. I talked through with my someone how sensible it seemed to do this reliably each day. Taking some time to think through the benefits I seem to get from it and how much time it costs me made it feel more like doing it daily was sensible rather than selfish. And having a second person share their view on this seeming reasonable felt validating.</p><p><strong>Transport:</strong> I periodically go to San Francisco / Berkeley. I spent a longish while each time using ubers and feeling guilty about not trying out the subway. I once or twice used the subway, and found it kind of stressful. I put \u2018figure out what I endorse about how to get around SF/Berkeley\u2019 on my list of things to answer during my yearly personal review. When I reviewed it, I decided I should was willing to bear the extra cost of using uber whenever I\u2019m there rather than trying to get into the habit of using a different transport system I find kind of confusing and unsafe feeling. I\u2019ve ended up feeling glad not to worry everytime I take a journey there about which I should use.</p><p><strong>Pain killers:&nbsp;</strong>A friend of mine has the policy \u2018If I think \u2018should I take an ibuprofen?\u2019, then I take one\u2019. They found they were systematically taking pain killers less often than they endorsed through forgetting, or wondering whether they \u2018really needed to\u2019, or not wanting to get out of bed at night. They find having this policy helps actually spur them to action.</p><p><strong>Communal policies:&nbsp;</strong>My household (I imagine like most!) had various policies in place during covid. I liked that I didn\u2019t need to think about \u2018what\u2019s the risk of doing x and is that worth it?\u2019 but could just follow policies like \u2018stay x far away from people\u2019 or \u2018use a bathroom only if no-one outside the household has used it for at least y long\u2019.&nbsp;</p><p><strong>Avoiding anxiety:&nbsp;</strong>I tend to find it stressful not to get the types of food I\u2019m used to, particularly if there are other sources of stress around too. For example, I am pretty caffeine addicted but I find it stressful to drink coffee black or with non-dairy milk. I used to sometimes go to events and skip out on coffee (leading to headache) or try to drink black coffee (usually didn\u2019t end up drinking it), and then feel generally guilty about being so difficult. Now my policy to avoid relying on an event for providing my morning caffeine - I carry caffeine tablets, and have given myself permission to buy coffee from a cafe before events.</p><p><strong>Taking holiday:&nbsp;</strong>It feels hard to know how much is the right amount of holiday to take. My current written policy on this is: \u201cIf I really want a holiday and I have holiday days left, I'll take a holiday, and not think at the object level whether I theoretically could continue / go back to being motivated without one.\u201d This might sound kind of vacuous rather than action guiding. But I have actually found having the policy useful. It originated from a time I wanted to visit some friends during the pandemic, but to do so I\u2019d have had to pull Leo out of nursery for two weeks beforehand to isolate, and it felt hard to justify to myself so much time away from helping others. I think the appropriate attitude for me to have in that and other cases though was that I want to be living a happy and sustainable life alongside helping others, rather than agonising over every individual case. So I took the trip, and wrote down a policy for the future.</p><h3>Two more habits</h3><p>Here are two additional habits which aren\u2019t that much like policies, but which I get a lot out of and so wanted to share.&nbsp;</p><p><strong>Mitigating anxiety:</strong> At times when I get anxious, I\u2019m typically not very sensible about steps to take to become calmer and happier. For that reason, I have a checklist of things to do if I\u2019m feeling particularly anxious. That makes it as easy as possible to remember all the basic things that might help. (Eg Have I had the right amount of food? Of caffeine? Should I call someone - such as x, y or z? Should I sit quietly listening to music?)</p><p><strong>Savouring:&nbsp;</strong>I have an Anki deck devoted to memories I want to savour. I\u2019ve been really enjoying having this. I\u2019ve added photos of people I care about / events I enjoyed, and screenshots of nice things people said in emails/slack. I\u2019ve also taken photos of cards I particularly appreciated and of objects that reminded me of good times (partly so I don\u2019t have to make space to keep all those physical things). I add things periodically to the deck. I typically go through it after going through the facts I want to memorise in anki. I love being periodically reminded of lovely moments I might otherwise forget, and being prompted to spend extra time savouring my favourite parts of life.&nbsp;</p>", "user": {"username": "Michelle_Hutchinson"}}, {"_id": "hCExaMoRsmLeonx6p", "title": "Responsibility and Reform: Are we learning the right lessons from FTX?", "postedAt": "2022-11-13T14:48:42.767Z", "htmlBody": "<p>Many have now written about the implications of what is unfolding at FTX, including those considered thought leaders within the community. What is missing is wider acknowledgment that <strong>core parts of existing EA methodology make things like fraud defensible, and therefore likely</strong>. And unless the community is able to acknowledge this, I would assign a high probability to something similar happening again in the future.</p><p>There are two main categories of responses I've seen so far.</p><ol><li>SBF made a bad calculation. His math was wrong. His decisions were not positive EV. He should have seen this. Instead he irrationally doubled down on bad bets.</li><li>Consequentialists must keep one foot in deontology and virtue ethics as a check and balance. Fraud does not pass muster on those, so don't do it.</li></ol><p>I'd argue that the (2) is actually a variation of (1). For, what is the point of identifying as a consequentialists (or having one foot in it), unless we know when to shift our weight from one side to the other?&nbsp;</p><p>What I think the second response is actually saying is: \"We are currently not sophisticated enough to make certain calculations. Each additional order of effects adds compounding error and uncertainty . Until we have more accurate models, let other ethical models serve as our guide\". i.e. treat deontology as a fallback heuristic when uncertainty is high.&nbsp;</p><p>Though, I'd imagine this should be unsatisfactory and hand wavy for committed consequentialists or utilitarians. The world is highly uncertain! If the output of an ethical model when faced with uncertainty is to defer to other models, then it's not a particularly useful model. And I think those making the second response recognize this. The post <a href=\"https://forum.effectivealtruism.org/posts/XHrHsrQGyr4NnqCA7/we-must-be-very-clear-fraud-in-the-service-of-effective\">We must be very clear: fraud in the service of effective altruism is unacceptable</a> ends with the following:</p><blockquote><p>Additionally, if you're familiar with decision theory, you'll know that credibly pre-committing to follow certain principles\u2014such as never engaging in fraud\u2014is extremely advantageous, as it makes clear to other agents that you are a trustworthy actor who can be relied upon. In my opinion, I think such strategies of credible pre-commitments are extremely important for cooperation and coordination.</p><p>Furthermore, I will point out, if FTX did engage in fraud here, it was clearly in fact not a good idea in this case: I think the lasting consequences to EA\u2014and the damage caused by FTX to all of their customers and employees\u2014will likely outweigh the altruistic funding already provided by FTX to effective causes.</p></blockquote><p>i.e. SBF made a bad calculation. And that costs will likely outweigh the benefit. And so what we're currently left with as the primary responses of this community, are variations of \"What SBF did was wrong, because his math was wrong\".&nbsp;</p><p>The implication is that the math for committing fraud almost never works out. But, it might. And at some point, at some odds, in some model, fraud will output a positive EV. Much of what SBF has spoken about in the past suggests that is what happened with him. And the lower probability outcome (fraud detected) from his model ended up hitting. Whether or not one believes this to be the case with SBF, there is a reasonable hypothetical case to be made for someone to do something similar using these principles.</p><p>Further, there is a chance that at some point in the future, people will look back at some of the positive changes and reforms that come out of this episode, and think - \"You know, maybe it was a good thing this happened!\"&nbsp;</p><p>This is unlikely to be a healthy approach or conclusion for this community. And so, here are some thoughts on things to consider and/or reform.&nbsp;</p><h3><strong>A Possible Way Forward</strong></h3><p><u>High Certainty Altruism:</u> \"Effective Altruism\" till recently was primarily about things like funding for distributing bednets, cash transfers, RCTs etc. These interventions have had a large positive impact on the world and saved many lives. We can say this because evidence shows this to be the case with <strong>high certainty</strong>. And I'm confident that this community will play an important role in finding many more of these interventions.</p><p>High Certainty Altruism (HCA) is a more responsible and honest description of the methods involved in this type of giving. We often push back on this, but by labelling the giving done by this community as \"effective\", one implies that other approaches are \"ineffective\" - further implying that the very same ethical models one is treating as essential fallback heuristics are lacking in some way.</p><p>Also, many of the experimental actions, discussions and research done by those in the EA community are often conflated with the more run-of-the-mill HCA stuff - both internally and externally. Decoupling the two will be important going forward.</p><p><u>Abandon Expected Value Calculations</u>: Or, continue to assign numbers, but skip the math. Unlike money, most things we care about in the world are not fungible. People are not fungible. Aspirations are not fungible. Gains in one place do not neatly cancel out or substitute losses in the other. This does not mean we abandon the rigor and clarity that comes with assigning numbers and probabilities when faced with complex tradeoffs. Nor do we ignore them as information or <i>inputs</i> into our decision making.&nbsp;</p><p>But the act of adding or subtracting these numbers for EV or net benefit calculations, to arrive at a neat answer or conclusion, is I think the source of many problematic decisions. It aids an aura of objectivity which is undeserved, and strips accountability. I plan to write more about this later, but here is an example to demonstrate the two different approaches:</p><p><strong>With EV:</strong><br>Policy A: 100k Net Jobs Added<br>Policy B: 80k Net Jobs Added<br><br><strong>Without EV:</strong><br>Policy A: 120k Jobs Added, 20k Jobs Lost<br>Policy B: 85k Jobs Added, 5k Jobs Lost<br><br>Which is the better policy? Unlike \"With EV\", the \"Without EV\" approach presents numbers and information as inputs, but does not editorialize or imply what the \"right\" decision is. There is responsibility and accountability that comes with not treating these numbers as fungible.&nbsp;</p><p>It's also a good check on hubris - one does not hide the negative impacts behind a positive sounding aggregate number. Some jobs are going to be lost. Whatever a person decides, that person will be at the receiving end of blame. The processes and ideas that can help a person navigate these decisions, and deal with the consequences and responsibilities, are where this community can play a healthy role.</p><p><u>Epistemology, not Ontology</u>: One of the most fulfilling things about this community is its perpetual curiosity and quest for information. Collecting evidence, asking questions and discussing all the different ways in which people look at the world is where this community really shines. I'd like to see a shift of focus to becoming a source for all these different <i>inputs</i> into ethical decision making; and less on generating neat outputs of what is right vs. wrong. The trolley problem does not have a right answer - I'd feel horrible choosing either option. But there are still many things to be learned, and questions we can ask about why people might choose one option vs. the other, and how they might deal with the consequences of their tough decisions.</p>", "user": {"username": "hawkebia"}}, {"_id": "4HG6FvkJ69BEskvvW", "title": "Equanimity: a word of hope, for hope.", "postedAt": "2022-11-13T06:22:48.430Z", "htmlBody": "<p><i>(quickly written: </i>a short meditation on hope and resilience, that others might find helpful in emotionally navigating the ftx crisis. i love my friends in the EA community, and i hope this little amateur writing offers some serenity. i find myself occasionally seeking comfort in the writings of other EAs, feels right to pay it forward; seeing as posts like this might be helpful, i decided to just share it)</p><p>&nbsp;</p><p><i><strong>Equanimity</strong></i><strong> </strong>:</p><blockquote><p>noun<i>&nbsp;</i></p><p><i>mental calmness, composure, and evenness of temper, especially in a difficult situation.&nbsp;</i></p></blockquote><p>there\u2019s loss, there\u2019s grief, there\u2019s sorrow.</p><p>there\u2019s anger, there\u2019s confusion, there\u2019s pain.</p><p>possibly less from what has transpired,</p><p>but more from the potential of all that could have been.</p><p>of financial loss, distrust, distraught.</p><p>of jeopardised plans, career uncertainties,&nbsp;</p><p>scrapped proposals and projects.</p><p>it will take time for us to heal.&nbsp;</p><p>&nbsp;</p><p><i>please, have a good cry</i>.&nbsp;</p><p>it hurts, it\u2019s hard \u2014 but it\u2019s never the end.</p><p>we are tested \u2014 we bend, but do not break.</p><p>take time \u2014 &nbsp;reclaim sanity, regain clarity.</p><p>we will bounce back wiser, clearer, stronger as a community.</p><p>open yourself up to hope, as there is life \u2014</p><p>there <i>lies</i> life.</p><p>&nbsp;</p><p>hope where it's hardest is hope at its greatest.</p><p>\"we are not drowning, we are fighting.\"&nbsp;</p><p>our collective dreams (of a thoughtful community) are worth fighting for.</p><p>beyond resilience \u2014 embrace <a href=\"https://fs.blog/an-antifragile-way-of-life/\">antifragility</a>.</p><p>this too, shall pass.</p><p>we fail, we learn, we grow.</p><p>we\u2019ll grow.</p><h1>\ud83c\udf3b</h1>", "user": {"username": "tzukit"}}, {"_id": "vuWGw4nvq2qJn5eCE", "title": "Noting an unsubstantiated belief about the FTX disaster", "postedAt": "2022-11-13T05:37:02.860Z", "htmlBody": "<p>There is a narrative about the FTX collapse that I have noticed emerging<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0hj8vxyyolts\"><sup><a href=\"#fn0hj8vxyyolts\">[1]</a></sup></span>&nbsp;as a commonly-held &nbsp;belief, despite little concrete evidence for or against it. The belief goes something like this:</p><ul><li>Sam Bankman Fried did what he did primarily for the sake of \"Effective Altruism,\" as he understood it. Even though from a purely utilitarian perspective his actions were negative in expectation, he justified the fraud to himself because it was \"for the greater good.\" As such, poor messaging on our part<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefq4tl61u2tg\"><sup><a href=\"#fnq4tl61u2tg\">[2]</a></sup></span>&nbsp;may be partially at fault for his downfall.</li></ul><p>This take may be more or less plausible, but it is also unsubstantiated. <a href=\"https://twitter.com/astridwilde1/status/1591474124224081922?s=20&amp;t=NN2h20ROFVVoKu8MpmgDGg\">As Astrid Wilde noted on Twitter</a>, there is a distinct possibility that the causality of the situation may have run the other way, with SBF as a conman taking advantage of the EA community's high-trust environment to boost himself.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgqikujmyods\"><sup><a href=\"#fngqikujmyods\">[3]</a></sup></span>&nbsp;Alternatively (or additionally), it also seems quite plausible to me that the downfall of FTX had something to do with the social dynamics of the company, much as Enron's downfall can be traced back to [insert your favorite theory for why Enron collapsed here]. We do not, and to some degree cannot, know &nbsp;what SBF's internal monologue has been, and if we are to update our actions responsibly in order to avoid future mistakes of this magnitude (which we absolutely should do), we must deal with the facts as they most likely are, not as we would like or fear them to be.</p><p>All of this said, I strongly suspect<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0vret8m5so0l\"><sup><a href=\"#fn0vret8m5so0l\">[4]</a></sup></span>&nbsp;that in ten years from now, conventional wisdom will hold the above belief as being basically cannon, regardless of further evidence in either direction. This is because it presents an intrinsically interesting, almost Hollywood villain-esque narrative, one that will surely evoke endless \"hot takes\" which journalists, bloggers, etc. will have a hard time passing over. Expect this to become the default understanding of what happened (from outsiders at least), and prepare accordingly. At the same time, be cautious when updating your internal beliefs so as not to assume automatically that this story <i>must be</i> the truth of the matter. We need to carefully examine where our focus in self-improvement should lie moving forward, and it may not be the case that a revamping of our internal messaging is necessary (though it may very well be in the end; I certainly do not feel qualified to make that final call, only to point out what I recognize from experience as a temptingly powerful story beat which may influence it).</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0hj8vxyyolts\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0hj8vxyyolts\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Primarily on the Effective Altruism forum, but also on Twitter.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnq4tl61u2tg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefq4tl61u2tg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See e.g. \"pro fanaticism\" messaging from some community factions, though it should be noted that this has always been a minority position.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngqikujmyods\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgqikujmyods\">^</a></strong></sup></span><div class=\"footnote-content\"><p>EDIT: Some in the comments have pointed out that as SBF has been involved with EA since pretty much forever, it's unlikely that he was sociopathically taking advantage of the community, and therefore we should not morally absolve ourselves. To this I have two primary responses: A) This may be the case, but it do not mistake this objection as defeating the main point, which is that EA ideology was not necessarily the cause of <i>this aspect</i> of his life. We should definitely be introspective in considering how to prevent this in the future, but we should also not beat ourselves up unnecessarily if doing so would be counterproductive. &nbsp;B) It is unclear how deeply he actually believed in EA ideals, and how much of his public persona has been an act\u2014anecdotes (<s>and </s><a href=\"https://twitter.com/SilverBulletBTC/status/1591403692246589444?s=20&amp;t=Vv3IXrf-pDybyEpzSNR5Ug\"><s>memes like this one</s></a><s>, which I am unsure how much weight to put on it as evidence; probably fairly little</s>) suggest the latter, though as someone who's never met him personally it's hard to say.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0vret8m5so0l\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0vret8m5so0l\">^</a></strong></sup></span><div class=\"footnote-content\"><p>With roughly 80% confidence, conditional on 1.) No obviously true alternative story coming out about FTX that totally accounts for all their misdeeds somehow, and 2.) This post (or one containing the same observation), not becoming widely cited (since feedback loops can get complex and I don't want to bother accounting for that).</p></div></li></ol>", "user": {"username": "Yitz"}}, {"_id": "j7sDfXKEMeT2SRvLG", "title": "FTX FAQ", "postedAt": "2022-11-13T05:00:48.582Z", "htmlBody": "<p><strong>What is this?</strong></p><ul><li>I thought it would be useful to assemble an FAQ on the FTX situation.</li><li>This is not an <i>official </i>FAQ. I'm not writing this in any professional capacity.</li><li>This is <i>definitely</i> not legal or financial advice or anything like that.</li><li>Please let me know if anything is wrong/unclear/misleading.&nbsp;</li><li>Please suggest questions and/or answers in the comments.&nbsp;</li></ul><p><strong>What is FTX?</strong></p><ul><li><a href=\"https://ftx.com/\">FTX</a> is a Cryptocurrency Derivatives Exchange. It is <a href=\"https://twitter.com/SBF_FTX/status/1591089317300293636\">now bankrupt</a>.</li></ul><p><strong>Who is Sam Bankman-Fried (SBF)?</strong></p><ul><li>The founder of FTX. He was recently a billionaire and the <a href=\"https://www.forbes.com/video/6275693545001/meet-sam-bankmanfried-the-richest-person-under-30-in-the-world/?sh=49ee333d21ea\">richest person under 30</a>.</li></ul><p><strong>How is FTX connected to effective altruism?</strong></p><ol><li>In the last couple of years, effective altruism received millions of dollars of funding from SBF and FTX via the <a href=\"https://ftxfuturefund.org/\">Future Fund</a>.&nbsp;</li><li>SBF was following a strategy of \"make tons of money to give it to charity.\" This is called \"earning to give\", and it's an idea that was spread by EA in early-to-mid 2010s. SBF was <a href=\"https://80000hours.org/stories/sam-bankman-fried/\">definitely encouraged onto his current path</a> by engaging with EA.</li><li>SBF was something of a \"golden boy\" to EA. For example, <a href=\"https://80000hours.org/stories/sam-bankman-fried/\">this</a>.</li></ol><p><strong>How did FTX go bankrupt?</strong></p><ul><li>FTX gambled with user deposits rather than keeping them in reserve.&nbsp;</li><li>Binance, a competitor, triggered a run on the bank where depositors attempted to get their money out.&nbsp;</li><li>It looked like Binance was going to acquire FTX at one point, but they pulled out after due diligence.</li><li><a href=\"https://forum.effectivealtruism.org/posts/yjGye7Q2jRG3jNfi2/ftx-crisis-what-we-know-and-some-forecasts-on-what-will\">Source</a></li></ul><p><strong>How bad is this?</strong></p><ul><li>\"It is therefore very likely to lead to the loss of deposits which will hurt the lives of 10,000s of people eg <a href=\"https://twitter.com/pepecasso1/status/1590231920147042304?s=46&amp;t=LLZm3tq5on5Uf3QLrmLhNA\">here</a>\" (<a href=\"https://forum.effectivealtruism.org/posts/yjGye7Q2jRG3jNfi2/ftx-crisis-what-we-know-and-some-forecasts-on-what-will\">Source</a>)</li><li>Also:</li></ul><figure class=\"media\"><div data-oembed-url=\"https://manifold.markets/mr22222222/sbf-convicted-of-a-felony-before-20\">\n\t\t\t\t<div class=\"manifold-preview\">\n\t\t\t\t\t<iframe src=\"https://manifold.markets/embed/mr22222222/sbf-convicted-of-a-felony-before-20\">\n\t\t\t\t</iframe></div>\n\t\t\t</div></figure><p><strong>Does EA still have funding?</strong></p><ul><li>Yes.&nbsp;</li><li>Before FTX there was Open Philanthropy (OP), which has billions in funding from Dustin Muskovitz and Cari Tuna. None of this is connected to FTX.</li></ul><p><strong>Is Open Philanthropy funding affected?</strong></p><ul><li>Global health and wellbeing funding will continue as normal.</li><li>Because the total pool of funding to longtermism has shrunk, Open Philanthropy will have to raise the bar on longtermist grant making.</li><li>Thus, Open Philanthropy is \"pausing most new longtermist funding commitments\" (longtermism includes AI, Biosecurity, and Community Growth) for a couple of months to recalibrate.</li><li><a href=\"https://forum.effectivealtruism.org/posts/mCCutDxCavtnhxhBR/some-comments-on-recent-ftx-related-events\">Source</a></li></ul><p><strong>How much of EA's money came from FTX Future Fund?</strong></p><ul><li>As per the post <a href=\"https://forum.effectivealtruism.org/posts/ZbaDmowkXbTBsxvHn/historical-ea-funding-data\">Historical EA funding data</a> from August 2022, the estimation for 2022 was:<ul><li>Total EA funds: 741 M$</li><li>FTX Future Fund contribution: 262 M$ (35%)</li></ul></li><li>(This Q&amp;A contributed by <a href=\"https://forum.effectivealtruism.org/posts/j7sDfXKEMeT2SRvLG/ftx-faq?commentId=2KLz7mXQDbfqahrha\"><strong>Miguel Lima Med\u00edn</strong></a>.)</li></ul><p><strong>If you got money from FTX, do you have to give it back?</strong></p><ul><li>\"If you received money from an FTX entity in the debtor group anytime on or after approximately August 11, 2022, the bankruptcy process will probably ask you, at some point, to pay all or part of that money back.<strong>\"</strong></li><li><strong>\"</strong>you will receive formal notice and have an opportunity to make your own case<strong>\"</strong></li><li>\"this process is likely to take months to unfold\" and is \"going to be a multi-year legal process\"</li><li>If this affects you, please read <a href=\"https://forum.effectivealtruism.org/posts/o8B9kCkwteSqZg9zc/thoughts-on-legal-concerns-surrounding-the-ftx-situation\">this</a> post from Open Philanthropy. They also made an <a href=\"https://docs.google.com/document/d/1XLEnM9FBzH9XKNw_wrB5snz4Iru95dqKaM_GTlbgBrA/edit?usp=sharing\">explainer document</a> on clawbacks.</li></ul><figure class=\"media\"><div data-oembed-url=\"https://manifold.markets/SpencerGreenberg/will-projects-which-received-grants\">\n\t\t\t\t<div class=\"manifold-preview\">\n\t\t\t\t\t<iframe src=\"https://manifold.markets/embed/SpencerGreenberg/will-projects-which-received-grants\">\n\t\t\t\t</iframe></div>\n\t\t\t</div></figure><figure class=\"media\"><div data-oembed-url=\"https://manifold.markets/EliezerYudkowsky/will-5-of-an-ftx-grant-be-clawed-ba\">\n\t\t\t\t<div class=\"manifold-preview\">\n\t\t\t\t\t<iframe src=\"https://manifold.markets/embed/EliezerYudkowsky/will-5-of-an-ftx-grant-be-clawed-ba\">\n\t\t\t\t</iframe></div>\n\t\t\t</div></figure><figure class=\"media\"><div data-oembed-url=\"https://manifold.markets/NathanpmYoung/will-there-be-more-than-5-accounts\">\n\t\t\t\t<div class=\"manifold-preview\">\n\t\t\t\t\t<iframe src=\"https://manifold.markets/embed/NathanpmYoung/will-there-be-more-than-5-accounts\">\n\t\t\t\t</iframe></div>\n\t\t\t</div></figure><p><strong>What if you've already spent money from FTX?</strong></p><ul><li>It's still possible that you may have to give it back.</li></ul><p><strong>If you got money from FTX, </strong><i><strong>should</strong></i><strong> you give it back?</strong></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/FKJ8yiF3KjFhAuivt/impco-don-t-injure-yourself-by-returning-ftxff-money-for?commentId=aZg5h72GyYd3Jz98L\">You probably shouldn't</a>, at least for the moment. If you gave the money back, there's the possibility that because it wasn't done through the proper legal channels you end up having to give the money back <i>twice</i>.</li></ul><p><strong>If you got money from FTX, should you spend it?</strong></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/3Bq3PFRfzaEEAATti/clawbacks-probably-don-t-spend-ftx-grantee-money-for-the\">Probably not</a>. At least for the next few days. You may have to give it back.</li></ul><p><strong>I feel bad about having FTX money.</strong></p><ul><li>Reading <a href=\"https://forum.effectivealtruism.org/posts/FKJ8yiF3KjFhAuivt/impco-don-t-injure-yourself-by-returning-ftxff-money-for\">this</a> may help.</li><li>\"It's fine to be somebody who sells utilons for money, just like utilities sell electricity for money.\"</li><li>\"You are not obligated to return funding that got to you ultimately by way of FTX; especially if it's been given for a service you already rendered, any more than the electrical utility ought to return FTX's money that's already been spent on electricity\"</li></ul><p><strong>What if I'm still expecting FTX money?</strong></p><ul><li>The FTX Future Fund team has <a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1\">all resigned</a>, but \"grantees may email <a href=\"mailto:grantee-reachout@googlegroups.com\">grantee-reachout@googlegroups.com</a>.\"</li><li>\"To the extent you have a binding agreement entitling you to receive funds, you may qualify as an unsecured creditor in the bankruptcy action. This means that&nbsp;<strong>you could have a claim for payment</strong> against the debtor\u2019s estate\" (<a href=\"https://forum.effectivealtruism.org/posts/o8B9kCkwteSqZg9zc/thoughts-on-legal-concerns-surrounding-the-ftx-situation\">source</a>)</li></ul><p><strong>I needed my FTX money to pay the rent!</strong></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/L4S2NCysoJxgCBuB6/announcing-nonlinear-emergency-funding\">Nonlinear has stepped in to help:</a> \"If you are a Future Fund grantee and &lt;$10,000 of bridge funding would be of substantial help to you, fill out this&nbsp;<a href=\"https://tally.so/r/w4arzd\"><u>short form</u></a> (&lt;10 mins) and we\u2019ll get back to you ASAP.\"</li></ul><p><strong>I have money and would like to help rescue EA projects that have lost funding.</strong></p><ul><li>You can contribute to the Nonlinear emergency fund mentioned above.</li><li>\"If you\u2019re a funder and would like to help, please reach out:&nbsp;<a href=\"mailto:katwoods@nonlinear.org\"><u>katwoods@nonlinear.org</u></a>\"</li></ul><p><strong>How can I get support/help (for mental health, advice, etc)?</strong></p><ul><li>\"Here\u2019s the&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLScJooJD0Sm2csCYgd0Is6FkpyQa3ket8IIcFzd_FcTRU7avRg/viewform\"><u>best place</u></a> to reach us if you\u2019d like to talk. I know a form isn\u2019t the warmest, but a real person will get back to you soon.\" (<a href=\"https://forum.effectivealtruism.org/posts/BesfLENShzSMeb7Xi\">source</a>)</li><li>Some mental health advice <a href=\"https://forum.effectivealtruism.org/posts/ReexceojC5mwDQwzX/for-the-mental-health-of-those-affected-by-the-ftx-crisis\">here</a>.</li><li>Someone set up a support network for the FTX situation.<ul><li><a href=\"https://docs.google.com/spreadsheets/d/1ztWUq7ZN21nlgU74O51zPfrDLi9WZy-9SvPhkpeWk1A/edit#gid=0\">This table</a> lists people you can contact for free help. It includes experienced mental health supporters &nbsp;and EA-informed coaches and therapists<i>.</i></li><li><a href=\"https://join.slack.com/t/effectivepeersupport/shared_invite/zt-1jkv97xxh-I~_8J7tTBreFnfCOyi312Q\">This Slack channel</a> is for discussing your issues, and getting support from the trained helpers as well as peers.</li></ul></li></ul><p><strong>How are people reacting?</strong></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/WdeiPrwgqW2wHAxgT/a-personal-statement-on-ftx\">Will MacAskill</a>: \"If there was deception and misuse of funds, <strong>I am outraged</strong>, and I don\u2019t know which emotion is stronger: my utter rage at Sam (and others?) for causing such harm to so many people, or my sadness and self-hatred for falling for this deception.\"</li><li><a href=\"https://forum.effectivealtruism.org/posts/txGgPvKgZFpphdkHe/my-reaction-to-ftx-appalled\">Rob Wiblin</a>: \"I am ******* appalled. [...] FTX leaders also betrayed investors, staff, collaborators, and the groups working to reduce suffering and the risk of future tragedies that they committed to help.\"</li><li><a href=\"https://forum.effectivealtruism.org/posts/mCCutDxCavtnhxhBR/some-comments-on-recent-ftx-related-events\">Holden Karnofsky</a>: \"I dislike \u201cend justify the means\u201d-type reasoning. The version of effective altruism I subscribe to is about being a good citizen, while ambitiously working toward a better world. As I wrote <a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous#Can_we_avoid_these_pitfalls_by__just_maximizing_correctly__\">previously</a>, I think effective altruism works best with a strong dose of pluralism and moderation.\"</li><li><a href=\"https://forum.effectivealtruism.org/posts/XHrHsrQGyr4NnqCA7/we-must-be-very-clear-fraud-in-the-service-of-effective\">Evan Hubinger</a>: \"We must be very clear: fraud in the service of effective altruism is unacceptable\"</li></ul><p><strong>Was this avoidable?</strong></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/4zjnFxGWYkEF4nqMi/how-could-we-have-avoided-this\">Nathan Young</a> notes a few red flags in retrospect.</li><li>But on the other hand:<img src=\"http://res.cloudinary.com/cea/image/upload/v1668266674/mirroredImages/4zjnFxGWYkEF4nqMi/zsk8e6h2zpzjqmyte3nc.png\"></li></ul><p><strong>Did leaders in EA know about this?</strong></p><figure class=\"media\"><div data-oembed-url=\"https://manifold.markets/IsaacKing/by-the-end-of-2023-will-substantial\">\n\t\t\t\t<div class=\"manifold-preview\">\n\t\t\t\t\t<iframe src=\"https://manifold.markets/embed/IsaacKing/by-the-end-of-2023-will-substantial\">\n\t\t\t\t</iframe></div>\n\t\t\t</div></figure><p><strong>Will there be an investigation into whether EA leadership knew about this?</strong></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/HyHCkK3aDsfY95MoD/cea-ev-op-rp-should-engage-an-independent-investigator-to\">Tyrone-Jay Barugh</a> has suggested this, and Max Dalton (leader of CEA) <a href=\"https://forum.effectivealtruism.org/posts/HyHCkK3aDsfY95MoD/cea-ev-op-rp-should-engage-an-independent-investigator-to?commentId=MM389p2rCjbWeiw5a\">says</a> \"this is something we\u2019re already exploring, but we are not in a position to say anything just yet.\"</li></ul><p><strong>Why didn't the EA criticism competition reveal this?&nbsp;</strong></p><ul><li>(This question was posed by <a href=\"https://forum.effectivealtruism.org/posts/HyHCkK3aDsfY95MoD/cea-ev-op-rp-should-engage-an-independent-investigator-to?commentId=CBTFwsdYhMrisoCqE\"><strong>Matthew Yglesias)</strong></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/HyHCkK3aDsfY95MoD/cea-ev-op-rp-should-engage-an-independent-investigator-to?commentId=EgpfjJaxvscow6FRZ\">Bruce</a> points out that there <i>were</i> a number of relevant criticisms which questioned the role of FTX in EA (<a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=FqA7gH6MBP4whxNig#YvE9MMmeEFmJiDKgi\">eg</a>). However, there was no good <i>system </i>in place to turn this into meaningful change.</li></ul><p><strong>Does the end justify the means?</strong></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/mCCutDxCavtnhxhBR/some-comments-on-recent-ftx-related-events#Separate_from_the_details_of_the_FTX_situation__do_you_think_that_fraud_could_be_justified_if_it_raises_huge_amounts_of_money_for_good_causes_\">Many</a> <a href=\"https://forum.effectivealtruism.org/posts/WdeiPrwgqW2wHAxgT/a-personal-statement-on-ftx\">people</a> <a href=\"https://forum.effectivealtruism.org/posts/XHrHsrQGyr4NnqCA7/we-must-be-very-clear-fraud-in-the-service-of-effective\">are</a> <a href=\"https://forum.effectivealtruism.org/posts/txGgPvKgZFpphdkHe/my-reaction-to-ftx-appalled\">reiterating</a> that <i>EA values go against doing bad stuff for the greater good.</i></li><li>Will MacAskill <a href=\"https://twitter.com/willmacaskill/status/1591218030364995585\">compiled a list</a> of times that prominent EAs have emphasised the importance of integrity over the last few years.</li><li>Several people have pointed to <a href=\"https://www.lesswrong.com/posts/K9ZaZXDnL3SEmYZqB/ends-don-t-justify-means-among-humans\">this</a> post by Eliezer Yudkowsky, in which he makes a compelling case for the rule \"Do not cheat to seize power <i>even when it would provide a net benefit.\"</i></li></ul><p><strong>Did Qualy the lightbulb break character?</strong></p><ul><li><a href=\"https://twitter.com/QualyThe/status/1590675196419399682\">Yes</a>.</li></ul><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/j7sDfXKEMeT2SRvLG/h4g2vb23vjviryraidjv\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/j7sDfXKEMeT2SRvLG/astokcmenq8dfvbsws3w 130w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/j7sDfXKEMeT2SRvLG/crjudlmiwhptz0rqsocj 260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/j7sDfXKEMeT2SRvLG/jp2fhy38ep0xazvf91or 390w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/j7sDfXKEMeT2SRvLG/w0d1y7g3ywy4krcc5mid 520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/j7sDfXKEMeT2SRvLG/k8hoj7byxvyosn8axde7 650w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/j7sDfXKEMeT2SRvLG/fbjccbuchkvkhzl2rzw0 780w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/j7sDfXKEMeT2SRvLG/hngy5buzce1shcycojld 910w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/j7sDfXKEMeT2SRvLG/t4xpohugrcunmtaw9okx 1040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/j7sDfXKEMeT2SRvLG/hvd465ukzgd4j3zaijzz 1170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/j7sDfXKEMeT2SRvLG/cwyv01hnbfw4piwkfddc 1278w\"></figure>", "user": {"username": "Hamish Doodles"}}, {"_id": "PkFenL3DcEJDjERwY", "title": "FTX prob related: Strongly Recommending Creating An Internal Audit Committee that will Periodically Assess EA Organizations, Its Objectives And Processes Based on IIA Standards", "postedAt": "2022-11-13T01:42:37.755Z", "htmlBody": "<p>Hi EA Community,</p>\n<p>In light of the FTX case that rocked the community, I would like to know if how much will the EA community will be willing to invest in better systems of control testing and performance assessments. Periodically doing these methodologies in accordance to Internal Audit Standards will provide the best shielding to as far as major errors or large scale fraudulent activities are concerned.</p>\n<p>If the community wants a lasting solution, this is probably the best that you can arrive with at avoiding the same scandal to occur again.</p>\n<p>I have expected that what I'm recommending here are already installed in the first place as but apparently yeah, the expertise in error and fraud detection seems to be not in the community's skillset - yet.</p>\n<p>This community is probably one of the best groups of people I've come across so far in the field of altruism.. I don't want these movement go down like Enron or Lehman.</p>\n<p>Let me know your thoughts..</p>\n<p>Miguel</p>\n", "user": {"username": "Miguel"}}, {"_id": "Xs6CovYsRDpKnz3jf", "title": "Giving away a fully registered Texas nonprofit entity (C-Corp) in good standing with a Form 1023 (501(c)(3) application) on file with the IRS.", "postedAt": "2022-11-13T05:38:30.150Z", "htmlBody": "<p>Hello EAs,</p><p>I received a Future Fund grant. My funding never came through, but before FF collapsed I registered a Texas C-corp, filed for nonprofit status, and opened a Bank of America business checking account for the entity to receive its funding. My project will be proceeding due to generous funding from another source, but I will not be using the entity.&nbsp;</p><p>I would be happy to give away the entity to any EA who wants it, as long as you do the legwork to change the registration and whatnot (I think this would mostly require drafting director/shareholder consents and maybe a bit of paperwork registered with the Texas Secretary of State officer / IRS). That would require some legwork, but probably less legwork than registering a new entity, filling out a Form 1023, drafting bylaws, etc.</p><p>Feel free to contact me with any questions.</p>", "user": {"username": "Ross H"}}, {"_id": "sdAgwtHkBZmkmkcpZ", "title": "In favour of compassion, and against bandwagons of outrage", "postedAt": "2022-11-13T00:42:15.061Z", "htmlBody": "<p>I hate to add to the number of FTX posts on the forum, but after some (imo) inappropriate and unkind memes<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref28bnl44ssym\"><sup><a href=\"#fn28bnl44ssym\">[1]</a></sup></span>&nbsp;and comments in the Dank EA Memes fb group and elsewhere, I wanted to push back against what seems like a <a href=\"https://en.wikipedia.org/wiki/Bandwagon_effect\">bandwagon</a> of anger and ridicule spiralling too far, and I wish to call attention to it.</p><p>But first, I should point out that I personally, at this time, know not nearly enough to make confident conclusions regarding what's happened at FTX. That means I will not make any morally relevant judgments. I will especially not <i>insinuate them</i> without sufficient evidence. That just amounts to irresponsibly fuelling the bandwagon while maintaining plausible deniability, which is arguably worse.</p><p>You are not required to pretend to know more than you do just so you can empathise with the outrage of your friends. That shouldn't be how friendship works.</p><hr><p>This topic is not without nuance. There's a good case to be made for why ridicule can be pro-social, and I think Alex makes it here:</p><blockquote><p><i>\"Ridicule makes clear our commitment to punishing ultimately harmful behavior, in a tit-for-tat sense; we are not the government so we cannot lock up wrongdoers, and acting as a vigilante assassin is precluded by other issues, so our top utility-realizing option is to meme harmful behavior out of the sphere of social acceptability.\"</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflmq3ibqglbd\"><sup><a href=\"#fnlmq3ibqglbd\">[2]</a></sup></span></p></blockquote><p>I don't disagree with condemning someone for having behaved unethically. It's a necessary part of maintaining civil society, and it enables people to cooperate and trade in good faith. But if you accuse someone of having ill-advisedly forsaken ethics in the (putative) service of the greater good, then retaliating by <i>forsaking compassion</i> in the service of <i>unchecked mockery</i> can't possibly make anything better.</p><p>Why bother with compassion, you might ask? After all, compassion is superfluous for positive-sum cooperation. What we really need for essential social institutions to work at all is widespread trust in the basic ethics of people we trade with. So when a public figure gets caught depreciating that trust, it's imperative that we send a strong signal that this is completely unacceptable.</p><p>This I all agree with. Judicious punishments are essential for safeguarding prevailing social institutions. Plain fact. But what if prevailing social institutions are unjust? When we jump on a bandwagon for humiliating the accused transgressor after their life has already fallen apart, we are exercising our instincts for mob justice, and we are indirectly strengthening the norm for coercing deviants more generally.</p><p>Advocating punitive attitudes trades off against advocating for compassion to some extent. <i>Especially</i> if the way you're trying to advocate for punishments is by means of gleefwly inflicting harm.</p><p>In a society where most people are all too eager to join in when they see their in-group massing against deviants, and where groups have wildly different opinions on who the deviants are in the first place, we need an alternative set of principles.</p><p>Compassion is a corrective on unjust social norms. It lets us see more clearly where prevailing ethics strays from what's kind and good. In essence, that's the whole purpose of effective altruism: to do better than the unquestioned norms that's been handed down to us.</p><p>Hence why I hope we can outgrow--or at least lend nuance to--our reflexive instinct to punish, and instead cultivate whatever embers of compassion we can find. Let that be our <a href=\"https://80000hours.org/problem-profiles/promoting-positive-values/\">cultural contribution</a>, because the alternative, advocating punitive sentiments, just isn't a neglected cause area.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn28bnl44ssym\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref28bnl44ssym\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.facebook.com/photo.php?fbid=8309777115730916&amp;set=p.8309777115730916&amp;type=3\">example</a>, <a href=\"https://www.facebook.com/photo/?fbid=8309732025735425&amp;set=gm.3299786910336204&amp;idorvanity=1479475219034058\">example</a>, &nbsp;<a href=\"https://www.facebook.com/photo.php?fbid=8310433525665275&amp;set=p.8310433525665275&amp;type=3\">example</a>, and <a href=\"https://www.facebook.com/groups/OMfCT/posts/3300281583620070/?comment_id=3300303486951213&amp;reply_comment_id=3300320456949516\">example</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlmq3ibqglbd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflmq3ibqglbd\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.facebook.com/groups/OMfCT/posts/3300281583620070/?comment_id=3300303486951213&amp;reply_comment_id=3300342930280602\">comment</a></p></div></li></ol>", "user": {"username": "Emrik"}}, {"_id": "iiwZsKNdFt6yKcBHC", "title": "Reading suggestions for how regulatory bodies can do better on vaccine approval in a pandemic?", "postedAt": "2022-11-12T23:38:36.432Z", "htmlBody": "<p>Would be much appreciated!</p>\n", "user": {"username": "freedomandutility"}}, {"_id": "kzAck8hGkiiCvTwSt", "title": "FTX Crisis: how should EA groups' leaders and board members act in the context of the current situation?", "postedAt": "2022-11-12T23:36:44.988Z", "htmlBody": "<p>Context: I'm talking about any types of EA groups (national, local, workplace, uni and similar).&nbsp;<br>I think it would be beneficial to chat about optimal communication, actions and any predicted impact (other than financial) which can be now expected. How to deal with the situation in a best possible way and prepare for the complications ahead?</p>", "user": {"username": "Liv"}}, {"_id": "24Cjdn4S7uvbwYhaD", "title": "A Newcomer's Critique of EA - Underprioritizing Systems Change?", "postedAt": "2022-11-12T23:04:54.399Z", "htmlBody": "<p>This is my first post, so I\u2019m eager to hear feedback. Hope it stimulates discussion!</p><p>I would like to share my perspective as a newcomer to the EA community\u2013someone who has read a couple external articles about EA from larger news sites and a dozen or so forum posts. From this position, my useful contributions will probably be limited to a couple categories:</p><ol><li>Reporting my perception of EA from the outside to provide anecdotal evidence that may aid recruiting or outreach efforts</li><li>Introducing relatively novel discussions</li></ol><p>In this post, I am hoping to do both, by giving an initial reaction to the priorities and methods that I have gathered that the EA community favors.&nbsp;</p><p>From what I have seen, EA is not very involved in the political sphere, with a spoken or unspoken consensus that the most pressing issues are the following:</p><ol><li>Global extreme poverty</li><li>AI risk, nuclear risk, climate risk, biorisk, and general technological regulation</li></ol><p>To address these priorities, EAs (seem to) favor extremely direct interventions: direct donation via GiveWell and similar organizations and direct, specific advocacy for greater international and domestic cooperation to keep potentially dangerous technologies under control.</p><p>Again, I could be missing huge parts of the average EA\u2019s agenda. This is just my interpretation of the community\u2019s priorities. With that said, I would like to offer some constructive criticism of this approach, which to me reflects an attitude of \u201ctrust the system and make the greatest impact on the world you can&nbsp;<i>from within the system</i>\u201d that may lead to overlooking the potential to do high-impact work by changing the system itself.</p><p>After all, in a world that&nbsp;<i>already has&nbsp;</i>the productive capacity to feed and house everyone, mustn\u2019t both the source of and solution to our most pressing problems come from underlying systems of governance, culture, and ideology? And, if these systems will determine the majority of future human happiness, as well as humanity's resilience to existential risks, then is it not worth investing more time and social capital, if not money, into improving those systems?&nbsp;</p><p>For example, it appears to me that economic and political inequality within both developed and developing countries is one the most significant chronic failures of the global system. The conclusion I jump to is that, to have the greatest impact, it is worthwhile to expend certain resources on reducing inequality and democratizing institutions, rather than focusing exclusively on work done from within the system, such as earning-to-give.&nbsp;</p><p>I want to explore the issue of inequality further because I think it is an integral concept for a movement that advocates giving from the globally well-off to the globally worse-off. I will argue that reducing this inequality&nbsp;<i>through underlying structural means</i>, in addition to manual corrections via the private sector, is a very high priority for humanity for two main reasons:</p><p>A) This is what a world with this level of inequality looks like: a group of the benevolent global rich trying to<i>&nbsp;</i>band-aid the issue, while most of the world faces stagnating or reduced buying power and influence over their own future, and most of the powerful look the other way. Given that the status quo has not produced desired results, there must be substantial issues with the status quo, and although earning to give may help alleviate the suffering caused by the status quo, it may not have a significant impact on the structures that led to that suffering in the first place. Essentially, some mixture of plugging holes in the boat and using buckets to throw out the water that already seeped in must be optimal, and I worry that we should be spending more time plugging holes.&nbsp;</p><p>B) A global system that tolerates the political and economic disenfranchisement of many of its constituents is one that is extremely likely to cause the \u201clock-in\u201d of a negative set of values. If this is really our most important century, it is of paramount importance for us to develop institutions that are capable of cultivating human wellbeing. Also, dramatic inequality makes us more likely to suffer either a debilitating global war or an intense internal political and cultural instability that makes addressing anything difficult. Political instability and the general breakdown of social cooperation are existential threats not only in of themselves, but also in regard to their potential to damage our ability to collaborate on other existential risks like AI or bioengineering.</p><p>&nbsp;</p><p>1. The long-term impact of institutional crises affecting the developed world</p><p>Ironically, this belief leads me to challenge some of the fundamental assumptions behind a heavy prioritization of global extreme poverty. Obviously, this issue is of huge importance\u2013the average person living in poverty experiences far more harm and probably less happiness on a daily basis than the average person not living in poverty. It is also undeniably true that to lift someone out of poverty in Zimbabwe is cheaper than to lift someone out of poverty in the U.K.&nbsp;</p><p>However, with regard to the long-term impact of direct-transfer type interventions, I am more skeptical. Political and social change seems to pose the solution to the issues facing both the U.K. and Zimbabwe. An injection of cash or even a life saved may make less of an impact in the long run than an investment in democratic norms and the rule of law in the case of Zimbabwe, or an expansion of state capacity and the social safety net in the U.K.&nbsp;</p><p>Going further, it is conceivable that the maintenance/creation of a strong democracy and an egalitarian economy in a rich country will have more compounding effects than efforts to introduce democracy in developing countries. This may be true for two reasons: a higher ROI on institution change in rich countries enabled by greater rule of law and stability in the region, or global economic dynamics, wherein the rich country commands a larger share of global capital and skilled labor, and therefore plays a more leading role in resource allocation.&nbsp;</p><p>Obviously, investments in the stability of developing regions and in the institutional quality of developed regions are both extremely high-impact, and the former may provide more opportunities that will yield a high dollar/impact rate. However, I do think that the latter should not be overlooked, especially given that most EAs live in developed countries, speak the local language, and generally make their lives there, which provides them extra opportunities to make an impact.</p><p>I think that this is a time in which local community-building and political organization in developed countries may be particularly impactful. Many high-income countries are currently experiencing crises of faith in their most foundational institutions, and a historic share of the populations of these countries are experiencing high financial and general stress. Real median wages have declined across most of the developed world and the quality of democracy has declined in the United States, Europe, and South America according to Our World in Data:&nbsp;<a href=\"https://ourworldindata.org/democracy.\"><u>https://ourworldindata.org/democracy</u>.</a> Anglophone countries, particularly the US and UK, are feeling the effects of chronic underinvestment in public services and infrastructure, while many East Asian countries and Southern Europe face a looming demographic crisis and attendant economic decline. Some developed countries, notably the US, also chronically violate civil rights at home and abroad, which threatens the integrity of their institutions.</p><p>These issues seem to pose risks to the general functioning of the institutions that enable EA in the first place, like free, democratic societies and equitable economic growth that ensures \u201cProgress\u201d actually improves lives. As effective altruists, are we not hoping that the whole world\u2019s institutions eventually come to resemble those of our favorite developed countries? If so, should we not be concerned when those institutions show signs of decay, or simply fail to grow and improve over time?</p><p>I realize that these goals are inherently political, and perhaps EAs shy away from them for that reason. Maybe it is most efficient, from a bird\u2019s eye view, for groups who think political interventions are the most important priority to silo themselves off from those who favor essentially politically-agnostic interventions. In theory, this allows each group to maximize their impact without being paralyzed by ideological disagreement.&nbsp;</p><p>Unfortunately, it may be impossible to know what the net impact of the movement adopting more political or social positions will be until it is actually tried. However, I think that if there is any chance that taking a greater interest in modifying institutions would improve most EAs\u2019 moral impact, it is worth seriously examining the possibility of making it a greater focus, especially because the combined influence on institutions that EAs could have through concerted effort is probably quite substantial.&nbsp;</p><p>If certain forms of social stewardship in one\u2019s local community/workplace become signals of commitment to effective altruism in the same way that taking the Giving What We Can pledge signals commitment, I can envision the long-term impact of EA being especially great. For example, geographically-concentrated EA groups hold monthly or quarterly evaluations of the highest-impact ways to make a local impact, with members expected to invest at least some time in the projects selected by the group.&nbsp;</p><p>&nbsp;</p><p>2. Local Community Building as Effective Altruism</p><p>Inspiring more people to join and retaining current EAs might be closely tied to one another: both depend on powerful traditions and forms of organization that can be replicated internationally and tie local and global stewardship together.</p><p>It may be surprisingly impactful to lead by example and spend time and energy contributing to community or invite-only events. Especially if one believes one\u2019s worldview and habits (such as donating to high-impact causes) ought to be more widespread, investing in one\u2019s own wellbeing and social involvement could lead to those beliefs getting far more attention, potentially multiplying one\u2019s own moral impact while simultaneously maximizing personal happiness.&nbsp;</p><p>I think this vision of an ideally moral person differs from the one outlined in Singer\u2019s \u201cFamine, Affluence, and Morality\u201d. By emphasizing cultivation of the self, this approach trusts in the human desire to do good in all areas of one\u2019s life, assuming that the individual who donates a modest to moderate amount of their income without neglecting the rest of their life will have the highest moral impact through occasionally indirect, difficult-to-quantify means, many of which center around the creation of new, better social realities.</p><p>For EAs, I think adopting a more community-oriented approach would demand relatively little of the time and money currently spent on farther-flung, high impact per dollar aims. Far from encouraging a reduction in donation quantity (unless one\u2019s donations were so high that they were unavoidably damaging their physical health and social life), this philosophy might motivate more involvement in organizing community events, more time at the local climbing gym, or making more trips by bike, foot, or bus rather than by car.&nbsp;</p><p>More contentiously, it might suggest, for example, a preference for the government-related job that pays $10,000/year less but enables one to make a significant local impact over the private sector job that would free up $10,000/year more to give. I suspect that minor reductions in donation quantity that result from taking on a personally preferable or more locally impactful career, or from spending a bit more money hosting social events or obtaining hobby-related goods (assuming that these events wouldn\u2019t have happened otherwise and the hobby wouldn\u2019t be accessible otherwise) will be repaid in the end via faster spread of EA ideas to friends and acquaintances, as well as higher personal future earnings resulting from higher physical and mental health. The best community building tools are those that both save money AND improve quality of life, like going to the park instead of the movies, biking to a local 5k instead of driving to a gym to run on a treadmill, or volunteering with friends instead of doing an expensive activity. I think that most of us still have a lot of low-hanging fruit in these areas, and baking these sensibilities into EA groups could bring said groups together.</p><p>I really enjoyed the post from a while back about taking inspiration from the Quakers. Building novel subcultures among EAs and like-minded people could be hugely impactful over time by building deliberately-welcoming and resilient communities. The primary barrier for admission might be something like taking the Giving What We Can pledge and uniting forces might include regular community events and group volunteering. Other uniting factors could be building coworking places for remote workers, sharing tools and expertise with regard to house/car/bike maintenance, watching each other\u2019s children, etc. Groups that elicit a high degree of implicit trust of other group members are relatively rare and potentially powerful.</p><p>&nbsp;</p><p>3. Applying Logical Analysis of the EA Caliber to Political Positions</p><p>This is a somewhat different way that EA could make a large positive political impact without violating its sense of political neutrality and openness to debate.&nbsp;</p><p>When people first become interested in politics, they often have a years-long road ahead of them to generate even mildly-informed opinions on the issues of the day. Few have time for this, and even those that do could have saved a lot of time at the beginning of their journey by viewing the entirety of various informed people\u2019s political beliefs in a thorough, categorized, searchable database of political positions.&nbsp;</p><p>I don\u2019t have the technical skills to create such a tool, but if someone could improve upon&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1lNoH1UyLRVGQIlH7nDgNdpki1eAFVv1dC-UXtG1WxUQ/edit#gid=1920464849\"><u>this general idea</u></a> and flesh it out, I think the EA community would be an ideal starting point for acquiring data on political positions and opening discussion on which policies are most promising. The quantitative, relatively anti-partisan, and debate-oriented bent of EA would make this community great stewards for the concept. I envision a system where a policy that is endorsed by, say, 80% of EAs, would qualify for an official recommendation by the community. Of course, it wouldn\u2019t just be EAs using this tool\u2013it could expand far beyond the movement, and people would have the ability to create their own subcommunities with their own recommendations on the platform.&nbsp;</p><p>With regard to the kind of overtly political national policies I think EA could unite around, some examples of policies to advocate for might be voting rights and universal healthcare (for American EAs) or investment in public infrastructure (including infrastructure following the principles of New Urbanism). By prioritizing the quality of the underlying systems of government, defending civil rights, and advocating for the most thoroughly-evidenced economic policies, a considerable impact could be made by applying consolidated EA influence and expertise to institutions change. It is only logical for EA, as a movement with an intense interest in effective resource allocation, to attempt to improve the most powerful resource-allocating organizations to ever exist on Earth: governments.</p><p>&nbsp;</p><p>Again, I\u2019m excited to join this community and I welcome feedback and/or counterarguments.</p><p><br><br>&nbsp;</p>", "user": {"username": "Sam Battis"}}, {"_id": "dk9HTJKNAAwaEZTgk", "title": "Will MacAskill's role in connecting SBF to Elon Musk for a potential Twitter deal", "postedAt": "2022-11-12T23:37:16.133Z", "htmlBody": "<p>Several media outlets reported recently that Will MacAskill was a liaison between Sam Bankman-Fried and Elon Musk, trying to set them up to discuss a joint Twitter deal. The story gained traction after Musk's texts were released as a part of court proceedings related to the said deal. The texts can be viewed <a href=\"https://assets.bwbx.io/documents/users/iqjWHBFdfxIU/ro.xehDmXvHk/v0\">here</a>, the discussion between Will and Elon carries on across a few pages, starting on page 87.<br><br>In no particular order, after a quick Google search, the outlets that ran the story are:</p><ul><li><a href=\"https://www.businessinsider.com/sam-bankman-fried-guru-texted-elon-musk-helping-buy-twitter-2022-9\">Business Insider</a></li><li><a href=\"https://www.axios.com/2022/10/03/twitter-musk-sam-bankman-fried\">Axios</a></li><li><a href=\"https://www.dailymail.co.uk/news/article-11419575/Sam-Bankman-Fried-tried-Elon-Musks-Twitter-deal-according-texts-banker.html\">Daily Mail</a></li><li><a href=\"https://finance.yahoo.com/news/elon-musk-sam-bankman-fried-043720906.html\">Yahoo Finance</a></li><li><a href=\"https://www.nytimes.com/2022/10/08/business/effective-altruism-elon-musk.html\">New York Times</a></li><li><a href=\"https://www.benzinga.com/news/22/11/29686991/revisiting-the-texts-between-elon-musk-and-sam-bankman-fried-about-potential-twitter-deal\">Benzinga</a></li><li><a href=\"https://www.fastcompany.com/90809796/sam-bankman-fried-and-elon-musk-just-killed-effective-altruism\">Fast Company</a> (titled: Sam Bankman-Fried and Elon Musk just killed effective altruism - Twitter deal is not explicitly mentioned, however a good read nonetheless)</li><li><a href=\"https://www.vice.com/en/article/bvmybv/private-texts-reveal-worlds-rich-and-famous-groveling-to-elon-musk\">Vice</a></li><li><a href=\"https://www.cnbc.com/2022/09/30/who-texted-elon-musk-to-get-involved-or-offer-advice-on-twitter-deal.html\">CNBC</a></li><li><a href=\"https://blockworks.co/news/sam-bankman-fried-elon-musk-weighed-joint-effort-to-acquire-twitter/\">Blockworks</a></li><li><a href=\"https://www.theatlantic.com/technology/archive/2022/09/elon-musk-texts-twitter-trial-jack-dorsey/671619/\">The Atlantic</a></li><li><a href=\"https://fortune.com/2022/09/30/elon-musk-sam-bankman-fried-twitter-acquisition-billions/\">Fortune</a></li></ul><p>Some users already provided additional sources and raised good concerns about Will's involvement in comments <a href=\"https://forum.effectivealtruism.org/posts/WdeiPrwgqW2wHAxgT/a-personal-statement-on-ftx?commentId=vZJ8dKhuZdYvQRndx\">here</a> and <a href=\"https://forum.effectivealtruism.org/posts/WdeiPrwgqW2wHAxgT/a-personal-statement-on-ftx?commentId=w9waXs8znModHob4x\">here</a>. I think that in light of what is recently going on with SBF, FTX, Elon Musk and Twitter this involvement warrants a response from Will (different than the <a href=\"https://forum.effectivealtruism.org/posts/WdeiPrwgqW2wHAxgT/a-personal-statement-on-ftx\">general response</a> to the FTX debacle) - a response both to the community and to media at large. &nbsp;The story is already spreading across media big and small, serious and tabloid and a reaction is very much needed.<br>&nbsp;</p>", "user": {"username": "dyj34650"}}, {"_id": "ArDhtEcRbkwo42N9H", "title": "The FTX Situation: Wait for more information before proposing solutions", "postedAt": "2022-11-13T20:28:29.502Z", "htmlBody": "<p><em>Edit: <a href=\"https://forum.effectivealtruism.org/posts/ArDhtEcRbkwo42N9H/the-ftx-situation-wait-for-more-information-before-proposing?commentId=Nc3L4LCjQPg7ieoNb#comments\">Eli has a great comment on this which I suggest everyone read</a>. He corrects me on a few things, and gives his far more informed takes.</em></p>\n<p>I'm slightly scared that EA will overcorrect in an irrelevant direction to the FTX situation in a way I think is net harmful, and I think a major reason for this fear is seeing lots of people espousing conclusions about solutions to problems <em>without us actually knowing what the problems are yet</em>.</p>\n<p>Some examples of this I've seen recently on the forum follow.</p>\n<h1>Integrity</h1>\n<p>It is uncertain whether SBF intentionally committed fraud, or just <a href=\"https://nitter.net/SBF_FTX/status/1590709172936798208#m\">made a mistake</a>, but people seem to be reacting as if <a href=\"https://forum.effectivealtruism.org/posts/HyHCkK3aDsfY95MoD/cea-ev-op-rp-should-engage-an-independent-investigator-to\">the</a> <a href=\"https://forum.effectivealtruism.org/posts/XHrHsrQGyr4NnqCA7/we-must-be-very-clear-fraud-in-the-service-of-effective\">takeaway</a> <a href=\"https://forum.effectivealtruism.org/posts/txGgPvKgZFpphdkHe/my-reaction-to-ftx-appalled\">from</a> <a href=\"https://forum.effectivealtruism.org/posts/WdeiPrwgqW2wHAxgT/a-personal-statement-on-ftx\">this</a> is that fraud is bad.</p>\n<p>These articles are mostly saying things of the form 'if FTX engaged in fraud, then EA needs to make sure people don't do more fraud in the service of utilitarianism.' from a worrying-about-group-think perspective, this is only a little less concerning than directly saying 'FTX engaged in fraud, so EA should make sure people don't do more fraud'.</p>\n<p>Even though these articles aren't literally saying that FTX engaged in fraud in the service of utilitarianism, I worry these articles will shift the narrative EA tells itself towards up-weighting hypotheses which say FTX engaged in fraud in the service of utilitarianism, especially in worlds where it turned out that FTX did commit fraud, but it was motivated by <a href=\"https://forum.effectivealtruism.org/posts/WdeiPrwgqW2wHAxgT/a-personal-statement-on-ftx\">pride</a>, or other selfish desires.</p>\n<h1>Dating</h1>\n<p>Some have claimed FTX's downfall happened as a result of <a href=\"https://www.businessinsider.com/ftx-inner-circle-all-dated-each-other-in-bahamas-report-2022-11\">everyone sleeping with each other</a>, and this interpretation is <a href=\"https://forum.effectivealtruism.org/posts/NacFjEJGoFFWRqsc8/women-and-effective-altruism\">not obviously unpopular</a> on the forum. This seems quite unlikely compared to alternative explanations, and the post Women and Effective Altruism takes on a tone &amp; content I find toxic to community epistemics<sup class=\"footnote-ref\"><a href=\"#fn-F5AsPxNdJ5zJbpzeh-1\" id=\"fnref-F5AsPxNdJ5zJbpzeh-1\">[1]</a></sup>, and anticipate wouldn't fly on the forum a week ago.</p>\n<p>I worry the reason we see this post now is that EA is confused, wants to do something, and is really searching for anything to blame for the FTX situation. If you are confused about what your problems are, you should not go searching for solutions! You should ask questions, make predictions, and try to understand what's going on. <em>Then</em> you should ask how you could have prevented or mitigated the bad events, and ask whether those prevention and mitigation efforts would be worth their costs.</p>\n<p>I think this problem is important to address, and am uncertain about whether this post is good or bad on net. The point is that I'm seeing a bunch of heated emotions on the forum right now, this is not like the forum I'm used to, and lots of these heated discussions seem to be directed towards pushing new EA policy proposals rather than trying to figure out what's going on.</p>\n<h1><a href=\"https://forum.effectivealtruism.org/posts/HyHCkK3aDsfY95MoD/cea-ev-op-rp-should-engage-an-independent-investigator-to\">Vetting funding</a></h1>\n<p>We could immediately launch a costly investigation to see who had knowledge of fraud that occurred before we actually know if fraud occured or why. In worlds where we\u2019re wrong about whether or why fraud occurred this would be very costly. My suggestion: wait for information to costlessly come out, discuss what happened when not in the midst of the fog and emotions of current events, and <em>then</em> decide whether we should launch this costly investigation.</p>\n<p>Adjacently, some are arguing EA could have vetted FTX and Sam better, and averted this situation. This reeks of hindsight bias! Probably EA could not have done better than all the investors who originally vetted FTX before giving them a buttload of money!</p>\n<p>Maybe EA should investigate funders more, but arguments for this are orthogonal to recent events, unless CEA believes their comparative advantage in the wider market is high-quality vetting of corporations. If so, they could stand to make quite a bit of money selling this service, and should possibly form a spinoff org.</p>\n<h1>Conclusion</h1>\n<p>EA is not thinking straight right now, and everyone should stop it with putting their ill-informed conclusions about the takeaways from recent events on the forum, and discuss the object-level events more in the hopes the community can actually update on information once it gets in, instead of getting stuck into an incorrect and unhelpful narrative about what happened.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-F5AsPxNdJ5zJbpzeh-1\" class=\"footnote-item\"><p>In particular, it ties together observations and policy proposals so that in order to disagree with the policy proposals, you have to trip over your words in order to also not call the poster a liar. <a href=\"#fnref-F5AsPxNdJ5zJbpzeh-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "D0TheMath"}}, {"_id": "3Bq3PFRfzaEEAATti", "title": "Clawbacks: Probably don't spend FTX grantee money for the next few days(?)", "postedAt": "2022-11-12T21:04:39.977Z", "htmlBody": "<p><strong>Update</strong>: <a href=\"https://forum.effectivealtruism.org/users/molly\">Molly</a> has now posted her <a href=\"https://forum.effectivealtruism.org/posts/o8B9kCkwteSqZg9zc/thoughts-on-legal-concerns-surrounding-the-ftx-situation\">Thoughts on legal concerns surrounding the FTX situation</a>. So please defer to that. The key section related to this post is:</p><blockquote><p><strong>Essentially, if you received money from an FTX entity in the debtor group anytime on or after approximately August 11, 2022, the bankruptcy process will probably ask you, at some point, to pay all or part of that money back.</strong> It\u2019s almost impossible to say right now whether any specific grant or contract will be subject to clawback \u2013 there just isn\u2019t enough information on the court docket \u2013 and if your transaction is captured, you will eventually receive formal notice from the bankruptcy court and will have an opportunity to raise a defense, negotiate a settlement, or litigate. You can read a legal explainer about preference claims&nbsp;<a href=\"https://blogs.orrick.com/distressed-download/2015/02/13/decoding-the-code-preferences-under-section-547-of-the-bankruptcy-code/\"><u>here</u></a>.&nbsp;</p></blockquote><p>[Edit: if you can reasonably avoid it]</p><p><strong>Disclaimer: </strong>This is not legal advice, and I do not know anything about the law. However, I have not seen this topic be discussed in any prominent place on the EA forum, so want to raise it.</p><p>I have seen various mentions around the EA forum and Twitter around the concern of clawbacks. Wikipedia <a href=\"https://en.wikipedia.org/wiki/Clawback#Financial_crime_clawbacks\">notes</a> that:</p><p><i>\"The term <strong>clawback</strong> or <strong>claw back</strong> refers to any money or benefits that have been given out, but are required to be returned (clawed back) due to special circumstances or events, such as the monies having been received as the result of a financial crime, or where there is a clawback provision in the </i><a href=\"https://en.wikipedia.org/wiki/Executive_compensation\"><i>executive compensation</i></a><i> contract\"</i></p><p><i>\"Clawback lawsuits in US courts, especially from innocent individuals and entities who profited from financial crimes of others, have increased in the years since 2000.\"</i></p><p>In an EA Forum comment, Molly notes that:</p><p><i>[Open Philanthropy] expects to put out an explainer about clawbacks tomorrow. It'll be written by our outside counsel and probably won't contain much in the way of specifics, but I think generally FTX grantees should avoid spending additional $$ on legal advice about this just yet.</i></p><p><i>Update that I don't think I'm going to be able to meet the timeline I set myself and have a product I think is worth sharing. Given that this is a holiday weekend it might be hard to find good advisors who are willing to work on this before Monday, but I'm going to try.&nbsp;</i></p><p>It seems to me that the responsible things is not to spend any FTX Future Fund money if it can be at all avoided \u2013 until we have legal clarity. Above and beyond potential moral obligations<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefex32l9lzqwl\"><sup><a href=\"#fnex32l9lzqwl\">[1]</a></sup></span>&nbsp;it seems really bad if people continue to spend grant money that they may end up being legally obliged to return.&nbsp;</p><p>I see that there has been discussion on EA Forum and Twitter about possible legal distinctions (Chapter 7 versus Chapter 11 bankruptcy; the 90-day window; if only FFF is affected or it includes grantees; what entity people received grants together etc.). However, until we have a clearer picture of what's going on, I think it would be prudent not to make irreversible decisions right now.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnex32l9lzqwl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefex32l9lzqwl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See <a href=\"https://forum.effectivealtruism.org/posts/CcNaQmrtdeC9PPixK/under-what-conditions-should-ftx-grantees-voluntarily-return#kMC5RFcNPw7F7HzcA\">here</a> for discussion. Fwiw, I agree with the sentiment \"<a href=\"https://forum.effectivealtruism.org/posts/FKJ8yiF3KjFhAuivt/impco-don-t-injure-yourself-by-returning-ftxff-money-for\">don't injure yourself by returning FTXFF money for services you already provided</a>\" \u2013 but I also don't want to see people injured if they go on to spend FTXFF money and still end up being legally obliged to return it.</p></div></li></ol>", "user": {"username": "Error404"}}, {"_id": "K4LCmbsAzsedWoNzg", "title": "Ways to buy time", "postedAt": "2022-11-12T19:31:10.434Z", "htmlBody": "", "user": {"username": "Akash"}}, {"_id": "qGrNAH3PoA7ipDq7k", "title": "fully aligned singleton as a solution to everything", "postedAt": "2022-11-12T18:19:59.597Z", "htmlBody": "", "user": {"username": "carado"}}, {"_id": "2wdanfCRFbNmWebmy", "title": "EA should consider explicitly rejecting *pure* classical total utilitarianism", "postedAt": "2022-11-12T17:52:08.672Z", "htmlBody": "<p>Under classical utilitarianism, it is permissible to take actions which severely violate moral rules if the action has sufficiently high expected value.</p>\n<p>This includes rules like \u201cdon\u2019t break the law\u201d, \u201cdon\u2019t be dishonest\u201d and \u201cdon\u2019t be violent\u201d.</p>\n<p>Many classical utilitarians argue that even under classical utilitarianism, in practice, we shouldn\u2019t violate important moral rules. This is because actions which do so don\u2019t have positive expected value, because of considerations such as \u201cyou\u2019ll encourage other people to break these moral rules, which will do lots of harm over the long term and the positives of your actions won\u2019t outweigh this\u201d.</p>\n<p>However, <em>some</em> strains of EA thinking contain 2 key ideas that will encourage classical total utilitarians to break moral rules despite these considerations:</p>\n<ol>\n<li>\n<p>There is almost infinite value in humanity\u2019s future, especially if a techno-utopian future involving space colonisation and digital minds comes to fruition. Mitigating extinction risks will help make this future happen, giving actions to mitigate extinction risks infinite expected value.</p>\n</li>\n<li>\n<p>Extinction from AGI disaster is likely in the next 20-30 years.</p>\n</li>\n</ol>\n<p>Idea 1 means that x-risk mitigating actions which violate moral rules will still have positive expected value.</p>\n<p>Idea 2 means that there isn\u2019t much time for long-term harmful effects of moral rule-breaking to accumulate.</p>\n<p>Crucially, I think if you agree with both idea 1 and 2, you must think that stealing money to fund AI safety research has positive expected value, and you cannot object to alleged fraud at FTX to fund AI safety research on classical utilitarian grounds.</p>\n<p>I think there <em>is</em> strong consensus in EA against severely violating moral rules, even amongst the classical utilitarians in EA, but this is because most self-described classical utilitarians don\u2019t endorse the classical utilitarian action in <em>every</em> situation, they just endorse it in <em>most</em> situations.</p>\n<p>However, as movements grow, the chances of someone holding a more extreme version of other people\u2019s views also grows.</p>\n<p><em>If</em> EA becomes a very big movement, I predict that individuals on the fringes of the movement will commit theft with the goal to donate more to charity and violence against individuals and organisations who pose x-risks.</p>\n<p>I think this is an <em>almost</em> inevitable result of becoming a large social movement (think environmentalism and ecoterrorism).</p>\n<p>However, I think we can minimise this risk while retaining a difference from common sense morality and while continuing to be highly impactful.</p>\n<p>This would involve:</p>\n<p>EA organisations and leaders more prominently expressing an explicit belief that <em>pure</em> classical total utilitarianism is wrong (I think Will MacAskill has said stuff along these lines in the past)</p>\n<p>EA orgs, leaders and community builders prominently emphasising that EAs should pursue their goals within the confines of legality and non-violence, <em>even when breaking the law and being violent has very high expected value</em></p>\n", "user": {"username": "freedomandutility"}}, {"_id": "9rmGJQNgHJeseTGQL", "title": "After recent FTX events, what are alternative sources of funding for longtermist projects?", "postedAt": "2022-11-12T16:51:43.764Z", "htmlBody": "<p>Now that we know \"that it looks likely that there are many committed grants that the Future Fund will be unable to honor\" according to the former team of <a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1\">FTX Future Funds</a>, it would be useful for a number of us to have alternatives.</p><p>Current large funders are re-considering their grant strategy (such as <a href=\"https://forum.effectivealtruism.org/posts/mCCutDxCavtnhxhBR/some-comments-on-recent-ftx-related-events\">OpenPhil). </a>These donors are going to be extremely busy in the next few weeks. If you're not too funding-constrained, it seems like a good and pro-social strategy is to wait until after the storm and let others who have urgent and important grants figure things out.</p><p>However, it seems that this may not be true if you are very funding-restrained right now, or if some grants or fellowships have deadlines coming up soon that it may be useful to have on your radar.</p><p>So, what are good places to apply for funding now? (and in the future too)</p><p>To start, the <a href=\"https://www.grantinterface.com/Process/Apply?urlkey=fli\">FLI AI Existential Safety PhD Fellowship</a> has a deadline on <i><strong>November 15.&nbsp;</strong></i></p><blockquote><p>The Vitalik Buterin PhD Fellowship in AI Existential Safety is for PhD students who plan to work on AI existential safety research, or for existing PhD students who would not otherwise have funding to work on <a href=\"https://www.dropbox.com/s/74j0mf4mw5c0yoy/AI%20Existential%20Safety%20Research%20Definition.pdf?dl=0\">AI existential safety research</a>. It will fund students for 5 years of their PhD, with extension funding possible. At universities in the US, UK, or Canada, annual funding will cover tuition, fees, and the stipend of the student's PhD program up to $40,000, as well as a fund of $10,000 that can be used for research-related expenses such as travel and computing. At universities not in the US, UK or Canada, the stipend amount will be adjusted to match local conditions. Fellows will also be invited to workshops where they will be able to interact with other researchers in the field.&nbsp; Applicants who are short-listed for the Fellowship will be reimbursed for application fees for up to 5 PhD programs, and will be invited to an information session about research groups that can serve as good homes for AI existential safety research.</p></blockquote><p>More about the fellowship <a href=\"https://forum.effectivealtruism.org/posts/wFC3axfuwABHmoQ9H/the-vitalik-buterin-fellowship-in-ai-existential-safety-is\">here</a>.</p><p>What are other options?</p><p><br>&nbsp;</p>", "user": {"username": "CarolineJ"}}, {"_id": "WdeiPrwgqW2wHAxgT", "title": "A personal statement on FTX", "postedAt": "2022-11-12T16:40:36.404Z", "htmlBody": "<p><i>This is a repost </i><a href=\"https://twitter.com/willmacaskill/status/1591218014707671040\"><i>from a Twitter thread I made last night.</i></a><i> It reads a little oddly when presented as a Forum post, but I wanted to have the content shared here for those not on Twitter.</i></p><p>This is a thread of my thoughts and feelings about the actions that led to FTX\u2019s bankruptcy, and the enormous harm that was caused as a result, involving the likely loss of many thousands of innocent people\u2019s savings.</p><p>Based on publicly available information, it seems to me more likely than not that senior leadership at FTX used customer deposits to bail out Alameda, despite terms of service prohibiting this, and a (later deleted) tweet from Sam claiming customer deposits are never invested.</p><p>Some places making the case for this view include <a href=\"https://www.wsj.com/articles/ftx-tapped-into-customer-accounts-to-fund-risky-bets-setting-up-its-downfall-11668093732\">this article from Wall Street Journal</a>, <a href=\"https://twitter.com/jonwu_/status/1590099676744646656\">this tweet from jonwu.eth</a>, &nbsp;<a href=\"https://www.bloomberg.com/opinion/articles/2022-11-08/binance-s-zhao-sbf-ed-ftx-s-bankman-fried#xj4y7vzkg\">this article from Bloomberg</a> (and follow on articles).</p><p>I am not certain that this is what happened. I haven\u2019t been in contact with anyone at FTX (other than those at Future Fund), except a short email to resign from my unpaid advisor role at Future Fund. If new information vindicates FTX, I will change my view and offer an apology.</p><p>But if there was deception and misuse of funds, I am outraged, and I don\u2019t know which emotion is stronger: my utter rage at Sam (and others?) for causing such harm to so many people, or my sadness and self-hatred for falling for this deception.</p><p>I want to make it utterly clear: if those involved deceived others and engaged in fraud (whether illegal or not) that may cost many thousands of people their savings, they entirely abandoned the principles of the effective altruism community.</p><p>If this is what happened, then I cannot in words convey how strongly I condemn what they did. I had put my trust in Sam, and if he lied and misused customer funds he betrayed me, just as he betrayed his customers, his employees, his investors, &amp; the communities he was a part of.</p><p>For years, the EA community has emphasised the importance of integrity, honesty, and the respect of common-sense moral constraints. If customer funds were misused, then Sam did not listen; he must have thought he was above such considerations.</p><p>A clear-thinking EA should strongly oppose \u201cends justify the means\u201d reasoning. I hope to write more soon about this. In the meantime, here are some links to writings produced over the years.</p><p>These are some relevant sections from What We Owe The Future:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1668271237/mirroredImages/WdeiPrwgqW2wHAxgT/ezgxkctsldcyfzkjn9h9.png\"><br><img src=\"http://res.cloudinary.com/cea/image/upload/v1668271237/mirroredImages/WdeiPrwgqW2wHAxgT/qw3xlgit2nwcvkxvd0ti.png\"></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1668271237/mirroredImages/WdeiPrwgqW2wHAxgT/o463qr55st3hl0ujl3cj.png\"></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1668271237/mirroredImages/WdeiPrwgqW2wHAxgT/rb3fagshs20lkybb4rj7.png\"></p><p>Here is Toby Ord in The Precipice:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1668271237/mirroredImages/WdeiPrwgqW2wHAxgT/kqqyqrggwdccto1ln62w.png\"></p><p>Here is Holden Karnofsky:&nbsp; <a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\"><u>https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous</u></a></p><p>Here are the Centre for Effective Altruism\u2019s Guiding Principles:&nbsp; <a href=\"https://forum.effectivealtruism.org/posts/Zxuksovf23qWgs37J/introducing-cea-s-guiding-principles\"><u>https://forum.effectivealtruism.org/posts/Zxuksovf23qWgs37J/introducing-cea-s-guiding-principles</u></a></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1668271237/mirroredImages/WdeiPrwgqW2wHAxgT/leq9ne003oe4wfvrlgix.png\"></p><p>If FTX misused customer funds, then I personally will have much to reflect on. Sam and FTX had a lot of goodwill \u2013 and some of that goodwill was the result of association with ideas I have spent my career promoting. If that goodwill laundered fraud, I am ashamed.</p><p>As a community, too, we will need to reflect on what has happened, and how we could reduce the chance of anything like this from happening again. Yes, we want to make the world better, and yes, we should be ambitious in the pursuit of that.</p><p>But that in no way justifies fraud. If you think that you\u2019re the exception, you\u2019re duping yourself.</p><p>We must make clear that we do not see ourselves as above common-sense ethical norms, and must engage criticism with humility.</p><p>I know that others from inside and outside of the community have worried about the misuse of EA ideas in ways that could cause harm. I used to think these worries, though worth taking seriously, seemed speculative and unlikely.</p><p>I was probably wrong. I will be reflecting on this in the days and months to come, and thinking through what should change.</p>", "user": {"username": "William_MacAskill"}}, {"_id": "HyHCkK3aDsfY95MoD", "title": "CEA/EV + OP + RP should engage an independent investigator to determine whether key figures in EA knew about the (likely) fraud at FTX", "postedAt": "2022-11-12T14:22:26.439Z", "htmlBody": "<p>I think that key EA orgs (perhaps collectively) like the Center for Effective Altruism/Effective Ventures, Open Philanthropy, and Rethink Priorities should consider engaging an independent investigator (with no connection to EA) to try to identify whether key figures in those organisations knew (or can reasonably be inferred to have known, based on other things they knew) about the (likely) fraud at FTX.</p><p>The investigator should also be contactable (probably confidentially?) by members of the community and others who might have relevant information.</p><p>Typically a lawyer might be engaged to carry out the investigation, particularly because of professional obligations in relation to confidentiality (subject to the terms of reference of the investigation) and natural justice. But other professionals also conduct independent investigations, and there is no in principle reason why a lawyer needs to lead this work.</p><p>My sense is that this should happen very promptly. If anyone did know about the (likely) fraud at FTX, then delay potentially increases the risk that any such person hides evidence or spreads an alternative account that vindicates them.</p><p>I'm torn about whether to post this, as it may well be something that leadership (or lawyers) in the key EA orgs are already thinking about, and posting this prematurely might result in those orgs being pressured to launch an investigation hastily with bad terms of reference. On the other hand, I've had the concern that there is no whistleblower protection in EA for some time (raised in my <a href=\"https://forum.effectivealtruism.org/posts/7knuuhSauLFbiTLWz/legal-support-for-ea-orgs-useful\">March 2022 post</a> on legal needs within EA), and others (e.g. <a href=\"https://forum.effectivealtruism.org/posts/gx7BEkoRbctjkyTme/democratising-risk-or-how-ea-deals-with-critics-1\">Carla &nbsp;Zoe C</a>) have made this point earlier still. I am not posting this because I have a strong belief that anyone in a key EA org did know - I have no information in this regard beyond vague speculation I have seen on Twitter.</p><p>If you have a better suggestion, I would appreciate you sharing it (even if anonymously).</p><p><i>Epistemic status: pretty uncertain, slightly anxious this will make the situation worse, but on balance think worth raising.</i></p><p><i>Relevant disclosure: I received a regrant from the FTX Future Fund to investigate the legal needs of effective altruist organisations.</i></p><p>Edit: I want to clarify that I <strong>don't</strong> think that any particular person knew. I still trust all the same community figures I trusted one week ago, other than folks in the FTX business. For each 'High Profile EA' I can think of, I would be very surprised if that person in particular knew. But even if we think there is only a 0.1% chance that any of the most influential, say, 100 EAs knew, then the chance that <i>none</i> of them knew is 0.999^100, which is about 90.4% (assuming we naively treat those as independent events). If we care about the top 1000 most influential EAs, then we could get that 90.4% chance with just a 0.01% chance of failure.</p><p>Edit: I think Ryan Carey's comment is further in the right direction than this post (subject to my view that an independent investigation should stick to fact-finding rather than making philosophical/moral calls for EA) plus I've also had other people contact me spitballing ideas that seem sensible. I don't know what the terms of reference of an investigation would be, but it does seem like simply answering <i>\"did anybody know\"</i> might be the wrong approach. If you have further suggestions for the sorts of things that should be considered, it might be worth dropping those into the comments.</p>", "user": {"username": "Tyrone Barugh"}}, {"_id": "umMpBFQoEW5LtyFfM", "title": "Which core competencies should an EA refresh?", "postedAt": "2022-11-12T10:14:08.186Z", "htmlBody": "<p>In the Royal Navy, there are a series of shortish online courses called Naval Core Training (NCTs), on subjects like resisting capture, operational law, and diversity &amp; inclusion policies, that every sailor must remain \"in date\" for. &nbsp;The powers that be decided that these are the fundamental competencies that all Service personnel must have, and the courses are to be completed anew every year or two. I understand some Universities require students to review similar courses in research integrity, and I expect similar schemes exist in many other professional settings.&nbsp;</p><p>It struck me yesterday that I had largely forgotten some prominent topics discussed in <a href=\"https://www.lesswrong.com/rationality\">Rationality: A-Z</a>, as it is several years since I read it, and I resolved to read the How To Actually Change Your Mind section again. Having worked through the safety awareness NCT earlier that day, it occurred to me that I should treat the most important concepts in Rationality: A-Z as core competencies, to be routinely refreshed.&nbsp;</p><p>This inevitably lead me to a broader question - what other competencies should be included in Rationalist Core Training, if only that acronym weren't already taken? For someone intent on optimising charitable donations and planning their career with altruistic impact in mind, which resources should be regularly reviewed to ensure a somewhat detailed understanding is retained? I'm very keen to see what the people of this forum would suggest!</p>", "user": {"username": "Tom Gardiner"}}, {"_id": "6mnNHDdYaBBzL7Cb8", "title": "Knowing the cost of SBF's fraud, what size donation to effective causes would have made it ethical? $1B? $5B?", "postedAt": "2022-11-12T10:48:51.433Z", "htmlBody": "", "user": {"username": "qiaochu_fan"}}, {"_id": "WfeWN2X4k8w8nTeaS", "title": "Theories of Welfare and Welfare Range Estimates", "postedAt": "2022-11-14T13:52:58.974Z", "htmlBody": "<h1>Key Takeaways</h1><ul><li>Many theories of welfare imply that there are probably differences in animals\u2019 welfare ranges. However, these theories do&nbsp;not agree about the&nbsp;sizes&nbsp;of those differences.</li><li>The Moral Weight Project assumes that hedonism is true. This post tries to estimate how different our welfare range estimates could be if we were to assume some other theory of welfare.</li><li>We argue that even if hedonic goods and bads (i.e., pleasures and pains) aren't all of welfare, they\u2019re a lot of it. So, probably, the choice of a theory of welfare will only have a modest (less than 10x) impact on the differences we estimate between humans' and nonhumans' welfare ranges.</li></ul><p>&nbsp;</p><h1>Introduction</h1><p>This is the third post in the <a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\">Moral Weight Project Sequence</a>. The aim of the sequence is to provide an overview of the research that Rethink Priorities conducted between May 2021 and October 2022 on interspecific cause prioritization. The aim of this post is to suggest a way to quantify the impact of assuming hedonism on welfare range estimates.</p><h1>Motivations</h1><p>Theories of welfare disagree about the determinants of welfare. According to hedonism, the determinants of welfare are positively and negatively valenced experiences. According to desire satisfaction theory, the determinants are satisfied and frustrated desires. According to a garden variety objective list theory, the determinants are something like knowledge, developing and maintaining friendships, engaging in meaningful activities, and so on. Now, some animals probably have more intense pains than others; some probably have richer, more complex desires; some are able to acquire more sophisticated knowledge of the world; others can make stronger, more complex relationships with others. If animals systematically vary with respect to their ability to realize the determinants of welfare, then they probably vary in their welfare ranges. That is, some of them can probably realize more positive welfare at a time than others; likewise, some of them can probably realize more negative welfare at a time than others. As a result, animals probably vary with respect to the differences between the best and worst welfare states they can realize. The upshot: many theories of welfare imply that there are probably differences in animals\u2019 welfare ranges.</p><p>However, theories of welfare do&nbsp;<i>not&nbsp;</i>obviously agree about the&nbsp;<i>sizes&nbsp;</i>of those differences. Consider a garden variety objective list theory on which the following things contribute positively to welfare: acting autonomously, gaining knowledge, having friends, being in a loving relationship, doing meaningful work, creating valuable institutions, experiencing pleasure, and so on. Now consider a simple version of hedonism (i.e., one that rejects <a href=\"https://plato.stanford.edu/entries/mill-moral-political/#HapHigPle\"><u>the higher / lower pleasure distinction</u></a>) on which just one thing contributes positively to welfare: experiencing pleasure. Presumably, while many nonhuman animals (henceforth, animals) can experience pleasure, they can\u2019t realize many of the other things that matter according to the objective list theory. Given as much, it\u2019s plausible that if the objective list theory is true, there will be larger differences in welfare ranges between many humans and animals than there will be if hedonism is true.</p><p>For practical and theoretical reasons, the Moral Weight Project <a href=\"https://forum.effectivealtruism.org/posts/hxtwzcsz8hQfGyZQM/an-introduction-to-the-moral-weight-project\">assumes that hedonism is true</a>. On the practical side, we needed to make some assumptions to make any progress in the time we had available. On the theoretical side, <a href=\"https://plato.stanford.edu/entries/hedonism/\"><u>there </u></a><a href=\"http://www.olemartinmoen.com/wp-content/uploads/AnArgumentForHedonism.pdf\"><u>are </u></a><a href=\"https://www.utilitarianism.net/guest-essays/naturalistic-arguments-for-ethical-hedonism\"><u>powerful </u></a><a href=\"https://www.utilitarianism.net/guest-essays/precis-of-the-feeling-of-value\"><u>arguments</u></a> <a href=\"https://quod.lib.umich.edu/e/ergo/12405314.0003.004/--new-defense-of-hedonism-about-well-being?rgn=main;view=fulltext\"><u>for </u></a><a href=\"https://philpeople.org/profiles/ben-bramble\"><u>hedonism</u></a>. Still, those who reject hedonism will rightly wonder about the impact of assuming hedonism. How different would our welfare range estimates be if we were to assume some other theory of welfare?</p><p>In the rest of this post, we argue that while assuming hedonism makes a difference to welfare range estimates, it makes a surprisingly small difference\u2014likely less than 10x. Our central claim here is that even if hedonic goods and bads (pleasures and pains) aren\u2019t&nbsp;<i>all</i> of welfare, they\u2019re a lot of it.</p><h1>Tortured Tim</h1><p>The task here is to determine the impact of assuming hedonism on welfare range estimates. We don\u2019t need to consider all possible theories of welfare to do this. Instead, we need to identify a theory that\u2019s plausible and can serve as an upper bound. That is, we need to identify a theory of welfare that meets two conditions: first, those concerned with relative welfare range estimates would be willing to assign it a reasonably high credence; second, of the theories meeting the first condition, it supports the largest differences in welfare range estimates.</p><p>These conditions are fuzzy; we won\u2019t try to make them precise here. Moreover, we won\u2019t dwell on the challenge of showing that a given theory meets these conditions. Instead, we\u2019ll simply propose a candidate that seems promising and argue from there: namely, the garden variety objective list theory that we mentioned earlier. Given as much, our strategy is to assess the impact that the choice between hedonism and our garden variety objective list theory is likely to have on our welfare range estimates&nbsp;<i>for humans</i>. Then, we can extrapolate to animals.</p><h2>Round 1: Can Non-Hedonic Goods Outweigh Hedonic Bads?</h2><p>Our garden variety objective list theory needs to specify&nbsp;<i>how much&nbsp;</i>the various items on the list contribute to an individual\u2019s welfare range. Recall: our objective list theory says that there are many things that are good for individuals, including friendship, romantic love, acquiring knowledge, engaging in theoretical contemplation, doing meaningful work, developing practical skills, and so on. Now imagine Tortured Tim, an individual who is experiencing extraordinarily intense physical suffering. Nevertheless, he may have many other goods in his life: friendship, romantic love, knowledge, practical skills, and so on. However, it seems highly implausible that these goods put Tortured Tim into a net positive welfare state: no matter how valuable they are, they aren\u2019t so valuable as to outweigh the welfare costs of intense physical suffering.&nbsp;</p><p>Imagine the objective list theorist denying this. Then, the objective list theorist would be committed to saying that it would be prudentially rational for Tim to choose to extend his life by a day of torture rather than die a day sooner, as long as that wouldn\u2019t affect the other goods in his life. Or, if&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/10.1111/j.1468-0114.1991.tb00410.x\"><u>the arc of a life matters</u></a>, the objective list theorist would be committed to saying that it would be prudentially rational for Tim to choose days of torture now for an equivalent number of normal days tacked onto the end of his life. It seems obvious, though, that it would&nbsp;<i>not&nbsp;</i>be prudentially rational for Tim to make either choice.</p><p>Let\u2019s grant, then, that the non-hedonic goods in Tim\u2019s life don\u2019t put Tim into a net positive welfare state. Let\u2019s suppose further that welfare ranges are symmetrical around the neutral point\u2014that is, individuals can be made as well off as they can be made badly off. It follows that those non-hedonic goods, in the aggregate, increase Tim\u2019s welfare less than 50% of the hedonic portion of his welfare range (his \u201chedonic capacity,\u201d for short). That is, if the positive contributions of the various non-hedonic items on the objective list don\u2019t jointly outweigh Tim\u2019s suffering\u2014i.e., they leave him in a net negative welfare state\u2014then they aren\u2019t contributing more than half of that portion of Tim\u2019s welfare range that\u2019s grounded in his capacity to experience positive and negative affective states. (See Figure 1.)</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1668433980/mirroredImages/WfeWN2X4k8w8nTeaS/f0irvj8g5bkupbpwd1gz.png\"></p><p>Figure 1: Tortured Tim</p><p>If that\u2019s right, then we have a data point that\u2019s relevant to our initial question: just based on the above, it looks like the choice of a theory of welfare won\u2019t change our welfare range estimates by more than a factor of 1.5. That is, suppose that if hedonism is true, then chickens have a welfare range of 10 welfare units. It would follow that if our objective list theory is true, their welfare range will be less than 15 welfare units (depending on how well they can realize the relevant objective goods). In other words, when we \"tack on\" the portion of the welfare range associated with unrealized non-hedonic goods, it doesn't change Tim's total welfare range all that much.</p><p>Before going any further, let\u2019s pause to note that nothing said thus far entails that&nbsp;<i>interspecific variation&nbsp;</i>in welfare ranges is limited to a factor of 1.5. To see why not, let\u2019s assume that given hedonism, chickens have a welfare range of 10 welfare units, whereas given our objective list theory, they have a welfare range of 12. Now consider the abilities that permit securing various non-hedonic goods. For all we\u2019ve said, those may significantly enhance an individual\u2019s hedonic capacities too. That is, it could work out that the cognitive capacities that permit, say, theoretical contemplation <i>also </i>permit individuals to feel pleasures and pains more intensely than those without such capacities.&nbsp;</p><p>However, if hedonic capacities are enhanced by the same capacities that matter for securing various non-hedonic goods, then hedonism might imply that humans have a welfare range of 50, whereas given our objective list theory, they might have a welfare range of 74. So, the choice between hedonism and objective list theory could, in principle, be the difference between saying that humans have five times chickens\u2019 welfare range vs. saying that humans have roughly seven times their welfare range. Still, there\u2019s an&nbsp;<i>intra</i>specific cap. Suppose that hedonic capacities are enhanced by the same capacities that matter for securing various non-hedonic goods. Then, given some set of empirical facts,<i>&nbsp;</i>the shift from one theory of welfare to another shouldn\u2019t affect our estimate of&nbsp;<i>one species\u2019s&nbsp;</i>welfare range by a factor greater than 1.5.</p><h2>Tortured Tim, Round 2: Can Hedonic Bads Compromise Non-Hedonic Goods?</h2><p>All that said, there\u2019s a problem with our estimate. Our description of Tim\u2019s case assumes that Tim can face extraordinary suffering while retaining all the relevant non-hedonic goods. However, it could well be that at least some non-hedonic goods are compromised as suffering increases. For instance, if exercising your autonomy enhances welfare and Tim is suffering to the point that he can\u2019t exercise his autonomy, then he lacks that non-hedonic good.&nbsp;</p><p>By way of reply, let\u2019s first note that plausible though this observation may be, it doesn\u2019t seem to apply to every non-hedonic good. You can still have friends while being tortured; you can still have knowledge; indeed, you can actually be doing meaningful work&nbsp;<i>by</i> being tortured, as might be true of martyrs in the right (horrible) circumstances. So, while this observation may show that we\u2019ve underestimated the ceiling on the difference that non-hedonic goods can make to welfare ranges, there are limits to its significance.</p><p>The second thing to say is more concessive. Suppose that there are 11 items on the objective list, with hedonic goods being one of them, and suppose that of the 10 remaining, half the items are zeroed out due to Tim\u2019s suffering. Finally, suppose that all 10 of those remaining items make equal contributions to welfare. Originally, we concluded that if all the non-hedonic goods don\u2019t outweigh Tim\u2019s suffering, then they increase Tim\u2019s welfare by less than 50% of his hedonic capacity\u2014that is, his ability to have those goods means that his total welfare range is no more than 1.5 times his hedonic capacity. With these new assumptions, we can conclude that his ability to have all the objective goods no more than doubles his welfare range. (See Figure 2.)&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1668433980/mirroredImages/WfeWN2X4k8w8nTeaS/il9hajpoz5bp4spdgjms.png\"></p><p>Figure 2: Factoring in Unrealized Non-Hedonic Goods</p><p>However, the third thing to say is that we\u2019ve been charitable\u2014and perhaps too charitable\u2014in our estimate of the contribution of non-hedonic goods. For what it\u2019s worth, we don\u2019t think it\u2019s plausible that those goods get Tim anywhere near a net positive welfare state. His net welfare state remains very, very low even if he has a significant number of non-hedonic goods. So, if we suppose that those non-hedonic goods do far less to offset Tim\u2019s suffering, it follows that even if some non-hedonic goods are compromised as suffering increases, non-hedonic goods still make a much smaller contribution to individuals\u2019 welfare ranges than hedonic bads\u2014perhaps not even enough to justify our original, factor-of-1.5 estimate.</p><h2>Round 3: What about Non-Hedonic Bads?</h2><p>Unfortunately, there\u2019s another problem with our estimate. Thus far, we\u2019ve been ignoring non-hedonic&nbsp;<i>bads</i>. For instance, Tortured Tim has been tortured, which means that he\u2019s been&nbsp;<i>violated</i>. Perhaps it\u2019s objectively bad to be violated. If so, then we\u2019ve been ignoring factors that can drag Tim\u2019s welfare level down further, which means that his welfare range is larger.</p><p>How&nbsp;<i>much</i> larger, of course, depends on the list of objective bads and their relative weights. Let\u2019s assume, however, that the non-hedonic bads mirror the non-hedonic goods\u2014which may not be true, but seems like a friendly concession. That is, let's assume that if there are 10 non-hedonic goods, then there are 10 non-hedonic bads. Moreover, let's assume that Tim isn\u2019t realizing every non-hedonic bad simply in virtue of being tortured. Then, factoring in both non-hedonic goods and bads might triple his total welfare range relative to his hedonic capacity. (See Figure 3.)</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1668433980/mirroredImages/WfeWN2X4k8w8nTeaS/zw8s3ibjmcmdac3gnbcc.png\"></p><p>Figure 3: Factoring in Realized and Unrealized Non-Hedonic Bads</p><p>Of course, in saying this, we\u2019re ignoring the point we made at the end of the last section: namely, that we\u2019ve been too charitable in our estimate of the contribution of non-hedonic goods, which suggested that doubling was too high an estimate. If that\u2019s right, then tripling is too high an estimate as well.</p><h1>Applying Tortured Tim to animals</h1><p>Still, let\u2019s suppose that tripling is correct. Let\u2019s also suppose that&nbsp;<a href=\"https://www.journals.uchicago.edu/doi/10.1086/694272\"><u>welfare invariabilism</u></a> is true, according to which the same theory of welfare is true of every welfare subject. So, to illustrate, if our garden variety objective list theory is true of humans, then it\u2019s true of all nonhumans. (This is a friendly assumption: if variabilism is true, we might not get any differences in welfare ranges at all.) Given invariabilism, we can say that if the choice between a simple version of hedonism and an objective list theory wouldn\u2019t affect our estimate of humans\u2019 welfare range by more than 3x, it shouldn\u2019t affect our estimates of nonhumans\u2019 welfare ranges by more than 3x.</p><p>Again, this is&nbsp;<i>not&nbsp;</i>the same thing as saying that&nbsp;<i>interspecific variation in welfare ranges</i> is limited to 3x. For all we\u2019ve said here, many nonhumans may have much smaller hedonic capacities than many humans. If chickens, for instance, have just 0.1x humans\u2019 hedonic capacity, then the choice between hedonism and an objective list theory could be the difference between the conclusion that chickens have 0.1x humans\u2019 welfare range (given hedonism) and ~0.03x humans\u2019 welfare range (given our objective list theory). So, the point is about the impact of theories of welfare on welfare range estimates. We aren\u2019t making any claims here about welfare range estimates themselves, nor even trying to bound them.</p><h1>Conclusion</h1><p>Prior to investigating this issue, we would have guessed that disagreements about the correct theory of welfare could cause welfare range estimates to differ by orders of magnitude. If we\u2019ve drawn the right lesson from poor Tortured Tim, that simply isn\u2019t true. We suggest that, compared to hedonism, an objective list theory might 3x our estimate of the differences between humans\u2019 and nonhumans\u2019 welfare ranges. But just to be cautious, let\u2019s suppose it\u2019s 10x. While not insignificant, that multiplier makes it far from clear that the choice of a theory of welfare is going to be practically relevant. To see this, recall that Open Philanthropy <a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\">once estimated</a> that \"[if] you value chicken life-years equally to human life-years, this implies that corporate campaigns do about 10,000x as much good per dollar as top [global health] charities.\" We've argued that the \"discount\" from switching to an objective list theory will be relatively small. So, while it might matter alongside other ways of \"discounting\" chicken welfare, it would be surprising if a theory of welfare could alter the outcome of such analyses on its own.</p><p>&nbsp;</p><h1>Acknowledgments</h1><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668021658/mirroredImages/tnSg6o7crcHFLc395/jwshmyotzvycfj6bwvrz.png\"></figure><p>This research is a project of Rethink Priorities. It was written by Bob Fischer. Thanks to Jason Schukraft, Adam Shriver, Rachel Norman, Martina Schiestl, Alex Schnell, and Anna Trevarthen for helpful feedback on earlier versions of this post. Thanks to Catarina Kissinger for the visuals.&nbsp;If you\u2019re interested in RP\u2019s work, you can learn more by visiting our&nbsp;<a href=\"https://www.rethinkpriorities.org/research\">research database</a>. For regular updates, please consider<i>&nbsp;</i>subscribing to our<a href=\"https://www.rethinkpriorities.org/newsletter\"> newsletter</a>.<br>&nbsp;</p>", "user": {"username": "bob-fischer"}}, {"_id": "4zjnFxGWYkEF4nqMi", "title": "How could we have avoided this?", "postedAt": "2022-11-12T12:45:50.739Z", "htmlBody": "<p>It seems to me that the information that betting so heavily on FTX and SBF was an avoidable failure. So what could we have done ex-ante to avoid it?</p><p>You have to suggest things we could have actually done with the information we had. Some examples of information we had:</p><p>First, the best counterargument:</p><figure class=\"image image_resized\" style=\"width:70.51%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668266674/mirroredImages/4zjnFxGWYkEF4nqMi/zsk8e6h2zpzjqmyte3nc.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/ptelqcqgvmic7zpp0fmr 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/aqky7yibi3f1huahju9o 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/pfcn3irkoohda3xri2gm 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/hsyurhla05s4yckmul1z 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/fkki1rtknl3y4ltqutb8 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/r9yrfpuke9rxg7tdvd3x 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/w1nll92fjio5eiek8lt0 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/zdywwfm4x0ldktqtwv0m 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/hsyertha31uj3y8bbcc2 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/uwsv9r89rq5vzooyaean 1050w\"><figcaption><a href=\"https://twitter.com/ESYudkowsky/status/1591215030351323136?t=vpcfDIJISjusStVlvdMcBA&amp;s=19\">https://twitter.com/ESYudkowsky/status/1591215030351323136?t=vpcfDIJISjusStVlvdMcBA&amp;s=19</a>&nbsp;</figcaption></figure><p>Then again, if we think we are better at spotting x-risks then these people maybe this should make us update towards being worse at predicting things.</p><p>Also I know there is a temptation to wait until the dust settles, but I don't think that's right. We are a community with useful information-gathering technology. We are capable of discussing here.</p><h1>Things we knew at the time</h1><p>&nbsp;We knew that about half of Alameda left at one time.&nbsp;I'm pretty sure many are EAs or know them and they would have had some sense of this.<br><br><img src=\"http://res.cloudinary.com/cea/image/upload/v1668266674/mirroredImages/4zjnFxGWYkEF4nqMi/vonw2njeysvulxllyput.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/lxal3znx7smo6qhqfnrd 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/dw3oxlxsazobcwccnaly 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/oyg3hexeglvqwx52lydx 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/isrijux7tetks045xxhe 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/giib1yc1qui2tb900g78 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/qro2jnwtdcrgtioewbmo 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/fegiepjn4cr4nxyrjy1c 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/ckijailecm9n53lfp89o 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/tvxil9xu9hkta3frmyzj 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/lsy4rilrbpejkvlgiuos 1486w\"></p><p>We knew that SBF's wealth was a very high proportion of effective altruism's total wealth. And we ought to have known that something that took him down would be catastrophic to us.</p><p>This was Charles Dillon's take, but he tweets behind a locked account and gave me permission to tweet it.<br><br><img src=\"http://res.cloudinary.com/cea/image/upload/v1668266674/mirroredImages/4zjnFxGWYkEF4nqMi/wtyuxwz76amjoxdgbnfw.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/afzkuo79kkd0alnosw5j 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/hfzj71ilwpmygflarknv 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/stwtwsr7tdnvjsmqchkk 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/cyscv5eru4jn5aodlkqx 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/kr1baoyntwofee5xpf4r 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/myzb7kyfhylpj56fcgnl 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/bec0effmwk0azhithqkw 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/rwfiyctic9croyfcgeln 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/ppkbtineq0r8qk6y7uhl 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/y85dsqgevxvcvmasmotm 1062w\"></p><p>Peter Wildeford noted the possible reputational risk 6 months ago:</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668266674/mirroredImages/4zjnFxGWYkEF4nqMi/ruzktjykss7vxa0if33q.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/pkzfizulrshwhejfbqep 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/vfbe0ezczwxiu8ulsrws 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/bov3vzed4tx9teujd491 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/wtgqnvpzwebahfowskdk 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/np5kwfq84hxsdpgp0si3 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/l5cpvrgcb7bjqvddc7qf 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/parwxsqs1xc2zcpknctb 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/bma4dooixd8ge3hq3d4n 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/e6vgawelm9uf0kdnince 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4zjnFxGWYkEF4nqMi/tylxqf7ouzgztljjvxrb 1420w\"><figcaption><a href=\"https://forum.effectivealtruism.org/posts/hDK9CZJwH2Cqc9n9J/some-clarifications-on-the-future-fund-s-approach-to?commentId=6Mvj7XapNkGuLCzLn#6Mvj7XapNkGuLCzLn\">https://forum.effectivealtruism.org/posts/hDK9CZJwH2Cqc9n9J/some-clarifications-on-the-future-fund-s-approach-to?commentId=6Mvj7XapNkGuLCzLn#6Mvj7XapNkGuLCzLn</a>&nbsp;</figcaption></figure><p>We knew that corruption is possible and that large institutions need to work hard to avoid being coopted by bad actors.&nbsp;</p><p>Many people found crypto distasteful or felt that crypto could have been a scam.</p><p>FTX's Chief Compliance Officer, Daniel S. Friedberg, had behaved fraudulently In the past. <a href=\"https://coingeek.com/tether-links-to-questionable-market-makers-yet-another-cause-for-concern/ \">This</a> from august 2021.</p><blockquote><p>In 2013, an audio recording surfaced that made mincemeat of UB\u2019s original version of events. The recording of an early 2008 meeting with the principal cheater (Russ Hamilton) features Daniel S. Friedberg actively conspiring with the other principals in attendance to (a) publicly obfuscate the source of the cheating, (b) minimize the amount of restitution made to players, and (c) force shareholders to shoulder most of the bill.</p></blockquote>", "user": {"username": "nathan"}}, {"_id": "MmRuBNWCJmWiFGdzF", "title": "What are prestigious psychiatry departments?", "postedAt": "2022-11-14T11:13:51.821Z", "htmlBody": "<p>I am already quite far in my medical education and considering becoming a psychiatrist (because I enjoy it) to earn to give (because I still want to contribute). I\u2019m now searching for psychiatric clinics and departments (Europe, US or basically anywhere) for work and education that are prestigious and would look good on a CV (it would also be nice if they were acutally good). Of course there is Havard Medical School etc., but at this point, it is highly unrealistic for me to switch to an ivy league university. There have to be some lower hanging fruits.</p><p>I am aware of the good arguments against becoming a doctor, but this is out of the scope for this question which focuses on just one of the career options I\u2019m considering.</p>", "user": {"username": "SamMumm"}}]