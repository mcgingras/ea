[{"_id": "iR5keJapwMN2Zj2A4", "postedAt": "2022-11-28T12:21:54.981Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>TL;DR:</p><p>Reasons to think that \"neuron count\" correlates with \"moral weight\":</p><ol><li>&nbsp;Neuron counts <a href=\"https://slatestarcodex.com/2019/05/01/update-to-partial-retraction-of-animal-value-and-neuron-number/\"><u>correlate </u></a>with our intuitions of moral weights</li><li>\"Pains, for example, would seem to minimally require at least some representation of the body in space, some ability to quantify intensity, and some connections to behavioral responses, all of which require a certain degree of processing power.\"</li><li>\"There are studies that show increased volume of brain regions correlated with valenced experience, such as a study showing that cortical thickness in a particular region increased along with pain sensitivity.\" (But the opposite is also true. See 6. below.)</li></ol><p>Reasons to think that \"neuron count\" does <strong>NOT</strong> correlate \"moral weight\"</p><ol><li>There's more to information processing capacity than neuron count. There's also:<ol><li>Number of neural connections (synapses)</li><li>Distance between neurons (more distance -&gt; more latency)</li><li>Conduction velocity of neurons</li><li>Neuron refactory period (\"rest time\" between neuron activation)</li></ol></li><li>\"There's no consensus among people who study general intelligence across species that neuron counts correlate with intelligence\"</li><li>\"It seems conceptually possible to increase intelligence without increasing the intensity of experience\"</li><li>Within humans, we don't think that more intelligence implies more moral weight. We don't generally give less moral to children, elderly, or the cognitiviely impaired.</li><li>The <a href=\"https://link.springer.com/article/10.1007/BF00846566\"><u>top-down cognitive influences on pain</u></a> suggest that maybe intelligence actually mitigates suffering.</li><li>There are \"studies showing that increased pain is correlated with decreased brain volume in areas associated with pain\"</li><li>Hundreds of brain imaging experiments haven't uncovered any simple relationship between quantity of neurons firing and \"amount of pain\"</li><li>Bees have small brains, but have \"<a href=\"https://www.science.org/doi/10.1126/science.aag2360\"><u>cognitive flexibility</u></a>,&nbsp;<a href=\"https://www.science.org/doi/10.1126/science.aay8064\"><u>cross-modal recognition of objects</u></a>, and&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S0003347222002366\"><u>play behavior</u></a><u>\"</u></li><li>There are competing ideas for correlates of moral weight/consciousness/self-awareness:<ol><li><a href=\"https://global.oup.com/academic/product/in-natures-interests-9780195152012?cc=us&amp;lang=en&amp;\"><u>Reverals learning</u></a>&nbsp;</li><li><a href=\"https://www.jstor.org/stable/3506216#metadata_info_tab_contents\"><u>Trace conditioning</u></a></li><li><a href=\"https://link.springer.com/article/10.1007/s10539-020-09772-0\"><u>Unlimited associative learning</u></a></li><li><a href=\"https://www.science.org/doi/10.1126/science.167.3914.86\"><u>Mirror self-recognition</u></a></li></ol></li></ol>", "parentCommentId": null, "user": {"username": "Hamish Doodles"}}, {"_id": "eAN6hLKEB8FWR9Gwi", "postedAt": "2022-11-28T12:41:59.745Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Not sure I agree with the \"TL\" part haha, but this is a pretty good summary. However, I'd also add that there's no consensus among people who study general intelligence across species that neuron counts correlate with intelligence (I guess this would go between 1d and 2) and also that I think the idea that more neurons are <i>active</i> during welfare-relevant experiences is a separate but related point to the idea that more brain volume is correlated with welfare-relevant experiences.&nbsp;<br><br>I'd also note that your TL/DR is a summary of the summary, but there are some additional arguments in the report that aren't included in the summary. For example, here's a more general argument against using neuron counts in the longer report: &nbsp;<a href=\"https://docs.google.com/document/d/1p50vw84-ry2taYmyOIl4B91j7wkCurlB/edit#bookmark=id.3mp7v7dyd88i\">https://docs.google.com/document/d/1p50vw84-ry2taYmyOIl4B91j7wkCurlB/edit#bookmark=id.3mp7v7dyd88i</a>&nbsp;</p>", "parentCommentId": "iR5keJapwMN2Zj2A4", "user": {"username": "Adam Shriver"}}, {"_id": "FbDEYyPLH3cnex94p", "postedAt": "2022-11-28T13:09:20.269Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Thanks for feedback.</p><blockquote><p>Not sure I agree with the \"TL\" part haha</p></blockquote><p>Well, yeah. Maybe. It's also about making the structure more legible.</p><blockquote><p>there are some additional arguments in the report that aren't included in the summary.</p></blockquote><p>Anything specific I should look at?</p>", "parentCommentId": "eAN6hLKEB8FWR9Gwi", "user": {"username": "Hamish Doodles"}}, {"_id": "qvKoK44WnjwzoNNq2", "postedAt": "2022-11-28T14:34:43.779Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<blockquote><p>Anything specific I should look at?</p></blockquote><p>My link above was to a bookmark in the report, which includes an additional argument.&nbsp;<br>&nbsp;</p>", "parentCommentId": "FbDEYyPLH3cnex94p", "user": {"username": "Adam Shriver"}}, {"_id": "oTq9mQY2omJBqgtdj", "postedAt": "2022-11-28T20:14:00.084Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Re: more neurons = more valenced consciousness, does the full report address the <a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/#appendixH\">hidden qualia</a> possibility? (I didn't notice it at a quick glance.) My sense was that people who argue for more neurons = more valenced consciousness are typically assuming hidden qualia, but your objections involving empirical studies are presumably assuming no hidden qualia.</p>\n", "parentCommentId": null, "user": {"username": "lukeprog"}}, {"_id": "swdRQ4ihfYCHFCBh2", "postedAt": "2022-11-28T20:34:55.530Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>I really appreciate this post and the conclusion here seems very reasonable. As someone who is personally guilty of using neuron counts as a sole proxy for moral weight, I would love to include additional metrics that more closely proxy something like capacity for suffering and pleasure. However, my problem is that while the metrics mentioned (mirror-test, &nbsp;trace conditioning, unlimited associative learning, reversal learning) might be more accurate proxies, they are (as far as I can tell) not available for a wide variety of species. For me, the main goal of employing these moral weights is to get a framework that decisionmakers can use for evaluating the impact of any project. I am particularly interested in government cost-benefit analyses, where the ideal use case would be to have a spreadsheet where government economists could just plug in available proxies for moral weight and get an estimated valuation for suffering reduction for an individual of a particular species. Neuron counts are nice for this because you can pretty easily find an estimated neuron count for almost any species. With this issue in mind,</p><ol><li>Are you aware of any papers/databases that have a list of species for which any of the four recommended factors have been tested and the results? It seems, for example, that scientists make headlines when they find a species that passes the mirror test but I can't tell which species have \"failed\" it versus which have not been tested.</li><li>&nbsp;Other factors that are widely available for many species include brain mass, body mass, brain-to-body mass ratio, cortical neurons, whether the animal has any particular brain/anatomical structure, class/order, etc. It sounds like maybe the ratio of cortical neurons to brain size might be a reasonable proxy based on the section on processing speed--would you agree that would be an improvement over just neurons? Do any of these other characteristics stand out as plausible proxies?</li></ol>", "parentCommentId": null, "user": null}, {"_id": "HNxcpFT5MgRy97DuE", "postedAt": "2022-11-28T20:35:18.911Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>We have a report on conscious subsystems coming out I believe next week, which considers the possibility of non-reportable valenced conscious states.&nbsp;<br><br>Also (speaking only about my own impressions), I'd say that while some people who talk about neuron counts might be thinking of hidden qualia (eg Brian Tomasik), it's not clear to me that that is the assumption of most. I don't think the hidden qualia assumption, for example, is an explicit assumption of Budolfson and Spears or &nbsp;of MacAskill's discussion in his book (though of course I can't speak to what they believe privately).&nbsp;</p>", "parentCommentId": "oTq9mQY2omJBqgtdj", "user": {"username": "Adam Shriver"}}, {"_id": "qdWbMMrKm7eqmNbaL", "postedAt": "2022-11-28T20:58:53.607Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Hi Monica! We hear you about wanting a table with those results. We've tried to provide one here for 11 farmed species: <a href=\"https://forum.effectivealtruism.org/posts/tnSg6o7crcHFLc395/the-welfare-range-table\">https://forum.effectivealtruism.org/posts/tnSg6o7crcHFLc395/the-welfare-range-table</a></p><p>We tend to think that if the goal is to find a single proxy, something like encephalization quotient might be the best bet. It's imperfect in various ways, but at least it corrects for differences in body size, which means that it doesn't discount many animals nearly as aggressively as neuron counts do. (While we don't have EQs for every species of interest, they're calculable in principle.)</p><p>Finally, we've also developed some models to generate values that can be plugged into cost-benefit analyses. We'll post those in January. Hope they're useful!</p>", "parentCommentId": "swdRQ4ihfYCHFCBh2", "user": {"username": "bob-fischer"}}, {"_id": "Zovy2m3j4ywZSvPRD", "postedAt": "2022-11-28T21:20:00.675Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Thank you, this is very helpful and I definitely agree that EQs are available/practical enough to use in most cases. &nbsp;Really looking forward to seeing the new models in January!</p>", "parentCommentId": "qdWbMMrKm7eqmNbaL", "user": null}, {"_id": "uXTFYLnmGuw3sPZEf", "postedAt": "2022-11-28T21:51:05.360Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Thanks for doing this. Post is too long, could have been dot points. I want to see more TL;DRs like this</p>", "parentCommentId": "iR5keJapwMN2Zj2A4", "user": {"username": "Henry Howard"}}, {"_id": "ZLcgcZMByPCnD5fAE", "postedAt": "2022-11-29T11:08:09.025Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Thanks for the report &nbsp;!&nbsp;</p><p>I must admit that I didn't dig much into the debate, and only offering personal intuition, but I always found something odd off with the argument that \"more neurons = greater ability to feel pain\".</p><p>The implication of this argument would be <strong>\"Children and babies have a lower neuron count than adults, so they should be given lower moral value\"</strong>, as pointed out (i.e. it's less problematic if they die).</p><p>And I just don't see many people defending that. Many people would say the opposite: that children tend to be happier than adults. So I kept wondering why people used that approximation for other species.</p>", "parentCommentId": null, "user": {"username": "Corentin Fressoz"}}, {"_id": "kPvqQAMmaNNyfHkwJ", "postedAt": "2022-11-29T14:48:28.053Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>I'm really grateful for this post and the resulting discussion (and I'm curating the post). I've uncritically used neuron counts as a proxy in informal discussions (more than once), and have seen them used in this way a lot more.&nbsp;</p><p>It helped me to draw out a diagram as I was reading this post (would appreciate corrections! although I probably won't spend time trying to make the diagram nicer or cleaner). My understanding is that the post sketched out the rough case for neuron counts as a proxy for moral weight as predictors of the grey properties below (information-processing capacity, intelligence, extent of valenced consciousness, and the number of morally relevant thresholds crossed by the organism), and then disputed the (predictive power of the) arrows I've greyed out and written on.&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7cda4736b71c04e5539d15c5e35ecaf96a1f7d31e4e4fae2.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7cda4736b71c04e5539d15c5e35ecaf96a1f7d31e4e4fae2.png/w_350 350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7cda4736b71c04e5539d15c5e35ecaf96a1f7d31e4e4fae2.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7cda4736b71c04e5539d15c5e35ecaf96a1f7d31e4e4fae2.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7cda4736b71c04e5539d15c5e35ecaf96a1f7d31e4e4fae2.png/w_1400 1400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7cda4736b71c04e5539d15c5e35ecaf96a1f7d31e4e4fae2.png/w_1750 1750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7cda4736b71c04e5539d15c5e35ecaf96a1f7d31e4e4fae2.png/w_2100 2100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7cda4736b71c04e5539d15c5e35ecaf96a1f7d31e4e4fae2.png/w_2450 2450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7cda4736b71c04e5539d15c5e35ecaf96a1f7d31e4e4fae2.png/w_2800 2800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7cda4736b71c04e5539d15c5e35ecaf96a1f7d31e4e4fae2.png/w_3150 3150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7cda4736b71c04e5539d15c5e35ecaf96a1f7d31e4e4fae2.png/w_3432 3432w\"></figure>", "parentCommentId": null, "user": {"username": "Lizka"}}, {"_id": "v7tk5N7rzyzsESZvG", "postedAt": "2022-11-29T16:49:03.443Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Thanks, this is a great point.&nbsp;</p>", "parentCommentId": "ZLcgcZMByPCnD5fAE", "user": {"username": "Adam Shriver"}}, {"_id": "H4fp7Zf3SEFnGNuJD", "postedAt": "2022-11-29T16:52:43.331Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Wow, this is really cool, Lizka, thanks! &nbsp;I think it's a really nice visualization of the post and report. I would say, in regards to the larger argument, that @lukeprog is right that hidden qualia/conscious subsystems is another key route people try to take between neuron count and moral weight, so the full picture of the overall debate would probably need to include that. (and again, RP's report on that should be published next week).</p>", "parentCommentId": "kPvqQAMmaNNyfHkwJ", "user": {"username": "Adam Shriver"}}, {"_id": "47XrdmtBwptmmxddP", "postedAt": "2022-11-29T18:52:19.831Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>I think children and babies actually have about as many neurons as adults, but also far more synapses/connections.</p>\n<p><a href=\"https://extension.umaine.edu/publications/4356e/\">https://extension.umaine.edu/publications/4356e/</a></p>\n<p><a href=\"https://www.ncbi.nlm.nih.gov/books/NBK234146/\">https://www.ncbi.nlm.nih.gov/books/NBK234146/</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Neuron#Connectivity\">https://en.wikipedia.org/wiki/Neuron#Connectivity</a></p>\n", "parentCommentId": "ZLcgcZMByPCnD5fAE", "user": {"username": "MichaelStJules"}}, {"_id": "rrazkfA6J9JmGNFgy", "postedAt": "2022-11-30T00:48:21.276Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<blockquote><p>it's not clear to me that that is the assumption of most</p></blockquote><p>Thinking that much about anthropics will be common within the movement, at least.</p>", "parentCommentId": "HNxcpFT5MgRy97DuE", "user": {"username": "MakoYass"}}, {"_id": "kB4TJwahwQJLwPQuo", "postedAt": "2022-11-30T11:39:56.438Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Oh, ok, I didn't know that. Thanks for the link, this is surprising.</p>", "parentCommentId": "47XrdmtBwptmmxddP", "user": {"username": "Corentin Fressoz"}}, {"_id": "7LfYvwvo4u5GjkDLy", "postedAt": "2022-11-30T15:40:52.205Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>I think this is a good article and attacks some assumptions I've thought were problematic. That being said, I think it's worth elaborating on the claim that intelligence scales with moral worth. You say:</p><p>&nbsp;\"Furthermore, it certainly is not the case that in humans we tend to associate greater intelligence with greater moral weight. Most people would not think it\u2019s acceptible to dismiss the pains of children or the elderly or cognitively impaired in virtue of them scoring lower on intelligence tests.\"&nbsp;</p><p>&nbsp;This seems true, and is worth further discussion. Most famously, Peter Singer has argued in Animal Liberation that intelligence doesn't determine moral worth. It also brings to mind Bentham's quote: \"The question is not, <strong>Can</strong> <strong>they</strong> <strong>reason</strong>?, <strong>nor</strong> <strong>Can</strong> <strong>they</strong> <strong>talk</strong>? <strong>but</strong>, <strong>Can</strong> <strong>they</strong> <strong>suffer</strong>?\" We don't think children have less moral worth due to their decreased intelligence, nor do we think that less intelligent people have less moral worth\u2013so why should we apply this standard to animals? This is why Singer has argued for equal consideration of interests between species. What does this imply, then, about how we should determine the interests of animals?</p><p>Perhaps, we may try to count the neurons involved with pain, pleasure, and other emotions\u2013rather than neurons as a whole\u2014and use this as a metric for moral worth. This isn't perfect, it still has many problems, but would probably be better than other approaches.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Yair Atlas"}}, {"_id": "zJ6vcpcupm9a7L9JS", "postedAt": "2022-11-30T18:14:53.007Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>\"the more measurable a metric we choose, the less accurate it is, and the more we prioritize accuracy, the less we are currently able to measure\"</p>\n<p>Can you expand on this? Is it a reference to Goodheart's Law?</p>\n", "parentCommentId": null, "user": {"username": "Matt g"}}, {"_id": "LZp3aEuiTNAWa8adx", "postedAt": "2022-11-30T19:19:40.996Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>No, it's not a reference Goodheart's Law.</p><p>It's just that one reason for liking neuron counts is that we have relatively easy ways of measuring neurons in a brain (or at least relatively easy ways of coming up with good estimate). However, as noted, there are a lot of other things that are relevant if what we really care about is information-processing capacity, so neuron count isn't an accurate measure of information processing capacity.</p><p>But if we focus on information-processing capacity itself, we no longer have a good way of easily measuring it (because of all the other factors involved).&nbsp;</p><p>This framing comes from Bob Fischer's comment on an earlier draft, btw.&nbsp;</p>", "parentCommentId": "zJ6vcpcupm9a7L9JS", "user": {"username": "Adam Shriver"}}, {"_id": "tcafTEbFPJeCmmfgX", "postedAt": "2022-11-30T19:24:31.963Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Thanks, I agree on these points. In regards to focusing on neurons involved in pain or other emotions, while I agree this would be the ideal thing to look at, the problem is that there is so much disagreement in the literature about issues that would be relevant for deciding which neurons/brain areas to include. There are positions that range from thinking that certain emotions can be localized to very specific regions to those who think that almost the whole brain is involved in every different type of experience, and lots of positions in between. So for that reason we tried to focus on more general criticisms.</p>", "parentCommentId": "7LfYvwvo4u5GjkDLy", "user": {"username": "Adam Shriver"}}, {"_id": "KgDvJEW467AyDhJgu", "postedAt": "2022-11-30T19:26:17.357Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Thanks Michael!&nbsp;</p>", "parentCommentId": "47XrdmtBwptmmxddP", "user": {"username": "Adam Shriver"}}, {"_id": "6ZDNrrvh9CmJFrkFP", "postedAt": "2022-11-30T19:29:20.670Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Thanks, that makes sense. For some reason I read it as a kind of generalisable statement about epistemics, rather than in relation to the neuron count issues discussed in the article.</p>\n", "parentCommentId": "LZp3aEuiTNAWa8adx", "user": {"username": "Matt g"}}, {"_id": "vaong5oAwzhGCEwgN", "postedAt": "2022-11-30T20:36:40.523Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>(this is partially echoing/paraphrasing <a href=\"https://forum.effectivealtruism.org/posts/Mfq7KxQRvkeLnJvoB/why-neuron-counts-shouldn-t-be-used-as-proxies-for-moral?commentId=oTq9mQY2omJBqgtdj\">lukeprog</a>) I want to emphasize the <s>anthropic measure/phenomenology</s> (never mind, this can be put much more straightforwardly) <i>observer count </i>angle, which to me seems like the simplest way neuron count would lead to increased moral valence. You kind of mention it, and it's discussed more in the full document, but for most of the post it's ignored.</p><p>Imagine a room where a pair of robots are interviewed. The robot interviewer is about to leave and go home for the day, they're going to have to decide whether to leave the light on or off. They know that one of the robots hates the dark, but the other strongly prefers it.<br>The robot who prefers the dark also happens to be running on 1000 redundant server instances having their outputs majority-voted together to maximize determinism and repeatability of experiments or something. The robot who prefers the light happens to be running on just one server.</p><p>The dark-prefering robot doesn't even know about its redundancy, it doesn't lead it to report any more intensity of experience. <i>There is no report, but it's obvious that the dark-preferring robot is having its experience magnified by a thousand times</i>, because it is exactly as if there are a thousand of them, having that same experience of being in a lit room, even though they don't know about each other.</p><p>You turn the light off before you go.</p><p>Making some assumptions about how the brain distributes the processing of suffering, which we're not completely sure of, but which seem more likely than not, we should have some expectation that neuron count has the same anthropic boosting effect.</p>", "parentCommentId": null, "user": {"username": "MakoYass"}}, {"_id": "t7kY7vcECdzRxgWdw", "postedAt": "2022-11-30T21:39:22.280Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Neuron count <i>relative to body-size, relative to the average ratio between the brain size/neuron count and body size [update: \"encephalization quotient\"],</i> matters I think for intelligence and other capabilities. I think I am missing these, rather crucial I think, qualifications (and these are quite commonly used within biology). Is that correct? And perhaps that is related to this other key route you mention here. And consequently there is the link between higher intelligence or capabilities (such as self-consciousness) and suffering, for which I agree arguments can be made in either direction. And I agree bare neuron count is a bad proxy, and against that the proposed non-relation to body-size can also I think be productively employed as a reductio ad absurdum. Cheers!!</p>", "parentCommentId": "H4fp7Zf3SEFnGNuJD", "user": {"username": "Indra"}}, {"_id": "z7gAZeMpJEW8FiWTo", "postedAt": "2022-12-01T01:26:46.660Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>This is nice, but I'd also be interested to see quantification of moral weights for different animals when accounting for all the factors besides neuron count, and how much it differs from solely using neuron count alone.</p>\n", "parentCommentId": null, "user": {"username": "zeshen"}}, {"_id": "BphvbaKiwMFARjPqr", "postedAt": "2022-12-01T22:49:23.669Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Do you have a sense of whether the case is any stronger for specifically using cortical and pallial neurons? That's the approach Romain Espinosa takes <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4113271\">in this paper</a>, which is among the best work in economics on animal welfare.</p>", "parentCommentId": null, "user": {"username": "zdgroff"}}, {"_id": "m4BvZSRqayvHdNvS9", "postedAt": "2022-12-02T15:30:30.564Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Thank you for the important post!<br>&nbsp;</p><blockquote><p>\u201cwe might question how well neuron counts predict overall information-processing capacity\u201d</p></blockquote><p>My naive prediction would be that many other factors predicting information-processing capacity (e.g., number of connections, conduction velocity, and refractory period) are positively correlated with neuron count, such that neuron count is <i>pretty strongly correlated</i> with information processing even if it only plays a minor part in <i>causing </i>more<i> </i>information processing to happen.&nbsp;</p><p>You cite one paper (<a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(09)01597-8?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0960982209015978%3Fshowall%3Dtrue\">Chitka 2009</a>) that provides some evidence against my prediction (based on skimming the abstract, this seemed to be roughly by arguing that insect brains are not necessarily worse at information processing than vertebrate brains). Curious if you think this is the general trend of the literature on this topic?</p>", "parentCommentId": null, "user": {"username": "Buhl"}}, {"_id": "HAzGxDr4fwurdPqxo", "postedAt": "2022-12-02T18:25:03.006Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>I've given a little thought to this hidden qualia hypothesis but it remains very confusing for me.</p>\n<p>To what extent should we expect to be able to tractably and knowably affect such hidden qualia?</p>\n", "parentCommentId": "oTq9mQY2omJBqgtdj", "user": {"username": "Oliver Sourbut"}}, {"_id": "63pLEEtbTks8Zy5n5", "postedAt": "2022-12-04T19:17:43.180Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>I don't come here often, and don't plan to write something verbose whatsoever. I have one simple thing to say, speaking as a neuroscientist -- why on earth would you think something like the number of neurons in a brain is a good proxy for moral weight?</p><p>What age group has the most neurons in humans? Babies. It's babies. The ones you constantly have to tell what is right and what is wrong? To stop hitting their siblings, being selfish, etc? This is not a nuanced conversation, it is one that, if you subscribe to scientific materialism, is quickly and efficiently done away with.</p><p>To utilize logic in parallel with the way it's been done here so far : in humans, <i>decreasing neural count is correlated with increased moral intelligence</i> (incl. the development of empathy, moral reasoning, etc.)</p>", "parentCommentId": null, "user": {"username": "rhiza"}}, {"_id": "BK5ApFAfgomp4HM5d", "postedAt": "2022-12-05T15:50:22.052Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Thanks, yeah, I agree those are better than just raw neuron count and we discuss those a bit more in the longer report. But also the objections are meant to apply to even these measures.&nbsp;</p>", "parentCommentId": "t7kY7vcECdzRxgWdw", "user": {"username": "Adam Shriver"}}, {"_id": "3qMNQeMoZwQLcwF7C", "postedAt": "2022-12-05T15:51:00.682Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Here's the report on conscious subsystems: <a href=\"https://forum.effectivealtruism.org/posts/vbhoFsyQmrntru6Kw/do-brains-contain-many-conscious-subsystems-if-so-should-we\">https://forum.effectivealtruism.org/posts/vbhoFsyQmrntru6Kw/do-brains-contain-many-conscious-subsystems-if-so-should-we</a>&nbsp;</p>", "parentCommentId": "HAzGxDr4fwurdPqxo", "user": {"username": "Adam Shriver"}}, {"_id": "GNkNseNPcgaqdZdHA", "postedAt": "2022-12-05T15:51:21.062Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Here's the report on conscious subsystems: <a href=\"https://forum.effectivealtruism.org/posts/vbhoFsyQmrntru6Kw/do-brains-contain-many-conscious-subsystems-if-so-should-we\">https://forum.effectivealtruism.org/posts/vbhoFsyQmrntru6Kw/do-brains-contain-many-conscious-subsystems-if-so-should-we</a>&nbsp;</p>", "parentCommentId": "oTq9mQY2omJBqgtdj", "user": {"username": "Adam Shriver"}}, {"_id": "Zawuv4t9AxrSGvPiM", "postedAt": "2022-12-06T01:31:23.867Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Neuron counts should not be used as a proxy for moral weights. Neurons are biological cells in the nervous system that are responsible for transmitting information, but they are not directly related to moral values or decisions. Moral weights are determined by an individual's values, beliefs, and ideas about what is right and wrong. Therefore, neuron counts cannot be used to accurately measure moral weights.</p>\n", "parentCommentId": null, "user": {"username": "Pseudonym101"}}, {"_id": "5xJDWuDCpiLhjGK8H", "postedAt": "2022-12-06T04:54:01.468Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Note: When we discuss an organism's \"moral weight,\" ultimately what we mean is whether or not its existence should be prioritized over others' in the event that a choice between one or the other had to be made. \"Moral weight,\" \"moral value,\" and \"value,\" and \"worthy of existential priority\" are all essentially synonymous.</p><p>Which is more likely to contribute a useful idea in preventing the potential apocalyptic scenarios, such as an AI takeover or Yellowstone erupting: a human or a chicken?&nbsp;<br><br>Obviously, a human.&nbsp;<br><br>In addition to the plausibly increased capacity for organisms with a greater breadth and quantity of synaptic connections to experience consciousness and suffering, the moral value or \"moral weight\" of an organism also depends upon the organism's CREATIVITY - i.e. whether or not it can produce ideas and objects that aid in the universal fight against extinction and entropy that is an eternal existential threat against all living organisms, and the entire history of living organisms that have ever lived.</p><p>&nbsp;Because organisms with higher neuron counts are simply more likely to generate new and potentially extinction-preventing ideas, it makes sense that those organisms, in terms of their health and preservation, should be valued above all other organisms. Furthermore, higher-neuron organisms also have a higher potential to generate ideas that would increase the quality of life and symbiosis with lower-neuron organisms. What is the point of prioritizing (i.e., allocating resources to) a lower-neuron organism equally or more than a higher-neuron organism if doing so would only increase the probability of extinction (and decrease the probability of innovations towards symbiosis being made)?<br><br>What I am not seeking to imply: This does not mean that we humans HAVE to consume all other organisms of less neural capacity, and in fact it could be argued that the quality of our synaptic connections would be better, and therefore increase the probability of producing extinction-preventing and symbiosis-innovating ideas, if we were not burdened with the guilt of abusing our fellow earthly creatures.&nbsp;<br><br>The ethical answer to the fundamental problem that this proxy quantification is attempting to address - i.e., the morality of factory-farming:&nbsp;<br>If we come to a point, for any reason manmade or natural, where there are only enough resources to sustain human beings and no other animal species on the planet, then we should allow all other species to die and humans to live; otherwise, the probability that all evolutionary history will be erased and all life on Earth will go extinct will increase because animals will not have the capacity to address other probabilistically impending extinction events with the same capacity that humans would be able to.&nbsp;<br><br>Until we reach that point, we should strive to minimize the cruelty within the main means of food production - e.g. the self-evidently horrific practices of unanesthetized castration, forced impregnation, or debeaking in factory farming. However, we must do so while recognizing that the means of production itself is necessary to provide food affordability that wards off starvation for humans, and that such means of production does not necessarily have to coexist with the cruelty that mars it.&nbsp;</p><p>As we strive to outlaw the cruelty in the factory farming praxis, we should concurrently strive to build more efficient food infrastructure that can optimize food distribution to prevent food waste and make it so that the animal suffering that went into the food (which is not negligible simply because it is plausibly of a lower gradation than that of a human being's suffering) is put to the best possible use: the nourishment of a human being, the highest neuron-count organism that has ever existed on Earth. Thus, we would simultaneously achieve several benefits, well cost-effective and worthy of investment:&nbsp;<br>(i) We would increase the quality of meat production</p><p>(ii) We would eliminate costs of over-production</p><p>(iii) We would eliminate emissions and land pollution caused by food waste.</p><p>(iv) Like the water cycle but with food: The infrastructure would transport food waste to massive compost centers, which would be an alternative, zero-cost fertilizer and thus lower the cost of fertilizer and therefore crops themselves ...</p><ul><li>=&gt; which would lower the price of food production,&nbsp;</li><li>=&gt; which would make food more affordable (in addition to being higher quality),&nbsp;</li><li>=&gt; which would make people more well-nourished, which would increase the total probability that humanity would be able to conceive of useful (i.e. extinction-preventing and unnecessary-suffering-ameliorating) ideas,</li><li>=&gt; all of which would culminate into unprecedented economic (and moral) growth for all of humanity.&nbsp;</li></ul>", "parentCommentId": null, "user": {"username": "AB_Philopoet"}}, {"_id": "Xn5vADQ5qCQBAvz5q", "postedAt": "2022-12-06T16:20:22.206Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<blockquote>\n<p>I have one simple thing to say, speaking as a neuroscientist\u2014why on earth would you think something like the number of neurons in a brain is a good proxy for moral weight?</p>\n<p>What age group has the most neurons in humans? Babies. It\u2019s babies. The ones you constantly have to tell what is right and what is wrong? To stop hitting their siblings, being selfish, etc?</p>\n</blockquote>\n<p>I'm confused. It seems as if you're arguing against using neuron counts as a proxy for moral agency, not moral weight.</p>\n", "parentCommentId": "63pLEEtbTks8Zy5n5", "user": {"username": "Erich_Grunewald"}}, {"_id": "9fdj2G9XEBLs53yms", "postedAt": "2022-12-06T16:59:06.244Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>I'm curious about this as well. I'm also really confused about the extent to which this measure is just highly correlated with overall neuron count. The <a href=\"https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons#List_of_animal_species_by_forebrain_(cerebrum_or_pallium)_neuron_number\">wikipedia page</a> on neuron and pallial/cortical counts in animals lists humans as having lower pallial/cortical neuron counts than orcas and elephants while \"Animals and Social Welfare\" lists the reverse. Based on the Wikipedia page, it seems that there is a strong correlation (and while I know basically nothing about neuroscience, I would maybe think the same arguments apply?). I looked at some of the papers that the wikipedia page cited and couldn't consistently locate the cited number but they might have just had to multiply e.g. pallial neuron density by brain mass and I wouldn't know which numbers to multiply.</p>", "parentCommentId": "BphvbaKiwMFARjPqr", "user": null}, {"_id": "RwELGNzEhcgSihGpc", "postedAt": "2022-12-06T18:08:28.184Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>I calculated ratios of neurons in the pallium (and mushroom bodies/corpora pedunculata in insects) vs whole brain/body from the estimates on that Wikipedia page not too long ago. In mammals, it was mostly 10-30%, with humans at 21.5%, and a few monkeys around 40%. Birds had mostly 20-70%, with red junglefowl (wild chickens) around 27.6%. The few insects where I got these ratios were:</p>\n<ol>\n<li>Cockroach: 20%</li>\n<li>Honey bee: 17.7%</li>\n<li>Fruit fly: 2.5% (a low-valued outlier, but also the smallest brained animal for which I could get a ratio based on that table).</li>\n</ol>\n<p>EDIT: Some of the insect mushroom body numbers might only be counting one hemisphere's mushroom body neurons, and only intrinsic neurons, so the ratios might be too low. This might explain the low ratio for fruit flies.</p>\n<p>There could also be some other issues that may make some of these comparisons unfair.</p>\n", "parentCommentId": "9fdj2G9XEBLs53yms", "user": {"username": "MichaelStJules"}}, {"_id": "erKkPmQNyXKdWJfe5", "postedAt": "2022-12-06T22:37:18.418Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Oh interesting, thanks. Based on this it sounds super correlated (especially given that there are orders of magnitude difference between species in neuron counts).</p>\n", "parentCommentId": "RwELGNzEhcgSihGpc", "user": null}, {"_id": "9XJpTqjfxGZff6JEF", "postedAt": "2022-12-08T12:07:57.783Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>Hi Rhiza,</p><p>I appreciate your interesting point! I would note that as Erich mentioned, we're interested in moral patiency rather than moral agency, and we ultimately don't endorse the idea of using neuron counts.<br><br>But in response to your comment, there are different ways of trying to spell out why more neurons would matter. Presumably, on some (or most) of those, the way neurons are connected to other neurons matters, and as you know in babies the connections between neurons are very different from the connections in older individuals. So I think a defender of the neuron count hypothesis would still be able to say, in response to your point, that it's not just the number of neurons but rather the number of neurons networked together in a particular way that matters.&nbsp;</p>", "parentCommentId": "63pLEEtbTks8Zy5n5", "user": {"username": "Adam Shriver"}}, {"_id": "jdihcthjQQLFso577", "postedAt": "2022-12-08T12:18:14.321Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>It's an interesting thought, although I'd note that quite a few prominent authors would disagree that the cortex is ultimately what matters for valence even in mammals (Jaak Panksepp being a prominent example). I think it'd also raise interesting questions about how to generalize this idea to organisms that don't have cortices. Michael used mushroom bodies in insects as an example, but is there reason to think that mushroom bodies in insects are \"like the cortex and pallium\" but unlike various subcortical structures in the brain that also play a role in integrating information from different sensory sources? &nbsp;I think there's need to be more of a specification of which types of neurons are ultimately counted in a principled way.</p>", "parentCommentId": "BphvbaKiwMFARjPqr", "user": {"username": "Adam Shriver"}}, {"_id": "nMbevgghLAbsMNon4", "postedAt": "2022-12-08T12:23:06.747Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>AB, I think you're looking at a different stage of analysis than we are here. We're looking at how to weigh the intrinsic value of different organisms so that we know <i>how</i> to count them. It sounds to me like you're discussing the idea of, once we have decided on a particular way to count them, what actions should we take in order to best produce the greatest amount of value.&nbsp;</p>", "parentCommentId": "5xJDWuDCpiLhjGK8H", "user": {"username": "Adam Shriver"}}, {"_id": "wkGLzCRRXp3NhgqTv", "postedAt": "2022-12-09T10:41:19.640Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>The general argument is that it's just a useful but imperfect proxy, your findings are that it correlates with some stuff we might find just imperfectly so how do you go from being imperfect to 'shouldnt be used' again?</p>\n", "parentCommentId": null, "user": {"username": "Tenoke"}}, {"_id": "jj2QyLuufqs5XjSMv", "postedAt": "2022-12-09T16:33:54.170Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>It shouldn't be used as a unitary measure, but can be included in a combined measure, which is likely to correlate better.&nbsp;</p>", "parentCommentId": "wkGLzCRRXp3NhgqTv", "user": {"username": "Adam Shriver"}}, {"_id": "qALnG8Aei9dd43M5j", "postedAt": "2022-12-10T01:37:22.260Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": "<p>That's nearly true by definition for imperfect proxys. They don't carry all the information so you can improve on them by using other measures.</p>\n", "parentCommentId": "jj2QyLuufqs5XjSMv", "user": {"username": "Tenoke"}}, {"_id": "7d8A3RJHwCkqgnwPa", "postedAt": "2022-11-28T20:25:51.987Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": null, "parentCommentId": "oTq9mQY2omJBqgtdj", "user": null}, {"_id": "2uzSrLxyEfjjAEypu", "postedAt": "2022-11-28T21:06:46.936Z", "postId": "Mfq7KxQRvkeLnJvoB", "htmlBody": null, "parentCommentId": "swdRQ4ihfYCHFCBh2", "user": null}]