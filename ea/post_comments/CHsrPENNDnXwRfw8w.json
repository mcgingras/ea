[{"_id": "QZNqQCjPDegxbrDQ3", "postedAt": "2017-06-01T20:23:37.005Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>One option is to split the EA Community Fund into a Movement/Community Building Fund (which could fund organizations that engage in outreach, support local groups, build online platforms etc.) and a Cause/Means Prioritization Fund (which could fund organizations that engage in cause prioritization, explore new causes, research careers, study the policy process etc.).</p>\n", "parentCommentId": null, "user": {"username": "RandomEA"}}, {"_id": "uXQKaaWwnkRxHFRBG", "postedAt": "2017-06-01T23:59:56.943Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>I do see some advantages of keeping the number of funds low at this amount of money moving through because it increases the chance that any one particular fund will be able to support a particularly promising project that isn't appreciated by other donors.</p>\n", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "4ZaSp5Kk6gKWWfmnc", "postedAt": "2017-06-02T12:13:34.603Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>Thanks for this Kerry, very much appreciate the update.</p>\n<p>Three funds I'd like to see:</p>\n<ol>\n<li><p>The 'life-improving' or 'quality of life'-type fund that tries to find the best way to increase the happiness of people whilst they are alive. My view on morality leads me to think that is what matters most. This is the area I do my research on too, so I'd be very enthusiastic to help whoever the fund manager was.</p>\n</li>\n<li><p>A systemic change fund. Part of this would be reputational (i.e. no one could then complain EAs don't take systemic change seriously) another part would be that I'd really like to see what the fund manager would choose to give money too if it <em>had</em> to go to systemic change. I feel that would be a valuable learning experience.</p>\n</li>\n<li><p>A 'moonshots' fund that supported high-risk, potentially high-reward projects. For reasons similar to 2 I think this would be a really useful way for us to learn. </p>\n</li>\n</ol>\n<p>My general thought is the more funds the better, presuming you can find qualified enough people to run them. It has the positive effect of demonstrating EA's openess and diversity, which should mollify our critics. As mentioned, it provides chances to learn stuff. And it strikes me as unlikely new funds would divert much money away from the current options. Suppose we had an EA environmentalism fund. I assume people who would donate to that wouldn't have been donating to, say, the health fund already. They'd probably be supporting green charities instead. </p>\n", "parentCommentId": null, "user": {"username": "MichaelPlant"}}, {"_id": "qdY26wjzZRJicoP6Y", "postedAt": "2017-06-02T16:46:39.389Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>Great point.</p>\n<p>A different option for handling this concern would be for us to let fund managers email the EA Funds users if they have a good opportunity, but lack funding.</p>\n", "parentCommentId": "uXQKaaWwnkRxHFRBG", "user": {"username": "Kerry_Vaughan"}}, {"_id": "SPFHDDTaoKwh6ToZ7", "postedAt": "2017-06-02T17:00:58.239Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>This is an interesting idea. I have a few hesitations about it, however:</p>\n<ol>\n<li>The number of organizations which are doing cause prioritization and not also doing EA Community Building is very small (I can't think of any off the top of my head).</li>\n<li>My sense is that Nick wants to fund both community building and cause prioritization, so splitting these might place artificial constraints on what he can fund.</li>\n<li>EA Community building has the least donations so far ($83,000). Splitting might make the resulting funds too small to be able to do much.</li>\n</ol>\n", "parentCommentId": "QZNqQCjPDegxbrDQ3", "user": {"username": "Kerry_Vaughan"}}, {"_id": "XGqpX2fg82ieSDf7S", "postedAt": "2017-06-02T17:02:58.935Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>Hey Michael, great ideas. I'd like to see all of these as well. My concern would just be whether there are charities available to fund in the areas. Do you have some potential grant recipients for these funds in mind?</p>\n", "parentCommentId": "4ZaSp5Kk6gKWWfmnc", "user": {"username": "Kerry_Vaughan"}}, {"_id": "jYBXx8BqoEqcjDkiX", "postedAt": "2017-06-02T19:51:46.394Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<blockquote>\n<p>or we could add a long-term future fund that focused on areas other than AI-Safety. </p>\n</blockquote>\n<p>+1 differentiation. A Fund specifically for AI Safety would probably have demand - I'd donate. Other Funds for other specific GCRs could be created if there's enough demand too.</p>\n<p>A mild consideration against would be if there are funding opportunities in the Long Term Future area that would benefit both AI Safety and the other GCRs, such as the cross-disciplinary Global Catastrophic Risks Institute, and splitting would make it harder for these to be funded, maybe?</p>\n", "parentCommentId": null, "user": {"username": "DonyChristie"}}, {"_id": "f5DAJ3ko5X2p2fM8w", "postedAt": "2017-06-03T06:39:32.901Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>RE #1, organizations doing cause prioritization and not EA community building: Copenhagen Consensus Center, Foundational Research Institute, Animal Charity Evaluators, arguably Global Priorities Project, Open Philanthropy Project (which would obviously not be a good place to donate, but still fits the criterion).</p>\n<p>RE #2: if the point is to do what Nick wants, it should really be a &quot;Nick Beckstead fund&quot;, not an EA Community fund.</p>\n", "parentCommentId": "SPFHDDTaoKwh6ToZ7", "user": {"username": "MichaelDickens"}}, {"_id": "udsqoEHLoYNCtJsek", "postedAt": "2017-06-03T06:46:47.334Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>Now that you mention it, I think this would be a much more interesting way to divide up funds. I have basically no idea whether AI safety or anti-factory farming interventions are more important; but given the choice between a &quot;safe, guaranteed to help&quot; fund and a &quot;moonshot&quot; fund I would definitely donate to the latter over the former. Dividing up by cause area does not accurately separate donation targets along the lines on which I am most confident (not sure if that makes sense). I would much rather donate to a fund run by a person who shares my values and beliefs than a fund for a specific cause area, because I'm likely to change my mind about which cause area is best, and perhaps the fund manager will, too, and that's okay.</p>\n<p>Some possible axes:</p>\n<ol>\n<li>live-improving vs. life-saving (or, similarly, total view vs. person-affecting view)</li>\n<li>safe bets vs. moonshots</li>\n<li>suffering-focused vs. &quot;classical&quot;</li>\n<li>short-term vs. far future</li>\n</ol>\n<p>Although having all possible combinations just along these axes would require 16 funds so in practice this won't work exactly as I've described.</p>\n", "parentCommentId": "4ZaSp5Kk6gKWWfmnc", "user": {"username": "MichaelDickens"}}, {"_id": "yQYR9AGEGevzaNLo3", "postedAt": "2017-06-03T17:17:54.826Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>Good point on the axes. I think we would, in practice, get less than 16 funds for a couple of reasons.</p>\n<ol>\n<li><p>It's hard to see how some funds would, in practice, differ. For instance, is AI safety a moonshot or a safe bet if we're thinking about the future?</p>\n</li>\n<li><p>The life-saving vs life-improving point only seems relevant if you've already signed up to a person-affecting view. Talking about 'saving lives' of people in the far future is a bit strange (although you could distinguish between a far future fund that tried to reduce X-risk vs one that invested in ways to make future people happier, such as genetic engineering).</p>\n</li>\n</ol>\n", "parentCommentId": "udsqoEHLoYNCtJsek", "user": {"username": "MichaelPlant"}}, {"_id": "QorniZYqd4ramPW8M", "postedAt": "2017-06-04T16:13:24.461Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>Hello Kerry. Building on what Michael Dickens said, I now think the funds need to be more tightly specified before we can pick the most promising recipients within each. For instance, imagine we have a 'systemic change' fund, presumably a totalist systemic change fund would be different from a person-affecting, life-improving one. It's possible they might consider the same things top targets, but more work would be required to show that.</p>\n<p>Narrowing down then:</p>\n<p>Suppose we had life-improving fund using safe bets. I think charities like Strong Minds and Basic Needs (mental health orgs) are good contenders, although I can't comment on their organisational efficiency.</p>\n<p>Suppose we have a life-improving fund doing systemic change. I assume this would be trying to bring about political change via government policies, either at the domestic or international level. I can think of a few areas that look good, such as mental health policy, increasing access to pain relief in developing countries, and international drug policy reform. However, I can't name and exalt particular orgs as I haven't narrowed down to what I think the most promising sub-causes are yet.</p>\n<p>Suppose we had a life-improving moonshots fund. If this is going to be different the one above, I imagine this would be looking for start ups, maybe a bit like EA Ventures did. I can't think of anything relevant to suggest here apart from the start up I work on (the quality of which I can't hope to be objective about). Perhaps this fund could be looking at starting new charities too, rather than looking to fund existing ones.</p>\n<p>I don't think not knowing who you'd give money to in advance is a reason not to pursue this further. For instance, I would consider donating to some type of moonshots fund precisely because I had no idea where the money would go and I'd like to see someone (else) try to figure it out. Once they'd made their we could build on their analysis and learn stuff.</p>\n", "parentCommentId": "XGqpX2fg82ieSDf7S", "user": {"username": "MichaelPlant"}}, {"_id": "4XffBqRXzrDugA4Xc", "postedAt": "2017-06-04T21:26:01.890Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>Yup! I've always seen 'animals v poverty v xrisk' not as three random areas, but three optimal areas given different philosophies:</p>\n<p>poverty = only short term</p>\n<p>animals = all conscious suffering matters + only short term</p>\n<p>xrisk = long term matters</p>\n<p>I'd be happy to see other philosophical positions considered.</p>\n", "parentCommentId": "udsqoEHLoYNCtJsek", "user": {"username": "Ben Pace"}}, {"_id": "6xgbjnEC7g7C96thm", "postedAt": "2017-06-04T22:31:32.903Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>mostly agree, but you need a couple more assumptions to make that work.</p>\n<p>poverty = person affecting view of population ethics or pure time discounting + belief poverty relief is the best way to increase well-being (I'm not sure it is. See my <a href=\"http://effective-altruism.com/ea/yv/is_effective_altruism_overlooking_human_happiness/\">old forum post</a></p>\n<p>Also, you could split poverty (things like Give Directly) from global health (AMF, SCI, etc.). You probably need a person-affecting view or pure time discounting if you support health over x-risk, unless you're just really sceptical about x-risks.</p>\n<p>animals = I think animals are only a priority if you believe in a impersonal population ethic like totalism (maximise happiness over history of the universe, hence creating happy life is good), and you either do pure time discounting or you're suffering focused (i.e. unhappiness counts more than happiness)</p>\n<p>If you're a straightforward presentist (a person-affecting population ethic on which only presently existing things count), which is what you might mean by 'short term'. You probably shouldn't focus on animals. Why? Animal welfare reforms don't benefit the presently existing animals, but the next generation of animals, who don't count on presentism as they don't presently exist.</p>\n", "parentCommentId": "4XffBqRXzrDugA4Xc", "user": {"username": "MichaelPlant"}}, {"_id": "TAr2FH7zAdMnBT3YK", "postedAt": "2017-06-07T16:00:28.653Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<blockquote>\n<p>RE #2: if the point is to do what Nick wants, it should really be a &quot;Nick Beckstead fund&quot;, not an EA Community fund.</p>\n</blockquote>\n<p>The fund is whatever he thinks is best in EA Community building. If he wanted to fund other things the EA Community fund would not be a good option.</p>\n", "parentCommentId": "f5DAJ3ko5X2p2fM8w", "user": {"username": "Kerry_Vaughan"}}, {"_id": "oATyQebACr7gbCNSS", "postedAt": "2017-06-07T16:02:29.847Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<blockquote>\n<p>I have basically no idea whether AI safety or anti-factory farming interventions are more important; but given the choice between a &quot;safe, guaranteed to help&quot; fund and a &quot;moonshot&quot; fund I would definitely donate to the latter over the former. Dividing up by cause area does not accurately separate donation targets along the lines on which I am most confident (not sure if that makes sense).</p>\n</blockquote>\n<p>Great idea. This makes sense to me.</p>\n", "parentCommentId": "udsqoEHLoYNCtJsek", "user": {"username": "Kerry_Vaughan"}}, {"_id": "GziwC5NyibCcpZ7Qm", "postedAt": "2017-06-07T22:10:48.095Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>Just wanted to mention that I thought this was a really good post.  I think it did a good job of asking for community input at a time where it's potentially decision relevant but where enough considerations are known that some plausible options can be put forth.</p>\n<p>I think it also did a good job of describing lots of considerations without biasing the reader strongly in favor/against particular ones.</p>\n", "parentCommentId": null, "user": {"username": "HowieL"}}, {"_id": "rt6qWMvb7cKwAysM5", "postedAt": "2017-06-09T18:24:25.303Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>But how is funding cause prioritization related to EA community building?</p>\n", "parentCommentId": "TAr2FH7zAdMnBT3YK", "user": {"username": "Peter_Hurford"}}, {"_id": "HNeTRXMPTE95dqytL", "postedAt": "2017-06-09T18:25:06.927Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>There are also independent EA researchers doing cause prioritization research without community building.</p>\n", "parentCommentId": "f5DAJ3ko5X2p2fM8w", "user": {"username": "Peter_Hurford"}}, {"_id": "mEhogpoLt4YRccd4X", "postedAt": "2017-06-09T18:48:40.433Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>It would be nice to see a fund dedicated toward research, especially empirical research, to gather information relevant to EA objectives.</p>\n", "parentCommentId": null, "user": {"username": "Peter_Hurford"}}, {"_id": "n2ooLD8WzddScoFHw", "postedAt": "2017-06-12T00:31:29.260Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>Was thinking that there could be a tie-in with Giving What We Can's My Giving. You could tick a box to make your My Giving profile public, and then have another box for people browsing to &quot;copy this donor's distribution of donations&quot; like some trading websites (such as eToro) offer. Although they would not, unfortunately, come with tallies of expected total utilons produced, there could be league tables of most copied donors by number of people copying, and amount donated following their distribution.</p>\n", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "TwKFbcBstbihAfzLi", "postedAt": "2017-06-17T05:32:38.735Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>I really like the idea of doing more to identify new potential cause areas. \nVetting is really important, but I'm wary of the idea of anointing a specific EA org with sole discretion over vetting decisions. If possible, democratic vetting would be ideal (challenging though such arrangements can be).</p>\n", "parentCommentId": null, "user": {"username": "Richenda"}}, {"_id": "YzeQ87GrAuspRNSFT", "postedAt": "2017-06-24T19:17:36.344Z", "postId": "CHsrPENNDnXwRfw8w", "htmlBody": "<p>I'm excited about the idea of new funds. As a prospective user, my preferences are:</p>\n<ul>\n<li><p>Limited / well-organised choices. This is because I, like many people, get overwhelmed by too many choices. For example, perhaps I could choose between global poverty, animal welfare, and existential risks, and then choose between options within the category (eg &quot;Low-Risk Global Poverty Fund&quot; or &quot;Food Security Research Fund&quot;).</p>\n</li>\n<li><p>Trustworthy fund managers / reasonable allocation of funds. There are many reasonable ways to vet new funds, but ultimately I'm using the service because I don't want to have to carefully vet them myself.</p>\n</li>\n</ul>\n", "parentCommentId": null, "user": {"username": "Khorton"}}]