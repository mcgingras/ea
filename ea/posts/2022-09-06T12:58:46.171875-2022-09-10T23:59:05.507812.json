[{"_id": "DvEiQ5mwFLfSB7mNW", "title": "[Job] Project Manager: Community Health", "postedAt": "2022-09-10T18:40:08.667Z", "htmlBody": "<p>With recent media attention, increased funding, and growing ambition among community members, this is one of the most exciting times for EA, but also one of the riskiest. We need an ops-minded generalist to help address these risks, through both end-to-end ownership of targeted projects as well as building broader systems and processes to support other team members.</p><p>Example projects you might own:</p><ol><li>Create and manage a fund for community members\u2019 physical and mental health</li><li>Categorize the EA community into useful segments and conduct interviews to discover and understand their challenges, then summarize this feedback for stakeholders</li><li>Organize a retreat for team members to discuss and work on key problems in the EA community</li></ol><p>If this sounds like you, <a href=\"https://cea-core.typeform.com/to/ShV15HYE\">please apply</a>!</p>", "user": {"username": "Ben_West"}}, {"_id": "2KbLLyxj5gYJniHp2", "title": "Could it be a (bad) lock-in to replace factory farming with alternative protein?", "postedAt": "2022-09-10T16:24:45.592Z", "htmlBody": "<p><i><u>Epistemic status</u></i></p><p>I have recently been encouraged to write more posts, so I decided to test my fast writing this time with a topic that sat in my mind for very long, and just post it quickly. I haven\u2019t discussed it with anyone, nor checked if the topic has been discussed already, nor done any research. I literally just wrote the whole thing out without doing any new research (or even just search), except when I went back to insert links to some claims. It took me 5 hours to write this.</p><p>&nbsp;</p><p><i><u>TL;DR</u></i></p><p>Letting technological advancement in alternative protein and economics do most/all of the job of replacing factory farming could be bad, especially from the longtermist perspective. We likely only have one chance to eliminate factory farming (for food) for&nbsp;<i>moral</i> reasons, and we might lose a lot by losing this chance.</p><p>UPDATE: My new conclusion after reading comments (especially Tehas') is that <strong>we should still do, actually speed up the development of alternative proteins. And then at a point where a lot of people are vegans, or at least big reducetarians (i.e. society's animal product consumption is 20% of now), we start doing a lot of moral advocacy.&nbsp;</strong></p><p>&nbsp;</p><p><i><u>Why lock-in?</u></i></p><p>The lock-in is this: Unless there are other planets with other intelligent beings doing factory farming, or we somehow restart it after eliminating it,&nbsp;<strong>we likely only have one chance to eliminate factory farming (for food) for&nbsp;</strong><i><strong>moral</strong></i><strong> reasons.&nbsp;</strong>The moment plant-based alternative/cultivated meat (PB/CM) replaces virtually all factory farming for food (btw, I doubt this will be certain, see <a href=\"https://forum.effectivealtruism.org/posts/bfdc3MpsYEfDdvgtP/why-the-expected-numbers-of-farmed-animals-in-the-far-future\">this post</a>), we lose the chance to do so for moral reasons. Yes, after that we can still change our laws and say that we \u201cban\u201d factory farming for food in a time where there is virtually none, but I argue that even the motivation behind making such legal bans matters. This leads to the second section.<br>&nbsp;</p><p><i><u>Why might the lock-in be bad?</u></i></p><p>Let\u2019s first talk about using laws to ban factory farming. We have, coarsely speaking, two options:&nbsp;</p><ol><li>Ban it when we still have factory farming&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (btw, please consider <a href=\"https://factory-farming.ch/\">supporting</a> the federal ballot initiative to abolish factory farming in Switzerland, September 25)</li><li>Ban it after we virtually eliminated it for non-morally relevant reasons&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(and excuse me for emphasizing again, I don\u2019t think it\u2019s 100% guaranteed).&nbsp;</li></ol><p>The moral character (and therefore the education that is based on it) we show in the two scenarios will be drastically different - It seems much better if we are so morally determined that we simply make a law to ban factory farming, than we eliminate it for economic reasons and then say we ban it. Some of my more particular worries include: If we ban one form of animal exploitation but not all, it might mislead people to think that those that are still legal are morally acceptable. I also worry that using laws to capture our abolition of moral catastrophes&nbsp;<i>after&nbsp;</i>they become economically inviable, can create a false sense of progress - making us feel overly confident about our moral progress and moral capacity, and therefore makes us not informed enough to have good future progress.&nbsp;</p><p>Another scenario is we simply replace factory farming for food with technologies, without ever banning it. There was a historical example that is very similar: Animal advocates often use the <a href=\"https://youtu.be/AULCDAFo6AY?t=1310\">example of automobiles replacing horses being exploited for transportation to explain the importance of technologies in our moral progress</a>. But the same example can also be evidence that the (near) elimination of a moral catastrophe using technological advancement can be bad in the long-term. Horse riding, and the riding of other animals, still exist in different forms of entertainment, such as tourism, sports, and gambling. Yes, they cause much less direct suffering than the use of animals as transports, but the value they communicate is still very bad, and virtually the same - an animal can be caused to exist, raised, and exploited for human use however we like.&nbsp;</p><p>Also, besides actively communicating speciesist values, the way we improve our values, generally, also matters. Always waiting for technological changes might mislead us to think that we have less obligation to improve our moral values or actions when the technological/economic incentives are lower. And it seems that there are people who hold this view. (For example, I am pretty depressed by the fact that people, including vegans and farmed animal advocates, often say \u201cyes maybe wild animals\u2019 lives are indeed horrible, but let\u2019s wait for technologies to be viable before we try to help. But if it is important to help, why not invest in research and technologies that will help, now?) Therefore, in a sense, by letting automobiles replace horses as practical transport instead of listening to the horse advocates and becoming better humans, humanity has lost a great opportunity to do something for the animals for moral reasons, and do so by accepting an economic loss.&nbsp;</p><p>Now let\u2019s turn this issue of how we do our moral progress to replacing factory farming for food. Look at the slogans of PB/CM companies, they want to make PB/CM tastier, safer, more healthy, more environmentally friendly, and cheaper - in other words, better than animal products in&nbsp;<i>every single way</i>. <strong>Now, humanity, are we going to admit to our future generations that this is really what it takes for us to eliminate factory farming for food?</strong> Let\u2019s say, fantastically, we eliminate factory farming for food with PB/CM in 2030, and then in 2100 people are facing another moral catastrophe (Maybe factory farming for reasons? Maybe exploitation of AI/digital beings?), how much confidence will the advocates at that time have, looking at how much incentives it took for humanity to stop doing factory farming for food? Well, I personally did not have much hope in humanity's moral progress, until I recently got moderately convinced that it\u2019s more likely than not that we abolished slavery mainly for non-economic reasons.</p><p>And in case you think that it is impossible to have moral progress without economic reasons. I tend to disagree, and Will Macaskill also. He wrote in What We Owe The Future that the view that it was economic incentives caused by new technologies that cause slavery to be abolished, is now out of fashion in academia. He thinks that it was pretty much the triumph of the abolitionists. So there's a reason to think that moral progress is a genuine alternative to technologically forced social progress.</p><p>A third reason this lock-in could be bad is explained in the next section, titled \u201cother types of factory farming\u201d.</p><p>&nbsp;</p><p><i><u>Other types of factory farming</u></i></p><p>You might have noticed I said \u201cfactory farming&nbsp;<i>for food</i>\u201d (FFFF) many times. This is because we can have factory farming for other reasons. As of now, there is factory farming for fashion materials, companion/ornamental animals, animal experimentation, <a href=\"https://en.wikipedia.org/wiki/Donkey-hide_gelatin\">medicines</a> and <a href=\"https://www.businesswire.com/news/home/20211019006233/en/Kerecis-Announces-First-Ever-Fish-Skin-Implantable-Medical-Product-for-Surgery\">medical supplies</a>, pigments, for <a href=\"https://www.scmp.com/news/hong-kong/health-environment/article/3161957/hong-kong-biotech-firm-wins-government-contract\">waste treatment</a>. In the future, there might be factory farming for <a href=\"https://www.nature.com/articles/d41586-022-01861-2\">replacement organs</a>,&nbsp;for <a href=\"https://www.weforum.org/agenda/2021/03/singapore-startup-biomaterial-pharmaceuticals-electronics/\"><u>making semiconductors</u></a>, and for <a href=\"https://www.newscientist.com/article/2301500-human-brain-cells-in-a-dish-learn-to-play-pong-faster-than-an-ai/\">animal neurons used to make computers/AI</a>. These non-food-motivated factory farming most likely won\u2019t be replaced by the same technology.</p><p>And this has huge relevance to the core claim of this post. If these types of factory farming are of smaller scales than FFFF at the point FFFF is replaced by technologies, there could be little incentive in continuing to try to eliminate them.</p><p>You might think: Well if the other types of factory farming are much smaller problems then we shouldn\u2019t worry! I disagree with this view.&nbsp;</p><ul><li>Moral catastrophes on smaller scales are still horrific.</li><li>They still communicate and propagate bad values, in this case, speciesism, or more particularly, animals can be raised and used in ways we like.<ul><li>Slightly side-tracking: Maybe speciesism could also carry over to other kinds of relations. Maybe it could affect how we treat digital beings. There were also memes, maybe even serious discussions, about AI learning from how humans treat animals as a model for how they treat humans.</li></ul></li><li><strong>They could become larger than the current FFFF.&nbsp;There is no way to guarantee that they won\u2019t. </strong>For example, let\u2019s say we somehow (fantastically) made cultivated meat 80% cheaper than animal products and eliminate FFFF in 2030, and along the way we also \u201cbanned\u201d it. At that point \u201conly\u201d 1 billion insects were farmed for semiconductors, and we didn\u2019t ban it at the same time. Yes, the scale of the problem is still relatively small in 2030 and it might appear to animal advocates that most of the problem of factory farming is solved, except maybe it is not. Who can say for sure that insects will not be the way to make most of the world\u2019s semiconductors (or something that we can't even imagine future generations producing) in the future?<br>&nbsp;</li></ul><p><i><u>Adopting longtermism in animal advocacy</u></i></p><p>I have made a few attempts trying to convince longtermists that animals could be one of the priorities for longtermists, including <a href=\"https://forum.effectivealtruism.org/posts/bfdc3MpsYEfDdvgtP/why-the-expected-numbers-of-farmed-animals-in-the-far-future\">on the EA forum</a>. But I have yet to make major attempts to convince effective animal advocates to adopt longtermism. And these are two separate projects. This post presents an opportunity to do a bit of the latter.</p><p>If we do not think in terms of very long-term (typical longtermists seem to think in terms of at least billions of years, if not trillions. So hundreds of years, or the time when we are very old, is not \"long-term\", not even \"mid-term\"), then it makes sense to just use technology to replace FFFF, because the chance of technology winning it does seem many orders of magnitude higher than moral progress winning it. But if we consider the long-term impacts of our actions, the conclusion can flip. This is because if the worries I wrote in this post are correct, we should be asking ourselves:</p><ul><li>Will eliminating FFFF using technologies increase the chance that other types of factory farming will stay very long (like billion-years-long)?</li><li>Will eliminating FFFF using technologies increase the difficulty of future advocacy for sentient beings who are not animals? (i.e. by making future advocates less confident about humanity)</li><li>Given that we pretty much can't stop PB/CM from advancing even if all effective animal advocates agree to stop our support. What would the timeline for PB/CM be like, if we do stop our share of our support? This question about the counterfactual timeline is important, as it will inform how we plan moral advocacy, or even how much the <i>whole</i> EA movement should prioritize resources and talents.</li><li>I said \"pretty much can't stop\", except, maybe we can? Even though I find this hard to say, and we should indeed be extremely careful in doing things that can be categorized as sabotaging (especially if we will be sabotaging what we do before!), theoretically speaking, it is possible to campaign against what we used to support. In thinking about whether that's worth doing, we should ask ourselves: How long will it take for FFFF to be eliminated solely by moral progress? What's the probability distribution of achieving that? What's the expected value, over the long-term, of doing so?</li><li>And, for people who are also interested in AI risk, having this step in our moral progress <i>before</i> we have AGI seems extremely valuable, and not having it before AGI seems extremely bad). This is because humanity is yet to have a major large-scale shift away from systematic speciesist actions that is not based made based on self-interests, such as our love for cute animals, public health, the environment (yes, I argue that environmentalism is fundamentally human-centric), or economics. This is possibly more important than any other considerations that I wrote in this post.</li></ul><p>I have a sense that quite a number of animal advocates think that we might soon go extinct because of pandemics and climate change, and therefore we don't have hundreds of years beyond, let alone billions or trillions. So in a sense, they are myopic because they think other humans are myopic. But even if you hold that global catastrophes will <i>likely</i> wipe humans out, the extremely long-term future is still worth thinking about if you think the near-term extinction risk is not 0. This is because if there is a future with billions or trillions of years, even a 0.1% chance of humanity living that long will make values and disvalues in the long-term dominate, and therefore should be enough to make you consider the possibility that most of the values of what we do now might be how we affect the future. And you can't justify a view that says \"no, it's literally 0%\", without doing any research.</p><p>&nbsp;</p><p><i><u>Counterarguments</u></i></p><ul><li>Future generations can lie about this part of history and spread a narrative that humanity has eliminated factory farming for moral reasons.<ul><li>Counter-counter: What if the lie is debunked?</li><li>A lie has to be developed by someone, whoever controls that can smuggle in other values that they want to propagate.</li><li>And if society in the future is then structured in a way that doesn\u2019t allow a small group of people to dominate the writing of history, (e.g. no authoritarian government) then how likely can a lie be established?</li><li>If it was a lie, then those good \u201cflow-through impacts\u201d that can only happen from&nbsp;<i>actual</i> moral progress might not exist.</li></ul></li><li>Economic reasons could happen together with moral reasons, giving something like \u201celiminating factory farming for 10% moral reasons, 10% aesthetic reasons, and 80% economic reasons.<ul><li>Counter-counter: But still, that means we can still lose the chance to eliminate it for&nbsp;<i>mostly</i> moral reasons by waiting for technologies to do the job.</li><li>There seems to be a tendency for people, even sometimes historians and philosophers, to claim that there is only one reason for some historical changes as if other factors didn\u2019t contribute. So there\u2019s a chance that the main reason will still be mentioned as the sole reason.</li></ul></li><li>The technology that will replace factory farming is exactly done by people motivated by helping animals!<ul><li>Counter-counter: Some players in the field are clearly not doing it for the animals.</li><li>And it seems that even for those who are motivated by concern for animals, most of them are not 100% doing it for animals only.</li><li>And even if all the PB/CM people are doing it solely for the animals, it\u2019s only&nbsp;<i>them,</i> not all humans. It\u2019s still not moral progress of all humans.</li></ul></li><li>(from Tejas' comment, but Tobias also mentioned it) \"A lot of people\u2019s moral reasoning about animals is posthoc/based on cognitive dissonance. That is, people like eating meat, or it\u2019s a valuable part of their culture, and their moral intuitions around animal exploitation are built around that. So it seems plausible to me that moral advocacy efforts become substantially more effective if we\u2019re able to quickly replace one of the biggest uses of animals. \"<ul><li>Counter-counter: Can't think of. I think the argument is powerful, especially if we consider that we can use this to our advantage to do moral advocacy when 80% of FFFF is replaced. This updated me a lot.</li></ul></li><li>The probability moral progress will eliminate factory farming is too low (even with a longtermist perspective), or is virtually 0, so there is no other choice other than technological advancement. This seems to me to be the best counterargument to my main thesis in the post. I would love to be wrong about this. But I am yet to be convinced otherwise.<ul><li>Counter-counter: Can\u2019t think of. Maybe this counterargument does make my post useless.</li></ul></li></ul>", "user": {"username": "tseyipfai@gmail.com"}}, {"_id": "q2MtTzR4brsJviEDF", "title": "Find out how utilitarian you are - a mega thread of philosophy polls", "postedAt": "2022-09-10T14:01:52.552Z", "htmlBody": "<p>I created a mega thread of philosophy polls to help you find out how utilitarian you are:</p><p>&nbsp;</p><p><a href=\"https://twitter.com/SpencrGreenberg/status/1568595511522852871\">https://twitter.com/SpencrGreenberg/status/1568595511522852871</a></p><p>&nbsp;</p><p>It includes thought experiments related to:</p><ul><li><a href=\"https://twitter.com/SpencrGreenberg/status/1568595515117371393\">Equality</a></li><li><a href=\"https://twitter.com/SpencrGreenberg/status/1568595517172584452\">Delusion</a></li><li><a href=\"https://twitter.com/SpencrGreenberg/status/1568595519303213056\">Preferences</a></li><li><a href=\"https://twitter.com/SpencrGreenberg/status/1568595521459060737\">The Repugnant Conclusion</a></li><li><a href=\"https://twitter.com/SpencrGreenberg/status/1568595523367583745\">Love</a></li><li><a href=\"https://twitter.com/SpencrGreenberg/status/1568595525393334278\">Theft</a></li><li><a href=\"https://twitter.com/SpencrGreenberg/status/1568595527515635714\">Shutting up and multiplying</a></li><li><a href=\"https://twitter.com/SpencrGreenberg/status/1568595529520619522\">Complexity</a></li><li><a href=\"https://twitter.com/SpencrGreenberg/status/1568600394665988097\">Abortion</a></li><li><a href=\"https://twitter.com/SpencrGreenberg/status/1568621304164630533\">Heaven lottery</a></li><li><a href=\"https://twitter.com/SpencrGreenberg/status/1568665823824207873\">Avoiding hell</a></li><li><a href=\"https://twitter.com/SpencrGreenberg/status/1568684191629312001\">Longevity</a></li><li><a href=\"https://twitter.com/SpencrGreenberg/status/1568698804685410304\">Animal suffering</a></li><li><a href=\"https://twitter.com/SpencrGreenberg/status/1568700874402447362\">Replacing beings</a></li></ul>", "user": {"username": "spencerg"}}, {"_id": "W7Ycou9HzQoZecRKg", "title": "Forecasting Newsletter: August 2022.", "postedAt": "2022-09-10T08:59:38.601Z", "htmlBody": "<h2><strong>Highlights</strong></h2><ul><li>CFTC <a href=\"https://comments.cftc.gov/PublicComments/CommentList.aspx?id=7311\">asking for public comments</a> about allowing Kalshi to <a href=\"https://www.youtube.com/watch?v=ygkvXT1BmaA\">phagocytize</a> PredictIt\u2019s niche</li><li>$25k <a href=\"https://news.manifold.markets/p/manifold-markets-forecasting-tournament\">tournament</a> by Richard Hanania on Manifold Markets.</li><li><a href=\"https://www.pastcasting.com/\">pastcasting.com</a> allows users to forecast on already resolved questions with unknown resolutions which hopefully results in faster feedback loops and faster learning</li><li>Hedgehog Markets now have <a href=\"https://amm.hedgehog.markets/\">automatic market-maker</a>-based markets</li><li>Jonas Moss looks at <a href=\"https://forum.effectivealtruism.org/posts/kbZjCF8kmCWgLbzxC/updating-on-the-passage-of-time-and-conditional-prediction\">updating just on the passage of time</a></li></ul><h2><strong>Index</strong></h2><ul><li>Prediction Markets &amp; Forecasting Platforms</li><li>Blog Posts and Research</li><li>In The News</li></ul><p>You can sign up for this newsletter on <a href=\"https://forecasting.substack.com\">Substack</a>, or browse past newsletters <a href=\"https://forum.effectivealtruism.org/s/HXtZvHqsKwtAYP6Y7\">here</a>. If you have a content suggestion or want to reach out, you can leave a comment or find me on <a href=\"https://twitter.com/NunoSempere\">Twitter</a>.</p><h2><strong>Prediction Markets &amp; Forecasting Platforms</strong></h2><h3><strong>Manifold Markets</strong></h3><p>Manifold markets <a href=\"https://news.manifold.markets/p/manifold-markets-forecasting-tournament\">partnered</a> (<a href=\"https://web.archive.org/web/20220909113910/https://news.manifold.markets/p/manifold-markets-forecasting-tournament\">a</a>) with Richard Hanania's <a href=\"https://www.cspicenter.com/p/introducing-the-salemcspi-forecasting\">Center for the Study of Partisanship and Ideology</a> (<a href=\"https://web.archive.org/web/20220909113929/https://www.cspicenter.com/p/introducing-the-salemcspi-forecasting\">a</a>) (some updates <a href=\"https://www.cspicenter.com/p/salem-tournament-5-days-in\">here</a> (<a href=\"http://web.archive.org/web/20220814093212/https://www.cspicenter.com/p/salem-tournament-5-days-in\">a</a>).</p><blockquote><p>The Salem Center at the University of Texas is hiring a new research fellow for the 2023-2024 academic year. This position will pay $25,000 and not require teaching or in-person residency. Rather, it will provide an academic job and financial support for a researcher to do whatever they want with their time, in order to advance their career or work on other projects.</p><p>Unlike a typical fellowship, you will not apply to this one by sending us letters of recommendation and a CV listing all of your publications and awards, and then relying on our subjective judgements about other people\u2019s subjective judgments about your work. Rather, you will participate in a forecasting tournament on economics and social and political issues. At the end of the process, we will interview the top five finalists and decide among them.</p></blockquote><p>Substack now <a href=\"https://news.manifold.markets/p/above-the-fold-anyone-want-a-loan\">supports manifold market embeds</a> (<a href=\"http://web.archive.org/web/20220823055758/https://news.manifold.markets/p/above-the-fold-anyone-want-a-loan\">a</a>), which looks much like the version in the EA Forum. But now, users who are logged into Manifold Markets in the same browser can bet in there directly:</p><figure class=\"media\"><div data-oembed-url=\"https://manifold.markets/BTE/will-a-delaware-judge-order-elon-mu\">\n\t\t\t\t<div class=\"manifold-preview\">\n\t\t\t\t\t<iframe src=\"https://manifold.markets/BTE/will-a-delaware-judge-order-elon-mu\">\n\t\t\t\t</iframe></div>\n\t\t\t</div></figure><h2><strong>Metaculus</strong></h2><p>Nikos Bosse <a href=\"https://www.metaculus.com/notebooks/11162/the-keep-virginia-safe-tournament-202122-project-summary/\">summarizes Metaculus\u2019 Keep Virginia Safe Tournament</a> (<a href=\"http://web.archive.org/web/20220808155107/https://www.metaculus.com/notebooks/11162/the-keep-virginia-safe-tournament-202122-project-summary/\">a</a>). I would have found it interesting to read some speculation about what decisions were changed back at the Virginia Department of Health as a result of this tournament.</p><p>I appreciated the comments on <a href=\"https://www.metaculus.com/questions/5320/chinese-annexation-of-half-of-taiwan-by-2050\">this Metaculus question</a> (<a href=\"http://web.archive.org/web/20220909113953/https://www.metaculus.com/questions/5320/chinese-annexation-of-half-of-taiwan-by-2050/\">a</a>) on China annexing at least half of Taiwan by 2050. Some examples:</p><ul><li><a href=\"https://www.metaculus.com/questions/5320/chinese-annexation-of-half-of-taiwan-by-2050/#comment-99163\">blednotik</a> on how hard Russia is to sanction.</li><li><a href=\"https://www.metaculus.com/questions/5320/chinese-annexation-of-half-of-taiwan-by-2050/#comment-98615\">nextbigfuture</a>: \"Apple CEO Tim Cook, other CEOS and the heads of Vanguard etc... would be on the phone to Biden, Pelosi, Schumer telling them... what are we paying you for. The chips must flow\".</li></ul><p>Metaculus is <a href=\"https://apply.workable.com/metaculus/\">still hiring</a>.</p><h3><strong>Polymarket</strong></h3><p>Polymarket now <a href=\"https://nitter.grimneko.de/PolymarketHQ/status/1555207858630918146\">supports deposits from Coinbase</a> (<a href=\"https://web.archive.org/web/20220909114016/https://nitter.grimneko.de/PolymarketHQ/status/1555207858630918146\">a</a>), and is <a href=\"https://lobster.polymarket.com/market/mlb-who-will-win-atlanta-braves-v-boston-red-sox-scheduled-for-august-9-710-pm-et\">trying out an order book</a> (<a href=\"http://web.archive.org/web/20220909114039/https://lobster.polymarket.com/market/mlb-who-will-win-atlanta-braves-v-boston-red-sox-scheduled-for-august-9-710-pm-et\">a</a>).</p><h3>PredictIt</h3><p>PredictIt CEO goes on <a href=\"https://starspangledgamblers.com/2022/08/08/predictit-ceo-were-not-quitting/\">Star Spangled Gamblers</a> (<a href=\"http://web.archive.org/web/20220809031554/https://starspangledgamblers.com/2022/08/08/predictit-ceo-were-not-quitting/\">a</a>) to give encouragement to the PredictIt community after the CFTC decided to withdraw its no-action letter. PredictIt veteran Domah is <a href=\"https://nitter.it/Domahhhh/status/1556865659799322624\">skeptical</a> (<a href=\"https://web.archive.org/web/20220909114137/https://nitter.it/Domahhhh/status/1556865659799322624\">a</a>).</p><p>Various news media covered the downfall of PredictIt, for example:</p><ul><li><a href=\"https://www.nationalreview.com/2022/08/the-cloudy-future-of-political-futures/amp/\">National Review</a> (<a href=\"http://web.archive.org/web/20220829151106/https://www.nationalreview.com/2022/08/the-cloudy-future-of-political-futures/amp/\">a</a>)</li><li><a href=\"https://slate.com/business/2022/08/predictit-cftc-shut-down-politics-forecasting-gambling.html?via=rss_socialflow_twitter\">Slate</a> (<a href=\"http://web.archive.org/web/20220814230649/https://slate.com/business/2022/08/predictit-cftc-shut-down-politics-forecasting-gambling.html?via=rss_socialflow_twitter\">a</a>)</li><li><a href=\"https://www.bloomberg.com/news/articles/2022-08-06/predictit-betting-on-us-elections-nixed-by-american-regulators#xj4y7vzkg\">Bloomberg</a></li><li><a href=\"https://news.ycombinator.com/item?id=32353795\">Y Combinator</a> (<a href=\"http://web.archive.org/web/20220806042452/https://news.ycombinator.com/item?id=32353795\">a</a>)</li><li><a href=\"https://www.politico.com/news/2022/08/09/no-future-regulator-orders-political-betting-market-to-shut-down-in-u-s-00050238\">Politico</a> (<a href=\"http://web.archive.org/web/20220905183640/https://www.politico.com/news/2022/08/09/no-future-regulator-orders-political-betting-market-to-shut-down-in-u-s-00050238\">a</a>)</li><li><a href=\"https://www.washingtonpost.com/opinions/2022/08/25/predictit-gambling-political-prediction-markets/\">The Washington Post</a> (<a href=\"http://web.archive.org/web/20220827024822/https://www.washingtonpost.com/opinions/2022/08/25/predictit-gambling-political-prediction-markets/\">a</a>)</li></ul><p>In the previous edition of this newsletter, I mentioned that I assigned a 60% chance that Kalshi caused the previous fall from grace of Polymarket, and a 40% chance that they caused PredictIt's demise.&nbsp;</p><p>I\u2019ve gotten some pushback on that, and a <a href=\"https://www.squiggle-language.com/playground/#code=eNqNkl9rgzAUxb%2FKxaeu%2F7TCYBT62Iexl4GvQojtrV6aJi6JDFf63Xct7QojVZ%2BMyfGcX871HLnKfGfN6SRtG629bXB%2B3druyRt73yFNnqTKvhoqS4WZt6TLaB3lurZkrDhK5SoStvGVQueEdOzBK9hAskzBG368%2FhNr2qHQSGVVmMZ20hUsoN8v17mOY%2Fi0ppAFKfItmAO%2F4p52%2Ft1DgcwFR1IK96DoiOBMrluUVqDzslDkKj7ZQJqsmEfJWkmmqB9%2B4vYx08STNEnTRSeFGaQvgfCbmA7wcSUGaRG08XAnhwd5fbN%2Bfv%2FeeqbwnHYs2Qiq0AgHZjzN9SRZvnFH3az7OP8qNM5jZ8oA2tiTVPTDgxMHueOfrksc6Go2ArzLqu9BPaUPRMUhxKB3sLrBep%2F6n4fx1zCkmI%2FhDNgE2rzkOrr8AjRymo0%3D\">simple calculation</a> (<a href=\"https://web.archive.org/web/20220909114211/https://www.squiggle-language.com/playground/#code=eNqNkl9rgzAUxb%2FKxaeu%2F7TCYBT62Iexl4GvQojtrV6aJi6JDFf63Xct7QojVZ%2BMyfGcX871HLnKfGfN6SRtG629bXB%2B3druyRt73yFNnqTKvhoqS4WZt6TLaB3lurZkrDhK5SoStvGVQueEdOzBK9hAskzBG368%2FhNr2qHQSGVVmMZ20hUsoN8v17mOY%2Fi0ppAFKfItmAO%2F4p52%2Ft1DgcwFR1IK96DoiOBMrluUVqDzslDkKj7ZQJqsmEfJWkmmqB9%2B4vYx08STNEnTRSeFGaQvgfCbmA7wcSUGaRG08XAnhwd5fbN%2Bfv%2FeeqbwnHYs2Qiq0AgHZjzN9SRZvnFH3az7OP8qNM5jZ8oA2tiTVPTDgxMHueOfrksc6Go2ArzLqu9BPaUPRMUhxKB3sLrBep%2F6n4fx1zCkmI%2FhDNgE2rzkOrr8AjRymo0%3D\">a</a>) just based on Laplace's law shows that the probability is probably higher.</p><h3><strong>Kalshi</strong></h3><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4f9fbcf0-b1a5-42c1-9735-e91e4e070eff_1332x850.jpeg\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4f9fbcf0-b1a5-42c1-9735-e91e4e070eff_1332x850.jpeg\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4f9fbcf0-b1a5-42c1-9735-e91e4e070eff_1332x850.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4f9fbcf0-b1a5-42c1-9735-e91e4e070eff_1332x850.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4f9fbcf0-b1a5-42c1-9735-e91e4e070eff_1332x850.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4f9fbcf0-b1a5-42c1-9735-e91e4e070eff_1332x850.jpeg 1456w\"></a></p><p>The US CFTC is asking for public comments about <a href=\"https://comments.cftc.gov/PublicComments/CommentList.aspx?id=7311\">allowing Kalshi to host politics prediction markets</a> (<a href=\"http://web.archive.org/web/20220828210656/https://comments.cftc.gov/PublicComments/CommentList.aspx?id=7311\">a</a>). I particularly liked <a href=\"https://comments.cftc.gov/PublicComments/ViewComment.aspx?id=69666&amp;SearchText=\">this comment</a> (<a href=\"https://web.archive.org/web/20220909114108/https://comments.cftc.gov/PublicComments/ViewComment.aspx?id=69666&amp;SearchText=\">a</a>) by a JP Morgan executive, and <a href=\"https://comments.cftc.gov/PublicComments/ViewComment.aspx?id=69617%5C\">this one</a> (<a href=\"https://web.archive.org/web/20220909114122/https://comments.cftc.gov/PublicComments/ViewComment.aspx?id=69617&amp;SearchText=\">a</a>) by a representative of the \"Center for Effective Bribery\". I drafted my own comment, but the CFTC\u2019s website isn\u2019t playing nice: I\u2019ll report back next month.</p><h3>Good Judgment Inc and Good Judgment Open</h3><p>Good Judgment releases a <a href=\"https://goodjudgment.com/wp-content/uploads/2022/08/FF1FF2-Climate-report-final.pdf\">report </a>(<a href=\"https://web.archive.org/web/20220909114223/https://goodjudgment.com/wp-content/uploads/2022/08/FF1FF2-Climate-report-final.pdf\">a</a>) on Superforecasters\u2122's probabilities on various climate risks.</p><p>I appreciated these two comments on Good Judgment Open:</p><ul><li><a href=\"https://www.gjopen.com/comments/1491315\">belikewater</a> (<a href=\"https://web.archive.org/web/20220909114239/https://www.gjopen.com/comments/1491315\">a</a>) considers the <a href=\"https://www.gjopen.com/comments/1491891\">chance</a> (<a href=\"https://web.archive.org/web/20220909114844/https://www.gjopen.com/comments/1491891\">a</a>) of an electrical blackout lasting at least one hour and affecting 60 million or more people in the US and/or Canada before April 2023, and pegs it at 2%.</li><li><a href=\"https://www.gjopen.com/comments/1493360\">orchidny</a> (<a href=\"https://web.archive.org/web/20220909114357/https://www.gjopen.com/comments/1493360\">a</a>) considers the chance that Donald Trump be criminally charged with or indicted for a federal and/or state crime in the US.</li></ul><h3><strong>Odds and ends</strong></h3><p><a href=\"https://www.pastcasting.com/\">pastcasting.com</a> is a new website which allows users to forecast on already resolved questions they don\u2019t have prior knowledge about to get quick feedback. Would recommend!</p><p>Hedgehog Markets now have <a href=\"https://amm.hedgehog.markets/\">automatic market-maker</a>-based markets (<a href=\"http://web.archive.org/web/20220714024403/https://amm.hedgehog.markets/\">a</a>). This is a type of prediction market that I've come to know and love because it moves the game a bit from user vs user to platform vs user. They also have a neat piece that covers <a href=\"https://scribe.citizen4.eu/decentralized-prediction-markets-past-to-present-23ba1c5b186a\">the recent history of prediction markets</a> (<a href=\"https://web.archive.org/web/20220909114417/https://scribe.citizen4.eu/decentralized-prediction-markets-past-to-present-23ba1c5b186a\">a</a>). I\u2019d say that Hedgehog Markets has matured a fair bit since launch, and I would encourage readers to <a href=\"https://hedgehog.markets/\">explore their markets</a> a bit.</p><p>Hypermind has a <a href=\"https://mailchi.mp/hypermind/russia-sanctions-challenge?e=1d84f56c13\">small contest</a> (<a href=\"https://web.archive.org/web/20220909114441/https://us13.campaign-archive.com/?u=f028e1d3668703556410a42ec&amp;id=d69d549943&amp;e=1d84f56c13\">a</a>) on Russian sanctions. In the question creation phase, with a $4k prize pool, participants can propose questions and bet on which questions will be chosen. Then a $10k tournament will take place on the basis of those questions.</p><p><a href=\"https://www.yolorekt.finance\">Yoloreked</a> (<a href=\"https://web.archive.org/web/20220909114503/https://www.yolorekt.finance/\">a</a>) from <a href=\"https://www.urbandictionary.com/define.php?term=Yolo\">YOLO</a> (<a href=\"http://web.archive.org/web/20220901050316/https://www.urbandictionary.com/define.php?term=Yolo\">a</a>) and <a href=\"https://dictionary.cambridge.org/us/dictionary/english/wreck\">wreck</a> (<a href=\"http://web.archive.org/web/20190730205128/https://dictionary.cambridge.org/us/dictionary/english/wreck\">a</a>) is a new crypto prediction market. I\u2019m mentioning it because I find the name funny, but it\u2019s probably on the scammer side.</p><p>A consultant reviews <a href=\"https://jbf-consulting.com/review-of-oracle-transportation-management-platform-otm-roadmap/\">some Oracle products related to forecasting in the transportation industries</a> (<a href=\"http://web.archive.org/web/20220810215248/https://jbf-consulting.com/review-of-oracle-transportation-management-platform-otm-roadmap/\">a</a>). I found this a neat look into that industry; seems much more professionalized.</p><h2><strong>Blog Posts and Research</strong></h2><p>Aleja Cotra\u2014a researcher known for producing a <a href=\"https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines\">Biological Anchors estimate</a> (<a href=\"http://web.archive.org/web/20220806162221/https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines\">a</a>) for the time until AGI\u2014posted a <a href=\"https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines\">two-year update on her personal AI timelines</a> (<a href=\"http://web.archive.org/web/20220906194336/https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines\">a</a>). These have become shorter.</p><p>Friend of the newsletter Eli Lifland has been upping his publishing pace, starting with a <a href=\"https://www.foxy-scout.com/retro/\">Personal forecasting retrospective: 2020-2022</a> (<a href=\"http://web.archive.org/web/20220821081309/https://www.foxy-scout.com/retro/\">a</a>).</p><p><a href=\"https://www.pnas.org/doi/full/10.1073/pnas.1917165118\">Comparing expert elicitation and model-based probabilistic technology cost forecasts for the energy transition</a> (<a href=\"http://web.archive.org/web/20220611150709/https://www.pnas.org/doi/full/10.1073/pnas.1917165118\">a</a>), h/t Dan Carey:</p><blockquote><p>We conduct a systematic comparison of technology cost forecasts produced by expert elicitation methods and model-based methods. Our focus is on energy technologies due to their importance for energy and climate policy&nbsp;</p><p>We show that, overall, model-based forecasting methods outperformed elicitation methods&nbsp;</p><p>However, all methods underestimated technological progress in almost all technologies, likely as a result of structural change across the energy sector due to widespread policies and social and market forces.</p></blockquote><p>Stephanie Losi writes <a href=\"https://riskmusings.substack.com/p/the-silence-of-risk-management-victory\">The Silence of Risk Management Victory</a> (<a href=\"http://web.archive.org/web/20220901154711/https://riskmusings.substack.com/p/the-silence-of-risk-management-victory\">a</a>), giving past examples of scenarios which might have led to catastrophe if not for preventative measures. See also <a href=\"https://en.wikipedia.org/wiki/Preparedness_paradox\">the preparedness paradox</a> (<a href=\"http://web.archive.org/web/20220827083518/https://en.wikipedia.org/wiki/Preparedness_paradox\">a</a>).</p><p><a href=\"https://journals.sagepub.com/doi/pdf/10.1177/1948550618789607\">Social Perception of Forecasters: People See Forecasts of Future Outcomes as Cues to Forecasters\u2019 Desires, Attitudes, and Identity</a>:</p><blockquote><p>While people\u2019s forecasts of future outcomes are often guided by their preferences (\u201cdesirability bias\u201d), it has not been explored yet whether people infer others\u2019 preferences from their forecasts.</p><p>Across 3 experiments and overall 30 judgments, forecasters who thought that a particular future outcome was likely (vs. unlikely) were perceived as having a stronger preference for this outcome</p></blockquote><p>Holden Karnofsky looks at <a href=\"https://forum.effectivealtruism.org/posts/ktEzS3pkfeqPNh6r5/ai-strategy-nearcasting\">AI strategy nearcasting</a> (<a href=\"http://web.archive.org/web/20220908052328/https://forum.effectivealtruism.org/posts/ktEzS3pkfeqPNh6r5/ai-strategy-nearcasting\">a</a>), defined as \"trying to answer key strategic questions about transformative AI, under the assumption that key events (e.g., the development of transformative AI) will happen in a world that is otherwise relatively similar to today's.\"</p><p>The Quantified Uncertainty Research Institute, the NGO for which I work, recently released an \"early access\" version of <a href=\"https://forum.effectivealtruism.org/posts/ZrWuy2oAxa6Yh3eAw/usd1-000-squiggle-experimentation-challenge\">Squiggle</a> (<a href=\"http://web.archive.org/web/20220824162645/https://forum.effectivealtruism.org/posts/ZrWuy2oAxa6Yh3eAw/usd1-000-squiggle-experimentation-challenge\">a</a>), a language for probabilistic estimation. We are also <a href=\"https://quantifieduncertainty.org/careers\">hiring</a> (<a href=\"http://web.archive.org/web/20220907233928/https://quantifieduncertainty.org/careers\">a</a>)!</p><p>Nathan Barnard looks at how <a href=\"https://thegoodblog.substack.com/p/intelligence-failures-and-a-theory\">forecasting could have prevented intelligence failures</a> (<a href=\"http://web.archive.org/web/20220902204310/https://thegoodblog.substack.com/p/intelligence-failures-and-a-theory\">a</a>), speculating that better forecasting would lead to better outcomes by allowing nations to better know when to hold 'em and when to fold 'em. I am sympathetic to the general argument, but a bit uncertain about the extent to which Tetlock-style forecasting could have provided better guidance in the specific historical case studies mentioned, as opposed to on average, across many such cases.</p><p><a href=\"https://www.realclimate.org/index.php/archives/2021/02/dont-climate-bet-against-the-house/\">This blog post</a> (<a href=\"http://web.archive.org/web/20220815134913/https://www.realclimate.org/index.php/archives/2021/02/dont-climate-bet-against-the-house/\">a</a>), via <a href=\"https://statmodeling.stat.columbia.edu/2022/08/11/bets-as-forecasts-bets-as-probability-assessment-difficulty-of-using-bets-in-this-way/\">Stat Modeling</a> (<a href=\"http://web.archive.org/web/20220829033825/https://statmodeling.stat.columbia.edu/2022/08/11/bets-as-forecasts-bets-as-probability-assessment-difficulty-of-using-bets-in-this-way/\">a</a>), covers some recent bets on climate change.</p><p>Nostalgebraist <a href=\"https://nostalgebraist.tumblr.com/post/692086358174498816/idk-who-needs-to-hear-this-but-metaculus-is\">picks a beef</a> (<a href=\"http://web.archive.org/web/20220809164313/https://nostalgebraist.tumblr.com/post/692086358174498816/idk-who-needs-to-hear-this-but-metaculus-is\">a</a>) with <a href=\"https://nostalgebraist.tumblr.com/post/692246981744214016/more-on-metaculus-badness\">Metaculus</a> (<a href=\"http://web.archive.org/web/20220827092332/https://nostalgebraist.tumblr.com/post/692246981744214016/more-on-metaculus-badness\">a</a>).</p><p>Nikos Bosse and Sam Abbott <a href=\"https://forum.effectivealtruism.org/posts/WFbf2d4LHjgvWJCus/cause-exploration-prizes-training-experts-to-be-forecastersThree\">argue that one currently neglected strategy of making forecasting more useful is to focus on making domain experts better forecasters</a> (<a href=\"http://web.archive.org/web/20220826124413/https://forum.effectivealtruism.org/posts/WFbf2d4LHjgvWJCus/cause-exploration-prizes-training-experts-to-be-forecastersThree\">a</a>).</p><p>An article in Nature publishes a <a href=\"https://www.nature.com/articles/s41597-022-01517-w\">standardized and comparable set of short-term forecasts</a> (<a href=\"http://web.archive.org/web/20220820063200/https://www.nature.com/articles/s41597-022-01517-w\">a</a>) on COVID-19 in the US.</p><p>Issues from the Technological Forecasting and Social Change journal can be seen <a href=\"https://www.sciencedirect.com/journal/technological-forecasting-and-social-change/issues\">here</a> (<a href=\"http://web.archive.org/web/20191224192411/https://www.sciencedirect.com/journal/technological-forecasting-and-social-change/issues\">a</a>). I only briefly skimmed it, and I don't particularly expect it to be particularly good, but it's possible it might be of interest to some in the community.</p><h3>Technical content</h3><p>Jonas Moss looks at <a href=\"https://forum.effectivealtruism.org/posts/kbZjCF8kmCWgLbzxC/updating-on-the-passage-of-time-and-conditional-prediction\">updating just on the passage of time</a> (<a href=\"https://web.archive.org/web/20220909114605/https://forum.effectivealtruism.org/posts/kbZjCF8kmCWgLbzxC/updating-on-the-passage-of-time-and-conditional-prediction\">a</a>). In particular, he works out the details for questions about hazard rates, like \"Will Putin stay in power until August 11th 2030?\", and given a few possible forms of the hazard rate (constant, Weibull, <a href=\"https://en.wikipedia.org/wiki/Gompertz%E2%80%93Makeham_law_of_mortality\">Gompertz\u2013Makeham</a> (<a href=\"http://web.archive.org/web/20220830122559/https://en.wikipedia.org/wiki/Gompertz%E2%80%93Makeham_law_of_mortality\">a</a>)).&nbsp;</p><p>I found it amusing that his more complicated Gompertz-Makeham model gave essentially the same answer as a much simpler constant hazard rate model:</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F45dc1c0a-5d61-408f-9866-fee713485347_600x450.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F45dc1c0a-5d61-408f-9866-fee713485347_600x450.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F45dc1c0a-5d61-408f-9866-fee713485347_600x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F45dc1c0a-5d61-408f-9866-fee713485347_600x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F45dc1c0a-5d61-408f-9866-fee713485347_600x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F45dc1c0a-5d61-408f-9866-fee713485347_600x450.png 1456w\"></a></p><p>Ege Erdil presents <a href=\"https://www.lesswrong.com/posts/MFm3A4ihz9s5j2cCo/variational-bayesian-methods\">Variational Bayesian Methods</a> (<a href=\"http://web.archive.org/web/20220903082102/https://www.lesswrong.com/posts/MFm3A4ihz9s5j2cCo/variational-bayesian-methods\">a</a>), and interprets na\u00efve k-means clustering as a Bayesian approximation.</p><p>There was some neat back and forth on continuous prediction market at the <a href=\"https://ethresear.ch/t/continuous-prediction-markets/12993\">Eth Research Forum</a> (<a href=\"http://web.archive.org/web/20220716043542/https://ethresear.ch/t/continuous-prediction-markets/12993\">a</a>). In particular, <a href=\"https://www.bowaggoner.com/\">Bo Wagonner</a> proposes the following <a href=\"https://papers.nips.cc/paper/2015/hash/7af6266cc52234b5aa339b16695f7fc4-Abstract.html\">scheme</a> (<a href=\"http://web.archive.org/web/20210726022240/https://papers.nips.cc/paper/2015/hash/7af6266cc52234b5aa339b16695f7fc4-Abstract.html\">a</a>):</p><blockquote><p>The market maker offers to sell \u201cshares\u201d in any given point on the real line. The payoff of a share drops off with distance between its center point and the actual outcome, in the shape of a Gaussian. E.g. if you bought a share of 500k, and the outcome is 300k, your share pays out something like e^{-(500k - 300k)^2 / 2sigma^2} where sigma is a constant chosen ahead of time.</p></blockquote><p>I think this is ingenious because it allows users to bet in favour or against a distribution, but without having to specify all of it, and while being resilient against small perturbations.</p><h2><strong>In the News</strong></h2><p>Fraser Nelson writes about <a href=\"https://www.spectator.co.uk/article/why-liz-truss-is-right-to-say-forecasts-are-not-destiny-\">fallible forecasts in the UK's recent history</a> (<a href=\"http://web.archive.org/web/20220810102427/https://www.spectator.co.uk/article/why-liz-truss-is-right-to-say-forecasts-are-not-destiny-\">a</a>).</p><p><a href=\"https://www.reuters.com/technology/metas-mark-zuckerberg-companys-pandemic-era-forecast-was-too-rosy-2022-07-29/\">Zuckerberg: Company's pandemic-era forecast was too rosy</a> (<a href=\"http://web.archive.org/web/20220901164537/https://www.reuters.com/technology/metas-mark-zuckerberg-companys-pandemic-era-forecast-was-too-rosy-2022-07-29/\">a</a>). \"Zuckerberg told staffers the world's biggest social media company had planned for growth too optimistically, mistakenly expecting that a bump in usage and revenue growth during COVID-19 lockdowns would be sustained.\"</p><p><a href=\"https://www.bbc.com/news/world-europe-62640051\">Hungary's weather chief sacked over the wrong forecast</a> (<a href=\"http://web.archive.org/web/20220904034607/https://www.bbc.com/news/world-europe-62640051\">a</a>).</p><hr><p>Note to the future: All links are added automatically to the Internet Archive, using this <a href=\"https://github.com/NunoSempere/longNowForMd\">tool</a> (<a href=\"http://web.archive.org/web/20220711161908/https://github.com/NunoSempere/longNowForMd\">a</a>). \"(a)\" for archived links was inspired by <a href=\"https://www.flightfromperfection.com/\">Milan Griffes</a> (<a href=\"http://web.archive.org/web/20220814131834/https://www.flightfromperfection.com/\">a</a>), <a href=\"https://www.andzuck.com/\">Andrew Zuckerman</a> (<a href=\"http://web.archive.org/web/20220316214638/https://www.andzuck.com/\">a</a>), and <a href=\"https://guzey.com/\">Alexey Guzey</a> (<a href=\"http://web.archive.org/web/20220901135024/https://guzey.com/\">a</a>).</p><hr><blockquote><p>When you lose \u2014 and you <i>sure can</i> lose, with N large, you can lose real big. Q.E.D.</p></blockquote><p>Paul Samuelson, in <a href=\"http://www-stat.wharton.upenn.edu/~steele/Courses/434/434Context/Kelly%20Resources/Samuelson1979.pdf\">Why we should not make mean log of wealth big though years to act are long</a> (<a href=\"http://web.archive.org/web/20220113223222/http://www-stat.wharton.upenn.edu/~steele/Courses/434/434Context/Kelly%20Resources/Samuelson1979.pdf\">a</a>), points out that Kelly-betting is not <a href=\"https://en.wikipedia.org/wiki/Stochastic_dominance#Statewise_dominance\">statewise dominant</a> over more risk-averse approaches, and thus its preferability depends on one's risk profile.</p>", "user": {"username": "NunoSempere"}}, {"_id": "cMAZ2mkqX8NqWTKnm", "title": "Tips for living with colleagues (including when not to do it)", "postedAt": "2022-09-10T07:56:24.801Z", "htmlBody": "<p>I was asked a little while back if I had any advice on how to make living with colleagues go well. I\u2019ve lived with many friends and colleagues over the last 10 years, including people I work directly with, people who essentially managed me, and my direct reports. I feel I\u2019ve learned quite a bit over the years, so thought I\u2019d share some of what I\u2019ve learned along the way.&nbsp;</p><p>Living with colleagues is not for everyone, but I\u2019ve gotten a huge amount out of it: I\u2019ve enjoyed living with people I\u2019m close to, formed closer relationships with people and enjoyed being able to talk about work that hugely interests me with others who are similarly passionate. I do think there\u2019s work involved in making this go well - living with colleagues raises the stakes on household and work relationships. This post is just a description of some of the work involved in it. It\u2019s not intended to be a comprehensive list of all the pitfalls of living with colleagues and how to avoid them, and it\u2019s not intended to encourage those who are skeptical about this choice to push ahead through discomfort.</p><p>Quite a bit of the below essentially comes down to best practices for interacting with housemates in general, though some of it is only relevant to living with colleagues. I think the highest stakes relationship to have with a housemate is to be their manager, so this will be particularly relevant to people in that situation. That\u2019s the case where you have the strongest duty of care and need to take some responsibility for the person feeling comfortable and happy in their home rather than feeling under pressure due to always being with their manager. If you\u2019re not sure you\u2019re able to navigate this well, it\u2019s possible that the right answer is to not live with your manager or your direct reports.&nbsp;</p><p>Some of this will also be affected by what housing relationship you\u2019re in. I\u2019ve aimed to write the below to be applicable to any situation including for example co-leasing together. But if you are in the position of being the home owner or lead tenant who is sub-letting then you are more likely taking on the responsibility for making sure this goes well.&nbsp;</p><h2>Deciding whether to do it</h2><p>Living and working together is pretty intense. It makes it hard to take a break from each other, and it inevitably blurs your personal and professional personas (for example, it makes it more likely your colleague will be hearing about your family dramas, and will see you cry or get angry).&nbsp;</p><p>So you likely want to have a reasonably high bar for living and working with the same people. My impression is that standard business advice tends to advise against living with people you work with. (Though it depends a bit on life stage and sector: it seems to be more common for people in graduate schemes and for startup founders to live together.) There are good reasons for that, so it\u2019s important to think carefully about whether to enter situations like this, and be ready to put in additional effort and consideration to make sure they go well.&nbsp;&nbsp;</p><p>In particular, it\u2019s considered best practice to avoid living with people you manage. There are a number of reasons for that. One is that it can be hard to avoid (at least the appearance of) favouritism towards your housemates compared to your other reports. Another is that it may be extremely hard to deal with managing a housemate who is not performing well, especially as their performance becomes bad enough to consider needing to let them go.&nbsp;</p><p>Being effective altruists can also make a difference to colleagues living together.&nbsp;</p><ul><li>On the one hand, it means that we have values and often reasoning styles in common, which makes things easier.&nbsp;</li><li>On the other hand, it can make things more stressful.&nbsp;<ul><li>One reason for that is that EAs often work hard and care a lot about their work, which makes it all the more important that other parts of their life don\u2019t require much thought or work.&nbsp;</li><li>Another is that there are many different areas of life in which people can work hard to help others, and different people care about (or feel able to) optimising in different areas - whether saving money, saving time, recycling or avoiding animal products.&nbsp;</li></ul></li></ul><h2>Before moving in</h2><h3>Figure out whether you\u2019re a fit</h3><p>There are a lot of things that make a difference to how well people live together that you won\u2019t necessarily know about from working together, so you might want to have an explicit conversation about each of your preferences around:&nbsp;</p><ul><li><strong>Tidiness</strong></li><li><strong>How much you want to socialise</strong> - both with each other and inviting others over</li><li><strong>Your style of resolving disagreements</strong>: do you want to talk things through thoroughly? Resolve things concisely via text?</li><li><strong>How much and when you each work</strong>: does one of you work at weekends and the other need full space away from work, including not wanting to be in common spaces with people who are working?</li><li><strong>Moral behaviours</strong>: does one of you find it hard to see others eat animal products and the other eat meat? Does one of you want to save time at all cost and the other think recycling should be non-negotiable?</li><li><strong>Noise - between what times do you each expect the house to be quiet? Does one of you play an instrument, and where will they do that?</strong></li></ul><p>If you\u2019re moving in with a colleague or friend and their partner will also be moving in, don\u2019t forget to talk to them, or at least hear about their preferences too.&nbsp;&nbsp;</p><h3>Try to anticipate problems in advance</h3><p>It might be useful to think through what problems you expect to face in advance. In particular, what stressors might come up? For example - think about how easy it would be for one of you to move out without too long a lead time if living together wasn\u2019t working out.</p><p>Other situations I have faced which made things harder than they would otherwise be include: having a small child, experiencing a miscarriage, and a global pandemic. These kinds of things mean there\u2019s less slack in the system. During those times I was lucky enough to live with people who were really easy going about, for example, how tidy the house was. Had that not been the case I can imagine things could have gotten really stressful because I had little bandwidth to do more housework.&nbsp;</p><p>One thing to bear in mind is that deciding to go back to living apart should be kept as a perfectly reasonable option, not a last resort. Two people can be well suited as friends and colleagues, without being well suited as housemates. Or they can be well suited as housemates at some times but not others. Ideally you would both feel as if you could decide to cease living together without that feeling like it diminishes your friendship or working relationship. For example, one of my colleagues and I decided to stop living together after a few months of doing so for a number of reasons including wanting more work/life separation and different tidiness standards. That felt sad, but didn\u2019t end up impacting our relationship over the longer run at all. (Obviously, financially all this can be easier said than done.)</p><h3>Agree to work norms</h3><p>You might want to explicitly agree to norms when you first move in together about when you\u2019ll discuss work in the house, or even when and where in the house you\u2019ll do work. Some norms that have worked for me in different situations:&nbsp;</p><ul><li><strong>Never discuss work in the house</strong>. This worked well for me with someone with whom I had an uneasy working relationship, and allowed us to live together very happily despite that. It meant we literally avoided questions like \u2018how was your day?\u2019 when we got home from work.&nbsp;</li><li><strong>Only discuss work in one specific room of the house</strong>. In that case, if one of us wanted to initiate a work conversation in the house we had to suggest physically changing locations. That put a naturally high bar to doing so, and reminded each of us to keep separation between work and relaxation.</li><li><strong>Talk about work whenever</strong>. This has been the norm I\u2019ve had most often, but I\u2019ve tried to always proactively suggest other norms and check that all parties are genuinely happy with talking about work whenever.&nbsp;&nbsp;</li></ul><p>Of course, there are plenty of other norms you could imagine, such as talking about work at home only between certain times, or only talking about particular types of work projects. You might also want to discuss other types of norms around work, such as where (and at what times) it\u2019s fine to work. I\u2019m a big fan of covering these topics explicitly in advance rather than one of you feeling increasingly awkward but not knowing how to bring it up.&nbsp;</p><p>These norms are extra important to set in advance if there are power relations involved. For example, you\u2019re likely to feel less comfortable bringing up to your manager that you\u2019d prefer not to see them working in the living room on a Sunday morning than they would a random housemate. Yet, probably it\u2019s actually even more stressful to see your manager working in a common space on a Sunday than another housemate. If you\u2019re the manager in the relationship, you should likely take responsibility for making sure that the norms you\u2019re setting up are ones your direct is happy with, and making sure they don\u2019t feel they ought to say eg that they\u2019re happy to talk about work whenever.</p><p>It could be worth setting up more systems for making things like housework minimally contentious if you all have to work together as well as live together. For example, for a while our house had a points system where we tallied up amount of housework in time increments, so that everyone was on the same page about who was doing more and less than their fair share of housework. This got more comfortable when we got a fortnightly cleaner, because no-one had to do as much housework and the house never got too bad.&nbsp;<br>&nbsp;</p><h2>Once you\u2019ve moved in</h2><p>Once you\u2019ve moved in, you might want to periodically check in about the norms you agreed when you first moved in. It\u2019s likely worth proactively checking in at some time when everyone is feeling good and comfortable with each other, to make sure everyone continues to feel at ease. I\u2019ve typically tried to start off with stricter norms (like only talk about work in a particular room) and then relax them when it turns out we all live and work comfortably together, because it\u2019s less socially awkward for the norms to get looser rather than stricter.&nbsp;</p><p>Even if your explicit house norm is that you can talk about work whenever, if you live with someone you directly manage, you might want to think about how to avoid causing unnecessary pressure on your colleagues. For example:&nbsp;</p><ul><li>You might avoid asking questions that could be interpreted as \u2018how is your work going?\u2019, which could even include questions like \u2018how was your day?\u2019.&nbsp;</li><li>You likely should avoid discussing worries you have about work which could be interpreted as being down to a housemate\u2019s work not going as you/they wanted</li><li>You might consider not working too much in common areas at weekends / late at night. Note that I\u2019m not advocating lying or even misleading people here. But there\u2019s a big difference between knowing in the abstract that your colleague works on a Saturday and getting a bunch of work messages from on Saturday. There\u2019s an even bigger difference between knowing it in theory and them sitting next to you working while you\u2019re trying to relax.&nbsp;</li></ul><p>Of course, for all these things, it really depends on the dispositions of your housemates. I find it stressful to have people around me working when I\u2019m not because I feel guilty. My husband, at the opposite extreme, positively likes it because it makes him feel fortunate he gets to have time off.&nbsp;</p><h2>In the workplace</h2><p>If you\u2019re in a direct management relationship, it seems worth the junior person having a \u2018mentor\u2019 at work. Preferably they would be at a similar level to the manager, but not on the same team. They would meet periodically to check in and ensure that if things were strained that was picked up on quickly and could be figured out as sensitively as possible.&nbsp;</p><p>Living together of course makes it even more important than usual that your working relationship is collaborative. Neither of you can switch off from it, so it\u2019s worth investing extra time and energy making sure that even if a project is hard, you each know you care about each other as people. Of course, this is much easier said than done.&nbsp;</p><p>It could be worth thinking about contingencies for if your relationship comes under strain. For example, it could make it easier to know that you could switch up who manages who if your household relationship is struggling. Or it could be worth taking a bit of time off work to maintain the bandwidth to work things out swiftly and amicably.&nbsp;&nbsp;</p><p>On the other end of things, you probably get on better with your housemates than with most other people, including your other colleagues. You might want to pay explicit attention to whether you\u2019re preferentially discussing projects with or giving responsibilities to a housemate in cases where that doesn\u2019t make sense from a purely work perspective. That seems particularly important when a new person joins the team: It\u2019s hard enough joining a group who already know and trust each other based on working together, let alone if they spend their non-work time together too.</p><h2>A couple of benefits</h2><p>This has overall been a rather negative post, about things that could go wrong. So I wanted to finish with a couple of the unexpected benefits I\u2019ve been lucky enough to get from living with colleagues:&nbsp;</p><ul><li>A colleague and I, who had a somewhat uneasy working relationship, turned out to be extremely compatible housemates and through that lifelong friends.</li><li>When I had a stillbirth, a housemate colleague ensured everyone at work was told sensitively so that I didn\u2019t need to think about who I needed to inform of what and how to do that.</li><li>Short term collaborators who I got to know much better through living together for a summer. That led to finding it easier to work together on later projects.<br>&nbsp;</li></ul><p><i>Thanks for comments from Habiba Islam and Brenton Mayer. Also to the many kind, fun people I\u2019ve gotten to live with over the years.</i></p>", "user": {"username": "Michelle_Hutchinson"}}, {"_id": "g8aBf2oLwDvgd4ovf", "title": "Much EA value comes from being a Schelling point", "postedAt": "2022-09-10T07:26:29.664Z", "htmlBody": "<p><strong>TL;DR</strong>: A significant way in which the EA community creates value is by acting as a <a href=\"https://en.wikipedia.org/wiki/Focal_point_(game_theory)\"><u>Schelling point</u></a> where talented, ambitious, and altruistic people tend to gather and can meet each other (in addition to more direct sources of EA value like identifying the most important problems and directly pushing people to work on them). It might be useful to think about what optimising for being a Schelling point looks like, and I list some vague thoughts on that.</p><hr><p>A Schelling point, also known as a focal point, is what people decide on in the absence of communication, especially when it's important to coordinate by coming to the same answer.</p><p>The classic example is: you were arranging a meeting with a stranger in New York City by telephone, but you used the last minute of your phone credit and the line cut off after you had agreed on the date but not location or time - where do you meet? \"Grand Central Station at noon\" is an answer that other people may be especially likely to converge on.</p><p>(Schelling points can be thought of as a type of acausal negotiation.)</p><h2><strong>When the Schelling point is the selling point</strong></h2><p>Schelling points are often extremely powerful and valuable. A key function of top universities is to be Schelling points for talented people. (Personally, I'd call it the most important function.) There are other valuable things too: courses that go deeper, the signalling value to employers, and so on. However, talented people generally have a preference for hanging out with other talented people, both for social reasons and to find collaborators for ambitious projects and future colleagues. At the same time, talented people are also generally spread out and present only at low densities. Top universities select hard on (some measures of) talent, and through this create environments with high talent density. A big chunk of the reason why people apply to top universities is because other people do so too, and I'd guess that even if the academic standards of Stanford, MIT, or Cambridge eroded significantly, the fact that they've established themselves as congregating points for smart people will keep people applying and visiting for a long time.</p><p>(Note that this is related to, but not equal to, the prestige and status of these places. It is possible to imagine Schelling points that are not prestigious. For example, my impression is that this described MIT at one point - it became a congregating point for uniquely ambitious STEM students and defence research before it achieved high academic status. It is also possible to imagine prestigious places that are not Schelling points, though this is a bit harder since anything with prestige becomes a Schelling point for high social status (though prestige Schelling points and talent Schelling points need not co-occur). More generally, since prestige is a thing many people care a lot about, there is a high correlation between a place being prestigious or high status and being a Schelling point for at least some type of person. However, the mechanisms are distinct - a person selecting their university based on status is selecting based on what they get to write on their CV, while a person selecting their university based on it being a Schelling point for smart people is selecting based on the fact that many other smart people that they can't coordinate with but would like to meet will also choose to go there.)</p><p>Another example is Silicon Valley. Sure, the area has many strengths - being rich and inside a large stable free market - but by far the greatest argument for living in Silicon Valley is that others also choose it. This leads to a (for now) unique combination of entrepreneurial people, great programmers, venture capitalists, and all the other types of people you need for a thriving tech business ecosystem, all there primarily because all the others are there too (how touching!). There's a lot of value of having everything in one place, and it would be very hard for all the different people who make up the value of Silicon Valley to coordinate to move to another place. That's why the Schelling point value of Silicon Valley is so enduring that people continue to tolerate large numbers of homeless drug addicts and sell kidneys to pay rent for years on end.</p><p>Note that a big part of the mechanism isn't that <i>specific</i> people you want to find are there, but that the <i>types of person</i> you'd want to find are likely to also be there, because both those people and yourself are likely to converge on the strategy of going there.</p><h2><strong>Schelling EA</strong></h2><p>The Effective Altruism (EA) community provides a lot of value, for example:</p><ul><li>research into figuring out what are the most important problems to solve to maximise human flourishing;</li><li>research and concrete efforts into how to solve the most important problems discovered by the above;</li><li>high epistemic standards and truth-seeking discussion norms;</li><li>a uniquely wide-ranging and well-reasoned set of resources to help people pursue high-impact careers;</li><li>tens of billions of dollars in funding.</li></ul><p>However, in addition to these, a very critical part of the value that EA provides is being a Schelling point for talented, ambitious, and altruistically-motivated people.</p><p>Even without EA, there would be researchers studying existential risks, animal welfare, and global poverty; people trying to assess charities; communities with high epistemic norms; and billionaires trying to use their fortunes for effective good. However, thanks to EA, people in each of these categories can go to the same Effective Altruism Global conference or quickly find people in local groups, and meet collaborators, co-founders, funders, and so on. A lot of the reason why this can happen is that if you hang out with a certain group of people or on the right websites, EA looms large.</p><p>The biggest <i>personal</i> source of value I've gotten from EA has been having a shortcut to meeting people very high in all of talent, ambition, and altruistic motivation.</p><p>Much of this is obvious - breaking news: communities bring people together and foster connections, more at 11 - but I think taking seriously just how much of counterfactual EA community impact comes from being a Schelling point leads to some less-obvious points about possible implications.</p><h2><strong>Implications</strong></h2><p>The Schelling-point-based (and therefore necessarily incomplete) answer to \"what is the EA community for?\" might be something like \"be an obvious Schelling point where relevant people gather, the chance of interactions that lead to useful work is maximised, and have a community and infrastructure that pushes work in the most useful direction possible\". (This is in contrast to answers that emphasise e.g. directly increasing the number of people working on the most pressing problems.) (I will not argue for this being the best possible answer; my point is just that it is one possible answer, and an interesting one to examine further.)</p><p>If I were a Big Tech marketing consultant, I might call this \"EA-as-a-platform\".</p><p>What might maximising for such a Schelling point strategy look like?</p><h3><strong>Being obvious</strong></h3><p>A Schelling point is not a Schelling point unless it's obvious enough. For EA to be an effective Schelling point for talented/ambitious/altruistic people, those people must hear about it. Silicon Valley is obvious enough that entrepreneurial people from South Africa to Russia hear about it and decide it's where they want to be. To maximise its Schelling point value, EA should have world-spanning levels of recognition.</p><p>Note that recognition does not equal prestige or likeability. We don't care (for Schelling point reasons at least) if most people hear about EA and go \"eh, sounds weird and unappealing\"; what matters is that the core target demographic is excited enough to put effort into pursuing EA. Consider how Silicon Valley was not particularly high-prestige in the public even when it was already attracting tech entrepreneurs, or how many people hear about the intensity of academics at top universities and (very reasonably) think \"no thanks\".</p><h3><strong>Providing value</strong></h3><p>Though most of a Schelling point's value typically comes from the other people who congregate at it, a Schelling point is easier to create if it is obviously valuable. Even though the smart people they meet might be most of the benefit of university, high schoolers are still more likely to go to top universities if they provide good education, good facilities, and unambiguous social status.</p><p>Some obvious ways in which EA provides value are through funding sufficiently promising projects, and by having a very high concentration of intellectually interesting ideas.</p><p>There are risks to communicating loudly about the value-add, since this brings in people who are in it purely for personal gain (<a href=\"https://forum.effectivealtruism.org/posts/W8ii8DyTa5jn8By7H/the-vultures-are-circling\"><u>\"the vultures are circling\", as one Forum post put it</u></a>). This works for Schelling points like Silicon Valley, but not altruism.</p><h3><strong>Optimising for matchmaking</strong></h3><p>A specific way that Schelling points provide value is by making it easy to meet other people in the specific ways that lead to productive teams forming. An existing example of this is that everyone says one-on-one meetings are the main point of conferences, and there is (of course) a lot of <a href=\"https://forum.effectivealtruism.org/posts/pKbTjdopzSEApSQfc/doing-1-on-1s-better-eag-tips-part-ii\"><u>thinking about how to make these effective</u></a>. On the more informal end of the scale, <a href=\"https://www.reciprocity.io/\"><u>Reciprocity</u></a> exists.</p><p>However, the scope and value of EA matchmaking could be expanded. I'm not aware of many ways to match together entrepreneurial teams (the <a href=\"https://www.charityentrepreneurship.com/incubation-program\"><u>Charity Entrepreneurship incubation program</u></a> is the only one that comes to mind). I recently took part in an informally-organised co-founder matching process and found it extremely helpful to quickly get a lot of information on what it's like to work together with several promising people.</p><p>I'd advise for someone to think more about how to make the EA environment even more effective at matching people who should know about each other. However, I expect someone is already designing a 53-parameter one-on-one matching system with Calendly, Slack, and Matplotlib integration for the next conference, and therefore I will hold off on adding any more fuel to this fire.</p><h3><strong>Being legit</strong></h3><p>One of the specific ways in which a Schelling point becomes one is if things associated with it seem uniquely competent, successful, or otherwise good, in a clearly unfakeable way. It is helpful for Cambridge's Schelling point status that it can brag about having 121 Nobel laureates. That so many successful tech companies emerged from Silicon Valley specifically is an unfakeable signal. Any government or city can afford to throw some millions at putting up posters advertising its startup-friendliness; few can consistently produce multi-billion dollar tech companies.</p><p>No amount of community-building or image-crafting is likely to replicate the Schelling point power of <i>obviously being the place where things happen</i>. In some areas, I think EA already has such power: much of the research and work on existential risks happens within EA, and it might be hard to be a researcher on those topics without running into the large body of EA-originating work. However, EA goals require more than just research; note how being a <a href=\"https://80000hours.org/career-reviews/founder-impactful-organisations/\"><u>project/organisation founder</u></a> or <a href=\"https://80000hours.org/articles/operations-management/\"><u>working in an operations role</u></a> have been creeping up the 80 000 Hours list of recommended career paths.</p><p>It would be extremely powerful, not just for direct impact reasons but also for building up EA's Schelling point status, if the EA community clearly spawned very obviously successful real-world projects. <a href=\"https://www.alveavax.com/\"><u>Alvea</u></a> succeeding or working <a href=\"https://forum.effectivealtruism.org/posts/gLPEAFicFBW8BKCnr/announcing-the-nucleic-acid-observatory-project-for-early\"><u>Nucleic Acid Observatories</u></a> being built would be powerful examples. Likewise if <a href=\"https://www.charityentrepreneurship.com/\"><u>Charity Entrepreneurship</u></a>-incubated charities become clear stars of the non-profit world.</p><h3><strong>Meritocracy and impartial judgement</strong></h3><p>Right now, I think if a person somewhere in the world has a well-thought out idea for how to make the world a better place, likely their best bet to get a fair hearing, useful feedback, and - if it is competitive with the most valuable existing projects - funding and support is to post it on the <a href=\"https://forum.effectivealtruism.org/\"><u>EA Forum</u></a>. I don't think this is very obvious outside the EA community. However, this fact, and awareness of it, could make EA a more useful Schelling point, in the same way that the impression that Silicon Valley doesn't frown on weird ideas as long as they're important enough makes it a better Schelling point.</p><p>That EA endorses cause neutrality, has high and transparent epistemic standards, and a quantitative mindset are key parts of this. However, to use this to increase EA Schelling point power, these properties need to be clearly visible to outsiders.</p><p>The most likely way for this to be become more obvious might be if specific EA organisations achieved such a reputation widely within their field (and then there was some path by which knowing of these organisations points people towards knowing about EA).</p><p>GiveWell might be an example of a clearly-EA-linked organisation with visibly high epistemics and judgement quality, though I don't know what their image or recognition level is outside the EA community. Another example is if someone created successful and famous organisations along the lines of FTX Future Fund's proposed <a href=\"https://ftxfuturefund.org/projects/epistemic-appeals-process/\"><u>epistemic appeals process</u></a> or <a href=\"https://ftxfuturefund.org/projects/expert-polling-for-everything/\"><u>widespread expert polling</u></a> projects.</p><h3><strong>Openness and approachability</strong></h3><p>Good Schelling points are easy to enter, and don't select on attributes that they don't have to.</p><p>Every human sub-group, even if loose and purpose-driven, tends to develop a distinctive culture that is much more specific than strictly implied by its purpose. Sometimes this is useful, since it makes it easy for humans in even a loose group to bond with each other. However, a strong and distinct internal culture is also a barrier to entry. EA is already high-risk for having a strong barrier to entry, because</p><ul><li>many arguments and concepts in EA require background knowledge to understand, and sometimes dense philosophical or technical background knowledge (and this is not the case just for more formal things like Forum posts; I've frequently heard \"EV [expected value]\", \"QALY [quality-adjusted life year\", and \"Pascal's mugging\" assumed as obvious common terminology in casual conversation);</li><li>EA (quite obviously, given what it's about) has a high concentration of non-obvious arguments that are obscure in public discussion but have huge implications; and</li><li>perhaps the main route into EA is caring very strongly about intellectual arguments about abstract moral principles, which tends not to be a natural way for humans to join communities.</li></ul><p>These largely unavoidable factors already make EA somewhat unapproachable, and seem like a tightly-knit weird in-group/subculture (anecdotally, this seems to be the most common complaint about EA among Cambridge students). Weird cultural norms or quirks are (among other things!) barriers to entry. Therefore, they should be minimised - to the extent that they can be without impinging on what EA is about - <i>if</i> the goal is to maximise Schelling point value.</p><h3><strong>(Mostly implicit) selectivity for the right things</strong></h3><p>Some selection is usually part of a Schelling point's value. Top universities select for academic merit (though perhaps less so in the US). Silicon Valley selects for openness and interest/talent in tech/business. EA selects for openness, altruistic orientation (especially if consequentialist-leaning), good epistemics, and quantitative thinking.</p><p>I think it is counterproductive to view openness and selectivity as two ends of one scale that apply to everything. You want to select on important features and be open otherwise (note that, when creating a Schelling point, most of the selection is usually implicit - what types of people you attract - rather than explicit filtering). The key choice is not \"open or selective overall?\" but rather \"for which X do we want to appeal only to people who have a value of X in some specific range?\"</p><p>Here's a heuristic for when selectivity for X is useful: when the way X provides value is through its <i>concentration</i> rather than its <i>amount</i>. If you're at a party where you can only talk to a subset of the people during its course, you're going to care a lot about what fraction of people there are interesting - 10 interesting people in a party of 20 is better than 50 in a party of 5000.</p><p>Some cases are ambiguous. For example, if there exists a way for the good and important research to bubble to the top regardless of how much other research exists, it seems like total amount of (infohazard-free) research is the thing to maximise. However, a research area where the average paper is very high quality might help newcomers to the field, or might help lift the prestige of the field, so concentration matters at least somewhat.</p><p>To take another example, there was a <a href=\"https://forum.effectivealtruism.org/posts/dsCTSCbfHWxmAr2ZT/open-ea-global\"><u>recent debate</u></a> over whether EA Global should be open access. Many of the arguments against boil down to thinking the path to impact runs through a uniquely high concentration of EA engagement (or other variables) among the participants; arguments in favour are often either claiming that concentration matters less than sheer amount of interactions, or that the choice of selection variable(s) is wrong, or that CEA fails to select on their chosen selection variable(s) so even if the intention is right the selection variable selected for in practice is wrong.</p><h3><strong>Hubs, and hub-related infrastructure</strong></h3><p>Finally, a key point of a Schelling point is that it is a point <i>somewhere</i>. Here, EA is increasingly better. Berkeley, Cambridge, Oxford, London, and Berlin all have large groups, and offices that you can apply to in order to work on EA-relevant things in the company of other EAs.</p><p>In Schelling point terms, there's also a risk that it might be better to have one really obvious and strong hub than many weaker ones (I've heard some Bay Area EAs in particular endorsing this view; invariably, their hub of choice is the Bay Area, though there is <a href=\"https://forum.effectivealtruism.org/posts/bnzwL6tu4pdYf3hpZ/say-nay-to-the-bay-as-the-default\"><u>push back</u></a>). In practice, it seems that many physical hubs but one virtual/intellectual hub may be best. Both airplanes and people's desires to not uproot their lives are real and relevant things.</p><p>The organisers at each EA hub might benefit from applying Schelling point thinking to the context of their local scene.</p><h3><strong>Being one thing</strong></h3><p>Finally, a Schelling point needs to be one thing, at least in some loose sense. If New York had two Grand Central Stations, the classic Schelling point game would become a lot harder to solve.</p><p>One way to increase the One Thingness of the EA Schelling point is to merge it with other things. In Schelling point land, \"merging\" does not mean making them the same cluster, but rather creating an obvious and visible path from one thing to another. My understanding is that increasing the obviousness of EA in somewhat-adjacent communities (tech, longevity, space, and Emergent Ventures grantees) was a large part of what <a href=\"https://forum.effectivealtruism.org/posts/szeE3je8MD4sZcevL/announcing-future-forum-apply-now\"><u>Future Forum</u></a> tried to achieve.</p><hr><p><i>Thanks to Hugo Eberhard for feedback and discussion on drafts and the general topic.</i></p>", "user": {"username": "LRudL"}}, {"_id": "pomC9SiLkFBWeNZiy", "title": "A model about the effect of total existential risk on career choice", "postedAt": "2022-09-10T07:18:15.878Z", "htmlBody": "<p>Which existential risk cause should you focus on? The cause where you have the largest impact on decreasing <a href=\"https://forum.effectivealtruism.org/topics/total-existential-risk\">total existential risk</a>. That's not the same as working on the cause where you have the largest impact when seen in isolation.</p><h2>Model</h2><p>Suppose there are&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"K\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.04em;\">K</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>existential risks, each with its probability&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p_{k}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span></span></span></span>&nbsp;of ending the world. For each cause&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"k\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span>&nbsp;you can reduce the probability of the world ending from that cause by&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"d_{k}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span></span></span></span>, but only if you spend your whole career doing it.</p><p>For instance, suppose the risks are AI, biorisk, and asteroids. They have associated probabilities&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p_{1}=0.9,p_{2}=0.10\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.9</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.10</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p_{3}=0.01\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">3</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.01</span></span></span></span></span></span></span>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffmn8z1npk54\"><sup><a href=\"#fnfmn8z1npk54\">[1]</a></sup></span>&nbsp;How much could you decrease the probability of extinction for each cause? You're pretty good at deflecting asteroids and killing viruses escaping from labs, but not that good at making humans lovable for AIs. Your probabilities are, say,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"d_{3}=10^{-7}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">3</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">7</span></span></span></span></span></span></span></span></span></span></span>,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"d_{2}=10^{-7}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">7</span></span></span></span></span></span></span></span></span></span></span>,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"d_{3}=5\\cdot10^{-8}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">3</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">8</span></span></span></span></span></span></span></span></span></span></span>.</p><figure class=\"table\"><table><thead><tr><th>Risk type</th><th>Probability (<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p_k\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span></span>)</th><th>Probability reduction (<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"d_k\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span></span>)</th></tr></thead><tbody><tr><td>AI</td><td>0.9</td><td><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"5\\cdot10^{-8}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">8</span></span></span></span></span></span></span></span></span></span></span></td></tr><tr><td>Biorisk</td><td>0.1</td><td><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10^{-7}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">7</span></span></span></span></span></span></span></span></span></span></span></td></tr><tr><td>Asteroids</td><td>0.01</td><td><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10^{-7}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">7</span></span></span></span></span></span></span></span></span></span></span></td></tr></tbody></table></figure><p>Which career should you choose? It sounds plausible that you should be agnostic between the biorisk and asteroids path. That's where you'll reduce the probability of extinction the most, after all. But we should do a decision-theoretic analysis of the problem to make sure.</p><p>Let's use the utility function where <i>the world survives</i> has utility&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>&nbsp;and the <i>world ceases to exist</i> has utility&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span></span>. Let&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"a=(a_{1},\\ldots,a_{K})\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">\u2026</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.04em;\">K</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;be a&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0-1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>&nbsp;vector with&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"a_{k}=1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>&nbsp;if you choose action&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"k\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span></span>&nbsp;otherwise. Then you ought to solve the total utility maximization problem&nbsp;</p><span><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"\\max_{a\\in\\mathcal{A}}\\prod_{k=1}^{K}(1-p_{k}+d_{k}a_{k}).\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-munderover\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">max</span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.308em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">\u2208</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.021em;\">A</span></span></span></span></span></span></span></span></span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-stack\"><span class=\"mjx-over\" style=\"font-size: 70.7%; padding-bottom: 0.258em; padding-top: 0.141em; padding-left: 0.459em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.04em;\">K</span></span></span></span></span><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size2-R\" style=\"padding-top: 0.74em; padding-bottom: 0.74em;\">\u220f</span></span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.004em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span></span><p>&nbsp;Why? Because you don't care which event causes extinction, only that it doesn't happen. And the total probability of no extinction equals&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\prod_{k=1}^{K}(1-p_{k}+d_{k}a_{k})\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-munderover\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size1-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">\u220f</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.31em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.405em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.04em;\">K</span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>.</p><p>Anyway, we can show that</p><ol><li>the optimal action, i.e., career path, is the one with the highest&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"d_{k}/(1-p_{k})\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>;</li><li>the multiplicative improvement you're causing by choosing action&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"k\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span>&nbsp;is&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1+d_{k}/(1-p_{k})\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>.</li></ol><h3>Proof</h3><p>Define&nbsp;</p><span><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"\\Pi = \\prod_{k=1}^{K}(1-p_{k}).\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">\u03a0</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-munderover MJXc-space3\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-stack\"><span class=\"mjx-over\" style=\"font-size: 70.7%; padding-bottom: 0.258em; padding-top: 0.141em; padding-left: 0.459em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.04em;\">K</span></span></span></span></span><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size2-R\" style=\"padding-top: 0.74em; padding-bottom: 0.74em;\">\u220f</span></span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.004em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span></span><p>&nbsp;The utility when taking action&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"k\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span>&nbsp;equals&nbsp;</p><span><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"\\Pi\\frac{1-p_{k}+d_{k}}{1-p_{k}}=\\Pi\\left(1+\\frac{d_{k}}{1-p_{k}}\\right),\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">\u03a0</span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.975em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 4.975em; top: -1.478em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span><span class=\"mjx-denominator\" style=\"width: 4.975em; bottom: -0.95em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.975em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.428em; vertical-align: -0.95em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">\u03a0</span></span><span class=\"mjx-mrow MJXc-space1\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size3-R\" style=\"padding-top: 1.256em; padding-bottom: 1.256em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mfrac MJXc-space2\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 2.829em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 2.829em; top: -1.439em;\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span><span class=\"mjx-denominator\" style=\"width: 2.829em; bottom: -0.95em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 2.829em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.39em; vertical-align: -0.95em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size3-R\" style=\"padding-top: 1.256em; padding-bottom: 1.256em;\">)</span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span></span></span></span></span></span><p>&nbsp;which is clearly maximized in&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"k\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span>&nbsp;that maximises&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"d_{k}/(1-p_{k})\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>.</p><h2>Consequences</h2><p>You need to take both the probability of extinction by cause&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"k\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span>&nbsp;<i>and</i> your ability to reduce the probability into account when you choose your career. If, for instance, the probability of AI ending the world (<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p_{1}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span></span></span></span>) is higher than biorisk ending the world (<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p_{2}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span></span></span></span>), you need to be at least&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\frac{1-p_{1}}{1-p_{2}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 1.725em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 2.439em; top: -1.591em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 2.439em; bottom: -0.883em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 1.725em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.749em; vertical-align: -0.625em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span>&nbsp;times better at biorisk than AI risk (in terms of reducing the probability) to justify working on biorisk. If the probability of bio extinction is&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0.01\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.01</span></span></span></span></span></span></span>&nbsp;and the probability of AI extinction is&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0.90\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.90</span></span></span></span></span></span></span>, you need to be&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0.99/0.10=9.9\\approx10\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.99</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.10</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">9.9</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u2248</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span></span></span></span></span>&nbsp;better at biorisk to justify doing biorisk instead of AI.</p><p>We can expand the table above to include the benefit of taking each action:</p><figure class=\"table\"><table><thead><tr><th>Risk type</th><th>Probability (<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p_k\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span></span>)</th><th>Probability reduction (<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"d_k\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span></span>)</th><th>Benefit (<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\approx\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u2248</span></span></span></span></span></span></span>)</th></tr></thead><tbody><tr><td>AI</td><td>0.9</td><td><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"5\\cdot10^{-8}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">8</span></span></span></span></span></span></span></span></span></span></span></td><td><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1 + 5 \\cdot 10^{-7}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">7</span></span></span></span></span></span></span></span></span></span></span></td></tr><tr><td>Biorisk</td><td>0.1</td><td><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10^{-7}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">7</span></span></span></span></span></span></span></span></span></span></span></td><td><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1 + 1.11\\cdot 10^{-7}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1.11</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">7</span></span></span></span></span></span></span></span></span></span></span></td></tr><tr><td>Asteroids</td><td>0.01</td><td><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10^{-6}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">6</span></span></span></span></span></span></span></span></span></span></span></td><td><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1 + 10^{-7}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">7</span></span></span></span></span></span></span></span></span></span></span></td></tr></tbody></table></figure><p>So, the AI safety career is better than the asteroid career. But not by a lot, as the number&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(1 + 5 \\cdot 10^{-7})/(1 + 10^{-7})\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">7</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">7</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;is virtually indistinguishable from&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>. But of course, a higher number is a higher number, and they do add up. If we only care about the part&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"d_{k}/(1-p_{k})\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>, which might be reasonable, doing the AI career is&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"5\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span></span></span></span></span></span>&nbsp;times better than the asteroids career. Which is more impressive.</p><h2>A model with uncertainty</h2><p>So, you say you have epistemic uncertainty about the probabilities of extinction from each cause? Perhaps you think your choice of entering a fiend may remove the risk entirely, not reduce it by a small number? (E.g., either you solve AI alignment, or you don't).</p><p>That turns out not to matter. For the problem doesn't change much when you allow for uncertainty. Provided&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p_{j},d_{k}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span></span></span></span>,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p_{j},p_{k}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"d_{j},d_{k}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span></span></span></span>&nbsp;are independent when&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"j\\neq k\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">\u2260</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span>&nbsp;we find that</p><span><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"\\begin{eqnarray*} \\text{argmax}_{a\\in\\mathcal{A}}E\\left[\\prod_{k=1}^{K}(1-p_{k}+d_{k}a_{k})\\right] &amp; = &amp; \\text{argmax}_{a\\in\\mathcal{A}}\\prod_{k=1}^{K}E\\left[1-p_{k}+d_{k}a_{k}\\right],\\\\ &amp; = &amp; \\text{argmax}_{a\\in\\mathcal{A}}\\Pi\\frac{E(1-p_{k})+E(d_{k})a_{k}}{E(1-p_{k})},\\\\ &amp; = &amp; \\text{argmax}_{a\\in\\mathcal{A}}\\Pi\\left[1+\\frac{E(d_{k})a_{k}}{1-E(p_{k})}\\right], \\end{eqnarray*}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mtable\" style=\"vertical-align: -4.44em; padding: 0px 0.167em;\"><span class=\"mjx-table\"><span class=\"mjx-mtr\" style=\"height: 3.201em;\"><span class=\"mjx-mtd\" style=\"padding: 0px 0px 0px 0px; text-align: right; width: 14.574em;\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.519em;\">argmax</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.377em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">\u2208</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.021em;\">A</span></span></span></span></span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mrow MJXc-space1\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size4-R\" style=\"padding-top: 1.551em; padding-bottom: 1.551em;\">[</span></span><span class=\"mjx-munderover\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-stack\"><span class=\"mjx-over\" style=\"font-size: 70.7%; padding-bottom: 0.258em; padding-top: 0.141em; padding-left: 0.459em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.04em;\">K</span></span></span></span></span><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size2-R\" style=\"padding-top: 0.74em; padding-bottom: 0.74em;\">\u220f</span></span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.004em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size4-R\" style=\"padding-top: 1.551em; padding-bottom: 1.551em;\">]</span></span></span><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0px 0.139em 0px 0px; width: 1.056em;\"><span class=\"mjx-mrow\" style=\"margin-top: 0.776em;\"><span class=\"mjx-mi\"></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0px 0px 0px 0.139em; text-align: left; width: 14.973em;\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.519em;\">argmax</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.377em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">\u2208</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.021em;\">A</span></span></span></span></span></span></span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-stack\"><span class=\"mjx-over\" style=\"font-size: 70.7%; padding-bottom: 0.258em; padding-top: 0.141em; padding-left: 0.459em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.04em;\">K</span></span></span></span></span><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size2-R\" style=\"padding-top: 0.74em; padding-bottom: 0.74em;\">\u220f</span></span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.004em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mrow MJXc-space1\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-strut\"></span></span></span></span><span class=\"mjx-mtr\" style=\"height: 2.98em;\"><span class=\"mjx-mtd\" style=\"padding: 0.15em 0px 0px 0px; text-align: right;\"><span class=\"mjx-mrow\" style=\"margin-top: 0.59em;\"><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0.15em 0.139em 0px 0px;\"><span class=\"mjx-mrow\" style=\"margin-top: 0.59em;\"><span class=\"mjx-mi\"></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0.15em 0px 0px 0.139em; text-align: left;\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.519em;\">argmax</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.377em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">\u2208</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.021em;\">A</span></span></span></span></span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">\u03a0</span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 8.992em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 8.992em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span><span class=\"mjx-denominator\" style=\"width: 8.992em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 8.992em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-strut\"></span></span></span></span><span class=\"mjx-mtr\" style=\"height: 3.199em;\"><span class=\"mjx-mtd\" style=\"padding: 0.15em 0px 0px 0px; text-align: right;\"><span class=\"mjx-mrow\" style=\"margin-top: 0.775em;\"><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0.15em 0.139em 0px 0px;\"><span class=\"mjx-mrow\" style=\"margin-top: 0.775em;\"><span class=\"mjx-mi\"></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0.15em 0px 0px 0.139em; text-align: left;\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.519em;\">argmax</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.377em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">\u2208</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.021em;\">A</span></span></span></span></span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">\u03a0</span></span><span class=\"mjx-mrow MJXc-space1\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size4-R\" style=\"padding-top: 1.551em; padding-bottom: 1.551em;\">[</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mfrac MJXc-space2\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.371em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 4.371em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span><span class=\"mjx-denominator\" style=\"width: 4.371em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.371em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size4-R\" style=\"padding-top: 1.551em; padding-bottom: 1.551em;\">]</span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-strut\"></span></span></span></span></span></span></span></span></span></span></span><p>where&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Pi=\\prod_{k=1}^K E(1-p_k).\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">\u03a0</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-munderover MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size1-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">\u220f</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.31em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.405em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.04em;\">K</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span></span>&nbsp;The problem is maximized in the&nbsp;action with the highest&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"E[d_{k}]/(1-E[p_{k}]).\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span></span></p><h2>Footnotes</h2><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfmn8z1npk54\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffmn8z1npk54\">^</a></strong></sup></span><div class=\"footnote-content\"><p>These probabilities sum to more than&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>, but that doesn't matter for our purposes. Think about them as the probabilities of independent events and the event \"the world ends\" as an event that happens if at least one of them occurs.</p></div></li></ol>", "user": {"username": "Jonas Moss"}}, {"_id": "AgQx44vtw5bi4LmBh", "title": "Puzzles for Everyone", "postedAt": "2022-09-10T02:11:50.674Z", "htmlBody": "<p>Some of the deepest puzzles in ethics concern how to coherently extend ordinary <i>beneficence</i> and <i>decision theory </i>to extreme cases. The notorious puzzles of <a href=\"https://www.utilitarianism.net/population-ethics\">population ethics</a>, for example, ask us how to trade off quantity and quality of life, and how we should value future generations. Beckstead &amp; Thomas discuss <a href=\"https://globalprioritiesinstitute.org/nick-beckstead-and-teruji-thomas-a-paradox-for-tiny-probabilities-and-enormous-values/\">a paradox for tiny probabilities and enormous values</a>, asking how we should take risk and uncertainty into account. <a href=\"https://handsandcities.com/2022/01/30/on-infinite-ethics/\">Infinite ethics</a> raises problems for both axiology and decision theory: it may be unclear how to rank different infinite outcomes, and it\u2019s hard to avoid the \u201cfanatical\u201d result that the tiniest chance of infinite value swamps all finite considerations (unless one embraces alternative commitments that may be even more counterintuitive).</p><p>Puzzles galore! But these puzzles share a strange feature, namely, that people often mistakenly believe them to be problems <i>specifically for utilitarianism</i>.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F05e91819-c21a-41e2-a46f-d043b226ef84_1024x1024.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F05e91819-c21a-41e2-a46f-d043b226ef84_1024x1024.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F05e91819-c21a-41e2-a46f-d043b226ef84_1024x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F05e91819-c21a-41e2-a46f-d043b226ef84_1024x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F05e91819-c21a-41e2-a46f-d043b226ef84_1024x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F05e91819-c21a-41e2-a46f-d043b226ef84_1024x1024.png 1456w\"></a></p><p>&nbsp;</p><p>[Image caption: \"Fear not: there\u2019s enough for everyone!\"]</p><p>Their error, of course, is that <i>beneficence</i> and <i>decision theory</i> are essential components of <i>any</i> complete moral theory. (As even Rawls acknowledged, \u201cAll ethical doctrines worth our attention take consequences into account in judging rightness. One which did not would simply be irrational, crazy.\u201d Rossian pluralism explicitly acknowledges a prima facie <i>duty of beneficence</i> that must be weighed against our other\u2014more distinctively deontological\u2014prima facie duties, and will determine what ought to be done if those others are not applicable to the situation at hand. And obviously any account relevant to fallible human beings needs to address how we should respond to uncertainty about our empirical circumstances and future prospects.)</p><p>Why, then, would anyone ever think that these puzzles were limited to utilitarianism? One hypothesis is that only utilitarianism is sufficiently clear and systematic to actually <i>attempt an answer</i> to these questions. Other theories too often remain silent and non-committal. Being incomplete in this way is surely not an <i>advantage</i> of those theories, unless there\u2019s reason to think that a better answer will eventually be fleshed out. But what makes these questions such deep puzzles is precisely that we know that no wholly satisfying answer is possible. It\u2019s a \u201cpick your poison\u201d situation. And there\u2019s nothing clever about mocking utilitarians for endorsing a poisonous implication when it\u2019s provably the case that <i>every possibility remaining</i> amongst the non-utilitarian options is similarly poisonous!</p><p>When all views have costs, you cannot refute a view just by pointing to one of its costs. You need to actually gesture towards <i>a better alternative</i>, and do the difficult work of determining which view is the <i>least bad</i>. Below I\u2019ll briefly step through some basic considerations that bring out how difficult this task can be.</p><h3>Population Ethics</h3><p>In \u2018<a href=\"https://bostonreview.net/articles/the-new-moral-mathematics/\">The New Moral Mathematics</a>\u2019 (reviewing <i>WWOTF</i>), Kieran Setiya sets up a false choice between total utilitarianism and \u201cthe intuition of neutrality\u201d which denies positive value to creating happy lives. (Note that <a href=\"https://rychappell.substack.com/p/review-of-what-we-owe-the-future\">MacAskill\u2019s longtermism is in fact much weaker than total utilitarianism</a>.) He swiftly dismisses the total view for implying <a href=\"https://www.utilitarianism.net/population-ethics#objecting-to-the-total-view\">the repugnant conclusion</a>. But he doesn\u2019t mention any costs to neutralism, which may give some readers the misleading impression that this is a cost-free, common-sense solution. It isn\u2019t. <a href=\"https://www.utilitarianism.net/population-ethics#person-affecting-views-and-the-procreative-asymmetry\">Far from it</a>.</p><p>Neutrality implies that utopia is (in prospect) no better than a barren, lifeless rock. It implies that the total extinction of all future value-bearers could be more than compensated for by throwing a good enough party for those who already exist. These implications strike me as far more repugnant than the repugnant conclusion. (If you think the big party doesn\u2019t sound so bad, given that you\u2019re already invited, instead imagine Cleopatra making the decision millennia ago.) Moreover, neutrality doesn\u2019t even fully avoid the original problem! It still doesn\u2019t imply that future utopia A is <i>better</i> than the repugnant world Z; just that they are \u201con a par\u201d. (This is a result that totalists can just as well secure through a more limited <a href=\"https://www.utilitarianism.net/population-ethics#critical-level-and-critical-range-theories\">critical range</a> that still allows awesome lives to qualify as positive additions to the world.)</p><p>To fully avoid repugnance, we want a population axiology that can <i>at least </i>deliver both of the following verdicts:</p><p>(i) utopia (world A) is better than Parfit\u2019s world Z, and</p><p>(ii) utopia is better than a barren rock.</p><p>The total view can\u2019t secure (i), but at least it\u2019s got (ii) covered. Neutrality gets us neither! (The only hope for both, I think, is some kind of <a href=\"https://www.utilitarianism.net/population-ethics#variable-value-theories\">variable value view</a>, or possibly <a href=\"https://rychappell.substack.com/p/the-nietzschean-challenge-to-effective\">perfectionism</a>, both of which allow that we have strong moral reasons to want more awesome, excellent lives to come into existence.)</p><p>To bring out just how little is gained by neutrality, note that all the same puzzles re-emerge when trading off quantity and quality <i>within a single life</i>, where neutrality is clearly not an option. (The intrapersonal \u201cneutral\u201d view would hold that early death is harmless, and adding extra good time to your life\u2014<i>however wonderful that time might be\u2014</i>is strictly \u201con a par\u201d with never having that time at all. Assuming that you\u2019d prefer to experience bliss than instant death, you already reject the \u201cintuition of neutrality\u201d in <i>this</i> domain!)</p><p>Consider the intrapersonal repugnant conclusion: A life contain zillions of barely-positive drab moments is allegedly <i>better for you</i> than a century in utopia. Seems wrong! So how are you going to avoid it? <strong>Not by appealing to neutrality</strong>, for the reasons we\u2019ve just seen. An intrapersonal analogue of <a href=\"https://www.utilitarianism.net/population-ethics#variable-value-theories\">variable value</a> or <a href=\"https://www.utilitarianism.net/population-ethics#critical-level-and-critical-range-theories\">critical range</a> views is surely more promising, though these views have their own significant costs and limitations (follow the links for details). Still, if you settle on a view that works to avoid the intrapersonal repugnant conclusion, why not carry it over to the inter-personal (population) case, if you\u2019re also concerned to avoid the repugnant conclusion there?</p><p>Once you acknowledge that (i) the intrapersonal repugnant conclusion is just as counterintuitive as the inter-personal one, and yet (ii) unrestricted \u201cneutrality\u201d about creating new moments of immense value is not a feasible option, it becomes clear that neutrality about creating happy lives is no panacea for the puzzles of population ethics. Either we make our peace with some form of the repugnant conclusion, <i>or </i>we settle on an alternative account that\u2019s <i>nonetheless compatible with ascribing value to creating new loci of value</i> (times or lives) at least when they are <i>sufficiently</i> good. Folks who think neutrality offers an acceptable general solution here are deluding themselves.</p><h3>Decision Theory</h3><p>In an especially striking example of conflating <i>utilitarianism</i> with <i>anything remotely approaching systematic thinking</i>, popular substacker Erik Hoel <a href=\"https://erikhoel.substack.com/p/why-i-am-not-an-effective-altruist\">recently characterized</a> the <a href=\"https://globalprioritiesinstitute.org/nick-beckstead-and-teruji-thomas-a-paradox-for-tiny-probabilities-and-enormous-values/\">Beckstead &amp; Thomas paper</a> on decision-theoretic paradoxes as addressing \u201chow poorly utilitarianism does in extreme scenarios of low probability but high impact payoffs.\u201d Compare this with the very first sentence of the paper\u2019s abstract: \u201cWe show that <strong>every theory</strong> of the value of uncertain prospects must have one of three unpalatable properties.\u201d Not utilitarianism. <i><strong>Every</strong></i> theory.</p><p>(Alas, when I tried to point this out in the comments section, after a brief back-and-forth in which Erik initially doubled down on the conflation, he abruptly decided to instead <i>delete</i> my comments explaining his mistake.)</p><p>Just to briefly indicate the horns of the paradox: in order to avoid the \u201crecklessness\u201d of orthodox (risk-neutral) expected utility in the face of tiny chances of enormous payoffs, you must either <i>endorse</i> <i>timidity</i> or <i>reject transitivity</i>. Timidity \u201cpermit[s] passing up arbitrarily great gains to prevent a tiny increase in risk.\u201d (<a href=\"https://forum.effectivealtruism.org/posts/xAoZotkzcY5mvmXFY/longtermism-risk-and-extinction?commentId=sFWnxyJDkWQDSDeyp\">Relatedly</a>: risk-averse views may imply that we should prefer to destroy the world rather than risk a <i>1 in 10 million</i> chance of a dystopian future, even on the assumption that a correspondingly wonderful utopia is vastly more likely to otherwise eventuate.) Doesn\u2019t sound great! And rejecting transitivity strikes me as basically just giving up on the project of coherently systematizing how we should respond to uncertain prospects; I don\u2019t view that as an acceptable option at all.</p><h3>Conclusion</h3><p>It\u2019s really not easy for <i>anyone</i> to avoid uncomfortable verdicts in these puzzle cases. However bad the \u201cutilitarian\u201d verdict looks at first blush, a closer examination suggests that many alternatives are likely to be significantly <i>worse</i>. (When discussing related issues in \u2018<a href=\"https://rychappell.substack.com/p/double-or-nothing-existence-gambles\">Double or Nothing Existence Gambles</a>\u2019, I suggest that a moderate mix of partiality and diminishing marginal value of intrinsic goods might help in at least some cases. But it\u2019s really far from obvious how best to deal with these problems!)</p><p>Most of those who are most confident that the orthodox utilitarian answers are absurd haven\u2019t actually thought through any sort of systematic alternative, so their confidence seems severely misplaced. Personally, I remain <i>hopeful</i> that both the Repugnant Conclusion and (at least some) reckless \u2018Double or Nothing\u2019 existence gambles can be avoided with appropriate tweaks to our axiology. But I\u2019m far from confident: these puzzles are really tricky, and the options all have severe costs! Non-consequentialists may superficially <i>look</i> better by refusing to even talk about the problems, so\u2014like skilled politicians\u2014they cannot so easily be pinned down. But <strong>gaps in a theory shouldn\u2019t be mistaken for solutions</strong>. It\u2019s important to appreciate that any coherent completion of their view will likely end up looking just as bad\u2014or worse.</p><p>As a result, I think many people who (like Erik Hoel) <i>think</i> they are opposed to utilitarianism are <i>really </i>reacting against a broader phenomenon, namely, <i>systematic theorizing</i>. The only way to entirely avoid the problems they deem so sneer-worthy is to <i>stop thinking</i>. Personally, I just can\u2019t shake the feeling that that would be the most repugnant response of all.</p>", "user": {"username": "RYC"}}, {"_id": "z77ocer2Bk3S2PnhC", "title": "Reminder (Sept 15th deadline): Apply for the Open Philanthropy Technology Policy Fellowship", "postedAt": "2022-09-09T18:58:54.020Z", "htmlBody": "<p>The deadline for the&nbsp;<a href=\"https://www.openphilanthropy.org/open-philanthropy-technology-policy-fellowship/\"><u>Open Philanthropy Technology Policy Fellowship</u></a> (OPTPF) is September 15th. If you are interested in working in US policy on topics such as AI and biosecurity, we strongly encourage you to consider&nbsp;<a href=\"https://jobs.ashbyhq.com/techpolicyfellowship/6c86c108-e547-45c0-a918-f923bc26be82/application\"><u>applying</u></a>!&nbsp;</p><p><br>The&nbsp;<a href=\"https://www.openphilanthropy.org/the-open-philanthropy-technology-policy-fellowship-2022-cohort/\"><u>first cohort of 15 OPTPF fellows</u></a> received 10 weeks of policy training sessions together, and 100% of them matched with a host organization. They have now started placements in the executive branch (in the Departments of Defense, Health and Human Services, and Homeland Security), congressional offices, and think tanks (CSET, NTI, CHS, CSIS, Brookings, CDT, and CEIP).</p><p><br>To learn more about the program, you can check out&nbsp;<a href=\"https://www.openphilanthropy.org/open-philanthropy-technology-policy-fellowship/\"><u>the fellowship page</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/p5qtmu7KfGKaa6Ydt/apply-to-the-open-philanthropy-technology-policy-fellowship\"><u>this past Forum post</u></a>, and&nbsp;<a href=\"https://docs.google.com/document/d/1M7kgTZN5nC3eOnUjkr8Qp2d9gfJEeRU3i1atMaXfRgA/edit\"><u>the fellowship FAQ</u></a>.&nbsp;&nbsp;</p>", "user": {"username": "JoanGass"}}, {"_id": "XBHx9zhAtkiBJnZNu", "title": "Cause Exploration Prizes: Announcing our prizes", "postedAt": "2022-09-09T13:47:31.064Z", "htmlBody": "<p>We were gratified to receive over 150 <a href=\"https://www.causeexplorationprizes.com/rules-faqs\"><u>good-faith submissions</u></a> to Open Philanthropy\u2019s <a href=\"https://www.causeexplorationprizes.com/\"><u>Cause Exploration Prizes</u></a>, where we invited people to suggest a new area for us to support or respond to our suggested questions. We hoped that these submissions would help us find new ways to carry out our mission \u2014 helping others as much as possible with the resources available to us.&nbsp;&nbsp;</p><p>You can <a href=\"https://forum.effectivealtruism.org/topics/cause-exploration-prizes\"><u>read them on the EA Forum</u></a>. Below, we highlight the submissions to which we are awarding major prizes and honorable mentions.</p><p>We\u2019re awarding these prizes to entries that we thought engaged well with our prompts and helped us to better understand the questions and issues they addressed. We have not investigated each and every claim made in these entries, and the awarding of a prize does not imply that we necessarily endorse their claims or arguments as correct.</p><h3>Our top prize</h3><p>We are awarding our top prize ($25,000) to: <a href=\"https://forum.effectivealtruism.org/posts/LSDZ22GFryC3dhWvd/cause-exploration-prize-organophosphate-pesticides-and-other\"><u>Organophosphate pesticides and other neurotoxicants</u></a> by <a href=\"https://forum.effectivealtruism.org/users/ben-stewart\"><u>Ben Stewart</u></a>.</p><h3>Second prizes</h3><p>We are awarding three second-place prizes ($15,000) for the following submissions. These are listed in no particular order.</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/majcwf7i8pW8eMJ3v/new-cause-area-violence-against-women-and-girls\"><u>Violence against women and girls</u></a> by <a href=\"https://forum.effectivealtruism.org/users/akhil-bansal-1\"><u>Akhil</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/rvACMsMmmDonNhskD/cause-exploration-prizes-sickle-cell-disease\"><u>Sickle Cell Disease</u></a> by anonymous</li><li><a href=\"https://forum.effectivealtruism.org/posts/fqf4vgCWebTszvHm9/shareholder-activism\"><u>Shareholder activism</u></a> by <a href=\"https://forum.effectivealtruism.org/users/sbehmer\"><u>sbehmer</u></a></li></ul><h3>Honorable mentions</h3><p>We are awarding $500 to the authors of the following entries. These are listed in no particular order.&nbsp;</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/WAnJw5bhuQwhJiLTm/cause-exploration-prizes-expanding-access-to-infertility\"><u>Expanding access to infertility services in Low- and Middle-Income Countries</u></a> by <a href=\"https://forum.effectivealtruism.org/users/sscotney\"><u>Soleine Scotney</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/pxrN28gfn26Z7dzch/new-cause-area-maternal-morbidity\"><u>Maternal morbidity</u></a> by <a href=\"https://forum.effectivealtruism.org/users/alexhill\"><u>alexhill</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/a4sWmWNufYvAjimNg/cause-exploration-prizes-farmed-animal-welfare-in-sub\"><u>Farmed Animal Welfare in Sub-Saharan Africa</u></a> by anonymous</li><li><a href=\"https://forum.effectivealtruism.org/posts/j4eTj9dGqzEC5LEzK/cause-exploration-prizes-indoor-air-quality-to-reduce\"><u>Indoor Air Quality to Reduce Infectious Respiratory Disease</u></a> by <a href=\"https://forum.effectivealtruism.org/users/gavriel-kleinwaks\"><u>Gavriel Kleinwaks</u></a>, <a href=\"https://forum.effectivealtruism.org/users/alastair-fraser-urquhart\"><u>Alastair Fraser-Urquhart</u></a>, and <a href=\"https://forum.effectivealtruism.org/users/joshcmorrison\"><u>joshcmorrison</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/dk48Sn6hpbMWeJo4G/to-wellby-or-not-to-wellby-measuring-non-health-non\"><u>To WELLBY or not to WELLBY? Measuring non-health, non-pecuniary benefits using subjective wellbeing</u></a> by <a href=\"https://forum.effectivealtruism.org/users/joelmcguire\"><u>JoelMcGuire</u></a>, <a href=\"https://forum.effectivealtruism.org/users/samuel-dupret-1\"><u>Samuel Dupret</u></a>, and <a href=\"https://forum.effectivealtruism.org/users/michaelplant\"><u>MichaelPlant</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/bQh82m2zr3enC9vK9/cause-exploration-tobacco-harm-reduction\"><u>Tobacco harm reduction</u></a> by <a href=\"https://forum.effectivealtruism.org/users/kristof\"><u>kristof</u></a>&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/posts/nE827LwrRk5ep3Xao/cause-area-climate-adaptation-in-low-income-countries\"><u>Climate adaptation in low-income countries</u></a> by <a href=\"https://forum.effectivealtruism.org/users/karthik-tadepalli\"><u>Karthik Tadepalli</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/var4x7y3TfeAYgLH5/cause-exploration-prizes-social-and-behavioral-science-r-and\"><u>Social and behavioral science R&amp;D</u></a> by <a href=\"https://forum.effectivealtruism.org/users/anna-harvey\"><u>Anna Harvey</u></a> and <a href=\"https://forum.effectivealtruism.org/users/stuart-buck-1\"><u>Stuart Buck</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/zgBmSgyWECJcbhmpc/family-planning-a-significant-opportunity-for-impact\"><u>Family Planning: A Significant Opportunity for Impact</u></a> by <a href=\"https://forum.effectivealtruism.org/users/sarah-h\"><u>Sarah H</u></a> and <a href=\"https://forum.effectivealtruism.org/users/ben-williamson\"><u>Ben Williamson</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/ft7ChZnzyq2SwsgBa/new-cause-area-improving-diagnosis-and-treatment-of-bipolar\"><u>Improving diagnosis and treatment of bipolar spectrum disorders</u></a> by <a href=\"https://forum.effectivealtruism.org/users/karolina-soltys-1\"><u>Karolina Soltys</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/E2BghQq9pwPgtHgiH/war-between-the-us-and-china-a-case-study-for-epistemic\"><u>War between the US and China: A case study for epistemic challenges around China-related catastrophic risk</u></a> by <a href=\"https://forum.effectivealtruism.org/users/jordan_schneider\"><u>Jordan_Schneider</u></a> and <a href=\"https://forum.effectivealtruism.org/users/pradyusp\"><u>pradyusp</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/fvN3DF5LBkCRiEDh5/cause-exploration-prizes-improved-quality-control-in-science\"><u>Improved quality control in science</u></a> by <a href=\"https://forum.effectivealtruism.org/users/sophie-schauman\"><u>Sophie Schauman</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/sCQBJhcNZ4Ye36Cni/cause-exploration-prizes-more-animal-advocacy-r-and-d\"><u>More animal advocacy R&amp;D</u></a> by anonymous</li><li><a href=\"https://forum.effectivealtruism.org/posts/g7Rn2fiq52QN8pbJa/cause-exploration-adapting-to-extreme-heat-exposure-in-south\"><u>Adapting to Extreme Heat Exposure in South Asia</u></a> by <a href=\"https://forum.effectivealtruism.org/users/surbhi-b\"><u>Surbhi B</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/iZagToiirYgqiX3XR/cause-exploration-prizes-rich-to-poor-country-spillovers\"><u>Rich to Poor Country Spillovers</u></a> by anonymous</li><li><a href=\"https://forum.effectivealtruism.org/posts/TMjRuTLjQa6z6rdeY/large-scale-international-educational-migration-a-shallow\"><u>Large-scale International Educational Migration: A shallow investigation</u></a> by <a href=\"https://eahub.org/profile/jasmin-baier/\"><u>Jasmin Baier</u></a>, <a href=\"https://haushofer.ne.su.se/\"><u>Johannes Haushofer</u></a>, and Hannah Lea Shaw</li><li><a href=\"https://forum.effectivealtruism.org/posts/B7wohgDDdwPoQAatt/new-cause-area-training-health-workers-to-prevent-newborn\"><u>Training health workers to prevent newborn deaths</u></a> by <a href=\"https://forum.effectivealtruism.org/users/mpt7\"><u>Marshall</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/m2tgGev5XpkTkvERL/cause-area-developmental-cognitive-neuroepidemiology-1\"><u>Developmental Cognitive Neuroepidemiology</u></a> by <a href=\"https://forum.effectivealtruism.org/users/haukehillebrandt\"><u>Hauke Hillebrandt</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/Qhn5nyRf93dsXodsw/cause-area-differential-neurotechnology-development\"><u>Differential Neurotechnology Development</u></a> by <a href=\"https://forum.effectivealtruism.org/users/mwcvitkovic\"><u>mwcvitkovic</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/nSwaDrHunt3ohh9Et/cause-area-short-sleeper-genes\"><u>Short sleeper genes</u></a> by <a href=\"https://forum.effectivealtruism.org/users/johnboyle\"><u>JohnBoyle</u></a></li></ul><p>We are contacting all prize recipients by email.</p><h3>Good-faith submissions</h3><p>Next week we will begin the process of emailing everyone who submitted a good-faith submission in order to offer them participation awards of $200.&nbsp;&nbsp;</p><h3>Future plans</h3><p>As we stated in our <a href=\"https://forum.effectivealtruism.org/posts/iqcph4DbcP4PZGyDB/open-philanthropy-s-cause-exploration-prizes-usd120k-for\"><u>announcement</u></a>, this was a trial process for us. We\u2019re grateful to those who sent us feedback and suggestions for how to improve. At this stage, we don\u2019t know if or when we will repeat a process like this. We might write a public update later this year on what we have learned from this exercise and any plans to repeat this or a similar exercise again.</p><h3>Thank you</h3><p>We are grateful to <a href=\"https://forum.effectivealtruism.org/users/lizka\"><u>Lizka</u></a> and the other operators of the EA Forum, and to everyone who engaged with or submitted an entry to the Cause Exploration Prizes for making this possible.</p><p><br>&nbsp;</p>", "user": {"username": "ChrisSmith"}}, {"_id": "ug3aE3rbwLaoQmWri", "title": "GiveWell and Similar Orgs Should *Really* Research Family Planning as a Potential Intervention", "postedAt": "2022-09-09T20:37:15.369Z", "htmlBody": "<p>There is a strong case to be made that research into family planning should be a priority for GiveWell and other similar organizations (Open Philanthropy, Rethink Priorities, Founders Pledge etc.).</p><p>The Copenhagen Consensus Center has identified universal access to contraception/family planning as one of the most cost-effective ways to improve global welfare, in 2 separate analyses, with estimated extremely large returns for each dollar spent. In fact, excluding projects that can mostly only be implemented by governments (free trade and R&amp;D), the Copenhagen Consensus estimated contraception/family planning to be the most cost-effective interventions possible. The first estimation, from the <a href=\"https://www.copenhagenconsensus.com/sites/default/files/post2015brochure_m.pdf\">Post-2015 Consensus Project</a>, estimated 120$ returns for every $ spent on universal access to contraception. The second, from <a href=\"https://www.copenhagenconsensus.com/best-buys-africa\">Best Buys for Africa</a>, estimates 94$ returns for every $ spent on family planning.</p><p>Apparently, <a href=\"https://docs.google.com/spreadsheets/d/1XkDq7SFmCn8iqSHutdldGuudzIODsD3QpcspjCRZ1as/edit#gid=1322214050\">Charity Entrepreneurship also estimated</a> the return on investment of postpartum family planning, at 105$ for every $. They wrote, <a href=\"https://3394c0c6-1f1a-4f86-a2db-df07ca1e24b2.filesusr.com/ugd/9475db_2050ace6d2154590a08f6dc2af479afb.pdf\">in a report on postpartum family planning</a>: \u201cThrough our research, we found that this idea is among the strongest from the perspective of evidence base, cost-effectiveness, and execution difficulty\u201d.</p><p><br><strong>The Post-2015 Consensus Project </strong><a href=\"https://www.copenhagenconsensus.com/sites/default/files/post2015brochure_m.pdf\"><strong>also supplies us</strong></a><strong> with an estimation of the relative effectiveness of family planning to direct money transfers. It estimates that 5$ are generated for every $ in money transfers to end extreme poverty. Thus, on a very rough estimate, family planning is between 18.8 to 24 times as effective as programs such as GiveDirectly\u2019s cash transfers. This is well within the margin of investment for GiveWell, and justifies further research into family planning. If we consider the prevention of miscarriages and other health issues, it may even be higher.&nbsp;&nbsp;</strong></p><p><br>I know that these cost-benefit estimates have many limitations, and that they also depend on effective charities in that area of work, but there are 3 main considerations that show this area is still very worth investigating. First, the external validity of the aforementioned estimates is probably reasonable. Past priorities of the Copenhagen Consensus align quite well with the charities GiveWell has historically supported (I can elaborate further if needed), and Charity Entrepreneurship works within a similar framework to GiveWell. Second, the only current research article on contraception I\u2019ve found in GiveWell, on Sayana Press, is very neglected - It does not mention the demographic dividend and some other benefits of contraception, and hasn\u2019t been updated since 2017. Third, If family planning will be found by GiveWell to be an effective cause, and GiveWell will be vocal in support of family planning, that could greatly incentivize existing charities to extend further resources to the topic, and inspire people to donate more to that cause. Alternatively, if GiveWell will conclude that family planning may be an effective cause, but more research is needed, that could incentivize more research. Even if no current charities in family planning are good enough, GiveWell can accelerate progress in the field.</p><p><br>For what it\u2019s worth, <a href=\"https://www.youtube.com/watch?v=sPR1qKr-cqU\">Melinda Gates also said</a> that contraception is the \u201cgreatest anti-poverty tool in the world\u201d. While the Gates Foundation does have massive support for family planning, that does not mean they made a significant effort to identify the most effective charities working in the field, like GiveWell would. Some forms of family planning are likely much less effective than others, as seen in <a href=\"https://www.charityentrepreneurship.com/health-reports\">Charity Entrepreneurship\u2019s reports</a> (potentially more than an order of magnitude).</p><p><br>Mechanisms of benefit:&nbsp;</p><p>I will only briefly describe these as I absolutely trust interested people to perform their own research. Bj\u00f8rn Lomborg, the head of the CCC, wrote that about 40 dollars of expected benefit from contraception access will come improved health associated with less births, while the additional 80 dollars will come from the \u201cdemographic dividend\u201d, caused by parents and the local government having less people to take care of, and more revenue (because of increased work time) <a href=\"https://www.shine.cn/opinion/1910314938/\">[source</a>].&nbsp;</p><p>Additional benefits may come from climate change mitigation. Although today poor countries aren\u2019t responsible for much GHG emission, these numbers are expected to increase as they get richer. Currently, Charity Entrepreneurship estimated that through family planning, 3 tonnes of CO2 can be mitigated for 1$. This is in <a href=\"https://www.givinggreen.earth/us-policy-change-research/clean-air-task-force\">line with the Clean Air Task Force</a>, as estimated by Founders Pledge, and may be even better than them.&nbsp;</p><p><br>&nbsp;From a quick search, some more research and support for family planning can be found <a href=\"https://forum.effectivealtruism.org/posts/zgBmSgyWECJcbhmpc/family-planning-a-significant-opportunity-for-impact\">here</a>, <a href=\"https://forum.effectivealtruism.org/posts/WYmJoDxJZToDcA9Bq/population-size-growth-and-reproductive-choice-highly\">here</a>, <a href=\"https://forum.effectivealtruism.org/posts/wx6Xw63yJt67YKdzh/why-start-a-family-planning-charity-founders-needed-1\">here</a>.</p><p>&nbsp;</p><p>In conclusion, family planning may be a very cost-effective way to improve the lives of women and children in the developing world. GiveWell and similar organizations should research family planning so that they can provide more information about this very promising intervention.</p><p><br>&nbsp;*Other organizations that could benefit from research on family planning are Open Philanthropy, Giving What We Can, Happier Lives Institute, Rethink Priorities, Founders Pledge and Evidence Action (The Life You Can Save and Charity Entrepreneurship already have pages on contraception).<br>&nbsp;</p><p>Some additional considerations:&nbsp;</p><p>One other source - <a href=\"https://academic.oup.com/epirev/article/32/1/152/503735?login=false\"><i>\u201cFamily Planning and the Burden of Unintended Pregnancies\u201d</i></a>, estimates the <i>health </i>benefit-cost ratio much lower, at 2-9$/$, and on average 8$/$. I don\u2019t think it estimated the benefits from the demographic dividend.</p><p>Obviously there's a need for a choice and for some reproduction on the population level, but the initial implementation will not be able to hurt population&nbsp;reproduction in a significant way. Might need to make sure that women aren't forced to take contraception by others, although I don\u2019t think that\u2019s usually a problem.&nbsp;</p><p>It\u2019s worth checking what are GiveWell current donators' opinions on family planning, and the opinions of potential donators.</p><p>Family planning is relatively relatable to people in developed countries, especially with the recent turnover of Roe v. Wade. It might get more support than most other charities for that reason, which may even increase donations to GiveWell in general.</p><p>If Charity Entrepreneurship is already doing research on the matter, why should GiveWell? CE\u2019s work seems good, but GiveWell is much more well known - searched about 12 times as much last year, <a href=\"https://trends.google.com/trends/explore?q=%2Fm%2F03h570b,Charity%20Entrepreneurship\">according to Google Trends</a>. The Life You Can Save is searched more than GiveWell, but the <a href=\"https://www.thelifeyoucansave.org/blog/contraception-an-impactful-poverty-intervention/\">page on contraception</a> is hidden in the site (it\u2019s in the site's blog), and the charities recommended don\u2019t have much research behind them. In addition, GiveWell is probably the most well regarded research site on effective charities in global health.&nbsp;&nbsp;</p>", "user": {"username": "alamo 2914"}}, {"_id": "EiaNkrwfQr4chu2kX", "title": "The Domestication of Zebras", "postedAt": "2022-09-09T10:58:09.762Z", "htmlBody": "<p>I missed the deadline for Cause Exploration Prize submissions. However, I am not bothered about the prize, I just want to put forward the case for focussing some attention on the domestication of zebras. Here is the case in a nutshell - other should please feel free to take the ideas and develop them as they wish.&nbsp;</p><ul><li>Horses (and similar animals) have been a major source of power for agricultural, industrial and transport uses throughout human history. I focus on transport but similar considerations apply to other uses of horses.</li><li>In the event of a disaster which made mechanised transport impossible then humans would likely return to the use of horses, especially for transport. Fortunately, there is a reasonably large number of horses and people who know how to train, manage and breed horses (e.g. farms, the British Army).</li><li>Horses are ill-suited to conditions in sub-Saharan Africa (e.g., tsetse fly). In the event of mechanised transport becoming unavailable to humanity as a whole or to sub-Saharan Africa in particular then humans there would not be able to fall back on the horse as an alternative.</li><li>Zebras, however, have evolved to endure conditions in sub-Saharan Africa. If the zebra were domesticated then its power would be available to people living there.&nbsp;</li><li>The domestication of the zebra therefore represents an insurance policy which would mitigate the worst effects of a disaster rendering mechanised transport unavailable in sub-Saharan Africa. The thought process here is similar to the well-known recent suggestion that coal deposits should not be exhausted in order that re-industrialisation can take place if needed: if humanity is returned to pre-industrial conditions then the more resources available to humanity at that time, the better.&nbsp;</li><li>The domestication of the zebra is likely nowhere near as difficult as has been widely reported, e.g. in <i>Guns, Germs and Steel</i>. Here <a href=\"https://www.youtube.com/watch?v=etV8YAnyjcE\">https://www.youtube.com/watch?v=etV8YAnyjcE</a> is a link to a video of a woman confidently riding an evidently tame zebra. (The YouTube channel from which this comes, 4hoovesnstrips, shows more examples.)&nbsp;</li><li>This <a href=\"http://messybeast.com/history/working.htm\">http://messybeast.com/history/working.htm</a> is a link to a set of resources showing the successes that colonial powers and hobbyists had in domesticating zebras in the late 19<sup>th</sup> and early 20<sup>th</sup> centuries, before mechanised transport became more cost effective than continuing the domestication project. Examples include:<ul><li>Multiple photographs of men and women riding zebras.</li><li>A photograph of a zebra being ridden by a man while jumping a fence.</li><li>Excerpts from books written around the turn of the 20<sup>th</sup> century explaining how to break in zebras. E.g. \u201c<i>Of all asses, the mountain zebra (see Fig. 122) is the most difficult to break; as he is sulky, stupid, and has an almost immovable neck. I have found the Burchell\u2019s zebra, which is more nearly akin to the horse than any other ass, comparatively easy to break.</i>\u201d</li><li>Examples of zebras used for pulling carts or carriages. Some examples are no more than aristocratic japes or publicity stunts, but others include mail coaches and similar commercial enterprises.</li><li>Zebra hybrids bred in German East Africa used to pull gun carriages.</li></ul></li><li>Modern gene-editing techniques are likely to make it even easier to develop tame zebra varieties suitable for human use than the older selective breeding and training techniques.&nbsp;</li><li>The domestication of the zebra, so long as presented as beneficial to the animals themselves, is likely to be a reasonably popular project. A large number of people like horses and animals that look like horses. Many 18/19 year olds would be happy to work on a zebra farm in southern Africa in their gap year at their own cost.&nbsp;</li><li>Related: bringing back the quaggas would appear to be sensible. There are indications from the material at the link above that quaggas were likely even better candidates for domestication than zebras.</li></ul>", "user": {"username": "Further or Alternatively"}}, {"_id": "7DPvmefCHKxhdh8kD", "title": "How likely is it that malaria vaccines will soon become more effective than bednets?\n", "postedAt": "2022-09-09T10:59:30.824Z", "htmlBody": "", "user": {"username": "Alexander de Vries"}}, {"_id": "NM6BPx86dndwwRxqt", "title": "Universal Declaration of Truth - DRAFT Rev A", "postedAt": "2022-09-09T07:57:54.622Z", "htmlBody": "<p>The Universal Declaration of Truth (DRAFT) can be found in my free Substack newletter linked. Any input, comments or direction would be most welcome.&nbsp;</p>", "user": {"username": "Ross McMath"}}, {"_id": "hprGdAiW73EsPfWzu", "title": "Zzapp Malaria: More effective than bed nets? (Wanted: CTO, COO & Funding)", "postedAt": "2022-09-09T07:38:51.549Z", "htmlBody": "<p><strong>TL;DR</strong>: This post describes Zzapp\u2019s approach and effectiveness from their own perspective, intended as an intro aimed at the Effective Altruism community, as an invitation to investigate further and maybe fund them. They claim to be 2x more cost effective than bed nets in reducing malaria in <strong>urban and semi-urban areas</strong> (<a href=\"https://www.oecd-ilibrary.org/sites/05384b8c-en/index.html?itemId=/content/component/05384b8c-en\"><u>over 70%</u></a> of Africa\u2019s population).</p><p><strong>Epistemic status</strong>: Based on conversations with Arnon, the CEO of <a href=\"https://www.zzappmalaria.com/\"><u>Zzapp Malaria</u></a>, not cross checked with other info such as Givewell\u2019s review of Against Malaria Foundation.</p><h1>Zzapp\u2019s approach and theoretical reason to think it would work</h1><p>You can <a href=\"https://forum.effectivealtruism.org/posts/hprGdAiW73EsPfWzu/zzapp-malaria-more-effective-than-bed-nets-wanted-cto-coo#How_effective_is_this_\">skip</a> to their experiment and how it went, if you prefer.</p><p><strong>TL;DR:</strong> Spray water bodies with larvicide to prevent mosquitoes from reproducing, and do it extra well by managing the considerable ops work of finding and spraying the water bodies using satellite imaging and an app for the people on the ground.</p><h2>Spraying water bodies with larvicide - is tried and works, unrelated to Zzapp</h2><p>Sources [<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S1473309902003973\"><u>link</u></a>] [<a href=\"https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD012736.pub2/full\"><u>link</u></a>].</p><h2>Theoretical advantages compared to bed nets</h2><ul><li>In every place that malaria was eliminated (which <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S1473309902003973\"><u>happened</u></a> many times), larvicide (the treatment of standing water bodies) was the main component.</li><li>Bed Nets only help people indoors during the night.</li><li>Many people don\u2019t use their bed nets.</li><li>Mosquitos developed resistance to the bes nets\u2019 insecticide in many countries</li></ul><p>Note I think Givewell already took the problems into account in their analysis, and Arnon emphasizes he thinks bed nets are great, and this is a pitch for using larvicide in urban (and semi urban) areas, not for stopping distributing bed nets. Zzapp think the ideal solution would probably combine many interventions. We are writing this as a comparison with bed nets since EAs already think bed nets are great.</p><h2>Problems in existing larvicide approaches</h2><h3>Existing solutions: Problems in theory</h3><p><strong>Coverage is important</strong></p><p>It\u2019s important [how many water bodies you find] and [how many of those you spray], and the difference between 95% and 50% is really big, similarly to the situation when vaccinating 95% or 50% of the population, because of the effect on R (reproduction number) - less infected people will infect less other people, it snowballs but in a good way (hopefully), and the same is true about reproduction of mosquitos.</p><p><strong>Existing solutions have bad coverage</strong></p><ul><li>People miss water bodies in the areas they are assigned to search</li><li>People miss entire areas</li><li>Even when water bodies are found, the spray team sometimes may still skip them or forget to treat them according to schedule</li></ul><p><strong>Small RCT</strong></p><p><a href=\"https://endmalaria.org/sites/default/files/4_Arnon%20Yafin.pdf\"><u>Ref</u></a> to a (tiny) randomized controlled trial run by Zzapp and <a href=\"https://www.ccmghana.net/index.php/2018-2020/malaria/anglogold-ashanti-malaria-control\"><u>AngloGold Ashanti Malaria Control (AGAMal</u></a>), where two groups scanned the same square kilometer, one group used the app and one didn\u2019t and the group with the app found 28% more water bodies.</p><p><strong>Scanning an entire town</strong></p><p>In a different operation, when scanning an entire town with AGAMal, they found 20x more water bodies when using Zzapp\u2019s app. (publication in progress, we\u2019ll add a link when it\u2019s ready). From that they think that on a larger scale the app has an even greater impact.What happened behind the scenes is that without the app - the scanners skipped entire neighborhoods.</p><p><strong>Not a problem: Poisoning water bodies</strong></p><p>The larvicide in the relevant quantities (<a href=\"https://www.cdc.gov/mosquitoes/mosquito-control/community/bti.html#:~:text=Bti%20is%20not%20harmful%20to,or%20other%20insects%2C%20including%20honeybees.\"><u>bti</u></a>) isn\u2019t poisonous to humans, animals, or other insects except for mosquitoes and black flies.</p><h3>Zzapp\u2019s advantages compared to \u201cmanual\u201d larvicide</h3><p>Zzapp has an app they give to the people \u201con the ground\u201d:&nbsp;</p><p>The app follows \u201cwhere do the people go go\u201d and lets the people mark \u201cI checked this house\u2019s garden\u201d and \u201cI found a water source here\u201d and \u201cthis house didn\u2019t let me in\u201d</p><p>The control room shows a map with&nbsp;</p><ul><li>\u05f4Here are the places the worker was in, and here are the places that not\u05f4</li><li>\u05f4this water source was only sprayed 2 weeks ago, someone has to go there again\u201d,</li></ul><p>giving more order to the huge mess of going over all different water sources again and again.</p><h1>Screen shots</h1><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hprGdAiW73EsPfWzu/ehbalxfexqwggyawgz23\"></figure><p>In this view, used by the control room, we see where all the water bodies are, including which ones were sprayed already and when.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hprGdAiW73EsPfWzu/glwaaebfgisuoidsu5z1\"></figure><p>Another screenshot with captions by Zzapp</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hprGdAiW73EsPfWzu/gwtg3ukuaycutnch4rmp\"></figure><p>This is a screenshot from the app, where they see different colors for areas they have already scanned and areas that still need scanning, gradually changing as they walk around.</p><p>Author\u2019s note: The last screenshot looks like one step away from Pokemon Go!</p><p>Arnon: Yeah, my colleague, also named Yonatan, always says that we should contact the Pokemon Go devs!</p><p>Me: You see, everyone? \u201cYonatan\u201d is a totally common name here!</p><h1>How effective is this?</h1><h2>Based on Zzapp\u2019s run of their program</h2><p><strong>Effectiveness</strong></p><ul><li>As effective in reducing malaria as bednets (Zzapp reduces 53% of cases, bed nets <a href=\"https://pubmed.ncbi.nlm.nih.gov/30398672/\"><u>reduce 45%</u></a>. Zzapp say this is statistically the same)<ul><li>There will be more about this comparison in a <a href=\"https://forum.effectivealtruism.org/posts/hprGdAiW73EsPfWzu/zzapp-malaria-more-effective-than-bed-nets-wanted-cto-coo#Next_posts_planned\">separate</a> post, it\u2019s more complicated than this.</li></ul></li></ul><p><strong>Cost</strong></p><p>In urban (and semi-urban) areas, the cost was <strong>41 cent per person</strong></p><p>In villages, it was <strong>1.23 dollars per person</strong>, with high variance between villages.</p><p><strong>How long does this help for?</strong></p><ul><li>Bednets<ul><li>Help for 3 years.</li><li><a href=\"https://www.sciencedirect.com/science/article/pii/S1098301521001479\"><u>Cost</u></a> $1.39 per person protected per year.</li></ul></li><li>Zapp\u2019s solution (in urban and semi-urban areas)<ul><li>Costed 0.41 cent.</li><li>For how long?<ul><li>The operation was run for 8 months.</li><li>In the first 2 months, they didn\u2019t spray (this was done on purpose, to measure the amount of mosquitoes and establish a baseline. In a real world operation, they would have started spraying immediately)<ul><li>So they protected for 6 months only, even though most (not all) of the cost components were for 8 months</li></ul></li></ul></li></ul></li><li>After effects<ul><li>The after effect of larvicide is expected to be more significant than the after effect of bed nets.</li></ul></li></ul><p><strong>Cost Effectiveness</strong></p><ul><li>Zzapp estimates this is 2x cheaper than bednets (in urban and semi-urban areas).</li><li>Conventional larvicide <a href=\"https://www.valueinhealthjournal.com/article/S1098-3015(21)00147-9/fulltext\"><u>costs</u></a> $1.18 per person protected per year</li></ul><p><strong>Disclaimers</strong></p><ul><li>These results were not yet peer reviewed.<ul><li>We\u2019ll add a link here when they\u2019re publicly available (expected within days/weeks).</li></ul></li><li>Expect <a href=\"https://forum.effectivealtruism.org/posts/hprGdAiW73EsPfWzu/zzapp-malaria-more-effective-than-bed-nets-wanted-cto-coo#Next_posts_planned\">another</a> post going into more detail here.</li><li>Did we already tell you that Zzapp wants funding for running another RCT?</li></ul><h2>Potential to be way more effective (aka: Zzapp\u2019s gaps)</h2><p><strong>TL;DR</strong>: Zzapp had things that can be significantly optimized during their first run which, if improved, could make them significantly more cost effective.</p><p><strong>Meta</strong>: I appreciate Arnon\u2019s openness about this.</p><p><strong>My priors</strong>: Obviously the first times one runs a very big op - it won\u2019t be the most effective it can be. But I\u2019ll add some examples anyway.</p><h3>Examples of why the cost could be lower:</h3><ul><li>Zzapp used taxies to drive the field workers between villages and towns. That\u2019s an expensive solution.</li><li>Their software has big, eh, \u201dgaps\u201d and they want to rewrite it. Did I mention they want to hire a CTO? We plan a post specifically about that.</li><li>They had trouble managing 70 people \u201con the ground\u201d.<ul><li>They got an effective 2.3 hours of scanning per person per day (but paid for 8).<ul><li>Arnon explains that this makes sense - employees will obviously need time to, for example, wait to get their equipment at the start of the day, but he still thinks it\u2019s realistic to raise this number to about 4 hours of scanning per day.</li></ul></li><li>They think this is way better than doing larvicide without the Zzapp app, but it could be much better.</li><li>Did I mention they want to hire a COO who can manage a lot of people, experienced in running projects in Africa?</li></ul></li></ul><h3>Examples of why the effectiveness could be higher (regardless of the cost) :</h3><ul><li>Zzapp could find more water bodies<ul><li>Some people skip water bodies. These people could be identified early, Zzapp could.. do something about it.</li></ul></li></ul><h1>Next posts planned</h1><ul><li><a href=\"https://forum.effectivealtruism.org/posts/zsLcixRzqr64CacfK/zzappmalaria-twice-as-cost-effective-as-bed-nets-in-urban\">A more detailed cost effectiveness analysis</a></li><li>Job postings</li></ul><h1>Arnon\u2019s contact info</h1><p>arnon at zzappmalaria dot com</p>", "user": {"username": "hibukki"}}, {"_id": "Rx3baBysEhdQFzPdo", "title": "Markus Anderljung On The AI Policy Landscape", "postedAt": "2022-09-09T17:27:21.006Z", "htmlBody": "<p><a href=\"https://twitter.com/manderljung?lang=en\">Markus Anderljung</a> is the Head of AI Policy at the <a href=\"https://governance.ai/\">Centre for Governance of AI</a> (GovAI) and was previously seconded to the UK government office as a senior policy specialist.</p><p>In this episode we discuss <a href=\"https://twitter.com/jackclarksf/status/1555980412333133824\">recent AI Policy takes</a>, what kind of work GovAI is doing and how you could have an impact in the AI Governance landscape more broadly.</p><p>Below are some highlighted quotes from our conversation (available on <a href=\"https://youtu.be/DD303irN3ps\">Youtube</a>, <a href=\"https://podcasts.apple.com/us/podcast/the-inside-view/id1565088425\">Apple Podcast</a>, <a href=\"https://podcasts.google.com/feed/aHR0cHM6Ly9hbmNob3IuZm0vcy81NmRmMjE5NC9wb2RjYXN0L3Jzcw/episode/MzJlMzk4YTAtYmMzZC00MDVkLWIzMTAtNTZhMmM2ZDc2MTg0?sa=X&amp;ved=0CAUQkfYCahcKEwiI2sT3hY35AhUAAAAAHQAAAAAQAQ\">Google Podcast</a> and <a href=\"https://open.spotify.com/episode/0h57uSenET1xi3ea0uXGRd?si=bMHdtXvSSPmB-CC7RUZSDA\">Spotify</a>). For the full context for each of these quotes, you can find the accompanying <a href=\"https://theinsideview.ai/markus\">transcript</a>.</p><hr><h2>Preparing The World For More Advanced Systems by Reducing Current Harm From AI</h2><blockquote><p>\"If you're trying to affect AI labs, what they're doing, I think a lot of the things that you want to do today looks like...<strong> you find cases where you're dealing with a present day problem or a problem that these companies will start feeling soon. And ways in which these AI systems might be causing harm now. And then you try to figure out how to solve that problem while also preparing the company, and preparing the world for more advanced systems. </strong>Part of the reason for that is you'll just have a larger coalition of actors, and people within the company, and people outside of the company who will be excited and interested in helping out. And I think that's just going to be quite ... A lot of the times it's going to be the really useful thing to do.\"</p></blockquote><h2>AI Policy Work Should Be Robust To Worldviews</h2><blockquote><p><strong>\"People's credences on the extent to which we'll have human level machine intelligence or whatever, they differ widely. And it's going to be difficult. A lot of the times, it's going to be difficult to push things through.</strong> If the only reason is like, \"Oh, this will only be helpful if it's the case that we, in the next 20 years or whatever, develop these very, very powerful systems.\" I think in general, you want to be thinking about things that makes sense from multiple perspectives, sort of robust to worldviews.\"</p></blockquote><h2>With Great Compute Comes Great Responsibility</h2><blockquote><p>\"Some of the kinds of things to explore there include this thing that we talked about earlier of, <strong>could we make it the case that there are maybe levels or something like this, of what amount of compute comes with what amount of responsibility</strong>. I think that's a really promising one and there's a bunch of ways in which you could try to make that happen. And there's a bunch of details to think through.\"</p></blockquote><h2>How To Incentivize External Scrutiny in Big Tech</h2><blockquote><p>\"<strong>How can you make it the case that sort of the world's most powerful or impactful models, that those models receive external scrutiny without that requiring that you give that model to a whole bunch of different actors?</strong> Partly just because these companies, you need to make these kinds of governance systems incentive-compatible. And so, I think you're going to be hard pressed to get Facebook to just give everyone their recommended algorithm or whatever, or their newsfeed algorithm.\"</p></blockquote><h2>Actors Start Taking Hits On Safety When They Assume Other Players Are Irresponsible</h2><blockquote><p><strong>\"If they believe that the other actor is going to act really irresponsibly...</strong> Even if they develop these very powerful systems, they're not going to use them for good purposes or whatever, <strong>then they're going to be much more likely to say, \"Okay, well it's worth it for me to take this hit on safety or act less responsibly, or develop my system less carefully, because it's really important that I make it there first.\"</strong> That seems like a worrying situation to me.\"</p></blockquote>", "user": {"username": "mtrazzi"}}, {"_id": "EG9xDM8YRz4JN4wMN", "title": "Samotsvety's AI risk forecasts", "postedAt": "2022-09-09T04:01:10.760Z", "htmlBody": "<p><i>Crossposted to </i><a href=\"https://www.lesswrong.com/posts/YMsD7GA7eTg2BafQd/samotsvety-s-ai-risk-forecasts\"><i>LessWrong</i></a><i> and </i><a href=\"https://www.foxy-scout.com/samotsvetys-ai-risk-forecasts/\"><i>Foxy Scout</i></a></p><h1>Introduction</h1><p>In <a href=\"https://forum.effectivealtruism.org/posts/9Y6Y6qoAigRC7A8eX/my-take-on-what-we-owe-the-future%23Other_inputs&amp;sa=D&amp;source=editors&amp;ust=1662684319699596&amp;usg=AOvVaw2Fe56xeK-qGWHj-IgisATF\"><u>my review of What We Owe The Future</u></a>&nbsp;(WWOTF), I wrote:</p><blockquote><p>Finally, I\u2019ve updated some based on my experience with <a href=\"https://www.google.com/url?q=https://samotsvety.org/&amp;sa=D&amp;source=editors&amp;ust=1662684319700120&amp;usg=AOvVaw1lVU67PXfb9AguucjUOYdf\"><u>Samotsvety forecasters</u></a>&nbsp;when discussing AI risk\u2026 When we discussed the report on power-seeking AI, I expected tons of skepticism but in fact almost all forecasters seemed to give &gt;=5% to disempowerment by power-seeking AI by 2070, with many giving &gt;=10%.</p></blockquote><p>In the comments, <a href=\"https://forum.effectivealtruism.org/posts/9Y6Y6qoAigRC7A8eX/my-take-on-what-we-owe-the-future?commentId%3DcB2FnhFRJujCpF6Dn%23comments&amp;sa=D&amp;source=editors&amp;ust=1662684319700657&amp;usg=AOvVaw0X3v5Z4WaVt_7DQMK0IlFh\"><u>Peter Wildeford asked</u></a>:</p><blockquote><p>It looks like Samotsvety also forecasted AI timelines and AI takeover risk - are you willing and able to provide those numbers as well?</p></blockquote><p>We separately received a request from the <a href=\"https://www.google.com/url?q=https://ftxfoundation.org/&amp;sa=D&amp;source=editors&amp;ust=1662684319701284&amp;usg=AOvVaw0NdHI6Y5L-x85GzlKDGpP-\"><u>FTX Foundation</u></a>&nbsp;to forecast on 3 questions about AGI timelines and risk.</p><p>I sent out surveys to get Samotsvety\u2019s up-to-date views on all 5 of these questions, and thought it would be valuable to share the forecasts publicly.</p><p>A few of the headline aggregate forecasts are:</p><ol><li>25% chance of misaligned AI takeover by 2100, barring pre-<a href=\"https://www.google.com/url?q=https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit%23heading%3Dh.14onymzb0y9&amp;sa=D&amp;source=editors&amp;ust=1662684319702030&amp;usg=AOvVaw1QjdoXC8eIWHc6RyVjtx5R\"><u>APS-AI</u></a>&nbsp;catastrophe</li><li>81% chance of <a href=\"https://www.google.com/url?q=https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/&amp;sa=D&amp;source=editors&amp;ust=1662684319702525&amp;usg=AOvVaw2d-B8TknoAOYW-GzYqghrV\"><u>Transformative AI</u></a>&nbsp;(TAI) by 2100, barring pre-TAI catastrophe</li><li>32% chance of AGI being developed in the next 20 years</li></ol><h1>Forecasts</h1><p>In each case I aggregated forecasts by removing the single most extreme forecast on each end, then taking the <a href=\"https://forum.effectivealtruism.org/posts/sMjcjnnpoAQCcedL2/when-pooling-forecasts-use-the-geometric-mean-of-odds&amp;sa=D&amp;source=editors&amp;ust=1662684319703178&amp;usg=AOvVaw02P1mz7htYpyi9fQ6pjTmx\"><u>geometric mean of odds</u></a>.</p><p>To reduce concerns of in-group bias to some extent, I calculated a separate aggregate for those who weren\u2019t highly-engaged EAs (HEAs) before joining Samotsvety. In most cases, these forecasters hadn\u2019t engaged with EA much at all; in one case the forecaster was aligned but not involved with the community. Several have gotten more involved with EA since joining Samotsvety.</p><p>Unfortunately I\u2019m unable to provide forecast rationales in this post due to forecaster time constraints, though I might in a future post. I provided my personal reasoning for relatively similar forecasts (35% AI takeover by 2100, 80% TAI by 2100) in <a href=\"https://forum.effectivealtruism.org/posts/9Y6Y6qoAigRC7A8eX/my-take-on-what-we-owe-the-future%23Underestimating_risk_of_misaligned_AI_takeover&amp;sa=D&amp;source=editors&amp;ust=1662684319703955&amp;usg=AOvVaw2WffYN7Vf935A0cEzEiPAz\"><u>my WWOTF review</u></a>.</p><h2>WWOTF questions</h2><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>&nbsp;</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>Aggregate (n=11)</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>Aggregate, non-pre-Samotsvety-HEAs (n=5)</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>Range</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>What's your probability of misaligned AI takeover by 2100, barring pre-<a href=\"https://www.google.com/url?q=https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit%23heading%3Dh.14onymzb0y9&amp;sa=D&amp;source=editors&amp;ust=1662684319705883&amp;usg=AOvVaw0TkrNbrZ-9z7DJ1cztreGk\"><u>APS-AI</u></a>&nbsp;catastrophe?</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>25%</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>14%</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>3-91.5%</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>What's your probability of <a href=\"https://www.google.com/url?q=https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/&amp;sa=D&amp;source=editors&amp;ust=1662684319707335&amp;usg=AOvVaw1RALHxnNIL1Fh5lucV1GKs\"><u>Transformative AI</u></a>&nbsp;(TAI) by 2100, barring pre-TAI catastrophe?</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>81%</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>86%</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>45-99.5%</p></td></tr></tbody></table></figure><h2>FTX Foundation questions</h2><p>For the purposes of these questions, FTX Foundation defined AGI as roughly \u201cAI systems that power a comparably profound transformation (in economic terms or otherwise) as would be achieved in [a world where cheap AI systems are fully substitutable for human labor]\u201d. See <a href=\"https://www.google.com/url?q=https://docs.google.com/document/d/1I2_pN42wkHJph7QjJnAHevgUu7JIg5fdugQHZbnEZi8/edit&amp;sa=D&amp;source=editors&amp;ust=1662684319708620&amp;usg=AOvVaw3Wj5gt3s_nIjE7jtPNCLjx\"><u>here</u></a>&nbsp;for the full definition used.</p><p>Unlike the above questions, these are not conditioning&nbsp;on no pre-AGI/TAI catastrophe.</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>&nbsp;</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>Aggregate (n=11)</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>Aggregate, non-pre-Samotsvety-HEAs (n=5)</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>Range</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>What's the probability of <a href=\"https://forum.effectivealtruism.org/topics/existential-catastrophe-1\"><u>existential catastrophe</u></a>&nbsp;from AI, conditional on AGI being developed by 2070?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqfvfbsamfmh\"><sup><a href=\"#fnqfvfbsamfmh\">[1]</a></sup></span></p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>38%</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>23%</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>4-98%</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>What's the probability of AGI being developed in the next 20 years?</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>32%</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>26%</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>10-70%</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>What's the probability of AGI being developed by 2100?</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>73%</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>77%</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>45-80%</p></td></tr></tbody></table></figure><h1>Who is Samotsvety Forecasting?</h1><p>Edited to add: Our track record is now online <a href=\"https://samotsvety.org/track-record/\">here</a>.</p><p><a href=\"https://www.google.com/url?q=https://samotsvety.org/&amp;sa=D&amp;source=editors&amp;ust=1662684319713730&amp;usg=AOvVaw0gEnVI6U5FmlqCKF8eEt09\"><u>Samotsvety Forecasting</u></a>&nbsp;is a forecasting group that was started primarily by <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/users/misha_yagudin&amp;sa=D&amp;source=editors&amp;ust=1662684319713961&amp;usg=AOvVaw3wMnZFR6oYSWhmyP5mGyAM\"><u>Misha Yagudin</u></a>, <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/users/nunosempere&amp;sa=D&amp;source=editors&amp;ust=1662684319714221&amp;usg=AOvVaw0hUtAjF-V1qlHR6MerjN9S\"><u>Nu\u00f1o Sempere</u></a>, and myself predicting as a team on <a href=\"https://www.google.com/url?q=https://www.infer-pub.com/teams/31&amp;sa=D&amp;source=editors&amp;ust=1662684319714462&amp;usg=AOvVaw2Q1bdhzD3G8qzLZMT6NCis\"><u>INFER</u></a>&nbsp;(then Foretell). Over time, we invited more forecasters who had very strong track records of accuracy and sensible comments, mostly on <a href=\"https://www.google.com/url?q=https://www.gjopen.com/&amp;sa=D&amp;source=editors&amp;ust=1662684319714697&amp;usg=AOvVaw0Fz8750gcO3HfdNuiKhsC2\"><u>Good Judgment Open</u></a>&nbsp;but also a few from INFER and <a href=\"https://www.google.com/url?q=https://www.metaculus.com/&amp;sa=D&amp;source=editors&amp;ust=1662684319714881&amp;usg=AOvVaw0IN5y04wwCoKetIHmimKGF\"><u>Metaculus</u></a>. Some strong forecasters were added through social connections, which means the group is a bit more EA-skewed than it would be without these additions. A few Samotsvety forecasters are also <a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Superforecaster&amp;sa=D&amp;source=editors&amp;ust=1662684319715101&amp;usg=AOvVaw2M9P99Dz-APVEndjaWthlk\"><u>superforecasters</u></a>.</p><h1>How much do these forecasters know about AI?</h1><p>Most forecasters have at least read Joe Carlsmith\u2019s report on AI x-risk, <a href=\"https://www.google.com/url?q=https://arxiv.org/abs/2206.13353&amp;sa=D&amp;source=editors&amp;ust=1662684319715616&amp;usg=AOvVaw1qOwi2kt1M-uV1TaMmxqNQ\"><u>Is Power-Seeking AI an Existential Risk?</u></a>. Those who are short on time may have just skimmed the report and/or watched the <a href=\"https://forum.effectivealtruism.org/posts/ChuABPEXmRumcJY57/video-and-transcript-of-presentation-on-existential-risk&amp;sa=D&amp;source=editors&amp;ust=1662684319715863&amp;usg=AOvVaw3MD18murVWevKpeOZb6p7V\"><u>presentation</u></a>. We discussed the report section by section over the course of a few weekly meetings.</p><p>~5 forecasters also have some level of AI expertise, e.g. I did some <a href=\"https://www.google.com/url?q=https://scholar.google.com/citations?user%3DQ33DXbEAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1662684319716331&amp;usg=AOvVaw1NM1tWd92c48HYwenQ5TmF\"><u>adversarial robustness research</u></a>&nbsp;during my last year of undergrad then worked at <a href=\"https://www.google.com/url?q=https://ought.org/&amp;sa=D&amp;source=editors&amp;ust=1662684319716540&amp;usg=AOvVaw3wa41vK0tyh5YHa1MGqhYB\"><u>Ought</u></a>&nbsp;applying AI to improve open-ended reasoning.</p><h1>How much weight should we give to these aggregates?</h1><p>My personal tier list for how much weight I give to AI x-risk forecasts to the extent I defer:</p><ol><li>Individual forecasts from people who seem to generally have great judgment, and have spent a ton of time thinking about AI x-risk forecasting e.g. <a href=\"https://www.google.com/url?q=https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines&amp;sa=D&amp;source=editors&amp;ust=1662684319717134&amp;usg=AOvVaw0HC_Ybnw6hMGjZm1FFu3tO\"><u>Cotra</u></a>, Carlsmith</li><li>Samotsvety aggregates presented here</li><li>A superforecaster aggregate (I\u2019m biased re: quality of Samotsvety vs. superforecasters, but I\u2019m pretty confident based on personal experience)</li><li>Individual forecasts from AI domain experts who seem to generally have great judgment, but haven\u2019t spent a ton of time thinking about AI x-risk forecasting (this is the one I\u2019m most uncertain about, could see anywhere from 2-4)</li><li>Everything else I can think of I would give little weight to.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv6pyn4x7wr8\"><sup><a href=\"#fnv6pyn4x7wr8\">[2]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefygoqnwl4id\"><sup><a href=\"#fnygoqnwl4id\">[3]</a></sup></span></li></ol><h1>Acknowledgments</h1><p>Thanks to Tolga Bilge, Juan Cambeiro, Molly Hickman, Greg Justice, Jared Leibowich, Alex Lyzhov, Jonathan Mann, Nu\u00f1o Sempere, Pablo Stafforini, and Misha Yagudin for making forecasts.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqfvfbsamfmh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqfvfbsamfmh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Unlike the WWOTF question, this includes any existential catastrophe caused by AI and not just misaligned takeovers (this is a non-negligible consideration for me personally and I\u2019m guessing several other forecasters, though I do give most weight to misaligned takeovers).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv6pyn4x7wr8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv6pyn4x7wr8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Why do I give little weight to Metaculus\u2019s views on AI? Primarily because of the <a href=\"https://forum.effectivealtruism.org/posts/S2vfrZsFHn7Wy4ocm/bottlenecks-to-more-impactful-crowd-forecasting-2%23Failure_modes1&amp;sa=D&amp;source=editors&amp;ust=1662684319718320&amp;usg=AOvVaw1fAtfmtM1lGgAFVVVgT0j2\"><u>incentives</u></a>&nbsp;to make&nbsp;very shallow forecasts on a ton of questions (e.g. probably &lt;20% of Metaculus AI forecasters have done the equivalent work of reading the Carlsmith report), and secondarily that forecasts aren\u2019t aggregated from a select group of high performers but instead from anyone who wants to make an account and predict on that question.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnygoqnwl4id\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefygoqnwl4id\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Why do I give little weight to AI expert surveys such as <a href=\"https://www.google.com/url?q=https://arxiv.org/abs/1705.08807&amp;sa=D&amp;source=editors&amp;ust=1662684319718836&amp;usg=AOvVaw1Q-hjAYd-UOUc6_RsA4Pjl\"><u>When Will AI Exceed Human Performance? Evidence from AI Experts</u></a>? I think most AI experts have incoherent and poor views on this because they don\u2019t think of it as their job to spend time thinking and forecasting about what will happen with very powerful AI, and many don\u2019t have great judgment.</p></div></li></ol>", "user": {"username": "elifland"}}, {"_id": "DXuwsXsqGq5GtmsB3", "title": "AI alignment with humans... but with which humans?", "postedAt": "2022-09-08T23:43:49.753Z", "htmlBody": "<p>Caveat: This post probably raises a naive question; I assume there's at least a 70% chance it's been considered (if not answered) exhaustively elsewhere already; please provide links if so. &nbsp;I've studied evolutionary psych &amp; human nature for 30 years, but am a relative newbie to AI safety research. Anyway....</p><p>When AI alignment researchers talk about 'alignment', they often seem to have a mental model where either (1) there's a single relevant human user whose latent preferences the AI system should become aligned with (e.g. a self-driving car with a single passenger); or (2) there's all 7.8 billion humans that the AI system should be aligned with, so it doesn't impose global catastrophic risks. In those relatively simple cases, I could imagine various current alignment strategies, such as cooperative inverse reinforcement learning (CIRL) being useful, or at least a vector in a useful direction.</p><p>However, there are large numbers of intermediate-level cases where an AI system that serves multiple humans would need to become aligned with diverse groups of users or subsets of humanity. And within each such group, the humans will have partly-overlapping but partly-conflicting interests.&nbsp;</p><p>Example 1: a smart home/domestic robot AI might be serving a family consisting of a mom, a dad, an impulsive teenage kid, a curious toddler, and an elder grandparent with Alzheimer's. Among these five humans, whose preferences should the AI try to align with? It can't please all of them all the time. They may have genuinely diverging interests and incommensurate preferences. So it may find itself in much the same position as a traditional human domestic servant (maid, nanny, butler) trying to navigate through the household's minefield of conflicting interests, hidden agendas, family dramas, seething resentments, etc. Such challenges, of course, provide much of the entertainment value and psychological complexity of TV series such as 'Downtown Abbey', or the P.G. Wodehouse 'Jeeves' novels.&nbsp;</p><p>Example 2: a tactical advice AI might be serving a US military platoon deployed near hostile forces, doing information-aggregation and battlefield-simulation services. The platoon includes a lieutenant commanding 3-4 squads, each with a sergeant commanding 6-10 soldiers. The battlefield also includes a few hundred enemy soldiers, and a few thousand civilians. Which humans should this AI be aligned with? The Pentagon procurement office might have intended for the AI to maximize the likelihood of 'victory' while minimizing 'avoidable casualties'. But the Pentagon isn't there to do the cooperative inverse reinforcement learning (or whatever preference-alignment tech the AI uses) with the platoon. The battlefield AI may be doing its CIRL in interaction with the commanding lieutenant and their sergeants -- who may be somewhat aligned with each other in their interests (achieve victory, avoid death), but who may be quite mis-aligned with each other in their specific military career agendas, family situations, and risk preferences. The ordinary soldiers have their own agendas. And they are all constrained, in principle, by various rules of engagement and international treaties regarding enemy combatants and civilians -- whose interests may or may not be represented in the AI's alignment strategy. &nbsp;</p><p>Examples 3 through N could include AIs serving various roles in traffic management, corporate public relations, political speech-writing, forensic tax accounting, factory farm inspections, crypto exchanges, news aggregation, or any other situation where groups of humans affected by the AI's behavior have highly divergent interests and constituencies.</p><p>The behavioral and social sciences focus on these ubiquitous conflicts of interest and diverse preferences and agendas that characterize human life. This is the central stuff of political science, economics, sociology, psychology, anthropology, and media/propaganda studies. I think that to most behavioral scientists, the idea that an AI system could become aligned simultaneously with multiple diverse users, in complex nested hierarchies of power, status, wealth, and influence, would seem highly dubious.</p><p>Likewise, in evolutionary biology, and its allied disciplines such as evolutionary psychology, evolutionary anthropology, Darwinian medicine, etc., we use 'mid-level theories' such as kin selection theory, sexual selection theory, multi-level selection theory, etc to describe the partly-overlapping, partly-divergent interests of different genes, individuals, groups, and species. &nbsp;The idea that AI could become aligned with 'humans in general' would seem impossible, given these conflicts of interest.</p><p>In both the behavioral sciences and the evolutionary sciences, the best insights into animal and human behavior, motivations, preferences, and values often involve some game-theoretic modeling of conflicting interests. And ever since von Neumann and Morgenstern (1944), it's been clear that when strategic games include lots of agents with different agendas, payoffs, risk profiles, and choice sets, and they can self-assemble into different groups, factions, tribes, and parties with shifting allegiances, the game-theoretic modeling gets very complicated very quickly. Probably too complicated for a CIRL system, however cleverly constructed, to handle.</p><p>So, I'm left wondering what AI safety researchers are really talking about when they talk about 'alignment'. Alignment with whoever bought the AI? Whoever users it most often? Whoever might be most positively or negatively affected by its behavior? Whoever the AI's company's legal team says would impose the highest litigation risk?</p><p>I don't have any answers to these questions, but I'd value your thoughts, and links to any previous work that addresses this issue.&nbsp;</p>", "user": {"username": "geoffreymiller"}}, {"_id": "qhX3WJbwMuNttkYcp", "title": "How to Get Housing in Berkeley", "postedAt": "2022-09-08T20:47:45.451Z", "htmlBody": "<h2>Staying with EA\u2019s</h2><h3>Short-Term Housing</h3><ul><li><a href=\"https://rajlearns.com/contact-me-DR8O5m7OXY3o\">Ask me</a> if I know any local availabilities<ul><li>have a low bar to asking</li></ul></li><li><a href=\"https://www.facebook.com/groups/2266502166822026\"><u>East Bay EA/Rationality Housing Board Facebook Group</u></a> is a good place to ask if anyone has rooms/couches open</li><li><a href=\"https://forum.effectivealtruism.org/posts/4zHWQNzCusaTfD7jz/ea-houses-live-or-stay-with-eas-around-the-world\"><u>Non-Linear\u2019s EA Houses project</u></a> is the EA equivalent of couchsurfing<ul><li>You can find a <a href=\"https://docs.google.com/spreadsheets/d/1wwNn1VEaQuSzDdedCVcmjktxeZj-apHele4Ry8DLJk0/edit#gid=0&amp;fvid=102684925\"><u>spreadsheet</u></a> with Bay Area options here</li></ul></li></ul><h3>Long-term EA Housing</h3><p>Options:</p><ul><li><a href=\"https://www.facebook.com/groups/2266502166822026\"><u>East Bay EA/Rationality Housing Board Facebook Group</u></a> often has vacancies posted</li><li>Again, <a href=\"https://rajlearns.com/contact-me-DR8O5m7OXY3o\">ask me</a> if I know any openings<ul><li>have a low bar to asking</li></ul></li></ul><h2>Non-EA Housing</h2><h3>Short-Term Housing</h3><ul><li><a href=\"https://www.airbnb.com/\"><u>Airbnb</u></a><ul><li>expensive</li></ul></li><li><a href=\"https://www.google.com/travel/hotels\"><u>Hotels</u></a><ul><li>even more expensive :(</li></ul></li><li>Facebook often has people looking to do short-term sublets, usually posted in groups like<ul><li><a href=\"https://www.facebook.com/groups/303241339725481/\"><u>Bay Area Conscious Community Housing Board</u></a></li><li><a href=\"https://www.facebook.com/groups/953139261371416/?hoisted_section_header_type=recently_seen&amp;multi_permalinks=5767668813251746\"><u>Berkeley Housing Group</u></a></li><li><a href=\"https://www.facebook.com/groups/128476910881473/?hoisted_section_header_type=recently_seen&amp;multi_permalinks=1586523335076816\"><u>UC Berkeley Off-Campus Housing</u></a></li></ul></li></ul><h3>Long-Term Housing</h3><ul><li><a href=\"https://www.zillow.com/\"><u>Zillow</u></a><ul><li>Pretty standard housing search app</li></ul></li><li><a href=\"https://apartments.com/\"><u>Apartments.com</u></a><ul><li>Pretty standard housing search app</li></ul></li><li><a href=\"https://sfbay.craigslist.org/search/oakland-ca/apa?lat=37.8466&amp;lon=-122.2409&amp;search_distance=6.4#search=1~list~0~0\"><u>Craigslist</u></a><ul><li>Can be a little sketchier than other apps but is how I found my current 8 bedroom house</li></ul></li><li>Facebook groups also have people looking to have their lease taken over/fill up vacancies in grouphouses<ul><li><a href=\"https://www.facebook.com/groups/303241339725481/\"><u>Bay Area Conscious Community Housing Board</u></a></li><li><a href=\"https://www.facebook.com/groups/953139261371416/?hoisted_section_header_type=recently_seen&amp;multi_permalinks=5767668813251746\"><u>Berkeley Housing Group</u></a></li><li><a href=\"https://www.facebook.com/groups/128476910881473/?hoisted_section_header_type=recently_seen&amp;multi_permalinks=1586523335076816\"><u>UC Berkeley Off-Campus Housing</u></a></li></ul></li></ul><p>To Add:</p><ul><li>Calibrations on general Berkeley rent cost</li><li>Recommended areas of Berkeley to stay in</li></ul>", "user": {"username": "rajlego"}}, {"_id": "aZhQhsHX4PyxCCpkz", "title": "The Maximum Impact Fund is now the Top Charities Fund", "postedAt": "2022-09-08T19:31:26.318Z", "htmlBody": "<p><em>Author: Elie Hassenfeld, GiveWell Co-Founder and Chief Executive Officer</em></p>\n<p>We\u2019ve decided to rename the Maximum Impact Fund to better describe what opportunities this fund supports. The Maximum Impact Fund will now be called the <a href=\"https://www.givewell.org/top-charities-fund\">Top Charities Fund</a>.</p>\n<p>We recently <a href=\"https://blog.givewell.org/2022/08/17/changes-to-top-charity-criteria/\">announced</a> changes to our top charity <a href=\"https://www.givewell.org/how-we-work/criteria#Additional_Criteria_for_Top_Charities\">criteria</a> that include a new requirement for our top charities: that we have a high degree of confidence in our expectations about the impact of their programs. Alongside this update, we also introduced a new giving option, the <a href=\"https://www.givewell.org/research/all-grants\">All Grants Fund</a>. The All Grants Fund supports the full range of GiveWell\u2019s grantmaking and can be allocated to any grant that meets our cost-effectiveness bar\u2014including opportunities outside of our top charities and riskier grants with high expected value.<sup class=\"footnote-ref\"><a href=\"#fn-igTjCw7ANk3cweCzQ-1\" id=\"fnref-igTjCw7ANk3cweCzQ-1\">[1]</a></sup></p>\n<p>The new All Grants Fund is a complement to what we have called our Maximum Impact Fund, which is granted to cost-effective opportunities among our <a href=\"https://www.givewell.org/charities/top-charities\">top charities</a>. However, we\u2019ve received feedback that describing the fund that supports grantmaking only to our top charities as having \u201cMaximum Impact\u201d is confusing in light of the opportunity to support a wider range of opportunities (with potentially higher expected value) through the All Grants Fund.</p>\n<p>Based on this feedback, we\u2019ve decided to change the name of the Maximum Impact Fund to the <a href=\"https://www.givewell.org/top-charities-fund\">Top Charities Fund</a>.</p>\n<p>Only the name is changing\u2014we aren\u2019t making any further changes to the underlying fund beyond those discussed in <a href=\"https://blog.givewell.org/2022/08/17/changes-to-top-charity-criteria/\">our previous post</a>, so any incoming donations designated for the Maximum Impact Fund will automatically be allocated to the Top Charities Fund. We will continue to use donations to this fund to support the highest-priority funding needs among our top charities each quarter. As before, we will apply the same cost-effectiveness bar across our grantmaking, regardless of whether the funding comes from the All Grants Fund or Top Charities Fund.</p>\n<p>We appreciate everyone who provided feedback and hope that the new name provides more clarity about how donations to our funds will be used.</p>\n<p>More details about our funds and their impact are <a href=\"https://www.givewell.org/our-giving-funds\">here</a>.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-igTjCw7ANk3cweCzQ-1\" class=\"footnote-item\"><p>The <a href=\"https://conceptually.org/concepts/expected-value\">expected value</a> of a grant is the value of the grant\u2019s outcomes multiplied by the probability that those outcomes will be realized. <a href=\"#fnref-igTjCw7ANk3cweCzQ-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "GiveWell"}}, {"_id": "KkPo9cpLThWyd7yju", "title": "Save the Date: EAGx LatAm", "postedAt": "2022-09-08T18:22:10.288Z", "htmlBody": "<p>EAGx LatAm will take place in <strong>Mexico City at the </strong><a href=\"http://www.museoshaghenbeck.mx/museo-casa-de-la-bola/\"><strong><u>Casa de la Bola Museum</u></strong></a><strong>, 6<sup>th</sup>-8<sup>th</sup> of January</strong>. One of our goals is<strong> to connect the LatAm community with the broader international community.</strong> So, although this event is primarily for the Spanish- and Portuguese-speaking EA communities, applications from experienced members of the international community are very welcome! What better way to start 2023?</p><p>This will overlap with the<a href=\"https://forum.effectivealtruism.org/posts/kBgMD7GwJboxGFvsp/mexico-ea-fellowship\"> <u>EA Fellowships and Visitors Programme</u></a> that we\u2019re running in Mexico 1<sup>st</sup> Nov- 30<sup>th</sup> Jan with the aim of kickstarting an EA Hub in the city. So expect great EA vibes on top of fantastic local (vegan) cuisine and fun cultural activities like city tours and regional dance lessons.</p><p>Applications will open in October.</p><h2><strong>Why EAGx LatAm?</strong></h2><p>1) The EA movement in LatAm has grown over the last couple of years, with new local and university groups in <a href=\"https://www.altruismoeficaz.org/grupos\"><u>Mexico, Chile and Colombia</u></a> joining already well-established groups like <a href=\"https://altruismoeficaz.com.br/\"><u>EA Brazil</u></a>. This EAGx is an occasion to foster a sense of community and connect people who are facing similar opportunities and challenges.</p><p>2) Many people from Latin America experience barriers to attending international EAG(x) conferences in the US or Europe, including visa problems and inconvenient and expensive international connections. As a result, the international community is missing out on talent and insights from a sizable part of the population, while the LatAm community misses on opportunities and valuable networks.</p><p>With this in mind, the conference aims to:</p><p>-Raise the profile of EA in LatAm.</p><p>-Create and strengthen connections between EAs within LatAm, the Spanish- and Portuguese-speaking communities.</p><p>-Create and strengthen connections between EAs in LatAm and the rest of the world.</p><h2><strong>Who is this conference for?</strong></h2><p>This conference is for you if you\u2026:</p><p>-Are from LatAm, new to EA and looking forward to learning more and connecting with like-minded people. Most of our talks will be introductory and cover a broad range of topics.</p><p>-Are from LatAm, very much not new to EA, and excited to meet other members of the community.</p><p>-Are an experienced member of the community from outside LatAm and keen to connect with EAs who might be under your radar.</p><h2><strong>I don\u2019t speak Spanish, is this event for me?</strong></h2><p>Yes! First of all, although the lead organisers (Sandra Malag\u00f3n and Laura Gonz\u00e1lez) are also the coordinators of the Spanish-speaking community, we\u2019re in touch with the Brazilian community, and our goal is to welcome people interested in EA across LatAm and the world at large, regardless of language(s) spoken. Most talks will use English as a <i>lingua franca</i>, with the exception of a few topic-specific talks and workshops like \u201cCommunity building in the Portuguese/Spanish-speaking world\u201d, which will be run in the relevant languages. We will have a system of stickers or colour-coded tags for participants to signal the languages they speak.</p><p>If you\u2019re unsure about whether to apply, <strong>err on the side of applying.</strong></p><p>If you have any questions or comments don\u2019t hesitate to write to latam@eaglobalx.org . We accept emails in English, Spanish and Portuguese :)</p><p>See you in Mexico!</p><p>The Organising Team</p>", "user": {"username": "LGlez"}}, {"_id": "uxfwaQaPq2martJAv", "title": "Suggestions for how to talk about pregnancy losses", "postedAt": "2022-09-08T17:57:11.541Z", "htmlBody": "<p>The advice on how to talk to a person who\u2019s experienced a pregnancy loss seems generally pretty bad to me. That\u2019s a great shame because these losses are a hard thing to talk about but also a common occurrence. Having had a miscarriage and a late term stillbirth, I\u2019ve thought a bit about ways of communicating about them in ways that land well for people, and I thought that might be useful to share.&nbsp;</p><p><br>\u2018Pregnancy loss\u2019 covers a wide range of experiences, including early stage miscarriages, abortions and still births. Most of this is aimed at how to talk to the person who was physically pregnant.&nbsp;</p><p>What follows is most applicable for people you know well - for talking to friends or colleagues. If you\u2019re meeting someone you don\u2019t know too well at an event who you know has suffered a pregnancy loss, it\u2019s quite likely that the right approach is simply to let them go about the event as normal, rather than bringing it up. Talking about it can be pretty hard for people, particularly with someone they don\u2019t know that well and when they weren\u2019t expecting to.&nbsp;</p><h3>&nbsp;</h3><h3>People\u2019s experiences differ widely</h3><p>People experience pregnancy losses really differently on the spectrum from \u2018I\u2019m sick in a particularly painful/private way and my future suddenly looks really different\u2019 to \u2018a person I\u2019ve been thinking a lot about for months has died\u2019. That means people\u2019s ideal way of talking about it / being comforted is pretty different. On the other end you want to focus just on the people losing the pregnancy: \u2018How are you feeling? Are you in a lot of pain?\u2019. On the other end are things like \u2018I\u2019m sorry for your loss. Did you name her?\u2019.&nbsp;</p><p>The ideal is to figure out how the person is experiencing the pregnancy loss, and try to mirror that. Before that, you might want to keep with general comments along the lines of \u2018I\u2019m really sorry; that must be really tough; how are you doing?\u2019.&nbsp;</p><p>Also, people\u2019s experiences can change day to day. And they don\u2019t necessarily even know themselves how they\u2019re experiencing it. So the ideal is to be open-mindedly sympathetic and hold space for the person to feel anywhere from fine to awful.</p><h3>&nbsp;</h3><h3>Beware of escalating grief narratives</h3><p>A failure mode that happened with me a bunch after the stillbirth was a kind of escalating cycle of everyone upping the amount to treat the pregnancy loss as a death. For people talking to me, it may have seemed better to err on the side of acknowledging the loss of a baby because it\u2019s worse to fail to acknowledge a loss than to acknowledge one that didn\u2019t happen. But that caused me to feel callous to not match their level of acknowledging a loss. That\u2019s actually pretty bad. The reason is that the people undergoing the pregnancy loss don\u2019t actually know how to feel about it, feel differently at different times, and so can be quite affected by different narratives. The more other people treat it like someone you know died, the more it can feel like they did. That can feel more sad than the alternative (because you start feeling an increased sense of loss) or it can cause guilt (\u2018given that someone I know died, shouldn\u2019t I be feeling more grief for them and less upset about the effects on me and my future?\u2019).</p><p>As far as I can tell it used to be that people\u2019s sense of \u2018losing a person\u2019 was mostly ignored, so now there are various things put in place to try to compensate for that, and medical staff are trained to try to acknowledge people might be feeling it as a death. Society also is more used to talking about good things and also to pregnancies being successful. So narratives that avoid talking about the pregnancy loss as a death feel less available/salient.&nbsp;</p><h3>&nbsp;</h3><h3>Specific narratives people might be experiencing</h3><p>Here\u2019s some ways people might experience things, that could give you a sense of what kind of sympathy you might express that don\u2019t assume the narrative of a death:&nbsp;</p><ul><li>\u2018Over the last many months it\u2019s been continuously on my mind that I have to change my behaviour in a lot of ways to avoid this specific situation (no alcohol / shower not too hot / \u2018can I eat this kind of cheese/fish?\u2019 / don\u2019t forget to wash salad and fruit particularly carefully). The precise thing I\u2019ve been trying so hard to avoid has just happened.</li><li>I\u2019ve been making specific plans I was looking forward to, though with trepidation. My next year suddenly looks entirely different and emptier.</li><li>This pain is a lot, and also kind of at my core. But I\u2019m not really allowed to talk about the parts of me that hurt because they\u2019re private. The hospital and midwives didn\u2019t really want to give me as much pain medication as seemed like would usually be warranted for the amount of pain. That implies this pain is somehow good, but I don\u2019t see a way in which it\u2019s good.&nbsp;</li><li>Confusion: I feel like something\u2019s changed and missing, and yet it\u2019s actually more that nothing\u2019s changed and nothing is going to change. Given that, why do I feel like this?</li></ul><p>I also found that my way of experiencing it changed day by day. Eg sometimes it seemed terrible and sometimes it seemed like not much had happened and I felt guilty for not feeling sadder. That meant it was pretty good for people who I had talked to before still to approach how I was doing on a particular day with an open mind.&nbsp;</p><h3>&nbsp;</h3><h3>What you might say</h3><p>The thing I most appreciated was people being open to me experiencing this however I happened to experience it:</p><ul><li>Is it feeling like the world is crashing down, or is it feeling like not much has happened?&nbsp;</li><li>Would you like to be distracted by doing something fun and exciting, or go back to work, or be left alone?</li><li>Would you like to talk about how you\u2019re doing physically? Or about how you have to change your plans for the coming months? Or about other things entirely?<br>&nbsp;</li></ul><p>Another dynamic that was true of us was that my husband felt like he had had very little direct interaction with the pregnancy, so his main orientation towards the situation was something really bad having happened to me. So the relevant questions to ask him might have been things like \u2018how is Michelle?\u2019.</p><p>It can feel pretty bad talking about the specifics of what happened, whether they\u2019re known or not. It can make you focus on things you\u2019d rather not (\u2018the foetus was underweight\u2019 is hard to say without viscerally reminding you there was an almost-baby), or remind you of ruminations (\u2018they don\u2019t know why it happened; was it something I did? How long will the autopsy take?\u2019). So rather than asking \u2018what happened?\u2019 you might want to show interest while giving full affordance not to give any details \u2018would you like to talk about what happened?\u2019, or instead focus questions on things like \u2018how are you both feeling?\u2019.<br>&nbsp;</p>", "user": {"username": "Michelle_Hutchinson"}}, {"_id": "bXKNrYnr4gFKYzLGd", "title": "Incentivising people to learn about plant based eating by paying them.", "postedAt": "2022-09-08T11:20:52.103Z", "htmlBody": "<p>I am with this post requesting feedback from experienced EA:s engaged in research, funding, animal welfare and behaviouralism, Could this project be worth working on and if not - discuss alternative solutions.&nbsp;</p><p><strong>Story: </strong>On some Australian long distance highways there are \u201cDrive-and-Revive\u201c stops, where you get coffee and a snack for free, with the intent of drivers taking a rest from their long drives. The idea is that the cost on society to provide snacks is lower than the potential cost on society of someone having an accident.&nbsp;</p><p>Can it be so that the cost of supporting free plant based cooking education will be lower than the cost on society for healthcare on unhealthy eating, non human animals dying and the planet suffering etc?&nbsp;</p><p><strong>Goal:</strong> Spread the plant-based diet (PBD).</p><p><strong>Why: </strong><a href=\"https://forum.effectivealtruism.org/posts/aMFFWhiQX5DvaZSDp/the-tipping-point-case-for-vegan-advocacy\">For the sake of the planet and its inhabitants.&nbsp;</a></p><p><strong>Ethos:</strong> Be understanding and forgiving about how difficult the transition to a PBD is and use realistic incentives to encourage the change: peoples\u2019 self interest. No shaming. Reward effort, not performance.&nbsp;</p><p><strong>Problem: </strong><a href=\"https://forum.effectivealtruism.org/posts/wwjyynGkh9uqF4CLq/new-faunalytics-study-on-the-barriers-and-strategies-for\">Transitioning to a PBD is hard</a> and requires substantial behaviour change, which is challenging already for someone with bandwidth to do it, but much harder for someone with lack of time and/or finances. We can not expect anyone to go through these changes without help or without correct incentive. Some, but not all, aspects that require change are:</p><p>-Motivation -Planning -Shopping &nbsp;-Cooking -Flavours -Digestion -Storage</p><p><strong>Project main activity:</strong> Use an existing <i>or</i> new movement to motivate and alleviate the transition to a PBD for consumer, group and societal levels. A backed-by-science curriculum gets developed with the help of psychologists, nutritionists, environmentalists, chefs, neuroscientists etc. It gets tested and reiterated. It contains actionable advice about how to go through the process of eating more plants, waste less, how to avoid future disease and the consequences of an unhealthy lifestyle.&nbsp;</p><p>- Teachers provide courses and coaching using the curriculum for consumers.&nbsp;</p><p>-Advisors provide free coaching on how to provide a PBD for groups like restaurants, catering, organisations and other corporations.&nbsp;</p><p>-Movement lobbyists and influencers meet with decision makers on societal level.</p><p><strong>Price: </strong>There can be no cost for the participants - as that generally will be a friction point too large for them to want to learn. On the contrary - they should be rewarded financially or with food products to learn. Only when given a strong incentive can you motivate otherwise uninterested consumers to want to make an effort in PB living. Measures can be taken to avoid people taking advantage of the system.</p><p><strong>Question: </strong>Could the project be cost effectively funded by donors, governments and potentially sponsors (like PB food companies etc), motivated by reducing suffering and cost on society?</p><p><strong>In my opinion:</strong> I care deeply about animal welfare, but in my opinion it needs to be separated from the curriculum. Shaming people into stop changing behaviour is not effective. I can highly recommend listening to <a href=\"https://theproof.com/vegans-we-need-to-talk-with-psychologist-melanie-joy/\">this podcast</a> with psychologist and author Melanie Joy, in which she goes over how to communicate with non vegans, and how to contribute to the solution even though you are not able to become vegan.</p><p><strong>Finally: </strong>I\u2019m looking forward to tapping into the great accumulated knowledge of this forum\u2019s participants. This is my first post, I\u2019m an aspiring EA and not in academia, so I\u2019m sure I\u2019ll learn a lot from your feedback. Thank you.</p>", "user": {"username": "JB"}}, {"_id": "D6oKe6kjHeBeiCtWn", "title": "Developing Next Gen PPE to Reduce Biorisks", "postedAt": "2022-09-08T09:28:56.692Z", "htmlBody": "<p>TL;DR We are intending to run a feasibility study to establish requirements and a theory of change for novel \u2018Super PPE\u2019. Assuming this concludes positively we will develop Super PPE to reduce GCBR risk.</p><p>This post builds on the<a href=\"https://forum.effectivealtruism.org/posts/u5JesqQ3jdLENXBtB/concrete-biosecurity-projects-some-of-which-could-be-big-1?fbclid=IwAR2vRFW43OfXy4_JtdDwKxY-L3rFOUhnE0p7Ks6OGzIvBmc_tShICoT734k\"> <u>11th Jan post on concrete biosecurity projects</u></a> by ASB, ECA, and the<a href=\"https://forum.effectivealtruism.org/posts/DcKo3Hx8hzrZWjYp5/list-of-lists-of-concrete-biosecurity-project-ideas#comments\"> <u>\u2018list of lists</u></a>\u2019 by TA, specifically with regard to \u2018Super PPE\u2019. Thank you to the authors of these posts and all the others that have helped with our research so far, particularly Vivian Belenky and their work with the NextGen PPE group, Cass Springer and his knowledge of the area, and Caleb Parikh for advice and for reviewing this post.</p><p>&nbsp;</p><p><strong>What is the context?</strong></p><p>Global Catastrophic Biological Risks (GCBRs) present a known and well documented threat - both in an existential form, and as s-risks from lesser but still greatly damaging pandemics or other biological events <a href=\"https://80000hours.org/problem-profiles/preventing-catastrophic-pandemics/\"><u>[1]</u></a>.</p><p>Many data, infrastructure, logistical, and biomedically based methods are in development to reduce catastrophic biorisk<a href=\"https://www.nti.org/analysis/articles/common-mechanism-prevent-illicit-gene-synthesis/\"> <u>[2]</u></a><a href=\"https://www.nti.org/analysis/articles/establishing-seal-approval-incentivize-adoption-biosecurity-norms/\"> <u>[3]</u></a><a href=\"https://www.naobservatory.org/\"> <u>[4]</u></a><a href=\"https://www.media.mit.edu/projects/secure-dna/overview/\"> <u>[5]</u></a>. However, there is a concerning lack of development of novel and next generation hardware and physical equipment in the same space.</p><p>One particular item in this area is <strong>Biohazard Personal Protective Equipment</strong>. In any potentially catastrophic biological event that breaches initial containment efforts, we have extremely high confidence that widespread use of high protection level PPE, likely akin to positive pressure BSL4 suits, would be required for recovery and continuation of civilisation (we don\u2019t have a source for this but literature in the area discusses it as if given).</p><p>Unfortunately, existing PPE of an appropriate protection level is not practical for widespread use during a catastrophic event. A quick list of some major issues with suits such as BSL4 suits and L5 Tychem suits:</p><ul><li>Incredibly bulky and usually poorly fitting -<a href=\"https://www.liebertpub.com/doi/full/10.1177/1535676018793151#bibr8-1535676018793151\"> <u>Restrictive in motion and visibility</u></a></li><li>Incredibly hot &amp; humid<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4648492/\"> <u>[6]</u></a>. (This is often reported as an issue in climate controlled labs, let alone out in the real world;<a href=\"http://smartppe.org/\"> <u>Ebola responders often couldn\u2019t work in an L5 suit for more than one hour due to heat</u></a>)</li><li>Prone to Damage of<a href=\"https://www.sciencedirect.com/science/article/pii/S2590053620300173\"> <u>suit fabric</u></a> (average of 3.5 damages over 6 months!) and hoses</li><li>Very Expensive: <a href=\"https://www.centerforhealthsecurity.org/our-work/pubs_archive/pubs-pdfs/2021/20211005-masks-and-respirators-for-the-21st-century.pdf\"><u>$1500</u></a>-5000+</li><li>Most require external hoses and immobile plant</li><li>Difficult to put on and take off</li><li>Limited reusability (in many cases)</li></ul><p>The development of \u2018Super PPE\u2019 was flagged earlier this year in the EA forums as a concrete biosecurity project. This is echoed by other organisations, such as the Apollo program for biodefence,<a href=\"https://biodefensecommission.org/reports/the-apollo-program-for-biodefense-winning-the-race-against-biological-threats/\"> <u>which has identified next-generation PPE as a technology priority for the next decade.</u></a></p><p>This is a sorely overdue development that could see a huge leap in technology/equipment quality, simply due to the fact that<a href=\"https://en.wikipedia.org/wiki/Positive_pressure_personnel_suit\"> <u>designs haven\u2019t significantly changed since 1979</u></a>.</p><p>We are aware of some efforts that have been made to improve on full body biohazard PPE,<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4330156/\"> <u>particularly relating to and following the 2014-16 Ebola outbreak</u></a>. Whilst these improvements are a brilliant start, there is still a long way to go until we reach global-catastrophe-ready suits.</p><p>&nbsp;</p><p><strong>What do we propose?</strong></p><p>In order to make substantial progress on Super PPE, the authors are preparing to apply for funding to perform an initial feasibility study into the design and development of next generation PPE for biorisks. Should the study return favourably (we have high confidence that it will), it will be followed by further stages to prove the concept, create prototypes, and hopefully scale to a final product within 2-5 years. We are intending to do this primarily through an<a href=\"https://www.amododesign.com/\"> <u>engineering consultancy that we run</u></a>, which enables access to engineers across several engineering disciplines (including Mechanical, Chemical, Bio, and Materials) and allows us to quickly scale product development work.</p><p>&nbsp;</p><p><strong>Why do a feasibility study?</strong></p><p>Following networking within the EA-sphere and preliminary research, our current understanding of the field is that some scoping of the wider problem has been undertaken by some EAs, and some early stage design/prototyping on a PAPR concept has been completed by Cass Springer. In order to gather momentum and make concrete progress on GCBR reduction, we want to set aside a block of our time to:</p><ul><li>Establish the real issues with existing PPE through:<ul><li>User research, case studies</li><li>Detailed critical analysis of existing products</li></ul></li><li>Work out how PPE can actually be used to reduce GCBRs<ul><li>With regard to engineering/technological requirements</li><li>From a mechanistic perspective (organisation funding, distribution, how is PPE expected to be used in a GCBR event etc.)</li><li>How GCBR scenarios may differ to normal use cases for PPE</li></ul></li><li>Plan next steps in order to make clear gains in the area and better prepare the world for biological risks.</li></ul><p>&nbsp;</p><p><strong>What we could use help with:</strong></p><p>Biorisk expertise &amp; advice to keep expanding our knowledge of the area, particularly regarding theoretical GCBR events, what they might look like, how civilization might adapt, how PPE might be used, etc.</p><p>Criticism of the premise - is this a worthwhile use of our time?</p><p>General thoughts, collaboration, and advice from teams and individuals that have worked on PPE in the past</p><p>Signposting to funding - we are currently intending to apply to the LTFF to fund the feasibility study. We plan to initially fund with grants, ideally until second generation prototypes, at which stage we would likely aim to secure VC funding (unless grant funding of an appropriate scale could be secured).</p><p>&nbsp;</p><p><strong>Some extra positives for the project:</strong></p><p>PPE for biorisks represents an opportunity for threat agnostic biorisk reduction. The physical barriers could not be easily overcome by the re-engineering or evolving of a hazard.</p><p>A physical suit which you put around yourself is about as tangible as you can get with regard to threat protection. This would likely make preparatory and reactive uptake &amp; use much more widespread than some of the more conceptual or high level alternatives.</p><p>This is very speculative, but the threat agnosticism may have potential to be powerful enough to provide protection from any airborne particulate based threat, possibly providing minor risk reduction against some low x-risk threats such as nuclear war or supervolcanic eruptions.</p><p>&nbsp;</p><p><strong>Extra notes &amp; Appendices:</strong></p><p><strong>As we are engineers, we couldn\u2019t help but start speculating and designing as our research progressed.</strong><a href=\"https://docs.google.com/document/d/1Y1f2-OMysvrSHhfJatVqR6nqmUPcB8-hNjotvOwUKaU/edit?usp=sharing\"> <u>See some of our speculative thoughts and drawings on Super PPE here :)</u></a></p><p>Due to the physical nature of the project, we expect this work to be less infohazardous than other work in this space, but we are new to the area and would appreciate thoughts on how our project may end up being net negative in the comments or via email if sensitive (<a href=\"mailto:andy.graham@amododesign.com\"><u>andy.graham@amododesign.com</u></a> , <a href=\"mailto:tom.milton@amododesign.com\"><u>tom.milton@amododesign.com</u></a>)</p>", "user": {"username": "AndyGraham"}}, {"_id": "aRj3yqbwGJigvq3DG", "title": "How Useful is Utilitarianism?", "postedAt": "2022-09-08T00:01:01.537Z", "htmlBody": "<p>Over on my substack, I ask: <i>how might utilitarian-leaning philosophers seek to do more good through their work</i>?</p><p>There's recently been <a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous?commentId=KY3nNJQe4v2Ak4A8r\">some skeptical discussion</a> of the value of utilitarianism here on the forum. Philosophers have long been fascinated by the idea that the view might be<i> self-effacing</i>. But <strong>I don't think such a negative verdict is empirically supported.</strong> Utilitarians and utilitarian-influenced individuals (from Bentham, Mill, and Singer, to Toby Ord, Will MacAskill, and <s>Sam Bankman-Fried</s> oops that didn't age well...) have demonstrably done immense good -- not least in giving rise to EA itself. &nbsp;What utilitarian-influenced figures have done comparable harm? (And how many non-utilitarian philosophers have done better?)</p><p><strong>Given this track record</strong>, I think our credences should strongly favour the view that utilitarianism is a force for good in the world, and that shifting more people in a utilitarian direction would, on current margins, generally be a good thing. (Though, as I note in my <a href=\"https://rychappell.substack.com/p/how-useful-is-utilitarianism\">linked post</a>, the general public isn't necessarily interested in moral theory, so in many contexts I think it'd make more sense to advocate for a more limited view--such as <a href=\"https://rychappell.substack.com/p/beneficentrism\">beneficentrism</a>--rather than for full-blown utilitarianism.)</p><p>If this is correct, it's worth asking <i>what utilitarian-leaning philosophers can do</i>, as moral philosophers, to advance utilitarianism as a force for good in the world. &nbsp;I call this endeavour <i>the beneficence project</i>.</p><figure class=\"image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0bc1878-1d8c-4a82-922e-ee79aa1fa3eb_1024x1024.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0bc1878-1d8c-4a82-922e-ee79aa1fa3eb_1024x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0bc1878-1d8c-4a82-922e-ee79aa1fa3eb_1024x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0bc1878-1d8c-4a82-922e-ee79aa1fa3eb_1024x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0bc1878-1d8c-4a82-922e-ee79aa1fa3eb_1024x1024.png 1456w\"><figcaption>Thinking constructively</figcaption></figure><p>My post then steps through four potential branches for such a Beneficence Project:</p><ul><li><strong>Pure research</strong> to better develop and test the details of the utilitarian worldview.</li><li><strong>Persuasively-oriented research</strong>, to better communicate to fellow academics the theory\u2019s virtues and responses to objections.</li><li><strong>General outreach</strong>, to students and the general public.</li><li><strong>Targeted outreach</strong>, to policymakers and policy-adjacent academics (e.g. in the public health community).</li></ul><h2>Pure research</h2><p>This will likely be the most intrinsically appealing \u201ccause area\u201d for my fellow philosophers, as it involves doing just what we tend to most enjoy anyway. And I think it is important! I\u2019m a big fan of <a href=\"http://yetterchappell.net/Helen/idealism-book.html\">even the most impractical pure philosophy</a> (so long as it\u2019s genuinely philosophically <i>interesting</i>). But even putting aside the purely intellectual value, I would expect (high quality) pure research in ethics to have significant long-run instrumental value by contributing to our collective understanding of moral (and decision) theory. It\u2019s important to test and develop the best ideas of both utilitarianism and its most promising competitors, to give us a better chance of accurately discerning what\u2019s really important and true.</p><h2>Persuasively-oriented research</h2><p>This category isn\u2019t wholly distinct from the first one, but involves some degree of deliberately addressing one\u2019s work to non-utilitarians (in contrast to purely \u201cinternal\u201d debates between utilitarians of subtly different stripes). Much of <a href=\"http://yetterchappell.net/Richard/papers.html\">my own work</a> falls under this category. I\u2019m interested in developing the strongest, most broadly persuasive case for a (<a href=\"https://www.philosophyetc.net/2021/03/three-dogmas-of-utilitarianism.html\">mostly</a>) utilitarian approach to ethics. Three projects I\u2019m currently most excited about [and will discuss more in future posts on my substack]:</p><ul><li>A paper developing a \u2018New Paradox of Deontology\u2019.</li><li>A paper arguing that <i>importance</i>, rather than <i>permissibility</i>, should be considered the central concept of normative ethics.</li><li>My book project on <i>Bleeding Heart Consequentialism</i>, which develops a sympathetic picture of the beneficent agent, to finally put to rest the awful caricature of the \u201ccold and calculating\u201d consequentialist agent.</li></ul><p>I\u2019d love to hear from others doing work in this same vein!</p><p>The potential instrumental value I see emerging from this sort of work is greater sympathy for (broadly) utilitarian ethics within the academy, which is important because an academic consensus has a tendency to eventually \u201ctrickle down\u201d to society at large (both through the teaching of undergraduates / professional masters students, and via quotes in the media or other \u201cpublic philosophy\u201d contributions from our academic colleagues).</p><h2>General outreach</h2><p>Here I think it makes an important difference whether we\u2019re targeting <i>philosophy students</i> or the <i>general public</i>. For philosophy students, I think it makes sense to talk about moral theory, and hence utilitarianism, since this is stuff they\u2019re going to be taught about anyway. To this end, I\u2019m really proud of the work we\u2019re doing on <a href=\"https://www.utilitarianism.net/\">utilitarianism.net</a> to present the theory in a clear, accessible, and sympathetic way.</p><p>(I think there\u2019s a lot of value to producing high-quality teaching materials that make it easier for professors to teach important topics well. I\u2019d also encourage other philosophers to check out <a href=\"https://www.stafforini.com/blog/effective-altruism-syllabi/\">EA syllabi</a>\u2014such as <a href=\"https://rychappell.substack.com/p/updated-syllabus-on-ealongtermism\">my recent syllabus on Effective Altruism and Longtermism</a>\u2014and consider teaching a similar course if they think it looks interesting.)</p><p>For the general public, it\u2019s less clear that moral theory per se is worth talking about. The practically important part of utilitarianism is just its <a href=\"https://rychappell.substack.com/p/beneficentrism\">beneficentrism</a> (which may also be shared by any other decent view). It\u2019s not as though we really want to encourage anyone to go around pushing people in front of trolleys. So in many contexts, I expect it would make a lot more sense to promote <a href=\"https://www.effectivealtruism.org/\">effective altruism</a> rather than utilitarianism.</p><h2>Targeted outreach</h2><p>Likewise for policy influence, I think what we really want to promote is not utilitarianism per se, but just a <i>beneficence-focused</i> ethic. (See: <a href=\"https://rychappell.substack.com/p/theory-driven-applied-ethics?s=w\">Theory-Driven Applied Ethics</a>.) You don\u2019t have to be a utilitarian to think that passing a cost-benefit analysis is at least a <i>necessary condition</i> for justifying <a href=\"https://rychappell.substack.com/p/beware-status-quo-risks\">restrictive medical policies</a>, for example.</p><p>So I\u2019d be keen to see a lot more <i>NY Times</i> op-eds and other elite-focused communications that explicitly foreground <i>beneficence</i> or <i>doing the most good</i> as key considerations that should guide policy choice and analysis.</p><p>Another promising avenue for (longer-term) positive influence in this area would be though producing better teaching materials specifically in <i>applied ethics</i> for \u201cprofessional\u201d masters students (in public policy, bioethics, etc.). <a href=\"https://www.philosophyetc.net/2021/01/lessons-from-pandemic.html\">As the pandemic made clear</a>, something has gone terribly wrong with how the public health community (for example) thinks about ethics and policy evaluation. I\u2019m not sufficiently familiar with <i>how</i> they got to this point to be well-positioned to prescribe solutions, but \u201cbetter teaching materials in applied ethics\u201d seems like a promising first step, at any rate (though much would then depend upon <i>securing uptake</i> from the relevant teachers). I welcome others\u2019 thoughts and proposals.</p><h2>Conclusion</h2><p>I think utilitarian philosophy can be useful, both for encouraging more beneficent behaviour in general, and for encouraging better public policies in particular. Within the academy, there\u2019s much to be done to develop and rehabilitate utilitarian moral theory, which is unjustly maligned in many quarters. Outside of the academy, encouraging a more <i>beneficence-focused</i> moral perspective could do even more good. Putting this all together suggests plenty of room for sympathetic philosophers to contribute to a <i>Beneficence Project</i> that explicitly seeks to use utilitarianism for good.</p><p>Comments / suggestions on this project conception would be most welcome! (How would you prioritize between the various project branches identified above? Do any strike you as potentially or likely counterproductive? What good ideas have I missed?)</p><p>For any moral philosophers in the audience: please also get in touch if you might be interested in contributing to (some aspect of) the project yourself. (It\u2019s always helpful to find more potential collaborators! The more people doing good things, the better, in my view\u2026)</p>", "user": {"username": "RYC"}}, {"_id": "Cpki4sE4whMkYiCR6", "title": "Has EA content been translated into other languages yet?", "postedAt": "2022-09-08T02:29:01.790Z", "htmlBody": "<p>There is a Futures Fund project about translating ea.org, the key ideas series, and The Precipice into new languages, starting with Spanish and Mandarin. Has this been done?</p>", "user": {"username": "morgan"}}, {"_id": "Bd4xeHeNgBywrofW6", "title": "'Psychology of Effective Altruism' course syllabus", "postedAt": "2022-09-07T17:31:10.660Z", "htmlBody": "<p>In case anyone's interested, here's a link to the syllabus for a &nbsp;course titled 'The Psychology of Effective Altruism' that I've taught 3 times at University of New Mexico (in 2018, 2019, and 2020): <a href=\"https://geoffrey-miller-y5jr.squarespace.com/s/syllabus-EA-2020-spring-final.docx\">https://geoffrey-miller-y5jr.squarespace.com/s/syllabus-EA-2020-spring-final.docx</a>&nbsp;</p><p>I'd posted an earlier version of this syllabus to EA Forum in 2018 (see <a href=\"https://forum.effectivealtruism.org/posts/GhHirpH9uzxKCr3Lx/new-effective-altruism-course-syllabus\">https://forum.effectivealtruism.org/posts/GhHirpH9uzxKCr3Lx/new-effective-altruism-course-syllabus</a> )</p><p>Feel free to&nbsp;borrow any of this material if you teach a course on EA, X risk, longtermism, moral psychology, etc.</p><p>This is&nbsp;an advanced undergraduate seminar with about 20-25 students, mostly psychology majors. It was designed to be suitable for diverse students at a large state university.&nbsp;Whereas some other EA courses have focused mostly on technical moral philosophy, this is pitched more towards the psychology of EA -- both why it's appealing, and why it's difficult.</p><p>The course's main topics are EA principles, cause prioritization, utilitarian ethics, moral psychology, charity evaluation, global poverty and health, existential risks, AI safety, moral &amp; cognitive enhancement, near-term technologies (robots, EMs, VR), animal sentience &amp; welfare, and career choices.&nbsp;</p><p>I'm currently updating this course to teach it again in spring 2023, so would welcome anybody else's syllabi that might be helpful, and any other suggestions of good readings, videos, exercises, etc. (I've already looked at the helpful course materials mentioned on this Open Philanthropy site: <a href=\"https://www.openphilanthropy.org/open-philanthropy-course-development-grants/\">https://www.openphilanthropy.org/open-philanthropy-course-development-grants/</a> )</p><p>-- Geoffrey</p>", "user": {"username": "geoffreymiller"}}, {"_id": "wwjyynGkh9uqF4CLq", "title": "New Faunalytics' Study On The Barriers And Strategies For Success When Going Vegan or Vegetarian", "postedAt": "2022-09-07T16:20:55.272Z", "htmlBody": "<p>When looking at the experiences of new vegans and vegetarians (for simplicity referred to collectively as veg*ns here), research organization <a href=\"http://www.faunalytics.org\"><u>Faunalytics</u></a> set out to identify barriers to their diet change and what strategies can help them be successful.&nbsp;</p><p><strong>Read the full study here: </strong>https://faunalytics.org/going-veg-barriers-and-strategies</p><p><strong>Background</strong></p><p>This is the third and final report in our series describing the results of Faunalytics\u2019 longitudinal study of new veg*ns. It focuses on the critical issue of barriers and supports facing people who start a new veg*n diet, as well as the effectiveness of various strategies.&nbsp;</p><p><strong>Research Team</strong></p><p>The project authors are Jo Anderson (Faunalytics) and Marina Milyavskaya (Carleton University). However, this project was a massive undertaking and could not have happened without the support of multiple individuals and organizations.&nbsp;</p><p>We are very grateful to Faunalytics volunteers Renata Hlavov\u00e1, Erin Galloway, Susan Macary, and Lindsay Frederick for their support and assistance with this work, as well as former Carleton student Marta Kolbuszewska and the dozens of animal advocates who helped with recruitment. We are also very thankful to Animal Charity Evaluators, the Social Science and Humanities Research Council (SSHRC), and VegFund for funding this research. Finally, we thank all of our survey respondents for their time and effort.&nbsp;</p><p><strong>Conclusions</strong></p><p><strong><u>Barriers</u></strong></p><p>This study examined a number of barriers to veg*n diet change that have been identified in previous research, with the goal of determining how they influence success over a moderate time period\u2014the first six months of one\u2019s new diet.&nbsp;</p><p>We identified three barriers as the most problematic because people who experience them when first trying to go veg*n were more likely to abandon the attempt within the first six months. Those three were:</p><ol><li>Feeling unhealthy on one\u2019s veg*n diet,</li><li>Not seeing veg*nism as part of one\u2019s identity, and&nbsp;</li><li>Believing that society sees veg*nism negatively.</li></ol><p>Additional problematic barriers were those associated with having more trouble reaching one\u2019s goal level of consumption, which included:</p><ol><li>Low autonomy support from friends and family,&nbsp;</li><li>Negative cultural influence,&nbsp;</li><li>Weak habit formation,</li><li>High cost,</li><li>Being ashamed of one\u2019s diet, and</li><li>Difficulty finding or preparing food.</li></ol><p>Dietary perfectionism\u2014not feeling satisfied unless following one\u2019s new diet perfectly\u2014was also associated with consumption success, such that people who were more perfectionist tended to get closer to their goal. However, we do not refer to low perfectionism as a barrier because while it may work well for people who chose it freely in this correlational study, perfectionism has a dark side and recommending it as a strategy could have harmful consequences (<a href=\"https://eprints.whiterose.ac.uk/91796/1/Sirois%20et%20al%202010%20PSPB.pdf\"><u>Sirois et al., 2010</u></a>). It should be tested experimentally before being considered a potential strategy.</p><p>For full details, see the section <i>Barriers and Supports for Successful Diet Change.</i></p><p><strong><u>Strategies&nbsp;</u></strong></p><p>Recommending strategies for diet maintenance can be as simple or as complicated as you like. At the simplest level, using more strategies and using them frequently is helpful: Just using strategies more often was predictive of consumption success. We also know from previous research into other types of goal pursuit that personal strategies people come up with themselves can be more effective than \u201cexpert\u201d strategies (<a href=\"https://www.sciencedirect.com/science/article/pii/S0022103121000925\"><u>Peetz &amp; Davydenko, 2021</u></a>), so it\u2019s a good idea to encourage people to try strategies that they think might work for them regardless of whether or not they appear on our list.</p><p>But it\u2019s also possible to get a lot more specific and take account of an individual\u2019s particular barriers, as outlined below.</p><p><i>Cost Strategies</i></p><p>Regardless of the barriers a person was experiencing, cost strategies were often associated with a lower likelihood of abandoning one\u2019s veg*n diet. This suggests that even when people don\u2019t identify cost as a concern or are dealing with other barriers, having affordable plant-based options available is important for diet maintenance.&nbsp;</p><p>Cost strategies included four individual strategies, of which one was most promising for success: Researching low-cost products (e.g., tofu). This doesn\u2019t mean that the others aren\u2019t useful\u2014they certainly may be, especially for some people or in combination with other strategies\u2014but if you are looking for a particular cost strategy to recommend to someone, helping them find low-cost products is the best option.</p><p><i>Motivation Strategies</i></p><p>Strategies for increasing motivation were effective for people with a range of barriers and were sometimes associated with a lower likelihood of abandoning one\u2019s veg*n diet, including for people who suffered from low motivation. These strategies appeared to help people with low motivation cut out animal products and make them less likely to abandon their diet.</p><p>For those considering motivation strategies to help with low motivation or for other reasons, any of them may help, but the following were the most promising:</p><ul><li>Learn more about animals that are used for food</li><li>Learn more about world hunger or social justice reasons for following a veg*n diet</li><li>Watch unpleasant or graphic images/video of farmed animals</li><li>Learn more about religious/spiritual reasons for following a veg*n diet</li><li>Learn more about health benefits of following a veg*n diet</li><li>Learn more about saving money by following a veg*n diet</li></ul><p><i>Health Strategies</i></p><p>Strategies for improving health effects were moderately effective. They helped people who had several different barriers, including people who were feeling unhealthy on their diet, get closer to their goal level of animal product consumption. However, these strategies did not appear to protect against diet abandonment, which is a key risk for people who feel unhealthy on their veg*n diet. This suggests that feeling unhealthy remains a difficult challenge to overcome, though using health strategies in combination with other strategies that reduce the risk of diet abandonment (cost and motivation strategies) may be protective.&nbsp;</p><p>Of the strategies we considered, two were identified as more promising than the rest:</p><ul><li>Research how to be healthy on a veg*n diet</li><li>Talk to a medical professional about your diet</li></ul><p>For the latter, however, we encourage advocates to let people know that not all medical professionals are up to date on the health benefits of plant-based diets, despite a wealth of evidence and direct recommendations to physicians to advise them (<a href=\"https://faunalytics.org/nutritional-update-for-physicians-plant-based-diets/\"><u>Tuso, 2013</u></a>).</p><p><i>Social Strategies</i></p><p>Social strategies were helpful for people who were experiencing most of the barriers we measured, making them the most flexible type of strategy. Most notably, they were helpful for people with the social barriers of low autonomy support (support from friends and family), negative influence from one\u2019s culture, or having a&nbsp; small veg*n network. Social strategies helped individuals with those barriers cut out animal products and get closer to their veg*n goals.</p><p>Unfortunately, however, social strategies were less effective for people who don\u2019t identify strongly as a veg*n, suggesting that advocates may need to suggest other strategies and find ways to increase identification. This is somewhat surprising, as we might expect that spending more time around other veg*ns would increase that identification.</p><p>For those considering social strategies, using any could help, but the following were the most promising:</p><ul><li>Participate in an online community (e.g., Facebook group) for people with diets similar to yours</li><li>Ask your family or friends to be supportive of your diet&nbsp;</li><li>Try to meet new people with diets similar to yours</li><li>Avoid people who are unsupportive or critical of your diet</li><li>Explain to your family or friends why this diet is important to you</li></ul><p><i>Ability Strategies</i></p><p>Strategies for improving one\u2019s ability to follow the diet were somewhat effective, helping people with a range of different barriers be successful. However, they had no apparent effect on people who were experiencing the ability-related barriers of difficulty finding or preparing veg*n food or having low personal control over food, indicating the challenge of overcoming these practical problems. Other research highlights the problem of systemic lack of access to healthy and affordable food in many areas (see <a href=\"https://foodispower.org/access-health/food-deserts/\"><u>Food Empowerment Project</u></a>), and this finding further illustrates that individual-level solutions to these problems may not exist.</p><p>For individuals who do have choices available, the following ability strategies can be recommended as the most promising:</p><ul><li>Research products (e.g., meat alternatives) that fit your diet</li><li>Switch to a restaurant, dining hall, etc., with better options for your diet</li><li>Switch to a grocery store with better options for your diet</li><li>Eat products that are designed as meat replacements (e.g., veggie burger, soy chick\u2019n)</li><li>Increase the amount of cooking you do yourself</li></ul><p><i>Cravings Strategies</i></p><p>Strategies for dealing with cravings were less useful than the rest, but some individuals may feel that they need them and may find them helpful. For those who want to try them, we recommend several strategies that were individually associated with better consumption success:</p><ul><li>Plan a strategy for dealing with temptation if it occurs</li><li>Avoid places or situations that might tempt you&nbsp;</li><li>Change the way you were thinking about a craving or a food you craved&nbsp;</li><li>Distract yourself from a craving&nbsp;</li><li>Plan meals in advance (e.g., before grocery shopping, going to a restaurant)&nbsp;</li><li>Remind yourself why you\u2019re following this diet&nbsp;</li></ul>", "user": {"username": "JLRiedi"}}, {"_id": "G6EWTrArPDf74sr3S", "title": "Bernard Williams: Ethics and the limits of impartiality", "postedAt": "2022-09-07T11:26:14.914Z", "htmlBody": "<blockquote>\n<p>Once upon a time, in some out of the way corner of that universe which is dispersed into numberless twinkling solar systems, there was a star upon which clever beasts invented knowing. That was the most arrogant and mendacious minute of \"world history,\" but nevertheless, it was only a minute. After nature had drawn a few breaths, the star cooled and congealed, and the clever beasts had to die.</p>\n<p>\u2015 Friedrich Nietzsche, On Truth and Lies in a Nonmoral Sense</p>\n</blockquote>\n<blockquote>\n<p>There is no test of cosmic significance. If there's no such thing as the cosmic point of view, if the idea of absolute importance in the scheme of things is an illusion, a relic of a world not yet thoroughly disenchanted, then there is no other point of view except ours, from which our activities can have, or lack, significance.</p>\n<p>\u2015 Bernard Williams: Philosophy as Humanistic Discipline</p>\n</blockquote>\n<blockquote>\n<p>Philosophy contributes to the project of making sense of being human, and that is not [\u2026] best served by abstracting from the local perspectives or idiosyncrasies of human beings.</p>\n<p>\u2015 Adrian Moore on Bernard Williams</p>\n</blockquote>\n<p><strong>What are we doing when we do moral philosophy?</strong> How should this self-understanding inform our practice of philosophy, and what we might hope to gain from it?</p>\n<p>Bernard Williams, like Nietzsche, took these questions seriously. I hope to revisit and write on his <em>Ethics &amp; The Limits of Philosophy</em> sometime soon. For now I\u2019ll just quote heavily from his lecture \u201cPhilosophy as a Humanistic Discipline\u201d (<a href=\"https://www.youtube.com/watch?v=szgMiqbR57s\">video</a>, <a href=\"https://thevalmy.com/26\">audio</a>).</p>\n<p>Williams wants to sharpen the distinction between science and moral philosophy. On his conception, science aims to give us \u201ca representation of [the world] which is to the largest possible extent independent of the local perspectives or idiosyncrasies of enquirers\u201d. Moral philosophy, by contrast, is concerned with the broader and more fundamental task of making sense of the situation in which we find ourselves, and deciding what to do about it.</p>\n<p>Williams wants to push back against a \u201cscientistic\u201d trend in moral philosophy, and against philosophers who exhibit \u201ca Platonic contempt for the the human and the contingent in the face of the universal\u201d. Such philosophers believe that:</p>\n<blockquote>\n<p>if there were an absolute conception of the world, a representation of it which was maximally independent of perspective, that would be better than more perspectival or locally conditioned representations of the world.</p>\n</blockquote>\n<p>And, relatedly:</p>\n<blockquote>\n<p>that offering an absolute conception is the real thing, what really matters in the direction of intellectual authority</p>\n</blockquote>\n<p>If we think of moral philosophy as about \u201cmaking sense of the situation in which we find ourselves, and deciding what to do about it\u201d, it becomes clear why the scientific focus on representing \u201cthe world as it is anyway\u201d, abstracted from any human perspective, is helpful, but insufficient.</p>\n<p>Thinking of moral philosophy as \"a search for timeless truths\" is attractive insofar as it seems to anchor our efforts, and permit us to tell a story of improvement rather than, simply, change.</p>\n<p>As Williams puts it, a \u201cvindicatory\u201d explanation of moral change is such that:</p>\n<blockquote>\n<p>the later theory, or (more generally) outlook, makes sense of itself, and of the earlier outlook, and of the transition from the earlier to the later, in such terms that both parties (the holders of the earlier outlook, and the holders of the later) have reason to recognize the transition as an improvement.</p>\n</blockquote>\n<p>The history of science usually involves such vindicatory changes, which are stimulated by crises of explanation that are recognised by all serious parties. But this\u2014on Williams' view\u2014not true for changes in moral or political \"outlook\":</p>\n<blockquote>\n<p>In the case of scienti\ufb01c change, it may occur through there being a crisis. If there is a crisis, it is agreed by all parties to be a crisis of explanation, and while they may indeed disagree over what will count as an explanation, to a considerable extent there has come to be agreement, at least within the limits of science since the eighteenth century, and this makes an important contribution to the history being vindicatory. But in the geographically extended and long-lasting and various processes by which the old political and ethical order has changed into modernity, while it was propelled by many crises, they were not in the \ufb01rst instance crises of explanation. They were crises of con\ufb01dence or of legitimacy, and the story of how one conception rather than another came to provide the basis of a new legitimacy is not on the face of it vindicatory.</p>\n</blockquote>\n<p>On liberalism:</p>\n<blockquote>\n<p>If we consider how [liberal] forms of argument came to prevail, we can indeed see them as having won, but not necessarily as having won an argument. For liberal ideas to have won an argument, the representatives of the <em>ancien r\u00e9gime</em> would have had to have shared with the nascent liberals a conception of something that the argument was about, and not just in the obvious sense that it was about the way to live or the way to order society. They would have had to agree that there was some aim, of reason or freedom or whatever, which liberal ideas served better or of which they were a better expression, and there is not much reason, with a change as radical as this, to think that they did agree about this, at least until late in the process. The relevant ideas of freedom, reason, and so on were themselves involved in the change.</p>\n</blockquote>\n<p>Philosophers such as Sidgwick and Parfit want to tell a story of historical improvement, where our moral beliefs gradually converge on human-independent moral truths. They want a story that \"vindicates\" our current ethical beliefs, and offers the possibility of further improvement. For Parfit, clearly:</p>\n<blockquote>\n<p>...a vindicatory history of our outlook is what we would really like to have</p>\n</blockquote>\n<p>Therefore, the idea that:</p>\n<blockquote>\n<p>liberalism, in particular (but the same is true of any outlook), has the kind of contingent history that it does have is a disappointment, which leaves us with at best a second best.</p>\n</blockquote>\n<p>So Parfit's story about moral philosophy risks severe disappointment.</p>\n<p>Williams has a different story:</p>\n<blockquote>\n<p>But, once again, why should we think that? Precisely because we are not unencumbered intelligences selecting in principle among all possible outlooks, we can accept that this outlook is ours just because of the history that has made it ours; or, more precisely, has both made us, and made the outlook as something that is ours. We are no less contingently formed than the outlook is, and the formation is signi\ufb01cantly the same. We and our outlook are not simply in the same place at the same time. If we really understand this, deeply understand it, we can be free of what is indeed another scientistic illusion, that it is our job as rational agents to search for, or at least move as best we can towards, a system of political and ethical ideas which would be the best from an absolute point of view, a point of view that was free of contingent historical perspective.</p>\n<p>If we can get rid of that illusion, we shall see that there is no inherent con\ufb02ict among three activities: \ufb01rst, the \ufb01rst-order activity of acting and arguing within the framework of our ideas; second, the philosophical activity of re\ufb02ecting on those ideas at a more general level and trying to make better sense of them; and third, the historical activity of understanding where they came from. The activities are in various ways continuous with one another. This helps to de\ufb01ne both intelligence in political action (because of the connection of the \ufb01rst with the second and the third), and also realism in political philosophy (because of the connection of the second with the \ufb01rst and the third). If there is a dif\ufb01culty in combining the third of these activities with the \ufb01rst two, it is the dif\ufb01culty of thinking about two things at once, not a problem in consistently taking both of them seriously.</p>\n</blockquote>\n<p>At a certain point, the chain of justification ends in an affirmation of human identity\u2014contingent, ephemeral, and possibly otherwise, but worth affirming all the same. We are who and what we are; justification is not required. Elsewhere, Williams quotes Max Stirner:</p>\n<blockquote>\n<p>The tiger who attacks me is in the right, and so am I when I strike him down. I defend against him not my right, but myself.</p>\n</blockquote>\n<p>For Williams, justification only makes sense within a context of similarity, of common ground, where we can appeal to shared values, sensibilities, ways of thinking, and so on. He writes:</p>\n<blockquote>\n<p>Wittgenstein in\ufb02uentially and correctly insisted that there was an end to justi\ufb01cations, that at various points we run into the fact that \u2018this is the way we go on\u2019. But, if I may say again something that I have said rather often before, it makes a great difference who \u2018we\u2019 are supposed to be, and it may mean different groups in different philosophical connections. It may mean maximally, as I mentioned earlier, any creature that you and I could conceive of understanding. Or it may mean any human beings, and here universal conditions of human life, including very general psychological capacities, may be relevant. Or it may mean just those with whom you and I share much more, such as outlooks typical of modernity.</p>\n</blockquote>\n<p>The mistake of the ahistorical liberals is that:</p>\n<blockquote>\n<p>they go on [\u2026] as though liberalism were timeless. It is not a reproach to these liberals that they cannot see beyond the outer limits of what they \ufb01nd acceptable: no-one can do that. But it is more of a reproach that they are not interested enough in why this is so, in why their most basic convictions should seem to be, as I put it, simply there.</p>\n</blockquote>\n<p>To me, these passages explain why non-naturalistic moral philosophy often seems oddly naive. Most days of the week, I agree with Williams\u2014and Nietzsche\u2014that we <em>should</em> be willing to pledge our allegiance to moral truths that are <em>not</em> timeless, or uncontingent. And that we should avoid excessive acquiescence to the demand for justification. At a certain point, we have to say: \u201cwe are like this and not like that and there is no justification necessary.\u201d <sup class=\"footnote-ref\"><a href=\"#fn-jhzwkvjwevtexEodm-1\" id=\"fnref-jhzwkvjwevtexEodm-1\">[1]</a></sup> <sup class=\"footnote-ref\"><a href=\"#fn-jhzwkvjwevtexEodm-2\" id=\"fnref-jhzwkvjwevtexEodm-2\">[2]</a></sup></p>\n<p>Further reading:</p>\n<ul>\n<li>Podcast: <a href=\"https://thevalmy.com/25\">Adrian Moore on Bernard Williams on Ethics</a></li>\n<li>Lecture: \u201cPhilosophy as a Humanistic Discipline\u201d (<a href=\"https://www.youtube.com/watch?v=szgMiqbR57s\">video</a>, <a href=\"https://thevalmy.com/26\">audio</a>)</li>\n<li>Article: <a href=\"https://www.lrb.co.uk/the-paper/v08/n14/bernard-williams/a-passion-for-the-beyond\">A passion for the beyond (Review of \u201cThe View From Nowhere\u201d, by Thomas Nagel)</a></li>\n<li>Book: <a href=\"https://www.hup.harvard.edu/catalog.php?isbn=9780674268586\">Ethics and The Limits of Philosophy</a></li>\n<li>Book: <a href=\"https://www.jstor.org/stable/j.ctt7rx9w\">Philosophy as a Humanistic Discipline</a></li>\n</ul>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-jhzwkvjwevtexEodm-1\" class=\"footnote-item\"><p>Incidentally, I think that Jeremy Bentham agreed with Williams on this. My read, following <a href=\"https://notes.pjh.is/people/David+Runciman\">David Runciman</a>, is that The Principle of Utility was a pragmatic political principle\u2014the result of his search for something we can all agree on\u2014not a metaphysical principle. Tyler Cowen agrees (personal correspondence). Simon Blackburn has a similar take to Williams, <a href=\"https://sun.pjh.is/simon-blackburn-on-people-who-think-that-anti-realism-entails-nihilism\">denying that anti-realism is equivalent to nihilism</a>. And Nick Bostrom and Carl Shulman recently shared a <a href=\"http://www.nickbostrom.com/papers/mountethics.pdf\">similar story</a>. <a href=\"#fnref-jhzwkvjwevtexEodm-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-jhzwkvjwevtexEodm-2\" class=\"footnote-item\"><p>Some philosophers who defend a non-naturalistic story: <a href=\"https://notes.pjh.is/%3DKatarzyna+de+Lazari-Radek\">Peter Singer and Katarzyna de Lazari-Radek</a>, Derek Parfit, Henry Sidgwick.  The first three describe themselves as secular. <a href=\"#fnref-jhzwkvjwevtexEodm-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "Peter_Hartree"}}, {"_id": "2JQQZevGENbChSA8k", "title": "Agree/disagree voting (& other new features September 2022)", "postedAt": "2022-09-07T11:07:45.382Z", "htmlBody": "<p>We\u2019re<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2v6id1c6b1p\"><sup><a href=\"#fn2v6id1c6b1p\">[1]</a></sup></span>&nbsp;enabling <a href=\"https://forum.effectivealtruism.org/posts/2JQQZevGENbChSA8k/agree-disagree-voting-and-other-new-features-september-2022#Two_factor_voting__karma_and_agree_disagree__on_all_comment_threads\">two-factor voting (karma and agree/disagree)</a> on all (new) comment threads in the Forum. We\u2019ll be reviewing feedback and will evaluate how the change has gone in October, but we expect to keep the feature.</p><p>We wrote this post to share this update and some other changes. On that note, the Forum team continues to grow; <a href=\"https://forum.effectivealtruism.org/users/ollie-etherington\">Ollie Etherington</a> and <a href=\"https://forum.effectivealtruism.org/users/will-howard-1\"><u>Will Howard</u></a> recently joined our team, and we\u2019re eagerly anticipating our new product manager. We\u2019re also still excited to hear about promising UX designers interested in working on the Forum.&nbsp;</p><p>As always, feedback is welcome \u2014 you can comment on this post with specific input, or request more features via the <a href=\"https://forum.effectivealtruism.org/posts/NhSBgYq55BFs7t2cA/ea-forum-feature-suggestion-thread\"><u>Forum feature suggestion thread</u></a>.</p><h1>Summary of the changes</h1><ul><li>Two-factor voting is going live on the Forum. This adds agree-disagree voting on top of the usual karma system. <a href=\"https://forum.effectivealtruism.org/posts/2JQQZevGENbChSA8k/agree-disagree-voting-and-other-new-features-september-2022#Two_factor_voting__karma_and_agree_disagree__on_all_comment_threads\">\u2b07\ufe0f&nbsp;</a></li><li>We\u2019ve started curating Forum posts that don\u2019t get enough visibility or are especially good as examples of what Forum posts should be (according to us). <a href=\"https://forum.effectivealtruism.org/posts/2JQQZevGENbChSA8k/agree-disagree-voting-and-other-new-features-september-2022#Curation_of_especially_good_posts\">\u2b07\ufe0f&nbsp;</a></li><li>There\u2019s finally a way to copy-paste from a Google Doc with footnotes. <a href=\"https://forum.effectivealtruism.org/posts/2JQQZevGENbChSA8k/agree-disagree-voting-and-other-new-features-september-2022#Copy_pasting_footnotes_from_a_Google_Document\">\u2b07\ufe0f&nbsp;</a></li><li>We\u2019re testing a 1:1 service to connect people interested in working in a field with experts in that field (the <a href=\"https://forum.effectivealtruism.org/advice\"><u>current service</u></a> is for people interested in mitigating global catastrophic biological risks). <a href=\"https://forum.effectivealtruism.org/posts/2JQQZevGENbChSA8k/agree-disagree-voting-and-other-new-features-september-2022#1_1_service_to_get_advice_on_biosecurity\">\u2b07\ufe0f&nbsp;</a></li><li>Crossposting to and from LessWrong is easier. <a href=\"https://forum.effectivealtruism.org/posts/2JQQZevGENbChSA8k/agree-disagree-voting-and-other-new-features-september-2022#Crossposting_to_and_from_LessWrong_is_easier\">\u2b07\ufe0f&nbsp;</a></li><li>You can add topics to your public profile. <a href=\"https://forum.effectivealtruism.org/posts/2JQQZevGENbChSA8k/agree-disagree-voting-and-other-new-features-september-2022#You_can_add_topics_to_your_public_profile\">\u2b07\ufe0f&nbsp;</a></li><li>Some other changes <a href=\"https://forum.effectivealtruism.org/posts/2JQQZevGENbChSA8k/agree-disagree-voting-and-other-new-features-september-2022#Other_changes\">\u2b07\ufe0f&nbsp;</a></li></ul><figure class=\"image image_resized\" style=\"width:71.26%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994651/mirroredImages/2JQQZevGENbChSA8k/pitjezigjofhyxp24lo7.png\"><figcaption>Image created with DALL-E</figcaption></figure><h1>What\u2019s new</h1><h2>Two-factor voting (karma and agree/disagree) on all comment threads</h2><p>You can now vote separately on whether you appreciate a comment (and think more people should see it) and on whether you agree with the contents of the comment. Only the first of these will affect the poster\u2019s <a href=\"https://forum.effectivealtruism.org/posts/Y8gkABpa9R6ktkhYt/forum-user-manual#Karma_and_voting\"><u>karma</u></a>.&nbsp;</p><figure class=\"image image_resized\" style=\"width:78.54%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994651/mirroredImages/2JQQZevGENbChSA8k/onaur06hnnxiglzhndi0.png\"><figcaption><a href=\"https://forum.effectivealtruism.org/posts/DCZhan8phEMRHuewk/person-affecting-intuitions-can-often-be-money-pumped?commentId=EHt2CdD7rSiBHwgNt\">One test</a> of two-factor voting</figcaption></figure><p>The LessWrong team implemented this feature on LessWrong two months ago, and <a href=\"https://www.lesswrong.com/posts/HALKHS4pMbfghxsjD/lesswrong-has-agree-disagree-voting-on-all-new-comment\"><u>their announcement post</u></a> shares details about how it works and what they like about it; we echo basically everything in that post. Some key points made in that post:&nbsp;</p><blockquote><ul><li><strong>Agree/disagree voting does not translate into a user's or post's karma \u2014 its sole function is to communicate agreement/disagreement.</strong> It has no other direct effects on the site or content visibility (i.e. no effect on sorting algorithms).</li><li>For both regular voting and the new agree/disagree voting, you have the ability to normal-strength vote and strong-vote. Click once for normal-strength vote. For strong-vote, click-and-hold on desktop or double-tap on mobile. The weight of your strong-vote is approximately proportional to your karma on a log-scale (<a href=\"https://www.lesswrong.com/posts/7Sx3CJXA7JHxY2yDG/strong-votes-update-deployed\"><u>exact numbers here</u></a>).</li></ul></blockquote><p>We\u2019re really grateful to the LessWrong team for all their work on this and are excited about this as a potential improvement on the Forum.&nbsp;</p><p><strong>How sure are we that we want to keep this feature?&nbsp;</strong></p><p>Our default plan is to keep it, although we welcome <a href=\"https://forum.effectivealtruism.org/posts/Y8gkABpa9R6ktkhYt/forum-user-manual#Give_feedback__suggest_features__or_report_bugs\"><u>feedback</u></a> and plan to re-evaluate in October. We had planned to test the feature on the Forum more thoroughly by asking authors to opt into enabling it on their posts\u2019 comment sections. However, the testing has been very slow, and, seeing the results on LessWrong (and on the Forum), we were increasingly feeling like we were wasting a great tool.</p><h2>Curation of especially good posts</h2><p>The number of <a href=\"https://forum.effectivealtruism.org/allPosts?timeframe=daily&amp;sortedBy=topAdjusted\"><u>posts that get shared on a single day</u></a> is growing, and it\u2019s increasingly hard for Forum users to keep up and see the best and most relevant-to-them posts. One way we\u2019re trying to address this is by <strong>curating some of our favorite posts</strong>. We\u2019ll generally leave a comment explaining why we curated the post.&nbsp;</p><p><strong>How this works</strong></p><p>The exact details of this system might change, but for now, they\u2019re as follows. When a post is curated, it reappears at the top of the Frontpage with a little star next to it. The timestamp on it will show the time it got curated (although the timestamp on the post page itself will remain unchanged).</p><figure class=\"image image_resized\" style=\"width:74.86%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994651/mirroredImages/2JQQZevGENbChSA8k/l1ditiktlyqcijcdz3nr.png\"><figcaption><a href=\"https://forum.effectivealtruism.org/posts/sW6RggfddDrcmM6Aw/how-might-we-align-transformative-ai-if-it-s-developed-very\"><u>One example</u></a></figcaption></figure><p>If two or more posts are curated, the three most recently curated will go to the top of the Frontpage. They\u2019ll generally go back to where they \u201cshould\u201d be when you open them (they\u2019ll go to where they would <a href=\"https://forum.effectivealtruism.org/posts/Y8gkABpa9R6ktkhYt/forum-user-manual#The_Frontpage\"><u>normally be on the Frontpage</u></a>, given their timestamp and karma), except the most recently curated post (or the one that you opened last) but if you open them, only one of them will remain at the top of the page for you.&nbsp;</p><p><strong>Who decides what to curate</strong></p><p>Some people have the ability to \u201csuggest curation\u201d by clicking on the three buttons under the title of a post and selecting the appropriate option. For now, Lizka (that\u2019s me) is the only one making the final decisions.&nbsp;</p><p><strong>Reasons for curating a post</strong></p><ul><li>We think it\u2019s just extremely valuable or important, or it prompts a conversation that\u2019s very important to have</li><li>We think it\u2019s very valuable and was overlooked for some reason (like an unfortunate time-of-posting or a boring title)</li><li>We want to signal that this is the kind of post we want more of on the Forum</li></ul><p>You\u2019re very welcome to <a href=\"https://forum.effectivealtruism.org/posts/Y8gkABpa9R6ktkhYt/forum-user-manual#Give_feedback__suggest_features__or_report_bugs\"><u>share feedback</u></a> on curation.&nbsp;</p><h2>Copy-pasting footnotes from a Google Document</h2><p>We\u2019re excited to share that we\u2019ve developed a <a href=\"https://forum.effectivealtruism.org/posts/NhSBgYq55BFs7t2cA/ea-forum-feature-suggestion-thread?commentId=er4a8gKANTuh5gqH8#er4a8gKANTuh5gqH8\"><u>much</u></a>-<a href=\"https://forum.effectivealtruism.org/posts/NhSBgYq55BFs7t2cA/ea-forum-feature-suggestion-thread?commentId=2hQaYrD894QrB8MR9\"><u>asked</u></a>-<a href=\"https://forum.effectivealtruism.org/posts/LpCewmJgosEaz7ZkW/open-thread-june-september-2022?commentId=ynmSGeMkCfTegFkr5#cnovr98Cxw8AXN5Qe\"><u>for</u></a> feature. If you're copy-pasting from a Google Document, <strong>you can now copy-paste footnotes</strong> in the normal (<a href=\"https://forum.effectivealtruism.org/posts/Y8gkABpa9R6ktkhYt/forum-user-manual#Two_different_editors__WYSIWYG_and_Markdown\"><u>WYSIWYG</u></a>) post editor in the following way.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkt5iz5gmyhm\"><sup><a href=\"#fnkt5iz5gmyhm\">[2]</a></sup></span></p><ol><li>Publish your Google Doc to web<ol><li>You can do this by clicking on File &gt; Share &gt; Publish to web</li><li>Then approve the pop-up asking you to confirm (hit \"Publish\")</li><li>Then open the link that you'll be given; this is now the published-to-web version of your document.</li></ol></li><li>Select the whole text, including footnotes, and copy that. (If you'd like, you can now unpublish the document.)</li><li>Open the Forum text editor (WYSIWYG), and paste the selection.</li></ol><p>Instructions in image form:&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994652/mirroredImages/2JQQZevGENbChSA8k/gwdafrciku6ytfvkqadh.png\"></figure><p>If you don't want to copy-paste, you can <a href=\"https://forum.effectivealtruism.org/posts/Y8gkABpa9R6ktkhYt/forum-user-manual#Footnotes\"><u>insert footnotes manually or use Markdown</u></a>.&nbsp;</p><h2>1:1 service to get advice on biosecurity</h2><p>You can <a href=\"https://forum.effectivealtruism.org/advice\"><u>get some personalized advice</u></a> from a professional working to reduce <a href=\"https://80000hours.org/problem-profiles/preventing-catastrophic-pandemics/\"><u>global catastrophic biological risks</u></a>. The 30-minute meeting with an advisor will allow you to get career advice, learn about their experience, and ask questions about topics of common interest.</p><p>We're testing this service, and depending on how the test goes, we might expand the service to other fields and professions.&nbsp;</p><p>This is how the <a href=\"https://forum.effectivealtruism.org/advice\"><u>service</u></a> looks right now:</p><figure class=\"image image_resized\" style=\"width:82.4%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994651/mirroredImages/2JQQZevGENbChSA8k/jpjgxicpxbb2lxoegexo.png\"></figure><h2>Crossposting to and from LessWrong is easier</h2><p>When you draft a post on the Forum, you now have the option to automatically <a href=\"https://forum.effectivealtruism.org/posts/8yDsenRQhNF4HEDwu/link-posting-is-an-act-of-community-service\"><u>crosspost</u></a> it to <a href=\"https://www.lesswrong.com/\"><u>LessWrong</u></a> (you\u2019ll need to be logged in on both sites), and vice versa. While you're editing the post, you should go to \"Options\" on the bottom, and select \"Crosspost to [LessWrong].\" &nbsp;</p><p>Any updates you make in the original post will appear in the crosspost. The comment sections will be distinct, but both will prompt users to see the other.&nbsp;</p><figure class=\"image image_resized\" style=\"width:80.13%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994651/mirroredImages/2JQQZevGENbChSA8k/wgnffi3ll06oerpwjnzk.png\"><figcaption>A cross-post from the EA Forum to LessWrong, viewed from the EA Forum</figcaption></figure><figure class=\"image image_resized\" style=\"width:78.82%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994651/mirroredImages/2JQQZevGENbChSA8k/qnssdxknvfjypc9bfszr.png\"><figcaption>The same cross-post from the EA Forum to LessWrong, viewed from LessWrong</figcaption></figure><h2>You can add topics to your public profile</h2><p>If you\u2019d like to share your interests, you can add <a href=\"https://forum.effectivealtruism.org/topics/all\"><u>topics</u></a> to your public profile, which will also <a href=\"https://forum.effectivealtruism.org/posts/2jYDXwqSj87ZjLtwy/follow-and-filter-topics-and-an-update-to-the-community\"><u>subscribe</u></a> you to them by default (you can undo that). To do this, go to \u201cEdit Profile\u201d from your user profile page, then scroll down to \u201cMy Activity\u201d and search for the topics you\u2019re interested in. (Here\u2019s <a href=\"https://forum.effectivealtruism.org/topics/all\">a page that lists <u>all topics</u></a>.)</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994651/mirroredImages/2JQQZevGENbChSA8k/fcnm7aecxvs9ty6nkawq.png\"></figure><h2>Other changes</h2><p>Some of the smaller changes are parts of larger projects, and some are more routine improvements to the Forum (e.g. elimination of ongoing bugs).</p><ul><li>We\u2019re ending (un-pinning) the \u201c<a href=\"https://forum.effectivealtruism.org/posts/DXcg6N6CGvRA2vrCk/who-s-hiring-may-september-2022\"><u>Who\u2019s hiring</u></a>\u201d and \u201c<a href=\"https://forum.effectivealtruism.org/posts/swcBj4rBC4HvwmwfD/who-wants-to-be-hired-may-september-2022\"><u>Who wants to be hired</u></a>\u201d threads this week. We might bring back a variation on them, but we\u2019re currently evaluating how they went. If you have any feedback, please feel free to share it!</li><li>We\u2019ve added profile images to private messages.</li></ul><figure class=\"image image_resized\" style=\"width:70.89%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994651/mirroredImages/2JQQZevGENbChSA8k/ykuilncqp4yftjwil2ee.png\"><figcaption>(I \u2014 Lizka \u2014 screen-shotted this, and your own profile image doesn't show up for you in private messages, so you can't see my cool profile picture.)</figcaption></figure><ul><li>We\u2019ve fixed a bug in our \u201cReport user\u201d button, so that should work properly now.</li><li>You can pin comments in your profile.</li></ul><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994651/mirroredImages/2JQQZevGENbChSA8k/q5cbmqxtudc4gxqcbmo7.png\"><figcaption>Click on the three dots in the top right corner of the comment and select \"Pin to my profile,\" as shown.&nbsp;</figcaption></figure><ul><li>If you have edit access to an <a href=\"https://forum.effectivealtruism.org/posts/Y8gkABpa9R6ktkhYt/forum-user-manual#Events__groups__and_community_spaces\"><u>event</u></a> (if you\u2019re an author of the event, or if you\u2019re a declared group organizer of the group hosting it), you can duplicate the event so that you don\u2019t need to start from scratch when creating a new event.</li></ul><p>&nbsp;</p><p><i>Note from Lizka: I'm really grateful to everyone who helped with these features, including everyone who worked on the code, the users who reported issues or contributed user interviews, and the other people supporting the Forum in a variety of other ways. Thanks, all!</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2v6id1c6b1p\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2v6id1c6b1p\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This post was written by Lizka and reviewed by <a href=\"https://forum.effectivealtruism.org/users/jpaddison\">JP Addison</a> and some other members of the Forum team.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkt5iz5gmyhm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkt5iz5gmyhm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This allowed us to get around the impossibility of selecting all footnotes in a Google Doc.</p></div></li></ol>", "user": {"username": "Lizka"}}, {"_id": "bsqtndpfKbcbQRFEs", "title": "Fundraising Campaigns at Your Organization: A Reliable Path to Counterfactual Impact", "postedAt": "2022-09-07T10:40:24.932Z", "htmlBody": "<p><i>This post is Part 1 in a series on organizing fundraising campaigns in the workplace:</i></p><ul><li><i>Part 2: </i><a href=\"https://forum.effectivealtruism.org/posts/sE7Y43JRQARAKzZ6k/key-factors-for-success-in-organizing-a-fundraising-campaign\"><i>Key Factors for Success in Organizing a Fundraising Campaign at Your Company</i></a></li><li><i>Part 3: </i><a href=\"https://forum.effectivealtruism.org/posts/erpcHiAPE3sAwej2Y/a-playbook-for-running-corporate-fundraising-campaigns\"><i>A Playbook for Running Corporate Fundraising Campaigns</i></a></li></ul><h1>TL;DR</h1><ul><li>Running a fundraising campaign at your workplace can be a highly effective way to multiply your impact.</li><li>In 2021, <a href=\"https://bit.ly/3AUkQbE\"><u>High Impact Professionals</u></a> (\u201cHIP\u201d) supported EAs in organizing 8 fundraising campaigns at 8 different companies, counterfactually raising about 240,000 USD for effective charities.</li><li>The average amount raised per event was about 30,000 USD (median 3,900 USD), with most donations coming from a few events.</li><li>On average, it took about 25 hours to organize and run a campaign (20 hours by organizers and 5 hours by HIP).</li><li>The events generated an average of 786 USD per hour of counterfactual donations to effective charities.</li><li>This makes fundraising campaigns a very cost effective means of counterfactual impact; as a comparison, direct work that generates 1,000,000 USD of impact equivalent per year equates to around 500 USD per hour.</li><li>If you\u2019re interested in exploring what a fundraising campaign could look like at your organization, please contact <a href=\"mailto:federico@highimpactprofessionals.org\">federico@highimpactprofessionals.org</a> for 1-on-1 support and to inquire about our step-by-step fundraising guide.</li><li>If you\u2019re interested in learning more about the 2021 campaigns\u2019 data and methodology, please keep reading.</li><li>We would love to get feedback on our data and methodology, so don\u2019t hesitate to reach out <a href=\"https://bit.ly/3TOQ7Fn\"><u>here</u></a> or in the comments.</li></ul><h1>Intro</h1><p>Running a fundraising campaign at your workplace can be a highly effective way to multiply your impact as a working professional. Getting colleagues to donate money to effective charities not only increases your donation leverage but also has the potential to get others involved in the EA movement. It can also help you build relevant EA career capital.</p><p>During the 2021 giving season, HIP supported EAs in organizing 8 fundraising campaigns at 8 different companies. Essentially, the workplace campaigns encouraged those EAs\u2019 colleagues to donate to effective charities. The results detailed below point to these events being a strong way to enhance one\u2019s impact.</p><h1>Results</h1><p>In total, the 8 campaigns counterfactually raised about 240,000 USD for effective charities. The average amount raised per event was about 30,000 USD, with a 90% Confidence Interval (\u201cCI\u201d) of 80 USD to 140,000 USD. The graph below shows the distribution in logarithmic scale.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995573/mirroredImages/bsqtndpfKbcbQRFEs/jwozjbqqakiz0o1eqnyw.png\"></figure><p>The main cost of the initiative is the time taken to organize the campaign. On average, organizers spent 20 hours on their events (CI 6 hours to 42 hours) and received 5 hours of support from HIP (CI 2 hours to 10 hours). The distributions are shown below.&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995573/mirroredImages/bsqtndpfKbcbQRFEs/iioerxdmtmzwocd711vo.png\"></figure><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995573/mirroredImages/bsqtndpfKbcbQRFEs/iocr4jfvasbkhxjalnop.png\"></figure><p>Computing the ratio of money raised to time spent, we arrive at an average of 786 USD per hour (CI 7 USD to 3,100 USD).</p><p>To put this into perspective, let\u2019s assume that direct work generates between 100,000 USD and 1,000,000 USD of impact equivalent per year<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgk3bp9lwo2d\"><sup><a href=\"#fngk3bp9lwo2d\">[1]</a></sup></span>&nbsp;and that people work between 1,800 and 2,200 hours per year. This computes to an average of 290 USD per hour (CI 57 USD to 530 USD), which means that organizing fundraising campaigns can be about 2.7x as effective as direct work on an hourly basis.&nbsp;</p><p>Of course, we are not saying that people should stop considering direct work and do fundraising events instead since, for example, the impact of doing fundraising campaign work year-round may not scale as well. Still, strategically timed campaigns (e.g., around giving season, your organization\u2019s raise/bonus season, or another liquidity event) could be a very high impact seasonal side gig.</p><h3>Heavy Tail</h3><p>One could raise a fair argument that the donation distribution above is quite heavily tailed, and because of the limited data points, the result may be anomalous. Though we have already been quite conservative in the estimation of the counterfactual impact of the events, we could take an even more conservative stance by removing the outlier from the distribution. Doing so, we arrive at a per-campaign result of 4,000 USD on average (CI 60 USD to 15,000 USD). The distribution is shown below. Note that the scale of the distribution is now linear.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995573/mirroredImages/bsqtndpfKbcbQRFEs/y9cy3w6jfys214onjjrk.png\"></figure><p>However, even under this more conservative approach, the average hourly value of the organizer\u2019s time is 343 USD, which still compares favorably to direct work at 290 USD per hour. Moreover, donations are usually heavily tailed, <a href=\"https://forum.effectivealtruism.org/posts/nb6tQ5MRRpXydJQFq/ea-survey-2020-series-donation-data\"><u>as indicated in the EA Survey</u></a>, so it is reasonable to expect that there could be some big outlier hits across a range of campaigns.</p><h1>Main Benefits</h1><p>The main benefit of running a fundraising campaign is the money counterfactually moved towards effective causes, especially considering its high cost effectiveness. But, we found that there are additional side benefits.</p><p>For example, effective donations can be a good introduction to EA, and some organizers reported finding new people sympathetic to EA in this way.</p><p>Also, from our interviews with the organizers after the fact, they reported a slight increase in their engagement with EA thanks as the campaigns provided a way for the organizers to do something tangible.</p><p>Finally, running a campaign is a good way to build EA career capital that could facilitate a potential future move into direct work. For example, thanks to having organized fundraising campaigns in his previous work, HIP cofounder, Federico, got bonus points on his application to join the Charity Entrepreneurship incubation program, where he was able to find a cofounder and launch HIP.</p><h1>Potential Challenges</h1><p>The main cost of running a fundraising campaign is the time needed to organize the events. In 2021, some events took as little as 5 hours, but the average was 20 hours. Strategies to mitigate this are to get ready-made material and external support. Recognizing this, we created a <a href=\"https://bit.ly/3KRMj29\"><u>step-by-step fundraising guide</u></a> and we are also happy to help you set up the campaign. Another potential approach is to do something requiring very low effort, like sending a short message to your colleagues encouraging effective giving and offering suggested resources.</p><p>Also, in order to set up a campaign, you often will need to deal with intra-company processes and the like. In a flat hierarchy organization this can be pretty minor; in other cases, this might require some time. The best mitigation strategies here are to start soon and get project buy-in from someone higher up in your organization.</p><p>Finally, not all people view philanthropy in the same way as EA, so some people in your organization might want to, for example, focus charitable giving locally, etc. Thus, there is the potential for some friction. On one hand, you don\u2019t want to be too harsh and ideally can find a compromise; on the other hand, you don\u2019t want to support things which are not effective. A mitigation strategy here is to try to meet people where they are, walk them through your thought process, and be ready to make compromises if necessary.</p><h1>Wrap Up</h1><p>The high leverage opportunity presented by fundraising campaigns makes them a compelling way to multiply your impact.&nbsp;</p><p>If you're interested in seeing what a campaign could look like at your organization, please contact <a href=\"mailto:federico@highimpactprofessionals.org\">federico@highimpactprofessionals.org</a> and we can share insights, resources (like our <a href=\"https://bit.ly/3KRMj29\"><u>step-by-step fundraising guide</u></a>), tips, and tricks to help you set up your own successful event.</p><p>We will publish more materials on fundraising campaigns in the near future, including an updated Fundraising Guide, so stay tuned! You can also <a href=\"https://bit.ly/3Bekr59\"><u>subscribe to our newsletter</u></a> to get updates on resources and other information for EA working professionals.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngk3bp9lwo2d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgk3bp9lwo2d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It\u2019s hard to put a number on direct work. <a href=\"https://80000hours.org/2021/08/effective-altruism-allocation-resources-cause-areas/\"><u>80,000 Hours mentioned</u></a> a conversion rate of 100,000 USD per year and stated it could be off by an order of magnitude. <a href=\"https://www.charityentrepreneurship.com/\"><u>Charity Entrepreneurship estimates</u></a> the impact of charity entrepreneurs at 200,000 USD per year and that top founders can have 5x that impact. All things considered, we think that 100,000 USD to 1,000,000 USD per year should be on the reasonable to generous side of things for direct work. Moreover, we didn\u2019t want to use conservative numbers that would inflate the comparative effectiveness of fundraising campaigns.</p></div></li></ol>", "user": {"username": "High Impact Professionals"}}, {"_id": "dimzwwuXqkfuBPKfW", "title": "Is there more guidance/resources on what to do if one feels like they are part of a sugnificant minority that should focus on potentially pressing global issues beyond current 80 000h priorities?", "postedAt": "2022-09-07T09:24:59.917Z", "htmlBody": "<p>Long lists of potentially pressing global issues beyond our current priorities <a href=\"https://80000hours.org/problem-profiles/\">https://80000hours.org/problem-profiles/</a></p>\n<p>80 000h website says this:</p>\n<p>\u201csignificant minority of our readers (say 10\u201320%) to explore new areas like those listed below rather than focusing on our current priority problem areas. This would be the 10\u201320% who are relatively best suited to these areas, which probably means those with some kind of pre-existing interest.\u201d</p>\n<p>I think I\u2019m part of that significant minority but cannot really find any further help or enough material regarding those topics from an EA angle, for example safeguarding democracy, risks of stable totalitarianism, risks from malevolent actors, global public goods etc.</p>\n<p>I think the lack of discussion and materials and research is probably due to resource optimisation towards what people think is highest on the priority list(?). Then the website says this:</p>\n<p>\u201cWe\u2019d be excited to see more discussion and exploration of many of these areas from the perspective of trying to improve the long-term future, and hope to help facilitate such exploration going forward.\u201d</p>\n<p>\u27a1\ufe0f what could this mean? I think I could do something with this information but I don\u2019t know what.</p>\n", "user": {"username": "Elskivi"}}, {"_id": "jww9Bg4rLscGb6ofv", "title": "What highly-impactful work is the most similar to solving fun math puzzles (if any)?", "postedAt": "2022-09-07T06:16:07.007Z", "htmlBody": "<p>Kind of a selfish question, but I really enjoy doing math puzzles and making an impact, and my general impression is that these two aren't that compatible. However, it could have a very large impact on my life if this turns out to be false.</p><p>FWIW, my current best guesses would be theoretical alignment research (especially <a href=\"https://alignment.org/\">ARC's research</a> &nbsp;and <a href=\"https://intelligence.org/\">MIRI's research</a>), forecasting, and maybe some economics work related to global priorities or policy that involves game theory or something like that.&nbsp;</p>", "user": {"username": "oh54321"}}, {"_id": "3o5yqaEnghNcyhekW", "title": "EA jobs for people with dyslexia?", "postedAt": "2022-09-07T05:56:01.548Z", "htmlBody": "<p>I recently made a new friend from outside my typical social circle. He\u2019s a smart, thoughtful sort of guy, wants to make the world a better place\u2026 and is currently homeless and living out of his car. The reasons for that are outside the scope of this post, but the situation is not self-inflicted, and he wants to get out of it as soon as possible. I mentioned that the EA community has a number of open bounties/initiatives that theoretically anyone can apply for, but as it turns out, he\u2019s dyslexic, and cannot  do any job which requires reading or writing that isn\u2019t transcribed in audio. As such, he felt pretty hopeless about being able to do paid work for EA causes.</p>\n<p>This is mentioned to give some context to the question: Are there any jobs in EA available to people who cannot (effectively) read or write in length? It seems like in theory there should be a lot of good work that it is possible to do under those constraints, but in practice I have trouble finding any EA-focused job that doesn\u2019t require some amount of serious writing.\n(Note that this isn\u2019t a cry for financial help or anything\u2014my friend has applied for a number of minimum-wage manual labor jobs, one of which will likely accept him, but all things considered would prefer to do work which improves the state of the world/is in the EA space.)</p>\n", "user": {"username": "Yitz"}}, {"_id": "A2YwuXe3Eo5kMZhZo", "title": "13 background claims about EA", "postedAt": "2022-09-07T03:54:45.147Z", "htmlBody": "<p>I recently attended EAGxSingapore. In 1-1s, I realized that I have picked up a lot of information from living in an EA hub and surrounding myself with highly-involved EAs.&nbsp;</p><p>In this post, I explicitly lay out some of this information. I hope that it will be useful for people who are new to EA or people who are not living an EA Hub.&nbsp;</p><p>Here are some things that I believe to be important \u201cbackground claims\u201d that often guide EA decision-making, strategy, and career decisions. (In parentheses, I add things that I believe, but these are \"Akash's opinions\" as opposed to \"background claims.\")&nbsp;</p><p><i>Note that this perspective is based largely on my experiences around longtermists &amp; the Berkeley AI safety community.&nbsp;</i></p><p><strong>General</strong></p><p>1. <strong>Many of the most influential EA leaders believe that there is a &gt;10% chance that humanity goes extinct in the next 100 years. </strong>(Several of them have stronger beliefs, like a 50% of extinction in the next 10-30 years).</p><p>2. <strong>Many EA leaders are primarily concerned about AI safety</strong> (and to a lesser extent, other threats to humanity\u2019s long-term future). Several believe that artificial general intelligence is likely to be developed in the next 10-50 years. Much of the value of the present/future will be shaped by the extent to which these systems are aligned with human values.</p><p>3. <strong>Many of the most important discussions, research, and debates are happening in-person in major EA hubs. </strong>(I claim that visiting an EA Hub is one of the best ways to understand what\u2019s going on, engage in meaningful debates about cause prioritization, and receive feedback on your plans.)</p><p>4. <strong>Several \u201cEA organizations\u201d are not doing highly impactful work, and there are major differences in impact between &amp; within orgs</strong>. Some people find it politically/socially incorrect to point out publicly which organizations are failing &amp; why. (I claim people who are trying to use their careers in a valuable way should evaluate organizations/opportunities for themselves, and they should not assume that generically joining an \u201cEA org\u201d is the best strategy.)</p><p><strong>AI Safety</strong></p><p>5. <strong>Many AI safety researchers and organizations are making decisions on relatively short AI timelines (e.g., artificial general intelligence within the next 10-50 years).</strong> Career plans or research proposals that take a long time to generate value are considered infeasible. (I claim that people should think about ways to make their current trajectory radically faster\u2014 e.g., if someone is an undergraduate planning a CS PhD, they may want to consider alternative ways to get research expertise more quickly).</p><p>6. <strong>There is widespread disagreement in AI safety about which research agendas are promising,&nbsp; what the core problems in AI alignment are, and how people should get started in AI safety.&nbsp;</strong></p><p>7. <strong>There are several programs designed to help people get started in AI safety. </strong>Examples include <a href=\"https://www.serimats.org/\"><u>SERI-Mats</u></a> (for alignment research &amp; theory), <a href=\"https://www.lesswrong.com/posts/3ouxBRRzjxarTukMW/apply-to-the-second-iteration-of-the-ml-for-alignment\"><u>MLAB</u></a> (for ML engineering), the <a href=\"https://www.lesswrong.com/posts/CphfDP4ynz3QQ4AKY/introducing-the-ml-safety-scholars-program\"><u>ML Safety Scholars Program </u></a>(for ML skills), <a href=\"https://www.agisafetyfundamentals.com/ai-alignment-curriculum\"><u>AGI Safety Fundamentals</u></a> (for AI alignment knowledge), <a href=\"https://www.pibbss.ai/\"><u>PIBBS</u></a> (for social scientists), and the newly-announced <a href=\"https://philosophy.safe.ai/\"><u>Philosophy Fellowship</u></a>. (I suggest people keep point #6 in mind, though, and not assume that everything they need to know is captured in a well-packaged Program or Reading List).</p><p>8. <strong>There are not many senior AIS researchers or AIS mentors, and the ones who exist are often busy. </strong>(I claim that the best way to \u201cget started in AI safety research\u201d is to apply for a grant to spend ~1 month reading research, understanding the core parts of the alignment problem, evaluating research agendas, writing about what you\u2019ve learned, and visiting an EA hub).</p><p>9. <strong>People can apply for grants to skill-up in AI safety. You do not have to propose an extremely specific project, and you can apply even if you\u2019re new</strong>. Grant applications often take 1-2 hours. Check out the <a href=\"https://funds.effectivealtruism.org/funds/far-future\"><u>long-term future fund</u></a>.</p><p>10. <strong>LessWrong is better than the EA Forum for posts/discussions relating to AI safety</strong> (though the EA Forum is better for posts/discussions relating to EA culture/strategy)</p><p><strong>Getting Involved</strong></p><p>11. <strong>The longtermist EA community is small.</strong> There are not tons of extremely intelligent/qualified people working on the world\u2019s most pressing issues. There is a small group of young people with relatively little experience. We are often doing things we don\u2019t know how to do, and we are scrambling to figure things out. There is a lot that needs to be done, and the odds that you could meaningfully contribute are higher than you might expect. (See also <a href=\"https://forum.effectivealtruism.org/posts/NgSfKWayWpLwn8Lz7/lifeguards-1\"><u>Lifeguards</u></a>)</p><p>12. <strong>Funders generally want to receive more applications</strong>. (I think most people should have a lower bar for applying for funding).</p><p>13. <strong>If you want to get involved but you don\u2019t see a great fit in any of the current job openings, consider starting your own project </strong>(get feedback and consider downside risks, of course). Or consider reaching out to EAs for ideas (<strong>if you're interested in longtermism or AI safety, feel free to message me</strong>).&nbsp;</p><p><i>I am grateful to Olivia Jimenez, Miranda Zhang, and Christain Smith for feedback on this post.</i></p>", "user": {"username": "Akash"}}, {"_id": "LwhzE3scZTqxERtNn", "title": " It's (not) how you use it ", "postedAt": "2022-09-07T13:28:29.281Z", "htmlBody": "<p>The phrase \"technology isn't bad in itself, it's just how you use it\" is commonplace and contains some truth. But I think it's a mistake to go straight into judging the usage of technological products and not think about their design. Sure, it's intuitive to suppose that the choices humans make with how they interact with technologies play a decisive role in what purpose the technology ends up serving. My argument is that these choices are to be made earlier in the design and production of a certain technology; they're not choices humans find themselves making once they've acquired a technology. At that point, it's usually too late.</p><p>In History &amp; Philosophy of Science (HPS) studies, this approach broadly falls into the camp of Marxist theories about the history of technology in the sense that the technological product has a \"purpose\", an \"end\" and it can have intrinsic risks. These risks, for this type of theorizing primarily concern the inscription of social norms and regularities that change the dynamics within society. Translated into the EA framework, these might be existential or suffering, and cost us the continuation of our species. It is, as a result, careless and irresponsible to create technologies without having clarity on what they'll be good for and how they could lead to catastrophic scenarios.</p><p>In the book Human Compatible, Stuart Russell shows how this irresponsibility applies to the development of ML. The analogy is simple: it's like preparing a mission to another planet without considering in advance how your crew is going to survive once they're on the new planet. If you expect them to deal with whatever risks and problems the environment of the new planet might have for humans after landing there, then you're not taking seriously the inherent dangers of your project, and quite frankly, the project itself. In other words, this is not about using, let's say, a spaceship carelessly; it's about missing crucial parts in the agenda and set up of your mission.</p><p>Obviously, the same argument applies to our current situation: what we have been observing is fast AI progress and most likely, not enough time, care, and deliberation to ensure AI safety, despite the efforts of the safety research community. And to my point: it's not that AI will be harmful if we use it in a harmful way. The technology carries inherent dangers we need to take precautions for and incorporate into the design before the product becomes available. For example, training models with machine learning has its own uncertainties which start early on when you begin the process. They're, in a way, inherent in the technology. It'd be unfair to suddenly start playing a game of blameworthiness once an advanced product is out and someone uses it in ways that increase risk.&nbsp;</p><p>Just to be clear, I'm not saying human agents shouldn't be careful with the various products of technology. My argument is that we have to ensure our carefulness, attention, and sensitivity, don't suddenly strike as important when a very difficult-to-understand/predict product is out there.&nbsp;</p><p>It may look like I simply described the need to solve the alignment problem once again. But that's only part of my intention. What I want to emphasize is that we need to reconceptualize the way we think about technology. Narratives about technologies have historically been just as dangerous as the technologies themselves. The AI safety community has an impressively clear narrative, mostly due to the rationality schema that supports it. But my concern is that for many scholars and the public, clarity tends to come in hindsight, e.g., the Manhattan Project and the atomic bomb. &nbsp;</p><p>So, remember: the \"how-you-use-it\" bit starts very early on in the design of a technology. Technologies can be intrinsically dangerous in a non-Luddistic sense, especially when they're developed with multiple parameters of uncertainty.&nbsp;</p>", "user": {"username": "eangelou"}}, {"_id": "DNHPYnoiewvwgoECa", "title": "Would creating and burying a series of doomsday chests to reboot civilization be a worthy use of resources?", "postedAt": "2022-09-07T02:45:31.265Z", "htmlBody": "<p>Imagine compiling information on how to restart civilization into a series of books that are preserved and buried, intended to last 100s or 1000s of years. Take electricity: the book(s) would describe what electricity is, how to harness it, maybe even the basics for building and operating a power plant. Many people have had this <a href=\"https://longnow.org/ideas/02010/04/06/manual-for-civilization/\">idea</a>.</p><p>But would it be worth it to start such a project? What is likelihood that it would be needed? My gut tells me it would be useful in most cases of civilization collapse, in that it would speed up the rebuild, but there are much fewer cases in which the restarting of civilization would be contingent on a doomsday chest.</p>", "user": {"username": "ewu"}}, {"_id": "AuewiZayG6xBeQfKK", "title": "Impact Shares For Speculative Projects", "postedAt": "2022-09-06T21:20:21.902Z", "htmlBody": "<p><i>Note: this post was cross-posted from Elizabeth's blog, </i><a href=\"https://acesounderglass.com/\"><i>Aceso Under Glass</i></a><i>, with the author's permission. The author may not see or respond to comments on this post. You can </i><a href=\"https://acesounderglass.com/2022/08/31/impact-shares-for-speculative-projects/\"><i>see the original post here</i></a><i>.&nbsp;</i></p><h1>Introduction</h1><p>Recently I founded a new project with Jasen Murray, a close friend of several years. At founding the project was extremely amorphous (\u201cpreparadigmatic science: how does it work?\u201d) and was going to exit that state slowly, if it at all. This made it a bad fit for traditional \u201capply for a grant, receive money, do work\u201d style funding. The obvious answer is impact certificates, but the current state of the art there wasn\u2019t an easy fit either. In addition to the object-level project, I\u2019m interested in advancing the social tech of funding. With that in mind, Jasen and I negotiated a new system for allocating credit and funding.</p><p>This system is extremely experimental, so we have chosen not to make it binding. If we decide to do something different in a few months or a few years, we do not consider ourselves to have broken any promises.&nbsp;</p><p>In the interest of advancing the overall tech, I wanted to share the considerations we have thought of and tentative conclusions.&nbsp;</p><p>&nbsp;</p><figure class=\"image\"><img src=\"https://acesounderglass.files.wordpress.com/2022/08/dallc2b7e-2022-08-31-20.11.59-golden-ticket-for-one-impact-share-for-speculative-project.png?w=300\" srcset=\"https://acesounderglass.files.wordpress.com/2022/08/dallc2b7e-2022-08-31-20.11.59-golden-ticket-for-one-impact-share-for-speculative-project.png?w=300 300w, https://acesounderglass.files.wordpress.com/2022/08/dallc2b7e-2022-08-31-20.11.59-golden-ticket-for-one-impact-share-for-speculative-project.png?w=150 150w\"></figure><p>&nbsp;</p><h1>Considerations</h1><p>All of the following made traditional grant-based funding a bad fit:</p><ul><li>Our project is currently very speculative and its outcomes are poorly defined. I expect it to be still speculative but at least a little more defined in a few months.</li><li>I have something that could be called integrity and could be called scrupulosity issues, which makes me feel strongly bound to follow plans I have written down and people have paid me for, to the point it can corrupt my epistemics. This makes accepting money while the project is so amorphous potentially quite harmful, even if the funders are on board with lots of uncertainty.</li><li>When we started, I didn\u2019t think I could put more than a few hours in per week, even if I had the time free, so I\u2019m working more or less my regular freelancing hours and am not cash-constrained.</li><li>The combination of my not being locally cash-constrained, money not speeding me up, and the high risk of corrupting my epistemics, makes me not want to accept money at this stage. But I would still like to get paid for the work eventually.</li><li>Jasen is more cash-constrained and is giving up hours at his regular work in order to further the project, so it would be very beneficial for him to get paid.</li><li>Jasen is much more resistant to epistemic pressure than I am, although still averse to making commitments about outcomes at this stage.</li></ul><h1>Why Not Impact Certificates?</h1><p>Impact certificates have been discussed within Effective Altruism for several years, <a href=\"https://forum.effectivealtruism.org/topics/certificate-of-impact\">first</a> by Paul Christiano and Katja Grace, who pitched it as \u201caccepting money to metaphorically erase your impact\u201d. Ben Hoffman had a really <a href=\"http://benjaminrosshoffman.com/tag/impact-certificate/\">valuable addition</a> with framing impact certificates as selling funder credits, rather than all of the credit. There is currently <a href=\"https://impactmarkets.io/\">a project</a> attempting to get impact certificates off the ground, but it\u2019s aimed at people outside funding trust networks doing very defined work, which is basically the opposite of my problem.&nbsp;</p><p>What my co-founder and I needed is something more like startup equity, where you are given a percentage credit for the project, and that percentage can be sold later, and the price is expected to change as the project bears fruit or fails to do so. If six months from now someone thinks my work is super valuable they are welcome to pay us, but we have not obligated ourselves to a particular person to produce a particular result.</p><p>Completely separate from this, I have always found the startup practice of denominating stock grants in \u201c% of company\u201d, distributing all the equity at the beginning but having it vest over time, and being able to dilute it at any time, kind of bullshit. What I consider more honest is distributing shares as you go and everyone recognizes that they don\u2019t know what the total number of shares will be. This still provides a clean metric for comparing yourself to others and arguing about relative contributions, without any of the shadiness around percentages. This is mathematically identical to the standard system but I find the legibility preferable.&nbsp;</p><h1>The System</h1><h2>In Short</h2><ul><li>Every week Jasen and I accrue n impact shares in the project (\u201cimpact shares\u201d is better than the first name we came up with, but probably a better name is out there). n is currently 50 because 100 is a very round number. 1000 felt too big and 10 made anything we gave too anyone else feel too small. This is entirely a sop to human psychology; mathematically it makes no difference.</li><li>Our advisor/first customer accrues a much smaller number, less than 1 per week, although we are still figuring out the exact number.</li><li>Future funders will also receive impact shares, although this is an even more theoretical exercise than the rest of it because we don\u2019t expect them to care about our system or negotiate on it. Funding going to just one of us comes out of that person\u2019s share, funding going to both of us or the project at large, probably gets issued new shares.</li><li>Future employees can negotiate payment in money and impact shares as they choose.</li><li>In the unlikely event we take on a co-founder level collaborator in the future, probably they will accrue impact shares at the same rate we do but will not get retroactive shares.</li></ul><h2>Details</h2><h3>Founder Shares</h3><p>One issue we had to deal with was that Jasen would benefit from a salary right away, while I found a salary actively harmful, but wouldn\u2019t mind having funding for expenses (this is not logical but it wasn\u2019t worth the effort to fight it). We have decided that funding that is paying a salary is paid for with impact shares of the person receiving the salary, but funding for project expenses will be paid for either evenly out of both of our shared pools, or with new impact shares.&nbsp;</p><p>We are allowed to have our impact shares go negative, so we can log salary payments in a lump sum, rather than having to deal with it each week.</p><p>Initially, we weren\u2019t sure how we should split impact shares between the two of us. Eventually, we decided to fall back on the YCombinator advice that uneven splits between cofounders is always more trouble than it\u2019s worth. But before then we did some thought experiments about what the project would look like with only one of us. I had initially wanted to give him more shares because he was putting in more time than me, but the thought experiments convinced us both that I was more counterfactually crucial and we agreed on 60/40 in my favor before reverting to a YC even split at my suggestion.&nbsp;</p><p>My additional value came primarily from being more practical/applied. Applied work without theory is more useful than theory without application, so that\u2019s one point for me. Additionally all the value comes from convincing people to use our suggestions, and I\u2019m the one with the reputation and connections to do that. That\u2019s in part because I\u2019m more applied, but also because I\u2019ve spent a long time working in public and Jasen had to be coaxed to allow his name on this document at all. I also know and am trusted by more funders, but I feel gross including that in the equation, especially when working with a close friend.&nbsp;</p><p>We both felt like that exercise was very useful and grounding in assessing the project, even if we ultimately didn\u2019t use its results. Jasen and I are very close friends and the relationship could handle the measuring of credit like that. I imagine many can\u2019t, although it seems like a bad sign for a partnership overall. Or maybe we\u2019re both too willing to give credit to other people and that\u2019s easier to solve than wanting too much for ourselves. I think what I recommend is to do the exercise and unless you discover something really weird still split credit evenly, but that feels like a concession to practicality humanity will hopefully overcome.&nbsp;</p><p>We initially discussed being able to give each other impact shares for particular pieces of work (one blog post, one insight, one meeting, etc). Eventually, we decided this was a terrible idea. It\u2019s really easy to picture how we might have the same assessment of the other\u2019s overall or average contribution but still vary widely in how we assess an individual contribution. For me, Jasen thinking one thing was 50% more valuable than I thought it was, did not feel good enough to make up for how bad it would be for him to think another contribution was half as valuable as I thought it was. For Jasen it was even worse because having his work overestimated felt almost as bad as having it underestimated. Plus it\u2019s just a lot of friction and assessment of idea seeds when the whole point of this funding system is getting to wait to see how things turn out. So we agreed we would do occasional reassessments with months in between them, and of course we\u2019re giving each other feedback constantly, but to not do quantified assessments at smaller intervals.</p><p>Neither of us wanted to track the hours we were putting into the project, that just seemed very annoying.&nbsp;</p><p>So ultimately we decided to give ourselves the same number of impact shares each week, with the ability to retroactively gift shares or negotiate for a change in distribution going forward, but those should be spaced out by months at a minimum.&nbsp;</p><h3>Funding Shares</h3><p>When we receive funding we credit the funder with impact shares. This will work roughly like startup equity: you assess how valuable the project is now, divide that by the number of outstanding shares, and that gets you a price per share. So if the project is currently $10,000 and we have 100 shares outstanding, the collaborator would have to give up 1 share to get $100.</p><p>Of course, startup equity works because the investors are making informed estimates of the value of the startup. We don\u2019t expect initial funders to be very interested in that process with us, so probably we\u2019ll be assessing ourselves on the honor system, maybe polling some other people. This is a pretty big flaw in the plan, but I think overall still a step forward in developing the coordination tech.&nbsp;</p><p>In addition to the lack of outside evaluation, the equity system misses the concept of funder\u2019s credit from <a href=\"http://benjaminrosshoffman.com/minimum-viable-impact-purchases/\">Ben Hoffman\u2019s blog post</a> which I think is otherwise very valuable.&nbsp; Ultimately we decided that impact shares are no worse than the current startup equity model, and that works pretty well. \u201cNo worse than startup equity\u201d was a theme in much of our decision-making around this system.&nbsp;</p><h3>Advisor Shares</h3><p>We are still figuring out how many impact shares to give our advisor/first customer. YC has standard advice for this (0.25%-1%), but YC\u2019s advice assumes you will be diluting shares later, so the number is not directly applicable. Advisor mostly doesn\u2019t care right now, because he doesn\u2019t feel that this is taking much effort from him.&nbsp;</p><p>It was very important to Jasen to give credit to people who got him to the starting line of this project, even if they were not directly involved in it. Recognizing them by giving them some of his impact shares felt really good to him, way more tangible than thanking mom after spiking a touchdown.</p><h1>Closing</h1><p>This is extremely experimental. I expect both the conventions around this to improve over time and for me and Jasen to improve our personal model as we work.&nbsp; Some of that improvement will come from saying our current ideas and hearing the response, and I didn\u2019t want to wait on starting that conversation, so here we are.&nbsp;</p><p>Thanks to several people, especially <a href=\"https://manifold.markets/Austin\">Austin Chen</a> and <a href=\"https://www.lesswrong.com/users/raemon\">Raymond Arnold</a>, for discussion on this topic.</p>", "user": {"username": "Elizabeth"}}, {"_id": "pB6zRSH4Pekmh9Gmo", "title": "Join ASAP (AI Safety Accountability Programme)", "postedAt": "2022-09-10T11:15:27.692Z", "htmlBody": "<p><i><strong>Edit - this group was an experiment which I consider to have mostly been unsuccessful, and the group is no longer very active</strong>. I think the two main reasons for this were: (1) lacking a single thing which people were congregating around (e.g. the TransformerLens library in the case of the Open Source Mech Interp Slack group, or the ARENA course material in the case of the ARENA Slack), and (2) top-down rather than bottom-up design of the Slack group and its features.</i></p><hr><h1>TL;DR</h1><p>I'm creating a Slack group for people who are interested in working in AI safety at some point in the future, but who aren't working on it right now<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc9hptcm13ze\"><sup><a href=\"#fnc9hptcm13ze\">[1]</a></sup></span>, and would like extra accountability and motivation while they pursue their goals.&nbsp;</p><p>Join with <a href=\"https://join.slack.com/t/join-asap/shared_invite/zt-1kkzoa53n-ImLZZpiM9L2uoV_bH7Oh2A\">this link</a>!</p><h1>Why am I creating this?</h1><p>I just spent an awesome summer in Berkeley doing <a href=\"https://www.lesswrong.com/posts/3ouxBRRzjxarTukMW/apply-to-the-second-iteration-of-the-ml-for-alignment\">MLAB</a>, surrounded by people who are really passionate about AI safety, and this <strong>definitely</strong> had a positive impact on my level of motivation. I think trying to recreate some (even much weaker) version of that would be really valuable. An accountability system is the most basic version of this, because making commitments to other people is a really nice way of motivating yourself to get shit done!</p><p>I've spoken to a few people from MLAB, and several seem to agree (at least five participants have mentioned to me that they'd like to join a group like this).</p><h1>How will this work?</h1><p><i>(Note - this might all be changed depending on how many people join, and their suggestions &amp; preferences. Hopefully by the end of next week, the group will be larger and we will have made many improvements to this basic design!)</i></p><p>The core mechanism of the group will be everyone posting <strong>regular short updates (might be Slack message, or filling out a Google Form)</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkx6ok36rxec\"><sup><a href=\"#fnkx6ok36rxec\">[2]</a></sup></span>&nbsp;(maybe once per 2 weeks) summarising what they've done over that period. For instance:</p><ul><li>Books you've read, or courses you've taken, or progress in structured self-study like <a href=\"https://www.lesswrong.com/posts/uLstPRyYwzfrx3enG/levelling-up-in-ai-safety-research-engineering\">this</a></li><li>Blog posts you've written</li><li>Projects you've done, or are doing, and your progress on them</li><li>Companies or other opportunities you've applied for</li><li><s>Dank AI memes you've designed</s></li></ul><p>There will also be optional extra commitment mechanisms like weekly Zoom calls. Also if enough people join (e.g. more than 6) then we'll probably divide people into smaller groups for personal check-ins, since larger groups tend to lead to individuals feeling less accountability. Progress reports will still be posted to the main Slack channel.</p><figure class=\"image image_resized\" style=\"width:78.58%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/sqjiwfpx0kl127pt5cnz\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/audtsvkru2fhr8zkkkny 81w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/guwnxp2dwx2eoxhdunjz 161w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/x3b5ewqqmpwtmn2bxhfw 241w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/hjgbjqkyu2apl3chtnxx 321w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/r61v4ntyceeukpccqy0h 401w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/kljbeagt80sxw8jdlvir 481w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/lsuy1c61gcagurb9l577 561w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/kspq4tqm07shjztjzuk4 641w\"></figure><h1>Who is this group for?</h1><p>I expect it will be most useful for you if one or more of the following holds:</p><ul><li>You aren't yet contributing to AI alignment directly, but you think you might at some point in the future.</li><li>You aren't necessarily surrounded by a group of people who are also working on AI safety.</li><li>You have a particular idea for ways you want to skill up / projects you want to do / places you want to apply for, but lack the motivation to do it.</li><li>You are interested in accountability systems, but don't have the ability to make regular time commitments.</li></ul><p>None of these are totally necessary, so feel free to join regardless if you think you'd benefit from this group.&nbsp;</p><h1>Why am I making this group, when there are other pre-existing groups that might work for this purpose?</h1><p>Three main reasons:</p><ol><li>I don't think size is necessarily an advantage in an accountability group. Larger groups can diminish the effectiveness, whereas <strong>smaller groups can often better supply accountability to each individual, and encourage a sense of community</strong>.</li><li>I think having an entire Slack dedicated to this mechanism, rather than just one channel of a larger Slack group, has lots of benefits. For instance, <a href=\"https://ai-alignment.slack.com\">AI Alignment Slack</a> has a study buddies channel, but since this isn't the primary focus of this group I expect ASAP to have a comparative advantage in providing motivation and accountability.</li><li>Many pre-existing AI safety related Slack / Discord groups have a much more specific topical focus (e.g. the <a href=\"https://join.slack.com/t/alignmentstudies/shared_invite/zt-1fexljmvb-~v3FiWGSZzfu5rPXKD4aiw#\">Alignment Studies</a> Slack group, which is mainly focused on the MIRI course list). I imagine most people would benefit from more flexibility, since everyone will probably be doing slightly different things.</li></ol><h1>What is the end goal?</h1><p>I created and joined the Slack group at the start of this week, and so far at least five people have expressed preferences to join. So by simple laws of exponential progression, I expect we'll reach the population of earth in approximately 14 weeks, or just before the end of 2022. The resulting galaxy-brained AI safety community would almost certainly be able to solve the alignment problem right away.</p><p>Just in case this doesn't succeed, some decent fallback goals would be:</p><ul><li>Encouraging more people to stick to their targets, and providing positive reinforcement.</li><li>Motivating people to keep skilling up in AI safety, and helping them take steps towards making direct contributions to the field in the future if they aren't there yet.</li></ul><h1>Why did you call it the \"AI Safety Accountability Programme\"?</h1><p>So I could title this post \"Join ASAP\" and no other reason.</p><h1>Last words</h1><p>Join ASAP, and come blast off from the land of amotivation into the stratosphere of becoming awesome <a href=\"https://emojipedia.org/upside-down-face/\"><u>\ud83d\ude43\ud83d\ude80</u></a></p><figure class=\"image image_resized\" style=\"width:58.67%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/mnn6jp8vckzp4ejurxs9\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/sygb5imr1zcxlyr7bniv 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/ny86ryp73tseau9jc9vb 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/myrnqkidhr8ywnozstzo 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/ksz14yz3mdmsblklqlou 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/es5gxho1ys3298v9laga 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/le6tqmxkquomoqvfp7er 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/mnkwt85u1kckua8xmvmh 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/rsktlqoqzkmivgr6gi1w 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/qrfkrszj0uephesednl8 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pB6zRSH4Pekmh9Gmo/bfuazmfitmylnuisusjh 1024w\"></figure><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc9hptcm13ze\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc9hptcm13ze\">^</a></strong></sup></span><div class=\"footnote-content\"><p>People who are already working in AI safety but would like to join an accountability group are of course also welcome, although I expect they'd get slightly less utility out of this group.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkx6ok36rxec\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkx6ok36rxec\">^</a></strong></sup></span><div class=\"footnote-content\"><p>These are just examples; people can feel free to update in whatever form works best for them.</p></div></li></ol>", "user": {"username": "Callum McDougall"}}, {"_id": "iMYbtfTizhC5C4mBo", "title": "[Link post] Optimistic \u201cLongtermism\u201d Is Terrible For Animals", "postedAt": "2022-09-06T22:38:48.863Z", "htmlBody": "<p>Oxford philosopher William MacAskill\u2019s new book, <a href=\"https://www.basicbooks.com/titles/william-macaskill/what-we-owe-the-future/9781541618633/\"><i>What We Owe the Future</i></a>, caused <a href=\"https://twitter.com/sapinker/status/1557072430836944896?s=21&amp;t=V6K65FWhj7MvfVKxYlOrxA\">quite a stir</a> this month. It\u2019s the latest salvo of effective altruism (EA), a social movement whose adherents aim to have the greatest positive impact on the world through use of strategy, data, and evidence. MacAskill\u2019s new tome makes the case for a growing flank of EA thought called \u201clongtermism.\u201d <a href=\"https://www.bbc.com/future/article/20220805-what-is-longtermism-and-why-does-it-matter#:~:text=Longtermism%20is%20the%20view%20that,might%20be%20in%20shaping%20it\">Longtermists</a> argue that our actions today can improve the lives of humans way, way, way down the line \u2014 we\u2019re talking billions, trillions of years \u2014 and that in fact it\u2019s our moral <a href=\"https://www.vox.com/future-perfect/2021/11/3/22760718/patient-philanthropy-fund-charity-longtermism\">responsibility</a> to do so.</p><p>In many ways, longtermism is a straightforward, uncontroversially good idea. Humankind has long been concerned with providing for future generations: not just our children or grandchildren, but even those we will never have the chance to meet. It reflects the Seventh Generation Principle held by the indigenous Haudenosaunee (a.k.a. Iroquois) people, which <a href=\"https://www.pbs.org/warrior/content/timeline/opendoor/roleOfChief.html\">urges</a> people alive today to consider the impact of their actions seven generations ahead. MacAskill echoes the defining problem of intergenerational morality \u2014 people in the distant future are currently \u201c<a href=\"https://www.nytimes.com/2022/08/09/podcasts/transcript-ezra-klein-interviews-will-macaskill.html\">voiceless</a>,\u201d unable to advocate for themselves, which is why we must act with them in mind. But MacAskill\u2019s optimism could be disastrous for non-human animals, members of the millions of species who, for better or worse, share this planet with us.</p><p><i>Read the rest on </i><a href=\"https://www.forbes.com/sites/briankateman/2022/09/06/optimistic-longtermism-is-terrible-for-animals/\">Forbes</a>.</p>", "user": {"username": "BrianK"}}, {"_id": "uFfa6JutYkE3qm6CG", "title": "Like The Setting Sun", "postedAt": "2022-09-06T19:23:26.923Z", "htmlBody": "<p>To celebrate his graduation from the academy and immediate hire, the new detective vacationed in the eternal city. He flew in on a Sunday and spent the week enjoying the tourist attractions, the food, the music. All the while, he was excited to return home, to protect the innocent, to bring some peace to the world.</p>\n<p>On a whim, he booked a train to return home through the scenery of that country. The farms were colored like nothing he had seen and when the train passed by the ocean the detective was happy to be alive in a world of such beauty.</p>\n<p>On his way back from the washroom, hidden behind the glare of the evening sun, he saw the body slouching over into the aisle, blood slinking down its neck.</p>\n<p>The detective called for staff and announced to the traincar that riders must stay in their seats, a man had been killed. Oddly, there was no response from them. Even the other man in the row was sleeping peacefully. No matter. The detective set to the work of his new profession. He examined the body. There was no sign of struggle, but the throat was cleanly slit. Male, older, baggage undisturbed.</p>\n<p>The staff manager arrived. \"Alert the conductor and call the police station ahead,\" said the detective. \"There's been a murder.\"</p>\n<p>He paused. He had been taught that civilians often reacted poorly to death, but the manager only nodded absentmindedly. \"Alright.\"</p>\n<p>\"And I need to talk to the staff who-\"</p>\n<p>\"Pardon me.\" Here the manager raised his hand in a stopping motion. \"Staff won't be able to perform their duties with constant interruptions.\"</p>\n<p>The detective stared at him in disbelief. \"Sir, a man has been murdered.\"</p>\n<p>The manager looked at his watch distractedly. \"Yes, I heard you the first time. You can ask questions of my staff, but you cannot become upset with them when they need to perform their other duties. And please, try not to disturb our riders so.\" He tipped his hat and walked away, leaving the detective to ponder if there was some cultural divide, yet he saw men and women of all races and nations on the train.</p>\n<p>It only got worse. A staff member came to cover the body and the detective pulled out his notebook to question her. Had she seen anybody go into the car? She seemed to think about it while unfolding the blanket to cover the body. Then she said: \"It seems like you have an unhealthy fixation upon this whole thing. What you want is peace, isn't it?\"</p>\n<p>\"No, I want to catch his killer.\"</p>\n<p>\"Ahh. I see. You're worried for your own life.\" She draped the blanket over the sprawled body. He opened his mouth to protest that it was his duty to catch the murderer, but she continued. \"I didn't see anybody come in. Who could be expected to watch for such things?\"</p>\n<p>The detective moved on, but the riders were no better. He gently shook to waking the other man in the row, elderly and small. \"How does that old song go?\" asked the feeble man. \"'People die every day, just like the setting sun'.\"</p>\n<p>\"Sir, there's a killer on the loose. You're in danger until he's caught.\"</p>\n<p>\"I suppose you're right.\" The man shifted in his seat. He looked too brittle to stay warm as the light of the sun abandoned them. \"But if it's my time, it's my time. I'll only suffer if I resist the truth.\"</p>\n<p>The detective moved on to the food car at the back of the train. The server, a tall, gaunt man, stooped down to shake his hand. He listened thoughtfully to the detective's explanation without interruption. Finally, he asked: \"Did the man struggle, at the end?\"</p>\n<p>\"No\", said the detective.</p>\n<p>\"Oh\", said the server. \"And what about his family? Were they made to watch?\"</p>\n<p>\"If he has family, they were not with him. He rode alone.\"</p>\n<p>A great and peaceful smile came over the server. \"Then this is good news indeed. Detective, a victimless crime is a paradox, and we are without a victim.\"</p>\n<p>\"Without a victim!\" the detective sputtered.</p>\n<p>\"He passed into peace, as will we all.\"</p>\n<p>Something broke inside the detective. \"Everything about this is wrong\", he said while jumping to his feet. \"Something is horribly wrong on this train.\"</p>\n<p>He marched back through the entire length of the train. He passed through the car of the dead man and had to shoo away children crawling over the body blanketed like a ghost. He passed through cars full of riders and shouted for anybody, anybody with information about the murder, to come forward. Mothers shushed him and fathers shrugged. He marched through the staff's sleeper car while the staff manager harangued him for disturbing the peace. The detective ignored him and reached the engine car.</p>\n<p>In the engine car sat another tiny old man, this one in a conductor's hat, watching a map on a screen, surrounded by a bank of dusty controls.</p>\n<p>\"Hello\", said the detective. \"I'm hoping you can explain something to me.\"</p>\n<p>\"I can try,\" replied the conductor.</p>\n<p>\"What's wrong with this train?\" asked the detective. \"Nobody's bothered that a man has died. Nobody's worried that the killer is still among us. Something's off.\"</p>\n<p>The conductor sighed. \"I was afraid you'd feel this way. For a detective, you certainly aren't observant.\"</p>\n<p>The detective said nothing. The conductor spoke with gravity and the detective was ready to learn of the clue he had missed, the piece that would unlock the puzzle.</p>\n<p>\"Tell me. When did you board this train?\"</p>\n<p>\"Only an hour ago\", admitted the detective. \"I had barely boarded when I saw the body.\"</p>\n<p>\"I thought so,\" the conductor said gently. \"This train has been roaring away for much, much longer than that. The killer has been among us since the beginning. The victim is only the latest in a long line. Many men like you have tried to find the killer, to no avail. My staff and I are wiser. We make our peace with it.\"</p>\n<p>The detective started to protest and for the first time, the conductor raised his voice.</p>\n<p>\"Detective! Are you blind to the thousand issues on my train? Have you not seen the shivering of my cold riders? Many of whom have taken on incredible debt for this journey? Or their children, who are morally defunct? Or my staff, who must serve a hundred masters? Your focus has been myopic. If my staff were to solve every other problem on this train, then the last enemy would be the killer. Until then, we must concern ourselves with every affair. Please, return to your seat, and pity the living.\"</p>\n<p>Properly chastised, the detective went back to his seat. He apologized to the staff manager. He gave his own blanket to the feeble passenger. He watched the blue afterglow of the sun turn to black. Far in the distance the mountains were crowned with stars. He counted them again and again until he fell asleep. Later, much later, the killer opened the door. He walked through the lightless car without hesitation or uncertainty until he stood over the detective. His work was performed quickly but elegantly. Then he left the car and resumed his work on the rest of the train.</p>\n", "user": {"username": "Ideopunk"}}, {"_id": "ErrJc34wChZW8oHYN", "title": "What is neglectedness, actually?", "postedAt": "2022-09-06T18:25:27.886Z", "htmlBody": "<p>The importance, tractability, neglected framework: while working on important, tractable issues makes sense, <strong>what does neglectedness tell us?</strong></p><p>The common response I hear in the community is that there exist diminishing marginal returns on working on an issue; however, I believe this explanation alone does not capture the full story. (And neither do a lot of EA organizations.)</p><p>Here, I explain what I believe to be a better catchphrase to explain the importance of neglectedness: <strong>contingency.</strong></p><h3><strong>A focus on neglectedness has multiple upsides that all deviate but are related:</strong></h3><ul><li><strong>(1)</strong> <strong>80/20: </strong>There are diminishing marginal returns on the n-th person working on an issue because all of the \u201clow-hanging fruits\u201d \u2013 e.g. easy research, development, and implementation of solutions to a cause area \u2013 will be taken early. <strong>This is the common argument for neglectedness, as it is understood \u2013 the 80/20 rule.</strong></li><li><strong>(2)</strong> <strong>Initial</strong> <strong>Momentum: </strong>The \u201cfounders\u201d of a cause area take the time to set up the infrastructure for a new cause area \u2013 such as talent searching &amp; recruiting; institution-building; securing funding; marketing; and building the worldview, values, and norms of a space. Helping to get the \u201cball rolling\u201d \u2013 e.g. for neglected research fields within the AI alignment or biosecurity space \u2013 feels extraordinarily impactful, and distinct from pure diminishing marginal return from direct progress.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcwt2h44j758\"><sup><a href=\"#fncwt2h44j758\">[1]</a></sup></span></li><li><strong>(3) Meta-Level Reminder: </strong>Neglectedness serves as an acknowledgement that the most tractable thing to do may not be in the most \u201cpopular fields\u201d or the ones that come directly to mind, serving as an epistemic reminder to be more open-minded and venture out further for the most effective solutions.</li></ul><p>These arguments are <strong>important to differentiate</strong>. Unfortunately, pure diminishing returns tends to be visualized as an extension of direct tractability (1) by newcomers<i>.&nbsp;</i></p><p>However, especially for longtermists, (2) and (3) are significant reasons for working on neglected fields; working on extremely neglected issues should thus be viewed as having a cascading, systemic impact. Bringing a field to prominence and looking for new fields to start entrepreneurial projects in is extremely important. Longtermists usually find neglected fields to establish <strong>systems</strong> \u2013 build new infrastructure, bring in funding, bring together stakeholders, and gradually establish a proper research field with its own literature and institutions \u2013 in a cascading butterfly effect where more people, talent, and ideas get brought in.</p><p><strong>Community-builders:</strong> how do we capture this, in a quick phrase? How do we replace \"diminishing marginal returns\" with something better?</p><h3><strong>The better argument in favor of neglectedness</strong></h3><p>Furthermore, diminishing marginal returns on a cause area tends to hold true, but has its exceptions. For example, what if you're able to leverage resources in a larger cause area? One may be able to climb to a powerful position &amp; redirect resources from a popular cause area, helping to push them toward more tractable ways of working on the cause area.&nbsp;</p><p>This is generally a special exception, but speaks to how <strong>\"what can the marginal person do?</strong>\" is too economic theory-esque, doesn't directly relate to actionability, and isn't generalizable.</p><p><strong>Therefore, in explaining EA to newcomers, I've found the need to rephrase and connect \"neglectedness\" into \"contingency.\"</strong></p><p>Having a <strong>contingent</strong> impact is just a better explanation for the importance of neglectedness: in the absence of your intervention, how would have things have played out?&nbsp;</p><ul><li>If the same, you likely didn't make that big of an impact.</li><li>If differently, you did.</li></ul><p>Newton &amp; Leipzig invented calculus at around the same time, so the invention of calculus may not have been contingent &amp; was bound to happen anyways. However, certain events &amp; movements likely were, especially key historic elections, formational institution-building, Cold War-era nuclear policy, and (hopefully) longtermist AI alignment efforts.&nbsp;</p><p>To tie it all together definitionally, very contingent efforts can be found in interventions that are neglected in the status quo.</p><p>We can then rephrase looking for <strong>neglected</strong> <strong>sub-areas</strong> of popular cause areas into looking at which points of leverage you can apply the most <strong>contingent impact</strong>, which generalizes the point more cleanly.</p><hr><p><strong>Edit Update: </strong>The<strong> contingency </strong>part <i><strong>was</strong></i> inspired by <i>What We Owe the Future</i>'s significance, persistency, contingency framework. At the time of this article's posting, however, I didn't read the appendix of<i>What We Owe the Future </i>where Will MacAskill suggests renaming <i>neglectedness</i> to <i>leverage</i>. I suppose we were both thinking similarly!</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncwt2h44j758\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcwt2h44j758\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Even if this effect is captured in the overall \"diminishing marginal returns\" argument in economics, it feels important enough to differentiate as as separate reason.</p></div></li></ol>", "user": {"username": "Richard Ren"}}, {"_id": "6wwK6Qduxt7mMmj8k", "title": "Announcing the Change Our Mind Contest for critiques of our cost-effectiveness analyses", "postedAt": "2022-09-06T18:10:18.374Z", "htmlBody": "<p><em>Author: Isabel Arjmand, GiveWell Special Projects Officer</em></p><p>We\u2019re extremely excited to be announcing the Change Our Mind Contest to encourage critiques of our cost-effectiveness analyses that could lead to substantial improvements of our overall allocation of funds. For all the details, see <a href=\"https://www.givewell.org/research/change-our-mind-contest\">this page</a>.</p><p>Cost-effectiveness is the single most important input in our decisions about what programs to recommend, and we believe it\u2019s possible that we\u2019re missing important considerations or making mistakes that lead us to allocate funding suboptimally. We\u2019ve been excited to see people engaging with our cost-effectiveness analyses, and we\u2019d like to inspire more of that engagement.</p><p>With that in mind, we\u2019re inviting you to identify potentially important mistakes or weaknesses in our existing cost-effectiveness analyses and tell us about them!</p><p>The first-place winning entry will receive $20,000, the runner-up will receive $10,000, and the honorable mention will receive $5,000. We may offer multiple runner-up and honorable mention prizes if the quality of submissions is particularly high. All other entries that meet our criteria will receive a participation prize of $500, capped at a total of 50 participation prizes for the first 50 submissions.</p><p>In addition to the monetary prizes, excellent entries may lead to changes in how we allocate millions of dollars of funding, leading to more lives saved or improved.</p><p>Entries must be received by October 31, 2022, and the requirements are described in detail on the <a href=\"https://www.givewell.org/research/change-our-mind-contest\">contest page</a>. We will announce winners by December 15, 2022, and will publish the winning entries online. Work that was submitted to the <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming\">EA Criticism and Red Teaming</a> contest is eligible as long as it meets the requirements stated on our <a href=\"https://www.givewell.org/research/change-our-mind-contest\">contest page</a>; authors are also welcome to submit an edited version of work they submitted to that contest, and might want to do so in order to better tailor their work to the prompt on <a href=\"https://www.givewell.org/research/change-our-mind-contest\">this page</a>. If you're posting your submission here on the EA Forum, please tag it with <strong>GiveWell Change Our Mind Contest</strong> so Forum readers can easily find all entries published here.</p><p>If you have any questions, please leave a comment on this post or email <a href=\"mailto:change-our-mind@givewell.org\">change-our-mind@givewell.org</a>.</p><p>We\u2019re running this contest because the recommendation decisions we make are extremely important, and we want to incentivize feedback that will improve our work, thereby enabling us to do more good. We hope you\u2019ll consider participating!</p>", "user": {"username": "GiveWell"}}, {"_id": "FKLAbyKwZt3KP4s5c", "title": "The discount rate effectively determines whether long- or near-termism is the best use of philanthropic resources", "postedAt": "2022-09-06T14:52:24.843Z", "htmlBody": "<p><i>Epistemic status: I am quite worried this is a very obvious point that doesn\u2019t need its own post. However, I've never seen this point made in this way on the forum, and it seems like it might advance a difficult debate. So I hope that at the very least spelling out the obvious is useful in that sense!</i></p><p>As I understand the state of the near-term vs long-term debate in EA, <strong>your view on the discount rate (almost) entirely determines whether you should be a longtermist or not</strong>.&nbsp;</p><p>Here is a sketch of the longtermist case as I understand it: \u201cThere is a lot of suffering in the world now which we could avert by using scarce resources, but if instead we used those resources to avert future suffering (especially but not exclusively suffering related to existential risk) we would have much greater bang for our buck. This is because we expect the human population to multiply significantly&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1dqcddxzgf2\"><sup><a href=\"#fn1dqcddxzgf2\">[1]</a></sup></span>, and hence while the best imaginable near-term intervention might help a few hundred people, interventions which prevent future suffering will help trillions of people. While we accept that there may be a case for discounting the value of future lives, the key beats of this argument are unaffected since we\u2019re not talking about even the same order of magnitude of impact when we talk about near- vs far- term\u201d.&nbsp;</p><p>The table below rigorously models this statement, showing the total value of all possible future human lives under different rates of increase in happiness and discount rates. The numbers are indexed to our current happiness \u2013 so for example if the total happiness in the world is currently a million QALYs, the total happiness that will be generated is a \u201867\u2019 scenario is 67 x one million, or 67 million QALYs. Cells shaded in blue are \u2018effectively infinite\u2019 \u2013 although they have a finite value in principle, in practice the number is so enormously large we should immediately drop what we are doing and become a long-termists if we think we can make these near-infinite QALY streams even a fraction of a percent more likely to come about.&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e416ace5e22a3bc4c0bd68a2b1f8cb26ca0f25f40e19cc8.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e416ace5e22a3bc4c0bd68a2b1f8cb26ca0f25f40e19cc8.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e416ace5e22a3bc4c0bd68a2b1f8cb26ca0f25f40e19cc8.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e416ace5e22a3bc4c0bd68a2b1f8cb26ca0f25f40e19cc8.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e416ace5e22a3bc4c0bd68a2b1f8cb26ca0f25f40e19cc8.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e416ace5e22a3bc4c0bd68a2b1f8cb26ca0f25f40e19cc8.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e416ace5e22a3bc4c0bd68a2b1f8cb26ca0f25f40e19cc8.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e416ace5e22a3bc4c0bd68a2b1f8cb26ca0f25f40e19cc8.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e416ace5e22a3bc4c0bd68a2b1f8cb26ca0f25f40e19cc8.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e416ace5e22a3bc4c0bd68a2b1f8cb26ca0f25f40e19cc8.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e416ace5e22a3bc4c0bd68a2b1f8cb26ca0f25f40e19cc8.png/w_940 940w\"></figure><p><i>NB The type of analysis here is 'threshold analysis' - I wrote </i><a href=\"https://forum.effectivealtruism.org/posts/CuuCGzuzwD6cdu9mo/methods-for-improving-uncertainty-analysis-in-ea-cost\"><i>an essay </i></a><i>about alternative approaches to uncertainty as part of the Red Teaming contest and you may be able to spot that there are ways to improve this analysis by applying more of those techniques to the problem. This post is just a sketch of the topic rather than a full analysis of the solution under uncertainty!</i></p><p>Three relevant points that come out from this table:</p><ul><li>There is a definite and sharp threshold where you are functionally \u2018forced\u2019 to be a long-termist. Specifically, this is where the rate of growth of happiness is greater than or equal to the discount rate, and so the total happiness in the future is effectively infinite. At any discount rate greater than this, the choice of near- vs long-termism depends on the specifics of the intervention being considered. This is interesting because hard discontinuities are inherently quite interesting \u2013 very tiny changes in your belief can lead to enormously outsized effects on how you live your life.</li><li>The actual values we assign to \u2018growth in happiness\u2019 and \u2018discount rate\u2019 aren\u2019t actually that important; what matters is the difference between them. A world where happiness grows at 3% per year but future happiness is discounted at 1% per year is the same as a world where happiness grows at 2% per year but we don\u2019t discount.&nbsp;I've seen a lot of people argue about the discount rate without realising that they need to <strong>simultaneously </strong>argue about the rate in growth of happiness for the statement to be meaningful as a guide to action on longtermist vs neartermist philanthropy.</li><li>In keeping with the economics literature, I\u2019ve used exponential discounting in the table above, meaning that QALYs are being \u2018front-loaded\u2019&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnop0fuh5r8k\"><sup><a href=\"#fnnop0fuh5r8k\">[2]</a></sup></span>. So for example in a scenario of a 1% growth rate, 2% discount rate we can calculate that 63% of all benefits accrue in the first hundred years (and 39% in the first fifty years). The table below shows how the delta between growth and discount rate affects the percentage of benefits accrued in the first 50 and 100 years. This is relevant to the long-termism debate, because long-term interventions are (mostly) intended to pay off in the future with no impact on happiness now. Although there's no equivalent 'sharp' threshold for when you're forced to be a near-termist, my read is if your discount rate is \u22654% it starts to become really hard to justify any long-term philanthropy \u2013 any intervention which only pays off after 100 years is basically pointless in this scenario.</li></ul><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c581b107e679239cdb26d2f99a46d4e1c489215d9a1e2039.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c581b107e679239cdb26d2f99a46d4e1c489215d9a1e2039.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c581b107e679239cdb26d2f99a46d4e1c489215d9a1e2039.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c581b107e679239cdb26d2f99a46d4e1c489215d9a1e2039.png/w_260 260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c581b107e679239cdb26d2f99a46d4e1c489215d9a1e2039.png/w_340 340w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c581b107e679239cdb26d2f99a46d4e1c489215d9a1e2039.png/w_420 420w\"></figure><p>I have also highlighted my best guess for what the actual growth in happiness and discount rate should be<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo23woq17rc\"><sup><a href=\"#fno23woq17rc\">[3]</a></sup></span>&nbsp;(in red). You can see that it straddles the line of \u2018definitely be a long-termist\u2019 and \u2018depends on the quality of intervention\u2019 very neatly. I suspect this is why debates about long and short-termism can be a little acrimonious; people think they are arguing about the quality of longtermist interventions, but they are actually swapping intuitions about whether the discount rate should be fractionally above 1.5% or fractionally below 1.5%, so nothing gets resolved.</p><p>The numbers themselves aren\u2019t as important as the general shape of the output table, but just by way of a simple numerical demonstration; imagine the world currently generates about 6.2 gigaQALYs per year. You can choose to spend your philanthropic dollars on a donation to AMF which (very roughly) might give 0.01 QALY per dollar donated. Alternatively, you could choose to spend your philanthropic dollars on an AI alignment foundation, which has a 0.1% chance of being fundamental in some sense to preventing an extinction-level event from occurring in 80 years given an operating budget of $7.5m / year&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpbhuoyho5fd\"><sup><a href=\"#fnpbhuoyho5fd\">[4]</a></sup></span>. Which is the better option? The answer is that it is impossible to say without knowing the discount rate; in scenarios where the growth in happiness is greater than the discount rate, the AI foundation is infinitely better. In scenarios where the delta between growth in happiness and discount rate is less than 4% the AI foundation is still better, but the decision is becoming increasingly close. In scenarios where the delta is greater than 4%, the AMF is better.</p><p>My core conclusion is this; if you believe the discount rate is likely less than or equal to the rate of growth in happiness over time, you should be a long-termist. If you believe anything else then the terms 'long-termist' and 'near-termist' are basically meaningless; there is a specific rate at which you should be prepared to trade off QALYs now against QALYs in the future, and this tradeoff doesn't inherently favour long-term interventions; it depends on the specifics of the intervention and your own views about the discount rate.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1dqcddxzgf2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1dqcddxzgf2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Actually it is a little more involved than this \u2013 I\u2019m simplifying for brevity\u2019s sake. My actual understanding of the argument is that the future might matter more than the past because of the interaction between three mechanisms:</p><blockquote><p>&nbsp;1) More people will be born, and these people will have their own stream of QALYs we can add to the total</p><p>2) Each person who is born will likely live longer in the future than they do now, so at the margin they have some QALYs that people born now do not</p><p>3) The moment-by-moment happiness of each individual might be higher in the future, for example because they worry less about disease and famine.</p></blockquote><p>This doesn't really change anything I wrote above, but it does affect how confident we can be that we've nailed the rate of growth of happiness when we move ahead of simple demographic projections - these three elements will interact with each other in complex ways.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnop0fuh5r8k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnop0fuh5r8k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>'Front-loading' is a feature of almost all possible models of discount rates, but in my experience exponential models are not especially front-loaded compared to other plausible specifications. The most common alternative specification to an exponential model is a hyperbolic model though, and that is a famously back-loaded approach. Note also that we don't have to have a fancy mathematical name for the shape of our discount rate - Will MacAskill <a href=\"https://forum.effectivealtruism.org/posts/vi8FP936nasxqGowX/on-discount-rates\">has written</a> that his discount rate for EA looks like \"30% for the next couple of years ... tailing down to something close to 0% (and maybe even going negative for a while) after 5 or 6 years\". This is a perfectly sensible specification of a discount rate, it just doesn't have a mathematical name (it looks a bit hyperbolic-y if you squint).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno23woq17rc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo23woq17rc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Very roughly, world population growth has been about 2% / year historically and tending downwards to about 1% / year in economically developed countries (i.e. what we hope the future will look like).&nbsp;</p><p>Life expectancy has been increasing at about 0.5% / year. However, I think this is slightly misleading because these hugely impressive gains mostly come from averting infant mortality. Averting infant mortality is extremely important but a \u2018finite\u2019 source of life expectancy improvement when it comes to very long-term scenario planning because eventually every country will have infant mortality rates near zero. In the UK life expectancy increase is more like 0.1% / year, which is probably a more credible long-term assumption.</p><p>Moment-by-moment happiness actually seems to be decreasing in economically developed countries, but maybe we think we are living in a particularly dismal period and in fact this trend might reverse in the future which would give a rate of about 0.25% / year.</p><p>All this by way of saying that based on current trends we\u2019d expect the total happiness of the world to increase by no more than 3% / year, and probably a bit below 2% / year as countries move through demographic transition. This would look like the below:</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7f6ec2e63cd2b8ab1cde118bf0ab50d0f75c1f41a01808.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7f6ec2e63cd2b8ab1cde118bf0ab50d0f75c1f41a01808.png/w_112 112w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7f6ec2e63cd2b8ab1cde118bf0ab50d0f75c1f41a01808.png/w_192 192w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7f6ec2e63cd2b8ab1cde118bf0ab50d0f75c1f41a01808.png/w_272 272w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7f6ec2e63cd2b8ab1cde118bf0ab50d0f75c1f41a01808.png/w_352 352w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7f6ec2e63cd2b8ab1cde118bf0ab50d0f75c1f41a01808.png/w_432 432w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7f6ec2e63cd2b8ab1cde118bf0ab50d0f75c1f41a01808.png/w_512 512w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7f6ec2e63cd2b8ab1cde118bf0ab50d0f75c1f41a01808.png/w_592 592w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7f6ec2e63cd2b8ab1cde118bf0ab50d0f75c1f41a01808.png/w_672 672w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7f6ec2e63cd2b8ab1cde118bf0ab50d0f75c1f41a01808.png/w_752 752w\"></figure><p>Discount rate is just the discount rate used by the UK government, less pure time preference.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpbhuoyho5fd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpbhuoyho5fd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Numbers picked because they are sort of plausible (I can defend them in the comments if requested), but mostly because they give a nice neat answer at the end of the calculation!</p></div></li></ol>", "user": {"username": "Froolow"}}, {"_id": "eRhyHxFYSK6JukgaX", "title": "How can we secure more research positions at our universities for x-risk researchers?", "postedAt": "2022-09-06T14:41:27.308Z", "htmlBody": "<p><i>Cross-posted from </i><a href=\"https://www.lesswrong.com/posts/PsdnTrwvHp95Nu2B7/how-can-we-secure-more-research-positions-at-our\"><i>LessWrong</i></a><i>.</i></p><p>Has anyone had experience with this? Is anyone working on this?<br><br>Reading <a href=\"https://www.lesswrong.com/posts/4jFnquoHuoaTqdphu/ai-x-risk-reduction-why-i-chose-academia-over-industry\">this</a> and <a href=\"https://www.lesswrong.com/posts/HXxHcRCxR4oHrAsEr/an-update-on-academia-vs-industry-one-year-into-my-faculty\">this</a> give me reason to think that it could be more impactful for some x-risk researchers to have research positions at universities.</p><p>However:</p><ul><li>Universities are financially constrained with regard to the number of researchers they can hire</li><li>When they <i>can </i>hire, they rarely select candidates based on their expected contribution to safeguarding future lives</li><li>Successful candidates are immediately tasked with non-research obligations which take up a substantial proportion of their time</li></ul><p>Can we remove these hurdles? Here is my proposal. <strong>A new or existing EA org establishes agreements with university departments/schools whereby:</strong></p><ol><li>The org decides on which researcher to hire (with the department having the ability to veto)</li><li>The org funds the researcher</li><li>The department treats the researcher as one of its own, except without the non-research obligations</li></ol><p>I do recognise that currently, university researchers can and do get EA funding which enables them to focus on their high-EV research. An example that comes to mind is Arif Ahmed at the University of Cambridge who, in 2019, received funding from CLR. Additionally, there are research institutes which are well-integrated at universities (e.g. GPI at the University of Oxford). Are these models better? Could my proposal still be impactful and worth pursuing?</p><p>If anyone has any experience with or thoughts on this, please comment below or reach out. You'll find my details on my profile page.</p>", "user": {"username": "Neil Crawford"}}, {"_id": "nhbeKbwMgFKfrzLNb", "title": "Marketing Messages Trial for GWWC Giving Guide Campaign ", "postedAt": "2022-09-08T16:22:43.860Z", "htmlBody": "<p>The trial was run in conjunction with Josh Lewis (NYU). Thanks to David Moss and others for feedback on this post, and to Jamie Elsey for support with the Bayesian analysis.&nbsp;</p><h2>TL;DR</h2><p>Giving What We Can together with the <a href=\"https://forum.effectivealtruism.org/posts/HboobjbDwc5KgpNWi/ea-market-testing\"><u>EA Market Testing Team</u></a> (EAMT) tested marketing and messaging themes on Facebook in their <a href=\"https://www.givingwhatwecan.org/giving-guide\"><u>Effective Giving Guide</u></a> Facebook Lead campaigns which ran from late November 2021 - January 2022. <a href=\"https://www.givingwhatwecan.org/giving-guide\"><u>GWWC's Giving Guide</u></a> answers key questions about effective giving and includes the latest effective giving recommendations to teach donors how to do the most good with their donations. These were exploratory trials to identify promising strategies to recruit people for GWWC and engage people with EA more broadly.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefexbaxjny32\"><sup><a href=\"#fnexbaxjny32\">[1]</a></sup></span>&nbsp;We report the most interesting patterns from these trials to provide insight into which hypotheses might be worth exploring more rigorously in future (\u2018confirmatory analysis\u2019) work.&nbsp;&nbsp;</p><p>Across four trials we compared the effectiveness of different types of (1) messages,&nbsp; (2) videos, and (3) targeted audiences. The key outcomes were (i) email addresses per dollar (when a Facebook user provides an email lead) and (ii) link clicks per dollar. Based on our analysis of 682,577 unique&nbsp; Facebook \u2018impressions\u2019, we found:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1fs8mohlmpj\"><sup><a href=\"#fn1fs8mohlmpj\">[2]</a></sup></span></p><ul><li>The cost of an email address was as low as $8.00 across campaigns, but it seemed to vary substantially across audiences, videos, and messages.</li><li>The message \"Only 3% of donors give based on charity effectiveness, yet the best charities can be 100x more impactful\" generated more link clicks and email addresses per dollar than other messages.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefiz813td49wf\"><sup><a href=\"#fniz813td49wf\">[3]</a></sup></span>&nbsp;In contrast, the message \"Giving What We Can has helped 6,000+ people make a bigger impact on the causes they care about most\" was less cost-effective than the other messages.</li><li>A \u2018short video with facts about effective giving\u2019 generated more email addresses per dollar than either (1) a long video with facts about effective giving or (2) a long video that explained how GWWC can help maximize charitable impact, the GWWC 'brand video.'</li><li>On a per-dollar basis \u2018Animal\u2019 audiences that were given animal-related cause videos performed among the best, both overall and in the most comparable trials. \u2018Lookalike\u2019 audiences (those with a similar profile as current people engaging with GWWC)&nbsp; performed best overall, for both cause and non-cause videos.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefx2f3oww8xaq\"><sup><a href=\"#fnx2f3oww8xaq\">[4]</a></sup></span>&nbsp;However, \u2018Climate\u2019 and \u2018Global Poverty\u2019 audiences basically underperformed the \u2018Philanthropy\u2019 audience when presented videos \u2018for their own causes.\u2019 The Animal-related cause video performed particularly poorly on the \u2018Philanthropy\u2019 audience.</li><li>Demographics were mostly not predictive of email addresses per dollar nor link clicks per dollar</li><li>See our Quarto dynamic document<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefghujzsvayrm\"><sup><a href=\"#fnghujzsvayrm\">[5]</a></sup></span>&nbsp;linked <a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html#outcomes\"><u>here</u></a> for more details, and ongoing analyses.</li></ul><h2>Purpose and Interpretation of this Report</h2><p>One of the primary goals of the EAMT is to identify the most effective, scalable strategies for marketing EA. Our main approach is to test marketing and messaging themes in naturally-occurring settings (such as advertising campaigns on Facebook, YouTube, etc.), targeting large audiences, to determine which specific strategies work best in the most relevant contexts. In this report, we share key patterns and insights about the effectiveness of different marketing and messaging approaches used in GWWC's Effective Giving Guide Facebook Lead campaigns. The patterns we share here serve as a starting point to consider themes and hypotheses to test more rigorously in our ongoing research project.&nbsp;</p><p>We are hoping for feedback and suggestions from the EA community on these trials and their implementation and analysis. We continue to conduct detailed analyses of this data.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefi00i2g12m68\"><sup><a href=\"#fni00i2g12m68\">[6]</a></sup></span>&nbsp;We'd like to get ideas from the community about how to improve trials like these and what other analyses would be informative. Additionally, we hope this report will give other groups ideas for messaging themes to try and tools and processes for starting and analyzing marketing campaigns.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflpgzpb1c6kp\"><sup><a href=\"#fnlpgzpb1c6kp\">[7]</a></sup></span>&nbsp;Finally, we seek to keep the community abreast of what we (at EA Market Testing and at GWWC) are up to (see the <a href=\"https://effective-giving-marketing.gitbook.io/untitled\"><u>EA Market Testing Gitbook page</u></a> for more details and resources).&nbsp;</p><h2>Research questions&nbsp;</h2><p>In these trials, we aimed to test two different approaches to messaging\u2013 (1) presenting facts about effective giving and (2) presenting cause-focused messages\u2013in order to get people to provide their email to download GWWC's Giving Guide, which also subscribed them to GWWC\u2019s email list. We tested (i) six short animated videos with facts about effective giving or a focus on specific cause areas and (ii)&nbsp; seven messages displayed with the videos on (iii) different segments: a general Facebook audience, audiences based on specific interests (animal rights, climate change, poverty, philanthropy), and a lookalike audience (people with a similar profile as current people engaging with GWWC).&nbsp;</p><h2>Content&nbsp;</h2><p>There were two dimensions of treatment content: (1) the text displayed above the videos and (2) the video ad's theme and content.&nbsp;</p><h3>Text</h3><ol><li><strong>Bigger difference next year: </strong>Want to make a bigger difference next year? Start with our Effective Giving Guide and learn how to make a remarkable impact just by carefully choosing the charities you give to.</li><li><strong>100x impact: </strong>Did you know that the best charities can have a 100x greater impact? Download our free Effective Giving Guide for the best tips on doing the most good this holiday season.</li><li><strong>6000 people: </strong>Giving What We Can has helped 6,000+ people make a bigger impact on the causes they care about most. Download our free guide and learn how you can do the same.</li><li><strong>Cause list: </strong>Whether we\u2019re moved by animal welfare, the climate crisis, or worldwide humanitarian efforts, our community is united by one thing: making the biggest impact we can. Make a bigger difference in the world through charitable giving. Start by downloading our Effective Giving Guide. You\u2019ll learn how to approach charity research and smart giving. And be sure to share it with others who care about making a greater impact on the causes closest to their hearts.</li><li><strong>Learn: </strong>Use our free guide to learn how to make a bigger impact on the causes you care about most.</li><li><strong>Only 3% research: </strong>Only 3% of donors give based on charity effectiveness yet the best charities can be 100x more impactful. That\u2019s incredible! Check out the Effective Giving Guide 2021. It'll help you find the most impactful charities across a range of causes.</li><li><strong>Overwhelming: </strong>It can be overwhelming with so many problems in the world. Fortunately, we can do *a lot* to help, if we give effectively. Check out the Effective Giving Guide 2021. It'll help you find the most impactful charities across a range of causes.</li></ol><h3>Video ads</h3><p><strong>Facts about effective giving&nbsp;</strong></p><ol><li><a href=\"https://drive.google.com/file/d/1EPgrkl7MpgcNUWBRUE9YPMG38gYClu2R/view?usp=sharing\"><u>Charity research facts short video</u></a> (8 seconds): Only 3% of donors research charity effectiveness, yet the best charities can 100x your impact, learn how to give effectively</li><li><a href=\"https://drive.google.com/file/d/1c58siggrMh2KN33bcj5f3afKgqY3RE5B/view?usp=sharing\"><u>Charity research facts long video</u></a> (22 seconds): Trivial things we search (<i>shows someone searching</i> how to do Gangnam style), things we should research (<i>shows someone searching</i> how to donate effectively), only 3% of donors research charity effectiveness, yet the best charities can 100x your impact, learn how to give effectively.</li></ol><p><strong>Cause-focus</strong></p><ol><li><a href=\"https://drive.google.com/file/d/1FtI8gA7V0Jj16Ha91jp6XeF2skNXOu7w/view?usp=sharing\"><u>Climate change (cause focus video)</u></a> (15 seconds): Care about climate change? You don\u2019t have to renounce all your possessions, But you could give to effective environmental charities, Learn how to maximize your charitable impact, Download the Effective Giving Guide</li><li><a href=\"https://drive.google.com/file/d/1qct5OPwrD3wJxZn7GEb9URTFMlkVN8ge/view?usp=sharing\"><u>Animal welfare (cause focus video)</u></a> (16 seconds): Care about animals? You don\u2019t have to adopt 100 cats, But you could give to effective animal charities, Learn how to maximize your charitable impact, Download the Effective Giving Guide</li><li><a href=\"https://drive.google.com/file/d/1POx8U1Y8Ce8xpiaJU2f0s2X3a45t89zI/view?usp=sharing\"><u>Poverty (cause focus video)</u></a> (16 seconds): Want to help reduce global poverty? You don\u2019t have to build a village, But you could give to effective global development charities, Learn how to maximize your charitable impact, Download the Effective Giving Guide</li></ol><p><strong>Arguments, rich content from brand video</strong></p><ol><li><a href=\"https://www.youtube.com/watch?v=CiFoHm7HD94\"><u>Brand video</u></a> (1 min 22 seconds): Animated and voiceover video that explains how GWWC can help maximize charitable impact (support, community, and information) and the problems GWWC addresses (good intentions don\u2019t always produce the desired outcomes, there are millions of charities that have varying degrees of impact and some can even cause harm). Call to action: Check out givingwhatwecan.org to learn how you can become an effective giver.</li></ol><h2>Outcome measures</h2><p>Outcome measures used in our analysis of the messages and videos were (1) email addresses per dollar and (2) link clicks per dollar. When we say 'results' below, we refer to these outcome measures. Other measures collected were amount spent, cost per impression, cost per link click, link click-through rate, and 3-second video plays.&nbsp;</p><p>The cost was determined in the Facebook ad auction for each impression on a per-result basis. However, the \u2018cost-per-result' is determined in part by factoring in the likelihood of getting the result. Results cost less if you are expected to get more results, so ultimately, marketers are paying for the value of impressions.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefr6so78zkmeo\"><sup><a href=\"#fnr6so78zkmeo\">[8]</a></sup></span>&nbsp;Generally, when we talk about a segment being 'expensive' we mean that it\u2019s expensive to serve them an impression - not that it\u2019s necessarily expensive on a cost-per-result basis.&nbsp;</p><h2>Analysis: data collection and caveats&nbsp;</h2><p>Facebook allows you to generate pivot tables showing the number of results for different segments. We used these pivot tables to generate datasets with a row for each impression and downloaded these into our repo for further analysis.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3od87lgpnh9\"><sup><a href=\"#fn3od87lgpnh9\">[9]</a></sup></span>&nbsp;We chose to treat each unique user impression (aka 'reach')&nbsp; as an independent observation and each result or link click as if it came from a different 'reach.' This enabled us to do more sophisticated statistical analyses than those available on the Facebook ads platform. Still, our approach has its limitations. Facebook only gives the total number of results of all people in a segment who saw an ad, so we do not know if the same person contributes more than one result to the total. (However, in this context it seems unlikely that many people would give their email to sign up for giving guides more than once.)&nbsp;&nbsp;</p><p><a href=\"https://effective-giving-marketing.gitbook.io/untitled/methodological-discussion/splits-randomization/facebook-split-testing-etc#facebook-trials-divergent-delivery-greater-than-limited-inference\">Divergent delivery</a>: Facebook does not facilitate randomized (or \u2018balanced\u2019) assignment. Unlike in standard experiments and RCTs, Facebook ads do not ensure that each participant has the same chance of ending up in each condition, nor does it allow an easy way to re-weight for imbalance. Specifically, Facebook\u2019s algorithm will always try to assign a particular content variation to people it thinks will be most receptive to that specific variant. All our results may come from\u2026</p><ul><li>\u2018how easy it is for&nbsp; Facebook's algorithm to find the most receptive people within a given audience to target a particular message\u2019 <i>rather than</i></li><li>how effective the message or receptive the audience was <i>generally</i>.</li></ul><p><strong>Thus, given these idiosyncrasies, our results may not generalize much beyond Facebook, nor even to an evolving Facebook environment.</strong> (We discuss this more fully <a href=\"https://effective-giving-marketing.gitbook.io/untitled/methodological-discussion/splits-randomization/facebook-split-testing-etc\"><u>in our Gitbook here</u></a>, where we maintain and update a knowledge base on these design and implementation issues.)&nbsp;</p><p>All analyses were conducted on an exploratory, non-preregistered basis.&nbsp; We had no prior hypotheses before conducting the research. The results we are highlighting were not findings predicted in advance. We made many comparisons between different segments, most of which did not show any substantial interesting difference. Although we did conduct statistical tests, we don\u2019t report them in detail here we do this for brevity and because of the limited potential for generalizable causal inference, given the caveats above. We will report a richer statistical analysis and summary, along with a complete pipeline of code, data, and results, in the transparent <a href=\"https://quarto.org/\"><u>\u2018Quarto</u></a>\u2019 dynamic document <a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html\"><u>HERE</u></a>, where we are working on a Bayesian 'decision relevant\u2019 approach. (See, e.g., plots <a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html#results-by-audience\"><u>here</u></a> and <a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html#outcomes-by-cost\"><u>here</u></a>.) More information about treatment assignment by campaign is explained in the Appendix to this post.</p><h2>Full results&nbsp;</h2><p><i>Note:<strong> </strong>We include charts that depict \u2018results\u2019 (link clicks or emails) as well as \u2018results per dollar.\u2019&nbsp;</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflk1j8c0200n\"><sup><a href=\"#fnlk1j8c0200n\">[10]</a></sup></span><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</i></p><h3>Demographics</h3><ul><li>Women and older people were more responsive to the ads (with more results per impression) but also more expensive to target \u2013 which means the results approximately balanced out on a \u2018cost per email\u2019 or \u2018cost per link click\u2019 basis. There is some evidence of interaction effects; E.g., the 45-54 age group appears to contain particularly uninterested men, even on a cost-per-result basis.</li><li>People aged 65+ <i>click</i> more, both per impression and per ad dollar spent</li><li>Caveats: The graphs below results pool across earlier and later campaigns<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6a6e8gcp06b\"><sup><a href=\"#fn6a6e8gcp06b\">[11]</a></sup></span>; the latter only included ages 18-44. However, the above results basically continue to hold (<a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html#other-outcome-by-group-comparisons-across-several-trials\"><u>see Quarto here</u></a>) when we separate these groups of campaigns; with the additional result that the 18-24 age group was particularly unpromising in the earlier trials.</li></ul><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995892/mirroredImages/nhbeKbwMgFKfrzLNb/k3pujritpgbbc8oyum15.png\"></figure><h3>Audiences</h3><p>We defined the following audiences. Within each of these groups, Facebook chose which ad content to allocate according to its maximizing algorithm.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa7s4hbkj6de\"><sup><a href=\"#fna7s4hbkj6de\">[12]</a></sup></span></p><ul><li>Lookalike: An audience that we set by telling Facebook to identify people whose characteristics <i>resembled</i> GWWC\u2019s \u2018core audience\u2019, i.e., who resembled pledgers, \u2018try givers\u2019, or people who liked GWWC\u2019s page.</li><li>Animal: People interested in animal rights (according to Facebook interests<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefi5bt7edad8\"><sup><a href=\"#fni5bt7edad8\">[13]</a></sup></span>)</li><li>Climate Change: People interested in climate change (according to Facebook interests)</li><li>Poverty: People interested in global poverty (according to Facebook interests)</li><li>Philanthropy: People interested in philanthropy (according to Facebook interests)</li><li>General: All Facebook users</li></ul><p>Results</p><ul><li>Predictably, the \u2018Lookalike\u2019 audience was the most responsive.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefb7zbk8y1u4q\"><sup><a href=\"#fnb7zbk8y1u4q\">[14]</a></sup></span></li><li>On a per-dollar basis, \u2018Animal\u2019 audiences given animal-related cause videos performed among the best, both overall and in the most comparable trials.</li><li>The \u2018Climate\u2019 audience was highly engaged (in terms of link clicks), but this didn't translate into more <i>results per impression (or per cost) </i>than for the \u2018Poverty\u2019 or \u2018Philanthropy\u2019 audiences.</li></ul><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995892/mirroredImages/nhbeKbwMgFKfrzLNb/xugovjem0cppdkufazoy.png\"></figure><p>When \u2018Climate\u2019 and \u2018Global Poverty\u2019 audiences were presented videos \u2018for their own causes\u2019, they performed worse than the \u2018Philanthropy\u2019 audience when presented these same videos.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnwymx0hb5um\"><sup><a href=\"#fnnwymx0hb5um\">[15]</a></sup></span></p><p>Aggregated across all trials, when presented with the Climate videos, \u2018Climate\u2019 audiences underperformed the \u2018Philanthropy\u2019 audience. In the most comparable trial (trial 4), they similarly underperformed both the \u2018Philanthropy\u2019 and \u2018General\u2019 audiences for this video. Aggregated across trials, the \u2018Global Poverty\u2019 audience appears to have done OK on with the Global Poverty related video. However, in the most comparable (4th) trial they did so poorly on this video that Facebook stopped administering it to them!&nbsp;</p><h3>Texts (messages)&nbsp;</h3><ul><li>\u201cOnly 3% of donors give based on charity effectiveness, yet the best charities can be 100x more impactful\u201d performed better than the other messages. However, there was some heterogeneity by audience, with this message doing poorly on \u2018Poverty\u2019, \u2018Lookalikes\u2019, and \u2018Climate\u2019 audiences in a comparable trial; see <a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html#focus-test-4-texts-and-audiences\"><u>tables in Quarto here</u></a>.</li><li>\u201cGiving What We Can has helped 6,000+ people make a bigger impact on the causes they care about most\u201d was the least effective message.</li></ul><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995892/mirroredImages/nhbeKbwMgFKfrzLNb/raxckpiwbgtdr62atuvb.png\"></figure><p><a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html#focus-test-4-texts-and-audiences\"><u>As reported in the Quarto,</u></a> some messages show strong heterogeneity across audiences, while others were fairly consistent. The second best overall message, \u2018100x impact\u2019 did adequately on all audiences. \u2018Learn\u2019 showed some variation, doing pretty well on \u2018Poverty\u2019 and \u2018Lookalike\u2019 audiences, OK on \u2018Animal\u2019 audiences, but poorly on the \u2018General\u2019, \u2018Philanthropy\u2019, and \u2018Climate\u2019 audiences. The '6000+ people' and 'Bigger Difference' messages performed poorly on nearly all audiences, doing at best OK on a few audiences.</p><p>In the Quarto, we also graph HDI intervals (somewhat like confidence intervals) for the \u2018results per unique impression\u2019 by message. These intervals appear extremely narrow; the HDI\u2019s do not overlap.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftqzld0a5xa7\"><sup><a href=\"#fntqzld0a5xa7\">[16]</a></sup></span>&nbsp;As each message is delivered at very nearly the same cost per impression (1.5-1.6 cents), the differences in&nbsp; \u2018results per cost\u2019 closely reflect the differences in \u2018results per 1k impressions\u2019.</p><h3>Videos</h3><ol><li>The \u2018factual long\u2019 video and the 'brand video' video were the least effective. All other videos were about the same.</li><li>The \u2018factual long\u2019 video was served to expensive people. It is possible that only more expensive people had the attention for it or it was better received by older people (who are more expensive).</li><li>The animal video was also sent to relatively expensive people \u2013 it was more targeted at women.</li><li>On a per-dollar basis \u2018Animal\u2019 audiences that were given animal-related cause videos performed among the best, both overall and in the most comparable trials. However, \u2018Climate\u2019 and \u2018Global Poverty\u2019 audiences underperformed the \u2018Philanthropy\u2019 audience when presented videos \u2018for their own causes.\u2019&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7zk671y2jbb\"><sup><a href=\"#fn7zk671y2jbb\">[17]</a></sup></span></li></ol><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995892/mirroredImages/nhbeKbwMgFKfrzLNb/m4kjcmp6ptyp4wxzauky.png\"></figure><p>Conclusions from video and age breakdown</p><ul><li>The \u2018factual short\u2019 seems to be the best video for older people</li><li>The poverty video seems to be less interesting to older people</li><li>The climate video produces more engagement from older people (in terms of clicks) but doesn\u2019t translate to more emails (a common theme with the climate video)</li></ul><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995892/mirroredImages/nhbeKbwMgFKfrzLNb/mjfe6aptdrkpqtyr2thi.png\"></figure><p>Conclusions about Video Results by Audience&nbsp;</p><ul><li>The brand video seemed to be served to the cheaper people in the \u2018Philanthropy\u2019 audience but otherwise seemed to do as poorly as the \u2018factual long\u2019 video.</li><li>For the \u2018Philanthropy\u2019 audience (in the most comparable trials: see <a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html#focus-test-4-videos-and-audiences\"><u>Quarto here</u></a>) the Animal video performed extremely poorly, and the Climate video performed rather well, especially on a per-cost basis.</li><li>Otherwise, for each audience, the brand and factual long videos both tended to do poorly.</li><li>\u2018Climate\u2019 and \u2018Global Poverty\u2019 audiences basically underperformed the \u2018Philanthropy\u2019 audience when presented videos \u2018for their own causes.\u2019</li></ul><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995892/mirroredImages/nhbeKbwMgFKfrzLNb/b4vyhfpsd5ppczxzx6ni.png\"></figure><h2>Next steps&nbsp;</h2><p>We are looking for collaborators with particular interests and expertise to help us design and analyze future campaigns as well as contribute to the ongoing analyses of trials we have completed (including the one discussed in this report). We are particularly interested in: adaptive trial designs to maximize \u2018value of information\u2019, Bayesian and robust simulation-based statistical analysis, meta-analysis and mixed models, social media advertising (e.g., Facebook \u2018pixels\u2019) and web analytics, and open data pipelines and visualizations (especially in R and Quarto). If you are interested in working with the EA Market Testing Team on this project or on similar research projects, please reach out to David Reinstein at dreinstein@rethinkpriorities.org.&nbsp;</p><p>In the future, we intend to test questions generated from these trials. For example:&nbsp;</p><ul><li>Is the success of the \u201conly 3% of donors\" message being driven by the desire to be part of a small group of like-minded people?</li><li>Does the effectiveness of short factual messaging generalize to other campaigns?</li><li>Are audiences interested in animal welfare particularly promising?</li><li>Do our results (for clicks and email signups) carry over to more impactful outcomes?</li></ul><h2>Appendix: Treatment assignment/campaigns</h2><p>Video content was manipulated across three \u2018split tests\u2019. (For a breakdown of treatment assignments by campaign, date, etc., see the <a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html#implemented-treatments-reach\"><u>tables in the dynamic document here.</u></a>)&nbsp;</p><ul><li>Test 1 (Nov 30, 2021 \u2013 Dec 8, 2021) displayed either the long factual video or a cause focus video. In the cause focus condition, cause-specific audiences for animal rights, climate change, and poverty (based on their behavior on Facebook) were shown the relevant cause video.</li><li>Test 2 (Dec 8 - 20, 2021) was the same as Test 1 but used the short factual video instead of the cause-focus videos.</li><li>Test 3 (Dec 23, 2021 - Jan 4, 2022) was the same as Test 2 but it had a new version of the videos, and it was restricted to ages 18-44.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefexqsf6zsyh9\"><sup><a href=\"#fnexqsf6zsyh9\">[18]</a></sup></span></li><li>Test 4 (Jan 7 - Jan 18, 2022: The brand video was displayed in a separate \u2018brand video\u2019 campaign which was tested against another campaign that allowed the algorithm to optimize between the short factual and cause-focus videos (although not allowing each cause-specific audience to see the ads for other cause areas).</li></ul><p>In all tests, the treatment assignment (which text and video were displayed to which of the users within the chosen audience) was determined by Facebook's algorithm. In the split test, the amount that each experimental condition was displayed<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbn2vvyledu\"><sup><a href=\"#fnbn2vvyledu\">[19]</a></sup></span>&nbsp;was set to equalize the total cost across conditions. The other features (e.g., \u2018what cause video to show\u2019 or \u2018how to introduce each video\u2019) were determined by Facebook's algorithm to optimize the rate of people leaving their emails (thus these were not balanced). None of the treatments were fully randomly assigned.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref14nf3e1emc5\"><sup><a href=\"#fn14nf3e1emc5\">[20]</a></sup></span></p><p>The videos were adapted across the trials as we learned. First, we updated the factual video to be shorter for Trial 2, and then we tried videos of Luke holding up signs spelling out the voiceover in Trial 3 for all videos. In many of our analyses, we pooled data across trials, yielding greater statistical power.<br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnexbaxjny32\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefexbaxjny32\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Learn more about GWWC's efforts to identify strategies to engage people with EA by watching Grace's talk at EAGxOxford 2022,&nbsp;<a href=\"https://www.youtube.com/watch?v=H9fIC0I6Rvk\">\"What we're learning about spreading EA ideas.\"</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1fs8mohlmpj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1fs8mohlmpj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>These results are largely based on simple comparisons: we continue the careful statistical analysis of this trial in the Quarto dynamic document linked <a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html#outcomes\"><u>here</u></a>, along with the analysis of other EAMT-linked trials.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fniz813td49wf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefiz813td49wf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Although there was some heterogeneity by audience, with this message doing poorly on \u2018Poverty\u2019, \u2018Lookalikes\u2019, and \u2018Climate\u2019 audiences in a comparable trial.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnx2f3oww8xaq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefx2f3oww8xaq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>However, our confidence/credible intervals (reported in Quatro) are wider for this smaller group.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnghujzsvayrm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefghujzsvayrm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u2018Dynamic documents\u2019 combine code, text, and results, making it clear exactly how the results are produced.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fni00i2g12m68\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefi00i2g12m68\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We continue and extend the presentation and statistical analysis of this trial in the <a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html\"><u>Quarto dynamic document linked here, </u></a>along with other chapters analyzing our other EAMT-linked trials. (This is publicly hosted on the Github repo <a href=\"https://github.com/daaronr/eamt_data_analysis/blob/main/chapters/gwwc_gg.qmd\"><u>here</u>,</a> which contains all code and data for this project). We are eager to have you engage with that resource. You can leave comments on the Quarto with the embedded \u2018Hypothesis\u2019 tool. You can also engage with the Github, and reach out to us if you want to get further involved in this and other analysis and reporting.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlpgzpb1c6kp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflpgzpb1c6kp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Also see the resources on \u2018implementing ads, messages, and designs\u2019 that we are building in the public Gitbook <a href=\"https://app.gitbook.com/o/-MfFk4CTSGwVOPkwnRgx/s/a3YtWoUiYYfiEQrBNztC/marketing-and-testing-opportunities-tools-tips/implementation-and-collecting-data-issues\"><u>here</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnr6so78zkmeo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefr6so78zkmeo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>You can read more about how Facebook's ad auction works <a href=\"https://www.webfx.com/social-media/pricing/how-much-does-facebook-advertising-cost/#:~:text=Facebook%20advertising%20costs%2C%20on%20average,%245.47%20per%20download%2C%20on%20average\"><u>here</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3od87lgpnh9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3od87lgpnh9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See our walk-through on how to extract such data from Facebook <a href=\"https://effective-giving-marketing.gitbook.io/untitled/marketing-and-testing-opportunities-tools-tips/collecting-data-trial-outcomes/facebook-meta-ads-interface\"><u>here</u></a>. (You can also import Facebook ad data into R directly via an API and helper tools; we intend to do this in our later work.)&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlk1j8c0200n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflk1j8c0200n\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We recognize (as noted above) these charts do not contain confidence/credible intervals, statistical tests, or metrics like \u2018probability of superiority\u2019. This presents particular challenges in this context; we have some preliminary analysis of this (as well as forest and ridge plots plots) in the Quarto, e.g.,&nbsp; <a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html#results-by-audience\"><u>here</u></a> and <a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html#outcomes-by-cost\"><u>here</u></a>.)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6a6e8gcp06b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6a6e8gcp06b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>More information about treatment assignment by campaign is explained in the <a href=\"https://docs.google.com/document/d/1vUL4xCvWMO8Inw2MmhOZ9TJ-an4IZXV1lD9dLuBlRnk/edit#heading=h.1rfwg9ensqt9\"><u>Appendix</u></a> to this post.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna7s4hbkj6de\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa7s4hbkj6de\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We also set a \u2018Retargeting\u2019 audience, consisting of people who had already been on GWWC\u2019s website. This was very small and obviously more promising than other audiences, so we do not include this audience in the analysis.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fni5bt7edad8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefi5bt7edad8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Facebook interests are determined by pages the user \u2018likes\u2019 and the content they engage with on Facebook as well as off the platform. Facebook's definition of \u2018being interested\u2019 is very broad and it might be different from what one would perceive as&nbsp; \u2018being interested in something\u2019 in the real world.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnb7zbk8y1u4q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefb7zbk8y1u4q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u2018Lookalike\u2019 audiences performed best overall, for both cause and non-cause videos. However, our confidence/credible intervals are wider for this smaller group. <a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html\"><u>See Quarto for more details, e.g., the table \u2018Results by audience; cause vs non-cause (and overall)\u2019</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnwymx0hb5um\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnwymx0hb5um\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See the last table in the Quarto (go <a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html#outcomes-by-cost\"><u>here</u></a> and scroll to bottom); note that the \u2018Climate\u2019 audience did worse than the \u2018Philanthropy\u2019 audience when both saw climate videos, while the global poverty and philanthropy audience performed similarly. The table <a href=\"https://daaronr.github.io/eamt_data_analysis/chapters/gwwc_gg.html#focus-test-4-videos-and-audiences\"><u>here</u></a> focuses on the most comparable trials. If you filter by \u201cvideo_theme = poverty\u201d or \u201c=climate\u201d you see the results discussed below, suggesting each of these cause audiences underperformed the \u2018Philanthropy\u2019 audiences, even when presented their \u2018own cause\u2019s videos.\u2019</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntqzld0a5xa7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftqzld0a5xa7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Are these \u2018real differences in the population?\u2019 This is subject to other caveats about divergent delivery etc.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7zk671y2jbb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7zk671y2jbb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As discussed in our 'Audiences' section \u2026 aggregated across all trials, when presented the Climate videos, \u2018Climate\u2019 audiences underperformed the \u2018Philanthropy\u2019 audience. In the most comparable trial (trial 4), they similarly underperformed both the \u2018Philanthropy\u2019 and \u2018General\u2019 audiences for this video. Aggregated across trials, the \u2018Global Poverty\u2019 audience appears to have done OK on this video. However, in the most comparable (4th) trial they did so poorly on this video that Facebook stopped administering it to them!</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnexqsf6zsyh9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefexqsf6zsyh9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The original videos in Tests 1 and 2 and the new videos in Test 3 used the same script; The videos in Tests 1 and 2 were animations. In Test 3, the videos had a voiceover instead and Luke held up signs with the words of the script.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbn2vvyledu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbn2vvyledu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I.e., factual vs. cause focus in test 1-3 and brand video vs. factual and cause based in test 4</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn14nf3e1emc5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref14nf3e1emc5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See our comments about random assignment in the <a href=\"https://docs.google.com/document/d/1vUL4xCvWMO8Inw2MmhOZ9TJ-an4IZXV1lD9dLuBlRnk/edit#heading=h.pm26sgfu2z85\"><u>data collection and caveats</u></a> section.</p></div></li></ol>", "user": {"username": "Erin Morrissey"}}, {"_id": "dfcvc9HSNLmq5cKtf", "title": "Who are some less-known people like Petrov?", "postedAt": "2022-09-06T13:22:11.040Z", "htmlBody": "<p><strong>Do you know of people who were under pressure to make a very net-negative decision and did not do so?</strong> If yes, please let me know! <a href=\"https://forum.effectivealtruism.org/topics/petrov-day\"><u>Petrov Day</u></a> is around the corner, and I\u2019d like to generate some examples of people who, like Petrov and <a href=\"https://forum.effectivealtruism.org/topics/vasili-arkhipov\"><u>Vasili Arkhipov</u></a>, made decisions that should be celebrated.</p><p>From the <a href=\"https://forum.effectivealtruism.org/topics/stanislav-petrov\"><u>Forum Wiki entry about Stanislav Petrov</u></a>:&nbsp;</p><blockquote><p>On 26 September 1983, Petrov defied Soviet military protocol and classified reports by an early-warning system of an incoming missile strike from the United States as a false alarm. Because of this decision, which likely avoided a large-scale <a href=\"https://forum.effectivealtruism.org/topics/nuclear-warfare-1\"><u>nuclear war</u></a> between the Soviet Union and the United States, Petrov is often referred to as \"the man who saved the world.\" His decision to report the incident as a false alarm has been described as \"the most important decision of all time.\"</p></blockquote><p>You can read more about the incident in the <a href=\"https://www.vox.com/2018/9/26/17905796/nuclear-war-1983-stanislav-petrov-soviet-union\"><i><u>Vox</u></i><u> article on the subject</u></a>.</p><figure class=\"image image_resized\" style=\"width:40.3%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995622/mirroredImages/dfcvc9HSNLmq5cKtf/pb97o1f5hpxkeardzsml.png\"></figure><p><strong>I\u2019m looking forward to learning about more people and incidents like this.&nbsp;</strong></p>", "user": {"username": "Lizka"}}, {"_id": "ymEqipmiM3SLyQvaC", "title": "Value of Life: VSL Estimates vs Community Perspective Evaluations", "postedAt": "2022-09-06T14:35:51.391Z", "htmlBody": "<h1>Summary</h1><p>The value of life, which we need to know if we are to correctly prioritize between interventions, is poorly captured by value of statistical life (VSL) estimates, and instead is better captured by community perspective evaluations.</p><p>&nbsp;</p><h1>Value of Statistical Life Estimates</h1><p>VSL estimates may involve stated preferences of survey respondents (i.e. from asking them their willingness to pay to avert some risk) or revealed preference (e.g. looking at how much people are willing to pay for airbags in cars, or at how much they need to be compensated to work in a riskier job). Regardless, the general idea behind the VSL approach is that we have the following implicit equation:</p><p><i>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Compensation needed = Probability of death * Badness of death</i></p><p>Hence, if we elicit the compensation needed, we can then just divide by the probability of death to get the badness of death. There are <a href=\"https://www.hup.harvard.edu/catalog.php?isbn=9780674931909\"><u>numerous flaws</u></a> in this approach, however:</p><p>&nbsp;</p><p><strong><u>Problems leading to the misestimation of the compensation needed</u></strong></p><ul><li><u>Humans are not hyperrational</u>, and it's not as if survey respondents, or people taking up risky jobs, will bother to literally think about how much they subjectively value their own life in monetary terms, and then use the additional probability of death to calculate the amount of compensation needed to get them to undertake a risky course of action. Whatever number they throw out in response to surveys, or whatever risk premium they implicitly accept in taking up jobs, therefore, will just not be a good guide to the actual compensation needed, and cannot be used to estimate the badness of death.</li><li><u>People exhibit </u><a href=\"https://www.aeaweb.org/conference/2011/retrieve.php?pdfid=269\"><u>insensitivity</u></a><u> with respect to changes in small risks</u>, reporting same or similar compensation amounts for risks that differ even in magnitudes. Hence, even if people were calculating the sums needed to compensate them for a risk, they may not be doing so accurately.</li><li><u>Estimates may be made under duress</u> (e.g. workers may fear quitting a risky job due to a lack of alternatives). The upshot of this is that the true equation people are implicitly solving is \"Value from avoiding unemployment &gt;= Probability of death * Badness of death\", in which case the actual risk premium does not necessarily equal the compensation needed, since even if the former were below the latter people would still choose the risk so long as the value of avoiding unemployment were sufficiently high. This potentially leads to the systematic understatement of the compensation needed and hence the badness of death, if the value from avoiding unemployment &gt; compensation needed &gt; actual risk premium.</li><li><u>People may substitute in the judgement of others</u> when estimating the compensation needed, rather than using their own, true subjective monetary value of life (e.g. workers leaving it up to their boss to decide what risks are reasonable, even if the bosses do not value their lives as much as they themselves would).</li><li><u>Estimates may be confounded by moral considerations</u> (e.g. workers like firefighters may take up jobs not because they are appropriately compensated for it from a purely self-interested perspective, but because it benefits others sufficiently that they see it as worth doing, the risk to themselves notwithstanding). And so, parallel to the duress case, the true equation people are implicitly solving is \"Social benefits &gt;= Probability of death * Badness of death\", and there is systematic understatement of the compensation needed and hence the badness of death, if the social benefits &gt; compensation needed &gt; actual risk premium.</li><li><u>Estimates may be confounded by social considerations</u> (e.g. workers valuing the social conditions in which the risk occurs, with them finding risks more acceptable when they trust their managers or have more say over the risk-taking). Therefore, the true equation people are implicitly solving is \"Financial compensation + Social compensation &gt;= Probability of death * Badness of death\", in which case the risk premium (i.e. the financial compensation) systematically understates the total compensation amount needed, and cannot be used to calculate value of life in conditions where the social elements (e.g. having control over the risk) are eliminated.</li><li><u>People have different preferences</u> \u2013 some care a lot about not dying, and others less so. When measuring the value of life through compensating differentials between dangerous and less dangerous jobs, what we end up <a href=\"https://www.amazon.com/Public-Finance-Policy-Jonathan-Gruber/dp/1464143331\"><u>measuring</u></a> is the value of life for those more risk loving and less concerned about dying (i.e. those systematically more likely to take up dangerous jobs), and we end up with an underestimate of the average person's value of life.</li></ul><p>&nbsp;</p><p><strong><u>Problems leading to the misestimation of the probability of death</u></strong></p><ul><li><u>Straightforwardly, people may not know the true risks</u><strong> </strong>when buying a product or taking up a job or the like.</li></ul><p>&nbsp;</p><h1>Community Perspective Evaluations</h1><p>An alternative approach to estimating the value of life, which the GiveWell-commissioned <a href=\"https://files.givewell.org/files/DWDA%202009/IDinsight/IDinsight_Beneficiary_Preferences_Final_Report_November_2019.pdf\">IDinsight survey</a> pioneers, is the community perspective evaluation - asking survey respondents to take the perspective of a decision-maker for their community and choose between saving more lives or doubling income. And while there are worries about social desirability bias (i.e. people overstating how much they value life relative to income, so as to not appear cold and amoral), this can potentially be <u>corrected for by applying a discount derived from past studies of social desirability bias in value self-reporting</u>. This allows us to arrive at potentially more accurate <a href=\"https://docs.google.com/spreadsheets/d/1JYLNyrx_QgdMbTDbVRxVQQUvTVhWmQ9C/edit#gid=43860184\">moral weights</a>, and this is the approach CEARCH itself takes. (Note: CEARCH is the cause prioritization research organization I work at; we came out from the recent Charity Entrepreneurship incubation round).</p><p>That said, there are also concerns over the SDB-corrected community perspective approach:</p><ul><li>The underlying community perspective estimate <u>arbitrarily caps out at 10,000 cash transfers</u> for doubling income for one year relative to saving a life, which means we may be systematically undervaluing life relative to income.</li><li>There are <u>uncertainties over the robustness of the specific error correction value</u> used to offset the social desirability bias in the survey. It would be preferable to conduct additional surveys using the community perspective, but whose research design eliminates social desirability bias (e.g. through <a href=\"https://dimewiki.worldbank.org/List_Experiments\"><u>list experiments</u></a>).</li></ul><p>&nbsp;</p><h1>Assessment</h1><p>Overall, the SDB-corrected community perspective is still preferable to the VSL approach, because on balance, <u>the flaws in both approaches point towards a likely undervaluation of life relative to income</u>. To the extent that the SBD-corrected community perspective yields higher values than the <a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/2019-moral-weights-research\"><u>VSL approach</u></a>, the former probably gets us closer than the latter to the true, average value of life.</p><p>&nbsp;</p><h1>Path Forward</h1><p>I'm a very big fan of the GiveWell/IDinsight's moral weights research in Kenya and Ghana, and think that this is a very promising area of meta-research, which can help us better estimate the value of life and hence better direct scarce resources (e.g. I have always thought that we undervalued life-saving charities like AMF relative to income-raising ones like SCI).</p><p>CEARCH is considering funding more research on this, and would be keen to work with any other organizations interested in this matter. All feedback (positive or negative) on the value of such additional moral weights research is, of course, welcome!</p><p><br>&nbsp;</p>", "user": {"username": "Joel Tan"}}, {"_id": "j4TDQfdhRXaGtxawt", "title": "Designing Effective Crowdfunding Campaigns", "postedAt": "2022-09-06T16:17:50.906Z", "htmlBody": "<p>Hello Everyone,</p><p>I am interested in studying the language used in Crowdfunding project pages (for example, Kickstarter projects).</p><p>I have two basic questions, and I would greatly appreciate your feedback.&nbsp;<br>&nbsp;</p><p><strong>1) While writing a project description, how would you rank-order the importance of these words? (from most important to least important)</strong></p><p><i><strong>a) cognitive or informational words&nbsp;</strong></i></p><p><i><strong>b) emotional or exciting words</strong></i></p><p><i><strong>c) words that evoke a sense of community</strong></i></p><p><i><strong>d) words that make the project sound similar in style to other projects</strong></i></p><p>&nbsp;</p><p><strong>2) Do you believe early vs. late visitors differ in terms of what type of words they value? That is, are people who visit the page early in the campaign influenced by certain types of words compared to people who visit the page later in the campaign?</strong></p><p>&nbsp;</p><p>Thank you in advance for your responses.&nbsp;</p>", "user": {"username": "Creator2022"}}, {"_id": "Q7gqF9ZCah2BEwZ9b", "title": "A California Effect for Artificial Intelligence", "postedAt": "2022-09-09T14:17:57.129Z", "htmlBody": "<p>I just finished writing a 50-page document exploring a few ways that the State of California could regulate AI with the goal of producing a <i>de facto</i> California Effect. You can read the whole thing as a Google doc <a href=\"https://docs.google.com/document/d/1XwrSatEUyfmkcRGBJKkyJLpemkmsBI_SOHDZb8NiN4U/\">here</a>, as a pdf <a href=\"https://drive.google.com/file/d/1-Pk4rp5uBTcma7lFqurfg9UtT2XrSr1I/view?usp=sharing\">here</a>, or as a webpage <a href=\"https://www.henryjos.com/p/a-california-effect-for-artificial.html\">here</a>,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgrzf2y8v2q5\"><sup><a href=\"#fngrzf2y8v2q5\">[1]</a></sup></span>&nbsp;or you can read the summary and a few key takeaways below. I'm also including some thoughts on my theory of impact and on opportunities for future research.</p><p>I built off work by <a href=\"https://academic.oup.com/book/36491?login=true\">Anu Bradford</a>, as well as a <a href=\"https://uploads-ssl.webflow.com/614b70a71b9f71c9c240c7a7/62fbe1c37eff7d304f0803ac_Brussels_Effect_GovAI.pdf\">recent GovAI paper</a> by Charlotte Siegmann and Markus Anderljung. This project was mentored by Cullen O'Keefe. I did this research through an existential risk summer research fellowship at the University of Chicago \u2014 thank you Zack Rudolph and Isabella Duan for organizing it!</p><h2>Abstract</h2><p>The California Effect occurs when companies adhere to California regulations even outside California\u2019s borders because of a combination of California\u2019s large market, its capacity to successfully regulate, its preference for stringent standards, and the difficulty of dividing the regulatory target or moving beyond California\u2019s jurisdiction. In this paper, I look into three ways in which California could regulate artificial intelligence and ask whether each would produce a <i>de facto</i> California Effect. I find it likely (~80%) that regulating training data through data privacy would produce a California Effect. I find it unlikely (~20%) that regulation based on the number of floating-point operations needed to train a model would produce a California Effect. Finally, I find it likely&nbsp; (~80%) that risk-based regulation like that proposed by the European Union would produce a California Effect.</p><p>If this seems interesting, please give the <a href=\"https://docs.google.com/document/d/1XwrSatEUyfmkcRGBJKkyJLpemkmsBI_SOHDZb8NiN4U/edit?usp=sharing\">full paper</a> a look. There's a more-detailed 1.5-page executive summary, and then (of course) the document itself.&nbsp;</p><h2>Key Takeaways</h2><ol><li>The California Effect is a powerful force-multiplier that lets you have federal-level impact for the low(er) price<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvz0pzkwwuwa\"><sup><a href=\"#fnvz0pzkwwuwa\">[2]</a></sup></span>&nbsp;of state-level effort.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdbngpkup27o\"><sup><a href=\"#fndbngpkup27o\">[3]</a></sup></span></li><li>There are ways to regulate AI which I argue would produce a California Effect.</li><li>State government in general and <a href=\"https://forum.effectivealtruism.org/posts/wBre7Rm6tBzKbZ86B/the-california-case-for-state-capacity-as-an-ea-cause-area\">California's government specifically</a> are undervalued by EAs. I believe that EAs interested in politics, structural change, regulation, animal welfare, preventing pandemics, etc, could in some cases have bigger and/or more immediate impacts on a state level than on a federal level.</li><li>There are still plenty of opportunity for further research.</li></ol><h2>Theory of Impact</h2><p>My hope \u2014 and ultimate theory of impact \u2014 is that this paper will help policymakers make better-informed decisions about future AI regulations. I hope to encourage those who believe in regulating artificial intelligence to give more attention to the State of California. At the very least, I hope that people with a broader reach than I have in the AI Governance space will read and even build off this work. I hope I can raise their awareness of the California Effect and ensure that they recognize the disproportionate impact it can have in the race to keep artificial intelligence safe.&nbsp;</p><h2>Opportunities for further research</h2><p>Before I list my own thoughts, I will direct readers to the <a href=\"https://forum.effectivealtruism.org/posts/gJGMFdGqFhs3mKo2s/supplement-to-the-brussels-effect-and-ai-how-eu-ai?commentId=udSNuK5RBkdq87NZX#Suggestions_for_additional_work\"><u>list of further research opportunities</u></a> that Charlotte Siegmann and Markus Anderljung collected in an announcement for their report on the potential Brussels Effect of the EU AI Act. I\u2019m personally choosing to highlight their fourth and sixth bullet points, which I think would be especially effective (the latter even more so):</p><ul><li>\u201cEmpirical work tracking the extent to which there is likely to be a Brussels Effect. Most of the research on regulatory diffusion focuses on cases where diffusion has already happened. It seems interesting to instead look for leading indicators of regulatory diffusion. For example, you could analyze relevant parliamentary records or conduct interviews, to gain insight into the potential global influence of the EU AI Act, the EU, and legal terms and framings of AI regulation first introduced in the EU discussion leading up to the EU AI Act. [...]</li><li>Work on what good AI regulation looks like from a TAI/AGI perspective seems particularly valuable. Questions include: What systems should be regulated? Should general-purpose systems be a target of regulation? Should regulatory burdens scale with the amount of compute used to train a system? What requirements should be imposed on high-risk systems? Are there AI systems that should be given fiduciary duties?\u201d</li></ul><p>Interested readers should also peruse the Centre for Governance of AI\u2019s <a href=\"https://www.governance.ai/research-paper/agenda\"><u>research agenda</u></a>, which is far more exhaustive than I could ever hope to be.</p><p>With other people\u2019s suggestions out of the way, I think there\u2019s a dearth of research into the impact state governments can have, in artificial intelligence governance but especially in other cause areas. State and local governments account for a bit less than half of all government spending in the US, yet they can be far more accessible than the federal government, which accounts for the other half. Especially in the context of AI governance, I would love to see more research into which state-level interventions are possible, anywhere from research funding/grants to tax breaks.</p><p>Interestingly, the California Privacy Rights Act gives the California Privacy Protection Agency the right to \u201cIssu[e] regulations governing access and opt-out rights with respect to businesses\u2019 use of automated decision-making technology, including profiling and requiring businesses\u2019 response to access requests to include meaningful information about the logic involved in such decision-making processes, as well as a description of the likely outcome of the process with respect to the consumer.\u201d However, their <a href=\"https://cppa.ca.gov/regulations/pdf/20220708_text_proposed_regs.pdf\">proposed regulations</a> do not seem to mention AI or automated decision-making. Though the CPPA is no longer accepting comments on their proposed regulations, it could be useful to look into what it would take to get them to include AI to a greater extent.</p><p>In this same vein, it could be useful to look at instances in the past when other states\u2019 regulatory authorities have attempted to regulate online commerce. Were they successful? What would these previous attempts at regulation mean for future attempts to regulate AI? Though I touched upon the California Privacy Protection Agency, it may be the case that such an agency isn\u2019t the right entity to create and enforce these regulations. Which other agencies, e.g. consumer protection, could effectively regulate AI? This could also be worth looking into at the federal level, too.</p><p>It could also be a good idea to require registration for training runs, data collection, or even the entirety of model creation. As such, research into prior attempts to require licenses for the creation and use of new technology (e.g. transportation, research technologies, weapons, etc) could be useful.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngrzf2y8v2q5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgrzf2y8v2q5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Given that the formatting doesn't translate perfectly from the Google doc, the webpage is probably best for those of you who like to download webpages and run them through text-to-speech software.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvz0pzkwwuwa\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvz0pzkwwuwa\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Granted, since California is both the largest state (~40 million citizens) and has the <a href=\"https://www.quantgov.org/state-regdata-definitive-edition\">most regulation-happy legislature</a>, \"state-level effort\" is far from zero. It's still smaller than the federal government, though.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndbngpkup27o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdbngpkup27o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This could be a double-edged sword if the California Effect ends up amplifying poorly-crafted or harmful regulation, though. Bad regulation in general and bad AI regulation specifically have the potential to do far more harm than good, and the potential for regulatory diffusion makes this issue all the more relevant.</p></div></li></ol>", "user": {"username": "henryj"}}, {"_id": "4CBoRsCxi5zMWdaDJ", "title": "Hypercerts: A new primitive for public goods funding", "postedAt": "2022-09-10T21:43:45.843Z", "htmlBody": "<p>Tl;dr: Protocol Labs, the team behind IPFS and Filecoin, are working on impact markets.</p><p>&nbsp;</p><blockquote><p>Impact certificates have been a recurring theme in our conference series <a href=\"https://fundingthecommons.io/\"><strong>Funding the Commons </strong></a>and Gitcoin\u2019s <a href=\"https://schellingpoint.gitcoin.co/\"><strong>Schelling Point conferences</strong></a>. A few people have discussed them previously; you can find some further resources at the end of this post. Here we offer our current take on impact certificates, and on some possibilities of how, \u201chypercerts,\u201d our proposed new primitive for public goods funding more generally, might be utilized.</p><p>Impact certificates are often said to enable retroactive public goods funding. Let\u2019s start with a real-world example of why this kind of retroactive funding is useful.</p></blockquote><p>&nbsp;</p><blockquote><p>The core idea, from the perspective of an entrepreneur building public goods, is this: If you can reasonably expect to get funded retroactively for your work once you produce a positive impact, then you can work now, in expectation of a probabilistic future cash flow. In another conception, you are effectively \u201cborrowing\u201d money from this anticipated future cash flow to fund the work in the first place; the expectation of future funding \u201cretro-causes\u201d the impactful work. Retroactive funding may be able to 1) provide incentives for creators to take on public goods projects with a potentially high, but uncertain, impact and 2) create a more efficient market by back-propagating signals on what outcomes were impactful post-hoc.</p></blockquote><p>&nbsp;</p><blockquote><p><a href=\"https://youtu.be/2hOhOdCbBlU\"><strong>Hypercerts, as introduced by David Dalrymple at Funding the Commons, </strong></a>is an interoperable data layer for impact-funding mechanisms. Each hypercert is an impact claim described by (1) the scope of work that has been (or will be) performed in a given time period by a set of specified contributors and (2) the scope of impact that this work has had (or will have) in another given time period. In addition, a hypercert has the potential to declare which rights the owner of the hypercert has, e.g. the right to publicly display the hypercert.</p><p>Individuals or organizations could create hypercerts of their work and sell all or some of them to funders, or award them commemoratively, in recognition of the funding provided. When, if or how hypercerts are sold or awarded isn\u2019t prescribed. Hypercerts open possibilities for creators of public goods to sell or award commemoratively some prospectively and others retrospectively. Hypercerts can be sold or awarded privately to a funder or as part of a public auction. Funders can outsource the funding decision to an expert panel or to a public poll via quadratic voting. These are just a few possibilities \u2013 hypercerts are agnostic about these mechanisms, and hence facilitate experimentation with them. This is the way in which hypercerts are a new primitive for public goods funding: They define the funding objects that claim to cause positive impact and make it possible to own and transfer this impact, including in new, innovative and diverse ways. All hypercerts together comprise a data layer for public goods funding.</p></blockquote><p>&nbsp;</p><blockquote><p>Now let\u2019s return to the central question, why funders might want to buy impact retrospectively. An advantage to retrospectively buying impact is that lots of uncertainty about projects and their impact is resolved. Hence, retrospective funders can, with less effort and costs, choose more impactful projects to fund.</p><p>The motivation to do this retrospectively is based on prestige, prior commitment and/or an understanding of the effectiveness of the whole system.</p></blockquote><p>&nbsp;</p><blockquote><p>We want to build the foundations for hypercerts and support 3+ impact economies in the next 12 months. If you want to contribute to these efforts, either by building the infrastructure with us, by funding through hypercerts, or by evaluating hypercerts, please reach out to us at <a href=\"mailto:commons@protocol.ai\"><strong>commons@protocol.ai</strong></a>. Our team is also actively looking for <a href=\"https://boards.greenhouse.io/protocollabs/jobs/4336675004\"><strong>software engineers</strong></a><strong> </strong>and a <a href=\"https://boards.greenhouse.io/protocollabs/jobs/4600519004\"><strong>product manager</strong></a>.</p></blockquote><p>Full post: <a href=\"https://protocol.ai/blog/hypercert-new-primitive/\">https://protocol.ai/blog/hypercert-new-primitive/</a></p>", "user": {"username": "DonyChristie"}}]