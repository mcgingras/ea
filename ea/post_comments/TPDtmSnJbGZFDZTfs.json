[{"_id": "vvnnGwpkB3T49GwDJ", "postedAt": "2023-09-21T05:23:06.646Z", "postId": "TPDtmSnJbGZFDZTfs", "htmlBody": "<p>A few general comments on this essay:</p><ul><li>I mostly agree with it. I agree, for example, that \"<strong>we can\u2019t use tech=good or tech=bad as a premise now to figure out what\u2019s going to happen with AI</strong>\". We should instead be sensitive to the specific details of AI when assessing whether and how it will be a risk. Many technologies have been bad before, and religiously adhering a rule that \"every technology is good\" would be absurd.</li><li>However, I also think that a large fraction of people, plausibly the majority, have a pessimistic bias when it comes to new technologies -- often while using what was considered new last decade without hesitation. <a href=\"https://www.econlib.org/historically-hollow-the-cries-of-populism/\">I agree with Bryan Caplan</a> that many people seem motivated by a bizarre search for \"dark linings in the silver clouds of business progress\". If you agree with me that this bias exists, then perhaps you can sympathize with my guess that the bias also affects how people perceive AI risk.</li><li>I don't think it's irrational to use a general heuristic of \"technological progress is good\" as long as the heuristic can be overridden with sufficient evidence. Lots of things in life are like this. For example, I generally use the heuristic \"lying is bad\" even though I don't think lying is <i>always</i> bad. The reason for the heuristic is that it's <i>usually</i> true, and that seems like an important rule of thumb to follow in situations where we don't know all the consequences of our behavior. Even in situations where I think \"maybe lying might actually be good here\", the heuristic reminds me that I generally need strong evidence. Does that mean I'm suffering from a \"dishonesty\" bucket error by lumping all lies together in the same bin? I don't think so.</li></ul>", "parentCommentId": null, "user": {"username": "Matthew_Barnett"}}, {"_id": "yRSG8c7Skkzzhahkz", "postedAt": "2023-09-21T06:03:23.724Z", "postId": "TPDtmSnJbGZFDZTfs", "htmlBody": "<p>I agree to most of the above but I\u2019m left more confused as to why you don\u2019t already see AI as an exception to tech progress being generally good.</p>\n", "parentCommentId": "vvnnGwpkB3T49GwDJ", "user": {"username": "Holly_Elmore"}}, {"_id": "PmD7naiTrXQyPNrfc", "postedAt": "2023-09-21T08:16:57.833Z", "postId": "TPDtmSnJbGZFDZTfs", "htmlBody": "<p>Strongly agree.</p>\n<p>I think it's important to not perceive this error as one of individual failures of rationality, but one that is predictably ideological and cultural.</p>\n<p>A position of pro-technology except for AI is a fairly idiosyncratic one to hold as it doesn't map onto standard ideologies and political fault lines.</p>\n", "parentCommentId": null, "user": {"username": "jackva"}}, {"_id": "hf8ggCjSTvwpfEmWj", "postedAt": "2023-09-21T08:17:22.193Z", "postId": "TPDtmSnJbGZFDZTfs", "htmlBody": "<p>AIs could help us achieve what we want. We could become extremely wealthy, solve aging and disease, find ways of elevating well-being, maybe even solve wild animal suffering, and accelerate alternatives to meat. I'm concerned about s-risks and the possibility of severe misalignment, but I don't think either are default outcomes. I just haven't seen a good argument for why we'd expect these catastrophic scenarios under standard incentives for businesses. Unless you think that these risks are probable, why would you think AI is an exception to the general trend of technology being good?</p>\n", "parentCommentId": "yRSG8c7Skkzzhahkz", "user": {"username": "Matthew_Barnett"}}, {"_id": "xDRi3hfYxyZ57uodo", "postedAt": "2023-09-21T12:22:07.570Z", "postId": "TPDtmSnJbGZFDZTfs", "htmlBody": "<p>Without getting into whether or not it's reasonable to expect catastrophe as the default under standard incentives for businesses, I think it's reasonable to hold the view that AI is probably going to be good while still thinking that the risks are unacceptably high.</p><p>If you think the odds of catastrophe are 10% \u2014 but otherwise think the remaining 90% is going to lead to amazing and abundant worlds for humans \u2014 you might still conclude that AI doesn't challenge the general trend of technology being good.</p><p>But I think it's also reasonable to conclude that 10% is still <i>way</i> too high given the massive stakes and the difficulty involved with trying to reverse/change course, which is disanalogous with most other technologies. IMO, the high stakes + difficulty of changing course is sufficient enough to override the \"tech is generally good\" heuristic.</p>", "parentCommentId": "hf8ggCjSTvwpfEmWj", "user": {"username": "julianhazell"}}, {"_id": "xhHwu2NhNGjAbLzEv", "postedAt": "2023-09-21T13:54:20.600Z", "postId": "TPDtmSnJbGZFDZTfs", "htmlBody": "<p>100% agree, that's my view. 80+% chance of being good (on a spectrum of good too, not just utopia good), but unacceptably high risk of being bad. And within that remaining 20ish (or whatever)% of possible bad, most of the bad in my mind is far from existential (bad actor controls AI, AI drives inequality to the point of serious unrest and war for a time etc.)</p><p>Its interesting to me that even within this AI safety discussion, a decent number of comments don't seem to have a bellcurve of outcomes in mind - many still seem to be looking at a binary between technoutopia and doom. I do recognise that its reasonable to think that those 2 are by far the most likely options though.<br><br>&nbsp;</p>", "parentCommentId": "xDRi3hfYxyZ57uodo", "user": {"username": "NickLaing"}}, {"_id": "SdwMTzJfR3sCuapjq", "postedAt": "2023-09-21T17:22:20.233Z", "postId": "TPDtmSnJbGZFDZTfs", "htmlBody": "<p>When you reason using probabilities, the more examples you have to reason over, the more likely your estimate is to be correct.</p>\n<p>If you make a bucket of \"all technology\" - because like you say, the reference class for AI is fuzzy - you consider the examples of all technology.</p>\n<p>I assume you agree that the net EV of \"all technology\" is positive.</p>\n<p>The narrower you make it \"is AGI exactly like a self replicating bioweapon\" you can choose a reference class that has a negative EV, but few examples.  I agree and you agree, self replicating bioweapons are negative EV.</p>\n<p>But...that kind of bucketing based on information you don't have is false reasoning.  You're wrong. You don't have the evidence, yet, to prove AGIs reference class because you have no AGI to test.</p>\n<p>Correct reasoning for a technology that doesn't even exist forces you to use a broad reference class.  You cannot rationally do better? (question mark is because I don't know of an algorithm that lets you do better.)</p>\n<p>Let me give an analogy.  There are medical treatments where your bone marrow is replaced.  These have terrible death rates, sometimes 66 percent.  But if you don't get the bone marrow replacement your death rate is 100 percent.  So it's a positive EV decision and you do not know the bucket you will fall in, [survivor| ! survivor].  So the rational choice is to say \"yes\" to the treatment and hope for the best.  (ignoring pain experienced for simplicity)</p>\n<p>The people that smile at your sadly - they are correct and the above is why.  The reason they are sad is well, we as a species could in fact end up out of luck, but this is a decision we still must take.</p>\n<p>All human scientific and decisionmaking is dependent on past information.  If you consider all past information we have and apply it to the reference class of \"AI\" you end up with certain conclusions.  (It'll probably quench, it's probably a useful tool, we probably can't stop everyone from building it).</p>\n<p>You can't reason on unproven future information.  Even if you may happen to be correct.</p>\n", "parentCommentId": null, "user": {"username": "Gerald Monroe"}}, {"_id": "4Zh56t6HGPc6o6Qbr", "postedAt": "2023-09-21T17:24:01.415Z", "postId": "TPDtmSnJbGZFDZTfs", "htmlBody": "<p>I also think existential risk from AI is way too high. That's why I strongly support AI safety research, careful regulation and AI governance. I'm objecting to the point about whether AI should be seen as an exception to the rule that technology is good. In the most probable scenario, it may well be the best technology ever!</p>\n", "parentCommentId": "xDRi3hfYxyZ57uodo", "user": {"username": "Matthew_Barnett"}}, {"_id": "gjBy6yeumrRi57FcL", "postedAt": "2023-09-21T17:27:20.403Z", "postId": "TPDtmSnJbGZFDZTfs", "htmlBody": "<p>If this debate were about whether we should do anything to reduce AI risk, then I would strongly be on the side of doing something. I'm not an effective accelerationist. I think AI will probably be good, but that doesn't mean I think we should simply wait around until it happens. I'm objecting to a narrower point about whether we should view AI as an exception to the general rule that technology is good.</p>\n", "parentCommentId": "xhHwu2NhNGjAbLzEv", "user": {"username": "Matthew_Barnett"}}, {"_id": "uBfcvZAvopmQEntpo", "postedAt": "2023-09-21T19:25:36.124Z", "postId": "TPDtmSnJbGZFDZTfs", "htmlBody": "<p>I think the answer to that question is how catastrophically bad tech of high enough capabilities could be, negative externalities or tech, and whether you include tech designed to cause harm like weapons. I have a very positive view of most technology but I\u2019m not sure how a category that included all of those would look in the end due to the tail risks.</p>\n", "parentCommentId": "gjBy6yeumrRi57FcL", "user": {"username": "Holly_Elmore"}}]