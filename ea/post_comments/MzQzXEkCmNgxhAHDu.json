[{"_id": "faohY9i8EsgiDyMWW", "postedAt": "2022-12-17T00:40:16.594Z", "postId": "MzQzXEkCmNgxhAHDu", "htmlBody": "<blockquote><p>I frequently hear people say EAs rely too much on quantifying uncertain variables, but I basically never hear the opposite criticism</p></blockquote><p>I mentioned this before several times, e.g. <a href=\"https://forum.effectivealtruism.org/posts/myp9Y9qJnpEEWhJF9/shortform?commentId=eQncvj8td8uCjfcAi\">here </a>and <a href=\"https://forum.effectivealtruism.org/posts/pxALB46SEkwNbfiNS/the-motivated-reasoning-critique-of-effective-altruism#Appendix_B__Not_using_cost_effectiveness_analyses_does_not_absolve_you_of_these_problems\">here</a>. Scott Alexander has also <a href=\"https://slatestarcodex.com/2013/05/02/if-its-worth-doing-its-worth-doing-with-made-up-statistics/\">said this</a>.&nbsp;<br>&nbsp;</p>", "parentCommentId": null, "user": {"username": "Linch"}}, {"_id": "TDyGrCqknxjNwGini", "postedAt": "2022-12-17T00:46:34.747Z", "postId": "MzQzXEkCmNgxhAHDu", "htmlBody": "<p>I disagree, I think this community in particular has a contrarian &nbsp;bias, probably as a result of the ties to the Rationalist community. A lot of people are here for the fun of discussion, and it is way more interesting to debate and discuss wacky and strange ideas than it is to go over the minutae of the most efficient malaria nets or whatever. Unfortunately, most of the time the boring, mainstream take also happens to be the true one.&nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "pmnP7oRhLCfniSkoi", "postedAt": "2022-12-17T02:22:03.445Z", "postId": "MzQzXEkCmNgxhAHDu", "htmlBody": "<blockquote>\n<p>Unfortunately, most of the time the boring, mainstream take also happens to be the true one.</p>\n</blockquote>\n<p>This is actually true, to a first approximation. Yet on the rare times that the mainstream is wrong, it really matters, so a version of this post still stands, EV wise.</p>\n", "parentCommentId": "TDyGrCqknxjNwGini", "user": {"username": "Sharmake"}}, {"_id": "6Xu2B4ufnDL6QmozX", "postedAt": "2022-12-17T02:24:30.737Z", "postId": "MzQzXEkCmNgxhAHDu", "htmlBody": "<p>I disagree with this disagreement.</p><p>EA is built on a foundation of rejecting the status quo. EA might only do that in places where the status quo is woefully inadequate of false in some way, but the status quo is still the status quo and it will strike back at people who challenge it.</p><p>The phenomenon described above is a side effect of optimization, not \"contrarian bias\". Contrarian bias is also a problem that many people in EA and especially rationalists have, but the only common factor is that there aren't the kind of people who assume that everything is all right and go along with it.</p>", "parentCommentId": "TDyGrCqknxjNwGini", "user": {"username": "trevorw96"}}, {"_id": "PBxZoZ9BHMvzHpZTM", "postedAt": "2022-12-17T03:41:30.340Z", "postId": "MzQzXEkCmNgxhAHDu", "htmlBody": "<p>I think this outline needs major revisions to be improved.</p><blockquote><p>It's like Lake Wobegon, where all the children are above average. It's impossible for every single person in the community to believe that the community is not X enough</p></blockquote><p>The above is the clearest example of why I think this post's argument fails. It is definitely possible for all members to believe something about the community is inadequate. &nbsp;For example, say a sports team is bad at defence. And, every member of the team could believe that they need to improve their defence. The fact that all members believe it does not disprove the empirical fact that the team's defence is inadequate.&nbsp;</p><p>Where this impossibility claim might have legs is where the group belief is about group belief itself. For example, it may be impossible for every member of the team to believe that every member of the team does not think about defence enough. &nbsp;But the examples you talked about are not like this - they concern actual action, not group belief.</p><p>You are conflating noticing/talking/writing about an action with the action itself. This is especially apparent for issues that are larger than individual action. For the example of systemic change: every member could believe sincerely that the community as a whole should 'do more work on systemic change', but reasonably &nbsp;continue their normal, &nbsp;everyday, non-systemic work. In that case, everyone agrees that more systemic change work is needed, but <i>no systemic change work actually gets done.</i><br><br>I think this is fairly common in EA counter-criticism, where people point to an old blog post about issue X, proving that EAs are already aware about X, and so, they argue, the criticism fails. While relevant, awareness of X pales in comparison to actually dealing with X itself. &nbsp;<br><br>This argument is further weakened by the fact that few critical stances are endorsed by near-100% of the community. There are significant counter-parties to most interesting critical claims. So making a critical &nbsp;claim is usually not in the situation of 'everyone already agrees with this'.&nbsp;</p><p>Finally, there's the common sense rebuttal that if a criticism is being made by many, that should (all else equal) increase your credence that the criticism is true. Contrarianism for contrarianism's sake is useful for checks and balances, but as a personal strategy is antithetical to epistemic modesty.</p><p><br>&nbsp;</p>", "parentCommentId": null, "user": {"username": "BenStewart"}}, {"_id": "cwRAGwgM8aj3aJEuk", "postedAt": "2022-12-17T05:26:21.933Z", "postId": "MzQzXEkCmNgxhAHDu", "htmlBody": "<p>My sense is if you look at \"wacky and strange ideas being explored by highly educated contrarians\" as a historical reference class, they've been important enough to be worth paying attention to.  I would put pre-WWW discussion &amp; exploration of hypermedia in this category, for instance.  And the <a href=\"https://wiki.c2.com/\">first wiki</a> was a rather wacky and strange thing.  I think you could argue that the big ideas underpinning EA (RCTs, veganism, existential risk) were all once wacky and strange.  (Existential risk was certainly wacky and strange about 10-15 years ago.)</p>\n", "parentCommentId": "TDyGrCqknxjNwGini", "user": {"username": "John_Maxwell_IV"}}, {"_id": "oPinTjbuwcmBSjhJn", "postedAt": "2022-12-17T05:28:10.240Z", "postId": "MzQzXEkCmNgxhAHDu", "htmlBody": "<p>Interesting argument!</p>\n<p>I'm not fully persuaded, because I think we're dealing with heterogeneous sub-populations.</p>\n<p>Consider the statement \"As a non-EA, I believe that EA funders don't allocate enough capital to funding development econ research\".  I don't think we can conclude from this statement that the opposite is true, and EA funders allocate too much capital to development econ research.</p>\n<p>The heterogeneous subpopulations perspective suggests that people who think development econ research is the most promising cause may be self-selecting out of the \"dedicated EA\" subpopulation.  I think criticism can be helpful in mitigating self-selection effects of this kind.</p>\n<p>Of course, we can't conclude that people who self-select out of EA on the basis of some disagreement are taking the correct side of that disagreement.  The point is that criticism allows us to hear their perspective even if they're not heavily involved.</p>\n<p>BTW, I thought the outline format was fine for this post.  Some individual sentences were choppy, but that was fine after I decided to read less thoroughly because it was a draft.</p>\n", "parentCommentId": null, "user": {"username": "John_Maxwell_IV"}}, {"_id": "JtFJn8wNrG3aK8uC4", "postedAt": "2022-12-17T10:27:16.529Z", "postId": "MzQzXEkCmNgxhAHDu", "htmlBody": "<p>&nbsp;I disagree with your disagreement of my disagreement!</p><p>The foundation of EA is (or at least should be), <i>finding the truth</i>. We should only reject the status quo if the status quo is wrong.&nbsp;</p><p>I don't have a problem with EA <i>trying out</i> hot takes and contrarian ideas, because finding cases where the status quo is genuinely wrong is valuable and gives a large competitive advantage. &nbsp;But I think this very fact leads to a bias towards accepting such ideas, even if they are not strictly true.&nbsp;</p>", "parentCommentId": "6Xu2B4ufnDL6QmozX", "user": {"username": "titotal"}}, {"_id": "RfJEkrYduDpPynbfT", "postedAt": "2022-12-17T10:34:27.065Z", "postId": "MzQzXEkCmNgxhAHDu", "htmlBody": "<p>I think it's good to discuss wacky and strange ideas, because on the occasions where they <i>actually are true</i>, it can lead to great things. &nbsp;A lot of great movements and foundations are built on disruptive ideas that were strange at the time but obvious in retrospect.&nbsp;</p><p>However, that doesn't really change my point that usually the reason a new idea seems wacky and strange is because it's wrong. And if you glorify the rare victories too much, you might start forgetting the many, many failures, leading towards a bias for accepting ideas that are somewhat half-baked.&nbsp;</p>", "parentCommentId": "cwRAGwgM8aj3aJEuk", "user": {"username": "titotal"}}, {"_id": "xw5MihJE7mzEwijSj", "postedAt": "2022-12-17T12:13:33.614Z", "postId": "MzQzXEkCmNgxhAHDu", "htmlBody": "<blockquote>\n<p>However, that doesn't really change my point that usually the reason a new idea seems wacky and strange is because it's wrong.</p>\n</blockquote>\n<p>I think seeming wacky and strange is mainly a function of difference, not wrongness per se.</p>\n<p>I'd argue that the best way to evaluate the merits of a wacky idea is usually to consider it directly. And discussing wacky ideas is what brings them from half-baked to fully-baked.</p>\n<p>If you can find a good way to count up the historical reference class of \"wacky and strange ideas being explored by highly educated contrarians\" and quantify the percentage of such ideas which were verifiable duds, I'd be very interested to see that post.  (The \"highly educated\" part is doing a lot of work here btw -- I know there's a lot of random occult type stuff that never goes anywhere.)  I don't think we're going to get anywhere talking about biases -- my view is that people <a href=\"https://en.wikipedia.org/wiki/Bandwagon_effect\">are</a> <a href=\"https://en.wikipedia.org/wiki/Status_quo_bias\">biased</a> in the other direction!  (Maybe that's the correct bias to have if you aren't experienced in the ways of highly educated contrarianism, though.)</p>\n", "parentCommentId": "RfJEkrYduDpPynbfT", "user": {"username": "John_Maxwell_IV"}}, {"_id": "SAsTAD76rXWMXYNSh", "postedAt": "2022-12-17T13:32:01.883Z", "postId": "MzQzXEkCmNgxhAHDu", "htmlBody": "<blockquote><p>wacky and strange ideas being explored by highly educated contrarians</p></blockquote><p>I mean, we can start with <a href=\"https://en.wikipedia.org/wiki/List_of_conspiracy_theories\">this list here</a>. I guarantee you there are highly educated people who buy into pretty much every conspiracy on that list. It's not at all hard to find, for example, engineers who think 9/11 was an inside job. Ted kascynski was a mathematics professor, etc, you get the point.&nbsp;</p><p>The list of possible wrong beliefs outnumbers the list of possible correct beliefs by many orders of magnitude. That stands for status quo opinions as well, but they have the advantage of withstanding challenges and holding for a longer period of time. That's the reason that if someone claims they've come up with a free energy machine, it's okay to dismiss them, unless you're feeling really bored that day.&nbsp;</p><p>Now, EA is exploring status quo ideas that are much less tested and firm that physics, so finding holes is much easier and worthwhile, and so I agree that strange ideas are worth considering. But most of them are still gonna be wrong, because they are untested.&nbsp;</p>", "parentCommentId": "xw5MihJE7mzEwijSj", "user": {"username": "titotal"}}, {"_id": "syHznhwaeNrz2cvwb", "postedAt": "2022-12-19T02:00:56.536Z", "postId": "MzQzXEkCmNgxhAHDu", "htmlBody": "<p>Yeah, it's mostly a heuristic argument, and the best you can do might be to just carefully look at the object level instead of trying to infer based on what people are saying.</p>", "parentCommentId": "PBxZoZ9BHMvzHpZTM", "user": {"username": "Jonas Vollmer"}}]