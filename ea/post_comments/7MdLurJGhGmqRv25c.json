[{"_id": "4cfrZwwLa39yvazmy", "postedAt": "2017-11-02T16:25:27.427Z", "postId": "7MdLurJGhGmqRv25c", "htmlBody": "<p>Thanks for writing this up!</p>\n<p>I think the idea is intriguing, and I agree that this is possible in principle, but I'm not convinced of your take on its practical implications. Apart from heuristic reasons to be sceptical of a new idea on this level of abstractness and speculativeness, my main objection is that a high degree of similarity with respect to reasoning (which is required for the decisions to be entangled) probably goes along with at least some degree of similarity with respect to values. (And if the values of the agents that correlate with me are similar to mine, then the result of taking them into account is also closer to my own values than the compromise value system of all agents.)</p>\n<p>You write: </p>\n<blockquote>\n<p>Superrationality only motivates cooperation if one has good reason to believe that another party\u2019s decision algorithm is indeed extremely similar to one\u2019s own. Human reasoning processes differ in many ways, and sympathy towards superrationality represents only one small dimension of one\u2019s reasoning process. It may very well be extremely rare that two people\u2019s reasoning is sufficiently similar that, having common knowledge of this similarity, they should rationally cooperate in a prisoner\u2019s dilemma. </p>\n</blockquote>\n<p>Conditional on this extremely high degree of similarity to me, isn't it also more likely that their values are also similar to mine? For instance, if my reasoning is shaped by the experiences I've made, my genetic makeup, or the set of all ideas I've read about over the course of my life, then an agent with identical or highly similar reasoning would also share a lot of these characteristics. But of course, my experiences, genes, etc. also determine my values, so similarity with respect to these factors implies similarity with respect to values. </p>\n<p>This is not the same as claiming that a given characteristic X that's relevant to decision-making is <em>generally</em> linked to values, in the sense that people with X have systematically different values. It's a subtle difference: I'm not saying that certain aspects of reasoning generally go along with certain values across the entire population; I'm saying that <em>a high degree of similarity</em> regarding reasoning goes along with similarity regarding values.</p>\n", "parentCommentId": null, "user": {"username": "Tobias_Baumann"}}, {"_id": "vLTqx6vm2f4GiuFF4", "postedAt": "2017-11-02T21:42:38.465Z", "postId": "7MdLurJGhGmqRv25c", "htmlBody": "<p>This is a very clear description of some cool ideas. Thanks to you and Caspar for doing this!</p>\n", "parentCommentId": null, "user": {"username": "MikeJohnson"}}, {"_id": "qcQraXnR8FLBBGffW", "postedAt": "2017-11-03T11:51:00.460Z", "postId": "7MdLurJGhGmqRv25c", "htmlBody": "<p>This was really interesting and probably as clear as such a topic can possibly be displayed. </p>\n<p>Disclaimer: I dont know how to deal with infinities mathematically. What I am about to say is probably very wrong.</p>\n<p>For every conceivable value system, there is an exactly opposing value system, so that there is no room for gains from trade between the systems (e.g. suffering maximizers vs suffering minimizers). </p>\n<p>In an infinite multiverse, there are infinite agents with decision algorithms sufficiently similar to mine to allow for MSR. Among them, there are infinite agents that hold any value system. So whenever I  cooperate with one value system, I defect on infinite agents that hold the exactly opposing values. So infinity seems to make cooperation impossble??</p>\n<p>Sidenote: If you assume decision algorithm and values to be orthogonal, why do you suggest to &quot;adjust [the values to cooperate with] by the degree their proponents are receptive to MSR ideas&quot;?</p>\n<p>Best, Jan</p>\n", "parentCommentId": null, "user": {"username": "JanBrauner"}}, {"_id": "7q8pMup7j2Kzu6Bym", "postedAt": "2017-11-04T17:11:43.021Z", "postId": "7MdLurJGhGmqRv25c", "htmlBody": "<blockquote>\n<p>For every conceivable value system, there is an exactly opposing value system, so that there is no room for gains from trade between the systems (e.g. suffering maximizers vs suffering minimizers).</p>\n</blockquote>\n<p>There is an intuition that &quot;disorderly&quot; worlds with improbable histories must somehow &quot;matter less,&quot; but it's very hard to cash out what this could mean. See <a href=\"http://lesswrong.com/lw/1iy/what_are_probabilities_anyway/\">this post</a> or <a href=\"http://lesswrong.com/lw/535/anthropics_in_a_tegmark_multiverse/\">this proposal</a>. I'm not sure these issues are solved yet (probably not). (I'm assuming that suffering maximizers or other really weird value systems would only evolve, or be generated when lightning hits someone's brain or whatever, in very improbable instances.) </p>\n<blockquote>\n<p>Sidenote: If you assume decision algorithm and values to be orthogonal, why do you suggest to &quot;adjust [the values to cooperate with] by the degree their proponents are receptive to MSR ideas&quot;?</p>\n</blockquote>\n<p>Good point; this shows that I'm skeptical about a strong version of independence where values and decision algorithms are completely uncorrelated. E.g., I find it less likely that deep ecologists would change their actions based on MSR than people with more EA(-typical) value systems. It is open to discussion whether (or how strongly) this has to be corrected for historical path dependencies and founder effects: If Eliezer had not been really into acausal decision theory, perhaps the EA movement would think somewhat differently about the topic. If we could replay history many times over, how often would EA be more or less sympathetic to superrationality than it is currently?</p>\n", "parentCommentId": "qcQraXnR8FLBBGffW", "user": {"username": "Lukas_Gloor"}}, {"_id": "SkztY7MpPMnp543uT", "postedAt": "2017-11-05T04:10:30.334Z", "postId": "7MdLurJGhGmqRv25c", "htmlBody": "<p>I\u2019m worried that people\u2019s altruistic sentiments are ruining their intuition about the prisoner\u2019s dilemma.  If Bob were an altruist, then there would be no dilemma. He would just cooperate. But within the framework of the one-shot prisoner\u2019s dilemma, defecting is a dominant strategy \u2013 no matter what Alice does, Bob is better off defecting.</p>\n<p>I\u2019m all for caring about other value systems, but if there\u2019s no causal connection between our actions and aliens\u2019, then it\u2019s impossible to trade with them. I can pump someone\u2019s intuition by saying, \u201cImagine a wizard produced a copy of yourself and had the two of you play the prisoner\u2019s dilemma. Surely you would cooperate?\u201d But that thought experiment is messed up because I care about copies of myself in a way that defies the set up of the prisoner\u2019s dilemma. </p>\n<p>One way to get cooperation in the one-shot prisoner\u2019s dilemma is if Bob and Alice can inspect each other\u2019s source code and prove that the other player will cooperate if and only if they do. But then Alice and Bob can communicate with each other! By having provably committed to this strategy, Alice and Bob can cause other player\u2019s with the same strategy to cooperate.</p>\n<p>Evidential decision theory also preys on our sentiments. I\u2019d like to live in a cool multiverse where there are aliens outside my light cone who do what I want them to, but it\u2019s not like my actions can cause that world to be the one I was born into.</p>\n<p>I\u2019m all for chasing after infinities and being nice to aliens, but acausal trade makes no sense. I\u2019m willing to take many other infinite gambles, like theism or simulationism, before I\u2019m willing to throw out causality.</p>\n", "parentCommentId": null, "user": {"username": "JamesDrain"}}, {"_id": "oKwnZzbXiYz5ZXQb3", "postedAt": "2017-11-05T05:53:16.979Z", "postId": "7MdLurJGhGmqRv25c", "htmlBody": "<p>Geographical distance is a kind of inferential distance.</p>\n", "parentCommentId": null, "user": {"username": "RomeoStevens"}}, {"_id": "f95szczMYAW23m6pd", "postedAt": "2017-11-07T21:06:24.058Z", "postId": "7MdLurJGhGmqRv25c", "htmlBody": "<p>I agree that altruistic sentiments are a confounder in the prisoner's dilemma. Yudkowsky (who would cooperate against a copy) makes a similar point in <a href=\"http://lesswrong.com/lw/tn/the_true_prisoners_dilemma/\"><em>The True Prisoner's Dilemma</em></a>, and there are lots of psychology studies showing that humans cooperate with each other in the PD in cases where I think they (that is, each individually) shouldn't. (Cf. section <a href=\"https://foundational-research.org/files/Multiverse-wide-Cooperation-via-Correlated-Decision-Making.pdf#page=92\">6.4 of the MSR paper</a>.)</p>\n<p>But I don't think that altruistic sentiments are the primary reason for why some philosophers and other sophisticated people tend to favor cooperation in the prisoner's dilemma against a copy. As you may know, <a href=\"https://en.wikipedia.org/wiki/Newcomb%27s_paradox\">Newcomb's problem</a> <a href=\"http://www.jstor.org/stable/2265034\">is</a> decision-theoretically similar to the PD against a copy. In contrast to the PD, however, it doesn't seem to evoke any altruistic sentiments. And yet, many people <a href=\"https://casparoesterheld.com/2017/06/27/a-survey-of-polls-on-newcombs-problem/\">prefer EDT's recommendations</a> in Newcomb's problem. Thus, the &quot;altruism error theory&quot; of cooperation in the PD is not particularly convincing.</p>\n<p>I don't see much evidence in favor of the &quot;wishful thinking&quot; hypothesis. It, too, seems to fail in the non-multiverse problems like Newcomb's paradox. Also, it's easy to come up with lots of incorrect theories about how any particular view results from biased epistemics, so I have quite low credence in any such hypothesis that isn't backed up by any evidence.</p>\n<blockquote>\n<p>before I\u2019m willing to throw out causality</p>\n</blockquote>\n<p>Of course, causal eliminativism (or skepticism) is one motivation to one-box in Newcomb's problem, but subscribing to eliminitavism is not necessary to do so.</p>\n<p>For example, in <em>Evidence, Decision and Causality</em> Arif Ahmed argues that causality is irrelevant for decision making. (The book starts with: &quot;Causality is a pointless superstition. These days it would take more than one book to persuade anyone of that. This book focuses on the \u2018pointless\u2019 bit, not the \u2018superstition\u2019 bit. I take for granted that there are causal relations and ask what doing so is good for. More narrowly still, I ask whether causal belief plays a special role in decision.&quot;) Alternatively, one could even endorse the use of causal relationships for informing one's decision but still endorse one-boxing. See, e.g., <a href=\"https://intelligence.org/files/TDT.pdf\">Yudkowsky, 2010</a>; <a href=\"http://www.justin-fisher.com/papers/DBDT.pdf\">Fisher, n.d.</a>; <a href=\"https://kops.uni-konstanz.de/bitstream/handle/123456789/32544/Spohn_0-316204.pdf?sequence=3\">Spohn, 2012</a> or <a href=\"https://www.youtube.com/watch?v=rGNINCggokM\">this talk by Ilya Shpitser</a>.</p>\n", "parentCommentId": "SkztY7MpPMnp543uT", "user": {"username": "caspar42"}}, {"_id": "DQ67KN69DeKETLxFW", "postedAt": "2017-11-12T22:48:26.641Z", "postId": "7MdLurJGhGmqRv25c", "htmlBody": "<p>Newcomb's problem isn't a challenge to causal decision theory. I can solve Newcomb's problem by committing to one-boxing in any of a number of ways e.g. signing a contract or building a reputation as a one-boxer. After the boxes have already been placed in front of me, however, I can no longer influence their contents, so it would be good if I two-boxed if the rewards outweighed the penalty e.g. if it turned out the contract I signed was void, or if I don't care about my one-boxing reputation because I don't think I'm going to play this game again in the future.</p>\n<p>The &quot;wishful thinking&quot; hypothesis might just apply to me then. I think it would be super cool if we could spontaneously cooperate with aliens in other universes.</p>\n<p>Edit: Wow, ok I remember what I actually meant about wishful thinking. I meant that evidential decision theory literally prescribes wishful thinking. Also, if you made a copy of a purely selfish person and then told them of the fact, then I still think it would be rational to defect. Of course, if they could commit to cooperating before being copied, then that would be the right strategy.</p>\n", "parentCommentId": "f95szczMYAW23m6pd", "user": {"username": "JamesDrain"}}, {"_id": "onEv8SCEFzr6y55Ts", "postedAt": "2017-11-13T03:47:37.061Z", "postId": "7MdLurJGhGmqRv25c", "htmlBody": "<blockquote>\n<p>After the boxes have already been placed in front of me, however, I can no longer influence their contents, so it would be good if I two-boxed</p>\n</blockquote>\n<p>You would get more utility if you were willing to one-box even when there's no external penalty or opportunity to bind yourself to the decision. Indeed, functional decision theory can be understood as a formalization of the intuition: &quot;I would be better off if only I could behave in the way I <em>would</em> have precommitted to behave in every circumstance, without actually needing to anticipate each such circumstance in advance.&quot; Since the predictor in Newcomb's problem fills the boxes based on your actual action, regardless of the reasoning or contract-writing or other activities that motivate the action, this suffices to always get the higher payout (compared to causal or evidential decision theory).</p>\n<p>There are also dilemmas where causal decision theory gets less utility even if it has the opportunity to precommit to the dilemma; e.g., <a href=\"https://arxiv.org/abs/1507.01986\">retro blackmail</a>.</p>\n<p>For a fuller argument, see the paper &quot;<a href=\"https://intelligence.org/2017/10/22/fdt/\">Functional Decision Theory</a>&quot; by Yudkowsky and Soares.</p>\n", "parentCommentId": "DQ67KN69DeKETLxFW", "user": {"username": "RobBensinger"}}, {"_id": "ehXf4h6bWg4ifDj7s", "postedAt": "2017-11-14T00:10:45.191Z", "postId": "7MdLurJGhGmqRv25c", "htmlBody": "<p>Ha, I think the problem is just that your formalization of Newcomb's problem is defined so that one-boxing is always the correct strategy, and I'm working with a different formulation. There are four forms of Newcomb's problem that jibe with my intuition, and they're all different from the formalization you're working with.</p>\n<ol>\n<li>Your source code is readable. Then the best strategy is whatever the best strategy is when you get to publicly commit e.g. you should tear off the wheel when playing chicken if you have the opportunity to do so before your opponent.</li>\n<li>Your source code is readable and so is your opponent's. Then you get mathy things like mutual simulation and lob's theorem.</li>\n<li>We're in the real world, so the only information the other player has to guess your strategy is information like your past behavior and reputation. (This is by far the most realistic situation in my opinion.)</li>\n<li>You're playing against someone who's an expert in reading body language, say. Then it might be impossible to fool them unless you can fool yourself into thinking you'll one-box. But of course, after the boxes are actually in front of you, it would be great for you if you had a change of heart.</li>\n</ol>\n<p>Your version is something like</p>\n<ol>\n<li>Your opponent can simulate you with 100% accuracy, including unforeseen events like something unexpected causing you to have a change of mind.</li>\n</ol>\n<p>If we're creating AIs that others can simulate, then I guess we might as well make them immune to retro blackmail. I still don't see the implications for humans, who cannot be simulated with 100% fidelity and already have ample intuition about their reputations and know lots of ways to solve coordination problems. </p>\n", "parentCommentId": "onEv8SCEFzr6y55Ts", "user": {"username": "JamesDrain"}}]