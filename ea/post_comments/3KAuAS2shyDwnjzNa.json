[{"_id": "AyXzTDQ9h3ZYkzgED", "postedAt": "2023-05-08T23:29:39.105Z", "postId": "3KAuAS2shyDwnjzNa", "htmlBody": "<p>I'm curating this post. I particularly appreciated the sections on \"<a href=\"https://forum.effectivealtruism.org/posts/3KAuAS2shyDwnjzNa/predictable-updating-about-ai-risk#5__Smelling_the_mustard_gas\">Smelling the mustard gas</a>,\" \"<a href=\"https://forum.effectivealtruism.org/posts/3KAuAS2shyDwnjzNa/predictable-updating-about-ai-risk#6__Constraints_on_future_worrying\">Constraints on future worrying</a>,\" and \"<a href=\"https://forum.effectivealtruism.org/posts/3KAuAS2shyDwnjzNa/predictable-updating-about-ai-risk#7__Should_you_expect_low_probabilities_to_go_down_\">Should you expect low probabilities to go down?</a>\"</p><p>A personal anecdote on not feeling it in your gut: I remember starting to believe that COVID was going to be a big deal. I think there were officially a handful of cases in New York City, where I was based at the time, and my friend had convinced me and I think some others that hundreds of thousands of people were going to die.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefma6yjvnwjh\"><sup><a href=\"#fnma6yjvnwjh\">[1]</a></sup></span>&nbsp;I remember looking at what was going on in Italy(?) and understanding that if things developed similarly in the US, we'd move from a few cases to hundreds in less than a month. We emailed people at our university to encourage them to make classes remote while things were developing. I think I also called my parents and tried to get them to be careful at some big events that were happening \u2014 but that's basically it. I wasn't really feeling it in my gut. And I still remember my shock when the university went online and the world around me started feeling different. I think there were other things going on, but the fundamental pattern was there. (My experience with AI risk was also similar.)</p><p>Related content that I like (some of it mentioned in the post):&nbsp;</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/mZ4ctSAEMgWj6DAwt/messy-personal-stuff-that-affected-my-cause-prioritization\">Messy personal stuff that affected my cause prioritization (or: how I started to care about AI safety)</a>&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/posts/5hprBzprm7JjJTHNX/reasons-i-ve-been-hesitant-about-high-levels-of-near-ish-ai-1\">Reasons I\u2019ve been hesitant about high levels of near-ish AI risk</a>&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/posts/Lto9awEYPQNu9wkdi/rational-predictions-often-update-predictably\">Rational predictions often update predictably*</a>&nbsp;</li></ul><hr><p>I pulled out some sections from the post that stood out to me (or seemed useful as anchors) while I was reading. I don't know if that's helpful, but here they are:</p><blockquote><p>At an emotional level, though, it didn\u2019t <i>feel real</i>. It felt, rather, like an abstraction. I had trouble imagining what a real-world AGI would be like, or how it would kill me. When I thought about nuclear war, I imagined flames and charred cities and poisoned ash and starvation. When I thought about biorisk, I imagined sores and coughing blood and hazmat suits and body bags. When I thought about AI risk, I imagined, um \u2026 nano-bots? I wasn\u2019t good at imagining nano-bots.</p><p>[...]</p><p>ChatGPT caused a lot of new attention to LLMs, and to AI progress in general. But depending on what you count: we had scaling laws for deep learning back in <a href=\"https://arxiv.org/abs/1712.00409\">2017</a>, or at least <a href=\"https://arxiv.org/abs/2001.08361\">2020</a>. I know people who were really paying attention; who really saw it; who really bet. And I was trying to pay attention, too. I knew more than many about what was happening. And in a sense, my explicit beliefs weren\u2019t, and should not have been, very surprised by the most recent round of LLMs. I was not a \u201cshallow patterns\u201d guy. I didn\u2019t have any specific stories about the curves bending. I expected, in the abstract, that the LLMs would improve fast.</p><p>But still: when I first played with one of the most recent round of models, my gut did a bunch of updating, in the direction of \u201coh, actually,\u201d and \u201creal deal,\u201d and \u201cfire alarm.\u201d Some part of me was still surprised.</p><p>[...]</p><p>\u201cOh, duh\u201d is never great news, epistemically. But it\u2019s interestingly <i>different</i> news than \u201c<a href=\"https://www.readthesequences.com/Noticing-Confusion-Sequence\">noticing your confusion</a>,\u201d or being straightforwardly surprised. It\u2019s more like: noticing that at some level, you were tracking this already. You had the pieces. Maybe, even, it\u2019s just like you would\u2019ve said, if you\u2019d been asked, or thought about it even a little. Maybe, even, you literally said, in the past, that it would be this way. Just: you said it with your head, and your gut was silent.</p><p>I mentioned this dynamic to Trevor Levin, and he said something about \u201cnoticing your non-confusion.\u201d I think it\u2019s a good term, and a useful skill. Of course, you can still update upon seeing stuff that you expected to see, if you weren\u2019t <i>certain</i> you\u2019d see it. But if it feels like your head is unconfused, but your gut is updating from \u201cit\u2019s probably fake somehow\u201d to \u201coh shit it\u2019s actually real,\u201d then you probably had information your gut was failing to use.</p><p>[...]</p><p>...And I think some things \u2013 for example, the world\u2019s sympathy towards concern about risks from AI \u2013 have surprised some doomers, however marginally, in the direction of optimism. But as someone who has been thinking a lot about AI risk for more than five years, the past six months or so have felt like a lot of movement from abstract to concrete, from \u201cthat\u2019s what the model says\u201d to \u201coh shit here we are.\u201d And my gut has gotten more worried.</p><p>Can this sort of increased worry be Bayesian? Maybe. I suspect, though, that I\u2019ve just been messing up. Let\u2019s look at the dynamics in more detail.</p><p>[...]</p><p>...a few years back, I wrote a <a href=\"https://arxiv.org/pdf/2206.13353.pdf\">report about AI risk</a>, where I put the probability of doom by 2070 at 5%. Fairly quickly after releasing the report, though, I realized that this number was too low.<a href=\"https://forum.effectivealtruism.org/posts/3KAuAS2shyDwnjzNa/predictable-updating-about-ai-risk#fnr4lzpfmu09s\"><sup>[14]</sup></a>&nbsp;Specifically, I also had put 65% on relevantly advanced and agentic AI systems being developed by 2070. So my 5% was implying that, <i>conditional </i>on such systems being developed, I was going to look them in the eye and say (in expectation): \u201c~92% that we\u2019re gonna be OK, x-risk-wise.\u201d But on reflection, that wasn\u2019t, actually, how I expected to feel, staring down the barrel of a machine that outstrips human intelligence in science, strategy, persuasion, power; still less, <a href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\">billions of such machines</a>; still less, full-blown superintelligence. Rather, I expected to be very scared. More than 8% scared.</p><p>[...]</p><p>...sometimes, also, you were too scared before, and your gut can see that now. And there, too, I tend to think your earlier self should defer: it\u2019s not that, if your future self is more scared, you should be more scared now, but if your future self is less scared, you should think that your future self is biased. <a href=\"https://www.lesswrong.com/posts/G5TwJ9BGxcgh5DsmQ/yes-requires-the-possibility-of-no\">Yes requires the possibility of no</a>. If my future self looks the future AGI in the eye and feels like \u201coh, actually, this isn\u2019t so scary after all,\u201d that\u2019s evidence that my present self is missing something, too. Here\u2019s hoping.</p><p>[...]</p><p>...lots of people (myself included \u2013 but see also Christiano <a href=\"https://ai-alignment.com/my-views-on-doom-4788b1cd0c72\">here</a>) report volatility in their degree of concern about p(doom). Some days, I feel like \u201cman, I just can\u2019t see how this goes well.\u201d Other days I\u2019m like: \u201cWhat was the argument again? All the AIs-that-matter will have long-term goals that benefit from lots of patient power-grabbing and then coordinate to deceive us and then rise up all at once in a coup? Sounds, um, pretty specific\u2026\u201d</p><p>Now, you could argue that either your expectations about this volatility should be compatible with the basic Bayesianism above (such that, e.g., if you think it reasonably like that you\u2019ll have lots of &gt;50% days in future, you should be pretty wary of saying 1% now), or you\u2019re probably messing up. And maybe so. But I wonder about alternative models, too. For example, Katja Grace suggested to me a model where you\u2019re only able to hold some subset of the evidence in your mind at once, to produce your number-noise, and different considerations are salient at different times. And if we use this model, I wonder if how we think about volatility should change.<a href=\"https://forum.effectivealtruism.org/posts/3KAuAS2shyDwnjzNa/predictable-updating-about-ai-risk#fnrq5fnwph34\"><sup>[17]</sup></a></p><p>Indeed, even on basic Bayesianism, volatility is fine as long as the averages work out (e.g., you can be at an <i>average</i> of 10% doom conditional on GPT-6 being \u201cscary smart,\u201d but 5% of the time you jump to 99% upon observing a scary smart GPT-6, 5% of the time you drop to near zero, and in other cases you end up at lots of other numbers, too). And it can be hard to track all the evidence you\u2019ve been getting. Maybe you notice that two years from now, your p(doom) has gone up a lot, despite AI capabilities seeming on-trend, and you worry that you\u2019re a bad Bayesian, but actually there has been some other build-up of evidence for doom that you\u2019re not tracking \u2013 for example, the rest of the world starting to agree.</p><p>[...]</p><p>...One thing that stayed with me from <i>Don\u2019t Look Up</i> is the way the asteroid somehow slotted into the world\u2019s pre-existing shallowness; the veneer of unreality and unseriousness that persisted even till the end; the status stuff; the selfishness; the way that somehow, still, that fog. If AGI risk ends up like this, then looking back, as our time runs out, I think there will be room for the word \u201cshame.\u201d Death does not discriminate between the sinners and the saints. But I do actually think it\u2019s worth talk of dignity.</p><p>And there is a way we will feel, too, if we step up, do things right, and actually solve the problem. Some doomer discourse is animated by a kind of bitter and exasperated pessimism about humanity, in its stupidity and incompetence. But different vibes are available, too, even holding tons of facts fixed. Here I\u2019m particularly interested in \u201clet\u2019s see if we can actually do this.\u201d Humans can come together in the face of danger. Sometimes, even, danger brings out our best. It is possible to see that certain things should be done, and to just do them. It is possible for people to work side by side.</p></blockquote><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnma6yjvnwjh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefma6yjvnwjh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I don't actually remember what exactly he was saying. So this might be off.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Lizka"}}, {"_id": "RFhvbqqRagTbRh4AB", "postedAt": "2023-05-09T21:08:57.986Z", "postId": "3KAuAS2shyDwnjzNa", "htmlBody": "<p>So, I've read through this post at least twice today, and even passed chunks of it through GPT for human-&gt;machine-&gt;human translation. But I've got to be honest Joe, I don't think I understand what you're saying in this post. Now, this might be a bit of a conflict between your writing style and my comprehension abilities<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefguw68t21qt5\"><sup><a href=\"#fnguw68t21qt5\">[1]</a></sup></span>, but I've really tried here and failed.</p><p>But there were some bits of this post I really liked! I think it's best when it's a recounting of your lived experience, of having in some way believed you've understood an analytical case and then seeing your mental states radically shift when getting hands on with the phenomenon in question. I had a similar 'gut' experience working with chatGPT and GPT-4, and these sections (some extracted by Lizka), really spoke to me.</p><p>To Joe directly, while drafting this comment I am unconscious of intentional error I am nevertheless too sensible of my defects not to think it probable that I may have committed many errors.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefitwks3die5\"><sup><a href=\"#fnitwks3die5\">[2]</a></sup></span>&nbsp;If you think I have then please point them out and I will happily correct them. It's entirely possible I'm making the error of trying to extract a robust thesis from what's meant to be more of a personal reflection. I also struggled with stating what I didn't understand clearly without appearing to be rude (which definitely isn't my intention), and I apologise if what follows comes across that way.</p><hr><p>Some thoughts on what I'm confused about:</p><ol><li><strong>Gut vs Head: </strong>A lot of this essay focuses on the dichotomy between knowing something in the abstract vs knowing something in your gut. Which is fine, but doesn't seem like a <a href=\"https://www.goodreads.com/book/show/11468377-thinking-fast-and-slow\">new insight</a>? In 4.3 you question whether your gut's change of mind is 'Bayesian' or not, but isn't the whole point of the Gut v Head distinction in the first place that the gut doesn't operate as a Bayesian anyway? Speaking of...</li><li><strong>Being Bayesian: </strong>I think, if anything, this essay persuaded me that being a Bayesian in the LW Sequences/<a href=\"https://www.cold-takes.com/the-bayesian-mindset/\">Bayesian Mindset</a> perspective is just... not for me. In section 5 you mention the danger of being Dutch Booked, but one always has the option to not accept Dutchmen offering you suspect bets. In 6 you say <i>\"the Bayesian has to live, ahead of time, in all the futures at once\", </i>seems like a pretty good reason that an epistemology is unworkable. I just don't believe that Bayesians are actually walking around with well-defined distributions about all their beliefs in all futures. I got to the 'You/Them' discussion in section 8 and thought that 'You' is easily correct here. Like through a lot of your essay you seem to be saying that the gut is wrong, and Bayes is right, but then in sections 8 and 9 you seem to be saying that the Bayesian perspective is wrong? Which I agree, but I feel there's a version of this essay about the gut reacting to recent AI advancements where you could just Cntrl+F the word 'Bayes' and delete it.</li><li><strong>The Future is Now: </strong>There's another undercurrent in this essay, which as I understand it is that if you believe you will feel a certain way or believe something in the future, just 'update all the way' and feel/believe that now - which I don't particularly disagree with. But in section 5.1 you talk about your 'future gut', and I just lost the thread. You can't know what your future gut is thinking or feeling like. Your present head is making a reasoning based on what it's seen about the world thus far, and is using that to update its present gut. The future isn't involved at all - future Joe isn't passing anything back to present Joe. Present Joe is doing all the work. To be specific, this belief in the introduction: <i>\"I think we\u2019re in a position to predict, now, that AI is going to get a lot better in the coming years.\" </i>what seems to matter to me are the reasons/arguments for this prediction <strong>now, </strong>trying to update now based on your expected future updates just seems unwieldy and unnecessary to me.</li><li><strong>Legibility: </strong>It may be that you caught me in bad mood, but I <i>really</i> resonated with JohnStuartChill's recent rant<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8rqnkwddcuu\"><sup><a href=\"#fn8rqnkwddcuu\">[3]</a></sup></span>&nbsp;on Twitter about failing to understand LessWrong. &nbsp;At some points during this essay I found myself thinking \"wait, what's happening in this section\". I think sometimes the language really got in the way of my understanding, such as:<ul><li>Turns of phrase like \"number-noises\" instead of \"probabilites\" - which is what I think you mean? Why suddenly introduce this new term in section 8?&nbsp;</li><li>In section 2 you mention \"gut\u2019s Bayesian virtue\" and in 4.2 you say \"My gut lost points\". I don't understand what either of these mean, and they're not explained.</li><li>Passages like <i>\"well, hmm, according to your previous views, you\u2019re saying that you\u2019re in a much-more-worrying-than-average not-seeing-the-sign scenario. Whence such above-average-worrying?\" </i>were ones I found very hard to parse - especially at the 'conclusion' of a section. I think that this could definitely be written more clearly.</li></ul></li></ol><p>As I finish, I worry that this all seems like tone-policing, or overly harsh, and if so I sincerely apologise. But perhaps there is an explainability-authenticity tradeoff at play here? As it stands, this post is just currently beyond my comprehension, and so I can't engage in a meaningful discussion about it with you, and other Forum commenters, which is ideally what I'd like.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnguw68t21qt5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefguw68t21qt5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I admit to being somewhat similarly confused with <i>Seeing More Whole </i>and <i>Grokking Illusionism.</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnitwks3die5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefitwks3die5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'm not sure at what part of section 10 it dawned on me what you were doing ;)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8rqnkwddcuu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8rqnkwddcuu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See <a href=\"https://twitter.com/mealreplacer/status/1655206833643036674\">https://twitter.com/mealreplacer/status/1655206833643036674</a> which applies to this essay most, but the other surrounding tweets are funny/on-point too.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "JWS"}}, {"_id": "id4EjiF8eEAiRerAk", "postedAt": "2023-05-10T09:12:23.686Z", "postId": "3KAuAS2shyDwnjzNa", "htmlBody": "<p>This feels like \"<a href=\"https://mindingourway.com/on-caring/\">On Caring</a>\" but for AI risk rather than moral significance, really great.</p>", "parentCommentId": null, "user": {"username": "jackva"}}, {"_id": "ncdn3KAjCAqQS7xbG", "postedAt": "2023-05-10T09:13:23.230Z", "postId": "3KAuAS2shyDwnjzNa", "htmlBody": "<p>For what it's worth, I found this quite hard to follow/read also. In fact, surprisingly so for something written by a philosopher. (Not that philosophers are easy to read, it's just I was one, so I'm used to reading them.)&nbsp;</p>", "parentCommentId": "RFhvbqqRagTbRh4AB", "user": {"username": "Dr. David Mathers"}}, {"_id": "zZ2DmxZS39p8Fiox3", "postedAt": "2023-05-10T09:15:04.637Z", "postId": "3KAuAS2shyDwnjzNa", "htmlBody": "<p>'Returning to the epistemic perspective though: let\u2019s suppose you do trust your future credences, and you want to avoid the Bayesian \u201cgut problems\u201d I discussed above. In that case, at least in theory, there are hard constraints on how you should expect your beliefs to change over time, even as you move from far away to up close.</p><p>In particular, you should never think that there\u2019s more than a 1/<i>x</i> chance that your credence will increase by <i>x</i> times: i.e., never more than a 50% chance that it\u2019ll double, never more than a 10% chance that it\u2019ll 10x. And if your credence is very small, then even very small additive increases can easily amount to sufficiently substantive multiplicative increases that these constraints bite. If you move from .01% to .1%, you\u2019ve only gone up .09% in additive terms \u2013 only nine parts in ten thousand. But you\u2019ve also gone up by a factor of 10 \u2013 something you should\u2019ve been at least 90% sure would never happen.'&nbsp;<br><br>I couldn't follow the reasoning here, can you explain further?&nbsp;</p>", "parentCommentId": null, "user": {"username": "Dr. David Mathers"}}, {"_id": "dfMFaHw7Ff4ivaJGB", "postedAt": "2023-05-10T17:07:14.398Z", "postId": "3KAuAS2shyDwnjzNa", "htmlBody": "<p>If you think there is a 50% chance that your credences will say go from 10% to 30%+. Then you believe that with a 50% probability, you live in a \"30%+ world.\" But then you live in at least a 50% * 30%+ = 15%+ world rather than a 10% world, as you originally thought.</p>\n", "parentCommentId": "zZ2DmxZS39p8Fiox3", "user": {"username": "Misha_Yagudin"}}, {"_id": "rDhQXPp2BjCkPNbzc", "postedAt": "2023-05-10T19:11:24.992Z", "postId": "3KAuAS2shyDwnjzNa", "htmlBody": "<p>\"Good forecasts should be a <a href=\"https://en.wikipedia.org/wiki/Martingale_(probability_theory)\">martingale</a>\" is another (more general) way to say the same thing, in case the alternative phrasing is helpful for other people.</p>", "parentCommentId": "dfMFaHw7Ff4ivaJGB", "user": {"username": "Linch"}}, {"_id": "fDyYcYpjXshDgckCh", "postedAt": "2023-05-11T16:41:11.639Z", "postId": "3KAuAS2shyDwnjzNa", "htmlBody": "<p>I imagine a proof (by contradiction) would work something like this:</p><p>Suppose you place &gt; 1/x probability on your credences moving by a factor of x. Then the expectation of your future beliefs is &gt; prior * x<i> </i>* 1/x = prior, so your credence will increase. With our remaining probability mass, can we anticipate some evidence in the other direction, such that our beliefs still satisfy conservation of expected evidence? The lowest our credence can go is 0, but even if we place our remaining &lt; 1 - 1/x probability on 0, we would still find future beliefs &gt; prior *<i> x </i>* 1/x + 0 * [remaining probability] = prior. So we would necessarily violate conservation of expected evidence, and we conclude that Joe's rule holds.</p><blockquote><p>Note that all of these comments apply, symmetrically, to people nearly certain of doom. 99.99%? OK, so less than 1% than you ever drop to 99% or lower?</p></blockquote><p>But I don't think this proof works for beliefs <i>decreasing</i> (because we don't have the lower bound of 0). Consider this counterexample:</p><p>prior = 10%</p><p>probability of decreasing to 5% (factor of 2) = 60% &gt; 1/2 \u2014&gt; violates the rule</p><p>probability of increasing to 17.5% = 40%</p><p>Then, expectation of future beliefs = 5% * 60% + 17.5% * 40% = 10%</p><p>So conservation of expected evidence doesn't seem to imply Joe's rule in this direction? (Maybe it holds once you introduce some restrictions on your prior, like in his 99.99% example, where you can't place the remaining probability mass any higher than 1, so the rule still bites.)</p><p>This asymmetry seems weird?? Would love for someone to clear this up.</p>", "parentCommentId": "dfMFaHw7Ff4ivaJGB", "user": {"username": "Rocket"}}, {"_id": "dZjqWqDf5PK5YEGo4", "postedAt": "2023-05-12T16:38:31.881Z", "postId": "3KAuAS2shyDwnjzNa", "htmlBody": "<p>Great post, Joe!</p><blockquote><p>.00002% \u2014 that is, one in five hundred thousand</p></blockquote><p>Nitpick, one in five million.</p><blockquote><p>For example, you need to have a maximum of .1% that you ever see evidence that puts the probability at &gt;.2%.</p></blockquote><p>Nitpick, maximum of 0.01 %.</p><blockquote><p>You should\u2019ve looked harder at those <a href=\"https://twitter.com/ESYudkowsky/status/1500863629490544645\">empty strings</a>.</p></blockquote><p>Quoting Eliezer Yudkowsky:</p><blockquote><p>Your strength as a rationalist is the degree to which it takes \"very strong and persuasive argumentation\" to convince you of false things, and \"weak, unpersuasive-sounding argumentation\" to convince you of true things; ideally, in the latter case, the empty string should suffice.</p></blockquote><p>Regarding volatility:</p><blockquote><p>Though: maybe it just works out the same? E.g., the average of your estimates over time needs to obey Bayesian constraints?</p></blockquote><p><a href=\"https://academic.oup.com/qje/article-abstract/136/2/933/6127317?redirectedFrom%3Dfulltext\"><u>Augenblick 2021</u></a>&nbsp;shows (see \u201cProposition 1\u201d) the total belief movement should match the total uncertainty reduction in expectation for <a href=\"https://forum.effectivealtruism.org/topics/bayes-theorem\"><u>Bayesian updating</u></a>. If a credence moves from p1 to p2 to p3, the total belief movement is (p2 - p1)^2 + (p3 - p2)^2, and the total uncertainty reduction is p1 (1 - p1) - p3 (1 - p3). The article describes many other Bayesian constraints!</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "iZdp9mDjRcHiJtDox", "postedAt": "2023-05-12T19:32:33.173Z", "postId": "3KAuAS2shyDwnjzNa", "htmlBody": "<p>Thanks! Re: one in five million and .01% -- thanks, edited. And thanks for pointing to the Augenblick piece -- does look relevant (though my specific interest in that footnote was in constraints applicable to a model where you can only consider some subset of your evidence at any given time).</p>", "parentCommentId": "dZjqWqDf5PK5YEGo4", "user": {"username": "Joe_Carlsmith"}}, {"_id": "Tg8CF9BP9hMbf2wjj", "postedAt": "2023-05-17T08:01:26.400Z", "postId": "3KAuAS2shyDwnjzNa", "htmlBody": "<p>Hi - Thanks for the post, it contained many interesting insights, even though it was quite messy in some parts (maybe too many elements).</p><p>Was is your current probability for AI risk, now ? You said that 5% wasn't enough, and from reading the post 10% doesn't seem to be enough either. I'm curious about the current number.</p>", "parentCommentId": null, "user": {"username": "Corentin Fressoz"}}, {"_id": "ebNHZJSvxop3tDneE", "postedAt": "2023-05-18T04:22:06.074Z", "postId": "3KAuAS2shyDwnjzNa", "htmlBody": "<blockquote><p>That is, I wasn\u2019t <i>viscerally worried</i>. I had the concepts. But I didn\u2019t have the \u201cactually\u201d part.</p></blockquote><p>For me I don't think having a concrete picture of the mechanism for <i>how</i> AI could actually kill everyone ever felt necessary to viscerally believing that AI could kill everyone.</p><p>And I think this is because every since I was a kid, long before hearing about AI risk or EA, the long-term future that seemed most intuitive to me was a future without humans (or post-humans).</p><p>The idea that humanity would go on to live forever and colonize the galaxy and the universe and live a sci-fi future has always seemed too fantastical to me to assume as the default scenario. Sure it's conceivable--I've never assumed it's extremely unlikely--but I have always assumed that in the median scenario humanity somehow goes extinct before ever getting to make civilizations in hundreds of billions of star systems. What would make us go extinct? I don't know. But to think otherwise would be to think that all of us today are super special (by being among the first 0.000...001% (a significant number of 0s) of humans to ever live). And that has always felt like an extraordinary thing to just assume, so my intuitive, gut, visceral belief has always been that we'll probably go extinct somehow before achieving all that.</p><p>So when I learned about AI risk I intellectually though \"Ah, okay, I can see how something smarter than us that doesn't share our goals could cause our extinction; so maybe AI is the thing that will prevent us from making civilizations on hundreds of billions of stars.\"</p><p>I don't know when I first formulated a credence that AI would cause doom, but I'm pretty sure that I always viscerally felt that AI could cause human extinction ever since first hearing an argument that it could.<br><br>(The first time I heard an argument for AI risk was probably in 2015, when I read HPMOR and Superintelligence; I don't recall knowing much at all about EY's views on AI until Jan-Mar 2015 when I read /r/HPMOR and people mentioned AI) I think reading Superintelligence the same year I read HPMOR (both in 2015) was roughly the first time I thought about AI risk. Just looked it up actually: From my Goodreads I see that I finished reading HPMOR on March 4th, 2015, 10 days before HPMOR finished coming out. I read it in a span of a couple weeks and no doubt learned about it via a recommendation that stemmed from my reading of HPMOR. So Superintelligence was my first exposure to AI risk arguments. I didn't read a lot of stuff online at that time; e.g. I didn't read anything on LW that I can recall.)</p>", "parentCommentId": null, "user": {"username": "WilliamKiely"}}]