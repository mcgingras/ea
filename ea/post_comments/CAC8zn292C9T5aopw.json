[{"_id": "uJuqjDAhy7T6wrmaX", "postedAt": "2023-05-10T19:49:25.575Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<p>Thanks for this update and for all the work you're doing!<br><br>A potential pivot toward AI safety feels pretty significant, especially for such a core \"big tent EA\" team. Is it correct to interpret this as a reflection of the team's cause prioritization? Or is this instead because of (1) particularly poor community health in the non-EA AI safety community relative to other causes, (2) part of a plan to spin off into other EA-adjacent spaces, or (3) something else?</p>", "parentCommentId": null, "user": {"username": "Rockwell Schwartz"}}, {"_id": "mMbvb92g8HLFXN4wi", "postedAt": "2023-05-11T01:59:17.122Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<p>I'm curious about how CHSP's practical ability to address \"concerns about individuals in the AI safety space\" might compare to its abilities in EA spaces. Particularly, it seems that the list of practical things CHSP could do about a problematic individual in the non-EA AI safety space could be significantly more limited than for someone in the EA space (e.g., banning them from CEA events).</p>", "parentCommentId": null, "user": {"username": "Jason"}}, {"_id": "cuFBjYDbimFh2wEDq", "postedAt": "2023-05-11T17:45:16.549Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<p>On expanding to AI safety: Given all of the recent controversies, I\u2019d think very carefully before linking the reputations of EA and AI safety more than they are already linked. It seems that the same group was responsible for community health for both and it either made a mistake or a correct, but controversial decision, there would be a greater chance of the blowback affecting both communities, rather than just one.</p>\n<p>Maybe community health functions being independent of CEA would make this less of an issue. I guess it\u2019s plausible, but also maybe not? Might also depend on whether any new org has EA in the name?</p>\n<p>I think that the root cause is that there is no AI safety field building co-ordinating committee which would naturally end up taking on such a function. Someone really needs to make that happen (I'm not the right person for this).</p>\n<p>This would have the advantage of allowing the norms of the communities to develop somewhat separately. It would sacrifice some operational efficiencies, but I think this is one of those areas where it is better not to skimp.</p>\n", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "dLvPuugtr6f4i8yfa", "postedAt": "2023-05-12T09:13:11.777Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<p>I\u2019m a little worried that use of the word pivot was a mistake on my part in that it maybe implies more of a change than I expect; if so, apologies.</p><p>I think this is best understood as a combination of</p><ul><li>Maybe this is really important, especially right now [which I guess is indeed a subset of cause prioritization]</li><li>Maybe there are unusually high leverage things to do in that space right now</li><li>Maybe the counterfactual is worse - it\u2019s a space with a lot of new energy, new organizations, etc, and so a lot more opportunity for re-making old mistakes, not a lot of institutional knowledge, and so on.&nbsp;<ul><li>I think this basically agrees with your point (1), but as a hypothesis, not a conclusion</li><li>In addition, there is an unusual amount of money and power flowing around this space right now, and so it might warrant extra attention</li><li>This is a small effect, but we\u2019ve received some requests from within this space to pay more attention to it, which seems like some (small) evidence</li></ul></li></ul>", "parentCommentId": "uJuqjDAhy7T6wrmaX", "user": {"username": "ChanaMessinger"}}, {"_id": "kjQLbEgy9WbwY4kD6", "postedAt": "2023-05-12T09:13:52.112Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<p>I think as an overall gloss, it\u2019s absolutely true that we have fewer levers in the AI Safety space. There are two sets of reasons why I think it\u2019s worth considering anyway:</p><ol><li>Impact - in a basic kind of \u201chigh importance can balance out lower tractability\u201d way, we don\u2019t want to only look where the streetlight is, and it\u2019s possible that the AI Safety space will seem to us sufficiently high impact to aim some of our energy there</li><li>Don\u2019t want to underestimate the levers - we have fewer explicit moves to make in the broader AI Safety space (e.g. disallowing people from events), but there is both a high overlap with EA and my guess is that some set of people in a new space will appreciate people who have thought about community management a lot giving thoughts / advice / sharing models and so on.</li></ol><p>But both of these could be insufficient for a decision to put more of our effort there, and it remains to be seen.</p>", "parentCommentId": "mMbvb92g8HLFXN4wi", "user": {"username": "ChanaMessinger"}}, {"_id": "rs6LoNu4fi4xoWX4L", "postedAt": "2023-05-13T15:00:16.235Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<p>As part of potential changes, will there be a review into the conflict of interests the Community Health team face and could face in the future? For example, one potential conflict of interest to me is that some Community Health team members are <a href=\"https://funds.effectivealtruism.org/team\">Fund Advisors for EA Funds</a> (which is also part of Effective Ventures).&nbsp;</p>", "parentCommentId": null, "user": {"username": "AgiDoomer"}}, {"_id": "h7tH6XSWcqiCuX4aF", "postedAt": "2023-05-13T22:33:09.792Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<p>Thanks; that makes sense. I think part of the background is about potential downsides of an EA branded organization -- especially one that is externally seen (rightly or not) as the flagship EA org -- going into a space with (possibly) high levels of interpersonal harm and reduced levers to address it. I don't find the Copenhagen Interpretation of Ethics as generally convincing as many here do. Yet this strikes me as a case in which EA could easily end up taking the blame for a lot of stuff it has little control over.</p>\n<p>I'd update more in favor if CHSP split off from CEA and EVF, and even more in favor if the AI-safety casework operation could somehow have even greater separation from EA.</p>\n", "parentCommentId": "kjQLbEgy9WbwY4kD6", "user": {"username": "Jason"}}, {"_id": "EhtYHA9BSdTiRQNu8", "postedAt": "2023-05-15T20:10:29.971Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<p>Why do you think this is a noteworthy conflict?</p>", "parentCommentId": "rs6LoNu4fi4xoWX4L", "user": {"username": "Larks"}}, {"_id": "bak2cTHRqAMZ3NAYa", "postedAt": "2023-05-16T09:31:27.174Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<p>The example I highlighted is striking to me, given how much information flows through the community health team (much of which may not be representative or may be false or even exaggerated). There is also a selection bias in the information they receive (ie, mostly negative as the CH team deals with reports and you <i>report</i> someone when something bad has happened).</p><p>For example, say you're socially unaware and accidentally make someone feel uncomfortable at an EA event without realising it. If the person you made feel uncomfortable mentioned this to the community health team, would the team then mention it to the grantmakers when you apply for an EA Funds grant? Would the grantmakers believe the worst in you (given the CH team suffers selection bias)? Why does the fact you accidentally made someone feel uncomfortable at an event matter when it comes to applying for grant?</p><p>Now there are dozens of other situations you could come up with and a dozen ways to mitigate this, but the easiest option seems to be removing yourself from that role entirely.</p><p>.</p><p>As for more widely, I think the CH team don't know how to handle conflict of interest and even overestimated their abilities to do so previously (eg, Julia Wise thinking she could handle the Owen Cotten-Barret situation herself instead of bringing in external professionals). I think the CH team should look into other places where conflicts of interest could arise in the future (by team members holding too many positions). The EA Funds may or may not be one such place.</p>", "parentCommentId": "EhtYHA9BSdTiRQNu8", "user": {"username": "AgiDoomer"}}, {"_id": "MHTwTQKmwPEXevZtB", "postedAt": "2023-05-16T09:50:38.114Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<blockquote><p>We decided to rename our team to better reflect the scope of our work. We\u2019ve found that when people think of our team, they mostly think of us as working on topics like mental health and interpersonal harm. While these areas are a central part of our work, we also work on a wide range of other things, such as advising on decisions with significant potential downside risk, improving community epistemics, advising programs working with minors, and reducing risks in areas with high geopolitical risk.</p></blockquote><p>Hmm, it's good that you guys are giving an updated public description of your activities. But it seems like the EA community let some major catastrophes pass through previously, and now the team that was nominally most involved with managing risk, rather than narrowing its focus to the most serious risks, is broadening to include the old stuff, new stuff, all kinds of stuff. This suggests to me that EA needs some kind of group that thinks carefully about what the biggest risks are, and focuses on just those ones, so that the major catastrophes are avoided in future - some kind of risk management / catastrophe avoidance team.</p>", "parentCommentId": null, "user": {"username": "RyanCarey"}}, {"_id": "uZWpzMtnbGDrg22mN", "postedAt": "2023-05-19T10:49:30.669Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<p>It seems very plausible to me that EA should have more capacity on risk management. That question is one of the things&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Dq69kvjKyxQzKNRH7/seeking-expertise-to-improve-ea-organizations\"><u>this taskforce</u></a> might dig into.</p>", "parentCommentId": "MHTwTQKmwPEXevZtB", "user": {"username": "Julia_Wise"}}, {"_id": "EwFv7JcLtvJ9M8kJ5", "postedAt": "2023-05-19T13:01:29.799Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<p>Fwiw, I think we have different perspectives here - outside of epistemics, everything on that list is there precisely because we think it\u2019s a potential source of some of the biggest risks. It\u2019s not always clear where risks are going to come from, so we look at a wide range of things, but we are in fact trying to be on the lookout for those big risks. Thanks for flagging it doesn\u2019t seem like we are; I\u2019m not sure if this comes from miscommunication or a disagreement about where big risks come from.</p><p>Maybe another place of discrepancy is that we primarily think of ourselves as looking for where high-impact gaps are, places where someone should be doing something but no one is, and risks are a subset of that but not the entirety.</p><p>(To be clear I also agree with <a href=\"https://forum.effectivealtruism.org/posts/CAC8zn292C9T5aopw/community-health-and-special-projects-updates-and-contacting-1?commentId=uZWpzMtnbGDrg22mN\">Julia</a> that it\u2019s very plausible EA should have more capacity on this)</p>", "parentCommentId": "MHTwTQKmwPEXevZtB", "user": {"username": "ChanaMessinger"}}, {"_id": "JjaNE3QnJR92N72rd", "postedAt": "2023-05-19T15:48:51.129Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<p>I imagine that, for a number of reasons, it's not a good idea to put out an official, full CHSP List of Reasonably-Specific, Major-to-Catastrophic Risks complete with current and potential evaluation and mitigation measures. And your inability to do so likely makes it difficult to fully brief the community about your past, current, and potential efforts to manage those kinds of risks.</p>\n<p>My guess is that a sizable fraction of the major-to-catastrophic risks center around a fairly modest number of key leaders, donors, and organizations. If that's so, there might be benefit to more specifically communicating CHSP's awareness of that risk cluster and high-level details about possible strategies to improve performance in that specific cluster (or to transition responsibility for that cluster elsewhere).</p>\n", "parentCommentId": "EwFv7JcLtvJ9M8kJ5", "user": {"username": "Jason"}}, {"_id": "JR9cqC8LxAvZ4oYEY", "postedAt": "2023-05-20T04:58:16.268Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<p>Yeah, I'm not trying to stake out a claim on what the biggest risks are.</p>\n<p>I'm saying assume that some community X has team A that is primarily responsible for risk management. In one year, some risks materialise as giant catastrophes - risk management has gone terribly. The worst. But the community is otherwise decently good at picking out impactful meta projects. Then team A says \"we're actually not just in the business of risk management (the thing that is going poorly), we also see ourselves as generically trying to pick out high impact meta projects. So much so that we're renaming ourselves as 'Risk Management and cool meta projects'\". And to repeat, we (impartial onlookers) think that many other teams have been capable of running impactful meta projects. We might start to wonder whether team A is losing their focus, and losing track of the most pertinent facts about the strategic situation.</p>\n", "parentCommentId": "EwFv7JcLtvJ9M8kJ5", "user": {"username": "RyanCarey"}}, {"_id": "vCvGKojWBDbwhk8Ak", "postedAt": "2023-06-15T20:56:34.667Z", "postId": "CAC8zn292C9T5aopw", "htmlBody": "<p>My understanding was that community health to some extent carries the can for catastrophe management, along with other parts of CEA and EA orgs. Is this right? I don't know whether people within CEA think anyone within CEA bears any responsibility for which parts of the past year's catastrophes. (I don't know as in I genuinely don't know - it's not a leading statement.) Per Ryan's comment, the actions you have announced here don't seem at all appropriate given the past year's catastrophes.&nbsp;</p>", "parentCommentId": "EwFv7JcLtvJ9M8kJ5", "user": null}]