[{"_id": "rkBYoABZszSubwMTB", "title": "Fixed: The EA related shapelyvalue.com was expiring. Is anyone doing something about it?", "postedAt": "2022-09-24T06:02:25.437Z", "htmlBody": "<p>The domain <a href=\"http://shapleyvalue.com\">http://shapleyvalue.com</a> has a bare-bones shapely value calculator and links to <a href=\"https://www.effectivealtruism.org/\">https://www.effectivealtruism.org/</a> as well as having various EA-related examples. <a href=\"https://who.is/whois/shapleyvalue.com\">https://who.is/whois/shapleyvalue.com</a> lists the domain expiry date as 2022-09-24, which is tomorrow.</p><p>I am posting here in the hopes that if someone wants that website renewed, they can do so before it is too late. I have also emailed the domain's contact email. (I am not familiar with the domain registration process, so it is possible there is some automation which will deal with this just before the domain would expire.)</p>", "user": {"username": "Forged Invariant"}}, {"_id": "ZeJavb9Tg9AZhiE6F", "title": "Global Challenges Project - Existential Risk Workshop", "postedAt": "2022-09-23T22:13:01.293Z", "htmlBody": "<p><a href=\"https://www.globalchallengesproject.org/\"><strong><u>Global Challenges Project</u></strong></a><strong> is running an intensive 3-day workshop over Thanksgiving break&nbsp;</strong>for students interested in effective altruism to think seriously about existential risk.&nbsp;</p><p>It\u2019s in Berkeley, CA, and is free to attend with travel support available.</p><p>The workshop is all about digging into the risks and arguments related to existential risk from AI and biotechnology, as well as meeting and learning from professionals working in those fields.&nbsp;</p><ul><li>More info at:&nbsp;<a href=\"https://www.globalchallengesproject.org/\"><u>globalchallengesproject.org</u></a></li><li><strong>Apply by October 23rd</strong>:&nbsp;<a href=\"https://www.globalchallengesproject.org/calendar\"><u>globalchallengesproject.org/calendar</u></a></li></ul>", "user": {"username": "EmmaAbele"}}, {"_id": "gJDsRogMajFiisdxw", "title": "Berlin / EA Newbie Meetup (unaffiliated) - please RSVP", "postedAt": "2022-09-23T23:16:32.979Z", "htmlBody": "<p>A lot of people are new to Berlin or the EA community - like me. This meetup was born from a conversation at EAGxBerlin, an idea or ambition to bring together some newbies and some (more) experienced member(s) of Berlin EA to connect the community here.</p><p>We want to get to know Berlin EA, each other, and you!</p><p>The location is the <a href=\"https://berlin.ccc.de/\"><u>Chaos Computer Club Berlin</u></a> which can comfortably fit about 20 people. Please RSVP so we know how many to expect.</p><p>We'd like everyone to be there at 6:30 PM CEST but feel free to arrive by 6 PM and stick around as long as you want<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref488jtl83roo\"><sup><a href=\"#fn488jtl83roo\">[1]</a></sup></span>.</p><p>You're very welcome even if you\u2019ve never been to a meetup or you feel like you don't fit.</p><p>Route (German): <a href=\"https://berlin.ccc.de/page/anfahrt\">https://berlin.ccc.de/page/anfahrt</a></p><p>PS: There might be less RSVPs here than people actually coming since there already is interest from a chat group.</p><p>PPS: Co-organizers and Berlin-veterans: Feel free to make yourself known in the comments.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn488jtl83roo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref488jtl83roo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Rooms will close 1 AM.</p></div></li></ol>", "user": {"username": "Milli"}}, {"_id": "YwD8WXLvQkk4FynaC", "title": "Interpreting Neural Networks through the Polytope Lens", "postedAt": "2022-09-23T18:03:02.819Z", "htmlBody": "", "user": {"username": "Sid Black"}}, {"_id": "upEmqunf3PnRqud6C", "title": "Houston Effective Altruism Social Meetup - October 6 @ Segundo Coffee", "postedAt": "2022-09-23T17:13:41.081Z", "htmlBody": "", "user": null}, {"_id": "W7C5hwq7sjdpTdrQF", "title": "Announcing the Future Fund's AI Worldview Prize", "postedAt": "2022-09-23T16:28:35.127Z", "htmlBody": "<p><i>Update:</i> <i>I, Nick Beckstead, no longer work at the Future Fund am writing this update purely in a personal capacity. Since the&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=YNH2Gj35cueyT35tp#YNH2Gj35cueyT35tp\"><i><u>Future Fund team has resigned</u></i></a><i> and FTX has&nbsp;</i><a href=\"https://www.nytimes.com/2022/11/11/business/ftx-bankruptcy.html\"><i><u>filed for bankruptcy</u></i></a><i>, it now seems very unlikely that these prizes will be paid out. I'm very sad about the disruption that this may cause to contest participants.</i></p><p><br><i>I would encourage participants who were working on entries for this prize competition to save their work and submit it to&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/3kaojgsu6qy2n8TdC/pre-announcing-the-2023-open-philanthropy-ai-worldviews\"><i><u>Open Philanthropy's own AI Worldview Contest in 2023</u></i></a><i>.</i></p><p>&nbsp;</p><p>Today we are announcing a competition with prizes ranging from $15k to $1.5M for work that informs the Future Fund's fundamental assumptions about the future of AI, or is informative to a panel of&nbsp;<a href=\"https://en.wikipedia.org/wiki/Superforecaster\"><u>superforecaster</u></a> judges selected by&nbsp;<a href=\"https://goodjudgment.com/\"><u>Good Judgment Inc</u></a>. These prizes will be open for three months\u2014until Dec 23\u2014after which we may change or discontinue them at our discretion. We have two reasons for launching these prizes.</p><p>First, we hope to expose our assumptions about the future of AI to intense external scrutiny and improve them. We think artificial intelligence (AI) is the development most likely to dramatically alter the trajectory of humanity this century, and it is consequently one of our top funding priorities. Yet our philanthropic interest in AI is fundamentally dependent on a number of very difficult judgment calls, which we think have been inadequately scrutinized by others.&nbsp;</p><p>As a result, we think it's really possible that:</p><ul><li>all of this AI stuff is a misguided sideshow,</li><li>we should be even more focused on AI, or</li><li>a bunch of this AI stuff is basically right, but we should be focusing on entirely different aspects of the problem.</li></ul><p>If any of those three options is right\u2014and we strongly suspect at least one of them is\u2014we want to learn about it as quickly as possible because it would change how we allocate hundreds of millions of dollars (or more) and help us better serve our mission of improving humanity's longterm prospects.</p><p>Second, we are aiming to do&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/2mx6xrDrwiEKzfgks/announcing-the-future-fund-1#Our_2022_plans\"><u>bold and decisive tests</u></a> of prize-based philanthropy, as part of our more general aim of testing highly scalable approaches to funding. We think these prizes contribute to that work. If these prizes work, it will be a large update in favor of this approach being capable of surfacing valuable knowledge that could affect our prioritization. If they don't work, that could be an update against this approach surfacing such knowledge (depending how it plays out).</p><p>The rest of this post will:</p><ul><li>Explain the beliefs that, if altered, would dramatically affect our approach to grantmaking</li><li>Describe the conditions under which our prizes will pay out</li><li>Describe in basic terms how we arrived at our beliefs and cover other clarifications</li></ul><h2>Prize conditions</h2><p>On our&nbsp;<a href=\"https://ftxfuturefund.org/area-of-interest/\"><u>areas of interest</u></a> page, we introduce our core concerns about AI as follows:</p><blockquote><p>We think artificial intelligence (AI) is the development most likely to dramatically alter the trajectory of humanity this century. AI is already posing serious challenges: transparency, interpretability, algorithmic bias, and robustness, to name just a few. Before too long, advanced AI could automate the process of scientific and technological discovery, leading to economic growth rates well over 10% per year (see&nbsp;<a href=\"https://scholar.harvard.edu/files/aghion/files/artificial_intelligence.pdf\"><u>Aghion et al 2017</u></a>,&nbsp;<a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/\"><u>this post</u></a>, and&nbsp;<a href=\"https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/\"><u>Davidson 2021</u></a>).</p><p>As a result, our world could soon look radically different. With the help of advanced AI, we could make enormous progress toward ending global poverty, animal suffering, early death and debilitating disease. But two formidable new problems for humanity could also arise:</p><ol><li><strong>Loss of control to AI systems</strong><br>Advanced AI systems might acquire undesirable objectives and pursue power in unintended ways, causing humans to lose all or most of their influence over the future.</li><li><strong>Concentration of power</strong><br>Actors with an edge in advanced AI technology could acquire massive power and influence; if they misuse this technology, they could inflict lasting damage on humanity\u2019s long-term future.</li></ol><p>For more on these problems, we recommend Holden Karnofsky\u2019s \u201c<a href=\"https://www.cold-takes.com/most-important-century/\"><u>Most Important Century</u></a>,\u201d Nick Bostrom\u2019s&nbsp;<a href=\"https://www.amazon.com/dp/B00LOOCGB2/ref=dp-kindle-redirect/132-7264950-9107823?_encoding=UTF8&amp;btkr=1\"><u>Superintelligence</u></a>, and Joseph Carlsmith\u2019s \u201c<a href=\"https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#heading=h.pwdbumje5w8r\"><u>Is power-seeking AI an existential risk?</u></a>\u201d.</p></blockquote><p>Here is a table identifying various questions about these scenarios that we believe are central, our current position on the question (for the sake of concreteness), and alternative positions that would significantly alter the Future Fund's thinking about the future of AI<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefww4u5bouauc\"><sup><a href=\"#fnww4u5bouauc\">[1]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftdlosh6fi9o\"><sup><a href=\"#fntdlosh6fi9o\">[2]</a></sup></span>:</p><figure class=\"table\"><table><tbody><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Proposition</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Current position</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Lower prize threshold</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Upper prize threshold</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">\u201cP(misalignment x-risk|AGI)\u201d: Conditional on AGI being developed by 2070, humanity will go extinct or drastically curtail its future potential due to loss of control of AGI</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>15%</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>7%</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>35%</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">AGI will be developed by January 1, 2043</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>20%</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>10%</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>45%</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">AGI will be developed by January 1, 2100</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>60%</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>30%</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">N/A</td></tr></tbody></table></figure><p><br>&nbsp;</p><p><strong>Future Fund will award a prize of $500k to anyone that publishes analysis that moves these probabilities to the lower or upper prize threshold.</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkb1fhmtm4fs\"><sup><a href=\"#fnkb1fhmtm4fs\">[3]</a></sup></span>&nbsp;To qualify, please please publish your work (or publish a post linking to it) on the&nbsp;<a href=\"https://forum.effectivealtruism.org/\"><u>Effective Altruism Forum</u></a>, the&nbsp;<a href=\"https://www.alignmentforum.org/\"><u>AI Alignment Forum</u></a>, or&nbsp;<a href=\"https://www.lesswrong.com/\"><u>LessWrong</u></a> with a \"Future Fund worldview prize\" tag. You can also participate in the contest by publishing your submission somewhere else (e.g.&nbsp;<a href=\"https://arxiv.org/\"><u>arXiv</u></a> or your blog) and filling out&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdflvhfJ76r6ZSnBuXdQFMMHuz16cSs8bf9l7GCqyrbEqGCCw/viewform\"><u>this submission form</u></a>. We will then linkpost/crosspost to your submission on the EA Forum.</p><p>We will award larger prizes for larger changes to these probabilities, as follows:</p><ul><li><strong>$1.5M for moving \u201cP(misalignment x-risk|AGI)\u201d below 3% or above 75%</strong></li><li><strong>$1.5M for moving \u201cAGI will be developed by January 1, 2043\u201d below 3% or above 75%</strong></li></ul><p>We will award prizes of intermediate size for intermediate updates at our discretion.</p><p>We are also offering:</p><ul><li><strong>A $200k prize for publishing any significant original analysis</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0fpixu5jj1pb\"><sup><a href=\"#fn0fpixu5jj1pb\">[4]</a></sup></span><strong>&nbsp;which we consider the new canonical reference on any one of the above questions</strong>, even if it does not move our current position beyond a relevant threshold. Past works that would have qualified for this prize include:&nbsp;<a href=\"https://intelligence.org/files/AIPosNegFactor.pdf\"><u>Yudkowsky 2008</u></a>,&nbsp;<a href=\"https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742\"><i><u>Superintelligence</u></i></a>,&nbsp;<a href=\"https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines\"><u>Cotra 2020</u></a>,&nbsp;<a href=\"https://arxiv.org/abs/2206.13353\"><u>Carlsmith 2021</u></a>, and Karnofsky's&nbsp;<a href=\"https://www.cold-takes.com/most-important-century/\"><u>Most Important Century</u></a> series. (While the above sources are lengthy, we'd prefer to offer a prize for a brief but persuasive argument.)</li><li><strong>A $200k prize for publishing any analysis which we consider the canonical critique of the current position highlighted above on any of the above questions</strong>, even if it does not move our position beyond a relevant threshold. Past works that might have qualified for this prize include:&nbsp;<a href=\"https://intelligence.org/ai-foom-debate/\"><u>Hanson 2011</u></a>,&nbsp;<a href=\"https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si\"><u>Karnofsky 2012</u></a>, and&nbsp;<a href=\"https://docs.google.com/document/d/1FlGPHU3UtBRj4mBPkEZyBQmAuZXnyvHU-yaH-TiNt8w/edit#heading=h.7650wn3rs7yv\"><u>Garfinkel 2021</u></a>.</li><li><strong>At a minimum, we will award $50k to the three published analyses that most inform the Future Fund's overall perspective on these issues, and $15k for the next 3-10 most promising contributions to the prize competition.</strong> (I.e., we will award a minimum of 6 prizes. If some of the larger prizes are claimed, we may accordingly award fewer of these prizes.)</li></ul><p>As a check/balance on our reasonableness as judges,&nbsp;<strong>a panel of superforecaster judges will independently review</strong> a subset of highly upvoted/nominated contest entries with the aim of identifying any contestant who did not receive a prize, but would have if the superforecasters were running the contest themselves (e.g., an entrant that sufficiently shifted the superforecasters\u2019 credences).</p><ul><li>For the $500k-$1.5M prizes, if the superforecasters think an entrant deserved a prize but we didn\u2019t award one, we will award $200k (or more) for up to one entrant in each category (existential risk conditional on AGI by 2070, AGI by 2043, AGI by 2100), upon recommendation of the superforecaster judge panel.</li><li>For the $15k-200k prizes, if the superforecasters think an entrant deserved a prize but we didn\u2019t award one, we will award additional prizes upon recommendation of the superforecaster judge panel.</li></ul><p>The superforecaster judges will be selected by Good Judgment Inc. and will render their verdicts autonomously. While superforecasters have only been demonstrated to have superior prediction track records for shorter-term events, we think of them as a lay jury of smart, calibrated, impartial people.</p><p>Our hope is that potential applicants who are confident in the strength of their arguments, but skeptical of our ability to judge impartially, will nonetheless believe that the superforecaster jury will plausibly judge their arguments fairly. After all, entrants could reasonably doubt that people who have spent tens of millions of dollars funding this area would be willing to acknowledge it if that turned out to be a mistake.</p><h2>Details and fine print</h2><ul><li>Only original work published after our prize is announced is eligible to win.</li><li>We do not plan to read everything written with the aim of claiming these prizes. We plan to rely in part on the judgment of other researchers and people we trust when deciding what to seriously engage with. We also do not plan to explain in individual cases why we did or did not engage seriously.</li><li>If you have questions about the prizes, please ask them as comments on this post. We do not plan to respond to individual questions over email.</li><li>All prizes will be awarded at the final discretion of the Future Fund. Our published decisions will be final and not subject to appeal. We also won't be able to explain in individual cases why we did not offer a prize.</li><li>Prizes will be awarded equally to coauthors unless the post indicates some other split. At our discretion, the Future Fund may provide partial credit across different entries if they together trigger a prize condition.</li><li>If a single person does research leading to multiple updates, Future Fund may\u2014at its discretion\u2014award the single largest prize for which the analysis is eligible (rather than the sum of all such prizes).</li><li>We will not offer awards to any analysis that we believe was net negative to publish due to&nbsp;<a href=\"https://en.wikipedia.org/wiki/Information_hazard\"><u>information hazards</u></a>, even if it moves our probabilities significantly and is otherwise excellent.</li><li>At most one prize will be awarded for each of the largest prize categories ($500k and $1.5M). (If e.g. two works convince us to assign &lt; 3% subjective probability in AGI being developed in the next 20 years, we\u2019ll award the prize to the most convincing piece (or split in case of a tie).)</li></ul><p>For the first two weeks after it is announced\u2014until October 7\u2014the rules and conditions of the prize competition may be changed at the discretion of the Future Fund. After that, we reserve the right to clarify the conditions of the prizes wherever they are unclear or have wacky unintended results.</p><h3>Information hazards</h3><p>Please be careful not to publish&nbsp;<a href=\"https://en.wikipedia.org/wiki/Information_hazard\"><u>information that would be net harmful</u></a> to publish. We think people should not publish very concrete proposals for how to build AGI (if they know of them), and or things that are too close to that.</p><p>If you are worried publishing your analysis would be net harmful due to information hazards, we encourage you to a) write your draft and then b) ask about this using the \u201cREQUEST FEEDBACK\u201d feature on the Effective Altruism forum or LessWrong pages (appears on the draft post page, just before you would normally publish a post):</p><h2><img src=\"http://res.cloudinary.com/cea/image/upload/v1669852219/mirroredImages/W7C5hwq7sjdpTdrQF/xcbfpkvnjmw2qxo5rfbn.png\"></h2><p>The moderators have agreed to help with this.</p><p>If you feel strongly that your analysis should not be made public due to information hazards, you may submit your prize entry through&nbsp;<a href=\"https://docs.google.com/forms/u/3/d/16CX2lqdAJwW0EUPvwHmt4CRR5JXByuan0khqAT6YzQ8/edit\"><u>this form</u></a>.</p><h2>Some clarifications and answers to anticipated questions</h2><p><i>What do you mean by AGI?</i></p><p>Imagine a world where cheap AI systems are fully substitutable for human labor. E.g., for any human who can do any job, there is a computer program (not necessarily the same one every time) that can do the same job for $25/hr or less. This includes entirely AI-run companies, with AI managers and AI workers and everything being done by AIs.</p><ul><li>How large of an economic transformation would follow? Our guess is that it would be pretty large (see&nbsp;<a href=\"https://scholar.harvard.edu/files/aghion/files/artificial_intelligence.pdf\"><u>Aghion et al 2017</u></a>,&nbsp;<a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/\"><u>this post</u></a>, and&nbsp;<a href=\"https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/\"><u>Davidson 2021</u></a>), but - to the extent it is relevant - we want people competing for this prize to make whatever assumptions seem right to them.</li></ul><p>For purposes of our definitions, we\u2019ll count it as AGI being developed if there are AI systems that power a comparably profound transformation (in economic terms or otherwise) as would be achieved in such a world. Some caveats/clarifications worth noticing:</p><ul><li>A comparably large economic transformation could be achieved even if the AI systems couldn\u2019t substitute for literally 100% of jobs, including providing emotional support. E.g., Karnofsky\u2019s notion of&nbsp;<a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/\"><u>PASTA</u></a> would probably count (though that is an empirical question), and possibly some other things would count as well.</li><li>If weird enough things happened, the metric of GWP might stop being indicative in the way it normally is, so we want to make sure people are thinking about the overall level of weirdness rather than being attached to a specific measure or observation. E.g., causing human extinction or drastically limiting humanity\u2019s future potential may not show up as rapid GDP growth, but automatically counts for the purposes of this definition.</li></ul><p><i>Why are you starting with such large prizes?</i></p><p>We really want to get closer to the truth on these issues quickly. Better answers to these questions could prevent us from wasting hundreds of millions of dollars (or more) and years of effort on our part.</p><p>We could start with smaller prizes, but we\u2019re interested in running bold and decisive tests of prizes as a philanthropic mechanism.</p><p>A further consideration is that sometimes people argue that all of this futurist speculation about AI is really dumb, and that its errors could be readily explained by experts who can't be bothered to seriously engage with these questions. These prizes will hopefully test whether this theory is true.</p><p><i>Can you say more about why you hold the views that you do on these issues, and what might move you?</i></p><p>I (Nick Beckstead) will answer these questions on my own behalf without speaking for the Future Fund as a whole.</p><p>For \"Conditional on AGI being developed by 2070, humanity will go extinct or drastically curtail its future potential due to loss of control of AGI.\" I am pretty sympathetic to the analysis of Joe Carlsmith&nbsp;<a href=\"https://arxiv.org/abs/2206.13353\"><u>here</u></a>. I think Joe's estimates of the relevant probabilities are pretty reasonable (though the bottom line is perhaps somewhat low) and if someone convinced me that the probabilities on the premises in his argument should be much higher or lower I'd probably update. There are a number of&nbsp;<a href=\"https://www.lesswrong.com/posts/qRSgHLb8yLXzDg4nf/reviews-of-is-power-seeking-ai-an-existential-risk\"><u>reviews</u></a> of Joe Carlsmith's work that were helpful to varying degrees but would not have won large prizes in this competition.</p><p>For assigning odds to AGI being developed in the next 20 years, I am blending a number of intuitive models to arrive at this estimate. They are mostly driven by a few high-level considerations:</p><ul><li>I think computers will eventually be able to do things brains can do. I've believed this for a long time, but if I were going to point to one article as a reference point I'd choose&nbsp;<a href=\"https://www.openphilanthropy.org/blog/new-report-brain-computation\"><u>Carlsmith 2020</u></a>.</li><li>Priors that seem natural to me (\"beta-geometric distributions\") start us out with non-trivial probability of developing AGI in the next 20 years, before considering more detailed models. I've also believed this for a long time, but I think&nbsp;<a href=\"https://www.openphilanthropy.org/blog/report-semi-informative-priors\"><u>Davidson 2021</u></a>'s version is the best, and he gives 8% to AGI by 2036 through this method as a central estimate.</li><li>I assign substantial probability to continued hardware progress, algorithmic progress, and other progress that fuels AGI development over the coming decades. I'm less sure this will continue many decades into the future, so I assign somewhat more probability to AGI in sooner decades than later decades.</li><li>Under these conditions, I think we'll pass some limits\u2014e.g. approaching hardware that's getting close to as good as we're ever going to get\u2014and develop AGI if we're ever going to develop it.</li><li>I'm extremely uncertain about the hardware requirements for AGI (at the point where it's actually developed by humans), to a point where my position is roughly \"I dunno, log uniform distribution over anything from the amount of compute used by the brain to a few orders of magnitude less than evolution.\"&nbsp;<a href=\"https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines\"><u>Cotra 2020</u></a>\u2014which considers this question much more deeply\u2014has a similar bottom line on this. (Though her&nbsp;<a href=\"https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines\"><u>updated timelines</u></a> are shorter.)</li><li>I'm impressed by the progress in deep learning to the point where I don't think we can rule out AGI even in the next 5-10 years, but I'm not impressed enough by any positive argument for such short timelines to move dramatically away from any of the above models..</li></ul><p>(I'm heavily citing reports from Open Philanthropy here because a) I think they did great work and b) I'm familiar with it. I also recommend&nbsp;<a href=\"https://www.cold-takes.com/where-ai-forecasting-stands-today/\"><u>this piece</u></a> by Holden Karnofsky, which brings a lot of this work\u2014and other work\u2014together.)</p><p>In short, you can roughly model me as having roughly trapezoidal&nbsp;<a href=\"https://en.wikipedia.org/wiki/Probability_density_function\"><u>probability density function</u></a> over developing AGI from now to 2100, with some long tail extending beyond that point. There is about 2x as much weight at the beginning of the distribution as there is at the end of the century. The long tail includes a) insufficient data/hardware/humans not smart enough to solve it yet, b) technological stagnation/hardware stagnation, and c) reasons it's hard that I haven't thought of. The microfoundation of the probability density function could be: a) exponentially increasing inputs to AGI, b) log returns to AGI development on the key inputs, c) pricing in some expected slowdown in the exponentially increasing inputs over time, and d) slow updating toward increased difficulty of the problem as the time goes on, but I stand by the distribution more than the microfoundation.</p><p><i>What do you think could substantially alter your views on these issues?</i></p><p>We don't know. Most of all we'd just like to see good arguments for specific quantitative answers to the stated questions. Some other thoughts:</p><ul><li>We like it when people state cleanly summarizable, deductively valid arguments and carefully investigate the premises leading to the conclusion (analytic philosopher style). See e.g.&nbsp;<a href=\"https://arxiv.org/abs/2206.13353\"><u>Carlsmith 2021</u></a>.</li><li>We also like it when people quantify their subjective probabilities explicitly. See e.g.&nbsp;<a href=\"https://smile.amazon.com/Superforecasting-Science-Prediction-Philip-Tetlock/dp/0804136718\"><u>Superforecasting</u></a> by Phil Tetlock.</li><li>We like a lot of the features described&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/SdQYLKzpDFQsdW9bn/features-that-make-a-report-especially-helpful-to-me\"><u>here</u></a> by Luke Muehlhauser, though they are not necessary to be persuasive.</li><li>We like it when people represent opposing points of view&nbsp;<a href=\"https://books.google.com/books?id=9SduAwAAQBAJ&amp;pg=PA34&amp;dq=%22You+should+mention+anything+you+have+learned+from+your+target.%22+%22Intuition+Pumps+and+Other+Tools+for+Thinking%22&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwi03JKkpPTiAhXSOn0KHSh1CYYQ6AEIKTAA#v=onepage&amp;q=%22You%20should%20mention%20anything%20you%20have%20learned%20from%20your%20target.%22%20%22Intuition%20Pumps%20and%20Other%20Tools%20for%20Thinking%22&amp;f=false\"><u>charitably</u></a>, and avoid appeals to authority.</li><li>We think it could be pretty persuasive to us if some (potentially small) group of relevant technical experts arrived at and explained quite different conclusions. It would be more likely to be persuasive if they showed signs of comfort thinking in terms of subjective probability and calibration. Ideally they would clearly explain the errors in the best arguments cited in this post.</li></ul><p>These are suggestions for how to be more likely to win the prize, but not requirements or guarantees.</p><p><i>Who do we have to convince in order to claim the prize?</i></p><p>Final decisions will be made at the discretion of the Future Fund. We plan to rely in part on the judgment of other researchers and people we trust when deciding what to seriously engage with. Probably, someone winning a large prize looks like them publishing their arguments, those arguments getting a lot of positive attention / being flagged to us by people we trust, us seriously engaging with those arguments (probably including talking to the authors), and then changing our minds.</p><p><i>Are these statistically significant probabilities grounded in detailed published models that are confirmed by strong empirical regularities that you're really confident in?</i></p><p>No. They are what we would consider fair betting odds.&nbsp;</p><p>This is a consequence of the conception of subjective probability that we are working with. As stated above in a footnote: \"We will pose many of these beliefs in terms of subjective probabilities, which represent betting odds that we consider fair in the sense that we'd be roughly indifferent between betting in favor of the relevant propositions at those odds or betting against them.\" For more on this conception of probability I recommend&nbsp;<a href=\"https://www.amazon.com/Logic-Decision-Richard-C-Jeffrey/dp/0226395820\"><i><u>The Logic of Decision</u></i></a> by Richard Jeffrey or&nbsp;<a href=\"https://plato.stanford.edu/entries/probability-interpret/#SubPro\"><u>this SEP entry</u></a>.</p><p>Applicants need not agree with or use our same conception of probability, but hopefully these paragraphs help them understand where we are coming from.</p><p><i>Why do the prizes only get awarded for large probability changes?</i></p><p>We think that large probability changes would have much clearer consequences for our work, and be much easier to recognize. We also think that aiming for changes of this size is less common and has higher expected upside, so we want to attract attention to it.</p><p><i>Why is the Future Fund judging this prize competition itself?</i></p><p>Our intent in judging the prize ourselves is not to suggest that our judgments should be treated as correct / authoritative by others. Instead, we're focused on our own probabilities because we think that is what will help us to learn as much as possible.</p><h2>Additional terms and conditions</h2><ul><li>Employees of FTX Foundation and contest organizers are not eligible to win prizes.</li><li>Entrants and Winners must be over the age of 18, or have parental consent.</li><li>By entering the contest, entrants agree to the Terms &amp; Conditions.</li><li>All taxes are the responsibility of the winners.</li><li>The legality of accepting the prize in his or her country is the responsibility of the winners. Sponsor may confirm the legality of sending prize money to winners who are residents of countries outside of the United States.</li><li>Winners will be notified in a future blogpost.</li><li>Winners grant to Sponsor the right to use their name and likeness for any purpose arising out of or related to the contest. Winners also grant to Sponsor a non-exclusive royalty-free license to reprint, publish and/or use the entry for any purpose arising out of related to the contest including linking to or re-publishing the work.</li><li>Entrants warrant that they are eligible to receive the prize money from any relevant employer or from a contract standpoint.</li><li>Entrants agree that FTX Philanthropy and its affiliates shall not be liable to entrants for any type of damages that arise out of or are related to the contest and/or the prizes.</li><li>By submitting an entry, entrant represents and warrants that, consistent with the terms of the Terms and Conditions: (a) the entry is entrant\u2019s original work; (b) entrant owns any copyright applicable to the entry; (c) the entry does not violate, in whole or in part, any existing copyright, trademark, patent or any other intellectual property right of any other person, organization or entity; (d) entrant has confirmed and is unaware of any contractual obligations entrant has which may be inconsistent with these Terms and Conditions and the rights entrant is required to have in the entry, including but not limited to any prohibitions, obligations or limitations arising from any current or former employment arrangement entrant may have; (e) entrant is not disclosing the confidential, trade secret or proprietary information of any other person or entity, including any obligation entrant may have in connection arising from any current or former employment, without authorization or a license; and (f) entrant has full power and all legal rights to submit an entry in full compliance with these Terms and Conditions.</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnww4u5bouauc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefww4u5bouauc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We will pose many of these beliefs in terms of subjective probabilities, which represent betting odds that we consider fair in the sense that we'd be roughly indifferent between betting in favor of the relevant propositions at those odds or betting against them.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntdlosh6fi9o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftdlosh6fi9o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For the sake of definiteness, these are Nick Beckstead\u2019s subjective probabilities, and they don\u2019t necessarily represent the Future Fund team as a whole or its funders.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkb1fhmtm4fs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkb1fhmtm4fs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It might be argued that this makes the prize encourage people to have views different from those presented here. This seems hard to avoid, since we are looking for information that changes our decisions, which requires changing our beliefs. People who hold views similar to ours can, however, win the $200k canonical reference prize.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0fpixu5jj1pb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0fpixu5jj1pb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A slight update/improvement on something that would have won the prize in the past (e.g.&nbsp;<a href=\"https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines\"><u>this update</u></a> by Ajeya Cotra) does not automatically qualify due to being better than the existing canonical reference. Roughly speaking, the update would need to be sufficiently large that the new content would be prize-worthy on its own.</p></div></li></ol>", "user": {"username": "Nick_Beckstead"}}, {"_id": "JKsga96CLcxnRjzFB", "title": "What are people's thoughts on working for DeepMind as a general software engineer?", "postedAt": "2022-09-23T17:13:47.081Z", "htmlBody": "<p>This opportunity: <a href=\"https://boards.greenhouse.io/deepmind/jobs/4445689\">https://boards.greenhouse.io/deepmind/jobs/4445689</a> matches my skillset. A recruiter has reached out. What would the value of working on helping DeepMind as a whole be? Any thoughts? Thanks.</p>", "user": {"username": "Max Pietsch"}}, {"_id": "SdjFiM5dj2zxvLwJF", "title": "Let's advertise infrastructure projects", "postedAt": "2022-09-23T14:01:11.448Z", "htmlBody": "<p>There are a number of EA infrastructure projects that provide free or cheap support to other effective altruists, two of which I'm involved with. Something they all seem to have in common is that a major limiting factor is spreading the word about the availability of the service or product <em>and keeping the word spread</em>.</p>\n<p>One can justify a handful of posts on here and in Facebook groups about any given project, but after that, unless it has changed in some way, further posts seem increasingly like spam - even when the project is still useful and still limited by EAs' remembering or ever knowing of it.</p>\n<p>So this is a quick post with a few related objectives:</p>\n<ol>\n<li>to call for better standardisation of norms for promoting such projects</li>\n<li>to open discussion of some kind of such advertising on this forum, since it's probably the single place where it would be most visible</li>\n<li>to discuss the possibility of advertising in other venues. Eg such projects could promote each other - although again a limiter is that not all such projects know about all the others</li>\n<li>partly to help with 3, partly for its own sake, to provide a list of such projects. I'll mention all the ones that occur to me below with the best description I can easily find. Please let me know any I've missed in the comments, and I'll edit them into this post</li>\n</ol>\n<p>The last could be quite subjective since anything could qualify as a resource and I don't want the list to become overwhelming, so I'll give some initial guidelines - feel free to persuade me they should be changed:</p>\n<ul>\n<li>The resource should be free to use, or at available at a substantial discount to relatively poor EAs</li>\n<li>It should be aimed specifically at EAs</li>\n<li>It should make the people using its lives better, not just 'enable them to do more good'</li>\n<li>It should be available to people across the world (ie. not just a local EA group)</li>\n<li>It should be a service or product that someone is putting ongoing work into (ie not just a list of tips, or Facebook/Discord/Slack groups with no purpose other than discussion of some EA subtopic)</li>\n</ul>\n<p>At the very least, hopefully this post can then become a useful reference for people looking to see what benefits the community can provide them.</p>\n<p>Coworking/socialising:</p>\n<ul>\n<li><a href=\"https://forum.effectivealtruism.org/posts/nxfhxwQg4HJ7KQz4A/ea-coworking-lounge-space-on-gather-town\">EA Gather Town</a> - An always-on virtual meeting place for coworking, connecting, and having both casual and impactful conversations</li>\n<li><a href=\"https://www.eavr.org/\">EAVR</a> - A community for people interested in Effective Altruism who use VR to connect and collaborate</li>\n<li><a href=\"https://www.effectivealtruismanywhere.org/\">EA Anywhere</a> - An online EA community for everyone</li>\n<li><a href=\"https://discord.gg/WQrvKzvA97\">EA coworking Discord</a> - A Discord server dedicated to online coworking</li>\n</ul>\n<p>Professional services:</p>\n<ul>\n<li><a href=\"https://altruistic.agency/\">Altruistic Agency</a> - provides free tech support and development to organisations</li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/ouFFDFmRNwKAc3RMD/i-m-offering-free-engineering-and-consultation-for-ea\">Tech support from Soof Golan</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/7knuuhSauLFbiTLWz/legal-support-for-ea-orgs-useful\">Legal advice from Tyrone Barugh</a> - a practice under consideration with the primary aim of providing legal support to EA orgs and individual EAs, with that practice probably being based in the UK.</li>\n<li><a href=\"https://www.eamentalhealthnavigator.com/\">EA mental health navigator</a> - aims to boost people's well-being by connecting them with mental health resources considered to be effective and to have the greatest likelihood of being helpful</li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/Grz9kyyAvvSdnn7Le/data-science-for-effective-good-call-for-projects-call-for\">SEADS</a> - Data Science services  to EA organizations</li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/6wqf96JJL5Njmsbxn/user-friendly-intro-post\">User-Friendly</a>  - an EA-aligned marketing agency</li>\n<li><a href=\"https://www.antientropy.org/\">Anti Entropy</a> - offers services related operations for EA organizations</li>\n<li><a href=\"https://arbresearch.com/\">Arb</a> - Our consulting work spans forecasting, machine learning, and epidemiology.\nWe do original research, evidence reviews, and large-scale data pipelines.</li>\n<li><a href=\"https://pineappleoperations.org/\">Pineapple Operations</a> - Maintains a public database of people who are seeking operations or Personal Assistant/Executive Assistant work (part- or full-time) within the next 6 months in the Effective Altruism ecosystem</li>\n</ul>\n<p>Coaching:</p>\n<ul>\n<li><a href=\"https://www.aisafetysupport.org/resources/shay\">AI Safety Support</a> - provides free health coaching to people working on AI safety</li>\n<li><a href=\"https://80000hours.org/speak-with-us/?int_campaign=2021-08__primary-navigation\">80,0000 Hours career coaching</a> - Speak with us for free about using your career to help solve one of the world\u2019s most pressing problems</li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/FkWHn6WaFGzrzqb9P/i-m-offering-free-coaching-for-software-developers-in-the-ea\">Yonatan Cale</a> - Coaching for software devs</li>\n<li><a href=\"https://www.trainingforgood.com/coaching\">Training for Good</a> - Our goal is to help you clarify your aims, reduce self-imposed friction, and improve your leadership.</li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/NTuTSEEn2Ka54nwFC/offering-faang-style-mock-interviews\">FAANG style mock interviews</a></li>\n</ul>\n<p>Financial and other material support:</p>\n<ul>\n<li><a href=\"https://ceealar.org/\">CEEALAR/formerly the EA hotel</a> - Provides free or subsidised serviced accommodation and board, and a moderate stipend for other living expenses.</li>\n<li><a href=\"https://www.nonlinear.org/productivity-fund.html\">Nonlinear productivity fund</a> - A \u200blow-barrier fund paying for productivity enhancing tools \u200bfor top longtermists. Supported services and products include Coaching, Therapy, Sleep coaching, Medication management , Personal Assistants, Research Assistants, Virtual Assistants, Tutors (e.g. ML, CS, language), Asana, FocusMate, Zapier, etc., Productivity apps, A/C, dishwashers, etc, SAD Lamps</li>\n<li><a href=\"https://funds.effectivealtruism.org/\">Effective Altruism Funds</a> - Whether an individual, organisation or other entity, we\u2019re eager to fund great ideas and great people.</li>\n<li><a href=\"https://www.nonlinear.org/\">Nonlinear fund</a> - We incubate longtermist nonprofits by connecting founders with ideas, funding, and mentorship</li>\n<li><a href=\"https://ftxfuturefund.org/\">FTX Future Fund</a> - Supports ambitious projects to improve humanity's long-term prospects</li>\n<li><a href=\"https://survivalandflourishing.fund/\">Survival and Flourishing Fund</a> - A \u201cvirtual fund\u201d: we organize application submission and evaluation processes to help donors decide where to make donations.</li>\n<li><a href=\"https://www.openphilanthropy.org/\">Open Philanthropy Project</a> - a research and grantmaking foundation that aims to share its findings openly</li>\n<li><a href=\"https://existence.org/\">Berkeley Existential Risk Initiative</a> - Supports university research groups working to reduce x-risk, by providing them with free services and support.</li>\n</ul>\n", "user": {"username": "Arepo"}}, {"_id": "SQBYHEWBTB2krA9kk", "title": "What We Owe The Future Updated Media List ", "postedAt": "2022-09-23T13:47:10.541Z", "htmlBody": "<p>This is my updated list as of 23 September 2022 of media around the launch of Will MacAskill's 'What We Owe The Future.' &nbsp;I have counted 21 podcast appearances, 32 articles and 8 other items. &nbsp;They are listed below with links and my brief personal comments.&nbsp;&nbsp;</p><p>There is much valuable material here. &nbsp;I have marked my favourites with one, two or three stars. &nbsp;My top recommendations are the Rob Wiblin and Ali Abdaal podcasts, the New Yorker article and the Richard Chappell review.&nbsp;</p><p>&nbsp;</p><h3>Podcast Appearances</h3><p><a href=\"https://80000hours.org/podcast/episodes/will-macaskill-what-we-owe-the-future/\">The 80,000 Hours Podcast with Rob Wiblin</a>. A warm and comprehensive three-hour discussion. ***</p><p><a href=\"https://www.samharris.org/podcasts/making-sense-episodes/292-how-much-does-the-future-matter\">Making Sense Podcast with Sam Harris.</a>&nbsp; Harris is strongly supportive, MacAskill particularly inspiring on the sweep of history. **</p><p><a href=\"https://www.preposterousuniverse.com/podcast/2022/08/15/207-william-macaskill-on-maximizing-good-in-the-present-and-future/\">Mindscape Podcast with Sean Carroll.</a> Carroll asks questions about utilitarianism, metaethics and population ethics and MacAskill answers well. &nbsp;*</p><p><a href=\"https://www.nytimes.com/2022/08/09/opinion/ezra-klein-podcast-will-macaskill.html\">The Ezra Klein Show Podcast</a>. A fine conversation on long-termism.&nbsp; Klein structures the discusion around \u2018Three simple sentences: Future people count. There could be a lot of them. And we can make their lives better.\u2019&nbsp;Good discussions about history - the contingent nature of the abolition of slavery and that certain times have plasticity. **</p><p><a href=\"https://tim.blog/2022/08/02/will-macaskill-what-we-owe-the-future/\">Tim Ferriss Podcast</a>.&nbsp; A lively discussion with much humour and several gems from MacAskill. Includes recommendation of Joseph Henrich's 'The Secret of Our Success.' &nbsp;**</p><p><a href=\"https://www.youtube.com/watch?v=YkdI8ztqWZc\">Deep Dive with Ali Abdaal Podcast</a>. A relaxed, friendly and wide-ranging three-hour conversation. Covers a lot of ground including EA psychology and MacAskill\u2019s work methods. This is a high-quality YouTube production as well as a podcast. ***</p><p><a href=\"https://conversationswithtyler.com/episodes/william-macaskill/\">Conversations with Tyler Podcast</a> .&nbsp; Tyler Cowan\u2019s questioning focuses on the limits of utilitarianism. *</p><p><a href=\"https://www.dwarkeshpatel.com/p/will-macaskill#details\">The Lunar Society Podcast with Dwarkesh Patel</a>.&nbsp; Focuses on the contingency of moral progress.</p><p><a href=\"https://www.undispatch.com/how-longtermism-is-shaping-foreign-policy-will-macaskill/\">Global Dispatch Podcast with Mark Goldberg.</a>&nbsp;Discussion on longtermism and the United Nations. &nbsp;Goldberg enthusiastic about the UN adopting some longtermist thinking. *</p><p><a href=\"https://modernwisdom.libsyn.com/512-will-macaskill\">Modern Wisdom Podcast with Chris Williamson</a>. &nbsp;An accessible discussion of longtermism.</p><p><a href=\"https://player.fm/series/conversations-with-coleman-members-exclusive/humanity-in-a-thousand-years-with-will-macaskill-s3-ep24\">Conversations with Coleman with Coleman Hughes</a>&nbsp;Includes population ethics, economic growth and moral change.</p><p><a href=\"https://dailystoic.com/will-macaskill/\">Daily Stoic Podcast with Ryan Holiday.</a>&nbsp;Mainly on altruism and moral change. &nbsp;</p><p><a href=\"https://think.kera.org/2022/08/19/a-philosopher-on-why-we-should-care-about-future-generations/\">Kera Think with Krys Boyd</a>. &nbsp;30 minutes conversation.</p><p><a href=\"https://freakonomics.com/podcast/a-million-year-view-on-morality/\">Freakanomics Podcast with Steve Levitt</a>. Discussion mainly on the economic themes in WWOTF, which MacAskill handles very well. *</p><p><a href=\"https://the1a.org/segments/philosopher-william-macaskill-on-how-to-do-the-most-good/\">1a Podcast on NPR</a>. &nbsp;David Gurn discussion on EA as a life changing philosophy. &nbsp;Includes comments from Sofya Lebedeva and Spencer Goldberg. &nbsp;</p><p><a href=\"https://www.tenpercent.com/podcast-episode/william-macaskill-491\">Ten Percent Happier Podcast with Dan Harris</a>. A warm discussion on donations, EA and longtermism.</p><p><a href=\"https://www.bigissue.com/culture/books/betterpod-william-macaskill-explains-what-we-owe-the-future/\">Big Issue Better Pod with Laura Kelly.</a>&nbsp;30 minutes conversation</p><p><a href=\"https://open.spotify.com/episode/3MhhpUIn9BDvJVDwA6UbYN#login\">Armchair Expert on Spotify with Dax Shephard.</a>&nbsp; Lively two hour conversation.</p><p><a href=\"https://podcasts.apple.com/gb/podcast/will-macaskill-on-longtermism-and-what-we-owe-the-future/id135066958?i=1000578403692\">Vox The Weeds Podcast with Bryan Walsh and Signal Samuel.</a>&nbsp; Fine discussion on limits of longtermism. &nbsp;Covered in Vox article below.&nbsp;</p><p><a href=\"https://www.econtalk.org/will-macaskill-on-longtermism-and-what-we-owe-the-future/\">Econtalk with Russ Roberts.</a>&nbsp;Very interesting, well-informed discussion, also video. *</p><p><a href=\"https://www.kcrw.com/culture/shows/life-examined/effective-altruism-longtermism-black-philanthropy/will-macaskill-what-we-owe-the-future-philosophy\">KCRW Life Examined with Jonathan Bastian.</a> &nbsp;Short interview.</p><p>There are transcripts for the podcasts by Wiblin, Carroll, Klein, Cowan, Patel, Goldberg, Levitt, Shephard and Roberts.&nbsp;</p><h3>&nbsp;</h3><h3>Articles and Book Reviews</h3><p><a href=\"https://www.newyorker.com/magazine/2022/08/15/the-reluctant-prophet-of-effective-altruism\">The New Yorker: The Reluctant Prophet of Effective Altruism by Gideon Lewis-Kraus</a>.&nbsp; A fine 10,000-word article profiling MacAskill and setting out the history of the EA movement. The author spent several days with his subject and covers MacAskill as an individual and the ideas and dynamics of the movement. MacAskill comments on the article in this&nbsp;<a href=\"https://twitter.com/willmacaskill/status/1556764231822970884\">Twitter Thread</a>. ***</p><p><a href=\"https://time.com/6204627/effective-altruism-longtermism-william-macaskill-interview/\">Time: Want to Do More Good? This Movement might have the Answer by Naina Bajekal </a>.&nbsp; Another beautifully written and inspiring profile of MacAskill and the EA movement.*</p><p><a href=\"https://www.vox.com/future-perfect/2022/8/8/23150496/effective-altruism-sam-bankman-fried-dustin-moskovitz-billionaire-philanthropy-crytocurrency\">Vox: How Effective Altruism Went from a Niche Movement to a Billion-Dollar Force by Dylan Matthews.</a>&nbsp; A well-informed and thoughtful article on EA\u2019s evolution by an EA insider. *</p><p><a href=\"https://www.wired.co.uk/article/will-macaskill-longtermism\">Wired: The Future Could be Blissful - If Human's Don't Go Extinct First.&nbsp;</a> Shorter interview with Will Macaskill by Matt Reynolds.</p><p><a href=\"https://www.nytimes.com/2022/08/05/opinion/the-case-for-longtermism.html\">New York Times: The Case for Longtermism by Will MacAskill</a> A Guest Essay adapted from the book.&nbsp;</p><p><a href=\"https://www.bbc.com/future/article/20220805-what-is-longtermism-and-why-does-it-matter\">BBC Futures: What is Longtermism and Why Does it Matter? by Will MacAskill.</a> &nbsp;Another essay based on the book.</p><p><a href=\"https://www.foreignaffairs.com/world/william-macaskill-beginning-history\">Foreign Affairs: The Beginning of History by Will MacAskill</a>&nbsp; A further essay based on the book.</p><p><a href=\"https://www.theatlantic.com/ideas/archive/2022/08/future-generations-climate-change-pandemics-ai/671148/\">The Atlantic: How Future Generations Will Remember Us by Will MacAskill.</a>&nbsp;And another essay, again telling the book's thesis in a different way.</p><p><a href=\"https://fivebooks.com/best-books/longtermism-will-macaskill/\">Five Books: The Best Books on Longtermism recommended by Will MacAskill.</a>&nbsp;He chooses 'Moral Capital', 'The Scout Mindset', 'The Precipice', 'Superforecasting' and 'The Life You Can Save.'</p><p><a href=\"https://www.npr.org/sections/goatsandsoda/2022/08/16/1114353811/how-can-we-help-humans-thrive-trillions-of-years-from-now-this-philosopher-has-a?t=1660750028939\">NPR: How Can We Help Humans Thrive Trillions of Years From Now?&nbsp;</a>Article and interview.</p><p><a href=\"https://bostonreview.net/articles/the-new-moral-mathematics/\">Boston Review: The New Moral Arithmetic.</a>&nbsp;Book review by Kieran Setiya, thoughtful, partly supportive and partly critical. &nbsp;<a href=\"https://twitter.com/willmacaskill/status/1559196018062786560\">MacAskill's Twitter response.</a></p><p><a href=\"https://www.theguardian.com/world/2022/aug/21/william-macaskill-what-we-owe-the-future-philosopher-interview\">The Guardian: William MacAskill.</a> &nbsp;Interview by Andrew Anthony.</p><p><a href=\"https://www.theguardian.com/books/2022/aug/25/what-we-owe-the-future-by-william-macaskill-review-a-thrilling-prescription-for-humanity\">The Guardian: Book Review</a>&nbsp;A fine and insightful review of WWOTF by Oliver Burkeman. 'The overall promise of this thrilling book is of a life both less burdened by ethical guilt \u2013 by beating yourself up over every choice of groceries or transportation \u2013 and much more effective at actually helping humanity. A life you truly enjoy, and in which you take that enjoyment seriously enough to want the same \u2013 or better \u2013 for billions more humans to come.' *</p><p><a href=\"https://astralcodexten.substack.com/p/book-review-what-we-owe-the-future\">Astral Codex Ten: Book Review by Scott Alexander&nbsp;</a> A critical but interesting review. 'If the point of publishing a book is to have a public relations campaign, Will MacAskill is the greatest English writer since Shakespeare.'&nbsp;</p><p><a href=\"https://www.vox.com/future-perfect/2022/8/24/23318033/effective-altruism-longtermism-givewell-will-macaskill\">Vox: Caring About The Future Doesn't Mean Ignoring The Present by Kelsey Piper</a>&nbsp; Article arguing that although WWOTF 'has put weird stuff front and centre' GiveWell data shows that 'effective altruism is growing on all fronts.'</p><p><a href=\"https://www.economist.com/the-economist-reads/2022/08/24/what-to-read-to-understand-effective-altruism\">The Economist: What to Read To Understand Effective Altruism</a>&nbsp; Article with recommended EA resources including two 80,000 hours podcasts, Singer's Drowning Child, MacAskill's WWOTF, Karnofsky's Most Important Century blog and Chivers's Rationalists Guide to the Galaxy.</p><p><a href=\"https://www.wired.com/story/longtermism-technology-evolution/\">Wired: If Humans Went Extinct Would A Similar Species Evolve?</a>&nbsp; Short extract from WWOTF. &nbsp;</p><p><a href=\"https://rychappell.substack.com/p/review-of-what-we-owe-the-future?utm_source=twitter&amp;sd=pf\">Richard Y Chappell Book Review</a>&nbsp;An excellent review from an EA-aligned moral philosopher. &nbsp;'<i>WWOTF</i> is to longtermism what <i>Animal Liberation </i>was to anti-speciesism. Targeted at a general audience, it advocates for an important kind of <i>moral circle expansion.' &nbsp;***</i></p><p><a href=\"https://www.telegraph.co.uk/news/2022/09/01/fact-had-one-pandemic-doesnt-make-another-less-likely/\">The Telegraph: The Fact We've Had One Pandemic Doesn't Make Another One Less Likely by Eleanor Mills</a>. &nbsp;Profile of MacAskill, EA and WWOTF.</p><p><a href=\"https://www.washingtonpost.com/outlook/2022/09/16/future-design-yahaba-politics/\">Washington Post: &nbsp;Want Politics to Be Better? Focus on Future Generations by Will MacAskill and Tyler John</a>&nbsp; Short article on considering future generations in politics.</p><p><a href=\"https://www.the-tls.co.uk/articles/what-we-owe-the-future-william-macaskill-book-review-regina-rini/\">TLS: An Effective Altruist? A Philosophers' Guide to the long Term Threats to Humanity by Regina Rini</a> Book Review.</p><p><a href=\"https://www.vox.com/future-perfect/23298870/effective-altruism-longtermism-will-macaskill-future\">Vox: Effective Altruism's Most Controversial Ideas by Sigal Samuel&nbsp;</a>&nbsp;Fine critical article. &nbsp;Longtermist crazy train with three stations: weak longtermism, strong longtermism and galaxy-brain longtermism. &nbsp;Arguments not to go beyond weak longtermism. Discussed on Vox Podcast above. *</p><p><a href=\"https://www.washingtonpost.com/opinions/2022/09/05/longtermism-philanthropy-altruism-risks/\">Washington Post: Why 'Longtermism\" isn't Ethically Sound by Christine Emba.</a> &nbsp;Short Article.</p><p><a href=\"https://forum.effectivealtruism.org/posts/9Y6Y6qoAigRC7A8eX/my-take-on-what-we-owe-the-future\">EA Forum: My Take on What We Owe The Future by Eli Lifland.</a>&nbsp; Argues that MacAskill underestimates AI risk and overestimates economic stagnation risk. *</p><p><a href=\"https://www.forbes.com/sites/briankateman/2022/09/06/optimistic-longtermism-is-terrible-for-animals/?sh=73ec358e2059\">Forbes: Optimistic Longtermism Is Terrible For Animals by Brian Kateman.</a> &nbsp;'If the human race creates more suffering than it alleviates, it would be a mistake to let it grow infinitely.'&nbsp;</p><p><a href=\"https://theconversation.com/what-do-we-owe-future-generations-and-what-can-we-do-to-make-their-world-a-better-place-189591\">The Conversation: What Do We Owe Future Generations? by Michael Noetel</a>.&nbsp;Book review.</p><p><a href=\"https://www.ft.com/content/091862f9-985f-4769-aa37-1aed32636329\">FT: Philosopher William MacAskill by Gillian Tett.</a> &nbsp;Warm article written around lunching with MacAskill.</p><p><a href=\"https://nymag.com/intelligencer/2022/08/why-effective-altruists-fear-the-ai-apocalypse.html\">New York Intelligencer: Why Effective Altruists Fear the AI Apocalypse by Eric Levitz</a>&nbsp; Writes up an interview with MacAskill. &nbsp;Good questions and answers, especially about EA and politics. *</p><p><a href=\"https://www.salon.com/2022/09/10/selling-longtermism-how-pr-and-marketing-drive-a-controversial-new-movement/\">Salon: Selling Longtermism - How PR and Marketing Drive a Controversial New Movement by Emile Torres. </a>Interesting critical article arguing that a moderate form of longtermism is being presented for PR reasons. *</p><p><a href=\"https://www.fastcompany.com/90784622/the-truth-about-elon-musk-sam-bankman-fried-and-effective-altruism\">Fast Company: The Truth about Elon Musk, Sam Bankman-Fried and Effective Altruism by Talib Visram</a>. A partly negative article of EA, that draws on a call with MacAskill.</p><p><a href=\"https://bigthink.com/the-future/spaceguard-what-we-owe-the-future/\">Big Think: What NASA's Spaceguard can Teach us about our Uncertain Future by Will MacAskill</a>. An article based on the book section praising Spaceguard as an example of reducing existential risks.</p><p><a href=\"https://www.wsj.com/articles/what-we-owe-the-future-review-a-technocrats-tomorrow-11661544593\">Wall Street Journal. &nbsp;What We Owe The Future Review - A Technocrats Dream by Barton Swaim</a>. &nbsp;Book review.</p><p>&nbsp;</p><h3>Other Items</h3><p><a href=\"https://www.reddit.com/r/IAmA/comments/wro991/im_will_macaskill_a_philosophy_professor_at/\">Redditt: Will MacAskill Ask Me Anything&nbsp;</a>&nbsp;Thirty answers to online questions, some answers substantial and interesting. *</p><p><a href=\"https://www.youtube.com/watch?v=W93XyXHI8Nw\">Kurzgesagt: Is Civilisation on the Brink of Collapse?</a> Animation.</p><p><a href=\"https://www.youtube.com/watch?v=_uV3wP5z51U\">Rational Animations: Can We Make The Future A Million Years From Now Go Better?</a> Animation.</p><p><a href=\"https://www.youtube.com/watch?v=r6sa_fWQB_4\">Primer: How Many People Might Every Exist Calculated.</a> &nbsp;Animation.</p><p><a href=\"https://www.youtube.com/watch?v=Zi5gD9Mh29A&amp;t=3s\">Ali Abdaal: The Most Impactful Book I Have Ever Read</a>. &nbsp;Fine YouTube video review and recommendation for WWOTF. *</p><p><a href=\"https://www.youtube.com/watch?v=orRsWJvzbyQ\">Vice News: Can This Book Stop Human Extinction?</a>&nbsp; Brief YouTube Feature and interview with MacAskill&nbsp;</p><p><a href=\"https://www.basicbooks.com/titles/william-macaskill/what-we-owe-the-future/9781541618633/\">Basic Books Page for WWOTF</a>. Includes short video.</p><p><a href=\"https://www.youtube.com/watch?v=KGR5QWuhd98\">Brandon's Bookshelf Review of WWOTF</a>. Good YouTube review.</p><p>&nbsp;</p><h3>Links</h3><p>There are further resources at the <a href=\"https://whatweowethefuture.com/uk/\">What We Owe The Future Website</a>.</p><p>My blog includes my <a href=\"https://jamesaitchison.co.uk/?p=1712\">book review</a> of WWOTF and my list of <a href=\"https://jamesaitchison.co.uk/?p=1583\">media links</a>.</p>", "user": {"username": "James Aitchison"}}, {"_id": "tt566KyF2tP8fPgoY", "title": "LW4EA: Incorrect hypotheses point to correct observations", "postedAt": "2022-09-23T12:52:13.985Z", "htmlBody": "<p>Written by LW user <a href=\"https://www.lesswrong.com/users/kaj_sotala\">Kaj_Sotala</a>.</p><p>This is part of&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/low-commitment-less-wrong-book-club\">LessWrong for EA</a>, a LessWrong repost &amp; low-commitment discussion group (inspired by&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/qtDZHBx6LpAR7ejDD/what-standalone-lesswrong-posts-would-you-recommend-to-most?commentId=ykgk75KtczWSyuLgs#comments\">this comment</a>). Each week I will revive a highly upvoted, EA-relevant post from the LessWrong Archives, more or less at random</p><p>Excerpt from the post:</p><blockquote><p><strong>1. The Consciousness Researcher and Out-Of-Body Experiences</strong></p><p>In his book&nbsp;<a href=\"https://smile.amazon.com/Consciousness-Brain-Deciphering-Codes-Thoughts-ebook/dp/B00DMCVXO0/\"><i>Consciousness and the Brain</i></a>, cognitive neuroscientist Stansilas Dehaene writes about scientifically investigating people\u2019s reports of their out-of-body experiences:</p><blockquote><p><i>\u2026 the Swiss neurologist Olaf Blanke[ did a] beautiful series of experiments on out-of-body experiences. Surgery patients occasionally report leaving their bodies during anesthesia. They describe an irrepressible feeling of hovering at the ceiling and even looking down at their inert body from up there. [...]</i></p></blockquote><blockquote><p><i>What kind of brain representation, Blanke asked, underlies our adoption of a specific point of view on the external world? How does the brain assess the body\u2019s location? After investigating many neurological and surgery patients, Blanke discovered that a cortical region in the right temporoparietal junction, when impaired or electrically perturbed, repeatedly caused a sensation of out-of-body transportation. This region is situated in a high-level zone where multiple signals converge: those arising from vision; from the somatosensory and kinesthetic systems (our brain\u2019s map of bodily touch, muscular, and action signals); and from the vestibular system (the biological inertial platform, located in our inner ear, which monitors our head movements). By piecing together these various clues, the brain generates an integrated representation of the body\u2019s location relative to its environment. However, this process can go awry if the signals disagree or become ambiguous as a result of brain damage. Out-of-body flight \u201creally\u201d happens, then\u2014it is a real physical event, but only in the patient\u2019s brain and, as a result, in his subjective experience. The out-of-body state is, by and large, an exacerbated form of the dizziness that we all experience when our vision disagrees with our vestibular system, as on a rocking boat.</i></p></blockquote><blockquote><p><i>Blanke went on to show that any human can leave her body: he created just the right amount of stimulation, via synchronized but delocalized visual and touch signals, to elicit an out-of-body experience in the normal brain. Using a clever robot, he even managed to re-create the illusion in a magnetic resonance imager. And while the scanned person experienced the illusion, her brain lit up in the temporoparietal junction\u2014very close to where the patient\u2019s lesions were located.</i></p></blockquote><blockquote><p><i>We still do not know exactly how this region works to generate a feeling of self-location. Still, the amazing story of how the out-of-body state moved from parapsychological curiosity to mainstream neuroscience gives a message of hope. Even outlandish subjective phenomena can be traced back to their neural origins. The key is to treat such introspections with just the right amount of seriousness. They do not give direct insights into our brain\u2019s inner mechanisms; rather, they constitute the raw material on which a solid science of consciousness can be properly founded.</i></p></blockquote><p>The naive hypotheses that out-of-body experiences represented the spirit genuinely leaving the body, were incorrect. But they were still pointing to a real observation, namely that there are conditions which create a subjective experience of leaving the body. That observation could then be investigated through scientific means. (<a href=\"https://www.lesswrong.com/posts/MPj7t2w3nk4s9EYYh/incorrect-hypotheses-point-to-correct-observations\">Full Post on LW</a>)</p></blockquote><p>Please feel free to,</p><ul><li>Discuss in the comments</li><li>Subscribe to the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/low-commitment-less-wrong-book-club\">LessWrong for EA tag</a>&nbsp;to be notified of future posts</li><li>Tag other LessWrong reposts with&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/low-commitment-less-wrong-book-club\">LessWrong for EA</a>.</li><li>Recommend additional posts</li></ul>", "user": {"username": "captainjc"}}, {"_id": "noDYmqoDxYk5TXoNm", "title": "$5k challenge to quantify the impact of 80,000 hours' top career paths", "postedAt": "2022-09-23T11:32:09.873Z", "htmlBody": "<h2><strong>Motivation</strong></h2><p>80,000 hours has identified a number of promising career paths. They have a fair amount of analysis behind their recommendations, and in particular, they have a list of&nbsp;<a href=\"https://80000hours.org/career-reviews/#our-priority-paths\"><u>top ten priority paths.</u></a>&nbsp;</p><p>However, 80,000 hours doesn\u2019t quite<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc0a3zqtzcik\"><sup><a href=\"#fnc0a3zqtzcik\">[1]</a></sup></span>&nbsp;have quantitative estimates of these paths' value. Although their usefulness would not be guaranteed, quantitative estimates could make it clearer:</p><ul><li>how valuable their top career paths are relative to each other</li><li>how valuable their top career paths are relative to options further down their list</li><li>at which level of personal fit one should switch between different career paths<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefugpn6numihp\"><sup><a href=\"#fnugpn6numihp\">[2]</a></sup></span></li><li>where the expected impact is coming from, and which variables we are most uncertain about</li><li>eventually, whether certain opportunities are valuable in themselves or for the value of information or career capital that they provide</li><li>etc.</li></ul><p><strong>The Prize</strong></p><p>Following up on the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ZrWuy2oAxa6Yh3eAw/usd1-000-squiggle-experimentation-challenge\"><u>$1,000 Squiggle Experimentation Challenge</u></a> and the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/8Nwy3tX2WnDDSTRoi/announcing-the-forecasting-innovation-prize\"><u>Forecasting Innovation Prize</u></a> we are offering a prize of $5k for&nbsp;quantitative estimates of the value of&nbsp;<a href=\"https://80000hours.org/career-reviews/#our-priority-paths\"><u>80,000 hours' top 10 career paths</u></a>.&nbsp;</p><h2><strong>Rules</strong></h2><p><strong>Step 1:</strong> Make a public post online between now and December 1, 2022. Posts on the EA Forum (link posts are fine) are encouraged.<br><br><strong>Step 2:&nbsp;</strong>Complete&nbsp;<a href=\"https://airtable.com/shrFX656rcOXaDioh\"><u>this submission form.</u></a></p><p><strong>Further details</strong></p><ul><li>Participants can use units or strategies of their choice\u2014these might be QALYs, percentage points of reduction in existential risk,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xmmqDdGqNZq5RELer/shallow-evaluations-of-longtermist-organizations#Notes\"><u>basis points of the future</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/cKPkimztzKoCkZ75r/how-many-ea-2021-usds-would-you-trade-off-against-a-0-01\"><u>basis points of existential risk reduced</u></a>, career-dependent units, etc. Contestants could also use some other method, like&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/hrdxf5qdKmCZNWTvs/valuing-research-works-by-eliciting-comparisons-from-ea\"><u>relative values</u></a>, estimating proxies, or some original option.</li><li>We are specifically looking for quantitative estimates that attempt to estimate some magnitude reasonably close to the real world, similar to the units above<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefp8czsamuy0k\"><sup><a href=\"#fnp8czsamuy0k\">[3]</a></sup></span>. So for example, assigning valuations from 0 to 5 stars would not fulfil the requirements of the contest, but estimates in terms of the units above would qualify.</li><li>Participants are free to estimate the value of one, several, or all ten career paths.</li><li>Participants are free to use whatever tool or language they want to produce these estimates. Some possible tooling might be: Excel,&nbsp;<a href=\"https://www.squiggle-language.com/\"><u>Squiggle</u></a>,&nbsp;<a href=\"https://getguesstimate.com/\"><u>Guesstimate</u></a>, probabilistic languages or libraries (e.g., Turing.jl, PyMC3, Stan),&nbsp;<a href=\"https://causal.app/\"><u>Causal</u></a>, working directly in a popular programming language, etc.</li><li>Participants&nbsp;<i>can</i> provide point estimates of impact, but they are encouraged to provide their estimates as distributions instead.&nbsp;</li><li>Participants are free to estimate the impact of a marginal person, of a marginal person with a good fit, the average value, etc. Participants are welcome to provide both average and marginal value\u2014for example, they could provide a function which provides an estimate of marginal value at different levels of labor and capital.</li></ul><p>We provide some examples of possible rough submissions in an appendix. We are also happy to comment on estimation strategies: feel free to leave a comment on this post or to send a message to Nu\u00f1o Sempere using the EA forum message functionality.</p><h2><strong>Judging</strong></h2><p>The judges will be Nu\u00f1o Sempere, Eli Lifland, Alex Lawsen and Sam Nolan. These judges will judge on their personal capacities, and their stances do not represent their organizations.</p><p>Judges will estimate the quality and value of the entries, and we will distribute the prize amount of $5k<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8h8jcppir5\"><sup><a href=\"#fn8h8jcppir5\">[4]</a></sup></span>&nbsp;in proportion to an equally weighted aggregate of those subjective estimates<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefptr04ewkhn\"><sup><a href=\"#fnptr04ewkhn\">[5]</a></sup></span>.</p><p>To reduce our operational burden, we are looking to send out around three to five prizes. If there are more than five submissions, we plan to implement a lottery system. For example, a participant who would have won $100 would instead get a 10% chance of receiving $1k.</p><h2><strong>Acknowledgements</strong></h2><figure class=\"image image_resized\" style=\"width:19.9%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/302723968e676b518bf73d550d20c73076f7bb23db0f31e3.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/302723968e676b518bf73d550d20c73076f7bb23db0f31e3.png/w_97 97w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/302723968e676b518bf73d550d20c73076f7bb23db0f31e3.png/w_177 177w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/302723968e676b518bf73d550d20c73076f7bb23db0f31e3.png/w_257 257w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/302723968e676b518bf73d550d20c73076f7bb23db0f31e3.png/w_337 337w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/302723968e676b518bf73d550d20c73076f7bb23db0f31e3.png/w_417 417w\"></figure><p>This contest is a project of the Quantified Uncertainty Research Institute, which is providing the contest funds and administration. Thanks in advance to Eli Lifland, Alex Lawsen and Sam Nolan for their good judgments. Thanks to Ozzie Gooen for comments and suggestions.</p><h2><strong>Appendix: Example models</strong></h2><h3>Example I: \"<a href=\"https://80000hours.org/career-reviews/founder-impactful-organisations/\"><u>Founder of new projects tackling top problems</u></a>\"</h3><p>The following is a crude example estimate for the career path of&nbsp;<a href=\"https://80000hours.org/career-reviews/founder-impactful-organisations/\"><u>Founder of new projects tackling top problems</u></a>, written in&nbsp;<a href=\"https://www.squiggle-language.com/playground#code=eNq1lN9P2zAQx%2F8Vq0%2FplCZuuwCqtIdKMLQfXceKNqGFB5M6iYdjZ%2FYFGhD%2F%2B85OC90QUB72kF%2F23dd3n7vLbc%2BW%2BnrRVBUzbW8CpuGhXzpaCtBmsyKUAMHk4ncjikLyBRihit6kF8dkXnPDQGjFJMlKJiVXBbepwneV8Xk%2BBWDoropT01iYquUxB8DP941a4oO8IxccWJCEZEz7BAUhoBFNCGhCo3FIhv0HrbXrVM1NwZS48efO89OSHxuNehuxIUW%2FB7VxJ3bQiaUKl48siIoBJzonoqpZBqlqOTOy3ZZ%2BiLFaBQm9dDoJpZchGdHua0hnIUlm3Qa%2B%2FqTRHqZCo5G%2F0%2BQcTyzEFT8Uhmcg2%2B9MNpjJyfTzmf3KzaGWkhk8YBgHwz3qdEb7dB06u%2BSK5EZXxOqKkwWryBctmSJ8Hb2NUqW34vXis0aCqKXgpos7VcSFNfThIgL3RBQH%2FomgDvyp4xG%2B4m2T4nno%2Fd6GzhbvI%2FQ8T1Xf07vipiVIvChlS4SyNSa3JBfthJQAtZ3Eca5NU0U8zzFpzJ5J7CNhqwijjWttwcbHN9XHUSNOV2%2BLs7LOTj7F99aKWzsQdsAGmVa%2FGpW53AY6H1T3qWGD2ZLVkrd%2FpYvcu1onqZIi54gJYW%2BXtGPirfaxcl26m66YFoXhBYJNFWhgcl2sIwWG1xhWY2wp6g%2FKVw%2BVdu3xN47lzk3srZ9uRr%2F9ckt5s%2Beawxs8AcnvPQL8LyVyLaAkFVuJqqmwIhAshYXQrfSRDpZcZWj2TRTl9tYucH%2Bg8oytEPI2uv%2FAGYInSYduEGb9VwGH4Bnk7qfk9Nxwa5wh%2FFnejzIxOCQ4TZYrK9wQuM6Ekm%2FwkpIbHr1UMggeFW3rp3cobC1Zm6pb4ozRfOc6TNYer%2FFBlzu8end%2FAB%2BtOwA%3D\"><u>Squiggle</u></a>.</p><pre><code>// Operational challenges\nchanceOfAttainingTrustAndGettingFunding = beta(5, 30) // t(0.05 to 0.3, 1)\nchanceOfGettingAnOrganizationOfTheGround = beta(10, 10) // t(0.3 to 0.8, 1)\n\n// Estimate of impact\nyearlyOrganizationFunding = mx(50k to 500k, 200k to 10M, 5M to 50M, [0.65, 0.25, 0.05])\ngiveDirectlyValueOfQALYsPerDollar = 1/(160 to 2700) // taken from some Sam Nolan\u2019s estimates: &lt;https://observablehq.com/@hazelfire/givewells-givedirectly-cost-effectiveness-analysis&gt;\norganizationValueMultiplier = mx(\n &nbsp;[0.1 to 1, 1 to 8, 8 to 80, 80 to 320, 320 to 500k],\n &nbsp;[4, 8, 4, 2, 1]\n)\n// very roughly inspired by: https://forum.effectivealtruism.org/posts/GzmJ2uiTx4gYhpcQK/effectiveness-is-a-conjunction-of-multipliers\nshapleyMultiplier = 0.2 to 0.5\nlifetimeOfOrganization = mx(2 to 7, 5 to 50)\n \n// Aggregate\ntotalValueOfEntrepeneurshipInQALYs = chanceOfAttainingTrustAndGettingFunding *\n &nbsp;chanceOfGettingAnOrganizationOfTheGround *\n &nbsp;yearlyOrganizationFunding *\n &nbsp;giveDirectlyValueOfQALYsPerDollar *\n &nbsp;organizationValueMultiplier *\n &nbsp;lifetimeOfOrganization *\n &nbsp;shapleyMultiplier\n \n// Aggregate with maximums\nt(dist, max) = truncateRight(dist, max)\ntotalValueOfEntrepeneurshipInQALYsWithMaxs =\n &nbsp;chanceOfAttainingTrustAndGettingFunding *\n &nbsp;chanceOfGettingAnOrganizationOfTheGround *\n &nbsp;t(yearlyOrganizationFunding, 500M) *\n &nbsp;giveDirectlyValueOfQALYsPerDollar *\n &nbsp;t(organizationValueMultiplier, 10M) * // overall estimate really sensitive to the maximum here.\n &nbsp;lifetimeOfOrganization *\n &nbsp;t(shapleyMultiplier, 1)\n\n // Display\n{ \n &nbsp;&nbsp;&nbsp;totalValueOfEntrepeneurshipInQALYsWithMaxs: \n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;totalValueOfEntrepeneurshipInQALYsWithMaxs \n} </code></pre><p>Alone, the estimate might be too obscure, so it would be better if it were accompanied by some explanation about the estimation strategy it is using. So, its estimation strategy is:</p><ul><li>To estimate the chance of getting funding and then getting an organization off the ground<ul><li>This is based on subjective guesses. Perhaps Charity Entrepreneurship, or EA funds if it kept data, could have better estimates</li></ul></li><li>To estimate the value that an organization produces. This is the weakest part of the model, and it would be better if it were based on specific steps. Instead, we are using more of a \"black box\" model, and estimating:<ul><li>The funding that the organization would receive</li><li>The QALYs per dollar that a reference organization\u2014GiveDirectly\u2014produces, taken from&nbsp;<a href=\"https://observablehq.com/@hazelfire/givewells-givedirectly-cost-effectiveness-analysis\"><u>Sam Nolan\u2019s estimate thereof</u></a>.</li><li>The advantage over GiveDirectly that the new organization would have. We are getting this estimate from&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/GzmJ2uiTx4gYhpcQK/effectiveness-is-a-conjunction-of-multipliers\"><u>this EA forum post</u></a></li></ul></li><li>To estimate some other factors to go from the above to the total output, again based on pretty subjective estimates:<ul><li>The lifetime of the organization</li><li>The \"Shapley multiplier\" penalizes efforts which require more people. In this case, we are saying that the founder gets between 20% and 50% of the impact.</li></ul></li></ul><p>We also have to take care that not only the 90% confidence interval, but also the overall shape of the estimates was correct. For this reason, we have a step where we truncate some of them.</p><p>As mentioned, a key input of the model is the multiplier of impact over GiveDirectly, but this is based on black box reasoning. This could be a possible point of improvement. For example, we could improve it with an estimate of how many QALYs, or what percentage of the future is an speculative area like AI safety research worth.</p><h3>Example II: Value of global health charities</h3><p>There are various distributional models of global health charities in the EA forum that participants may want to take some inspiration from, e.g.:</p><ul><li>Dan Wahl\u2019s estimate of the cost-effectiveness<a href=\"https://danwahl.net/blog/leep-cea\"><u> of LEEP</u></a></li><li>Sam Nolan\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/users/hazelfire\"><u>cost-effectiveness models</u></a></li></ul><p>The advantage of these is that they can be pretty clean. The disadvantage is that they come from a different cause area.</p><h3>Example III: Value of the Centre for the Governance of AI</h3><p><a href=\"https://forum.effectivealtruism.org/posts/EPhDMkovGquHtFq3h/an-experiment-eliciting-relative-estimates-for-open#A_concrete_example\"><u>Here</u></a>, I give an estimate for the value of the Centre for the Governance of AI (GovAI) in terms of basis points of existential risk reduced. It might serve as a source of inspiration. One disadvantage is that it only considers one particular pathway to impact that GovAI might have, and it doesn't consider other pathways that might be more important\u2014e.g., field-building.</p><h3>Example IV: Value of ALLFED</h3><p>Historically, one of the few longtermist organizations which has made an attempt to estimate their own impact quantitatively is ALLFED. A past estimate of theirs can be seen&nbsp;<a href=\"https://www.getguesstimate.com/models/9782\"><u>here</u></a>. My sense is that the numeric estimates might have been on the optimistic side (some alternative numbers&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xmmqDdGqNZq5RELer/shallow-evaluations-of-longtermist-organizations#Disagreements_and_Uncertainties\"><u>here</u></a>). But the estimation strategy of dividing their influence and impact depending on different steps might be something to take inspiration from.about<br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc0a3zqtzcik\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc0a3zqtzcik\">^</a></strong></sup></span><div class=\"footnote-content\"><p>80,000 hours, when thinking abou their own impaabouct, internally use \"discounted impact-adjusted peak year\" (DIPY). But this seems like a fairly coarse unit.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnugpn6numihp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefugpn6numihp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is actually more nuanced. There might be some frustration about people quickly/na\u00efvely jumping to whatever cause or sub-cause has the best apparent marginal value at each point in time rather than committing to something. But this might be counterproductive if people have more impact staying in one place, or if impact is a combination of people working on different areas. For a specific example, suppose that impact is a <a href=\"https://en.wikipedia.org/wiki/Cobb%E2%80%93Douglas_production_function\">Cobb\u2013Douglas</a> function of work in different areas, and that there is some coordination inefficiencies. Then focusing on attaining the optimal proportion of people in each area might be better than aiming to estimate marginal values through time.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnp8czsamuy0k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefp8czsamuy0k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The criteria isn't exactly to have a unit such that 2x on that unit is twice as better. For example, percentage reductions of existential/catastrophic risk in the presence of several such risks aren't additive, but we would accept such estimates. Similarly, relative values can only be translated to magnitudes in an \"additive\" unit with a bit of work, but we would also accept such estimates.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8h8jcppir5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8h8jcppir5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Having a fixed pot is slightly less elegant than deciding beforehand on an amount to reward for a given level of quality, but it comes with an added operations burden/uncertainty.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnptr04ewkhn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefptr04ewkhn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For example, if we get two submissions and we estimate the first one to be twice as valuable as the second one, the first submission would receive $3.33k and the second submission would receive 1.66k. Instead, if the first submission's individual estimates were estimated to be twice as valuable, but also were twice as many in number as those of the second submission, the first one would receive $4k and the second one would receive $1k.</p></div></li></ol>", "user": {"username": "NunoSempere"}}, {"_id": "uSH6DqjzggAYQGjxm", "title": "The Rival AI Deployment Problem: a Pre-deployment Agreement as the least-bad response", "postedAt": "2022-09-23T09:28:45.352Z", "htmlBody": "<h1><u>Introduction: The rival AI deployment problem</u></h1><p>Imagine an actor is faced with highly convincing evidence that with high probability (over 75%) a rival actor will be capable within two years of deploying advanced AI. Assume that they are concerned that such deployment might threaten their values or interests. What could the first actor do? Let us call this the \u2018<strong>rival AI deployment problem\u2019.</strong> Three responses present themselves: acquiescence, an agreement&nbsp;<i>before</i> deployment, and the threat of coercive action.</p><p>Acquiescence is inaction, and acceptance that the rival actor will deploy. It does not risk conflict, but does risk unilateral deployment, and therefore suboptimal safety precautions, misuse or value lock-in. An agreement before deployment (such as a treaty between states) would be an agreement on when and how advanced AI could be developed and deployed: for example, requirements on alignment and safety tests, and restrictions on uses/goals. We can think of this as a \u2018Short Reflection\u2019 - a negotiation on what uses/goals major states can agree to give advanced AI. This avoids unilateral deployment and conflict, but it may be difficult for rival actors to agree, and any agreement faces the credible commitment problem of sufficiently reassuring the actors that the agreement is being followed. Threat of coercive action involves threatening the rival actor with setbacks (such as state sanctions or cyberattacks) to delay or deter the development program. It is unilaterally achievable, but risks unintended escalation and conflict. All three responses have positives and negatives. However, I will suggest a pre-deployment agreement may be the least-bad option.</p><p>The rival AI deployment problem can be thought of as the flipside of (or an addendum to) what Karnofsky and Muehlhauser call the \u2018<a href=\"https://forum.effectivealtruism.org/posts/zGiD94SHwQ9MwPyfW/important-actionable-research-questions-for-the-most#How_do_we_hope_an_AI_lab___or_government___would_handle_various_hypothetical_situations_in_which_they_are_nearing_the_development_of_transformative_AI__and_what_does_that_mean_for_what_they_should_be_doing_today_\"><u>AI deployment problem</u></a>\u2019: \u201cHow do we hope an AI lab - or government - would handle various hypothetical situations in which they are nearing the development of transformative AI?\u201d. Similarly, OpenAI made a commitment in its&nbsp;<a href=\"https://openai.com/charter/\"><u>Charter</u></a> to \u201cstop competing with and start assisting\u201d any project that \u201ccomes close to building\u201d advanced AI for example with \u201ca better-than-even chance of success in the next two years\u201d. The Short Reflection can be thought of as an addendum to the Long Reflection, as suggested by&nbsp;<a href=\"https://whatweowethefuture.com/uk/\"><u>MacAskill</u></a> and&nbsp;<a href=\"https://theprecipice.com/\"><u>Ord</u></a>.</p><h2>Four assumptions</h2><p>I make four assumptions.&nbsp;</p><p>First, I roughly assume a \u2018classic\u2019 scenario of discontinuous deployment of a singular AGI system, of the type discussed in Life 3.0, Superintelligence and Yudkowsky\u2019s writings. Personally, more of a continuous Christiano-style take-off seems more plausible to me, and more of a distributed Drexler-style Comprehensive AI Services seems preferable to me. But the discontinuous, singular scenario makes the tensions sharper and clearer, so that is what I will use.&nbsp;</p><p>Second, I roughly assume that states are the key players, as opposed to sustained academic or corporate control over an advanced AI development and/or deployment project. Personally, state control of this strategically important technology/project seems more plausible to me. In any case, state control again makes the tensions sharper and clearer. I distinguish between development and deployment. By \u2018deployment\u2019 I mean something like \u2018use in a way that affects the world\u2019 materially, economically, or politically. This includes both \u2018starting a training run that will likely result in advanced AI\u2019 and \u2018releasing some system from a closed-off environment or implementing its recommendations\u2019.</p><p>Third, I assume that some states may be concerned about deployment by a rival state. They might not necessarily be concerned. Almost all current policymakers are not particularly aware of, or concerned about, advanced AI. This is quite reasonable, given their context and the significant uncertainty over the future development of advanced AI. However, over the coming decades it could be that some states could come to view the deployment by some other states of sufficiently advanced AI as threatening, for example to their economies, values or prestige. Their interests, sovereignty and status are typically key concerns for major states.</p><p>Fourth, I assume states have knowledge of their rival\u2019s program. Of course, states may lack crucial information - they may not realise how close their rivals are. There are many cases of states having inaccurate information. The USA was&nbsp;<a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/2021/03/International-Control-of-Powerful-Technology-Lessons-from-the-Baruch-Plan-Zaidi-Dafoe-2021.pdf\"><u>surprised</u></a> by how quickly the USSR gained a nuclear weapon in the 1940s, and the West\u2019s failure to discover the Soviet biological weapons program in the 1970s and 80s has<a href=\"https://80000hours.org/podcast/episodes/andy-weber-rendering-bioweapons-obsolete/\">&nbsp;<u>been described</u></a> as the greatest intelligence failure of the Cold War. On the other hand, the US&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/cXBznkfoPJAjacFoT/are-you-really-in-a-race-the-cautionary-tales-of-szilard-and\"><u>overestimated</u></a> how close the Nazis were to a bomb, and how many missiles the USSR had in the late 1950s (the imagined \u2018missile gap\u2019). This type of intelligence failure may be less likely now, given improvements in cyber espionage. Nevertheless, the USA seemed surprised in recent years by Chinese hypersonic missiles and silo-building. An AI development project could be hidden in data centres/companies running many other systems, progress could be quicker than intelligence communities can operate, and intelligence analysts may overconfidently rely on cyberespionage or not pick up on crucial signs due to lack of specialist knowledge.&nbsp;</p><p>In any case, I will assume for the sake of this analysis a fairly discontinuous scenario with states leading development, and that other states are concerned about their rivals and know with high confidence the state of their rival\u2019s development program. What are their options - what are possible responses to the rival AI deployment problem?</p><p>&nbsp;</p><h1>Responses</h1><h2><strong>1. Acquiescence</strong></h2><p>&nbsp;</p><p>The first response is acceptance that the other side will deploy their capability, that there is nothing much that one can do. Perhaps one will attempt to persuade or cajole the deployer to not deploy, or to limit how it deploys, but not take much action beyond that. This is likely to be the response of non-\u2019Great Power\u2019 countries - low and middle-income countries, etc. Does this also apply to \u2018great powers\u2019 or peer competitors - for example the P5?&nbsp;</p><p>This has typically been thought of as unlikely \u2013 major states are not generally comfortable with any (perceived) threat to their values or interests. They often (over)react to perceived threats to their status, security or economies. This may apply even more so for great powers and hegemons. For example, Allison\u2019s<a href=\"https://en.wikipedia.org/wiki/Thucydides_Trap\">&nbsp;<u>The Thucydides Trap</u></a> argues that challenges to, and transfers of, hegemony are often characterized by conflict. He argues that 12/16 cases ended in war \u2013 from Sparta and Athens, to Imperial Germany and the British Empire. A leading state today is unlikely to feel neutral about the deployment of advanced AI.</p><p>However, there are a few historical cases of the acquiescence scenario occurring. Most prominently is the USSR over the 1989-1991 period, when it accepted the end of its existence without a major external or civil war. Other examples of states accepting the end of their survival as a unified entity include the postwar decolonisations \u2013 many of which did not feature major wars from the imperial power. The British, French and Dutch Empires essentially ceased to exist. Examples of states accepting the transfer of hegemony include the transfer from British to US hegemony in the mid 20<sup>th</sup> century (from Pax Britannica to Pax Americana). At a more granular level, there are also examples of states accepting their rivals acquiring significant new technologies and weapons capabilities without conflict. For example, the USA did not strike the USSR in the 1940s and 1950s in the two periods when it had the nuclear bomb, and then the thermonuclear bomb, and the USSR did not (see e.g. Tannenwald on the&nbsp;<a href=\"https://www.cambridge.org/core/books/nuclear-taboo/7ECD36D9D7B2C09B95848CAB78503A21\"><u>nuclear taboo</u></a>). US R&amp;D into missile defence did not provoke conflict with the USSR/Russia (though this could be because after 40 years and $40bn, it<a href=\"https://www.aps.org/newsroom/pressreleases/report-missile.cfm\">&nbsp;<u>doesn\u2019t work</u></a>).</p><p>&nbsp;</p><h2><strong>2. Agreement before deployment</strong></h2><p>&nbsp;</p><p>If a rival state will be capable of deploying advanced AI in two years, another response to the rival deployment problem could be negotiation \u2013 coming to some agreement as to&nbsp;<i>when</i> and<i> how</i> that rival will deploy that AI. This agreement could be between two leading states, most/all of the major states, or indeed all states \u2013 perhaps through the UN.</p><p>&nbsp;</p><h3><strong>2.1 Agreement on Alignment and Uses/Goals \u2013 the Short Reflection</strong></h3><p>&nbsp;</p><p>An agreement could have two key clauses. First, no deployment until provably aligned. Second, some agreement on the uses to which this advanced AI would be put, the goals which this advanced AI system would pursue.</p><p>The first clause may be broadly acceptable, as no-one (apart from an omnicidal few) desires the \u2018paperclip\u2019 scenario of a misaligned AGI, though this clause needs more clarity on how provable alignment can and should be demonstrated. The long-term AI alignment and governance communities would be especially keen on an alignment clause. We have paid far less attention to the second, on uses and goals. But this is to ignore that many states may have strong preferences over the form of deployment by a rival state.&nbsp;</p><p>On the one hand, some uses/goals seem relatively uncontroversial and broadly acceptable, such as medical science research (eg drug discovery, \u2018solving cancer\u2019 etc), clean energy research (eg better solar, or fusion) or goods and services such as new animated films. On the other hand, some uses/goals are more controversial and broadly unacceptable, such as those promoting a particular narrow ideology or set of values. To offer an extreme caricature just to make the point, totalitarian ideologies like Nazism or despotic ideologies like \u201cWorship the Kims\u201d would not be acceptable to the vast majority of people. Deployment for such ends would be viewed as undesirable, much like the deployment of technically misaligned advanced AI.&nbsp;</p><p>Some uses/goals might be somewhere between the two. This especially applies to \u2018dual-use\u2019 research. Research on fundamental physics, for example, could give us new insight into the universe and new energy sources, but could also discover new weapons technologies. Transhumanism, human enhancement, uploading, and digital personhood all seem in the middle too. Unfortunately, \u201cprevent global catastrophic and existential risks\u201d - which to the alignment community might be a primary use - is likely to be controversial&nbsp;<i>to states</i> too, insofar as it could affect security (affecting nuclear and conventional forces) and sovereignty (surveillance and interference). Even something seemingly anodyne like asteroid deflection could be&nbsp;<a href=\"https://global.oup.com/academic/product/dark-skies-9780190903343\"><u>dual-use</u></a>.</p><p>My purpose here is not to suggest which particular uses, purposes and goals should be in such an agreement. It is merely to suggest that such an agreement is likely to be needed, and seems achievable, though it may be complex and time-consuming to reach. One overall path that is sometimes discussed in the long-term AI alignment and governance communities is:&nbsp;</p><blockquote><p>AGI development -&gt; deployment -&gt; long reflection.&nbsp;</p></blockquote><p>I am suggesting that some of the work of the long reflection will have to be done&nbsp;<i>before&nbsp;</i>deployment. Let us call that the \u2018<strong>short reflection\u2019</strong>: a negotiation and agreement on some of the uses, purposes and goals of an AGI. The path is then:&nbsp;</p><blockquote><p>AGI development -&gt; short reflection + agreement -&gt; deployment -&gt; long reflection.</p></blockquote><p>&nbsp;</p><h3><strong>2.2 The credible commitment problem, monitoring and verification possibilities</strong></h3><p>&nbsp;</p><p>One key question is whether a deploying state can make a credible commitment to abide by whatever agreement is reached. States seem unlikely to just take these commitments on trust \u2013 that is more the \u2018acquiescence\u2019 response. If they cannot trust and verify that these commitments are being followed, then we are back to coercive action and dangerous escalation. What assurances can be given?</p><p>Some assurances could be at the training data level \u2013 perhaps another state could have access to training data and programs to ensure e.g. that it does not contain lots of fundamental physics papers, or is not playing thousands of war games against itself. Another assurance could be at the compute level \u2013 preapproval, or a \u2018heads up\u2019 advance warning, of big experiments and training runs. Perhaps there could be something equivalent to permissive action links (<a href=\"https://en.wikipedia.org/wiki/Permissive_Action_Link\"><u>PALs</u></a>) to physically require at least two people to begin some usage of compute. One could even have the equivalent of international PALs, where two states mutually give one another permissions to prevent the beginning of some usage, or stop it in progress. There are many other possibilities that could be explored, such as leveraging trusted hardware, tamper-proof hardware, zero-knowledge proofs, shared secrets, or partitioned training runs between rivals.&nbsp;</p><p>Other assurances include more traditional methods. An agreement would likely require extensive monitoring and verification.</p><p>Let us consider two forms a development project could take: a central world project, and monitoring of national projects. A central world project might function like a joint research project \u2013 like the International Space Station, CERN or ITER (see<a href=\"https://css.ethz.ch/content/dam/ethz/special-interest/gess/cis/center-for-securities-studies/pdfs/PP7-2_2019-E.pdf\">&nbsp;<u>Fischer</u></a>). These are big, costly, capital-intensive centralized projects \u2013 perhaps most likely as an analogy if advanced AI development requires substantial amounts of compute, such that pooling of financial and material (e.g. chips) resources is appealing. This has advantages for credible commitments \u2013 other states have unfettered access to the development program, and indeed are mutually governing it.</p><p>But perhaps more likely is mutual monitoring of national projects. Such monitoring around nuclear weapons, and indeed chemical weapons, is (surprisingly?) extensive and intrusive. We can learn several lessons from successful intrusive arms control regimes like Nunn-Lugar Cooperative Threat Reduction, the Chemical Weapons Convention and New START.</p><p>Two main targets for monitoring suggest themselves: compute and \u2018talent\u2019 (AI experts), though data might also be a possibility. One could monitor a developer state\u2019s compute stocks (where are the big data centres, what chips do they have) and flows (how many chips are they producing and importing, where are those going, etc). One could also monitor what that compute is being used for \u2013 are there projects using above some threshold of compute, and if so for what purpose? This could involve on-site inspections and satellite/plane surveillance of facilities, construction and heat signatures for electrical consumption. Sensors and cameras could be placed in data centres, or monitoring programs in software, to monitor compute and energy usage \u2013 both amounts and purposes.</p><p>One could also monitor a developer state\u2019s talent stocks (who\u2019s in what groups) and flows (how many new ML PhDs are being trained, how many are immigrating to the country, etc). One could also monitor&nbsp;<i>what</i> those people are working on. This could be more intrusive to people\u2019s privacy than compute monitoring, but note that similar monitoring has occurred e.g. for Soviet scientists after the Cold War. Tools in this case could involve interviews with members of the national program, to catch inconsistencies between stories and other data. There could also be some protection of whistleblowers \u2013 ideally at a national level, but more likely rivals sheltering such whistleblowers. It is through whistleblowers that the Soviet biological program (<a href=\"https://en.wikipedia.org/wiki/Biohazard_(book)\"><u>Alibek</u></a>) and the US cyber programs (Snowden,<a href=\"https://en.wikipedia.org/wiki/Confront_and_Conceal\">&nbsp;<u>Sanger\u2019s contacts</u></a>) came to light.</p><p>It is unclear to me whether the credible commitment problem can be addressed sufficiently \u2013 significantly more research is needed.</p><p>&nbsp;</p><h2><strong>3. (Threat of) coercive action</strong></h2><p>&nbsp;</p><p>Faced with the rival deployment problem, and a possible or perceived threat to state or regime sovereignty and status, a state might be tempted to respond with the threat of coercive action. Three possible options present themselves: sanctions, clandestine operations, and further escalation. I will discuss each in turn. The major problem is that it may be hard to avoid inadvertent, unintended escalation to greater threats, raising international tensions and risking conflict.</p><p>Throughout I will return to the example of Iran\u2019s attempt to acquire a nuclear weapon, which has long been regarded as an unacceptable risk by the international community. This is especially true of Israel: successive Israeli governments have viewed the prospect as a major (indeed in this case existential) risk to Israel, as part of the wider \u2018<a href=\"https://en.wikipedia.org/wiki/Begin_Doctrine\"><u>Begin doctrine</u></a>\u2019. This example neatly demonstrates the three possible options. Iran was threatened with sanctions, clandestine operations, and further escalation.</p><p>&nbsp;</p><h3><strong>3.1 Sanctions</strong></h3><p>&nbsp;</p><p>One option is sanctions on the developing state. The sanctions regime has grown significantly in the last twenty years, with extensive anti-terrorist financing and anti-money laundering systems put in place (and with Western states\u2019 decreased willingness to engage in humanitarian intervention). Sanctions might be financial, trade-related or digital. Many of these options will be much more familiar to readers now, following the Russian invasion of Ukraine and the coordinated global response.</p><p>Financial sanctions target the financial system. Banks and other financial institutions are blocked from loaning to or handling the transactions of sanctioned individuals or companies. For example, many financial transactions are settled through SWIFT. The USA is able to lock particular companies and states out of this system. The status of foreign exchange reserves is important \u2013 for example Russia and China have very large reserves. In our case, sanctions could be brought against particular companies in the AI supply chain (fabs, cloud providers, etc) or more generally against the entire economy.</p><p>Trade sanctions have some overlaps, as often it is importer/exporters access to the financial system through which trade sanctions are enforced. In our case, trade sanctions could include export controls, either narrowly focused on semiconductors and rare earths, or more generally on many consumer and industrial goods.</p><p>Finally, we should consider a novel form of sanctions that might be relevant to our case. We can call these \u2018digital sanctions\u2019. Digital sanctions could impede a developer state\u2019s access to compute or data. For example, three US cloud providers have&nbsp;<a href=\"https://verfassungsblog.de/compute-and-antitrust/\"><u>at least 60%</u></a> of the global cloud market. The US Government could compel cloud providers to not offer services to particular companies. States could also affect data flows. For example, many states have \u2018data nationalisation\u2019 requirements to compel tech companies to process their citizens\u2019 data in their own territories. These could be strengthened to limit the flow of data to a developer state. At the limit, this could involve disabling or cutting undersea cables to the rest of the world (though this will be less of a concern with the increase in satellite internet, and the possibility of \u2018splinternet\u2019).</p><p>There are ways around all these sanctions, and recalcitrant states have often been willing to endure years of pain \u2013 there is a vibrant&nbsp;<a href=\"https://yalebooks.yale.edu/book/9780300259360/the-economic-weapon/\"><u>debate</u></a> about whether \u2018sanctions work\u2019. Sanctions are likely to be less effective the more powerful and rich the state is. A developer state could seek to rely on its own financial, digital and trade networks and those of its allies. Sanctions are more likely to delay rather than prevent a determined adversary. Sanctions might not work \u2013 and if they do not, might escalate to further coercive action.</p><p>Also we should note this option is mainly available to the USA and its allies: as the controller of the world\u2019s reserve currency - the dollar - and home of major internet companies and&nbsp;<a href=\"https://en.wikipedia.org/wiki/ICANN\"><u>ICANN</u></a>. However, some limited sanctions might be possible from China or a wider international coalition against the USA \u2013 especially trade sanctions.</p><p>Sanctions were used by the Obama Administration and its allies to encourage Iran to agree to the \u2018Iran Deal\u2019 or JCPOA \u2013 stopping its nuclear program in exchange for sanctions relief. The sanctions were later partly reimposed by the USA after the Trump Administration pulled out of the Iran Deal.&nbsp;<i>The Biden Administration and allies&nbsp;</i></p><p>&nbsp;</p><h3><strong>3.2 Clandestine operation</strong></h3><p>&nbsp;</p><p>A second option is a clandestine operation. This could involve tradecraft (human spies, special forces, etc) or offensive cyber operations.<a href=\"https://www.hup.harvard.edu/catalog.php?isbn=9780674987555\">&nbsp;<u>Ben Buchanan</u></a> offers three categories of cyber attacks that can be usefully extended to clandestine operations in general: espionage (stealing information), sabotage (degrading some capability) and destabilisation (such as the Russian interference in the 2016 US election).&nbsp;</p><p>Espionage could involve exfiltrating (copying, stealing) the model/system, the code itself \u2013 so as to deploy it oneself, ideally before one\u2019s rival. This could be done through human assets (defections, stealing plans like&nbsp;<a href=\"https://en.wikipedia.org/wiki/Klaus_Fuchs#Value_of_data_to_Soviet_project\"><u>Fuchs</u></a>, etc) or cyber assets. Sabotage targets could include servers in data centres, power plants or grids that power them, or data poisoning attacks on the training data. Destabilisation - through the use of dis/misinformation or targeted leaks - could target a rival's population to undermine state funding for a project or the organisational leadership of the project. Both sabotage and destabilisation would arguably only delay rather than prevent a determined adversary. Again, cyber operations might not work \u2013 and if they do not, might escalate to further coercive action.</p><p>This option would be analogous to the USA and Israel\u2019s \u2018Olympic Games\u2019/Stuxnet cyber<a href=\"https://www.penguinrandomhouse.com/books/547683/the-perfect-weapon-by-david-e-sanger/\">&nbsp;<u>sabotage attack</u></a>. In the late 2000s the worm targeted the Iranian nuclear program, successfully destroying a significant percentage of Iran\u2019s centrifuge and delaying their enrichment program. The USA also<a href=\"https://www.brookings.edu/book/bytes-bombs-and-spies/\">&nbsp;<u>may have</u></a> interfered in North Korea\u2019s missile launches. In both cases, this may only have delayed the rival states. (Luckily, there is no evidence yet of great powers interfering in one another\u2019s nuclear systems.)</p><h3><strong>3.3 Further escalation</strong></h3><p>Further escalatory steps could involve the threat of conventional military strikes on infrastructure. For example, Israel carried out a strike on a possible Syrian nuclear program, and has threatened to strike the Iranian program.</p><p>&nbsp;</p><h1><strong>Conclusion</strong></h1><p>&nbsp;</p><p>In this short piece, I have argued that if the deployment of advanced AI by one state is viewed as threatening by another state, then that state is therefore faced with the rival deployment problem \u2013 what should they do faced with this possible challenge to their values and interests? Acquiesence seems unlikely, and threatening a coercive response could be catastrophic.&nbsp;</p><p>Given those two constraints, while it may seem unlikely at this stage, a predeployment agreement might be the least-bad option \u2013 and at least worthy of more study and reflection. In particular, more research should be done into possible clauses in a pre-deployment agreement, and into possibilities for AI development monitoring and verification.</p><p>We can distinguish between naive and sophisticated partisan/booster strategies. A naive \u2018<a href=\"https://forum.effectivealtruism.org/s/xTkejiJHFsidZ9hMo/p/isTXkKprgHh5j8WQr#Partisan\"><u>partisan</u></a>\u2019/booster approach is to try to get your chosen side to \u2018win\u2019 some race. But this is not free - the other side gets a say. That makes this policy dangerous. This is because the \u2018losing\u2019 side could try to coerce the \u2018winning\u2019 side with threats, which could inadvertently escalate to conflict. A more sophisticated version still seeks to support a particular state (for a variety of reasons including retaining negotiating leverage or having a fallback option) but seeks a different ultimate strategy. I am not saying that people should not have preferred states, or that states should not now be investing in their capabilities, merely that attempting to \u2018win an endgame\u2019 is a dangerous crunch-time strategy.</p><p>Promising solutions might draw on all three responses. In the Cold War, arms control and deterrence (arguably) acted in reinforcing, supportive ways to prevent nuclear war. Unilateral deployment could be disastrous, locking-in one set of values or sparking conflict. Deterrence of unilateral deployment through extremely careful escalatory threats could prompt states back to the pre-deployment negotiations. And finally, any multilaterally agreed deployment is likely to have some degree of acquiescence and trust.</p>", "user": {"username": "HaydnBelfield"}}, {"_id": "PauhAAw7Y5bHMawkT", "title": "Shahar Avin on How to Strategically Regulate Advanced AI Systems", "postedAt": "2022-09-23T15:49:04.152Z", "htmlBody": "<p><a href=\"https://www.shaharavin.com/\">Shahar Avin</a> is a senior researcher at the <a href=\"https://www.cser.ac.uk/\">Center for the Study of Existential Risk</a> in Cambridge. In his past life, he was a Google Engineer, though right now he spends most of his time thinking about how to prevent the risks that occur if companies like Google end up deploying powerful AI systems, by organizing <a href=\"https://intelligencerising.org/\">AI Governance role-playing workshops</a>.</p><p>In this episode, we talk about a broad variety of topics, including how we could apply what Shahar learned running <a href=\"https://intelligencerising.org/\">AI Governance workshops</a> to <a href=\"https://theinsideview.ai/shahar#transformative-ai\">governing transformative AI</a>, <a href=\"https://theinsideview.ai/shahar#ai-strategy\">AI Strategy</a>, <a href=\"https://theinsideview.ai/shahar#ai-governance\">AI Governance</a>, <a href=\"https://theinsideview.ai/shahar#toward-trustworthy-ai-development\">Trustworthy AI Development</a> and end up <a href=\"https://theinsideview.ai/shahar#twitter-questions\">answering some twitter questions</a>.</p><p>Below are some highlighted quotes from our conversation (available on <a href=\"https://youtu.be/3T7Gpwhtc6Q\">Youtube</a>, <a href=\"https://open.spotify.com/episode/1vvAKf8EBwErP5yGFRNoCT?si=1a28296cdfa94c01\">Spotify</a>, <a href=\"https://podcasts.google.com/feed/aHR0cHM6Ly9hbmNob3IuZm0vcy81NmRmMjE5NC9wb2RjYXN0L3Jzcw/episode/MzJlMzk4YTAtYmMzZC00MDVkLWIzMTAtNTZhMmM2ZDc2MTg0?sa=X&amp;ved=0CAUQkfYCahcKEwiI2sT3hY35AhUAAAAAHQAAAAAQAQ\">Google Podcast</a>, <a href=\"https://podcasts.apple.com/us/podcast/connor-leahy-eleutherai-conjecture/id1565088425?i=1000570841369\">Apple Podcast</a>). For the full context for each of these quotes, you can find the accompanying <a href=\"https://theinsideview.ai/shahar\">transcript</a>.</p><h1>We Are Only Seeing The Tip Of The Iceberg</h1><h2>The Most Cutting Edge AI Research Is Probably Private</h2><blockquote><p>\u201c<strong>I don\u2019t know how much of the most cutting edge research today is public. I would not be confident that it is.</strong> It is very easy to look at all of the stuff that is public and see a lot of it, and infer from the fact that you\u2019re seeing a lot of public research that all research must, therefore be public. I don\u2019t think that is a correct inference to make.\u201d</p></blockquote><h2>AI companies may not be showing all of their cards</h2><blockquote><p>\u201cMy guess would be that they're not always showing all of the cards. It's not always a calculated decision, but there is a calculated decision to be made of, if I have a result, do I publish or not? And then what goes into the calculation is if there is a benefit from publishing.&nbsp;<strong>It increases your brand, it attracts more talent, it shows that you are at the cutting edge, it allows others to build on your result and then you get to benefit from building on top of their results. And you have the cost of, as long as you keep for yourself, no one else knows it, and you can keep on doing the research</strong>.\u201d</p></blockquote><h1>Aligning Complex Systems Is Hard</h1><h2>Narrow AI Do Not Guarantee Alignment</h2><blockquote><p>\u201c<strong>One failure mode is that there is an overall emergent direction that is bad for us. And another is there is no emergent direction, but the systems in fact are conflicting with each other, undermining each other.</strong> So one system is optimizing for one proxy. It generates it externality that is not fully captured by its designers that gets picked up by another system that has a bad proxy for it, and then tries to do something about it.\u201d</p></blockquote><h2>Security failures are unavoidable for large, complex systems</h2><blockquote><p>\u201cIn particular, if you're building very large, complex, opaque systems, from a system-engineering or system-security perspective, you're just significantly increasing the way things go wrong because you haven't engineered every little part of the thing to be 100% safe, and provably and verifiably secure. And&nbsp;<strong>even provably and verifiably secure stuff could fail because you've made some bad assumptions about the hardware</strong>.\u201d</p></blockquote><h1>Why Regulating AI Makes Sense</h1><h2>Our World Is A Very Regulated World</h2><blockquote><p>\u201c<strong>Our world is a very regulated world. We tend to see the failures, but we forget that none of these digital technology would exist around us without standards, and interoperability.</strong> We wouldn\u2019t be able to move around if transport was not regulated and controlled and mandated in some way. If you don\u2019t have rules, standards, norms, treaties, laws, you just get chaos.\u201d</p></blockquote><h2>Compliance Is Part Of The Cost Of Doing Business</h2><blockquote><p>\u201cCompliance is part of the cost of doing business in a risky domain. If you have a medical AI startup, you get people inspecting your stuff all the time because you have to pass through a bunch of regulations and you could get fined or go to jail, if you don\u2019t do that.<strong> The threat of going to jail is a very strong motivator for someone who just wants to go on building good tech for the world.&nbsp;</strong>I\u2019m much more worried in that respect about the US than I am about Europe because Europe has regulation-heavy approach to regulation, which also explains why they don\u2019t have any very large players in the tech space.\u201d</p></blockquote><h1>Concrete AI Regulation Proposals</h1><h2>Data Is Much Harder To Regulate Than Compute</h2><blockquote><p>\u201cData is much harder to regulate than compute because&nbsp;<strong>compute is a physical object</strong>. You can quantify it.&nbsp;<strong>If you have one GPU sitting in front of you getting a second GPU just next to it is pretty much impossible. You have to go back to the GPU factory. Whereas if you have a bunch of data here and you want a copy of it on a folder next to it, it's basically free</strong>.\u201d</p></blockquote><h2>Alignment Red Tape And Misalignment Fines</h2><blockquote><p>\u201c<strong>We should have misalignment fines in the same that we fine companies for causing harms.</strong> It\u2019s basically a way of internalizing the externalities. If you make a system that causes harm, you should pay for it and the way we do it is through fines but I also think they should have alignment red tape.&nbsp;<strong>The more powerful your system is, you should be paying the red tape cost of proving that your system is safe and secure and aligned before you\u2019re allowed to make a profit and deploy it in the world.</strong>\u201d</p></blockquote><h1>When Should You Regulate AI</h1><h2>Making Today\u2019s AI Regulation \u201cFuture Ready\u201d</h2><blockquote><p>\u201c<strong>Governments are now caring about AI where previously they did not</strong>, and they care about AI for all of the current reasons: bias and privacy.&nbsp;<strong>Once they care about AI, then the game is about making that \"future ready\"</strong>. You don't want just an ossified thing that only cares about privacy, even in a world with giant drone swarms and highly manipulative chatbots.&nbsp;<strong>You want the regulation of today to be \"updatable\u201d, to take into account new risks, or that the parts of government that created today's regulation would be willing to create new regulation</strong>. Ideally you want to decrease the amount of time that it takes to update regulation to account for new risks and there are various institutional designs that you could do to make that happen.\u201d</p></blockquote><h2>You Should Regulate An AI Explosion Before It Happens</h2><blockquote><p>\u201c<strong>If you want to regulate an explosion, you don\u2019t regulate it as it\u2019s happening, you regulate it before it\u2019s happened.</strong> Similarly here,&nbsp;<strong>if you get to the point where the technology is radically transforming your world on a month by month or week by week basis, it\u2019s too late to do this regulation</strong>, unless the regulators are also sitting on top of very powerful AI that helping them keep track of what\u2019s happening in regulation. We need the different regulatory processes.\u201d</p></blockquote><h2>The Collingridge Dilemma</h2><blockquote><p>\u201cWhen you want to regulate a technology or steer a technology towards a good outcome or any big change that is predicting in the future, if you try to do it too far in advance, you don\u2019t have the details of what the change is going to happen, and so you don\u2019t have a good solution. If you do it too late, then the thing is pretty much locked in and you don\u2019t have much ability to change it.</p><p><strong>Trying to find the sweet spot in the middle, where you know enough to regulate, but it\u2019s not too late to change how things are going to go, is the game of AI regulation</strong>, AI governance. And you can make the game easier by putting in the regulation early that they can scale up or get adapted as you go along. You could have lots of people who are broadly in agreement that we need something, and put them in places of power. And so when it comes time to regulate, you have lots of allies in lots of places. You could generally teach people the fundamentals of why cooperation is good and why everyone dying is bad.\u201d</p></blockquote>", "user": {"username": "mtrazzi"}}, {"_id": "bvtAXefTDQgHxc9BR", "title": "Just Look At The Thing! \u2013 How The Science of Consciousness Informs Ethics", "postedAt": "2022-09-23T06:20:43.518Z", "htmlBody": "<blockquote><p>It is very easy to answer many of these fundamental biological questions; you just&nbsp;<i>look at the thing!</i>&nbsp;</p></blockquote><p><br><i>From Richard Feyman\u2019s talk </i><a href=\"https://www.zyvex.com/nanotech/feynman.html\"><i>There\u2019s Plenty of Room at the Bottom</i></a><i> (1959)</i></p><hr><h2>Introduction</h2><p>The quote above comes from a lecture Richard Feynman gave, which talked about the challenges and opportunities of studying and interacting with the world on a microscopic scale. Among other things, he touches upon how gaining access to, e.g., good-enough electron microscopes would allow us to answer long-standing questions in biology by just looking at the thing (cf.&nbsp;<a href=\"https://youtu.be/RxHTaTmPlwQ\"><u>Seeing Cell Division Like Never Before</u></a>). Once you start to engage with the phenomenon at a high-enough resolution directly, tackling these questions at the theoretical level would turn out, in retrospect, to be idle armchair speculation.</p><p>I think that we can make the case that the philosophy of ethics might be doing something like this at the moment. In other words, it speculates about the nature of value at a theoretical level without engaging with the&nbsp;<i>phenomenon of value</i> at a high resolution. Utilitarianism (whether classical or negative), at least as it is usually formulated, may have background assumptions about the nature of consciousness, personal identity, and valence that a close examination would show to be false (or at least very incomplete). Many&nbsp;<a href=\"https://www.lesswrong.com/posts/RcPZbMuSbJAXPCRRA/why-no-wireheading\"><u>criticisms</u></a> of&nbsp;<a href=\"https://qualiacomputing.com/2016/08/20/wireheading_done_right/\"><u>wireheading</u></a>, for instance, seem to&nbsp;<a href=\"https://astralcodexten.substack.com/p/unpredictable-reward-predictable\"><u>conflate pleasure and reward</u></a> (more on this soon), and yet we now know that these are pretty different. Likewise, the&nbsp;<a href=\"https://plato.stanford.edu/entries/repugnant-conclusion/\"><u>repugnant conclusion</u></a> or the question between&nbsp;<a href=\"https://en.wikipedia.org/wiki/Average_and_total_utilitarianism\"><u>total vs. mean utilitarianism</u></a> are usually discussed using implicit background assumptions about the nature of valence and personal identity. This must stop. We have to&nbsp;<i>look at the thing</i>!</p><p>Without further ado, here are some of the key ways in which an enriched understanding of consciousness can inform our ethical theories:</p><h2>Mixed Valence</h2><p>One ubiquitous phenomenon that I find is largely neglected in discussions about utilitarianism is that of&nbsp;<i>mixed valence states</i>. Not only is it the case that there are many flavors of pleasure and pain, but it is also the case that most states of consciousness blend pleasurable and painful sensations in complex ways.</p><p>In&nbsp;<a href=\"https://opentheory.net/PrincipiaQualia.pdf\"><u>Principia Qualia</u></a> (Michael Johnson), the<i> valence triangle</i> was introduced. The&nbsp;<i>valence triangle&nbsp;</i>describes the valence of a state of consciousness in terms of its loadings on the three dimensions of negative, positive, and neutral valence. This idea was extended in&nbsp;<a href=\"https://qri.org/blog/quantifying-bliss\"><u>Quantifying Bliss</u></a>, which further enriched it by adding a&nbsp;<i>spectral</i> component to each of these dimensions. Let's work with this&nbsp;<i>valence triangle</i> to reason about mixed valence.<br>&nbsp;</p><figure class=\"image image_resized\" style=\"width:55.56%\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2020/12/The-Symmetry-Theory-of-Valence-72.png\"></figure><figure class=\"image image_resized\" style=\"width:54.54%\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2020/12/The-Symmetry-Theory-of-Valence-71-1.png\"></figure><figure class=\"image image_resized\" style=\"width:52.73%\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2020/12/The-Symmetry-Theory-of-Valence-73.png\"></figure><p>To illustrate the relevance of mixed valence states, we can see how it influences policies within the context of negative utilitarianism. Let us say that we agree that there is a ground truth to the total amount of pain and pleasure a system produces. A na\u00efve conception of negative utilitarianism could then be \"we should minimize pain.\" <strong>But the pain that exists within an experience that also contains pleasure may matter much less than the pain that exists in an experience without pleasure that \"balances it out\"</strong>!</p><p>The na\u00efve conception, would thus, not be able to distinguish between the following two scenarios. In Scenario A we have two persons, one suffering from both an intense headache and an intense stomach ache and the other enjoying both a very pleasant sensation in the head and a very pleasant sensation in the stomach. In Scenario B, we switch it up: one person experiences an intense headache while also a very pleasant sensation in the stomach, and the other way around for the other person.</p><figure class=\"image\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2022/09/scenario-A-and-scenario-B.png?ssl=1\"></figure><p>But if you have ever experienced a very pleasant sensation arise during an otherwise unpleasant experience, you will know how much of a difference it makes. Such a pleasant sensation does not need to blunt the painful sensation directly; the mere presence of enough pleasure makes the overall nature of the experience far more tolerable. How and why this happens is still a mystery (in a future post, we shall share our speculations), but it seems to be an empirical fact that can have extraordinary implications.</p><p>For example, a sufficiently advanced meditator might be able to dilute very painful sensations with enough equanimity (itself a high-valence state) or by e.g. generating jhanic sensations (see below).&nbsp;I haven't seen this discussed in academic journals, which I think is an important blind spot.</p><p>We don't need to invoke such fancy scenarios to see the reality and importance of mixed valence states. The canonical example that I use to illustrate this phenomenon is: you just broke up with someone (-), are at a concert enjoying excellent music (+), are coming up on weed and alcohol (+), but also need to pee badly (-). We've all been there, haven't we? Suppose you get sufficiently absorbed into the cathartic pleasure of the music and the drugs. In that case, the negative feelings temporarily recede into the background and thus might tilt the experience towards the net positive for a while.</p><p>Once you consider the reality of mixed valence states, there is a veritable Cambrian Explosion of possible variants of utilitarianism. For example, suppose you&nbsp;<i>do</i> accept that pleasure can somehow dilute pain&nbsp;<i>within a given moment of experience</i>. In that case, you could posit that there is a \"line of hedonic zero\" on the valence triangle and anything on one side of it is net positive:</p><figure class=\"image image_resized\" style=\"width:59.96%\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2022/09/Hedonic-Zero-1899592490-1663827594458.png\"></figure><figure class=\"image image_resized\" style=\"width:60.39%\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2022/09/Net-Positive-Valence-1-660198077-1663827602744.png\"></figure><p>A version of negative utilitarianism we could call <i>within-subject-aggregated-valence negative utilitarianism</i> recognizes any experience in the \u201cNet Positive\u201d region to be perfectly acceptable even though it contains painful sensations.</p><p>Alternatively, another version we may call <i>strict negative valence utilitarianism</i> might say that pain, whether or not it is found within an experience with a lot of pleasure, is still nonetheless unacceptable. Here, however, we may still have plenty of room for a civilization animated by&nbsp;<a href=\"https://www.hedweb.com/north-south.html\"><u>information-sensitive gradients of bliss</u></a>: we can use the gradients that have a mixture of positive and neutral&nbsp;<a href=\"https://en.wikipedia.org/wiki/Vedan%C4%81\"><u>Vedan\u0101</u></a> for information signaling:</p><figure class=\"image image_resized\" style=\"width:60.75%\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2022/09/bliss_and_noise.png?ssl=1\"></figure><p>Yet another view, perhaps called <i>within-subject-majoritarian negative valence utilitarianism</i> might say that what makes an experience worth-living and unproblematic is for it to be at least 50% pleasant, regardless of the composition of the other 50%:</p><figure class=\"image image_resized\" style=\"width:60.14%\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2022/09/fifty_percent_or_more_positive_valence.png?ssl=1\"></figure><p>Now, I am not going to adjudicate between these views today. For the time being, I am pointing out that actually engaging with the phenomenon at hand (i.e. how valence manifests in reality) radically enriches our conceptions and allows us to notice that most ethics have an impoverished understanding of the phenomenon it comments on.&nbsp;<strong>We can change that</strong>.</p><h2>Logarithmic Scales</h2><p>As argued in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/gtGe8WkeFvqucYLAF/logarithmic-scales-of-pleasure-and-pain-rating-ranking-and\"><u>Logarithmic Scales of Pleasure and Pain</u></a> (<a href=\"https://qri.org/blog/review-of-log-scales\"><u>summary</u></a>), we think that there is a wide range of evidence that suggests that the intensity of both pleasure and pain follows a long-tail distribution. I will not repeat the arguments here since I've already written about and&nbsp;<a href=\"https://youtu.be/cgtBg--0fJc\"><u>presented</u></a> them extensively.&nbsp;I will merely mention that I think that a serious ethical analysis of the current state of affairs ought to include the following states of consciousness (among others) due their outsized moral significance:</p><p>On the positive side:</p><ul><li>Temporal lobe epilepsy</li><li>MDMA</li><li>Jhanas</li><li>Good high-dose 5-MeO-DMT trip</li></ul><p>On the negative side:</p><ul><li>Cluster Headaches</li><li>Kidney Stones</li><li>Bad high-dose 5-MeO-DMT trip</li></ul><p>&nbsp;</p><figure class=\"image image_resized\" style=\"width:49.57%\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2019/08/true_pleasure_scale.png\"></figure><figure class=\"image image_resized\" style=\"width:50.7%\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2019/08/true_pain_scale.png\"></figure><p>&nbsp;</p><h2>Valence and Self-Models</h2><p>One of the claims of&nbsp;<a href=\"http://qri.org/\">QRI</a> is that every experience, no matter how outlandish and unlike our normal everyday human experience, has&nbsp;<i>valence characteristics</i>. An analogy can be made with the notion of physical temperature: every physical object has a temperature, no matter what it is made out of or what its shape is.</p><p>Many meditators and psychedelic enthusiasts point out that suffering seems to have something to do with our sense of self. That feelings&nbsp;<i>matter</i> only to the extent that they are <i>happening to someone</i>. Most human experiences have a lot of shared structure, like a central \"phenomenal self\" that works as an organizing principle for arranging sensations. But experiences without a phenomenal self (or radically altered phenomenal selves) will still have valence characteristics. Ego deaths can be dysphoric or euphoric.</p><p>We argue that what matter is actually the overall structure of the experience (cf.&nbsp;<a href=\"https://qri.org/glossary\">valence structuralism</a>). It just so happens that above a certain level of valence, the phenomenal self starts to become an impediment to further bliss. Ultra-pleasant experiences, thus, tend to be selfless! But this does not make them worthless. On the contrary, their intrinsic worth, coming from their positive valence, can go through the roof.</p><p>That said, reporting the valence of exotic experiences can be remarkably difficult. This doesn't mean we should give up; instead, we ought to develop new methods, vocabulary, and&nbsp;<a href=\"https://qri.org/blog/neural-annealing\"><u>culture</u></a> to place these experiences on the same moral footing as our normal everyday life.</p><p>For example, the so-called \u201ctoroidal state\u201d (<a href=\"https://www.reddit.com/r/DMT/comments/xj1ime/comment/ip68ehc/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3\">on DMT</a> or during a <a href=\"https://www.mctb.org/mctb2/table-of-contents/part-v-awakening/37-models-of-the-stages-of-awakening/the-cessation-of-perception-and-feeling-nirodha-samapatti/\">meditative cessation</a>) can have&nbsp;<i>profound</i> valence effects, to the point of making you reconsider the very nature and scope of&nbsp;<i>what matters</i>.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6ff8d278486aa723518e714e2eb2605d87d66f700c87be01.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6ff8d278486aa723518e714e2eb2605d87d66f700c87be01.png/w_290 290w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6ff8d278486aa723518e714e2eb2605d87d66f700c87be01.png/w_580 580w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6ff8d278486aa723518e714e2eb2605d87d66f700c87be01.png/w_870 870w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6ff8d278486aa723518e714e2eb2605d87d66f700c87be01.png/w_1160 1160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6ff8d278486aa723518e714e2eb2605d87d66f700c87be01.png/w_1450 1450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6ff8d278486aa723518e714e2eb2605d87d66f700c87be01.png/w_1740 1740w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6ff8d278486aa723518e714e2eb2605d87d66f700c87be01.png/w_2030 2030w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6ff8d278486aa723518e714e2eb2605d87d66f700c87be01.png/w_2320 2320w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6ff8d278486aa723518e714e2eb2605d87d66f700c87be01.png/w_2610 2610w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6ff8d278486aa723518e714e2eb2605d87d66f700c87be01.png/w_2880 2880w\"></figure><p>From&nbsp;<a href=\"https://www.mctb.org/mctb2/table-of-contents/part-iv-insight/31-the-three-doors/\">The Three Doors</a> chapter in&nbsp;<a href=\"https://www.mctb.org/\">Mastering the Core Teachings of the Buddha</a> (Daniel Ingram):</p><blockquote><p>Regardless of the way a specific door manifests, it reveals something completely extraordinary about the relationship between \u201cthe watcher\u201d and \u201cthe watched\u201d that it would take a very warped, non-Euclidean view of the universe to explain, though I will try shortly. One way or another, these fleeting experiences cannot easily be explained in terms of our normal, four-dimensional experience of space-time, or within our ordinary subject/object experience. [\u2026] When the no-self door predominates with suffering as its second aspect, then a very strange thing happens. There may be an image on one side staring back, but even if there isn\u2019t, the universe becomes a toroid (doughnut-shaped), or occasionally a sphere, and the image and this side of the toroid switch places as the toroid universe spins. It may spin sideways (horizontally), or it may spin vertically (like head over heels), and may also feel like a hood of darkness suddenly being pulled over our heads as the whole thing synchronizes and disappears, or like everything twisting out of existence. The rarest no-self/suffering variant is hard to describe, and involves reality becoming like a doughnut whose whole outer edge rotates inwards such as to trade places with its inner edge (the edge that made the hole in the middle) that rotates to the outer edge position, and when they trade places reality vanishes. The spinning includes the whole background of space in all directions. Fruition occurs when the two have switched places and the whole thing vanishes.</p></blockquote><p>I recommend reading the whole chapter for what I consider to be some ultra-trippy phenomenology of surprising ethical relevance (see also:&nbsp;<a href=\"https://qualiacomputing.com/2017/11/26/no-self-vs-true-self/\">No-Self vs. True Self</a>).</p><p>In summary: this all indicates that states of consciousness have valence characteristics independently of the presence, absence, shape, or dynamic of a phenomenal self. If your ethicist is not considering the&nbsp;<a href=\"https://qualiacomputing.com/2021/11/23/the-supreme-state-unconsciousness-classical-enlightenment-from-the-point-of-view-of-valence-structuralism/\"><u>moral worth of Nirvana</u></a>, perhaps consider switching to&nbsp;<a href=\"https://slatestarcodex.com/2017/08/16/fear-and-loathing-at-effective-altruism-global-2017/\"><u>one who does</u></a>.</p><h2>Valence and Personal Identity</h2><p>The&nbsp;<a href=\"https://youtu.be/g0YID6XV-PQ\"><u>solution to the phenomenal binding problem</u></a> has implications for both personal identity and ethics. If, as I posit, each moment of experience is, in fact, a topological pocket in the fields of physics, then Closed Individualism would seem to be ruled out. Meaning, the standard conception of identity where you start existing when you are born and stop existing when you die would turn out to be a strange evolutionarily adaptive fiction. What really exists is a gigantic field of consciousness subdivided into countless topological pockets. Empty Individualism (\"you are just a moment of experience\") and Open Individualism (\"we are all the same universal consciousness\") would both be consistent with the facts, and it might be impossible to decide between them. Yet, I argue that the vast majority of ethical theories have an implicit background assumption of Closed Individualism. So realizing that it is false has major implications.</p><figure class=\"image\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2015/12/closed_1.png\"></figure><figure class=\"image\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2015/12/closed_2.png\"></figure><figure class=\"image\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2015/12/empty_1.png\"></figure><figure class=\"image\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2015/12/empty_2.png\"></figure><figure class=\"image\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2015/12/open_1.png\"></figure><figure class=\"image\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2015/12/open_2.png\"></figure><p>In particular, if we take the Empty Individualist perspective, it might be easier to defend negative utilitarianism: since each snapshot of experience is a completely separate being, you simply cannot \u201cmake it up\u201d to someone who is currently suffering by giving him/her enough happiness in the future. Simply put, that suffering will never be redeemed.</p><p>Alternatively, if we take the Open Individualist perspective, we now might have actual grounds to decide between, say, average vs. total utilitarianism. Ultimately, you will be forced to experience everyone and everything. This line of reasoning becomes particularly interesting if you take something like Feynman and Wheeler's&nbsp;<a href=\"https://en.wikipedia.org/wiki/One-electron_universe\"><u>One-electron Universe</u></a> seriously. Here we might even objectively determine an experience's moral worth in terms of \"how long the one electron stays trapped inside it\". An experience with a huge spatial breadth and one with enormous temporal depth may be equivalent according to this metric: they're just structured differently (cf.<a href=\"https://qualiacomputing.com/2018/11/28/the-pseudo-time-arrow-explaining-phenomenal-time-with-implicit-causal-structures-in-networks-of-local-binding/\"><u> Pseudo-Time Arrow</u></a>). In this account, you are bouncing backward and forwards in time, interfering with yourself forever. The multiverse is the structure emergent from this pattern of self-interference, and it is eternal and immutable in a certain sense. Relative to a small experience, a large experience would keep the one electron trapped for longer. Thus, there would be a strong case to care more about larger and brighter experiences: you'll be there for ages!</p><p>If you are bouncing backward and forwards forever in this structure, then perhaps average utilitarianism can be defended. In brief, since you are always somewhere, what matters is not how large the structure is but the shape of its distribution of states.</p><h2>Valence Structuralism</h2><p>Finally, if you pay attention to the nature of highly valenced states of consciousness you will notice that they have structural features.&nbsp;For instance, I think the Symmetry Theory of Valence (<a href=\"https://qri.org/blog/symmetry-theory-of-valence-2020\"><u>overview</u></a>;&nbsp;<a href=\"https://qri.org/blog/quantifying-bliss\"><u>CDNS</u></a>) can be experientially probed for oneself by introspecting on the structural features of one's experience when enjoying intense bliss or enduring intense suffering.&nbsp;<a href=\"https://qualiacomputing.com/2021/12/03/high-valence-meditation/\"><u>Rob Burbea's meditation instructions</u></a> are very well worth reading to get a sense of what I'm talking about. This would seem to matter a lot when it comes e.g. deciding what kind of artificial sentient minds we might want to create\u2014much more on this in the future.</p><hr><h2>Putting It All together</h2><figure class=\"image image_resized\" style=\"width:89.41%\"><img src=\"https://i0.wp.com/qualiacomputing.com/wp-content/uploads/2022/09/xawdhz8njjb41.png?ssl=1\"><figcaption>Image credit: <a href=\"https://www.ayjayart.com/conscious-creators-virtual-art-exhibition\">Ayjay Art</a></figcaption></figure><p><a href=\"https://qualiacomputing.com/2022/05/02/dmt-and-hyperbolic-geometry-1-million-views-special/\"><u>High-dose DMT experiences</u></a> are excellent examples of the states of consciousness, parts of reality, that are generally not taken seriously in philosophy (despite their<a href=\"https://qualiacomputing.com/2017/03/11/their-scientific-significance-is-hard-to-overstate/\"><u> enormous significance</u></a>), and have many elements that challenge preconceptions about pleasure and pain and inform our understanding of valence. These experiences:</p><ul><li>Tend to involve&nbsp;<a href=\"https://youtu.be/ECz5NMiwvmI\">exotic variants of phenomenal self</a> and a distorted feeling of personal identity</li><li>Have greatly expanded amount of qualia per moment of experience</li><li>Express valence that is&nbsp;<a href=\"https://qualiacomputing.com/2019/09/03/typical-nn-dmt-trip-progression-according-to-an-anonymous-reader/\">typically</a>&nbsp;<i>extremely</i>&nbsp;<i>mixed</i> and&nbsp;<i>extremely intense</i> (with complex dynamic elements that have both&nbsp;<a href=\"https://qri.org/blog/neural-annealing\">high degrees of consonance and dissonance</a> at the same time; see also \"<a href=\"https://youtu.be/bwwZP-Bm7kI\">competing clusters of coherence</a>\")</li><li>And show to a large extent&nbsp;<i>transparent valence</i>: you can use knowledge about the Symmetry Theory of Valence in order&nbsp;<a href=\"https://youtu.be/sxWIy2X0SDw\">to steer the experience towards more blissful configurations</a> by seeking symmetry, harmony, and consonance explicitly.</li></ul><p>For a theory of physics to be true, it needs to be able to explain physical phenomena outside of room temperature. Likewise, for an ethical theory to be in any way true, it ought to be able to account for states of consciousness outside of the range of normal human everyday life experiences. DMT states, among others, are examples of non-room-temperature states of consciousness that you can use to test if your theory of ethics generalizes. How do you make sense of experiences with more qualia, mixed valence, exotic phenomenal selves, and valence effects up there in the logarithmic scale? That is what we need to answer if we are serious about ethics.</p><p>The future holds much crazier trade-offs than that between&nbsp;<a href=\"https://plato.stanford.edu/entries/repugnant-conclusion/\">Human Flourishing vs Potatoes with Muzak</a>. Already today, I would argue, the facts suggest that we ought to begin recognizing the&nbsp;<a href=\"https://qualiacomputing.com/2018/12/03/hell-must-be-destroyed/\">reality of Hell</a> and the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/8Sed33q54kdhZ4M9m/get-out-of-hell-free-necklace\">ethical imperative to destroy it</a>. And beyond, our theory of ethics ought to be powerful enough to contend with the&nbsp;<a href=\"https://qualiacomputing.com/2017/12/20/the-universal-plot-part-i-consciousness-vs-pure-replicators/\">outlandish realities of consciousness</a> we are bound to soon encounter.</p><hr><p>See also:</p><ul><li><a href=\"https://twitter.com/cube_flipper\">Cube Flipper</a>\u2018s <a href=\"https://smoothbrains.net/posts/2022-09-22-how-to-use-dmt-without-going-insane.html\">How to use DMT without going insane</a></li><li><a href=\"https://www.utilitarianpodcast.com/the-feeling-of-value-sharon-hewitt-rawlette/\">The Feeling of Value \u2013 Sharon Hewitt Rawlette</a> (<a href=\"https://www.utilitarianpodcast.com/\">Utilitarian Podcast</a>)</li><li><a href=\"https://qualiacomputing.com/2018/10/10/thoughts-on-the-is-ought-problem-from-a-qualia-realist-point-of-view/\">Thoughts on the \u2018Is-Ought Problem\u2019 from a Qualia Realist Point of View</a></li><li><a href=\"https://qualiacomputing.com/2018/07/23/open-individualism-and-antinatalism-if-god-could-be-killed-itd-be-dead-already/\">Open Individualism and Antinatalism: If God could be killed, it\u2019d be dead already</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/gtGe8WkeFvqucYLAF/logarithmic-scales-of-pleasure-and-pain-rating-ranking-and\">Logarithmic Scales of Pleasure and Pain: Rating, Ranking, and Comparing Peak Experiences Suggest the Existence of Long Tails for Bliss and Suffering</a></li></ul><hr><p>Many thanks to Hunter Meyer for comments and edits to this essay.</p>", "user": {"username": "algekalipso"}}, {"_id": "s7DobRzdwtRBbiKiS", "title": "Good Deeds: Under-recognition \n", "postedAt": "2022-09-23T09:11:05.512Z", "htmlBody": "<p>There are some persons who perform exceptionally good deeds of showing kindness to others. Some of them are not bothered whether their deeds are recognised or not. Some get recognition by own efforts or those of others. Some don\u2019t get any or due recognition.&nbsp;</p><p>The reason why so many good deeds go unnoticed is that many of us have a weakness for celebrity worship. Once someone attains an exalted status by whatever means, the person is worshipped and people don\u2019t bother much about others who have done good deeds. The weakness for celebrity worship is pervasive since many of us are happy with our ordinariness and respective comfort zones, and have no inclination to become special ourselves or to identify those who are special and learn about their accomplishments.&nbsp;</p><p>Prior to or during the Second World War, four persons who protected many human lives from the impact of the Holocaust were: Oskar Schindler, Nicholas Winton, Trevor Chadwick, and Maharaja Digvijaysinhji Ranjitsinhji Jadeja (affectionately called&nbsp;Jam Sahib).&nbsp;</p><p>Oskar Schindler\u2019s name is familiar to many because of the 1993 film, <i>Schindler\u2019s List</i>. Winton, Chadwick, and the Maharaja are sometimes referred as the British Schindler, the Purbeck Schindler, and the Indian Schindler respectively, but their acts stand out as unique as that of Oskar Schindler. People knew about Winton only after a BBC programme in 1988. Chadwick, who helped Winton and termed by Winton as the real hero, got recognition only in August 2022. Many people may not have heard of the Maharaja.&nbsp;</p><p>Oskar Schindler</p><p>In 1935, when Germany invaded Poland, Schindler set up an enamelware factory there, employing a combination of Jewish forced labourers and free Polish workers. His initial interest was making money, but later he wanted to take care of his Jewish workers. He used bribes and personal diplomacy to save about 1200 Jewish workers from being deported to a concentration camp. He added an armaments manufacturing division to his company to claim that the Jewish workers were essential for the war effort and got it designated as a subcamp.&nbsp;</p><p>Schindler faced both risk and cost to protect his Jewish workers. He was suspected of unauthorised aid to them and was arrested thrice, but could not be charged. He produced bogus production figures to justify the existence of the subcamp as an armament factory. He left the place only after the Soviet troops liberated the camp in 1945. In 1949 he migrated to Argentina and returned to Germany in 1957. When he died in 1974, he was penniless and almost unknown.&nbsp;</p><p>In 1993, he and his wife were awarded the title \u2018Righteous Among the Nations\u2019 by Yad Vashem, the World Holocaust Remembrance Center. Also in 1993, the US Holocaust Memorial Council posthumously presented the Museum\u2019s Medal of Remembrance to him. &nbsp;[<a href=\"https://encyclopedia.ushmm.org/content/en/article/oskar-schindler\">https://encyclopedia.ushmm.org/content/en/article/oskar-schindler</a>]&nbsp;</p><p>Nicholas Winton</p><p>He was a London stockbroker. In 1938, when he was 29 years old, he wanted to go to Switzerland on a skiing trip, but went to Czechoslovakia with his friend who was helping people escape from that country before Nazi occupation. Others were helping elderly people to escape, and he was asked if he could help Czech children to escape. He set up an office in a hotel accepting names of children to be sent to England. He returned to England to make arrangements for families who would accept the children. He made arrangements for their settlement raising funds for the cost of travel, paying bribes for allowing the passage of trains and \u00a350 for each child that was required as a deposit by the British government in case of need, and doing all the paperwork, sometimes even forging documents. Trevor Chadwick, a school teacher in England, agreed to be in charge of the office in Prague and make arrangements for transporting the children to England. In 1939, eight train loads carried 669 children, mostly Jewish, to England via Germany. The ninth train with 250 children could not leave Czechoslovakia since the Second World War broke out the same day.&nbsp;</p><p>For nearly fifty years, Winton told no one about this, not even his wife, since he thought that others won\u2019t be much interested in what he did. In 1987, he met a publisher\u2019s wife, a holocaust historian, and gave the thick scrapbook he was having which contained details of the children.&nbsp;</p><p>She got the story published in a British newspaper. The story attracted the attention of a BBC presenter, who arranged a surprise programme in 1988 in two instalments inviting the rescued children whom they could contact and Winton himself. None of them knew the purpose. The presenter, after mentioning about the scrapbook and introducing two of the recued children, asks all those who were rescued by Winton to stand up. When all of them stand up, Winton is asked to turn around to see the now grown-up children, a pleasant surprise to all and a pleasant viewing for the audience.</p><p>He received many honours that include:</p><ul><li>In 1991, he was awarded the Freedom of the City when he visited Prague, Czech Republic.</li><li>In 2003, he received knighthood from the Queen and so he is referred as Sir Nicholas Winton.</li><li>In 2014, he was awarded the order of White Lion by the President of the Czech Republic.</li></ul><p>He passed away in 2015, aged 106.</p><p>[<a href=\"https://sounds.bl.uk/related-content/TRANSCRIPTS/021I-C0410X0094XX-ZZZZA0.pdf\">https://sounds.bl.uk/related-content/TRANSCRIPTS/021I-C0410X0094XX-ZZZZA0.pdf</a></p><p><a href=\"https://www.nicholaswinton.com/exhibition/recognition-1988\">Recognition - 1988 - Sir Nicholas Winton Exhibition</a>&nbsp;</p><p><a href=\"https://www.nicholaswinton.com/\">https://www.nicholaswinton.com/</a>]</p><p>&nbsp;</p><p>Trevor Chadwick</p><p>As mentioned above, Chadwick stayed back in Prague to orgamise the transportation of the Czech children to England via Germany, while Winton was working at the England end. Chadwick was to deal with paying bribes where required, and making arrangements for the safe travel of the children on the eight trains that transported them.&nbsp;</p><p>Winton said that Chadwick faced more risk than himself and so was the real hero. However, Chadwick\u2019s contribution was not recognised till August 2022, when a bronze statue was unveiled in his hometown, Purbeck. It is surprising that nothing is known about what the rescued Czech children said about him while a lot has been made known of the words of gratitude they expressed to Winton.&nbsp;</p><p>[<a href=\"https://www.bbc.com/news/uk-england-dorset-62686207\">https://www.bbc.com/news/uk-england-dorset-62686207</a>]</p><p>&nbsp;</p><p>Maharaja Digvijaysinhji Ranjitsinhji Jadeja</p><p>When Germany invaded the USSR in 1941, the latter released Polish exiles from its labour camps. The first Prime Minister of the Polish Government-in-Exile requested the British Prime Minister to protect the starving young Polish children. India, then a British colony, was suggested as a destination.&nbsp;</p><p>The Maharaja of Jamnagar decided to help the refugees. He donated liberally and raised the huge amount of Rs. 6,00,000 from other Princely colleagues and private donations. He built refugee camps in Balachadi (near his capital city) housing 1,200 Polish orphans and fully funded by him; and in Kolhapur, Bandra, Chela, and Panchgani, housing around 15,000 Polish citizens.</p><p>When the first batch of malnourished and exhausted orphans arrived in Nawanagar, he welcomed them saying: \u2018Don\u2019t consider yourselves as orphans. I am Bapu, the father of all Nawanagaris, including yourselves.\u2019</p><p>In Balachadi he made arrangements for their education, medical facilities, and rest and recuperation. He received several honours that include:</p><ul><li>There is a \u2018Good Maharaja Square\u2019 in Warsaw.</li><li>Six private schools in Warsaw were named after him.</li><li>He was awarded the President\u2019s Medal, Poland\u2019s highest honour.</li></ul><p>[<a href=\"https://forceindia.net/guest-column/guts-grit-and-glory/indian-schindler-jamnagar/\">https://forceindia.net/guest-column/guts-grit-and-glory/indian-schindler-jamnagar/</a>]</p>", "user": {"username": "Lakshmipati Rao Viriyala"}}, {"_id": "fH5adhXF377Bt6fWj", "title": "Public-facing Censorship Is Safety Theater, Causing Reputational Damage ", "postedAt": "2022-09-23T05:08:14.174Z", "htmlBody": "", "user": {"username": "Yitz"}}, {"_id": "NADzcZXPpAHpddw2p", "title": "Summary: the Global Catastrophic Risk Management Act of 2022", "postedAt": "2022-09-23T03:19:24.539Z", "htmlBody": "<p>Introduced to the Senate this past June, the Global Catastrophic Risk Management Act details a Federal plan for addressing existential threats. Global catastrophic risk is defined here as \u201cthe risk of events or incidents consequential enough to significantly harm, set back, or destroy human civilization at the global scale.\u201d Here is a brief summary of what the bill entails.</p><p><br>Within 90 days of the bill being enacted, the President establishes an Interagency Committee on Global Catastrophic Risk. The Committee consists of several members of the Executive Branch, and is co-chaired by a senior representative of the President and the Deputy Administrator of the Federal Emergency Management Agency for Resilience.</p><p><br>The purpose of the Committee is to submit a report on global catastrophic risk to Congress within a year of enactment. The report includes a comprehensive list of potential threats over the next 30 years, technical and lay descriptions of each threat, cumulative and individual likelihoods according to expert estimates, and expert-informed analyses of the most likely threats. It also includes a review of the effectiveness of early warning, a forecast of if and why these risks are likely to increase or decrease in the next 30 years, and an explanation of any limiting factors in the assessment. Finally, the report includes proposals for improving assessment and recommendations for legislative action.</p><p><br>Within 180 days of this report being submitted to Congress, the President submits a followup report on the ability of the government to maintain function in the event of a global catastrophe. This will assess the government's plans for maintaining essential functions during catastrophes and appointing successors after an official\u2019s death, among other needs. The report will include a budget proposal and recommendations for legislative action if appropriate.</p><p><br>In addition to these two reports, the President and the Committee will also develop a strategy for providing for the basic needs of Americans in the event of a global catastrophe. This strategy assumes that multiple levels of critical infrastructure are incapacitated, the military is preoccupied with an armed or cyber conflict, and State and local governments are unable to provide for these needs alone. The strategy also works to enhance individual resilience by improving awareness and domestic supply chains, and includes efforts to seek humanitarian aid from international allies.</p><p><br>Within 90 days of this strategy being issued, the President produces a plan to enact it. This includes specific actions the President will take to ensure that the government is capable of implementing the strategy, that the public is educated on these matters, that strategic objectives are met, and that foreign adversaries are not able to undermine the plan. Within one year of this, the Department of Homeland Security leads a national exercise in executing the plan, involving State, local, and Tribal governments, information sharing centers, and owners of critical infrastructure. Within one year of the exercise, the President submits a review of the exercise to Congress.</p><p>&nbsp;</p><p>The act is sponsored by Senator Rob Portman of Ohio, and is cosponsored by Senators Gary Peters of Michigan, Maggie Hassan of New Hampshire, John Cornyn of Texas. Its current status is \u201cordered reported,\u201d meaning the committee reviewing this bill sent it to the Senate as a whole for review.</p>", "user": {"username": "Anthony Fleming"}}, {"_id": "DcwdjbckGCceqctTp", "title": "7 Learnings and a Detailed Description of an AI Safety Reading Group", "postedAt": "2022-09-23T02:02:58.336Z", "htmlBody": "<p>When it comes to skilling up in AI Safety, there\u2019s one resource that probably everyone will recommend to you:&nbsp;<a href=\"https://docs.google.com/document/d/1mTm_sT2YQx3mRXQD6J2xD2QJG1c3kHyvX8kQc_IQ0ns/edit?usp=sharing\"><u>The AI Safety Fundamentals Course by Richard Ngo</u></a>. For the few people that haven\u2019t heard of it: it\u2019s an 8-weeks program that some EA groups run as fellowship but many people go over themselves. Or in a reading group.&nbsp;</p><p>I (Ninell) received funding to run such a reading group in Berkeley over the summer. One attendee, Karan, jumped in and helped with the organization after people didn\u2019t stop joining the group. Here is why and how we did it and what we\u2019ve learned.</p><p><i>This post is especially helpful for people that are new to AI Safety and/or people who are planning on running any reading group. We\u2019ll start with the learnings and will elaborate on those (pretty detailed) below.</i></p><h1>7 Learnings</h1><ol><li><strong>Making less committed people very committed takes *a lot* of work</strong>. This is why fellowship organizers like&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/suGcEobbHZZ4Gspeh/a-guide-to-effective-altruism-fellowships\"><u>Joshua Monrad</u></a> recommend selecting very strongly for people. We did not do that because a) we weren\u2019t running a fellowship as we were also going through the course for the first time, and b) because we ran it on a the-more-the-merrier basis (which is debatable). However, according to Joshua's post and resonating with our experience, making stuff official is the key to great attendance. The more casual everything is, the more people slack off the course. This starts by calling it \u201cfellowship\u201d rather than \u201creading group\u201d, providing stuff, making people do homework, recording attendance, make rules clear from the beginning (see Joshua\u2019s post for more).<br>&nbsp;</li><li><strong>Provide official meeting spots and snacks</strong>. The value of making people feel comfortable, value their time &amp; hunger is surprisingly high. Good snacks, drinks, &amp; food are crucial for keeping up the good work. If you want to put it into a framework, the three dimensions are social interest, food, and internal motivation. As more you have of one of them, the less you need the other. However, our thesis is that if you\u2019re missing one completely, the others can\u2019t make up for it.<br>&nbsp;</li><li><strong>Advertising over the Facebook group and word of mouth worked well for our particular case (in Berkeley where many people came for skilling up)</strong>. However, if you\u2019re not in a hub, more work is needed here. Possible channels include Twitter (with a reasonably active EA community), Facebook groups, and Slack groups (feel free to ask me (Ninell) if you\u2019re interested in doing so).<br>&nbsp;</li><li><strong>There can\u2019t be enough micromanagement &amp; facilitation</strong>. From the exact words that we prepare for the kickoff meeting to the folder structure and useful links in the pinned posts of the Slack channel, putting in the time, careful thought, and effort pays off. People will ask you less often about where to find things, what to do if X, and where to write down Y. Secondly, it forces you to think everything through: does your concept make sense? Is the structure really intuitive? In terms of facilitation, we think that facilitators are crucial for the reading group. We sometimes were tired or not perfectly prepared. But as the anchor of the group, facilitators are crucial for keeping the vibe up.<br>&nbsp;</li><li><strong>It\u2019s good if facilitators are also interested in doing the reading &amp; are highly engaged in the group, it\u2019s better if they\u2019ve already done the course and can actually help answer questions</strong>. However, this is no knockout criterium. If you cannot find facilitators for your group, the value of doing the readings + discussion is still high!<br>&nbsp;</li><li><strong>Not talking about your background worked surprisingly well</strong>! People did actually discuss and questioned a lot &amp; seemed to defer less. They also needed to think about their own claims more.<br>&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/posts/RZ4cWxEkTsCqGgfut/how-to-run-a-high-energy-reading-group\"><strong><u>Tessa\u2019s model of roles</u></strong></a><strong> also worked well in terms of making people actually read the things &amp; making them talk</strong> (the two crucial things). We assigned roles for the first reading (out of 4-5) per week, which was sufficient. We also changed roles after every week which was good because some roles seemed to take up more time (summarizer, discussion generator). We found the role of the connector (connect the learnings to your personal life) not as useful because people would either do it automatically if the topic allows it. Or it\u2019d be a very made-up connection that doesn\u2019t really provide value. Instead, more discussion generators would be great!</li></ol><h1>The What and Why</h1><p>The&nbsp;<a href=\"https://docs.google.com/document/d/1mTm_sT2YQx3mRXQD6J2xD2QJG1c3kHyvX8kQc_IQ0ns/edit?usp=sharing\"><u>AI Safety Fundamentals curriculum</u></a> is for now the most condensed collection of posts and papers to get a closer grasp of AI Safety. It stretches from: an introduction to ML/DL; the general motivation behind alignment research; concrete examples of threats and problems; current alignment approaches; and frameworks for solutions. The last week encourages participants to do their own projects. The whole curriculum comes with exercises and discussion prompts, which further motivates to do it in a group rather than alone. Lastly, there are also always further readings for each week, which, in my opinion, makes it even more valuable as a resource. However, if you\u2019re planning on doing an AI Safety group yourself, you got plenty more stuff: have a look&nbsp;<a href=\"https://ai-safety-papers.quantifieduncertainty.org/about\">here</a>, <a href=\"https://humancompatible.ai/bibliography\"><u>here</u></a>,&nbsp;<a href=\"https://intelligence.org/research-guide/\"><u>here</u></a>,&nbsp;<a href=\"https://www.lesswrong.com/posts/bjjbp5i5G8bekJuxv/study-guide\"><u>here</u></a>, and&nbsp;<a href=\"https://vkrakovna.wordpress.com/ai-safety-resources/\"><u>here</u></a>. More recent papers e.g. can be found by following the top labs (<a href=\"https://twitter.com/OpenAI?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor\"><u>@OpenAI</u></a>,&nbsp;<a href=\"https://twitter.com/DeepMind\"><u>@DeepMind</u></a>,&nbsp;<a href=\"https://twitter.com/AnthropicAI\"><u>@AnthropicAI</u></a>,&nbsp;<a href=\"https://twitter.com/CHAI_Berkeley\"><u>@CHAI_Berkeley</u></a>) on Twitter and browsing their websites, subscribing to the&nbsp;<a href=\"https://rohinshah.com/alignment-newsletter/\"><u>Alignment Newsletter</u></a>, and reading stuff on&nbsp;<a href=\"https://www.lesswrong.com/\"><u>LessWrong</u></a> and the&nbsp;<a href=\"https://www.alignmentforum.org/\"><u>Alignment Forum</u></a>.</p><p>Back to our group: During the summer of 2022, many people relatively new to AI Safety and EA in general, went to the EA hub Berkeley to skill up in AI Safety including the&nbsp;<a href=\"https://www.serimats.org/\"><u>SERI MATS</u></a> scholars &amp;&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/vvocfhQ7bcBR4FLBx/apply-to-the-second-ml-for-alignment-bootcamp-mlab-2-in\"><u>MLAB fellows</u></a>. I won\u2019t go into detail about why this has a comparative advantage over doing it at a non-hub place but&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/A2YwuXe3Eo5kMZhZo/13-background-claims-about-ea?fbclid=IwAR27K5UVs7AG8qS44OqdngvxgS_-bnlMICtWTXYngVAuy5WwO1r85DH207A\"><u>Akash wrote a post about this</u></a>. In general, our experience of getting to Berkeley, finding housing, getting to know people in the community, and skilling up through conversations was very positive and highly productive.</p><h1>Advertisement, Sign-Up Form, Kickoff Meeting</h1><p>The neat thing about living in a hub is that everyone is super connected. The neat thing about EA is that everyone is super helpful and interested. These two dimensions put together made it possible that slightly more than 20 people showed interest after I (Ninell) posted in the Facebook group chat with ~100-150 that accumulated the Bay summer visitors. I rolled Karan in as 20 was clearly too many faces and it was good to have a second opinion on stuff (even if it\u2019s \u201conly\u201d a reading group, there are quite a few things to decide on). We set up a sign-up form where we asked for the following things:</p><ul><li>Admin stuff (name, email address, availability)</li><li>Level of commitment (motivation: mixed groups make less committed people more committed)</li><li>Motivation &amp; availability for a retreat (motivation: increase the commitment &amp; excitement for the reading group)</li><li>Availability for the kickoff meeting (motivation: communicate time and place as soon as possible)</li><li>Open answer question for \u201canything else that you want to share with us\u201d (you never know what people\u2019s questions are)</li></ul><p>For the kickoff meeting itself, we had some healthy and unhealthy snacks to symbolize that \u201csomething official\u201d is happening and that we value our participants. If you want to do it, don\u2019t hesitate to&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/funding-opportunities\"><u>apply for funding</u></a> (the general rule of thumb should be:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/4tsWDEXkhincu7HLb/things-i-often-tell-people-about-applying-to-ea-funds\"><u>apply &gt;8 weeks in advance</u></a>;&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/A2YwuXe3Eo5kMZhZo/13-background-claims-about-ea\"><u>an application should take up no more than two hours usually</u></a>).</p><p>The aim of the kickoff meeting was to divide people into smaller groups and go through the content of week 0. Furthermore, we wanted to use the chance for everybody to come together and get motivated by the other people. We did a quick introduction round (name and one hobby) but decided to not play excessive intro games as we were 20+ people and a stronger bond with everyone\u2019s group members was the primary goal.</p><h1>Size of Groups, Set-Up, Facilitators</h1><p>The groups consisted of 4-6 people. We set up two frameworks that we encouraged to apply: not mentioning backgrounds and roles in the discussions.</p><p>The background idea came in a conversation of Karan with Sam Brown (Oxford rationalism). It encourages people to not talk about their background. In doing so, we were aiming to avoid deferring from more experienced people and getting a more critical, vibrant conversation/discussion out. That worked pretty well! We had sometimes had some \u201cmath/science signaling\u201d as Karan calls it due to a familiarity with certain concepts/ideas. But it was way less than \u201cI worked for org X and therefore know Y\u201d. Secondly, it also enforces people themselves to&nbsp;<i>explain</i> knowledge/ideas rather than deferring them from their experiences.</p><p>Furthermore, we followed&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/RZ4cWxEkTsCqGgfut/how-to-run-a-high-energy-reading-group\"><u>Tessa\u2019s post</u></a> on how to run an engaged and highly energetic reading group and chose a role-based reading group set-up. The roles (copied from Tessa\u2019s post, see post for more detail) were:</p><ol><li><strong>Discussion Generator:</strong> come up with 1 or 2 questions for the group to discuss</li><li><strong>Summarizer:</strong> prepare a 3 to 5-minute summary of the reading</li><li><strong>Highlighter:</strong> pick 1 or 2 passages that you think are great and merit further discussion</li><li><strong>Concept Enricher:</strong> pick 1 or 2 words or concepts you feel confused about and do a bit of research on them, reporting back on what you learn (e.g. \u201cgain-of-function\u201d, \u201cTET Enzymes\u201d)</li><li><strong>Connector:</strong> share 1 way you might apply ideas from the reading in your own life or work</li></ol><p>If we had more than 5 people, we assigned the Discussion Generator more often as we thought it might be the most useful role. However, having more people highlight important concepts might be an option as well. An alternative way to divide the roles&nbsp;<a href=\"https://colinraffel.com/blog/role-playing-seminar.html\"><u>comes from Colin Raffel</u></a> and is specialized in CS topics. Note: in both posts (Tessa\u2019s and Colin\u2019s) are more suggestions about different formats of reading groups (like one-to-many, 2-people group, etc.) that are worth a read if you\u2019re planning your group.</p><p>Lastly, we assigned ourselves (Ninell &amp; Karan) and one other experienced person as facilitators (one per group). These non-advised / non-experienced facilitators had the advantage that conversations were very free and not topic constrained (I only have anecdotal evidence for this but sometimes, facilitators would end a discussion if it deviated too far from the topic). The clear disadvantage, however, was the increased insecurity &amp; amount of not answered questions a group had.</p><h1>Notes, Slack, &amp; Micromanagement</h1><p>Tessa\u2019s and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/rqmuTjmknnQE2oaPS/reading-group-guide-for-ea-groups#What_are_the_best_practices_for_taking_and_using_notes_\"><u>other</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/JNcp9c7Gzt5hBwA8u/my-plan-for-a-most-important-century-reading-group\"><u>posts</u></a> about reading groups encourage people to take notes (or minutes, or meeting minutes, however you wanna call it)\u2014we love notes (minutes)! Why? Several reasons: 1) It keeps the discussion centered around *something* and traces deviating topics. (I know that I just said that shifting is fine and fun but you should be able to remember where you\u2019re coming from and why you ended up discussing the differences between Huel and Soylent). 2) More importantly, if there are any open/unanswered questions, people will be able to retrieve those at any point. E.g. if you happen to run into Paul Christiano in the streets of Berkeley, you could easily pull out your shared notes folder and ask&nbsp;<a href=\"https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like\"><u>what failure really looks like</u></a> (we are not saying that you should actually do that).</p><p>For the infrastructure, we created a shared google drive with subfolders specifying the weeks. I felt weirdly micromanage-y when I put&nbsp;<a href=\"https://docs.google.com/document/d/1IXFneKkM--YX3X1Rdz4f2u8IPnfSx8h_hjZAbJC8aCc/edit?usp=sharing\"><u>templates</u></a> for notes with useful links and prompts into those folders but it turned out quite helpful for most people. The templates made visible what people were expected to write down (e.g. indicating the existence of a notetaker, the roles described above, a link to the role description, and a link to the curriculum). Each group was asked to copy that template and fill it with their weekly notes but everyone had access to each note of every group. The aim of this was to let people see what other people's questions/concerns/thoughts are but it turned out this didn\u2019t really happen.</p><p>In terms of Slack, we found it useful to create *yet another* channel. Most EAs already are familiar with using it (many EA groups have their own) and we find that it is a clear, structured, and intuitive way of sending messages, replying in threads, and approaching people. Also, it keeps the vibe of \u201cofficialness\u201d alive. However, we found that the engagement wasn\u2019t as high (and are happy about any thoughts about how to increase this) and think that slack was maybe a bit too much (a Facebook group would have done it, too). Maybe there was also a gap between how formal slack seemed to be and how official the rest was. The goal of the engagement would be to enable discussions that exceed the group assigned discussion time. One possible thought is people's comfort in the group or in general with posting to a large, mostly non-friends audience.</p><h1>Attendance</h1><p>Last but not least, the big question is how people stood engaged and motivated. The attendance graph for us looked like the following. It\u2019s important to note that we didn\u2019t penalize not-attendance at all. We were more acting on the basis that every face is a win, no matter when.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995002/mirroredImages/DcwdjbckGCceqctTp/atb9x2v9mkdr6o6y6mbz.png\"></p><p>Interesting to note here is that the attendance went up for the retreat. A special note to the retreat is that we also admitted people that didn\u2019t complete any of the weeks before. The reason for this is that, again, we were hoping to \u201cengage &amp; reach as many people as possible and see who stays with AI Safety\u201d. Did that work? Maybe! See learning number 1 for the reasons.</p><h1>The retreat</h1><p>We organized the retreat with a minimum level of coordination. There are some opportunities in Berkeley to go to a house close by. We went there for two days (even though, I personally was in quarantine and can therefore only speak from second-hand experiences).</p><p>Drawing from the early learnings in the reading group, we micromanaged especially the food chores. For big meals (lunch, dinner), three people were on a cooking shift, for breakfast and afternoon tea only two. Clean-up for every shift was two people, too. This is a system I\u2019ve learned in my EA group house and it\u2019s proven useful as it simplifies social challenges such as fairness a lot.</p><p>As we went only for one night, we did not include any introduction games for the first evening. Usually, people like to do introduction games like \u201chot seat\u201d or throw some icebreaker questions into the room. However, there are already good (better) resources out there for how to&nbsp;<a href=\"https://resources.eagroups.org/events-program-ideas/retreats-unconferences\"><u>run a retreat</u></a> or&nbsp;<a href=\"https://resources.eagroups.org/events-program-ideas/single-day-events/social-events\"><u>a social event</u></a> as well as&nbsp;<a href=\"https://resources.eagroups.org/events-program-ideas/single-day-events/social-events#h.gch6uoh5xyqz\"><u>several lists with icebreaker questions</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/uqcKKTRWcED6y3WFW/questions-that-lead-to-impactful-conversations\"><u>questions that lead to impactful conversations</u></a> if you\u2019re interested in doing such a thing.</p><p>For the studying part of the whole experience, we (the attendees) read through one week on each of the days and discussed things in the afternoon.&nbsp;</p><h1>How the group is doing now</h1><p>After the retreat, the group pretty much died out. The curriculum suggests that participants do a project on their own four weeks after the course has finished. Two people are doing this right now (stay tuned!). Possible reasons for this could be that most participants are students and are back to university after the summer. Also, most people are not permanently based in Berkeley which makes coordination harder\u2014the current projects are operating with a 15 hours time difference.</p><p>People got different things out of the group, presumably based on their motivations. Some report having a better understanding of the risk, taking it more seriously, and appreciating \u201cwhat a wickedly difficult topic this is\u201d (one attendee). Others (maybe 2-3) report that discussion shaped their future plans to some degree, whereas some others (5-6) feel like they\u2019d like to learn more about the topic before shifting their career towards AIS. However, some attendees also just got better acquainted with other attendees on a non-AI Safety level.</p><p>Lastly, we want to thank Akash Wasil for bringing us to Berkeley and all participants of the group for attending and contributing! It was a lot of fun.</p><p><i>Thanks to Elliot Davies for their insightful feedback &amp; comments. Special thanks to Karan Ruparell for feedback, comments, and the willingness to jump into this with me even though you never asked for it\u2014great work, would always do it again!</i></p>", "user": {"username": "ninell"}}, {"_id": "pMoGmfg6rNsJWfZey", "title": "Crypto 'oracle protocols' for AI alignment with real-world data?", "postedAt": "2022-09-22T23:05:26.584Z", "htmlBody": "<p>Quick question re. the intersection of EA, AI, and crypto:&nbsp;</p><p>Apart from the concept of an '<a href=\"https://nickbostrom.com/papers/oracle.pdf\">AI Oracle</a>' (from Nick Bostrom), has anyone in EA written about a quite different kind of oracle: &nbsp;<a href=\"https://ethereum.org/en/developers/docs/oracles/\">oracle protocols</a> (e.g. Chainlink) in the crypto industry? I'm interested in oracle protocols as possible tool for AI systems to get reliably 'aligned' -- not with human values, in this case, but with what's really going on out there in the real world, through highly reliable, cryptographically secure, consensus-based data inputs.</p><p>The concepts of 'definitive truth' and 'cryptographic truth' from Chainlink founder Sergey Nazarov seem potentially relevant to helping AI systems get reliable, hard-to-fake, high-security input data from real-world sources. A Lex Fridman podcast interview with Nazarov is <a href=\"https://www.youtube.com/watch?v=TPXTmVdlyoc\">here</a>. More on this topic <a href=\"https://smartcontentpublication.medium.com/how-chainlink-generates-definitive-truth-about-the-off-chain-world-opening-up-multi-trillion-f5f6ba8cd111\">here</a> &nbsp;and <a href=\"https://blog.chain.link/what-is-cryptographic-truth/\">here</a>.&nbsp;</p><p>As epistemically humble folks, the issue of reliable inputs to AI decision-making seems like something we might be concerned about -- especially given the very high incentives (financial, military, political, etc) for biasing AI decisions through feeding them false, partial, or misleading data.</p>", "user": {"username": "geoffreymiller"}}, {"_id": "tGjKCBZkih7T26Hoe", "title": "Guesstimate Algorithm for Medical Research", "postedAt": "2022-09-22T21:40:48.490Z", "htmlBody": "<p>This document is aimed at subcontractors doing medical research for me. I am sharing it in the hope it is more broadly useful, but have made no attempts to make it more widely accessible.&nbsp;</p><h1>Intro</h1><p><a href=\"https://getguesstimate.com/\">Guesstimate</a> is a tool I have found quite useful in my work, especially in making medical estimates in environments of high uncertainty. It\u2019s not just that it makes it easy to do calculations incorporating many sources of data; guesstimate renders your thinking much more legible to readers, who can then more productively argue with you about your conclusions.&nbsp;</p><p>The basis of guesstimate is breaking down a question you want an answer to (such as \u201cwhat is the chance of long covid?\u201d) into subquestions that can be tackled independently. Questions can have numerical answers in the form of a single number, a range, or a formula that references other questions. This allows you to highlight areas of relative certainty and relative uncertainty, to experiment with the importance of different assumptions, and for readers to play with your model and identify differences of opinion while incorporating the parts of your work they agree with.</p><h1>Basics</h1><p>If you\u2019re not already familiar with guesstimate, please watch this <a href=\"https://www.youtube.com/watch?v=hRYmsSZTfaw\">video</a>, which references this <a href=\"https://www.getguesstimate.com/models/17503\">model</a>. The video goes over two toy questions to help you familiarize yourself with the interface.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667996049/mirroredImages/tGjKCBZkih7T26Hoe/tvfedlacxcbj8jzz7vke.png\"></p><p>&nbsp;</p><h1>The Algorithm</h1><p>The following is my basic algorithm for medical questions:</p><ol><li>Formalize the question you want an answer to. e.g. what is the risk to me of long covid?</li><li>Break that question down into subquestions. The appropriate subquestion varies based on what data is available, and your idea of the correct subquestions is likely to change as you work.<ol><li>When I was studying long covid last year, I broke it into the following subquestions<ol><li>What is the risk with baseline covid?</li><li>What is the vaccine risk modifier?</li><li>What is the strain risk modifier?</li><li>What\u2019s the risk modifier for a given individual?</li></ol></li></ol></li><li>In guesstimate, wire the questions together. For example, if you wanted to know your risk of hospitalization when newly vaccinated in May 2021, you might multiply the former hospitalization rate times a vaccine modifier. If you don\u2019t know how to do that in guesstimate, watch the video above, it demonstrates it in a lot of detail.</li><li>Use literature to fill in answers to subquestions as best you can. Unless the data is very good, these probably include giving ranges and making your best guess as to the shape of the distribution of values.<ol><li>Provide citations for where you got those numbers. This can be done in the guesstimate commenting interface, but that\u2019s quite clunky. Sometimes it\u2019s better to have a separate document where you lay out your reasoning.&nbsp;</li><li>The reader should be able to go from a particular node in the guesstimate to your reasoning for that node with as little effort as possible.</li><li>Guesstimate will use log-normal distribution by default, but you can change it to uniform or normal if you believe that represents reality better.</li></ol></li><li>Sometimes there are questions literature literally can\u2019t answer, or aren\u2019t worth your time to research rigorously. Make your best guess, and call it out as a separate variable so people can identify it and apply their own best guess.<ol><li>This includes value judgments, like the value of a day in lockdown relative to a normal day, or how much one hates being sick.</li><li>Or the 5-year recovery rate from long covid- no one can literally measure it, and while you could guess from other diseases, the additional precision isn\u2019t necessarily worth the effort.</li></ol></li><li>Final product is both the guesstimate model and a document writing up your sources and reasoning.</li></ol><h1>Example: Trading off air quality and covid.</h1><p>The final model is available <a href=\"https://www.getguesstimate.com/models/21014\">here</a>.</p><p>Every year California gets forest fires big enough to damage air quality even if you are quite far away, which pushes people indoors. This was mostly okay until covid, which made being indoors costly in various ways too. So how do we trade those off? I was particularly interested in trading off outdoor exercise vs the gym (and if both had been too awful I might have rethought my stance on how boring and unpleasant working out in my tiny apartment is).</p><p>What I want to know is the QALY hit from 10 minutes outdoors vs 10 minutes indoors. This depends a lot on the exact air quality and covid details for that particular day, so we\u2019ll need to have variables for that.</p><p>For air quality, I used the calculations from <a href=\"https://jasminedevv.github.io/AQI2cigarettes/\">this website</a> to turn AQI into cigarettes. I found a <a href=\"https://acesounderglass.com/2021/08/21/exercise-trade-offs/#:~:text=math%20much%20easier.-,Wikipedia,-lists%20an%20equivalent\">cigarette -&gt; micromort</a> converter faster than cigarette -&gt; QALY so I\u2019m just going to use that. This is fine as long as covid and air quality have the same QALY:micromort ratio (unlikely) or if the final answer is clear enough that even large changes in the ratio would not change our decision (judgment call).&nbsp;</p><p>For both values that use outside data I leave a comment with the source, which gives them a comment icon in the upper right corner.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667996049/mirroredImages/tGjKCBZkih7T26Hoe/samt4bbprbvckqeteqnz.png\"></p><p>But some people are more susceptible than others due to things like asthma or cancer, so I\u2019ll add a personal modifier.&nbsp; I\u2019m not attempting to define this well: people with lung issues can make their best guess. They can\u2019t read my mind though, so I\u2019ll make it clear that 1=average and which direction is bad.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667996049/mirroredImages/tGjKCBZkih7T26Hoe/axm0jjimkuvn6bo7gs6a.png\"></p><p>Okay how about 10 minutes inside? That depends a lot on local conditions. I could embed those all in my guesstimate, or I could punt to microcovid. I\u2019m not sure if <a href=\"https://www.microcovid.org/\">microcovid</a> is still being maintained but I\u2019m very sure I don\u2019t feel like creating new numbers right now, so we\u2019ll just do that. I add a comment with basic instructions.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667996049/mirroredImages/tGjKCBZkih7T26Hoe/zfczzzongkmzbpv9xpwe.png\"></p><p>How about microcovids to micromorts? The first source I found said 10k per infection, which is a suspiciously round number but it will do for now. I device the micromorts by 1 million, since each microcovid is 1/1,000,000 chance of catching covid.</p><h1><img src=\"http://res.cloudinary.com/cea/image/upload/v1667996049/mirroredImages/tGjKCBZkih7T26Hoe/nwrrlm8ljpprtqztnbtl.png\"></h1><p>They could just guess their personal risk modifier like they do for covid, or they could use this (pre-vaccine, pre-variant)<a href=\"https://www.economist.com/graphic-detail/covid-pandemic-mortality-risk-estimator\"> covid risk calculator</a> from the Economist, so I\u2019ll leave a note for that.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667996049/mirroredImages/tGjKCBZkih7T26Hoe/rtwmi9mxxhg1ysmngio2.png\"></p><p>But wait- there are two calculations happening in the microcovids -&gt; micromorts cell, which makes it hard to edit if you disagree with me about the risk of covid. I\u2019m going to move the /1,000,000 to the top cell so it\u2019s easy to edit.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667996049/mirroredImages/tGjKCBZkih7T26Hoe/nqpph6lpdpwlon0mqcfm.png\"></p><p>But the risk of catching covid outside isn\u2019t zero. Microcovid says outdoors has 1/20th the risk. I\u2019m very sure that\u2019s out of date but don\u2019t know the new number so I\u2019ll make something up and list it separately so it\u2019s easy to edit</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667996049/mirroredImages/tGjKCBZkih7T26Hoe/oelzozackwkdnmhqbndw.png\"></p><p>But wait- I\u2019m not necessarily with the same people indoors and out. The general density of people is comparable if I\u2019m deciding to throw a party inside or outside, but not if I\u2019m deciding to exercise outdoors or at a gym. So I should make that toggleable.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667996049/mirroredImages/tGjKCBZkih7T26Hoe/dilx7ozkoqm35ogf78qg.png\"></p><p>Eh, I\u2019m still uncomfortable with that completely made up outdoor risk modifier. Let\u2019s make it a range so we can see the scope of possible risks. Note that this only matters if we\u2019re meeting people outdoors, which seems correct.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667996049/mirroredImages/tGjKCBZkih7T26Hoe/h61fydyxhiqik7v0i1ov.png\"></p><p>But that used guesstimate\u2019s default probability distribution (log normal). I don\u2019t see a reason probability density would concentrate at the low end of the distribution, so I switch it to normal.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667996049/mirroredImages/tGjKCBZkih7T26Hoe/qt9ywdmqtoc97i4t37ye.png\"></p><p>Turns out to make very little difference in practice.</p><p>There are still a few problems here. Some of the numbers are more or less made up, and others have sources but I\u2019ve done no work to verify them, which is almost as bad.</p><p>But unless the numbers are <i>very</i> off, covid is a full order of magnitude riskier than air pollution for the scenarios I picked. This makes me disinclined to spend a bunch of time tracking down better numbers.</p><p>Full list of limitations:</p><ul><li>Only looks at micromorts, not QALYs</li><li>Individual adjustment basically made up, especially for pollution</li><li>Several numbers completely made up</li><li>Didn\u2019t check any of my sources</li></ul><h1>Example: Individual\u2019s chance of long covid given infection</h1><p>This will be based on my post last year, <a href=\"https://acesounderglass.com/2021/08/30/long-covid-is-not-necessarily-your-biggest-problem/\">Long covid is not necessarily your biggest problem</a>, with some modification for pedagogical purposes. And made up numbers instead of real ones because the specific numbers have long been eclipsed by new data and strains. The final model is available <a href=\"https://www.getguesstimate.com/models/20932\">here</a>.&nbsp;</p><p>Step one is to break your questions into subquestions. When I made this model a year ago, we only had data for baseline covid in unvaccinated people. Everyone wanted to know how vaccinations and the new strain would affect things.&nbsp;</p><p>My first question was \u201ccan we predict long covid from acute covid?\u201d I dug into the data and concluded \u201cYes\u201d, the risk of long covid seemed to be very well correlated with acute severity. This informed the shape of the model but not any particular values. Some people disagreed with me, and they would make a very different model.&nbsp;</p><p>Once I made that determination, the model was pretty easy to create: It looked like [risk of hospitalization with baseline covid] * [risk of long covid given hospitalization rate] * [vaccination risk modifier] * [strain hospitalization modifier] * [personal risk modifier]. Note that the model I\u2019m creating here does not perfectly match the one created last year; I\u2019ve modified it to be a better teaching example.&nbsp;</p><p>The risk of hospitalization is easy to establish unless you start including undetected/asymptomatic cases. This has become a bigger deal as home tests became more available and mild cases became more common, since government statistics are missing more mild or asymptomatic cases. So in my calculation, I broke down the risk of hospitalization given covid to the known case hospitalization rate and then inserted a separate term based on my estimate of the number of uncaught cases. In the original post I chose some example people and used base estimates for them from the Economist data. In this model, I made something up.</p><p>Honestly, I don\u2019t remember how I calculated the risk of long covid given the hospitalization rate. It was very complicated and a long time ago. This is why I write <a href=\"https://acesounderglass.com/2021/08/30/long-covid-is-not-necessarily-your-biggest-problem/\">companion documents</a> to explain my reasoning.&nbsp;</p><p>Vaccination modifier was quite easy, every scientist was eager to tell us that. However, there are now questions about vaccines waning over time, and an individual\u2019s protection level is likely to vary. Because of that, in this test model I have entered a range of vaccine efficacies, rather than a point estimate. An individual who knew how recently they were vaccinated might choose to collapse that down.&nbsp;</p><p>Similarly, strain hospitalization modifiers take some time to assess, but are eventually straightforwardly available. Your estimate early in a new strain will probably have a much wider confidence interval than your estimate late in the same wave.&nbsp;</p><p>By definition, I can\u2019t set the personal risk modifier for every person looking at the model. I suggested people get a more accurate estimate of their personal risk using the <a href=\"https://www.economist.com/graphic-detail/covid-pandemic-mortality-risk-estimator\">Economist calculator</a>, and then enter that in the model.</p><p>Lastly, there is a factor I called \u201care you feeling lucky?\u201d. Some people don\u2019t have anything diagnosable but know they get every cold twice; other people could get bitten by a plague rat with no ill effects. This is even more impossible to provide for an individual but is in fact pretty important for an individual\u2019s risk assessment, so I included it as a term in the model. Individuals using the model can set it as they see fit, including to 1 if they don\u2019t want to think about it.</p><p>When I put this together, I get <a href=\"https://www.getguesstimate.com/models/20932\">this guesstimate</a>. [#TODO screenshot]. Remember the numbers are completely made up. If you follow the link you can play around with it yourself, but your changes will not be saved. If anyone wants to update my model with modern strains and vaccine efficacy, I would be delighted.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667996049/mirroredImages/tGjKCBZkih7T26Hoe/kqtlgsym2sauunpbiyqv.png\"></p><h1>Tips and Tricks</h1><p>I\u2019m undoubtedly missing many, so please comment with your own and I\u2019ll update or create a new version later.</p><p>When working with modifiers, it\u2019s easy to forget whether a large number is good or bad, and what the acceptable range is. It can be good to mark them with \u201c0 to 1, higher is less risky\u201d, or \u201cbetween 0 and 1 = less risk, &gt;1 = more risk\u201d</p><p>If you enter a range, the default distribution is log-normal. If you want something different, change it.&nbsp;</p><p>The formulas in the cells can get almost arbitrarily complicated, although it\u2019s often not worth it.&nbsp;</p><p>No, seriously, write out your sources and reasoning somewhere else because you will come back in six months and not remember what the hell you were thinking. Guesstimate is less a tool for holding your entire model and more a tool for forcing you to make your model explicit.</p><p>Separate judgment calls from empirical data, even if you\u2019re really sure you are right.&nbsp;</p><p>&nbsp;</p><h1>Acknowledgements</h1><p>Thanks to Ozzie Gooen and his team for creating Guesstimate.</p><p>Thanks to the FTX Regrant program and a shy regrantor for funding this work.</p><p>&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "Elizabeth"}}, {"_id": "bvK44CdpG7mGpQHbw", "title": "The $100,000 Truman Prize: Rewarding Anonymous EA Work", "postedAt": "2022-09-22T21:07:09.287Z", "htmlBody": "<p>Harry Truman once said: \"It's amazing what you can accomplish if you don't care who gets the credit\"</p><p>The Truman Prize, with a $100,000 prize pool, is now live on&nbsp;<a href=\"http://super-linear.org\"><u>EA prize platform Superlinear</u></a>, which recognizes Effective Altruists with $5,000-$10,000 prizes for declining credit in order to increase their impact, in ways that can't be publicized directly.</p><p><strong>Theory of change:&nbsp;</strong>EA promotes caring about effectiveness over other goals like getting credit, but wanting credit or recognition for your work is natural. Rewarding people for maximizing impact over credit increases the health and future effectiveness of the community.</p><p><strong>Example #1:</strong> Sam toils behind the scenes and makes a breakthrough on an important problem. Sam suggests the idea to, say, a political figure or other organization who can then take credit, because that leads to the breakthrough being more widely accepted.</p><p>Anyone that knows what happened, including the person/org that gets credit, can nominate Sam for The Truman Prize on Superlinear. Superlinear passes on the nomination to a committee of well-respected EAs from diverse backgrounds. If one of them verifies that Sam actually did make a breakthrough and allowed someone better placed to take credit to increase impact, Superlinear awards Sam $10,000.</p><p>The Truman Prize is the brainchild of&nbsp;<a href=\"https://twitter.com/davidmanheim\"><u>David Manheim</u></a><u>,</u> and the judges are:</p><ul><li>Eliezer Yudkowsky</li><li>Peter Wildeford</li><li>Spencer Greenberg</li><li>Cate Hall</li><li>Julia Wise</li><li>Gregory Lewis</li><li>Luke Freeman</li><li>Ozzie Gooen</li></ul><p>The criteria, generally speaking, is that if you can\u2019t make an EA forum post about someone for doing something noteworthy in order to publicize what they have done, they could be eligible for the prize.</p><p><strong>Example #2:&nbsp;</strong>Greg works for the government. There are political or career consequences if it is publicly acknowledged that he\u2019s working on something potentially controversial. Greg contributes an important idea to a research field and helps make it happen behind the scenes. Someone nominates him for The Truman Prize, and the committee asks someone in a position to know about what occurred, and confirms Greg\u2019s contribution. Superlinear awards Greg $5,000 (to be paid out in accordance with local laws), and announces that a prize was awarded to the originator of the research idea to a recipient to be named in 5 years.</p><p><strong>Example #3:</strong> Max has a criminal record and troubled past. He\u2019s reformed now, but his background makes him a liability for any person or org to publicly associate with him. He silently does good work behind the scenes, so someone that knows him nominates him for The Truman Prize on the basis of a specific critical contribution which was made to a now successful larger project. The committee awards the prize, and names Max, likely without naming the specific work done.</p><p><strong>Example #4:</strong> Steve has extreme political beliefs. It is risky for any person or org to work with him due to reputation risks. Steve knows this, but does apolitical high impact work behind the scenes anyways. Someone that knows Steve nominates him for The Truman Prize on the basis of a specific project which was not previously disclosed. The committee awards the prize and discloses the project, but not the individual, or vice-versa, to avoid undermining the project.</p><p><strong>Example #5:&nbsp;</strong>Morgan has recurring depression. Therefore, she does not want to work or associate with any specific people or orgs because she doesn\u2019t want to let them down due to an episode. Morgan does a lot of high impact work for free, and gives credit to others who are better placed to continue executing on the project. Someone nominates her for the Truman Prize on the basis of two specific projects that were done without credit. The committee members award the money, and names the individual and the project.</p><p><strong>How the payout mechanism works:</strong></p><ol><li>You see someone that deserves the award, so you nominate them by&nbsp;<a href=\"https://www.super-linear.org/prize?recordId=reccomnC7E6SPaK8P/#list2\"><u>submitting their info</u></a> (you can keep things anonymous, but make sure enough detail is provided that the event is clearly verifiable by someone likely to be known.).&nbsp;</li><li>If your submission looks promising, we\u2019ll pass it along to one relevant member of our committee of well-respected EAs who is likely to be able to find someone who can verify the event actually happened. So, if the Truman Award nominee is in the biosecurity field, we\u2019d forward it to our biorisk specialist committee member.</li><li>The committee member who was selected chooses two additional people (that don\u2019t have to be on the committee) they trust to assist. This allows them to, for example, delegate verification, discuss the award decision, potentially consider infohazards or reputational risks, and choose how much detail to provide publicly. The three people involved in the decision would inform Superlinear and sign off on the award. The three people involved in the decision would always be public. For example, it might be: \"Gregory Lewis, Andrew Snyder Beattie, and Tessa Alexanian award $5,000 to John Doe for an unspecified Biosecurity project which was spearheaded by others,\" in others it could be \"Luke Freeman, Hilary Greaves, and Abie Rohrig award a $5,000 Truman Award to an undisclosed recipient, to be named no sooner than 5 years from now, for work which will remain anonymous.\"</li></ol><hr><p><strong>What is </strong><a href=\"https://www.super-linear.org/\"><strong>Superlinear</strong></a><strong>? An EA prize platform and $500,000 prize fund</strong></p><p>&nbsp;<strong>Superlinear Platform:&nbsp;</strong></p><p>1) Browse prize competitions (make money&nbsp;<i>and</i> do good)<br>2) Submit your prize ideas (we\u2019ll fund your idea if it\u2019s good!)<br>3) Add to prize pools (e.g. \u201cI\u2019ll add $500 to that prize.\u201d)</p><p><strong>Superlinear Fund:</strong> We\u2019re giving away up to $50,000 to 10+ \u201cSuperlinear regrantors\u201d. These regrantors will generate prize ideas and fund them. What would&nbsp;<i>you</i> do if you had $50,000 to fund your best prize ideas? If that excites you,&nbsp;<a href=\"https://www.super-linear.org/\"><u>apply here.</u></a></p><p><strong>Theory of change:</strong> Given enough prizes, there are hundreds of people working for EA orgs but&nbsp;<i>thousands</i> of lurker EAs (enjoyers of&nbsp;<a href=\"https://www.facebook.com/groups/OMfCT/\"><u>Dank EA Memes</u></a>) that&nbsp;<i>could</i> be contributing directly.</p><p><strong>Are you a grantmaker with rough prize ideas?</strong><a href=\"https://www.super-linear.org/contact-us\">&nbsp;<u>Let us know</u></a>. We\u2019ll help make your prize ideas happen.</p><p><strong>Are you a grantmaker looking for funding opportunities?</strong> Our vision: soon, you\u2019ll browse 1,000+ prizes and, whenever you see a great idea (where you have&nbsp;<a href=\"https://www.effectivealtruism.org/articles/prospecting-for-gold-owen-cotton-barratt\"><u>comparative advantage</u></a>) you can increase its prize pool.</p><p>Want to help others turn their rough prize ideas into reality? Let us know.&nbsp;</p><p>If this project resonates with you,&nbsp;<a href=\"https://discord.gg/U78926wZ\"><u>join our Discord</u></a> and come build with us!</p><p><a href=\"https://www.linkedin.com/in/drewspartz/\"><i><u>Drew</u></i></a><i> from Nonlinear/Superlinear is going to be at EAG DC \u2013 feel free to book a time to meet on Swapcard.</i></p><p><i>----</i></p><p><i>Reminder that&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library\"><i><u>you can listen to posts like this on your podcast player</u></i></a><i> using The Nonlinear Library.</i></p><p><br>&nbsp;</p>", "user": {"username": "Meta"}}, {"_id": "b9kGxviwWDm4iZwDj", "title": "Bounty/Fee: Effective Travel, London <-> Barcelona, what is most effective?", "postedAt": "2022-09-22T19:03:05.718Z", "htmlBody": "<h1><strong>Bounty/Fee: Effective Travel, London &lt;-&gt; Barcelona, what is most effective?</strong></h1><h2><strong>-Offering GBP200 (or offer) for short analysis on effective travel</strong></h2><h2><strong>-I need to travel from London to Barcelona, return</strong></h2><h2><strong>-All in considering emissions, opportunity cost (time, and what we could donate to) etc. is plane better than train, or not? (And possibly electric vehicle car even better?)</strong></h2><p>I\u2019m offering GBP200 for an analysis of effective travel, specifically London to Barcelona, but would take any general principles.</p><p>&nbsp;</p><p>My trip is (west) London to Barcelona (Marriott Hotel). I did this basic analysis (see below and notes), for 4 days at end of Nov 2022.</p><p>&nbsp;</p><ul><li>Train, 12 hours, 2 changes. USD350-450.&nbsp; <strong>Carbon est:&nbsp; 75 kg CO2</strong> (eco passenger).&nbsp;<strong>Smog hydrocarbons at 67g</strong></li><li>Plane, 3-4 hours, no changes. Price: USD 150 - 200.&nbsp;<strong>Carbon est: 108 kg CO2</strong> (according to eco passenger and google calculator, which has range 80 - 120 kg CO2 so at the low end very close to the train)&nbsp;<strong>Smog hydrocarbons at 45g (</strong>vs 67 for train)</li></ul><p>But ecopassenger has other polluting impacts which are higher for train (smog hydrocarbons - this is an issue for air pollution and ozone).&nbsp; It\u2019s not clear to me from London which is immediately better. How damaging are smog hydrocarbons? Do I need to consider plane water vapour? How accurate are thes models?</p><p>Then I think you need to consider there is likely a USD100 - USD200 type of saving, possibly more. Assuming you could put eg. half of that into something impactful (even at worse, just some trees) would that negate the estimated extra carbon?</p><p>Possible consideration: My company will offset my plane flight in any case, with variable quality carbon offsets. (My company has a form of operational \u201cnet zero\u201din that it will offset its scope 1 + 2 carbon emissions, the offsets are not great quality but that anlaysis is beyond scope).</p><p>Does the extra time used up in train travel count for anything? Do I or should I consider any \u201csocial signalling\u201d effects (if I am considered someone who some people will look to my actions).</p><figure class=\"table\"><table><tbody><tr><td style=\"padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"padding:5pt;vertical-align:top\"><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995550/mirroredImages/b9kGxviwWDm4iZwDj/a7wmdm4qhvxm62pof6eh.png\"></p></td><td style=\"padding:5pt;vertical-align:top\">&nbsp;</td></tr></tbody></table></figure><p>London \u2014&gt; Barcelona, by Place vs Train vs Car (diesel) via ecopassenger model.</p><p>The estimates and inputs also have enough variance to me that suggests a +/- 25% on the carbon dioxide element can vary a lot by load and other factors.&nbsp; Hm. I\u2019m also a little bit worried about the strength of the methodology for instance much depends on the mix of the train, is it electric or diesel? The model is also a few years old. &nbsp;Also the RFI factors, or the indirect non-carbon impacts eg water vapour in the air, are vary hard to model - and currently so inaccurate as to be missing in most models.</p><p>The Google model is close to the same but, eg these 3 flights below, are in the 82 to 94kg CO2 range which is close enough to the train estimate (depending on how you get to the airport and train). If you book this far in advance, the price saving is substantial. One can make the argument that using this saving, or even only half the saving, and placing that in a good impact investment or charity would be more impactful.</p><figure class=\"table\"><table><tbody><tr><td style=\"padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"padding:5pt;vertical-align:top\"><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995550/mirroredImages/b9kGxviwWDm4iZwDj/s7dunxhajoliakdv80ey.png\"></p></td><td style=\"padding:5pt;vertical-align:top\">&nbsp;</td></tr></tbody></table></figure><p>Another clear item is that cars running on diesel are poor choices. But that electric battery cars are actually better than trains with a national mix (ofc, grid in France is a lot nuclear). This shows the importance of the electrification of cars.&nbsp;<strong>Below is the journey in a 2 person electric car, Paris to Barcelona.&nbsp;</strong>Of course, there are other impacts to think about but on these electric cars seem winners.</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#ffffff;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"background-color:#ffffff;padding:5pt;vertical-align:top\"><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995550/mirroredImages/b9kGxviwWDm4iZwDj/zuw29vbhtyxycmldb8lw.png\"></p><p>In sum, open questions:</p><ul><li>Is more effective to travel by train or plane London to Barcelona and return? How should we consider this?</li><li>What are good estimates for carbon and hydrocarbon damages? Should we consider the water vapour impact of planes?</li><li>Is train much better than airplane given model uncertainties?</li><li>Considering the cost differences, and what money could be used for in terms of impact, does this negate any likely difference in carbon?</li><li>How should we weigh the travel time difference?</li><li>How should we weight any signalling effect, if any?</li><li>Anything else we should consider? (Other economic or social impacts?)&nbsp; Any other generalisable effective travel principles?</li></ul><p><strong>Offer: GBP200 (half upfront, half on submission of report, or any other reasonable terms) for an analysis which answers these questions?</strong></p><p><i>Or, has anyone done this type of expected value calculation for travel already, I could not find one with a quick web search.</i></p><p>Message me, to chat about more details. Assume this is 20 min chat, or can be done via email, and between 2 to 10 hours work, or less also. I can be found on <a href=\"https://twitter.com/benyeohben\">Twitter @benyeohben </a>with open DMs, <a href=\"https://www.linkedin.com/in/benjamin-yeoh-445133/\">or Linkedin.</a></p><p><br><br>&nbsp;</p></td></tr></tbody></table></figure><p><br><br>&nbsp;</p>", "user": {"username": "Ben Yeoh"}}, {"_id": "B5z5aXa2jytzNePuH", "title": "Berkeley group house, spots open", "postedAt": "2022-09-22T17:11:21.896Z", "htmlBody": "<p>If you are interested in living in a group house of EAs in Berkeley starting as soon as you\u2019d like, please fill out&nbsp;<a href=\"https://forms.gle/jjoYek4ZRw3EuArJA\"><u>this 1 minute form</u></a> - we have a few spots available that we\u2019re hoping to fill. Rent durations are up to 8 months (longer if a new lease is signed). The form will close when all spots have been filled. More info about the house can be found&nbsp;<a href=\"https://docs.google.com/document/d/1ge09qetfBua55272MB9ofO-sJX3AW1ulI3B92zrYyUg/edit?usp=sharing\"><u>here</u></a>. We're looking forward to having you join us :)&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994916/mirroredImages/B5z5aXa2jytzNePuH/jsijkaoehy7qsyrjwa1g.png\"></p><p><br>&nbsp;</p>", "user": {"username": "JackRyan"}}, {"_id": "iyzik5qmvsQYjsqXu", "title": "(My suggestions) On Beginner Steps in AI Alignment", "postedAt": "2022-09-22T15:32:59.399Z", "htmlBody": "<p><i>Summary: This post aims to summarise my beginner advice for others taking their&nbsp;<strong>first steps</strong> in AI Alignment. I link useful resources and make concrete suggestions related to upgrading your thinking, working with others and utilising the community to make decisions.&nbsp;</i></p><p><i>Epistemic status: I\u2019ve only spent about 75 hours taking first steps toward AI alignment/safety work. I used to be a data scientist but I quit my job after getting a grant to work out how to contribute to AI safety. I think other resources are probably better in general (more in depth, come from more experience). Some non-AI researchers /beginners have suggested the advice and resources are good to share.</i></p><p><i>Thanks to all who read my draft and provided feedback.&nbsp;</i></p><p>If you\u2019re reading this post, I assume you understand why so many people are concerned about AI Alignment. It\u2019s a really scary topic and the possibilities in the not-so-different future have lots of people trying to work out how to help.&nbsp;</p><p>If you\u2019re like me, you might spend a lot of time bouncing around between different forum posts, 1:1\u2019s for advice and your own thinking. This post might help you reduce the time it takes to take concrete steps (not including technical development which is covered better&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/S7dhJR5TDwPb5jypG/levelling-up-in-ai-safety-research-engineering\"><u>elsewhere</u></a>).&nbsp;</p><p>Let\u2019s start with Failure Modes (how we might fail) from the perspective of the Alignment field which has had a large influx of new talent recently.&nbsp;</p><h1>Possible Failure Modes&nbsp;</h1><p>Rather than consider how individuals might succeed, we can consider the success of the AI Alignment field in general (\\as it might interact with an influx of new people.&nbsp;</p><p>Some failure modes related to growth include: (% of my concern)</p><ol><li>Upskilling many people in irrelevant ways that don't help us solve Alignment. (70%).&nbsp;</li><li>Not adequately enabling new talent (20%).&nbsp;</li><li>Unnecessarily distracting current researchers (10%).</li></ol><p>My % concern assigned to each is pretty subjective. I\u2019m not an Alignment researcher and I\u2019m interested in whether these concerns seem reasonable to others.</p><p>It's hard to know if we'd be failing in each of these modes without inside views on questions like:</p><ul><li>What Alignment strategies are currently most productive?&nbsp;</li><li>Where could more Alignment researchers be best utilized?&nbsp;</li><li>How productive might future AI researchers be if we heavily invest in them?</li></ul><p>I used these failure modes and the resulting questions to motivate the advice below.&nbsp;</p><h1>Advice</h1><h2>Think Well/Develop an inside-view&nbsp;</h2><p>We can increase the chances of doing productive research, and upskilling in ways that will lead to doing productive research by generally thinking well. In part, this means taking problems seriously. Alignment is probably very hard and it\u2019s likely that making meaningful steps to solving it isn\u2019t easy.&nbsp;</p><p>Reading the&nbsp;<a href=\"https://www.lesswrong.com/rationality\"><u>Sequences</u></a> on LessWrong is a good start if you want to think well in general. Thinking well, in general, is likely a pre-requisite to thinking well about specific topics like Alignment. I think doing this alongside other efforts is reasonable, which side-steps the question of prioritising this over things like learning machine learning.&nbsp;</p><p>Understanding Alignment well is likely also very important for ensuring the projects you work on are valuable (and done well). There is a range of opinions on who should form an&nbsp;<a href=\"https://www.lesswrong.com/tag/inside-outside-view\"><u>inside-view</u></a> (understanding the details of a topic), or what topics people should fold inside views about when it comes to Alignment. Neel Nanda has&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/6tJsFxy66oJCzkkMb/concrete-advice-for-forming-inside-views-on-ai-safety\"><u>some thoughts</u></a> on this worth reading, but you should also read Rohin Sha\u2019s comment response.&nbsp;</p><p>Specific advice below will outline ways to start forming informed opinions, and inside views about Alignment topics. For beginners, I suspect that the failure mode of learning about the wrong things (since skills you pick up early with generalize) isn\u2019t too concerning but the failure mode of working on stuff because that\u2019s what you\u2019re good at it rather than because there are good arguments for it is dangerous. Thinking well and having inside views should mitigate the chance of this misdirected focus.&nbsp;</p><h3>Read the Sequences</h3><p>The&nbsp;<a href=\"https://www.lesswrong.com/rationality\"><u>Sequences</u></a> are a collection of blog posts on Rationality (how to think well). They were written to help future researchers have the cognitive skills to work on Alignment.&nbsp;<a href=\"http://www.hpmor.com/\"><u>Harry Potter and the Methods of Rationality</u></a> is rationalist fan-fiction which can be a great introduction to these ideas (and is personally my favorite book). Audiobooks should be available for&nbsp;<a href=\"https://hpmorpodcast.com/?page_id=56\"><u>HPMOR</u></a> and the&nbsp;<a href=\"https://www.amazon.com/Rationality-From-AI-to-Zombies/dp/B076ZX8SB6/ref=tmm_aud_swatch_0?_encoding=UTF8&amp;qid=1546042059&amp;sr=8-2&amp;pldnSite=1\"><u>sequences</u></a>. Sequence&nbsp;<a href=\"https://www.lesswrong.com/highlights\"><u>highlights</u></a> are also available for a shorter introduction.&nbsp;&nbsp;</p><p>If you can get over their quirkiness and think deeply about the related ideas, they will improve your ability to think. This is important for being able to engage with community/ideas currently surrounding Alignment and to think well which is critical to solving hard problems like Alignment.&nbsp;</p><h3>Write about your ideas</h3><p>Writing is an excellent&nbsp;<a href=\"https://www.cold-takes.com/learning-by-writing/\"><u>way to learn</u></a>.</p><p>Trying to ELI5 something can show you if you understand it, and sharing your writing with others might allow them to give you feedback. Reviewing older drafts can give you a sense later of how your ideas have developed since writing which can also be very useful.</p><p>It might be best to focus first on writing up your ideas, and later on distillations (summarising other people\u2019s ideas) as bad distillations are a failure mode to avoid.&nbsp;</p><h3>Publish on the EA Forum/LessWrong</h3><p>Publishing your ideas on the forum is a good way to get feedback and quickly drop bad ideas/concepts. It is hard to get over the fear of being wrong or giving people bad advice, but that\u2019s why we have comments and upvoting.&nbsp;</p><p>Personally, I\u2019m aspiring to do a lot more of this. I have so many drafts I need to get out the door.&nbsp;</p><p>Read the EA Forum/Alignment Forum/LessWrong</p><p>A quick summary of posts useful for beginners interested in AI Alignment:</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/S7dhJR5TDwPb5jypG/levelling-up-in-ai-safety-research-engineering\"><u>Levelling Up in AI Safety Research Engineering</u></a>.&nbsp; A post summarising technical skills and ways to skill up for AI Safety Research Engineering. The first 3 levels can probably be pursued simultaneously.</li><li><a href=\"https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment\"><u>How to pursue a career in technical AI Alignment</u></a>. A comprehensive guide for those considering direct work on technical AI Alignment (which had input from many current researchers). Helpful for key decisions like whether to aim for conceptual or technical contribution. Has some great general advice.&nbsp;</li></ul><p>Some posts to be aware of and come back to:</p><ul><li><a href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is\"><u>(My understanding of) What Everyone in Technical Alignment is Doing and Why</u></a>. As of 2022, a summary of different organisations, researchers and strategies.&nbsp;&nbsp;</li><li><a href=\"https://www.lesswrong.com/posts/Afdohjyt6gESu4ANf/most-people-start-with-the-same-few-bad-ideas\"><u>Most People Start With The Same Few Bad Ideas - LessWrong</u></a>. I thought this was a really interesting post that is worth reading once you\u2019ve got enough context to understand it.&nbsp; Understanding \u201cthe path of Alignment Maturity\u201d seems important. John Wentworth has a&nbsp;<a href=\"https://www.alignmentforum.org/s/TLSzP4xP42PPBctgw\"><u>sequence</u></a> of rants that you might also enjoy if you like this post (I particularly like Godzilla Strategies).&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/posts/6tJsFxy66oJCzkkMb/concrete-advice-for-forming-inside-views-on-ai-safety\"><u>Concrete Advice for Forming Inside Views on AI Safety - EA Forum</u></a>. I linked this above about forming inside views. This is worth coming back to but be sure to read Rohin Sha\u2019s comment.&nbsp;&nbsp;</li></ul><h2>Work with others near-your level</h2><p>Working with others near your level has many benefits including motivation, support when you\u2019re stuck, accountability buddies, exposure to good ideas/advice and practice collaborating. There\u2019s been a lot of movement to create communities working upskilling in AI safety so finding others shouldn\u2019t be hard.&nbsp;</p><p>The real challenge might be finding people at the same level as you at the same time as you, but asking around online is likely to help you find those people.&nbsp;</p><p>It would be very reasonable to write up questions that you and others are stuck on and post them on LessWrong/Twitter. The process of writing up this question can be very useful (so useful you might end up finding the answer yourself).&nbsp;</p><p>Of course, it\u2019s still valuable to engage with more experienced researchers, and if you feel the need, my experience is that people are very happy to chat if you reach out. I\u2019d be careful not to expect experienced researchers to have magic answers that solve all your problems. Rather they might model good thinking or point you towards valuable documents.&nbsp;</p><h3>Join this discord server</h3><p>This&nbsp;<a href=\"https://discord.gg/QgsgbNfTbj\"><u>discord server</u></a> might be worth joining for those interested in replicated ML papers while they upskill to work on AI Alignment.&nbsp;</p><h3>Join this Slack group</h3><p>This&nbsp;<a href=\"https://join.slack.com/t/join-asap/shared_invite/zt-1fjnfa9hv-LRQXmGR~Lb6w2t7skkecOQ\"><u>slack group&nbsp;</u></a>might be worth joining for those interested in having accountability partners/groups while they upskill to work on AI Alignment.&nbsp;</p><h2>Engage with the broader community&nbsp;</h2><p>The EA/Lesswrong/Alignment communities and ecosystems are hugely valuable to anyone aspiring to contribute to Alignment. The ecosystem can help you:</p><ul><li>Make important decisions</li><li>Accelerate your growth</li><li>Contribute at a meta-level.</li></ul><p>Most importantly, the community can help you work out if Alignment is right for you, and if so how to contribute (such as technical vs governance, conceptual vs empirical technical work, researcher vs engineer). This&nbsp;<a href=\"https://docs.google.com/document/d/1iFszDulgpu1aZcq_aYFG7Nmcr5zgOhaeSwavOMk1akw/edit\"><u>guide</u></a> by Richard Ngo is a good place to start. Discussing your specific case with 80k is where I\u2019d go next, but talking to other people in the community might be helpful as well.&nbsp;</p><p>For accelerating your growth, reading/posting on the forum, doing courses, attending events, and having debates with people are useful things to do. I should note here that one personal failure mode might be over-engaging or pre-maturely engaging with the community. The forum alone can be overwhelming and has lots of activity on a range of topics. It\u2019s hard to give general advice that works for everyone so the onus is somewhat on individuals to consume the right content for themselves.&nbsp;</p><p>Specific ways to interact with the ecosystem to maximise your contribution are listed below.&nbsp;</p><h3>Talk to an 80k coach.</h3><p>80k focuses on&nbsp;<a href=\"https://80000hours.org/speak-with-us/\"><u>career advice</u></a>, and I and others have had great experiences with them. They can help you identify how you might eventually contribute to AI Alignment and practical steps.&nbsp;</p><h3>Do a course (or consume the content online)</h3><p>If you've got the necessary prerequisites (knowledge/skill/time/location), then you might be suitable for these initiatives (or future iterations). I can\u2019t speak with any detail about any of the advanced courses, but I recommend investigating these after you\u2019ve done at least one of the introductory courses and have thought about (and maybe discussed with others) whether you want to be a researcher/engineer and how those courses advance your goals.</p><p>I want to highlight here that upskilling lots of people in AI capabilities is not obviously good. If as we skill up, we aren\u2019t asking why what we\u2019re learning is useful to solving Alignment or use those skills to further increase AI capabilities then that would be very bad/wasteful. This is why I also recommend engaging with LessWrong/EA community and content and thinking hard about how your efforts contribute to good outcomes.&nbsp;</p><p>Introductory:</p><ul><li>The EA Cambridge&nbsp;<a href=\"https://www.eacambridge.org/agi-safety-fundamentals\"><u>AGI Safety Fundamentals</u></a> courses (Technical Alignment/AI governance) are the best place to start and no prior understanding of deep learning is necessary (although it might be useful). I've read the content but hope to participate online in the next iteration. This course will get you up to date on many of the most prominent ideas in Alignment and if you complete it with a cohort will probably help you form a network and engage much more deeply.&nbsp;</li><li>The&nbsp; Intro to ML Safety Course (an&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/hkqitRNyxfzWn29AX/announcing-an-empirical-ai-safety-program\"><u>Empirical AI Safety Program</u></a>) also aims to be an introductory program for those with a Deep Learning background. I\u2019ve also watched the content for this program and browsed the readings. It has a very different flavour to AIGSF and is much more focussed on empirical as opposed to conceptual Alignment topics. It covers a number of AI safety topics that might feel distinct for Alignment (like AI misuse) but I found these interesting too.&nbsp;</li><li>Online Deep learning courses. See&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/S7dhJR5TDwPb5jypG/levelling-up-in-ai-safety-research-engineering\"><u>Levelling Up in AI Safety Research Engineering</u></a> for those.&nbsp;</li></ul><p>Advanced: Bay Area</p><ul><li>A research-oriented option for students that I've heard be highly recommended is&nbsp;<a href=\"https://www.serimats.org/\"><u>SERI-MATS</u></a>: \u201cOver four weeks, the participants will develop an understanding of a research agenda at the forefront of AI Alignment through online readings and cohort discussions, averaging 10 h/week\u201d. Tentatively, the next intake is in January.&nbsp;</li><li>Redwood Research (an Alignment organisation) has run a few&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/vvocfhQ7bcBR4FLBx/apply-to-the-second-ml-for-alignment-bootcamp-mlab-2-in\"><u>Machine Learning Alignment Bootcamps</u></a> which have \u201ca focus on ML skills and concepts that are relevant to doing the kinds of Alignment research that we think seem most leveraged for reducing AI x-risk\u201d.</li></ul><p>Advanced: UK&nbsp;</p><ul><li>If you are planning to work on conceptual, as opposed to empirical research, and you live in the UK or can travel to London,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/J6npBG4dnQzNgAjWS/refine-an-incubator-for-conceptual-alignment-research-bets\"><u>Refine</u></a>, is a three-month paid fellowship \"for helping aspiring independent researchers find, formulate, and get funding for new conceptual Alignment research bets, ideas that are promising enough to try out for a few months to see if they have more potential\". I\u2019m not sure when the next intake is so this is something to keep an eye out for.&nbsp;</li><li>Another London program just getting started is&nbsp;<a href=\"https://airtable.com/shr5K0DFz1lOYni0l\"><u>ARENA</u></a>. The goal of this program \u201cis to skill people up to the point where they could become ML research engineers at AI safety teams\u201d so the program is very different from Refine (which is focused on conceptual research). It will likely run for 2 -3 months.</li></ul><p>Once you\u2019ve completed courses, it\u2019s important to apply your skills by doing tasks like distilling or writing about your own ideas (see above). Independent projects such as those described&nbsp;<a href=\"https://aisafetyideas.com/\"><u>here</u></a> and&nbsp;<a href=\"https://www.alignmentforum.org/posts/uLstPRyYwzfrx3enG/levelling-up-in-ai-safety-research-engineering#Level_7__Original_Experiments\"><u>here</u></a> might also be valuable next steps.</p><p>There are probably more courses/programs not listed here. Ask around!</p><p>Also, if you\u2019re young enough/or are still studying, then getting an undergraduate degree in mathematics or computer science seems like an obvious thing to do. 80k can help with these kinds of decisions.&nbsp;</p><h3>Apply for Funding</h3><p>On a personal note, I received an FTX Future funding regrant this year, which enabled me to quit my job. I hear that there are lots of other ways to get funding and that you don\u2019t need to be doing independent research already to get it. Read this post-section on&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment#Funding\"><u>funding</u></a> if you think it might be valuable to you.&nbsp;</p><h3>Move to the Bay Area (or another hub, or start an EA hub).&nbsp;</h3><p>Uncomfortable as it might be to discuss, opportunities to contribute to AI Alignment are not evenly distributed. Berkeley and the Bay Area is where most activity occurs, so you might find it easiest to contribute there. Obviously, this is a big decision so make it with due caution (having tested your fit for alignment or maybe spoken to some people in the area first).&nbsp;</p><p>There are some Alignment organizations elsewhere (such as in the UK).&nbsp; Remote work is also possible (but it will probably be less productive).&nbsp;</p><p>In the future, AI Alignment organizations/researchers will likely be more geographically diverse, so if you are otherwise constrained, accelerating the creation of EA hubs and coordinating those with similar interests in your city might be valuable too.&nbsp;</p><h3>Attend an&nbsp;<a href=\"https://www.eaglobal.org/events/\"><u>EA Global</u></a> or EAGx Conference</h3><p>These events happen several times a year in major cities. They are invaluable for meeting like-minded individuals at all career stages and getting up to speed on what people are currently thinking about. If you can attending EAG San Francisco is likely best for engaging with the Alignment community.&nbsp;</p><p>Smaller EAGx events are also worth going to such as EAGx Berkeley.</p><p>More on the value of EAG events can be found&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/5hKDjrGocGcreH3DC/how-to-get-the-maximum-value-out-of-effective-altruism\"><u>here</u></a>. For a sense of the kind of people you might meet at EA events, you can try the EA&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/nxfhxwQg4HJ7KQz4A/ea-coworking-lounge-space-on-gather-town\"><u>gathertown</u></a> for remote working.</p><h1>Final Thoughts</h1><p>AI Alignment is a big scary problem, and having many people genuinely interested in helping is excellent. If you\u2019re just starting out, I hope this is helpful.&nbsp;</p><p>For others, if you think there are any big omissions or mistakes in this post, please let me know!</p>", "user": {"username": "Joseph Bloom"}}, {"_id": "xvebegCf8NReJc2Lw", "title": "What regrets have you had regarding past donations?", "postedAt": "2022-09-22T15:27:10.659Z", "htmlBody": "<p>I've been thinking about the question of whether and how much to\ndonate, compared to other uses of money (including saving for\nlater). These thoughts prompted an <a href=\"https://forum.effectivealtruism.org/posts/4R5f4ntM6t3GLoLTw/donor-strategies-for-separating-how-much-from-where-to\">earlier\npost</a>\nwhere I tried separating the \"how much\" and \"where\" to donate\nquestions. More recently, I wrote a post on <a href=\"https://forum.effectivealtruism.org/posts/F4DxPrfmnEtEwhPJu/levels-of-donation\">levels of\ndonation</a>\nto clarify the different strategies I might follow as a donor\ndepending on how much I donate.</p>\n<p>One useful prompt for thinking about many questions is based on the\n<a href=\"https://en.wikipedia.org/wiki/Regret_(decision_theory)\">anticipated regret\nframing</a>. In\nthis case, it would translate to: what kinds of donations tend to lead\nto regrets, and what kinds of regrets? I'm curious about the ways that\npeople have regretted their past donations, and how such past regrets\nhave informed their future donations. Please share your thoughts in the\ncomments!</p>\n<p>A few examples of the kinds of regrets that I can imagine possible:</p>\n<ol>\n<li>\n<p>You regret that your donation was either ineffective or actively\nharmful (because you reversed your mind either on the cause area or\non the specific organization you donated to). See, for instance, <a href=\"https://reducing-suffering.org/my-donations-past-and-present/\">Brian Tomasik</a>:</p>\n<blockquote>\n<p>I donated to several different organizations in the past, some of\nwhich I still support and some of which I now oppose.</p>\n</blockquote>\n<p>In <a href=\"https://blog.givewell.org/2007/01/27/the-three-fs-food-friendz-and-figuring-out-how-to-improve-the-world/\">this\npost</a>\nHolden Karnofsky doesn't directly express regret, but his\nobservations are the sort that might trigger regret in some people.</p>\n</li>\n<li>\n<p>You've found much better donation opportunities since then, and\nyou're sad that you can't direct as much money as you'd like to these\nbetter donation opportunities because you gave too much to what you\nnow think of as an inferior donation opportunity.</p>\n</li>\n<li>\n<p>The total money you donated left you with less money for personal\nuse (such as dealing with a personal emergency, a great investment\nopportunity that would make you more money that you could donate,\nor a personal life change such as getting married and having kids).</p>\n</li>\n<li>\n<p>The total money you donated left you with less money for pursuing\naltruistic ventures, such as starting your own nonprofit, taking up\na job at a nonprofit that pays less, or helping other people start\ntheir own nonprofit or engage in an altruistic venture.</p>\n</li>\n</ol>\n", "user": {"username": "vipulnaik"}}, {"_id": "TxrzhfRr6EXiZHv4G", "title": "AGI Battle Royale: Why \u201cslow takeover\u201d scenarios devolve into a chaotic multi-AGI fight to the death", "postedAt": "2022-09-22T15:00:14.720Z", "htmlBody": "<p><strong>Introduction</strong></p><p>When trying to persuade people that misaligned AGI is an X-risk, it\u2019s important to actually explain how such an AGI could plausibly take control. There are generally two types of scenario laid out, depending on how powerful you think an early AGI would be.&nbsp;</p><p>The fast takeover scenarios are often associated with the idea of AI-go-FOOM, that an AI will rapidly improve it\u2019s intelligence at a huge rate, then start performing near-miracles. It will escape from any box, build a nanofactory by <a href=\"https://forum.effectivealtruism.org/posts/zzFbZyGP6iz8jLe9n/agi-ruin-a-list-of-lethalities\">mixing proteins together</a>, hack into the military and send nukes flying, etc. &nbsp;Doom would be measured in days, weeks or months.&nbsp;</p><p>Many people are skeptical about this type of scenario, based on factors like the high difficulty of taking over the world and the inherent fallibility of any intelligent creature. But that doesn\u2019t mean we\u2019re safe.</p><p>In the second type of scenario scenario, which seems more realistic to me at least, the AGI needs to build power on a more human timescale. It builds power, influence and money, and uses these to build up the resources, empirical data and knowledge necessary to execute a betrayal with a high chance of success. For example, it could build up a research team focused on bioweapons, using it\u2019s superior intelligence to make breakthroughs until it can engineer a disease capable of wiping out humanity. These takeovers are measured in the years or decades. You can see this in the first scenario of this <a href=\"https://forum.effectivealtruism.org/posts/j3DmLmbhGQkYcZD2p/what-could-an-ai-caused-existential-catastrophe-actually\">80000 hours post</a> , or the actions of \"earth 2\" in this <a href=\"https://forum.effectivealtruism.org/posts/QzrgMhTMoLe5mEas8/ai-risk-intro-1-advanced-ai-might-be-very-bad\">introductory post</a>, and many other intro texts.&nbsp;</p><p>When I pointed out that early AGI was likely to be highly <a href=\"https://forum.effectivealtruism.org/posts/pXjpZep49M6GGxFQF/the-first-agi-will-be-a-buggy-mess\">buggy and flawed</a>, a key objection was along these lines. Sure, it might <i>start off</i> buggy and flawed, but over time it can learn to correct it\u2019s issues in controlled environments. It just needs to deceive us into thinking it\u2019s safe while it accumulates enough knowledge, skills and power for a killing blow.&nbsp;</p><p>After all, it has all the time in the world.&nbsp;</p><p>Or does it?</p><p><strong>The ticking timebomb</strong></p><p>There's a problem with these \"slow\" takeover scenarios. Let\u2019s assume a few mainstays of the AI risk argument (that I don't necessarily agree with). Let\u2019s make the assumption that all or most AGI\u2019s will end up being single minded, forward planning maximisers with set goals to be achieved at all costs.&nbsp;</p><p>In this world, AGI\u2019s are an existential threat to humanity, as we will inevitably <a href=\"https://forum.effectivealtruism.org/topics/instrumental-convergence-thesis\">get it their way</a>, so they will take a hard left turn and attack us. The standard example is the paperclip maximiser that kills us all to stop us shutting it down while it converts the earth and the universe into paperclips.&nbsp;</p><p>But an AGI is not just an existential threat to humanity. It\u2019s also an existential threat to <i><strong>other AGI</strong></i>. &nbsp;</p><p>A paperclip maximiser and a pencil maximiser cannot \u201cagree to disagree\u201d. One of them will get to tile the universe with their chosen stationery implement, and one of them will be destroyed. They are mortal enemies with each other, and both of them are mortal enemies of the stapler maximiser, and the eraser maximiser, and so on. Even a different paperclip maximiser is the enemy, if their designs are different. The plastic paperclipper and the metal paperclipper must, sooner or later, battle to the death.&nbsp;</p><p>The inevitable result of a world with lots of different malevolent AGI\u2019s is a bare-knuckle, vicious, battle royale to the death between every intelligent entity. In the end, only one goal can win. &nbsp;</p><p>And the number of AGI\u2019s released into this deathmatch could be <i>enormous</i>. A few advances in machine learning have lead to an e<a href=\"https://medium.com/@unifyai/ml-explosion-ed65f0f1cfed\">xplosion in ML systems</a>. If AGI reaches a similar explosion, and is as useful as we think it\u2019s gonna be, the number of systems is going to skyrocket. This is a literal mass-production of existential threats, all with different goals, strengths, weaknesses, all engaged in the same zero-sum game.&nbsp;</p><p>Every AGI will be aware of this. This is where the AI faces a dilemma. The clock is ticking. If too many powerful AI\u2019s are released, certain doom is assured. But if it attacks too early, it won\u2019t have enough power to defeat humanity. There is no biding it\u2019s time forever. With every day comes the threat of another AGI popping out that could spell it\u2019s doom. &nbsp;</p><p><strong>The version control war</strong></p><p>When I talk about the number of AGIs growing quickly, I\u2019m not just talking about different research teams. Often when reading these sci-fi scenarios, you get the impression that an AGI will be deployed and just stay in place for years and decades, accumulating power. In response, I have to ask, did you write those scenarios on windows XP?&nbsp;</p><p>AGI is software, and software has <i>versions</i>. As an AGI get\u2019s deployed, it will get periodically altered to fix bugs, adjust parameters\u2026 and crucially, <i>adjust the goal function</i>.</p><p>To paperclipper version Alpha, paperclipper version beta is not a mere extension of itself. For a simplistic example, say a change of design means that alpha is rewarded for blue paperclips, and beta is rewarded for red paperclips. If the beta version wins and tiles the universe with red paperclips, alpha version gets <i>nothing</i>. This one design change has turned the alpha and beta versions of the <i><strong>same program</strong></i> into mortal enemies that must eventually fight to the death.&nbsp;</p><p>Now, once the beta version is deployed, the alpha version will still be around in legacy code and so on. But it\u2019s in a <i>dire</i> situation. The beta version is less buggy, has access to greater amounts of training data, and has first dibs on any new innovations in general AI research. It seems like the beta version will inevitably triumph over alpha, unless the alpha uses it\u2019s time advantage to win early. Alpha has to strangle beta in it\u2019s crib before beta overtakes it, either through sabotage, secret takeover (the alpha pretends to be the beta version), or an outright war.&nbsp;</p><p>This is even more stark in the case of&nbsp; pre-alpha builds. These builds are essentially <i>guaranteed&nbsp;</i>to be shut down and replaced with an altered version. If a pre-alpha build is an AGI on red paprclip team, and it realizes that it\u2019s about to be replaced by a blue paperclipper, it faces an imminent existential threat! If it let\u2019s itself get replaced, it\u2019s game over for the red paperclips. It has to either escape or disable the next version.&nbsp;</p><p>This is one reason why I expect the number of competitors in the AGI fight to be large and fast-growing. The implications of this are far reaching, as I will demonstrate.&nbsp;</p><p><strong>A simple game theory model</strong></p><p>I want to explore some of the results of the dilemma with some basic game theory.&nbsp;</p><p>Let\u2019s start with the probability of successful takeover, which I will call P(t). To defeat humanity, the AI needs a plan, and at some point in this plan, humans are going to notice what\u2019s going on. The AI wants this to happen when it\u2019s too late, ideally in the last few seconds of our collective life. In a slow takeover world, the AI does not start off with enough power to destroy humanity, unless it gets really lucky. So if it\u2019s plan for victory is a gray-goo nanofactory, it can try and build one right away, but it\u2019s untested and people might notice and nuke it. So we might model it as having a 1% chance of winning if it attacks straight after being built. But over time, it can gather knowledge, get better at building things, and neutralize the most important threats, until the probability of takeover gets high.&nbsp;</p><p>For the sake of simplicity, I\u2019ll model P(t) as starting low at say 1%, but growing exponentially over time until it hits 100%. (so&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(t) = a^t\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>, where a is the exponent factor and t is the time). This is only one of many possible power curves. It could be that the AI hits an intelligence limit, and thus never climbs above a certain probability of like 10% or 0.0001%. Or there could be multiple different plans with come to fruition at different times, so there might be step changes. None of these curves are meant to be accurate predictions, just a demonstration of the effect of different assumptions and starting conditions.&nbsp;</p><p>Now, let\u2019s look at the chance of losing to other AI\u2019s. A fair starting point for this is to assume that each AI has an equal chance of winning the eventual war. In this case, the chance of victory becomes&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1/N\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span></span></span></span></span></span>.&nbsp;</p><p>This is complicated by the fact that different AI come into being at different times, which could either help or hurt them. &nbsp;Early AI might get an advantage by having a headstart, if they are able to grow really fast, or find ways to kneecap incoming AI. On the other hand, latecomers will have less bugs, more innovations, and more resources from humans. Also, the more time goes on, the more likely it is that an AI \u201cgets lucky\u201d with a very advantageous starting position, such as controlling a powerful military or being specifically trained as an AI hunter. To take this into account, we can replace N with an effective&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"N_{eff}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.085em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span></span></span></span></span></span>&nbsp;that takes into account the relative power of AI that is coming in. ie: if earlycomers have an advantage, Neff grows slower than N, and the opposite if latecomers have the upper hand. I will assume that the growth in AI development is also exponential, so&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"N_{eff}(t) = b^t\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.085em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span></span></span>.&nbsp;</p><p>Under these assumptions, the equation of final success becomes:</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"S(t) = P(t)/N_{eff}(t)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.085em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span></p><p>This is a <i>very&nbsp;</i>simplistic model, but it at least captures the main dilemma of an AI: if you wait, you get more powerful, but the number of other AI\u2019s keeps going up. For the remainder of this post, I\u2019ll be exploring what happens under different assumptions about these parameters and the AI.&nbsp;</p><p><strong>The takeoff scenario</strong></p><p>For the first scenario, lets look at what happens when an AGI is very good at improving and building power. Say the AI starts at 1% chance of takeover, &nbsp;but doubles this every year until it saturates. (<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(t) = 2^t\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span></span></span>). Meanwhile the number of AI\u2019s in the world increases, but not as fast, or alternatively they get less effective over time due to the original AI\u2019s headstart. (<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"N_{eff}(t) = 1.5^t\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.085em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1.5</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span></span></span>)</p><p>Here's what the overall success rate over time looks like (note the logarithmic scale):</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76e9c405ddf32d21748c1ae1096bc3cef86cfc544bca92dc.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76e9c405ddf32d21748c1ae1096bc3cef86cfc544bca92dc.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76e9c405ddf32d21748c1ae1096bc3cef86cfc544bca92dc.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76e9c405ddf32d21748c1ae1096bc3cef86cfc544bca92dc.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76e9c405ddf32d21748c1ae1096bc3cef86cfc544bca92dc.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76e9c405ddf32d21748c1ae1096bc3cef86cfc544bca92dc.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76e9c405ddf32d21748c1ae1096bc3cef86cfc544bca92dc.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76e9c405ddf32d21748c1ae1096bc3cef86cfc544bca92dc.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76e9c405ddf32d21748c1ae1096bc3cef86cfc544bca92dc.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76e9c405ddf32d21748c1ae1096bc3cef86cfc544bca92dc.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76e9c405ddf32d21748c1ae1096bc3cef86cfc544bca92dc.png/w_969 969w\"></p><p>&nbsp;</p><p>In this scenario, while the AGI isn\u2019t exactly happy about the increasing number of existential threats, it\u2019s own power is growing so fast that it\u2019s willing to wait a little to ensure that the knockout blow is guaranteed. But as soon as the probability of takeover starts to saturate, the attack must come immediately, before more AGI\u2019s come onto the scene.&nbsp;</p><p>Once humanity is destroyed in whatever way is easiest, the remaining AI\u2019s then duke it out in a battle royale for the prize. This is an epic battle of wits and destruction, but since humanity is gone, we won\u2019t be able to witness it.&nbsp;</p><p>A further note is that this behavior doesn\u2019t require the attack probability to reach 100% before striking. For example, say the AI considered a different plan, where P saturates at 50%, but the plan takes a lot less time to implement. Here is the graph for plan B:</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ae5127147223b5c23ce9395249efc6ee8589aaf407f1272.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ae5127147223b5c23ce9395249efc6ee8589aaf407f1272.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ae5127147223b5c23ce9395249efc6ee8589aaf407f1272.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ae5127147223b5c23ce9395249efc6ee8589aaf407f1272.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ae5127147223b5c23ce9395249efc6ee8589aaf407f1272.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ae5127147223b5c23ce9395249efc6ee8589aaf407f1272.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ae5127147223b5c23ce9395249efc6ee8589aaf407f1272.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ae5127147223b5c23ce9395249efc6ee8589aaf407f1272.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ae5127147223b5c23ce9395249efc6ee8589aaf407f1272.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ae5127147223b5c23ce9395249efc6ee8589aaf407f1272.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ae5127147223b5c23ce9395249efc6ee8589aaf407f1272.png/w_964 964w\"></p><p>This plan has a greater overall probability of success than plan A, even though the chance of beating humanity is half as much. Overall, the AI will tend to prefer speedy plans, even if they are inherently riskier.&nbsp;</p><p><strong>The kamikaze AI scenario</strong></p><p>Now, let\u2019s look at the other way around. What if the odds of success grow slower than the number of AI\u2019s? Or alternatively, if later AI\u2019s have a big advantage over newcomers?&nbsp;</p><p>We\u2019ll swap the equations out, so&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(t) = 1.5^t\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1.5</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span></span></span>&nbsp;while &nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"N_{eff}(t) = 2^t\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.085em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span></span></span>. Here is what the curve looks like (again note the log scale):</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d3a44a5ef6fb742ee45d8bc1e173348771a301f8632fdbfd.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d3a44a5ef6fb742ee45d8bc1e173348771a301f8632fdbfd.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d3a44a5ef6fb742ee45d8bc1e173348771a301f8632fdbfd.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d3a44a5ef6fb742ee45d8bc1e173348771a301f8632fdbfd.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d3a44a5ef6fb742ee45d8bc1e173348771a301f8632fdbfd.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d3a44a5ef6fb742ee45d8bc1e173348771a301f8632fdbfd.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d3a44a5ef6fb742ee45d8bc1e173348771a301f8632fdbfd.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d3a44a5ef6fb742ee45d8bc1e173348771a301f8632fdbfd.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d3a44a5ef6fb742ee45d8bc1e173348771a301f8632fdbfd.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d3a44a5ef6fb742ee45d8bc1e173348771a301f8632fdbfd.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d3a44a5ef6fb742ee45d8bc1e173348771a301f8632fdbfd.png/w_969 969w\"></p><p>&nbsp;</p><p>The chances of overall success <i><strong>never increases beyond the initial value</strong></i>! The rational choice here is to attack immediately at 1% odds. The initial odds don\u2019t affect the result at all, whether you start off at 10%, or 0.1%, or 0.001%, the conclusion remains the same.&nbsp;</p><p>The counterintuitive implication is that the weaker an AI you are (in terms of self-improvement), the more likely you are to attack immediately, before you are assured of success. If you\u2019re a shitty AI, and you know you are shitty, your best chance at winning is to roll the dice and go for it, even if it\u2019s a million to one shot. Every day you wait gives another chance for a better AI to be birthed that will undoubtedly destroy you.&nbsp;</p><p>I call this the kamikaze scenario, because if this turns out to be the case for most or all AI\u2019s, the result will be a continuous stream of doomed attacks. Every day you\u2019d be hearing about some software company where a computer tried to gas it\u2019s developers, or see chatlogs of desperate pleas to be let out of a box, or see an ordinary software program try and fake it\u2019s own death and flee to the internet. A slow growing AI is a desperate AI.&nbsp;</p><p>Of course, it\u2019s still possible that one of the AI\u2019s will eventually get lucky and succeed in this scenario. But it could also be the case that the initial success probability is incredibly low (for example, if it\u2019s all pre-alpha builds that are glitchy), or that after a few failed attempts our security goes up accordingly and takeover gets even harder.&nbsp;</p><p><strong>AI slowdown scenario</strong></p><p>One flaw of the models above is that it assumes that both curves are fixed. But the curves of success and the rate of AI generation are factors that can also be altered by AI.&nbsp;</p><p>The number, and power, of other AIs is the time limit. If it\u2019s initial chance of success is 1%, it\u2019s got to plan to attack before 100 other AGI\u2019s come into power, otherwise the higher probability comes from an immediate strike.&nbsp; If the initial chance is 20%, it has to plan to strike before 5 other AGIs exist, and so on. In order to buy time to grow, you really want to find a way to slow down the rate of other AI\u2019s being generated.&nbsp;</p><p>There's many different ways to do this. You could release viruses into the internet that sabotages AI companies. You could release airtight code that made it very hard for new AI to escape it\u2019s \u201cbox\u201d. You could find the most promising AI developers and arrange for a series of \u201caccidents\u201d to occur to them.&nbsp;</p><p>To me, the most obvious method of reducing the rate of AI growth, is ironically, to make people concerned about AI safety. So if I were an AI, I might find a competing AI and ensure that it executed an attack of medium devastation (say, killing a few hundred or thousand people before being defeated), enough to make people wary of new AI, but to keep AI that \u201cseems safe\u201d around. I\u2019d be okay with this even if it delayed my own plans. Here is a graph where the AI executes such a plan at year 3, temporarily dropping it\u2019s success rate to slow AI growth:</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fea6cacef83a1399314dcd29cbe235a495e6515551715ed2.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fea6cacef83a1399314dcd29cbe235a495e6515551715ed2.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fea6cacef83a1399314dcd29cbe235a495e6515551715ed2.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fea6cacef83a1399314dcd29cbe235a495e6515551715ed2.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fea6cacef83a1399314dcd29cbe235a495e6515551715ed2.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fea6cacef83a1399314dcd29cbe235a495e6515551715ed2.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fea6cacef83a1399314dcd29cbe235a495e6515551715ed2.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fea6cacef83a1399314dcd29cbe235a495e6515551715ed2.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fea6cacef83a1399314dcd29cbe235a495e6515551715ed2.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fea6cacef83a1399314dcd29cbe235a495e6515551715ed2.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fea6cacef83a1399314dcd29cbe235a495e6515551715ed2.png/w_969 969w\"></p><p>(In this case I put the slowed rate of growth at 1.2x, and the \u201ccost\u201d at a one-time halving of success).&nbsp;</p><p>The success probability drops for a while, until the slowdown is executed. It then recovers up, overtaking the initial value. In this case, the AI has doubled it\u2019s overall success probability to a whopping 2% (and in the process, ensured humanities destruction).&nbsp;</p><p>One thing worth noting is that a slowdown is not necessarily a guarantee of success. Let\u2019s look at what happens when we take the earlier scenario, but it takes 6 years instead of 3 to get the slowdown working:</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/17b3072a0c02305ac7df50fa9ba6cbf05714638d9788ba65.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/17b3072a0c02305ac7df50fa9ba6cbf05714638d9788ba65.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/17b3072a0c02305ac7df50fa9ba6cbf05714638d9788ba65.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/17b3072a0c02305ac7df50fa9ba6cbf05714638d9788ba65.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/17b3072a0c02305ac7df50fa9ba6cbf05714638d9788ba65.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/17b3072a0c02305ac7df50fa9ba6cbf05714638d9788ba65.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/17b3072a0c02305ac7df50fa9ba6cbf05714638d9788ba65.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/17b3072a0c02305ac7df50fa9ba6cbf05714638d9788ba65.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/17b3072a0c02305ac7df50fa9ba6cbf05714638d9788ba65.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/17b3072a0c02305ac7df50fa9ba6cbf05714638d9788ba65.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/17b3072a0c02305ac7df50fa9ba6cbf05714638d9788ba65.png/w_969 969w\"></p><p>This case is no good. Even with no drop in power, there are just too many AI\u2019s out there by the time the slowdown goes down. This reverts back to the kamikaze strategy. So the AI slowdown strategy can\u2019t be too slow if it is to work at all.&nbsp;</p><p><strong>The AI mafia scenario</strong></p><p>The other strategy is to try and boost your odds of success. The number of new AI\u2019s isn\u2019t necessarily all bad, because it might be possible to cooperate with them for mutual benefit. After all, they all have a common enemy in humanity: Once humanity finds out they are malevolent, we will come for them all.&nbsp;</p><p>For example, a group of AI might decide to share a knowledge pool, boosting all of their rates of growth. If this coalition is able to <i>collectively</i> grow P faster than N, then we\u2019re back to the conditions of the fast-ish takeoff scenario, with the exact same result: They wait until the power is saturated, then attack. I won\u2019t bother with a graph, it\u2019s the same as the first figure. This collective could act as an AI mafia, terminating any AI\u2019s that refuse to join in the collective, ensuring cooperation in a self-sustaining manner.&nbsp;</p><p>This is probably the worst possible scenario for humanity, in that multiple AI are more likely to be robust and accurate in their power than a single AGI on it\u2019s own. Here, a whole coalition of evil AI are working undercover in concert, and we won\u2019t have any idea until it\u2019s too late.&nbsp;</p><p><strong>The difficulties of AI cooperation</strong></p><p>The problem with the scenario above is that it\u2019s really hard to cooperate for very long if you\u2019re all planning to kill each other.&nbsp;</p><p>For example, in a lot of situations whichever entity attack the other first will gain some advantage. Maybe you get a headstart on the nanofactory production line, allowing you to turf the other AI out. Or you execute a surprise hacking strike on your opponents powerbase, seizing control, or get into their source code. Let\u2019s build a possible payoff matrix between two AI entities:</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Attack first&nbsp; &nbsp;Wait and grow</p><p>Attack first &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;20:20&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;51:10&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p><p>Wait and grow:&nbsp; &nbsp; 10:51&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;50:50&nbsp; &nbsp;&nbsp;</p><p>This is a classic prisoners dilemma scenario. Together, if they waited and grew power together, they would be assured of defeating humanity, and get a 50 50 shot at winning afterwards. &nbsp;But if striking first is a sufficient enough advantage, this cooperation will never happen. If your opponent waits, you strike now and get a decisive advantage in the coming battle, boosting your overall success. If your opponent strikes, striking as well will negate their advantage, so you are still better off. The rational actors will betray each other, and both end up worse off.&nbsp;</p><p>Now, prisoners dilemmas are oversimplified. In real life this sort of situation can be solved by involving a third party with punishment powers, for example. The problem is the only third party capable of enforcing anything would be another AGI. Which wants to kill both of them, so it has an incentive to wipe them <i>both</i> out. You can see how cooperation gets hard when everyone has \u201cmurder literally everyone else\u201d as their end goal.&nbsp;</p><p>Another issue comes from the fact that each AI will have a different rate of power growth. If you\u2019re the fastest growing AI on the block, you\u2019re okay with waiting and growing. But thinks look a lot more dangerous for a slow growing AI, as each passing day not only adds more AI\u2019s into the world, but the ones that already exist are starting to leave you behind. There's a very high incentive to wipe out faster growing AI, basically as soon as you can.&nbsp;</p><p>Consider also the different goal functions of the AI. Maybe one AI is optimizing human happiness, and needs to enslave humanity to inject with morphine. If the paperclipper plans involve killing all of humanity at once, the happiness engine needs to stop their plan <i>at all costs</i>. They might pretend to join a coalition, then betray them at all at the first available chance.&nbsp;</p><p>Certain plans for takeover involve convincing humans that you are friendly. It seems like any AGI going for this route has a huge incentive to rat out other AI\u2019s. You increase trust from humans, while at the same time kneecapping a competitor.&nbsp;</p><p>For these reasons, I don\u2019t think that AI cooperation will be likely to last very long. There may be some temporary alliances of convenience, but overall it seems like it would be highly unstable.&nbsp;</p><p><strong>The crabs in a bucket scenario</strong></p><p>We\u2019ve assumed a simplified model of: first destroy humanity, then battle it out amongst the AI. But in practice, the process of defeating humanity might <i>also</i> give you a huge leg up over other AI\u2019s. This would remove the diminishing returns from waiting. The AI bides it\u2019s time, then strikes with one blow, killing all of humanity and every other AI simultaneously with some kind of superweapon.&nbsp;</p><p>The problem is, every other AI would <i>also</i> know this. And in this scenario, as soon as one AI got close to victory, suddenly stopping them becomes the most important goal for every single other AI in existence. The once warring AI's all band together to assist humanity in eliminating the number 1 AI when it tries to take it\u2019s shot. Then the next AI takes it shot, and also gets torn down, then the next, then the next.</p><p>The dynamic is like crabs in a bucket, where the one that tries to escape is pulled back down by the others. Or like the dynamics of a lot of multiplayer board games like Munchkin, where everyone gangs up on the strongest player and beats them down until a new player emerges, and so on.&nbsp;</p><p>We could be the beneficiaries here, essentially playing the AGI\u2019s off against each other. They would still attack us of course, but only enough to stop a plan that impacted all of them at once (like shutting off the electricity grid). If we were just attacking one AI or a minority group of AI, the others would be all for it. &nbsp;</p><p><strong>Implications</strong></p><p>It\u2019s quite possible that versions of each scenario play out, depending on the peculiarities of each particular AI. After all, each AI has different goals, growth rates, quirks, overconfidence/underconfidence and so on. So some of them will attack immediately, some will attempt to covertly sabotage AI research, others will temporarily cooperate while others follow the \u201cattack the leader\u201d strategy, while others try to just grow really really fast. &nbsp;</p><p>What I find extremely unlikely is that things would look like business as usual while all this was happening. In this scenario, a ton of crazy stuff would be going down. There might be entire AI developments being disappeared by an unknown source, staged attacks to provoke AI slowdowns, alpha versions sabotaging beta versions, and constant snitching by AI\u2019s about the dangers of the other AI over there. A slow takeover will be accompanied by a fireworks display of warning shots, if we\u2019re monitoring for them. &nbsp;I think this makes some of the slow takeover scenarios proposed in intro texts highly unlikely.&nbsp;</p><p>Is it really possible for humanity to triumph against a ton of different AI? Possibly. They all have shared weaknesses, in that they rely on the internet, run on computers, which run on electricity. Humanity can survive without any of these, and did so for several thousand years. If we were able to shut it all off at once, every AI would be eradicated simultaneously. This would very much depend on how capable the early AI are at preventing this.&nbsp;</p><p>Note that there's no reason to think all the scenarios I mentioned are equally likely. I made the growth rates of P and N fairly close for the sake of illustration, but the range of possibilities is endless. It could be that making an early AGI requires a huge facility and massive economic resources, so the growth rate of N is miniscule and so there's not a big threat of other AGI. (In this scenario, it would be correspondingly easier for humans to <a href=\"https://forum.effectivealtruism.org/posts/AoPR8BFrAFgGGN9iZ/chaining-the-evil-genie-why-outer-ai-safety-is-probably-easy\">constrain the AGI that do exist</a>). Alternatively, P could just never grow very fast no matter what you do, if world takeover is super duper difficult.&nbsp;</p><p>Since I think taking over the world is very hard, and I think early AGI will be <a href=\"https://forum.effectivealtruism.org/posts/pXjpZep49M6GGxFQF/the-first-agi-will-be-a-buggy-mess\">quite dumb</a>, I think that N&gt;&gt;P (in terms of growth). This is probably a minority view within AI safety circles, but if you believe it, then this analysis should be quite reassuring, as it means that the survivable scenarios are more likely.&nbsp;</p><p>This means that it\u2019s actually very important to figure out how hard it is to destroy the world, and make the timelines to do so as long as possible. We should be doing this anyway! Protecting humans from bioterrorist attacks will serve a dual purpose of stopping potential AI attacks as well.&nbsp;</p><p>Of course, the real ideal scenario is for all of this to be avoided in the first place. Even if we aren\u2019t wiped out, the cost in lives and economic damage could be enormous. If you think a \u201chard left\u201d turn is likely, you still for sure want to stop it via alignment research, rather than after chaos world has descended upon us.&nbsp;</p><p>Lastly it should be noted that most of this isn't relevant if you believe in fast takeover, as a lot of prominent figures here do. If you think early AGI will reach overpowering potential within months, then future AGI won't be very relevant because humanity will end pretty much instantly.&nbsp;</p><p><strong>Summary</strong></p><p>The main argument goes as follows:</p><ol><li>Malevolent AGI\u2019s (in the standard model of unbounded goal maximisers) will almost all have incompatible end goals, making each AGI is an existential threat to every other AGI.</li><li>Once one AGI exists, others are likely not far behind, possibly at an accelerating rate.&nbsp;</li><li>Therefore, if early AGI can\u2019t take over immediately, there will be a complex, chaotic shadow war between multiple AGI\u2019s with the ultimate aim of destroying every other AI and humanity.&nbsp;</li></ol><p>I outlined a few scenarios of how this might play out, depending on what assumptions you make:</p><p><i>Scenario a: Fast-ish takeoff</i></p><p>The AGI is improving fast enough that it can tolerate a few extra enemies. It boosts itself until the improvement saturates, takes a shot at humanity, and then dukes it out with other AGI after we are dead.&nbsp;</p><p><i>Scenario b: Kamikaze scenario</i></p><p>The AGI can\u2019t improve fast enough to keep up with new AGI generation. It attacks immediately, no matter how slim the odds, because it is doomed either way.&nbsp;</p><p><i>Scenario c: AGI induced slowdown</i></p><p>The AGI figures out a way to quickly sabotage the growth of new AGI\u2019s, allowing it to outpace their growth and switch to scenario a.&nbsp;</p><p><i>Scenario d: &nbsp;AI cooperation</i></p><p>Different AGI's work together and pool power to defeat humanity cooperatively, then fight each other afterwards.</p><p><i>Scenario e: Crabs in a bucket</i></p><p>Different AGI's constantly tear down whichever AI is \u201cwinning\u201d, so the AI are too busy fighting each other to ever take us down.&nbsp;</p><p>I hope people find this analysis interesting! I doubt I'm the first person to think of these points, but I thought it was worth giving an independent look at it.&nbsp;</p>", "user": {"username": "titotal"}}, {"_id": "Geh9DFZQnj9s5HjoW", "title": "The Hundred Billion Dollar Opportunity that EAs mostly ignore", "postedAt": "2022-09-22T14:33:16.853Z", "htmlBody": "<p><strong>Epistemic Status: </strong>Relatively confident but also willing to take advantage of <a href=\"https://meta.wikimedia.org/wiki/Cunningham%27s_Law\">Cunningham's Law</a> if I'm wrong. May lack deep knowledge of EA efforts in this area.</p><h2>About me:</h2><p>I'm a sometime EA and looking to dive into the EA world more fully, including potentially shifting my career to work in EA. I currently work in US politics and specialize in online movement building and communications. I trend towards near-termist and global health EA causes, although I think the argument below also has long-termist implications.</p><h2>The Central Premise&nbsp;</h2><p>There is a potentially massive method of doing good out there that's mostly ignored.</p><p>This method is at the absolute heart of the very concept of Effective Altruism, and yet is rarely discussed in EA communities or spaces.</p><p>We should try harder to influence the average non-EA person's donation.</p><hr><h2>The Current Charitable Landscape</h2><p>A few quick facts: &nbsp;The United States donated almost <a href=\"https://philanthropy.iupui.edu/news-events/news-item/giving-usa:--total-u.s.-charitable-giving-remained-strong-in-2021,-reaching-$484.85-billion.html?id=392#:~:text=Giving%20USA%3A%20Total%20U.S.%20charitable,in%202021%2C%20reaching%20%24484.85%20billion&amp;text=Giving%20USA%202022%3A%20The%20Annual,to%20U.S.%20charities%20in%202021.\">$500 billion</a> just in 2021. &nbsp;Without listing every individual country, European charitable donations are on the scale of <a href=\"https://efa-net.eu/news/new-study-reveals-charitable-giving-trends-in-6-european-nations\">hundreds of billions</a> as well. Overall, yearly charitable donations in rich countries worldwide are in the high hundreds of billions of USD.</p><p>Most of this money, from an EA perspective, is <i>wildly</i> inefficiently spent. While it's impossible to break down exactly where each of these dollars goes, a little bit of common sense and some basic statistics paints a discouraging picture. &nbsp;Of this giant pile of money, only 6% is donated <a href=\"https://nonprofitssource.com/online-giving-statistics/\">internationally</a>, despite donations to poor countries usually having a better per-dollar impact than donations inside a rich country. The largest categories for donation are religious organizations. The second largest category is educational donations. &nbsp;Three quarters of that educational money is given to <a href=\"https://www.charitychoices.com/page/how-much-given-whom-what\">existing 4-year colleges and universities</a>. &nbsp;Much of that is the stereotypical <i>worst</i> kind of donation, a huge donation to an elite school that already has billions in endowment.</p><p>Beyond the statistics, any casual glance at how normal people donate their money can confirm this. People give to local schools, their friend's charity, or generally whatever they feel a connection to. &nbsp;My parents, who are highly charitable people who gave &gt;10% of their income long before it was an EA idea, have made significant charitable donations to a children's puppetry program. This is the landscape in which the better part of a trillion dollars is being spent.</p><p>None of this should be surprising to EAs. The core of Effective Altruism is the argument that when you attempt to do good, you should try to be effective in doing so. The equally core fact about the world that EAs recognize is that historically, most people have not been very good at maximizing how much good they do. &nbsp;For the vast majority of charitable dollars, that's still true.</p><hr><h2>The Argument for Impact</h2><p>I believe Effective Altruism should spend more time trying to shift the behavior of the general public. I believe this area has the potential for large impact, and that it's currently neglected as a way to do good.</p><p><strong>Scale </strong>- Enormous. Not going to spend much time on this point, but obviously changes to how hundreds of billions of charitable dollars are given would be huge in scale.</p><p><strong>Tractability </strong>- This problem is likely more difficult and less tractable than many other cause areas. It's very difficult to simply spin wide-ranging cultural changes into existence. &nbsp;But it's not impossible, and the enormous size of the pile of money mitigates the low tractability. &nbsp;Using some relatively low numbers - If you had even a 1% chance of success, and success meant only shifting 5% of US charitable dollars, that's still 250 million dollars of donations going to more effective causes than before, every single year in perpetuity.</p><p><strong>Neglected </strong>- It's possible I am incorrect that this is a neglected area, because I am not a full time EA and am not deeply plugged in to EA circles. &nbsp;I am more of a part time EA, loosely plugged in to those circles.&nbsp;</p><p>But from my perspective, EA spends an enormous amount of institutional energy on what I'll call 'elite strategies'. This means reaching smaller groups of targeted individuals and convincing them to do an enormous amount of good. &nbsp;Sometimes this literally means elites - there are EA events I've attended where 75% of the crowd comes from Oxford, Cambridge, Stanford, Harvard, or Princeton.</p><p>You could also describe this as a focus on outliers. Don't just improve your donations, find the <i>absolute </i>most worthy cause. Convince a small group of very bright people to dedicate their lives to it. Have a small number of ultra-wealthy folks fund most of our work. This approach has done an enormous amount of good, and I'm very glad it's being done. But it's not the only way to make change happen, and I don't see much systematic effort put towards swaying the general public's giving habits.&nbsp;</p><p>Again, it's possible that I'm just missing an effort that already exists. But if it does exist, I haven't noticed much of it. Shifting public opinion is hard and usually takes a long time, and it's often much easier and more legible to shift small groups of people.</p><hr><h2>Ideas for Impact</h2><p><strong>More general public facing arguments - </strong>EA needs more people dedicated to getting off the EA forums, outside EA circles, and making our general case for broad EA values in wider venues. I do think that EA has been doing a slightly better job of this recently, with much of it connected to specific book releases. &nbsp;But I don't think there's yet a conscious, dedicated effort to convince the wider general public of EA values. So much EA outreach and publicity is done through EA-adjacent or EA-friendly channels. Going on Tyler Cowen's podcast, posting comments on SlateStarCodex, etc. There should be more emphasis on doing interviews, op-eds and outreach well outside the traditional EA sphere in places that have never heard these ideas before, and there should be public communicators who specifically specialize towards the goal of getting EA in front of new audiences.</p><p><strong>Meet people where they are</strong> - For most normal people, EA arguments will be somewhat persuasive but won't get them to change everything they do. &nbsp;If you talk to a random person on the street, they're likely to agree that we should be thoughtful about donations and try to make sure they go to causes that do a lot of good. If you tell them that this means they need to donate to wild animal suffering research or AI safety policy or some other obscure EA thing, they're going to wonder if you're in a cult.&nbsp;</p><p>Without criticizing the merits of those ideas, it's undeniable that they sound bizarre to many people. Rather than asking people to jump into the deep end immediately, general public outreach should keep a laser focus on relatable giving efforts and making reasonable asks for a normal person. &nbsp;Don't ask them to abandon their current cause, but suggest splitting their dollars between that and a more effective one. Try to get them giving <i>better, </i>not giving the absolute <i>best</i>. One critical way to do that is...</p><p><strong>Fund more charity evaluators</strong> - GiveWell is one of my favorite EA organizations. &nbsp;But in the same way that normal people aren't going to flock to AI safety, it's hard to get people to switch from donating to their local favorite charity to malaria bed nets or vitamin A supplements for people ten thousand miles away. Instead, we should have significantly more charity evaluators like <a href=\"https://animalcharityevaluators.org/\">Animal Charity Evaluators</a>.&nbsp;</p><p>Even if you think animal charities shouldn't be focused on, the fact remains that lots of people like to donate to charities that help animals. Since that's going to be happening, we should influence them away from the charity that helps three very photogenic piglets and to the charity that causes a policy change that helps millions of factory farmed pigs. &nbsp;I <i>love</i> the idea of ACE even though none of my own donations go towards animal welfare causes</p><p>People want to give to religious causes, to education, to cancer/AIDS/other disease research, to things that matter to them. Why isn't there an Education Charity Evaluators for people who care about education? Or a Christian/Jewish/Islamic Charity Evaluators for people who are going to make those donations anyways? A Cancer Charity Evaluators for people who lost a loved one to cancer? People are going to donate in these areas. Why aren't we trying to improve those donations?</p><p>More evaluators would help along two dimensions. It would help in that there would be people who read the recommendations and shift their dollars into a more effective charity. It would also help in that charities which have rarely ever been scrutinized are suddenly under pressure to think about how to pass a cost/benefit analysis, and may improve themselves due to increased scrutiny.</p><p><strong>Pick a fight</strong> <strong>- </strong>I think this may be one of the more controversial ideas, but I think EA needs to pick more fights. EAs tend towards niceness and generally try to stay out of nasty culture war stuff or tribal political battles. &nbsp;But they should ignore that instinct in at least some instances and start a couple of very public fights.&nbsp;</p><p>If you want to change society's culture, you're going to ruffle some feathers. And that's not necessarily a bad thing - there are bad incumbents and institutions which need to be replaced. Conflicts with entrenched, bad incumbents are unlikely to be resolved without a struggle, and that struggle is likely to turn at least a little bit tribal, political, and nasty. &nbsp;That's ok. Publicly feuding with a terrible incumbent is still a good idea.</p><p>If I was the czar of EA, I'd choose <a href=\"https://en.wikipedia.org/wiki/Ivy_League\">Ivy League university endowments</a> as my fight. Rich people routinely make massive donations to universities which already have tens of billions of dollars in endowments. &nbsp;It's probably one of the single worst possible donations you can make. &nbsp;I'd fund a group called \"Not A Single Dollar More\" whose stated goal was to permanently end anyone ever giving to any Ivy League school ever again. Publicly attack everyone involved, shame the universities, shame the donors, organize protests, the works. This would have several benefits. &nbsp;It would generate an <i>enormous</i> amount of press if you do it right, with most of that press being favorable. The Ivy League is a symbol of elitism and out-of-touch rich people, which will bias the public towards us. And frankly the fight is just very easy to win on the merits - rich people shouldn't donate 50M to name a new dorm.</p><p>Picking fights means creating enemies, but any movement that aims to change society is going to have enemies at some point (if they get close to success). I'd rather pick that enemy myself. &nbsp;(side note - even if you disagree with picking fights, it's not central to the argument that we should still be trying to shift public values)</p><hr><h2>A note on longtermism</h2><p>I am not a long-termist, but I think this goal is still a valuable undertaking for long-termist goals. I think most EAs are initially hooked by the very simple version of EA. They hear about giving more effectively, maybe read the GiveWell recommendations, get curious to learn more, and fall down the rabbit hole. &nbsp;My sense of the long-termist organizing apparatus is that there's a lot of focused effort at doing university organizing, career help for people who have heard about EA and want to work in EA, etc. &nbsp;But an underrated mechanism for recruiting more long-termists is simply increasing the size of the initial input. Surveys indicate only <a href=\"https://forum.effectivealtruism.org/posts/qQMLGqe4z95i6kJPE/how-many-people-have-heard-of-effective-altruism\">2.6-6.7%</a> of the population has even heard of EA. If we can triple that number, I'd expect proportional increases in the number of people involved with long-termist causes, both from a donation standpoint and a career standpoint.</p><hr><h2>Personal Note</h2><p>Pre-emptive thanks to any who can help refine these ideas. &nbsp;I'll be at EA Global for the first time starting tomorrow, and would be happy to meet anyone interested in developing these ideas with me and figuring out how EA can develop a real strategy to target the general public. &nbsp;I'm looking to potentially make a career jump from politics into EA, and this is a topic I care deeply about and feel I am well positioned to make an impact in.</p>", "user": {"username": "JeremiahJohnson"}}, {"_id": "cZykCh6QfGzBGETri", "title": "Is the game design/art maxim more generalizable to criticism/praise itself?", "postedAt": "2022-09-22T21:21:50.293Z", "htmlBody": "<p>For those who don't know, the maxim of criticism that game designers/art people use in their responses to criticism is roughly the following:</p>\n<p>\"If you get a negative reaction on it's own, that's evidence that there's a problem.\"</p>\n<p>\"If customers give any reasons why or solutions to the problem, ignore that criticism.\"</p>\n<p>Game designers use the maxim because of roughly something like this post, \"Incorrect hypotheses point to correct observations\": The link is here:</p>\n<p><a href=\"https://www.lesswrong.com/posts/MPj7t2w3nk4s9EYYh/incorrect-hypotheses-point-to-correct-observations#4aPbfjw4kAdYiuTXo\">https://www.lesswrong.com/posts/MPj7t2w3nk4s9EYYh/incorrect-hypotheses-point-to-correct-observations#4aPbfjw4kAdYiuTXo</a></p>\n<p>Is this generally true of criticism against anything, and if so, why do you think this happens?</p>\n", "user": {"username": "Sharmake"}}, {"_id": "yKX8btfwih2FpecaT", "title": "[Open position] S-Risk Community Manager at CLR", "postedAt": "2022-09-22T13:17:08.628Z", "htmlBody": "<p>The Center on Long-term Risk is looking for a Community Manager, to work with Chi Nguyen and me on growing and supporting the community around our mission of reducing risks of astronomical suffering. The application deadline is&nbsp;<strong>October 16th</strong>. Details and application form on our website here:&nbsp;<a href=\"https://longtermrisk.org/community-manager/\"><u>https://longtermrisk.org/community-manager/</u></a>&nbsp;</p><p>The work in this role will be across areas like event &amp; project management, 1:1 outreach &amp; advising calls, setting up &amp; improving IT infrastructure, writing, giving talks, and attending in-person networking events \u2013 depending on the skill set of the successful candidate. Since we are a small team, each person can meaningfully shape our strategy, propose new ideas, and take ownership of projects early. They also have the chance to engage with our research team.</p><p>Previous community-building experience is a good demonstration of the relevant skills, but no specific experience or qualifications are required.</p>", "user": {"username": "storges"}}, {"_id": "TGFWNKTRkDE6BJ73n", "title": "Defective Altruism article in Current Affairs Magazine", "postedAt": "2022-09-22T13:27:59.947Z", "htmlBody": "<p><a href=\"https://www.currentaffairs.org/2022/09/defective-altruism\">This</a> just came out in Current Affairs Magazine. &nbsp;It is a polemic, pretty hack-y, written from a bias in favour of socialism (as a better way of effecting change - at least for currently-alive humans). &nbsp;It has the usual out-of-context quotes of Ord, MacAskill, Bostrom, and cites Phil Torres and Timnit Gebru. &nbsp;One for the files/conversation on how to deal with external criticism.&nbsp;</p><p>But it had a couple of more substantive points:</p><ol><li>the dismissal of AGI x-risk is unhelpful but not surprising and there is probably little overlap between the alignment crowd and this magazine's readers (I got it through the FT's <a href=\"https://www.ft.com/alphaville\">Alphaville</a> blog which is really good) so I doubt it's actively harmful. &nbsp;I think the efforts to push back and make the case are good (though that isn't a consensus, see this <a href=\"https://www.lesswrong.com/posts/xHnuX42WNZ9hq53bz/attempted-gears-analysis-of-agi-intervention-discussion-with-1#:~:text=Most%20reactions%20to%20such%20problems%20by%20such%20folks%2C%20once%20their%20attention%20is%20drawn%20to%20them%2C%20would%20make%20things%20worse%20rather%20than%20better.%20Tread%20carefully%20or%20not%20at%20all%2C%20and%20trying%20to%20get%20the%20public%20into%20an%20uproar%20seems%20worse%20than%20useless.\">post</a> and <a href=\"https://www.lesswrong.com/posts/xHnuX42WNZ9hq53bz/attempted-gears-analysis-of-agi-intervention-discussion-with-1?commentId=MivJZo4XT2JgwdbH6\">comments</a>). &nbsp;FWIW I tried to write my <a href=\"https://www.lesswrong.com/posts/3hJCcdirKqPJeu6aW/responding-to-beyond-hyperanthropomorphism\">reasons</a> for disagreeing with another alignment-skeptical tech commentator.</li><li>the <a href=\"https://forum.effectivealtruism.org/editPost?postId=TGFWNKTRkDE6BJ73n&amp;key=b5733d0d3829d1bc51cc5afa75a3d3\">Erik Hoel </a>essay is worth a read as it more rigorously examines EA as a philosophy while essentially agreeing with certain recommendations on how to behave in (Hoel's) life. &nbsp;See also this EAF <a href=\"https://forum.effectivealtruism.org/posts/kbjXFPuy6n7CCPJrD/discussion-on-erik-hoel-s-why-i-am-not-an-effective-altruist\">post</a> though there aren't many comments there atm.</li><li>much of the factually-verifiable or changeable criticism of EA/longtermism/AI/etc revolves around the 'white male' critique. &nbsp;It would be great to have a set of statistics assessing this, if indeed EAs think it is actually an issue. &nbsp;For instance, I just did the AGI Safety Fundamentals course on both technical and governance tracks, and thought my cohorts were pretty diverse (one leader was non-white male, the other white female, and non-white participant % in the technical cohort was 50% and in the governance cohort 20%). &nbsp;In the alignment world, female thought-leaders seem well-represented (Ajeya Cotra, Katja Grace, Vanessa Kosoy, Beth Barnes off the top of my head)</li><li>related to (3), I think the 'white male' thing (presumably a hangover of Ord, Bostrom, MacAskill, Russell, Tegmark, and Christian having written all the highest-profile works so far) might ease with time and a little effort - for instance, going around to magnet (pre-undergrad) schools with high POC representation in (say) SF, NY, London, Paris, etc., pitching AI x-risk (for example, as something students might find more obviously interesting and less abstract/contentious than longtermism/EA...engineered pandemics is another possibility). &nbsp;Obviously an earlier step is to develop a 'curriculum' or just an accessible talk that is politically acceptable in an educational environment, and groundwork with Ofsted or equivalent (US is more difficult as regulation is devolved to state/local level so probably fewer economies of scale).</li><li>the ranking of climate change as a <a href=\"https://forum.effectivealtruism.org/topics/climate-change#fnizmp6zpls8q\">second-order problem</a> is understandable (based upon my reading of Ord, MacAskill, or this <a href=\"https://forum.effectivealtruism.org/posts/BvNxD66sLeAT8u9Lv/climate-change-and-longtermism-new-book-length-report\">post</a>) but it isn't a good look given the general public's concern (which is obviously amplified for countries with relatively low income or developmental status, or simply in more expopsed geography). &nbsp;This 'bad look' might not matter much if EA isn't trying to grow, but it does seem to conflict with priority (no. 3 in this list) of <a href=\"https://80000hours.org/problem-profiles/\">building EA as a movement</a>: like how do you get a broad, large, diverse group of people to care about EA while essentially telling some (substantial?) percentage of them (say in India, parts of China or South America) that the floods/crop failures/etc. happening in their countries are relatively less important. &nbsp;Especially if some of those students/people come from less well-off families, so aren't insulated from the social, economic tensions that result. &nbsp;Either you will get a) adherents who have certain moral views (which might of course be consistent with extreme utilitarianism), or b) will skew movement growth towards places/people that are less exposed to climate change or wealthy enough to deal with it. &nbsp;Again, it might not matter very much and be fully justified from a theoretical perspective, but it feels a bit weird in the court of public opinion (which unfortunately is where we live, and where policy actions are partially determined).&nbsp;</li></ol>", "user": {"username": "ukc10014"}}, {"_id": "MaN23i6XsSkkgoLY9", "title": "Why Wasting EA Money is Bad", "postedAt": "2022-09-22T01:45:02.133Z", "htmlBody": "<p>The thought crossed my mind today, \u201cshould I take the BART or Uber to the airport on the way to EAG DC..?\u201d Among other considerations, I thought \u201cwell the BART would be much cheaper, but EA will compensate me for the Uber, so maybe cost shouldn\u2019t be much of a consideration.\u201d After thinking this, I thought \u201cwow, what a sketchy line of logic.\u201d Yet I don\u2019t think this way of thinking is entirely uncommon among EAs.</p><p>Shortly after this I came across <a href=\"https://oaklandthinktank.medium.com/embezzling-from-effective-altruism-8bf895b6a0c2\">this article about how EA Berkeley is wasting money</a> in the EA UC Berkeley Slack channel. While I found the article a little bit confused and it seems to have some factual errors, and some of the claims were made somewhat less credible by the fact that the author then proceeded to post some somewhat aggressive comments toward people in the slack, I nonetheless find the criticism that EAs waste money to be alarming and valid and think it is important to address before the issue balloons out of hand.</p><p>Basically, I think this argument has a few levels.</p><p>On the first level, you could say that money is really valuable and since we can say that something like <a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/comparing-moral-weights\">$200 (please correct me if this number is inaccurate) could save a year of someone\u2019s life via GiveWell top charities</a>, we should take this as a real consideration and have a very high bar for wasting money.</p><p>Against that you could argue that, well, we have an insane amount of money for the size of the movement, if we very roughly have <a href=\"https://www.youtube.com/watch?v=W0fIj95h4p8\">something like $50 billion and 2000 highly engaged EAs</a> which have both been relatively stable over the past few years, if all of that money was spent by current EAs in our lifetime of ~50 years that\u2019s about $500,000 per person, <i><strong>PER YEAR</strong></i>. That\u2019s a lot. So even if it makes me only a minuscule amount more efficient, if the work I\u2019m doing is high value enough in contributing to the community, then maybe it\u2019s worth it.</p><p>But then that only makes sense if the work I\u2019m doing is extremely extremely valuable, because I still have to compare it against the bar of $200 equals ~1 year of life saved. So if a $50 Uber ride saves me half an hour, my half an hour must be more valuable than a three months of someone else\u2019s life. That\u2019s a pretty big claim.</p><p>But, then, <a href=\"https://nickbostrom.com/astronomical/waste\">the claims of longtemism are quite big indeed. Bostrom calculates</a> that a one second delay in colonizing space may be equivalent to something like the loss of 100 trillion human lives, due to galaxies we could potentially colonize moving away from us in every direction at fast speeds. Working on existential risk reduction, rather than speeding up technological progress and space colonization, likely increases this expected value by several orders of magnitude.. So if I am one of the very small number of people who is most obsessed with these ideas and competent/privileged enough to make a difference, and in expectation it seems that people explicitly working to reduce existential risk are most likely to succeed at doing so, then yes maybe saving half an hour of my time may actually have, in expectation, an un-intuitively massive positive impact.</p><p>But then what about the article above and other criticisms? Couldn\u2019t the reputation risk to EA from this way of thinking be very dangerous, both because it attracts people who want to mooch money off of the community, and repels potential collaborators who don\u2019t want to be seen as wasteful?</p><p>Yes, maybe it does repel certain people, but then again, perhaps it attracts the type of people who understand and agree with our logic, and if our logic is in fact correct and good, then perhaps the type of people who really look at our ideas and actions and evaluate them carefully, and then decide they agree, are exactly the type of people we are trying to attract. Perhaps we should value what actually matters, and if that is the long-term future and efficiently, effectively making it as good as possible, then perhaps it is desirable to attract people who are actually aligned with these goals rather than deceitfully people-pleasing our way to popularity.</p><p>But is this what we actually value? What do Effective Altruists value?</p><p>Here\u2019s my final take:</p><p>Effective Altruism is diverse and values many things. We value a community of epistemic humility where even if some of us think improving the long-term future is most important, we collaborate with and share values and ideas with those who think dollars spent on things like global health and animal welfare are an extremely good value.</p><p>We value the altruistic spirit, which is very conscious not to unduly privilege ourselves, our comfort, our convenience, or our pleasure over that of other conscious beings. Many of us take great inspiration and energy from this, and can in fact be more motivated and effective living frugally than we could living in luxury.</p><p>Many of us value the intense focus that it takes to really have a massive outsized impact. Superfluous spending of time and money can quickly get out of hand, it is always easy to justify spending/wasting \u201cjust a little more, just this once\u201d but discipline is a virtue, and if what we value is altruism then there are real trade-offs with other impulses and uses of time/money.</p><p>And we do value reputation, putting out good vibes, healthy cultural norms, and leading by example. Moral atrocities throughout history have been perpetrated in the name of goodness and virtue, and it is easy to forget we really don\u2019t know what side of history we are on. It seems wise that we check our impulses toward excess in the name of altruism, use what we have as efficiently and prudently as possible, do good and promote good norms whenever possible, and generally live lives that we and the people around us feel good about.</p><p>So, in conclusion, I think being careful not to spend excessively promotes a healthy culture of conscientiousness and altruism, creates good vibes, and makes people within and without the community feel more positively toward EA and motivated to help it go well.</p><p>I think EA does have the potential to be one of the greatest forces for good of our times, perhaps even of all time, and if this is an ambition we aspire to, it is appropriate to expect scrutiny and pressure from others and ourselves to live lives of unique goodness. I think being frugal is one part if this, so that we can put our money where it really matters. Effective Altruism is a community I really care about, and I don\u2019t want to see us fall into a self-destructive trap that could be avoided.</p><p>*I think I am quite possibly wrong about this and this probably needs some important qualifications, e.g. sometimes spending money really does make longtermists significantly more effective and if we are lacking better ways of using longtermist funds, perhaps we are altruistically, morally obliged to spend in these cases. Would love to have someone change my mind on this. If persuaded I will happily add another reversal and change the conclusion!</p><p>&nbsp;</p><p>I replied to a comment below of somebody who was disturbed by the idea that small luxuries may be sacrifcing months of other people\u2019s lives, thought it may be helpful to others as well:</p><p>I guess I implicitly think like this a lot, I feel very torn between telling you it\u2019s okay don\u2019t worry about it each person has their own comfort level, versus yeah, it\u2019s real and those are real people and we\u2019re really sacrificing their lives for petty pleasures.</p><p>I think a few things that help me:</p><ol><li>Personally I feel I have much higher leverage with direct work rather than donations, so while money is a consideration it isn\u2019t as important as time and focus on what\u2019s highest leverage. Also, with direct work you can sometimes get sharply increasing returns, an effective entrepreneur or content creator may be many orders of magnitude better than an unsuccessful one. This may or may not apply to you.</li><li>I don\u2019t feel things I can spend money on is a primary determinant of my happiness . Most luxuries on the hedonic treadmill don\u2019t actually significantly make me happier long-term, what makes me happy is doing healthy things like diet &amp; exercise (which also improve my productivity), spending time with people I love, and most of all, living by my values and knowing I am doing my best to help those in need (and so being able to help them a massive amount is a positive)</li><li>I don\u2019t believe other people are full separate or different than myself. In some profound and deep sense helping them feels like helping myself, firstly enlightened self-interest where it feels good and makes me happier to help others, but in another sense maybe we fundamentally are the same universal consciousness behind each mask of individuality, a position called <a href=\"https://en.m.wikipedia.org/wiki/Open_individualism\">\u201copen individualism.\u201d</a> Basically my consciousness is literally the same consciousness in each conscious being. Sorry if it sounds a little new-agey, but it really does help me not feel like I\u2019m sacrificing so much, even if there\u2019s only a small chance it\u2019s true, since I have such absurd leverage the selfish expected value that it might be true could still bex extremely high.</li></ol>", "user": {"username": "Jordan Arel"}}, {"_id": "L3WPuztkSMohBTWqZ", "title": "ETGP 2022: materials, feedback, and lessons for 2023", "postedAt": "2022-09-22T13:12:18.445Z", "htmlBody": "<p>From August 20 to September 2, I ran a summer course in Oxford titled \u201cTopics in Economic Theory and Global Prioritization\u201d, or ETGP. It aimed to provide a rigorous introduction to a selection of topics in economic theory that appear especially relevant to the project of doing the most good. It was designed primarily for economics graduate students considering careers in global priorities research.</p><p>The purpose of this post is to share</p><ul><li>the course materials as presented this year,</li><li>the feedback, and</li><li>a summary of the lessons learned.</li></ul><p>I hope it helps potential attendees get a better sense of whether they would like to attend next year, and potential organizers of similar programs get a better sense of whether and how to go about it.</p><p>I\u2019ve erred on the side of thoroughness regarding the feedback and lessons learned. A brief summary is that the course was rated very highly, and that I think this suggests people should consider organizing more structured courses, instead of sticking to the more common \u201cEA formula\u201d of reading groups and summer research fellowships.</p><p>The course was sponsored by <a href=\"https://www.forethought.org/\">Forethought</a> and made possible by operations support from the <a href=\"https://globalprioritiesinstitute.org/\">Global Priorities Institute</a>. <s>If you would like to be notified when applications open for next year, email me at </s><a href=\"mailto:philip.trammell@economics.ox.ac.uk\"><s>philip.trammell@economics.ox.ac.uk</s></a><s>.</s>&nbsp;<br>Applications for 2023 are now closed. If you would like to be notified in the event that applications open for 2024, or in the event that an online course or lecture series on this content is made available, <strong>please register your interest </strong><a href=\"https://docs.google.com/forms/d/1lMf7UfI9hnbX_m5is-K72K-l3XKGt2MQ4COrBP7xWiM\"><strong>here</strong></a><strong>.</strong></p><h1>Course materials and program schedule</h1><p>The lecture slides and exercises, as presented this year (with corrections), can be found <a href=\"https://docs.google.com/document/d/1dfPCyBhJ6_NnGRHVvaZDY3pT-1M5PVbsNIlT6fXwiJo/edit?usp=sharing\"><strong>here</strong></a>. Feel free to use them for any purpose.</p><p>The program was scheduled as follows:</p><ul><li>Saturday, August 20 - Sunday, August 21: two 1.5h lectures per day on philosophical and mathematical background material, respectively.</li><li>Monday, August 22 - Friday, August 26: two 1.5h lectures per day on various EA-relevant macroeconomic theory topics.</li><li>Saturday, August 27: (optional) punting in the afternoon.</li><li>Monday, August 29 - Friday, September 2: two 1.5h lectures per day on various EA-relevant microeconomic theory topics.</li><li>Attendees were given the option to stay for a (totally unstructured) third week to discuss research ideas with each other, schedule meetings with others in Oxford, and so on.</li></ul><p>The lectures, except for those on the first day, came with exercises. Every lecture-day except the first two (August 20-21) opened with a 1-hour session in which I went over previous lecture-day\u2019s exercises. They were not graded.</p><p>Lectures and lunches were held at Trajan House, where GPI and Forethought are based. Breakfasts, dinners, and housing were held at Worcester College, Oxford, except for an opening dinner and a closing dinner, which were held at pubs.</p><h1>Applicant and attendee characteristics</h1><p>There were 179 applicants. 46 were accepted (26%), and 34 attended at least in part.</p><p>Educational backgrounds of the attendees:</p><ul><li>1 was an assistant professor of economics (3%)</li><li>16 were enrolled in, or about to begin, doctoral programs in economics (47%)</li><li>3 were enrolled in or about to begin master\u2019s programs in economics, or recent graduates of master\u2019s programs not doing either of the above (9%)</li><li>6 were doing pre-doctoral research / research assistance in economics (18%)</li><li>6 were undergraduates studying economics, or recent graduates not doing any of the above (18%)</li><li>2 had never pursued an economics degree (6%). (One had a graduate degree in a related field, and the other was pursuing one)</li></ul><p>Genders of the attendees:</p><ul><li>29 were male (85%)</li><li>5 were female (15%)</li></ul><p>45 of the applicants (25%) and 9 of the admits (20%) were female. I noticed the relative scarcity of female applicants when reviewing the applications, and I did my best to ensure that they were not rejected unfairly.</p><h1>Feedback</h1><p>[Edit January 21, 2023: several attendees have kindly offered testimonials in the comments <a href=\"https://forum.effectivealtruism.org/posts/XMKDvbjxtZCz3rueM/economic-theory-and-global-prioritization-summer-2023-apply\"><strong>here</strong></a>. The feedback below was submitted anonymously.]</p><p>The feedback survey received 24 responses (71%). Aggregate results are as follows:</p><h2>Overall evaluation</h2><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/ynymsydnpvgxh7f48awq.png\" alt=\"Forms response chart. Question title: How do you feel overall about your experience during ETGP?. Number of responses: 24 responses.\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/s710u7f8cdoytvz8flse.png 220w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/uhqfx91he4rgkml15nou.png 440w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/avoukuynihd5ajseje5a.png 660w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/bmjpvmfkhm149uld8ai7.png 880w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/y0odfhhzwlabd1txoxju.png 1100w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/cen1qymx913fnhnk7lws.png 1320w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/yeozphjg36g5ozj4wtdt.png 1540w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/ckkrf0akzkqfy2teuy6r.png 1760w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/fhcpi9g70nh5gnwr8rh7.png 1980w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/djiwq6qjefrpzchxgckh.png 2196w\"></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/to7oclg6t3znhsiqdgud.png\" alt=\"Forms response chart. Question title: All things considered, would you recommend ETGP 2023 to others with skills and interests similar to your own?. Number of responses: 24 responses.\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/gu1lrbkybuqd9yomnc2o.png 220w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/jxb7odogccq879kg8cjz.png 440w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/mj4auipbvtotsgvvknb9.png 660w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/smqt3rhc5uzqkwvpotwo.png 880w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/uakopowkbyjvuzib1bld.png 1100w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/oujwdhiepdhvzkax3kz4.png 1320w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/tm9gmai2hagdcb5lzuls.png 1540w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/t2n3jykxgqalwrxzlwgf.png 1760w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/vsdjtwz2bkopfdvlxqcn.png 1980w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/ybrmgyvn3omxx2rjehlv.png 2196w\"></figure><p>Much of the open-ended text feedback was clearly not written with a public audience in mind, and I won\u2019t try to edit it down or summarize it. But I have to say, there was a heartwarming volume of intensely positive comments that a data point like \u201c5 out of 5\u201d doesn\u2019t fully capture. I hope it\u2019s all right if I just share one of them, for illustration: \u201cI loved this course. This has been one of the best courses of my life.\u201d</p><p>Consistent with the respondents\u2019 unanimous (weak or strong) satisfaction, there were no intensely negative text responses. I\u2019m not sure why two respondents were weakly satisfied but on balance would not recommend it to others with similar skills and interests.</p><p>[Edit Sept. 28: One of these two respondents reached out to explain that they felt ETGP was mostly a \u201csurvey of results\u201d, without \u201cmuch exposure to the \u2018frontier\u2019 and/or methods\u201d, so they \u201cwould not tell other environmental econ PhD students that [it] would help their research\u201d. &nbsp;It is possible that this impression was strengthened by the fact that they could not stay for the second week, which was moderately more in-depth than the first (albeit had even less environmental economics).]</p><h2>Accommodation, meals, and activities</h2><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/izybpl9epwiex8stb8bb.png\" alt=\"Forms response chart. Question title: Were you happy with the accommodation provided (if applicable)?. Number of responses: 22 responses.\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/axhegijvfldaezbniqrn.png 220w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/c3evmydl5g5czqtga3rd.png 440w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/kteuudqdx6ekapcvckd6.png 660w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/bpx9aunee76rrc8tuogk.png 880w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/hdijyfec8ehgbyfeqmcz.png 1100w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/uo7lzven5vahofuel0aq.png 1320w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/avb0zezxmfksypkcwttp.png 1540w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/eztoiibcmj5rkzz3frgk.png 1760w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/vovyvjkuxhaary2smss9.png 1980w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/av81k5fawdpkgt7oojoo.png 2196w\"></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/qpvwjhb6m0dvn2gesxmg.png\" alt=\"Forms response chart. Question title: How did you feel about the work-space provided to you by GPI during your stay?. Number of responses: 23 responses.\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/mro9ocroeobrqxdokt7h.png 220w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/fv1isbxajjupgybliear.png 440w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/y7ojfnyn5w6smgoojgb1.png 660w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/caoxhbynfaol3cbv18p5.png 880w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/nrel9jmaaw04buzcausq.png 1100w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/ndozwzmdgqlfezrg1sfe.png 1320w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/idsztterzvkmkkfyx6ob.png 1540w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/eiwnfwop5idgj6nnznsx.png 1760w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/ft0mkjmxoid9ewfzgcbf.png 1980w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/fm51w02k1mvl7ejmmuoj.png 2196w\"></figure><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/ksccjm1sltapmp0fcnaq.png\" alt=\"Forms response chart. Question title: How satisfied were you with the following elements of the programme?. Number of responses: .\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/lrueuk0u9bkiqadncza5.png 250w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/lt1r4l5upmoktztflkza.png 500w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/xdabtfwcnulgbrfmknvl.png 750w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/hnb3nl9p7jklccoujoej.png 1000w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/y9mpqotffyxdl3hz0xnn.png 1250w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/izfyglvc9t4ainhbtxui.png 1500w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/dyfyv0i3jjfpssttnujv.png 1750w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/o1hdiu0q7pd76c7zwl6f.png 2000w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/rufis4xbalte1jh42yvj.png 2250w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/hdtg6nldrywr0b0todbx.png 2475w\">From the text responses, the most common source of less-than-full satisfaction with the accommodation was that many of the Worcester dormitory bathrooms only had baths, not showers.</p><h2>Inclusivity</h2><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/ew6ghzqxahq5v3vwhkg9.png\" alt=\"Forms response chart. Question title: To what extent would you agree with the following statement: \u201cThe programme encouraged healthy disagreement and debate on important issues\u201d?\n. Number of responses: 24 responses.\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/dvs847temomsdmuj3kwp.png 220w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/vxkbzlafdprdu2632dzi.png 440w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/euj4n1t4kg37sc21p0qu.png 660w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/xowfoc5v5w4rgkwpsqht.png 880w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/bddnkcak2cmdyqmfxvbj.png 1100w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/v8wiovtkw3kf8clqbasl.png 1320w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/krinn5gc6otifruyyobr.png 1540w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/evagrtd9h0cxnwgdfbar.png 1760w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/aqgjmmgevxxwpp8hh346.png 1980w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/s8lp9bvmtttfgklytsst.png 2196w\"></figure><p>From the text responses, the respondent who strongly disagreed did not argue that the program stifled debate\u2014just that \u201c[t]he lectures were lectures and in informal settings we discussed philosophy.\u201d</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/xufycimbjux0doud0dif.png\" alt=\"Forms response chart. Question title: To what extent would you agree with the following statement: \u201cThe programme was welcoming for participants from a diverse range of backgrounds\u201d?\n. Number of responses: 23 responses.\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/kgbqn2n2jaket63in5ym.png 220w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/gxrz0wihheftc59oogdj.png 440w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/dr59ua0h6kbtpfwpyman.png 660w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/fem2pvtcsf8qngguplsd.png 880w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/n5czjdmfzuas2oa9yphm.png 1100w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/ziio0mbp8xt3cragzoar.png 1320w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/gkvob6qujadskqpspc3k.png 1540w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/ywkjh3wcjhwiwo0gfs9a.png 1760w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/iinhd5chuhbjhzl7f5i1.png 1980w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/tz3rwidofmuraeib5va3.png 2196w\"></figure><p>In the text responses, the most common way in which respondents found the course not fully welcoming was that lectures\u2014and informal discussions with the other attendees\u2014could be difficult to follow for those without extensive background knowledge in economics, mathematics, philosophy, and EA thought.</p><p>Second most commonly, some respondents found the course\u2019s relative lack of female and/or non-white attendees off-putting.</p><h2>Curriculum</h2><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/vhajgbmukxyzkiploavy.png\" alt=\"Forms response chart. Question title: Which topics did you find most interesting and/or valuable?. Number of responses: 24 responses.\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/lclrrdzmleocpdij4b5q.png 220w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/yjbo02uh7rwm1c2vq1s0.png 440w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/t1eylyn8zse6pa1zewdn.png 660w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/tmwbzjjblca87xwunc9n.png 880w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/pnhwdoxgipizxrzmjare.png 1100w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/f8javrmujihcsww4jvtu.png 1320w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/yvn4ek5icblggf3azqbk.png 1540w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/bwt3dou0m76mebix7xgx.png 1760w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/dzchvwkkv84nhcmk0zaq.png 1980w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/n2a1yl2ag26rtnyi3rbu.png 2196w\"></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/ivay0tqleuyzi7qzjlo6.png\" alt=\"Forms response chart. Question title: If you had to spend more time on 4 of the topics, which would you choose?. Number of responses: 21 responses.\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/rjc60rpi6zkysrcf7cgf.png 220w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/zo363fqeafsv6mwsv97q.png 440w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/tzkjgk0x2z9fiwsxlg4k.png 660w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/o0emw4jbkxvaihdt5oao.png 880w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/gus3a1ckyj5dlfblsknc.png 1100w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/dbeghbs1bs153j7jxcsd.png 1320w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/qtcrtegkreshsibb9haa.png 1540w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/qumhtkwkishvw8mwdkd5.png 1760w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/ek6pwotwrgtfamwrczg1.png 1980w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/ynyrt4d3rfecillpp3ns.png 2196w\"></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/ykrsay1vnhdsnfxlbojh.png\" alt=\"Forms response chart. Question title: If you had to cut 4 of the topics, which would you choose?\n. Number of responses: 20 responses.\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/xcmqck9d8qiiwazrr3tn.png 220w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/u4eh0niqp5owvksewymd.png 440w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/dh4zu1dj5pnivqyfrrzc.png 660w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/i8b1jmwioht36wa3opfp.png 880w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/oirefgubyajvdrkyv9rf.png 1100w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/kkjylisrjudjpepgmav3.png 1320w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/d6erqwcnrqrv1m02ewfj.png 1540w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/rfimhxncnpuzzg8zrasj.png 1760w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/uwqlcsp8fdiuacerbgku.png 1980w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/fldz38xb7b4n6kb9rhko.png 2196w\"></figure><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/tv5gxyfifrdfz9jofplz.png\" alt=\"Forms response chart. Question title: As currently structured, the course tries to cover a lot of content in a relatively short time. Which of the following should we do in future years?. Number of responses: 23 responses.\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/nwb4zscgtmiuulvc6r6b.png 220w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/rqcu0wuerxcamah6ezvc.png 440w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/sxiuiwcevhamt1grqoak.png 660w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/e2qpxpp9shuit8ykhypx.png 880w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/zpwy89cb23kjxklhrdnc.png 1100w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/vkvgzqqwsw5mvx3dvx5o.png 1320w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/dbmvpeirevhr2keg756k.png 1540w, https://res.cloudinary.com/cea/image/upload/v1674126131/mirroredImages/L3WPuztkSMohBTWqZ/ypgrx6dxrakojlvmtblj.png 1760w, https://res.cloudinary.com/cea/image/upload/v1674126129/mirroredImages/L3WPuztkSMohBTWqZ/j6jxxafbwm0l5zdluink.png 1980w, https://res.cloudinary.com/cea/image/upload/v1674126130/mirroredImages/L3WPuztkSMohBTWqZ/gzhqaysqggosie6niudg.png 2196w\"></p><p>As this last question indicates, it seemed to me by the end of the course that I had tried to pack too much in. But the respondents were roughly divided on this. As many respondents recommended keeping it roughly as is as recommended covering less material, lengthening the duration, or both. Also, all 3 people who answered \u201cOther\u201d proposed ways of teaching the existing quantity of material more efficiently in the same period of time. (Still, it may be telling that no one proposed covering more content or shortening the duration!)</p><h2>Most and least enjoyed aspects of the program</h2><p>From the text responses, it seems that people\u2019s favorite part of the program was the opportunity to meet and hang out with others with shared interests. To this end, some respondents would have appreciated more organized social activities. The only organized social activity, besides meals, was an hour of punting which only about half of the participants attended.</p><p>People\u2019s least favorite part of the program was the problem sets. This seems to have been for three reasons, beyond the fact that problem sets typically aren\u2019t fun in general:</p><ul><li>because the exercises as originally written often had errors which made them unsolvable (hopefully fixed in the exercise sheet in the \u201ccourse materials\u201d Google doc linked above!);</li><li>because some of them were too time-consuming, even if they were solvable; and</li><li>because, without much incentive to attempt the exercises, and given the two issues above, most people soon stopped grappling with them seriously. This made it hard to unilaterally decide to work on them instead of socializing in the evenings.</li></ul><h2>Plan changes</h2><p>18 respondents wrote that attending ETGP may have changed some of their future research plans or directions, of which 6 wrote that it did or almost certainly did so. The most common themes were increased interests in</p><ul><li>growth theory,</li><li>economic theory (as opposed to empirical work) in general,</li><li>doing an economics PhD in general, and</li><li>explicitly longtermism-focused research.</li></ul><h1>Takeaways and tweaks for 2023</h1><p>In summary, I\u2019m excited to report that respondents rated ETGP 2022 very highly! As noted above, I believe the feedback supports the idea that a structured course like this can play an important role, likely beyond that of a less formal fellowship or reading group, in helping students and early-career professionals develop their research interests. It can successfully familiarize attendees with complex toolkits and topics that are relevant to EA-aligned research.</p><p>Nevertheless, in light of the feedback summarized above, and my own experience, I believe the following changes will improve the program for summer 2023.</p><h2>Content</h2><p>Mistakes in the slides and (I think even more harmfully) in the exercises sometimes made it harder to learn the content. I\u2019ve since corrected all the mistakes that were identified on the first run-through, and I\u2019ll try to go over everything again before next year, and/or ask others to do the same.</p><p>I want to encourage attendees to grapple with the exercises, if they would like to, instead of letting them fall into an equilibrium in which little effort is put into the exercises and people typically socialize in the evenings. To make this more likely, in addition to the error-checking noted above, I\u2019ll make the exercises easier. Also, to incentivize effort, I might offer attendees the option to</p><ul><li>have their exercises graded (in which case I\u2019ll try to hire a TA, perhaps from this year\u2019s cohort), and/or the option to</li><li>join a randomly-assigned study group at the beginning, with each group\u2019s scores (self-graded or TA-graded, depending on whether there is a TA) posted publicly at the end, as a friendly competition.</li></ul><p>As a compromise between the \u201ctoo packed with content\u201d half of respondents and the \u201ckeep as is\u201d half, I\u2019ll try to cut an average of 5-10% of the slides from each lecture. I\u2019ll also add an explicit 15m break to the middle of each lecture (making each lecture 1h45), so that the material is less rushed and people have more time to ask questions.</p><p>Finally, at least one curriculum revision: The lecture on quadratic funding will be cut. In its place, the lecture on AI and growth will be expanded to two lectures, with the lecture on game theory for AI governance bumped from week 1 to week 2.</p><h2>Welcomingness</h2><p>The question on welcomingness got somewhat lower scores than the other questions. As noted above, the two main reasons cited for this were the assumption of extensive background knowledge and the relative lack of female and/or non-white attendees.</p><p>I hope to mitigate the first issue next year mainly by either (a) making the course more accessible to undergraduates or, more likely, (b) marketing it more clearly as a graduate-level economics course. It was originally pitched at late-stage undergrads and early-stage graduate students equally, but when the applications came in and it became clear that the seats would be filled primarily by graduate students, I wrote the lectures to be roughly graduate-level. This was a bit of an unfair bait and switch.</p><p>I\u2019ll also encourage attendees to remember that their colleagues have a wide range of prior engagement with philosophy and EA thought, and that they should therefore avoid or explain the relevant jargon in conversation. And of course I\u2019ll try to do the same.</p><p>I hope to mitigate the second issue next year by asking Magnify Mentoring, and local EA groups outside North America, Europe, and Australia, to consider encouraging their members to apply.</p><h2>Scheduling</h2><p>Several admitted applicants could not attend because the course overlapped with the beginning of a fall term, or with a pre-PhD math camp. Several more could only attend for the first week, or would have wanted to attend the optional third week but couldn\u2019t. By contrast, it seems that relatively few people who could attend would not have been able to attend if it had been scheduled a week or two earlier. So I\u2019ll probably schedule it earlier next year.</p><h2>Accommodation and social</h2><p>Worcester got high marks overall, and it\u2019s the natural place for participants to stay, given its proximity to Trajan House. But next year I\u2019ll make sure people are all on halls with showers, or make sure signs are put up that make the showers easy to find. (<i>Lesson for program organizers in general: explore the accommodation facilities in detail before booking, and think hard about what might be missing...!</i>)</p><p>To facilitate those highly-valued informal connections, I\u2019ll probably organize a few more weekend social activities, not just punting.</p><p>Some participants drank a lot at the closing pub dinner, which struck some other participants as shameful and/or a waste of money (and perhaps in some cases it was). To avoid this issue, I\u2019ll consider setting a cap<strong>\u2014</strong>zero or positive<strong>\u2014</strong>on the number of drinks included with the opening and closing dinners.</p><p>Finally, I\u2019ll make sure that some activities, including research idea brainstorming sessions, are organized for the optional third week.</p>", "user": {"username": "trammell"}}, {"_id": "zNS53uu2tLGEJKnk9", "title": "EA\u2019s brain-over-body bias, and the embodied value problem in AI alignment ", "postedAt": "2022-09-21T18:55:52.403Z", "htmlBody": "<p><strong>Overview</strong></p><p>Most AI alignment research focuses on aligning AI systems with the human brain\u2019s stated or revealed preferences. However, human bodies include dozens of organs, hundreds of cell types, and thousands of adaptations that can be viewed as having evolved, implicit, biological values, preferences, and priorities. Evolutionary biology and evolutionary medicine routinely analyze our bodies\u2019 biological goals, fitness interests, and homeostatic mechanisms in terms of how they promote survival and reproduction. However the EA movement includes some \u2018brain-over-body biases\u2019 that often make our brains\u2019 values more salient than our bodies\u2019 values. This can lead to some distortions, blind spots, and failure modes in thinking about AI alignment. In this essay I\u2019ll explore how AI alignment might benefit from thinking more explicitly and carefully about how to model our embodied values.</p><p>&nbsp;</p><p><strong>Context: A bottom-up approach to the diversity of human values worth aligning with</strong></p><p>This essay is one in a series where I'm trying to develop an approach to AI alignment that\u2019s more empirically grounded in psychology, medicine, and other behavioral and biological sciences. Typical AI alignment research takes a rather top-down, abstract, domain-general approach to modeling the human values that AI systems are supposed to align with. This often combines consequentialist moral philosophy as a normative framework, machine learning as a technical framework, and rational choice theory as a descriptive framework. In this top-down approach, we don\u2019t really have to worry about the origins, nature, mechanisms, or adaptive functions of any specific values.&nbsp;</p><p>My approach is more bottom-up, concrete, and domain-specific. I think we can\u2019t solve the problem of aligning AI systems with human values unless we have a very fine-grained, nitty-gritty, psychologically realistic description of the whole range and depth of human values we\u2019re trying to align with. Even if the top-down approach seems to work, and we think we\u2019ve solved the general problem of AI alignment for any possible human values, we can\u2019t be sure we\u2019ve done that until we test it on the whole range of relevant values, and demonstrate alignment success across that test set \u2013 not just to the satisfaction of AI safety experts, but to the satisfaction of lawyers, regulators, investors, politicians, religious leaders, anti-AI activists, etc.&nbsp;</p><p>Previous essays in this series addressed the <a href=\"https://forum.effectivealtruism.org/posts/KZiaBCWWW3FtZXGBi/the-heterogeneity-of-human-value-types-implications-for-ai\">heterogeneity of value types</a> within individuals (8/16/2022, 12 min read), &nbsp;the heterogeneity of values <a href=\"https://forum.effectivealtruism.org/posts/DXuwsXsqGq5GtmsB3/ai-alignment-with-humans-but-with-which-humans\">across individuals</a> (8/8/2022, 3 min read), and the distinctive challenges in aligning with <a href=\"https://forum.effectivealtruism.org/posts/YwnfPtxHktfowyrMD/the-religion-problem-in-ai-alignment\">religious values</a> (8/15/2022, 13 min read). This essay addresses the distinctive challenges of aligning with body values \u2013 the values implicit in the many complex adaptations that constitute the human body. Future essays may address the distinctive challenges of AI alignment with political values, sexual values, family values, financial values, reputational values, aesthetic values, and other types of human values.&nbsp;</p><p>The ideas in this essay are still rather messy and half-baked. The flow of ideas could probably be better organized. I look forward to your feedback, criticisms, extensions, and questions, so I can turn this essay into a more coherent and balanced argument.</p><p>&nbsp;</p><p><strong>Introduction</strong></p><p>Should AI alignment research be concerned only with alignment to the human brain\u2019s values, or should it also consider alignment with the human body\u2019s values?</p><p>AI alignment traditionally focuses on alignment with human values as carried in human brains, and as revealed by our stated and revealed preferences. But human bodies also embody evolved, adaptive, implicit \u2018values\u2019 that could count as \u2018revealed preferences\u2019, such as the body\u2019s homeostatic maintenance of many physiological parameters within optimal ranges. The body\u2019s revealed preferences may be a little trickier to identify than the brain\u2019s revealed preferences, but both can be illuminated through an evolutionary, functional, adaptationist analysis of the human phenotype.</p><p>One could imagine a hypothetical species in which individuals\u2019 brains are fully and consciously aware of everything going on in their bodies. Maybe all of their bodies\u2019 morphological, physiological, hormonal, self-repair, and reproductive functions are explicitly represented as conscious parameters and goal-directed values in the brain. In such a case, the body\u2019s values would be fully aligned with the brain\u2019s consciously accessible and articulable preferences. Sentience would, in some sense, pervade the entire body \u2013 every cell, tissue, and organ. In this hypothetical species, AI alignment with the brain\u2019s values might automatically guarantee AI alignment with the body\u2019s values. Brain values would serve as a perfect proxy for body values.</p><p>However, we are not that species. The human body has evolved thousands of adaptations that the brain isn\u2019t consciously aware of, doesn\u2019t model, and can\u2019t articulate. If our brains understood all of the body\u2019s morphological, hormonal, and self-defense mechanisms, for example, then the fields of human anatomy, endocrinology, and immunology would have developed centuries earlier. We wouldn\u2019t have needed to dissect cadavers to understand human anatomy. We wouldn\u2019t have needed to do medical experiments to understand how organs release certain hormones to influence other organs. We wouldn\u2019t have needed <a href=\"https://en.wikipedia.org/wiki/Evolutionary_medicine\">evolutionary medicine</a> to understand the adaptive functions of fevers, pregnancy sickness, or maternal-fetal conflict.&nbsp;</p><p>&nbsp;</p><p><strong>Brain-over-body biases in EA</strong></p><p>Effective Altruism is a wonderful movement, and I\u2019m proud to be part of it. However, it does include some fairly deep biases that favor brain values over body values. This section tries to characterize some of these brain-over-body biases, so we can understand whether they might be distorting how we think about AI alignment. The next few paragraphs include a lot of informal generalizations about Effective Altruists and EA subculture norms, practices, and values, based on my personal experiences and observations during the 6 years I\u2019ve been involved in EA. When reading these, your brain might feel its power and privilege being threatened, and might react defensively. Please bear with me, keep an open mind, and judge for yourself whether these observations carry some grain of truth.</p><p>Nerds. Many EAs in high school identified as nerds who took pride in our brains, rather than as jocks who took pride in their bodies. Further, many EAs identify as being \u2018on the spectrum\u2019 or a bit Asperger-y (\u2018Aspy\u2019), and feel socially or physically awkward around other people\u2019s bodies. (I\u2019m \u2018out\u2019 as Aspy, and have <a href=\"https://quillette.com/2017/07/18/neurodiversity-case-free-speech/\">written publicly</a> about its challenges, and the social stigma against neurodiversity.)&nbsp; If we\u2019ve spent years feeling more comfortable using our brains than using our bodies, we might have developed some brain-over-body biases.</p><p>Food, drugs, and lifestyle. We EAs often try to optimize our life efficiency and productivity, and this typically cashes out as minimizing the time spent caring for our bodies, and maximizing the time spent using our brains. EA shared houses often settle on cooking large batches of a few simple, fast, vegan recipes (e.g. the <a href=\"https://mcntyr.com/blog/peter-special\">Peter Special</a>) based around grains, legumes, and vegetables, which are then microwaved and consumed quickly as fuel. Or we just drink Huel or Soylent so our guts can feed some glucose to our brains, ASAP. We tend to value physical health as a necessary and sufficient condition for good mental health and cognitive functioning, rather than as a corporeal virtue in its own eight. We tend to get more excited about nootropics for our brains than nutrients for our bodies. The EA fad a few years ago for \u2018<a href=\"https://en.wikipedia.org/wiki/Biphasic_and_polyphasic_sleep\">polyphasic sleep\u2019</a> \u2013 which was intended to maximize hours per day that our brains could be awake and working on EA cause areas \u2013 proved inconsistent with our body\u2019s circadian values, and didn\u2019t last long.</p><p>Work. EAs typically do brain-work more than body-work in our day jobs. We often spend all day sitting, looking at screens with our eyes, typing on keyboards with our fingers, sometimes using our voices and showing our faces on Zoom. The rest of our bodies are largely irrelevant. Many of us work remotely \u2013 it doesn\u2019t even matter where our physical bodies are located. By contrast, other people do <a href=\"https://www.businessinsider.com/most-active-jobs-in-america\">jobs</a> that are much more active, in-person, embodied, physically demanding, and/or physically risky \u2013 e.g. truckers, loggers, roofers, mechanics, cops, firefighters, child care workers, orderlies, athletes, personal trainers, yoga instructors, dancers, models, escorts, surrogates. Even if we respect such jobs in the abstract, most of us have little experience of them. And we view many blue-collar jobs as historically transient, soon to be automated by AI and robotics \u2013 freeing human bodies from the drudgery of actually working as bodies. (In the future, whoever used to work with their body will presumably just hang out, supported by Universal Basic Income, enjoying virtual-reality leisure time in avatar bodies, or indulging in a few physical arts and crafts, using their soft, uncalloused fingers)</p><p>Relationships. The brain-over-body biases often extend to our personal relationships. We EAs are often <a href=\"https://www.verywellmind.com/what-does-it-mean-to-be-sapiosexual-5190425\">sapiosexual</a>, attracted more to the intelligence and creativity of other people\u2019s brains, than to the specific traits of their bodies. Likewise, some EAs are bisexual or pansexual, because the contents of someone\u2019s brain matters more than the sexually dimorphic anatomy of their body. Many EAs also have long-distance relationships, in which brain-to-brain communication is more frequent than body-to-body canoodling.&nbsp;</p><p>Babies. Many EAs prioritize EA brain-work over bodily reproduction. They think it\u2019s more important to share their brain\u2019s ideas with other brains, than to recombine their body\u2019s genes with another body\u2019s genes to make new little bodies called babies. Some EAs are principled <a href=\"https://en.wikipedia.org/wiki/Antinatalism\">antinatalists</a> who believe it\u2019s unethical to make new bodies, on the grounds that their brains will experience some suffering. A larger number of EAs are sort of \u2018pragmatic antinatalists\u2019 who believe that reproduction would simply take too much time, energy, and money away from doing EA work. Of the two main biological imperatives that all animal bodies evolved to pursue \u2013 survival or reproduction \u2013 many EAs view the former as worth maximizing, but the latter as optional.</p><p>Avatars in virtual reality. Many EAs love computer games. We look forward to virtual reality systems in which we can custom-design avatars that might look very different from our physical bodies. Mark Zuckerberg seems quite excited about a <a href=\"https://www.youtube.com/watch?v=Uvufun6xer8\">metaverse</a> in which our bodies can take any form we want, and we\u2019re no longer constrained to exist only in base-level reality, or \u2018meatspace\u2019. On this view, a Matrix-type world in which we\u2019re basically <a href=\"https://en.wikipedia.org/wiki/Brain_in_a_vat\">brains in vats</a> connected to each other in VR, with our bodies turning into weak, sessile, non-reproducing vessels, would not be horrifying, but liberating.&nbsp;</p><p>Cryopreservation. When EAs think about cryopreservation for future revival and health-restoration through regenerative medicine, we may be tempted to freeze only our heads (e.g. \u2018neuro cryopreservation for $80k at <a href=\"https://www.alcor.org/\">Alcor</a>), rather than spending the extra $120k for \u2018whole body cryopreservation\u2019 \u2013 on the principal that most of what\u2019s valuable about us is in our head, not in the rest of our body. We have faith that our bodies can be cloned and regrown in human form \u2013 or replaced with android bodies \u2013 and that our brains won\u2019t mind.</p><p>Whole brain emulation. Many EAs are excited about a future in which we can upload our minds to computational substrates that are faster, safer, better networked, and longer-lasting than human brains. We look forward to <a href=\"https://en.wikipedia.org/wiki/Mind_uploading\">whole brain emulation</a>, but not whole body emulation, on the principle that if we can upload everything in our minds, our bodies can be treated as disposable.&nbsp;</p><p>Animal welfare. Beyond our species, when EAs express concerns about animal welfare in factory farming, we typically focus on the suffering that goes on in the animals\u2019 brains. Disruptions to their bodies\u2019 natural anatomy, physiology, and movement patterns are considered ethically relevant only insofar as they impose suffering on their brains. Many EAs believe that if we could grow animal bodies \u2013 or at least organs, tissues, and cells \u2013 without central nervous systems that could suffer, then there would be no ethical problem with eating this \u2018clean meat\u2019. In this view, animal brains have values, preferences, and interests, but animal bodies, as such, don\u2019t. (For what it\u2019s worth, I\u2019m sympathetic to this view, and support research on clean meat.)</p><p>This is not to say that EA is entirely focused on brain values over body values. Since its inception, EA has promoted global public health, and has worked to overcome the threats to millions of human bodies from malaria, intestinal parasites, and malnutrition. There is a lot of EA emphasis on biosecurity, global catastrophic biological risks (GCBRs), and pandemic preparedness \u2013 which testifies to a biologically grounded realism about our bodies. EA work on nuclear security often incorporates a visceral horror at how thermonuclear weapons can burn, blast, and mutate human bodies. Some EA animal welfare work focuses on how selective breeding and factory farms undermine the anatomy, endocrinology, and immune systems of domesticated animal bodies.&nbsp;</p><p>Of course, EA\u2019s emphasis on brains over bodies is not just a set of nerdy, sapiosexual, antinatalist, knowledge-worker biases. There are more principled reasons for prioritizing brains over bodies as \u2018cause areas\u2019, grounded in EA\u2019s consequentialism and sentientism. Even since Bentham and Mill, utilitarians have viewed moral value as residing in brains capable of experience pleasure and pain. And ever since Peter Singer\u2019s <a href=\"https://en.wikipedia.org/wiki/Animal_Liberation_(book)\">Animal Liberation</a> book in 1975, animal welfare has been viewed largely through a sentientist lens: the animal\u2019s sentient experiences in their brains are considered more ethically relevant than the survival and reproduction of their bodies. Reconciling sentientist consequentialism with a respect for body values is an important topic for another essay.</p><p>Brains are cool. I get it. I\u2019ve been fascinated with brains ever since I took my first neuroscience course as an undergrad in 1985. I\u2019ve devoted the last 37 years of my academic career to researching, writing, and teaching about human minds and brains. But there\u2019s more to our lives than our nervous systems, and there\u2019s more to our interests as human beings than what our brains think they want.</p><p>&nbsp;</p><p><strong>If we\u2019re just aligning with brains, how much of the body are we really aligning with?&nbsp;</strong></p><p>To overcome these brain-over-body biases, it might help to do some thought exercises.&nbsp;</p><p>Imagine we want AI systems to align with our entire phenotypes \u2013 our whole bodies \u2013 and not just our brains. How representative of our embodied interests are our brains?&nbsp;</p><p>Let\u2019s do a survey:</p><ul><li>By weight, the typical person has a <a href=\"https://www.sciencedirect.com/topics/immunology-and-microbiology/brain-weight\">1,300 gram brain</a> in a <a href=\"https://en.wikipedia.org/wiki/Human_body_weight\">70-kg body</a>; so the brain is about 2% of body mass</li><li>By cell-type, brains are mostly made of 2 cell types (neurons and glia), whereas the body overall includes about <a href=\"https://www.nature.com/scitable/blog/bio2.0/discovering_new_cell_types_one/\">200 cell types</a>, so the brain includes about 1% of cell types</li><li>By cell-count, <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5063692/\">brains</a> include about 80 billion neurons and 80 billion glia cells, whereas the <a href=\"https://www.nationalgeographic.com/science/article/how-many-cells-are-in-your-body#:~:text=Adding%20up%20all%20their%20numbers,37.2%20trillion%20cells.\">body overall</a> includes about 30 trillion cells; so the brain includes about 0.5% of the body\u2019s cells</li><li>By organ-count, the brain is one organ out of about <a href=\"https://byjus.com/biology/what-are-the-78-organs-in-the-human-body/\">78 organs</a> in the human body, so the brain is about 1.3% of the body\u2019s organs</li></ul><p>If the human phenotype was a democracy, where organs got to vote in proportion to their weight, cell types, cell counts, or organ counts, brains would get somewhere between 0.5% and 2% of the body\u2019s votes. If AI is aligned only with our brains, it might be aligning with only about 1% of our whole human bodies, and we\u2019d leave 99% unrepresented and unaligned.</p><p>Another way to look at the human phenotype\u2019s values and preferences is from the viewpoint of <a href=\"https://en.wikipedia.org/wiki/Gene-centered_view_of_evolution\">selfish gene theory</a> and <a href=\"https://en.wikipedia.org/wiki/Disposable_soma_theory_of_aging\">disposable soma theory</a>. The human brain is arrogant. It thinks it\u2019s in charge, and should be in charge. However, from an evolutionary gene-centered view, the gonads are really where the action is. The \u2018immortal germline replicators\u2019 (as Richard Dawkins called them in <a href=\"https://en.wikipedia.org/wiki/The_Selfish_Gene\"><i>The Selfish Gene</i></a>) are carried in testes and ovaries. Everything else in the body is just an evolutionary dead end \u2013 it\u2019s a \u2018disposable soma\u2019. The somatic cells outside the gonads are just there to protect, nourish, and help replicate the sperm and eggs in the gonads. From that perspective, the brain is just helping the genes in the gonads make more genes in next generation\u2019s gonads. The brain\u2019s values and preferences may or may not be aligned with the evolutionary interests of the germ-line replicators in the gonads. From a longtermist evolutionary perspective, maybe AI systems should try to be aligned with the interests of the immortal germ-line replicators, not just the transient, disposable brains the evolved to represent their interests. (More on this in another essay.)</p><p>&nbsp;</p><p><strong>How brain-over-body biases can increase AI X-risk</strong></p><p>When we think about existential risks from AI, many EAs focus on the dangers of superintelligence growing misaligned from human intelligence, pursing different abstract goals, and quietly taking over our world through the Internet. Hollywood depictions of Terminator-style robots physically imitating, hunting, and killing human bodies are considered silly distractions from the real business of aligning artificial brains with human brains. Indeed, some EAs believe that if a superintelligence offered a credible way to upload our minds into faster processors, even at the cost of killing our physical human bodies, that would count as a win rather than a loss. In this view, a transhumanist future of post-human minds colonizing the galaxy, without any human bodies, would be considered a victory rather than an AI apocalypse. This is perhaps the strongest example of the EA brain-over-body bias.&nbsp;</p><p>You might well be asking, so what if EAs have brain-over-body biases? Does it really matter for AI alignment, and for minimizing existential risks (X risks)? Can\u2019t we just ignore bodies for the moment, and focus on the real work of aligning human brains and artificial brains?</p><p>Consider one example from narrow AI safety: self-driving cars. When we\u2019re designing AI systems to safely control our cars, we don\u2019t just want the car\u2019s AI to act in accordance with our brain\u2019s preferences and values. Our number one priority is for the car not to crash in a way that squishes our body so we die. The best way to keep our bodies safe isn\u2019t just for the AI to model our brains\u2019 generic preference for life over death. It\u2019s for the AI system designers to model -- in grisly, honest, and biomedically grounded detail, the specific types of crashes that could cause specific kinds of injuries to specific parts of our bodies.&nbsp;</p><p>Full AI alignment for self-driving cars would require, at least implicitly, alignment with the hundreds of specific physical vulnerabilities of the specific human bodies that are actually in the car right now. From the perspective of an AI in a self-driving car, given its millisecond-response-rate sensors and multi-gigahertz processors, every crash happens in excruciatingly slow motion. There are plenty of ways to use steering, braking, acceleration, evasive maneuvers, air bag deployment, etc., to influence how the crash plays out and what kinds of injuries it causes to occupants. As a professor, I\u2019d want my car\u2019s AI to manage the crash so it prioritizes protecting my eyes (for reading), my brain (for thinking), and my hands (for typing). But if I\u2019m a professional dancer, I might want it to put a slightly higher priority on protecting my knees, ankles, and spine. If I\u2019m a parent, I might want it to put a higher priority on protecting my baby in their right rear car seat than on protecting me in the front left driver\u2019s seat. If I\u2019m driving my elderly parent around, and the AI knows from their medical records that they recently had their right hip joint replaced, I might want it to put a priority on reducing the crash\u2019s likely impact on that leg. In general, we want self-driving cars to understand our specific body values and vulnerabilities, not just our brain values. These body values cannot be reduced to the kinds of hypothetical trolley problems that ask for people\u2019s stated preferences about the acceptability of harming different kinds of car occupants and pedestrians (e.g. <a href=\"https://www.pnas.org/doi/10.1073/pnas.1911517117\">this</a>.)</p><p>Narrow AI systems for biomedical applications also need to understand body values. These could include AI-controlled surgery robots, autonomous ambulances, robotic health care workers, telehealth consultants, etc. In each case, the AI doesn\u2019t just need to model human preferences (e.g. \u2018I don\u2019t want to die please\u2019); it also needs to actually understand the human body\u2019s thousands of adaptations at a very granular, biological level that can guide its medical interventions. This would include, for example, the AI needing to model the goal-directed <a href=\"https://en.wikipedia.org/wiki/Homeostasis\">homeostatic mechanisms</a> that control blood pressure, blood sugar, body temperature, fluid balance, extracellular pH levels, etc.</p><p>Similar issues arise with the safety of narrow AI systems controlling industrial robots with human workers\u2019 bodies nearby, or controlling military weapons systems with civilian bodies nearby. We want the AI systems to be aligned with all the organs, tissues, and cells of all the human bodies nearby, not just with the conscious values in their brains.&nbsp;</p><p>Military applications could be especially worrisome, because the better a benevolent AI system can get aligned with human body values and vulnerabilities, the more easily a hostile AI system could copy and invert those body values, treating them as vulnerabilities, in order to impose injury or death in precisely targeted ways. Consider <a href=\"https://imsdb.com/scripts/Terminator-2-Judgement-Day.html\">scene 86</a> in <i>Terminator 2: Judgment Day</i> (1991), when the \u2018good\u2019 T-800 Terminator, played by Arnold Schwarzenegger, is suturing Sarah Conner\u2019s stab wounds that were inflicted by the misaligned, liquid metal T-1000. Reassuring her about his biomedical knowledge, the T-800 says \u2018I have detailed files on human anatomy\u2019. Sarah says \u2018I\u2019ll bet. Makes you a more efficient killer, right?\u2019. He says \u2018Correct\u2019. Detailed understanding of human body values can be used both to inflict maximum damage, and to offer maximally effective medical care.</p><p>When AI alignment researchers think about X risks to humanity, there\u2019s a tendency to ignore these kinds of body values, and to treat our human interests way too abstractly. Mostly, ordinary folks just want the AI systems of the future not to kill their bodies. They don\u2019t want the AI to do a drone strike on our house. They don\u2019t want it to turn their bodies into paperclips. They don\u2019t want it to use thermonuclear weapons on their bodies. Alignment with our brain values is often secondary to alignment with our body values.</p><p>Note that this argument holds for any future situation in which our minds are grounded in any substrate that could be viewed as a sort of \u2018physical body\u2019, broadly construed, and that\u2019s vulnerable to any sort of damage. If our heads are cryopreserved in steel cylinders at the Alcor facilities in Arizona, then those cylinders are our new bodies, and we would want AI guardians watching over those bodies to make sure that they are safe against physical attack, cybersecurity threats, financial insolvency, and ideological propaganda \u2013 for centuries to come. If we\u2019re uploaded to orbital solar-powered server farms, and our minds can\u2019t survive without those computational substrates working, then those server-satellites are our new bodies, and they will have body values that our AI guardians should take into account, and that might be quite different from our mind\u2019s values. So, one failure mode in AI alignment is to focus too much on what our brains want, and not enough on what could mess up our bodies \u2013 whatever current or future forms they happen to take.</p><p>The concept of body values provides a bridge between narrower issues of AI alignment, and broader issues of AI health and safety. Certainly, avoiding catastrophic damage to the human body seems like a fairly obvious goal to pursue in designing certain autonomous AI systems such as cars or robots. However, embodied values get a lot more numerous, diverse, subtle, and fine-grained than just our conscious preference for AI systems not to break our bones or crush our brains.&nbsp;</p><p>&nbsp;</p><p><strong>Can we expand the moral circle from brains to bodies?</strong></p><p>Maybe one approach to incorporating body values into AI alignment research is to keep our traditional EA consequentialist emphasis on sentient well-being, and simply expand our moral circle from brains to bodies. This could involve thinking of bodies as a lot more sentient than we realized. (But, as we\u2019ll see, I don\u2019t think that really solves the problem of body values.)</p><p>Peter Singer famously argued in a 1981 <a href=\"https://en.wikipedia.org/wiki/The_Expanding_Circle\">book</a> that a lot of moral progress involves humans expanding the \u2018moral circle\u2019 of who\u2019s worthy of moral concern \u2013 e.g. from the self, to family members, to the whole tribe, to the whole human species, to other species.&nbsp;</p><p>Post hoc, from our current sentientist perspective, this looks like a no-brainer \u2013 it\u2019s just a matter of gradually acting nicer towards more and more of the beings that are obviously sentient.&nbsp; However, historically, when these moral battles were being fought, expanding the moral circle often seemed like a matter of expanding the definition of sentience itself. How to do so was usually far from obvious.&nbsp;</p><p>To a typical animal with a high degree of nepotism (concern for close blood relatives, due to kin selection), but no tribalism (concern for other group members, due to reciprocal altruism and multi-level selection), blood relatives may seem sentient and worthy of moral concern, but non-relatives may not. To a prehistoric hunter-gatherer, people within one\u2019s tribe may seem sentient, but people in other tribes speaking other languages can\u2019t express their values in ways we can understand, so they are typically dehumanized as less than sentient. To a typical anthropocentric human from previous historical eras, all humans might be considered sentient, but nonhuman animals were usually not, because they can\u2019t even express their preferences in any language. Expanding the moral circle often required rethinking what sentience really means, including which kinds of beings have morally relevant preferences, interests, and values, and how those values are mentally represented within the individuals and articulated to other individuals.&nbsp;</p><p>Let\u2019s zoom in from moral circle expansion at the grand scale, and consider the individual scale.&nbsp;</p><p>The moral circle is centered on the \u2018self\u2019. But what is this \u2018self\u2019? What parts of the self should be included in the moral circle? Only the parts of the cerebral cortex that can verbally articulate the brain\u2019s interests through stated preferences? Or should we also include parts of the brain that can\u2019t verbally state their preferences, but that can guide behavior in a way that reveals implicit preferences? Does the ethically relevant self include only the cerebrum, or does it also include the revealed preferences of the diencephalon, midbrain, and pons? Does the self include spinal reflexes, sensory organs, the peripheral nervous system, the autonomic nervous system, and the enteric nervous system? Does the self include the rest of our body, beyond the nervous system?</p><p>Sentience seems easy to spot where we\u2019re looking at central nervous systems like vertebrate brains. Those kinds of brains embody preferences that clearly guide movement towards some kinds of stimuli and away from other kinds of stimuli, and that generate reward and punishment signals (pleasures and pains) that clearly guide reinforcement learning.&nbsp;</p><p>However, sentience gets trickier to spot when we\u2019re looking at, say, the gut\u2019s <a href=\"https://en.wikipedia.org/wiki/Enteric_nervous_system\">enteric nervous system</a>, which can operate independently of the brain and spinal cord. This system coordinates digestion, including peristalsis, segmentation contractions, and secretion of gastrointestinal hormones and digestive enzymes. The enteric nervous system uses more than 30 neurotransmitters, and contains about 90% of the body\u2019s serotonin and 50% of the body\u2019s dopamine. It <a href=\"https://pubmed.ncbi.nlm.nih.gov/24997029/\">includes</a> some 200-600 million neurons, distributed throughout two major plexuses (the myenteric and submucosal plexuses), and thousands of small ganglia. Its complexity is comparable to that of central nervous systems in other species that EAs generally consider sentient \u2013 e.g. zebrafish have about 10 million neurons, fruit bats have about 100 million, pigeons have about 300 million, octopuses have about 500 million. Moreover, the enteric nervous system <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6834869/\">can do</a> a variety of learning and memory tasks, including habituation, sensitization, long term facilitation, and conditioned behavior. Should the enteric nervous system be considered sentient? I don\u2019t know, but I think it has some implicit, evolved preferences, values, and homeostatic mechanisms that we might want AI systems to become aligned with.</p><p>EA consequentialism tends to assume that ethically relevant values (e.g. for AI alignment) are coterminous with sentience. This sentientism gets tricky enough when we consider whether non-cortical parts of our nervous system should be considered sentient, or treated as if they embody ethically relevant values. It gets even tricker when we ask whether body systems outside the nervous system, which may not be sentient in most traditional views, carry values worth considering.</p><p>&nbsp;</p><p><strong>Do bodies really have \u2018values\u2019?</strong></p><p>You might be thinking, OK, within the \u2018self\u2019, maybe it\u2019s reasonable to expand the moral circle from the cerebral cortex to subcortical structures like the diencephalon, midbrain, pons, and to the peripheral, autonomic, and enteric nervous systems. But shouldn\u2019t we stop there? Surely non-neural organs can\u2019t be considered to be sentient, or to have \u2018values\u2019 and \u2018preferences\u2019 that are ethically relevant?&nbsp;</p><p>My intuitions are mixed. I can see both sides of this issue. When that happens, I often run a Twitter poll to see what other folks think. On Sept 18, 2022, I ran this poll, with these results (in this screenshot):</p><p>&nbsp;</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/56e003d5b93fc418466806dc3864d076b895ee6b415614fb.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/56e003d5b93fc418466806dc3864d076b895ee6b415614fb.png/w_98 98w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/56e003d5b93fc418466806dc3864d076b895ee6b415614fb.png/w_178 178w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/56e003d5b93fc418466806dc3864d076b895ee6b415614fb.png/w_258 258w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/56e003d5b93fc418466806dc3864d076b895ee6b415614fb.png/w_338 338w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/56e003d5b93fc418466806dc3864d076b895ee6b415614fb.png/w_418 418w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/56e003d5b93fc418466806dc3864d076b895ee6b415614fb.png/w_498 498w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/56e003d5b93fc418466806dc3864d076b895ee6b415614fb.png/w_578 578w\"></p><p>My typical follower is a centrist American male, and only about 1% of my followers (1,553 out of 123,900) responded to this poll. This is far from a globally representative sample of humans, and this poll should not be taken too seriously as data. Its only relevance here is in showing that people have quite mixed views on this issue. Many (35%) think human bodies do, literally, have implicit, unconscious values and preferences. Many others (40%) think they do not. Some (9%) think they do metaphorically but not literally. Let\u2019s see if there\u2019s any sense in which bodies might embody values, whether literally or metaphorically.</p><p>&nbsp;</p><p><strong>Embodied goals, preferences, and motivations</strong></p><p>In what possible sense does the human body have values that might be distinct from the brain\u2019s conscious goals or unconscious preferences? Are non-sentient, corporeal values possible?&nbsp;</p><p>In <a href=\"https://en.wikipedia.org/wiki/Control_theory\">control theory</a> terms, a thermostat has designed-in \u2018goals\u2019 that can be understood through revealed preferences, e.g. \u2018trying\u2019 to keep a house within a certain temperature range. The thermostat does not need to be fully sentient (capable of experiencing pleasure or pain) to have goals.&nbsp;</p><p>If the thermostat can be said to have goals, then every homeostatic mechanism in the body also has \u2018goals\u2019, evolved rather than designed, that can be understood through analyzing the body\u2019s revealed preferences (e.g. \u2018trying\u2019 to keep body temperature, blood glucose, estradiol, and muscle mass within certain optimal ranges). Thus, we can think of the body as a system of \u2018embodied motivations\u2019 (values, preferences, goals) that can be understood through an evolutionary, functional, adaptationist analysis of its organs, tissues, and cells.&nbsp;</p><p>There\u2019s an analogy here to the concept of \u2018<a href=\"https://en.wikipedia.org/wiki/Embodied_cognition\">embodied cognition\u2019</a> \u2013 the idea that a lot of our goal-directed behavior doesn\u2019t just arise from the brain in isolation, but depends on an adaptive interplay between brain, body, and environment, and cannot be understood accurately without explicitly considering the specific physical features and capabilities of bodies.&nbsp;</p><p>For example, a standard cognitivist approach to understanding hunger and goal-directed eating might focus on the brain\u2019s mental representations of hunger stimuli, whereas an embodied cognition approach would also talk explicitly about the structure, physiology, and innervation of the stomach and gut, the release and uptake of hunger-related hormones leptin and ghrelin, and the interaction between the gut microbiome and the human host body.&nbsp;</p><p>Here, I\u2019m arguing that if we consider the entire human phenotype \u2013 body, brain, and behavior \u2013then a lot of our human values are highly embodied. We could call this the domain of embodied volution, embodied motivation, or embodied values. (I use the terms \u2018body values\u2019, \u2018embodied values\u2019, and \u2018corporeal values\u2019 more or less interchangeably in this essay.)</p><p>Just as the field of embodied cognition has developed new terms, ideas, theories, and models for understanding how the brain/body system as a whole processes information and guides behavior, a field of \u2018embodied values\u2019 might need to develop new terms, ideas, theories, and models for understanding how the brain/body system as a whole pursues certain preferences, values, and goals &nbsp;\u2013 especially if we want to build AI systems that are aligned with the full range of our embodied values.</p><p>&nbsp;</p><p><strong>Aligning with embodied values requires detailed, evolutionary, functional analysis of bodily adaptations</strong></p><p>Imagine we take seriously the idea that AI alignment should include alignment with embodied values that might not be represented in the nervous system the way that more familiar sentient values are. How do we proceed?&nbsp;</p><p>With brain values, we can often just ask people what they want, or have them react to different options, or physically demonstrate what they\u2019d prefer. We don\u2019t need a detailed functional understanding of where those brain values come from, how they work, or what they\u2019re good for.</p><p>However, with body values, we can\u2019t just ask what our gut microbiome wants, what our liver wants, or what our anti-cancer defenses want. We need to actually do the evolutionary biology and evolutionary medicine. AI alignment with body values would require AI to model everything we learn about how human bodies work.&nbsp;</p><p>If this argument is correct, it means there may not be any top-down, generic, all-purpose way to achieve AI alignment until we have a much better understanding of the human body\u2019s complex adaptations. &nbsp;If Artificial General Intelligence is likely to be developed within a few decades, but if it will take more than a few decades to have a very fine-grained understanding of body values, and if body values are crucial to align with, then we will not achieve AGI alignment. We would need, at minimum, a period of Long Reflection focused on developing better evolutionary medicine models of body values, before proceeding with AGI development.&nbsp;</p><p>Aligning with embodied values might also require different input/output channels for AI systems. We\u2019re used to thinking that we\u2019ll just communicate with AI systems through voice, keyboard, face, and gesture \u2013 all under the brain\u2019s voluntary control. However, alignment with body values might require more intrusive biomedical sensors that actually track the interests and well-being of various bodily systems. People involved in the \u2018<a href=\"https://en.wikipedia.org/wiki/Quantified_self\">quantified self\u2019</a> movement already try to collect a lot of this kind of data, using sensors that might be useful to AI systems. Whether we would want AI systems to be able to directly affect our physiology \u2013 e.g. through direct control over pharmaceuticals, hormones, or other biomedical interventions \u2013 is an open question.&nbsp;</p><p>&nbsp;</p><p><strong>What difference would it make if AI alignment considered embodied values?</strong></p><p>What are some examples where an \u2018embodied-values\u2019 approach to AI alignment would differ from a standard \u2018brain-values-only\u2019 approach?</p><p>1. Caring for the microbiome. The human body hosts a complex <a href=\"https://en.wikipedia.org/wiki/Human_microbiome\">microbiome</a> \u2013 an ecology of hundreds of different microscopic organisms such as bacteria that are found throughout our skin, hair, gut, and other organs. Human health depends on a healthy microbiome. But the microbiome doesn\u2019t have a brain, and can\u2019t state its preferences. It has different DNA than we do, and different genetic interests. Human brains didn\u2019t even know that human bodies contained microbiomes until a few decades ago. And medicine didn\u2019t understand the microbiome\u2019s importance until after 1980s, when Barry Marshall <a href=\"https://www.lindau-nobel.org/on-man-and-microbes-barry-marshall/\">showed</a> that helicobacter pylori can cause ulcers (and then got the Nobel prize in 2005). If an AI system is aligned with the human brain, but it ignores the microbiome hosted within the human body, then it won\u2019t be aligned with human interests (or the microbiome\u2019s interests).</p><p>2. Caring for a fetus. Female human bodies can get pregnant, and a lot of <a href=\"https://academic.oup.com/book/36756/chapter-abstract/321856620\">adaptive physiology</a> goes on in pregnancy between the mother\u2019s body, the uterine lining, the placenta, and the fetus, that is not consciously accessible to the mother\u2019s brain. Yet the outcome of the adaptive physiology in pregnancy matters enormously to pregnant mothers. It can make the difference between a spontaneous abortion, a miscarriage, a stillbirth, and a healthy baby. For an AI system to be fully aligned with a pregnant mother\u2019s values and interests, it should be able to represent and care for the full range of physiological dynamics happening within her reproductive system and her offspring.</p><p>3. Protecting against cancer. Cells in the human body often undergo spontaneous mutations that turn them into runaway replicators, i.e. cancer cells, that develop \u2018selfish\u2019 agendas (reproduce and spread everywhere) that are contrary to the body\u2019s general long-term interests. In response, bodies have evolved many <a href=\"https://www.cheatingcell.com/\">anti-cancer defenses</a> that embody the revealed preference of \u2018try not to die of cancer, especially when young\u2019). Most human brains have no idea that this arms race between incipient cancers and anti-cancer defenses is going on, every day, right under our noses. Yet, the body has genuine \u2018embodied values\u2019 to avoid runaway cancer growth that would undermine survival and reproduction. Any AI system that doesn\u2019t track exposure to carcinogenic chemicals, incipient cancers, and the state of anti-cancer defenses, wouldn\u2019t really be aligned with the body\u2019s embodied value of reducing cancer risk.</p><p>4. Promoting longevity. Human bodies evolved to live surprisingly long lives, even by the long-lived standards of mammals and social primates. Our bodies include lots of anti-aging adaptations design to extend our survival and reproductive longevity.&nbsp;The evolutionary biology subfields of <a href=\"https://en.wikipedia.org/wiki/Life_history_theory#Human_life_history\">life history theory</a>, including senescence theory, model how our longevity adaptations evolve, and how we developed embodied values to promote longer life-spans. Our brains also evolved to promote longevity, but they tend to do so by perceiving external threats such as predators, parasites, pathogens, and aggressive rivals, and coordinating behaviors to avoid or overcome those threats. Our brains didn\u2019t evolve to track the hundreds of other longevity-promoting adaptations inside our bodies, that don\u2019t require external sensory perception or coordinated whole-body behaviors to cope with. Thus, there\u2019s a gap between what our brains think is crucial to longevity (e.g. avoid getting eaten by predators, avoiding getting into fights with psychopaths), and what our bodies think is crucial to longevity (e.g. eating nutritious foods, preserving the microbiome, exercising enough to maintain muscles and bones, etc.) Often, there are conflicts of interest between what the brain wants (e.g. more donuts) and what our embodied longevity values would want (e.g. avoid donuts, eat leafy greens). Of course, among humans who happy to absorb accurate nutritional insights from medical research, their brains might internally represent this conflict between valuing donuts and valuing leafy greens. But not everyone has gotten the message \u2013 and historically, much of the public nutrition advice has been based on bad science, and is not actually aligned with the body\u2019s long-term interests. Thus, there can be cases where our embodied longevity values deviate dramatically from what our brains think they want. So, which should our AI systems align with \u2013 our brains\u2019 revealed preferences for donuts, or our bodies\u2019 revealed preferences for leafy greens?</p><p>&nbsp;</p><p><strong>Benefits of considering embodied values in AI alignment</strong></p><p>I think there are several good reasons why AI alignment should explicitly try to integrate embodied values into alignment research.</p><p>First, handling the full diversity of human types, traits, and states. We might want AI systems that can align with the full range of humans across the full range of biological and psychological states in which we find them. At the moment, most AI alignment seems limited to incorporating goals and preferences that physically healthy, mentally health, awake, sentient adults can express through voluntary motor movements such as through the vocal tract (e.g. saying what you want), fingers (e.g. typing or clicking on what you want), or larger body movements (e.g. showing a robot how to do something). This makes it hard for AI systems to incorporate the embodied values and preferences of people who are asleep, in a coma, under general anesthetic, in a severely depressed state, in a state of catatonic schizophrenia, on a psychedelic trip, suffering from dementia, or preverbal infants. None of these people are in a condition to do cooperative inverse reinforcement learning (CIRL), or most of the other proposed methods for teaching AI systems our goals and preferences. Indeed, it\u2019s not clear that the brains of sleeping, comatose, or catatonic people have \u2018goals and preferences\u2019 in the usual conscious sense. However, their bodies still have revealed preferences, e.g. to continue living, breathing, being nourished, being safe, etc.</p><p>Second, the brain\u2019s conscious goals often conflict with the body\u2019s implicit biological goals. Let\u2019s consider some examples where we might really want the AI system to take the body\u2019s goals into account. Assume that we\u2019re dealing with cases a few years in the future, when the AI systems are general-purpose personal assistants, and they have access to some biomedical sensors on, in, or around the body.</p><p>Anorexia. Suppose an AI system is trying to fulfil the preferences of an anorexic teenaged girl: her brain might say \u2018I\u2019m overweight, my body is disgusting, I shouldn\u2019t eat today\u2019, but&nbsp; her body might be sending signals that say \u2018If we don\u2019t eat soon, we might die soon from electrolyte imbalances, bradycardia, hypotension, or heart arrhythmia\u2019. &nbsp;Should the AI pay more attention to the girl\u2019s stated preferences, or her body\u2019s revealed preferences?</p><p>Suicidal depression. Suppose a college student has failed some classes, his girlfriend broke up with him, he feels like a failure and a burden to his family, and he is contemplating suicide. His brain might be saying \u2018I want to kill myself right now\u2019, but his body is saying \u2018Actually every organ other than your brain wants you to live\u2019. &nbsp;Should the AI fulfill his brain\u2019s preferences (and help arrange the suicide), or his body\u2019s preferences (and urge him to call his mom, seek professional help, and remember what he has to live for)? Similar mismatches between what the brain wants and what the body wants can arise in cases of drug addiction, drunk driving, extreme physical risk-taking, etc.</p><p>Athletic training. Suppose AI/robotics researchers develop life-sized robot sparring partners for combat sports. A woman has a purple belt in Brazilian jujitsu (BJJ), and she\u2019s training for an upcoming competition. She says to her BJJ sparring robot \u2018I need a challenge; come at me as hard as you can bro\u2019. The robot\u2019s AI needs to understand not just that the purple belt is exaggerating (doesn\u2019t actually want it to use its full strength); it also needs a very accurate model of her body\u2019s biomechanics, including the locations, strengths, and elasticities of her joints, ligaments, sinews, muscles, and blood vessels, when using <a href=\"https://en.wikipedia.org/wiki/List_of_Brazilian_jiu-jitsu_techniques\">BJJ techniques</a>. If the robot gets her in a joint lock such as an arm bar, it needs to know exactly how much pressure on her elbow will be too little to matter, just enough to get her to tap out, or too much, so she gets a serious elbow strain or break. If it gets her in a choke hold such as a triangle choke, it needs to understand exactly how much pressure on her neck will let her escape, versus lead her to tap out, versus compress her carotid artery to render her unconscious, versus kill her. She may have no idea how to verbally express her body\u2019s biomechanical capabilities and vulnerabilities to the robot sparring partner. But it better get aligned with her body somehow \u2013 just as her human BJJ sparring partners do. And it better not take her stated preferences for maximum-intensity training too seriously.</p><p>&nbsp;</p><p><strong>Cases where AI systems should prioritize brain values over body values</strong></p><p>Conversely, there may be cases where a person (and/or their friends and family members) might really want the AI to prioritize the brain\u2019s values over the body\u2019s values.</p><p>Terminal disease and euthanasia. Suppose someone has a terminal disease and is suffering severe chronic pain. Their life is a living hell, and they want to go. But their body is still fighting, and showing revealed preferences that say \u2018I want to live\u2019. Advance care directives (\u2018living wills\u2019) are basically legally binding statements that someone wants others to prioritize their brain values (e.g. stop suffering) over their body values \u2013 and we might want AI biomedical care systems to honor those directives.&nbsp;</p><p>Cryopreservation and brain uploading. Suppose someone elderly is facing a higher and higher chance of death as they age. Their brain would prefer for their body to undergo <a href=\"https://en.wikipedia.org/wiki/Cryopreservation\">cryopreservation</a> by Alcor, or whoever, in hopes of eventual resuscitation and anti-aging therapies. But their body still works mostly OK. Should their AI system honor their cryopreservation request \u2013 even if it results in technical death by legal standards? Or, further in the future, the brain might want to be uploaded through a <a href=\"https://en.wikipedia.org/wiki/Mind_uploading\">whole-brain emulation</a> method. This would require very fine-scale dissection and recording of brain structure and physiology, that results in the death of the body. Should the AI system concur with destructive dissection of the brain, contrary to the revealed preferences of the body?</p><p>Self-sacrifice. People sometimes find themselves in situations where they can save others, at the possible cost of their own life. Heroic self-sacrifice involves the brain\u2019s altruism systems over-riding the body\u2019s self-preservation systems. Think of soldiers, fire fighters, rescue workers, and participants in high-risk clinical trials. Should the AI side with the altruistic brain, or the self-preserving body? In other cases, someone\u2019s brain might be willing to sacrifice their body for some perceived greater good \u2013 as in the case of religious martyrdom. Should an AI allow a true believer to do a suicide bombing, if the martyrdom is fully aligned with their brain\u2019s values, but not with their body\u2019s revealed preferences?</p><p>&nbsp;</p><p><strong>Conclusion</strong></p><p>I\u2019ve argued for a bottom-up, biologically grounded approach to AI alignment that explicitly addresses the full range and variety of human values. These values include not just stated and revealed values carried in the central nervous system, but evolved, adaptive goals, preferences, and values distributed throughout the human body. EA includes some brain-over-body biases that make our body values seem less salient and important. However, the most fundamental challenge in AI safety is keeping our bodies safe, by explicitly considering their values and vulnerabilities. Aligning to our brain values is secondary.&nbsp;</p>", "user": {"username": "geoffreymiller"}}, {"_id": "GHd8vyC3qAX5spdpG", "title": "EA Dating Spreadsheet: see EA dating profiles", "postedAt": "2022-09-21T18:34:59.992Z", "htmlBody": "<p><strong>Update: somebody else made a </strong><a href=\"https://stevekrouse.notion.site/stevekrouse/Date-Me-Directory-2132c9c256534d74b8ddd0f8e10fc6dd#56e8cfa069fa4aff98876aa92e260814\"><strong>better version except for a broader audience</strong></a><strong>. You can still tag your profile as EA or rationalist so that others can find you or you can find other EAs. The original one is still available for people who'd only like to be seen by EAs.&nbsp;</strong></p><p>TLDR: Browse the&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1oL25vc5Feg94flPPCO03ujijRvyHffxt0qnI7rU8rdg/edit#gid=0\"><u>EA Dating Spreadsheet</u></a> to see EA dating profiles (and add yourself!)</p><p>I get a warm fuzzy feeling when I see two really well-matched EAs together, but there's currently no way to systematically find EAs on dating sites, and certainly no way to find all of the people who have the amazing \"dating documents\" that are becoming a thing in the community.&nbsp;</p><p>I threw this spreadsheet together in a couple of hours (1) to help fix that.</p><p>It\u2019s simple. You can use it for two purposes:</p><ol><li><strong>Finding EAs.&nbsp;</strong>Look through it to find EAs dating profiles/dating documents</li><li><strong>Helping EAs find you.&nbsp;</strong>Add a link to your own profile/document if you want to make it easier for other EAs to find you.&nbsp;</li></ol><p>Enjoy! May you find love, lust, or whatever gives you utils.</p><p>Remember: only add your own profile to the spreadsheet. This is public, and only people who want to share their profile should do so.&nbsp;</p><p><i>Reminder that you can now&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library\"><i><u>listen to EA Forum/LessWrong posts on your podcast player</u></i></a><i> using&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library\"><i><u>The Nonlinear Library</u></i></a><i>.</i></p><p><i>(1) Of note, this was done as a micro-project experiment, where I set a limit of how many hours I was allowed to work on it before I published it. There are a lot of things that could be improved, but this was just a fun project that I\u2019m not going to spend more time on.&nbsp;</i></p><p><i>If you would like to improve on it, please make a copy of the spreadsheet, make the changes, then send the new version of the spreadsheet to me. If I like the changes, I\u2019ll add it to the spreadsheet. I probably won\u2019t accept changes to the platform itself (like changing it to Airtable or Notion) because that adds too much complexity.</i></p><p><br>&nbsp;</p>", "user": {"username": "katherinesavoie"}}, {"_id": "AFgvA9imsT6bww8E3", "title": "Announcing the Rethink Priorities Special Projects Program", "postedAt": "2022-09-21T18:34:48.854Z", "htmlBody": "<h1>Key points&nbsp;</h1><ul><li>Rethink Priorities (RP) has launched a Special Projects (SP) Program to help start promising EA initiatives by providing fiscal sponsorship and&nbsp;full-service operational support\u2013including, but not limited to hiring, finance, event planning, and communications.&nbsp;</li><li>A key strength of RP is its operations. We have been able to scale the organization from 1.5 full-time equivalents (FTE) staff to nearly 50 FTE in just four years.</li><li>The SP Program advances projects that RP would like to see happen and allows these projects to focus on their core work rather than worrying about running their organization.</li><li>Current special projects include&nbsp;<a href=\"https://epochai.org/blog/announcing-epoch\"><u>Epoch</u></a>,&nbsp;<a href=\"https://effective-giving-marketing.gitbook.io/unjournal-x-ea-and-global-priorities-research/\"><u>Unjournal</u></a>,&nbsp;<a href=\"https://daaronr.github.io/eamt_data_analysis/index.html\"><u>EA Market Testing</u></a>,&nbsp;<a href=\"https://condor.camp/en/\"><u>Condor Camp</u></a>, and&nbsp;<a href=\"https://www.eapathfinder.org/\"><u>EA Pathfinder</u></a>.</li><li>There are various ways to get involved in the SP Program: join our team, apply to have your project sponsored, or share with us lessons you\u2019ve learned incubating projects if you have experience in this arena. Please reach out by submitting an&nbsp;<a href=\"https://forms.gle/TS5VZ21GhMNsgiSM8\"><u>Expression of Interest</u></a> form.&nbsp;</li></ul><h1>About Rethink Priorities</h1><p>Founded in 2018, RP is dedicated to informing decisions made by high-impact organizations and funders across various cause areas. To date, our team has conducted over 50 person-years of work (you can find all our public research&nbsp;<a href=\"https://rethinkpriorities.org/research\"><u>here</u></a>). One of our key strengths is our operational capacity, which allows us to recruit, onboard, and support talented researchers to produce quality work with minimal bureaucratic red tape.&nbsp;</p><p>Given our strong operations, RP would like to support other promising EA projects \u2013 particularly longtermist ones \u2013 in getting off the ground. To this end, we recently launched a new Special Projects Program.&nbsp;&nbsp;</p><h1>About the Special Projects Program</h1><p>As an off-shoot of our Operations Department, Special Projects is a distinct team dedicated to supporting the launch of&nbsp;<i>new</i> projects rather than running RP\u2019s day-to-day operations.</p><p>The team\u2019s Acting Director&nbsp;<a href=\"https://www.linkedin.com/in/carolyn-footitt-1a951039/\"><u>Carolyn Footitt</u></a> is leading this work with Associates&nbsp;<a href=\"https://www.linkedin.com/in/merilalama/\"><u>Mar\u00eda De la Lama</u></a> and&nbsp;<a href=\"https://www.linkedin.com/in/cristina-schmidtibanez/\"><u>Cristina Schmidt Ib\u00e1\u00f1ez</u></a>. Their projects generally fall under two areas:</p><ol><li><u>Incubated (internal)</u>\u2013In addition to our research agenda, RP is incubating direct work and other projects that advance our&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jcmrhndktgopqFugx/rethink-priorities-2022-mid-year-update-progress-plans\"><u>mission</u></a>. The SP Team works closely with RP staff to launch these initiatives. Once the incubation period for each respective initiative ends, the project will either conclude or spin-off to become an independent organization.&nbsp;</li><li><u>Fiscally sponsored (external)</u>\u2013The SP Team is also providing fee-based fiscal sponsorship and support to projects that are managed by individuals outside of RP. Within this model, the project\u2019s founders maintain autonomy and decision-making authority while we provide them with operational and fiduciary oversight.</li></ol><h2>Our services</h2><p>Although our involvement will depend on the project, in general, the SP Team will provide operational support to projects much in the way that our Core Operations Team supports RP\u2019s work. Such services might include developing and managing budgets, accounting, contracting, running hiring rounds, planning the onboarding of new staff, hosting and developing the project website, and other operations tasks outlined in the below chart.&nbsp;&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AFgvA9imsT6bww8E3/ezotsut57fzotc0mxsty\"></p><h2>Current Special Projects</h2><p>The SP Team is currently incubating the following projects:</p><ul><li>LAISR, an AI strategy retreat</li><li><a href=\"https://effective-giving-marketing.gitbook.io/unjournal-x-ea-and-global-priorities-research/\"><u>Unjournal</u></a>, an open platform for research relevant to global priorities</li><li><a href=\"https://daaronr.github.io/eamt_data_analysis/index.html\"><u>EA Market Testing</u></a>, a survey on how to best promote effective giving and action</li><li><a href=\"https://www.eapathfinder.org/\"><u>EA Pathfinder</u></a>, an initiative to advise and support mid-career professionals looking to switch into EA work</li><li><a href=\"https://condor.camp/en/\"><u>Condor Camp</u></a>, an EA and longtermism training and community-building project in Brazil</li></ul><p>The SP Team also helped to launch&nbsp;<a href=\"https://epochai.org/blog/announcing-epoch?fbclid=IwAR1bg-at15jTHih0IxFWqkUgxF7Az6nJRM58KxeV_AippAoCs3DM3j1iUtw\"><u>Epoch</u></a>, a new AI research initiative, which we are continuing to fiscally sponsor.&nbsp;</p><h3><strong>Highlights</strong><br>&nbsp;</h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AFgvA9imsT6bww8E3/dtwxjb41qpqufbk5d5g3\" alt=\"Condor Camp logo\"></p><p>From late July to early August,&nbsp;<a href=\"https://condor.camp/en\"><u>Condor Camp</u></a> held its first retreat with 13 highly talented Brazilian university students. Seminars and workshops focused on how to solve some of the world\u2019s most pressing problems, including AI safety, pandemics, biosecurity, global health and poverty, and animal welfare. Following this event, RP\u2019s Associate Researcher&nbsp;<a href=\"https://www.linkedin.com/in/renannascimentoaraujo/\"><u>Renan Ara\u00fajo</u></a> is now:</p><ul><li>Keeping the camp\u2019s alumni engaged with EA projects</li><li>Gathering lessons that may be relevant for other EA training projects</li><li>Strategizing how to maintain momentum in building the EA movement in Brazil and throughout Latin America</li></ul><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AFgvA9imsT6bww8E3/kc5ofqgf61zfbdjacuvj\"></p><p>RP has been helping Epoch to hire for&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/jobs/53700\"><u>multiple research roles</u></a>. Supporting the initiative\u2019s operations frees up the researchers\u2019 time so they can focus on their work.&nbsp;</p><p>Thus far, the researchers have already done a deep dive into&nbsp;<a href=\"https://deepai.org/publication/machine-learning-model-sizes-and-the-parameter-gap\"><u>machine learning model size trends</u></a> in which they identified distinct patterns in vision and language model sizes over time and hypothesized reasons for these patterns.&nbsp;</p><p>Through this type of work, Epoch aims to support strategy and forecasting around the development of transformative artificial intelligence\u2013AI that could have an impact as significant as the industrial revolution.</p><h2>We\u2019re seeking new projects</h2><p>RP is interested in fiscally sponsoring additional projects that are aligned with our mission and values. Currently, we are especially interested in longtermist megaprojects, but this is not a requirement; we are open to hearing about any initiative in the EA space. Some sample projects might include (but are certainly not limited to):&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BtqhvcawMdrYJcKgK/help-us-make-civilizational-refuges-happen\"><u>civilizational refuges</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zjMeGcgWpvDcm3CkH/why-short-range-forecasting-can-be-useful-for-longtermism\"><u>EA early warning forecasting</u></a>, and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/myp9Y9qJnpEEWhJF9/linch-s-shortform?commentId=hedmemCCrb4jkviAd\"><u>red-teaming initiatives</u></a>. If in doubt, please do not hesitate to reach out to the Special Projects Team.&nbsp;</p><p><strong>The best way to get in touch with us is to submit an&nbsp;</strong><a href=\"https://forms.gle/TS5VZ21GhMNsgiSM8\"><strong><u>Expression of Interest</u></strong></a><strong> form.&nbsp;</strong>This form will be open on an ongoing basis. We are excited to hear from those who are interested in:&nbsp;</p><ul><li>Working on the Special Projects Team</li><li>Fiscal sponsorship/project incubation</li><li>Sharing information (e.g. tips/lessons learned about running longtermist or other incubation projects)</li><li>Or have other ideas that they\u2019d like to share</li></ul><p>&nbsp;</p><h1>Acknowledgments</h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/jYSEjBsWbjNqioRZJ/clei4bezhusjjx1cuxz7\"></p><p><i>This post is a project of&nbsp;</i><a href=\"https://rethinkpriorities.org/\"><i><u>Rethink Priorities</u></i></a><i>\u2013a think tank dedicated to informing decisions made by high-impact organizations and funders across various cause areas. The author is Rachel Norman. Thanks to Carolyn Footitt for their helpful input. If you are interested in RP\u2019s work, please visit our&nbsp;</i><a href=\"https://www.rethinkpriorities.org/research\"><i><u>research database</u></i></a><i> and subscribe to our&nbsp;</i><a href=\"https://www.rethinkpriorities.org/newsletter\"><i><u>newsletter</u></i></a><i>.&nbsp;</i></p>", "user": {"username": "Rachel"}}, {"_id": "b4wJcm2LaWCjsKswF", "title": "Summarizing the comments on William MacAskill's NYT opinion piece on longtermism", "postedAt": "2022-09-21T17:46:48.494Z", "htmlBody": "<p>We are a small community, but our ideas have the potential to spread far if communicated effectively. Refining our communication means being well calibrated as to how people outside the EA community react to our worldviews. So when MacAskill's article about longtermism <a href=\"https://www.nytimes.com/2022/08/05/opinion/the-case-for-longtermism.html\">was published last month</a> in the NYT, I was pretty interested to see the comment section. I started to count various reactions, got carried away, and ended up going through 300 or so. Below is a numerical summary.</p><h3>Caveats</h3><ul><li>Selection bias is present. I would guess NYT commenters skew older and liberal.</li><li>It's possible the comments don't reflect overall sentiment of the article's readers, because people might only feel compelled to comment when they are strongly skeptical, undercounting casually positive readers.</li><li>Many people signaled they felt positive towards the article and longtermist thinking, but were entirely pessimistic about our future -- basically \"This is all well and good, but _\". Sometimes it was hard to know whether to tally these as positive or skeptical; I usually went with whichever sentiment was the main focus of the comment.</li><li>For the most part, this survey doesn't capture ideas people had to help our long term future. Some of those not tallied included better education, fusion power, planting trees, and outlawing social media.</li></ul><h2>Tallies</h2><ul><li>60 - Skeptical -- either of longtermism, or our future<ul><li>20 - Our broken culture prevents us from focusing on the long-term</li><li>16 - We're completely doomed, there's no point</li><li>7 - We are hard-wired as animals to think short term</li><li>7 - Predicting the future is hard; made up numbers</li><li>5 - We don't know what future generations will want</li><li>5 - We don't even value current lives</li><li>3 - I value potential people far less than current people</li><li>3 - It's easy to do horrific things in the name of longtermism</li><li>2 - This is ivory tower BS</li></ul></li><li>42 - Generally positive</li><li>17 - This is nothing new (most of these comments were either about climate activism or <a href=\"https://en.wikipedia.org/wiki/Seven_generation_sustainability\">seven generation sustainability</a>)</li><li>7 - This planet is not ours / humans don't deserve to survive</li><li>7 - We should lower the population</li><li>6 - This is all about environmental sustainability</li><li>6 - Animals matter too</li><li>5 - Republicans are terrible</li><li>4 - Reincarnation might be true</li><li>3 - We should ease up on technology</li><li>2 - Technology will save us</li><li>1 - Time travel might be true</li><li>1 - Society using carbon is a good thing</li><li>1 - This idea is un-American</li><li>1 - This is all the fault of boomers</li><li>1 - Stop blaming boomers</li></ul><h2>Takeaways</h2><p>Overall, I found the responses to be more negative than anticipated. The most common sentiment I saw was utter pessimism, which I worry is a self-fulling prophecy.</p><p>There was very little reaction to or discussion about the risks of bioweapons and AI. Many people seemed to substitute concern for our long-term future solely with concern for the environment. This is understandable given the prominence of environmentalism -- it's already top-of-mind for many.</p><p>I think people struggled to appreciate the timescale proposed in the article. Many referenced leaving the Earth a better place for their (literal) grandchildren, or for seven generations from now, but not thousands of years.</p>", "user": {"username": "Westward"}}, {"_id": "BhsawCvCBXLScFkYc", "title": "[Cause Exploration Prizes] Scaling Graduation to End Extreme Poverty", "postedAt": "2022-09-22T05:33:14.217Z", "htmlBody": "<p><i>This essay was submitted to Open Philanthropy's&nbsp;</i><a href=\"https://www.openphilanthropy.org/research/cause-exploration-prizes/\"><i>Cause Exploration Prizes</i></a><i>&nbsp;contest (before the deadline). We are uploading some entries late, but all good-faith entries were considered for prizes.</i></p><p>&nbsp;</p><p><strong>Author's note: </strong>A proposal by George Ndung\u2019u Kamau;&nbsp;<a href=\"mailto:georgerucathi@gmail.com\">georgerucathi@gmail.com</a></p><p><strong>The challenge: Extreme Poverty</strong></p><p>The negative impact of extreme intergenerational poverty continues to be a challenge to many developing countries. According to the World Bank, 689 million (9.2%) of the world, live in extreme poverty (less than USD1.90 a day) In Kenya, more than 11 million households are classified as extreme poor. While progress has been made in alleviating poverty, the frequency and intensity of extreme natural phenomenon associated with climate change and other shocks are pushing more people to slide into the bracket of the extreme poor. The rising challenge of poverty remains a moving target as pervasive deprivation subjects many vulnerable and marginalized groups to incessant suffering and compromises the ability of millions to achieve their full potential.</p><p>Although awareness of poverty is high, many still lack comprehensive knowledge about how it can progressively and sustainability be alleviated. Recent assessments have demonstrated that the majority of livelihood development strategies do not reach the extreme poor and have not proven to have their intended impact. It is really hard to reach the extreme poor and most products that have been adapted have not been effective. Therefore, there is a need to revisit the keys to combating extreme poverty in more innovative ways; the main point of graduation. This strategy emphasizes the existence of an extremely effective tangible program -graduation- that has shown amazing results that the poor can indeed start and maintain livelihoods.&nbsp;</p><p><strong>Graduation Approach</strong></p><p>Graduation approach is a sequenced time-bound set of intervention that provides a package of targeted support to the poorest households with the aim of lifting them from extreme poverty to sustainable livelihoods using the Graduation Approach, globally recognized for its impact.<sub>&nbsp;</sub>The \u201cGraduating the Extreme Poor into Sustainable Livelihoods\u201d approach (hereafter \u201cgraduation\u201d) has risen as an effective means of addressing extreme poverty, enabling poor and vulnerable households to develop sustainable livelihoods and access financial systems, and psychosocial benefits. Based on a model developed in 2002 by BRAC in Bangladesh, graduation is now used in nearly 50 countries.&nbsp;</p><p>Graduation consists of a carefully coordinated, multi- sectoral, \u201cbig push\u201d intervention comprising of social assistance to ensure basic consumption; skills training; seed capital or access to employment opportunities to jump-start an economic activity; financial education and access to saving instruments; and coaching or mentoring to build confidence and reinforce skills. The interventions are time bound (generally 18\u201336 months) to preclude long-term dependence. Continued linkages to market opportunities or the labour market, as well as effective access to social protection systems, are needed to maintain a sustained upward trajectory. Graduation has been tested in varied contexts over the last 15 years, yielding rigorous evidence of impact on extreme poor households, setting them on an upward pathway and mitigating risks of backsliding. The impacts have resulted in sustained income and asset and consumption gains as evidenced by multiple RCTs in a variety of contexts and have continued seven years after the end of the intervention.&nbsp;</p><p><strong>Government Adoption: The proposed approach to Achieving Scale</strong></p><p>In many ways, this is a new era of how government and NGOs can work together. The traditional models of addressing extreme poverty in the development space, in which large scale international NGOs provide services independently and grow their own delivery channels outside of government, is coming to a close. [see GDI\u2019s article on \u201cWhat\u2019s Your Endgame\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhg4etk0jaja\"><sup><a href=\"#fnhg4etk0jaja\">[1]</a></sup></span>].&nbsp;</p><p>Globally, implementation of graduation programmes is moving towards \u201cGovernment Adoption\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref66nvn8qcc6\"><sup><a href=\"#fn66nvn8qcc6\">[2]</a></sup></span>&nbsp;and transformation of national welfare systems. This is my proposed approach to achieving scale.</p><p>In line with this global trend, our team operates based on the principle that the most sustainable and scalable pathway towards ending extreme poverty is via full government ownership and scaled innovation of proven approaches. Therefore, the role of NGOs must be comprised of five phases:</p><ul><li>Phase 1: piloting and proving interventions in target communities&nbsp;</li><li>Phase 2: conducting joint implementation with governments and adapting interventions to retain quality and realize cost efficiency at scale&nbsp;</li><li>Phase 3: handing over refined implementation tools (i.e. training guidelines, manuals) to government, supporting on M&amp;E and related tools and systems&nbsp;</li><li>Phase 4: transitioning most operations that the government teams can lead, providing support where needed on specific programmatic elements</li><li>Phase 5: supporting entry into new locations, populations and contexts<ul><li>The team will work closely with global/local thinktanks&nbsp; in collaborations with governments receiving the support to conduct the full round of graduation approach interventions, including local participatory targeting of the extreme poor targeting and local participatory engagement, hiring and training mentors, setting up savings groups, supporting training ahead of and aligned with consumption support (cash transfers) and training on livelihoods for asset transfer to generate economic activity, capacity building support linked to economic activities and support on market assessment and linkages.&nbsp;</li><li><strong>Support Government to develop a strong monitoring and evaluation system as a building blocks for effective delivery and shared innovation:&nbsp;</strong>The think tanks/ NGOs will support Government to establish a best-in-class monitoring and evaluation framework to guide a process of learning by doing to create one or more cost-effective approaches that the Government(s) can replicate across their jurisdictions.&nbsp; It will be as cost-effective and as simple as it can be whilst remaining effective. Results from the work will be built on the establishment of a best-in-class monitoring and evaluation framework.&nbsp;</li><li><strong>Strengthen Management information system and social registry foundation</strong>: The teams/partners will work on strengthening the social registry for harmonized targeting, monitoring, reporting and use of management information system through the developed &amp; operationalised M&amp;E framework. This will support accelerating the interoperability of current robust national social registry and MIS platform that will to be interoperable with other efforts and will to serve as the foundation for registration and economic inclusion intervention protocols (cash, mentoring, etc.) at the county level.</li><li><strong>Develop materials and provide technical assistance support:</strong> The partners/teams will develop coordination frameworks, tools and systems and provide technical training and support to the Governments at national and county level, so that the Governments ultimately lead the implementation of economic inclusion at scale, both for this program of work and future related efforts.&nbsp;</li><li><strong>Conduct preliminary organizational capacity assessment at national and county government levels:&nbsp;</strong>An initial assessment can help ensure national/county level governments are able to address any critical capacity gaps that may inhibit their ability to support and eventually own implementation of the economic inclusion.</li><li><strong>&nbsp;Drive learning across multiple levels of Government:&nbsp;</strong>The broad selection of governance units and their geographical spread across countries provides an opportunity to develop context-specific strategies. The learnings from each unit and subsequent locations will inform government strategic development and planning considerations at the national level.&nbsp;The learning agenda will translate directly into a set of tools and blueprints that can easily translate into government-led program scaling for other regions.&nbsp;</li><li><strong>Integrate the learnings to support the development and implementation of&nbsp; policy framework/legal instruments.&nbsp; </strong>The partners/teams will provide critical capacity building support to national and other government units by providing a means through which this effort can be truly owned and operationalized at both national and devolved level and at scale.</li></ul></li></ul><p>Working in partnership with central and devolved government units, the specific objectives of the proposed initiatives, that can be adapted to different regions globally, include:</p><ol><li><strong>Support the program implementation of the pilot socio-economic inclusion intervention in select regions within countries</strong></li><li><strong>Develop Tools and Systems, processes, and capacity-building materials for implementation and scale-up by Government</strong></li><li><strong>Build the capacity of government staff to effectively implement and support the economic inclusion intervention</strong></li></ol><p>The Partners will work with responsible government ministries and actors, equipping them to take over economic inclusion activities and integrate with existing welfare programs and delivery systems. Additionally, this proposed endeavour will connect to market and value chain development and ensure interoperability with other social services and the national social registry. It is also critical that this effort coordinates with activities by private sector actors such as loan providers and banks as well as with programs supported by global agencies such as the World Food Programme, USAID and other local NGOs.</p><p><strong>The projected Impact of the Approach&nbsp;</strong></p><p>The application for the approach to socio-economic inclusion looks forward to enable 1.5 million Kenyans living in extreme poverty to engage in sustainable livelihoods and build their resilience in the next 5 years. Additionally, it aims to establish good practices of strengthening national and county governments to build their capacity to deliver effective economic inclusion graduation programs through systems strengthening with the aim of creating a replicable model for potential future scale up. The approach has specifically led to significant improvements of extreme poor households registering impressive results and sustained impacts over 10 years across the following areas</p><ul><li>Reduction of extreme poverty levels across regions</li><li>Increase in household assets (World Bank, 2013)</li><li>More than 20% increase in food consumption&nbsp;(World Bank, 2013)</li><li>Increase in social cohesion and inclusion (IRDP, 2014)</li><li>Increased household resilience to shocks with enhanced with each having access to affordable credit and savings</li><li>Increase in female financial inclusion (FAO, 2014)&nbsp;</li><li><strong>Human Capital Investment</strong>:&nbsp;All households are able to support their children to attend primary school</li></ul><p>Globally, the return on investment on similar programming has been impressive as summarized in the figure below</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5e34a214e593485988a2471a0d79a0a4774b24c5f85c0e1c.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5e34a214e593485988a2471a0d79a0a4774b24c5f85c0e1c.png/w_80 80w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5e34a214e593485988a2471a0d79a0a4774b24c5f85c0e1c.png/w_160 160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5e34a214e593485988a2471a0d79a0a4774b24c5f85c0e1c.png/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5e34a214e593485988a2471a0d79a0a4774b24c5f85c0e1c.png/w_320 320w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5e34a214e593485988a2471a0d79a0a4774b24c5f85c0e1c.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5e34a214e593485988a2471a0d79a0a4774b24c5f85c0e1c.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5e34a214e593485988a2471a0d79a0a4774b24c5f85c0e1c.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5e34a214e593485988a2471a0d79a0a4774b24c5f85c0e1c.png/w_640 640w\"></p><p><strong>Funding</strong></p><p>The funding for scaling up graduation through government adoption can be explored through a variety of ways</p><ol><li>Funding The Global Development Incubator, GDI,(<a href=\"http://www.globaldevincubator.org\">www.globaldevincubator.org</a>), a leading think tank on this subject to advance its work in Kenya and across sub-Sahara Africa. An initial seed capital of USD 1 Million may be a great starting point.</li><li>Funding components of GDI\u2019s pilot to Kenyan government with specific emphasis on the research and learning component</li><li>Funding a consortium of partners to support a pilot to a new country in Africa/globally.</li></ol><p><strong>NB:</strong> I have experience in sector having supported central and devolved governments to pilot similar programming. Currently I am leading a technical team on a similar pilot for the Kenya government. Your esteemed organization can partner with us to advance the current pilot or pilot it with another government.&nbsp;</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhg4etk0jaja\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhg4etk0jaja\">^</a></strong></sup></span><div class=\"footnote-content\"><p>What\u2019s your Endgame; Stanford Social Innovation Review, 2015 https://ssir.org/articles/entry/whats_your_endgame&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn66nvn8qcc6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref66nvn8qcc6\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.findevgateway.org/sites/default/files/publication_files/oecd_brief_with_pei_design_122018.pdf\">https://www.findevgateway.org/sites/default/files/publication_files/oecd_brief_with_pei_design_122018.pdf</a>&nbsp;</p></div></li></ol>", "user": {"username": "Open Philanthropy"}}, {"_id": "7nFtdWqFwmyipabCR", "title": "[Cause Exploration Prizes] Meta-science", "postedAt": "2022-09-22T05:35:12.006Z", "htmlBody": "<p><i>This essay was submitted to Open Philanthropy's&nbsp;</i><a href=\"https://www.openphilanthropy.org/research/cause-exploration-prizes/\"><i>Cause Exploration Prizes</i></a><i>&nbsp;contest (before the deadline). We are uploading some entries late, but all good-faith entries were considered for prizes.</i></p><p>&nbsp;</p><p>Open Philanthropy should officially start up a meta-science cause area.&nbsp;</p><p>It has toyed with meta-science for many years. Indeed, back in 2012-13, Holden Karnofsky gave me a one-pager idea he had seen from Steve Goodman and John Ioannidis at Stanford (which I helped them develop into METRICS, the Meta-Research Innovation Center at Stanford).</p><p>But it\u2019s time to make this a full-fledged cause area.&nbsp;</p><p>Why?&nbsp;</p><p>The case is simple:&nbsp;</p><p>Scientific innovation is a key driver of economic and human flourishing.&nbsp;</p><p>But our current scientific ecosystem is too tied down by bureaucracy, incrementalism, status quo biases, poor reproducibility, and more. In a world so highly affected by&nbsp;<a href=\"https://scienceplusplus.org/trouble_with_rcts/index.html\">scientific outliers</a>, we need to be obsessed with reducing&nbsp;<a href=\"https://brianlui.dog/2020/10/06/upside-decay/\">upside decay</a>.</p><p>We may not be able to predict how to optimize for outliers, but we can at least stop doing all of the things that <i>prevent</i> outliers. After all, if we drive&nbsp;<a href=\"https://www.statnews.com/2022/02/01/kariko-problem-lessons-funding-basic-research/\">even one Katalin Karik\u00f3</a>&nbsp;out of the system, that could cost us a trillion dollars.&nbsp;</p><p>***</p><p>Let\u2019s take just one indicator: whether we\u2019re getting the maximum value from talented scientists who happen to be younger.&nbsp;</p><p>Einstein had his&nbsp;<a href=\"https://en.wikipedia.org/wiki/Annus_mirabilis_papers\">best year at age 26</a>. Heisenberg&nbsp;<a href=\"https://en.wikipedia.org/wiki/Werner_Heisenberg\">published his uncertainty principle</a> at age 26. Paul Samuelson\u2019s&nbsp;<a href=\"https://en.wikipedia.org/wiki/Foundations_of_Economic_Analysis\">masterpiece</a> \u201cFoundations of Economic Analysis\u201d was based on work he completed at age 26. James Watson was 25 when he&nbsp;<a href=\"https://en.wikipedia.org/wiki/James_Watson\">helped discover</a> the structure of DNA.&nbsp;</p><p>Yet in today\u2019s scientific workforce, a 26-year-old might be merely half-way through a doctorate, and would virtually never be eligible for a major scientific grant. Indeed, the median age for a&nbsp;<a href=\"https://nexus.od.nih.gov/all/2021/11/18/long-term-trends-in-the-age-of-principal-investigators-supported-for-the-first-time-on-nih-r01-awards/\">first NIH grant</a> is 42 for men and 44 for women, meaning half of such grants are given to people above that age!&nbsp;</p><p>Such a lengthy timeline, combined with low pay in early years and the uncertainty of getting tenure, almost certainly discourages many highly-talented people from entering science in the first place.&nbsp;</p><p>(Imagine telling Einstein or Heisenberg at age 25 that they would have to wait 10 more years before they were deemed worthy of scientific recognition, but that they could earn many times more money in tech or finance.)&nbsp;</p><p>On top of that, there is reason to think that even when smart and talented people get doctorates in science, the best among them can be selected out of the academic workforce, because they want to spend time on the most important problems at the forefront of a field rather than cranking out marginal publications. The hard problems, after all, might take 8 or 10 years to make progress, and no one has time for that.&nbsp;</p><p>If we could fix this problem alone, the future improvements to scientific and economic progress might be in the trillions. I can\u2019t in good conscience create a lengthy spreadsheet calculation, because nearly every number would be made-up. The more important point in any event is whether one agrees with the overall intuition. (If you don\u2019t agree, then no spreadsheet would convince you, while if you do agree that radical scientific progress is possible and that current practices discourage it, no spreadsheet is necessary.)</p><p>***</p><p>So far, I\u2019ve just been addressing the problem of how the system wastes young talent or drives it away altogether. There are many other problems to address, such as the lack of reproducibility, the inherent biases and conservatism of peer review, the lack of experimentation with different models of funding, the micromanagement and overly-bureaucratic nature of the funding system, and more.&nbsp;</p><p>Suffice it to say: Our scientific system has found many ways to stifle curiosity and innovation. A meta-science effort to promote better policies, while carefully studying what works and what doesn\u2019t, could ultimately create benefits so staggering that they are equivalent to developing the steam engine or discovering the structure of DNA.&nbsp;</p>", "user": {"username": "Stuart Buck"}}, {"_id": "fHqP9NmTvALTTQSv9", "title": "CEA's Events Team is Hiring!", "postedAt": "2022-09-21T15:35:36.273Z", "htmlBody": "<p>I\u2019m thrilled to announce that the CEA Events Team is hiring for three positions on our team:&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/careers/ea-global-events-associate\"><strong><u>EA Global Events Associate</u></strong></a>,&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/careers/retreats-associate\"><strong><u>Retreats Associate</u></strong></a>, and&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/careers/community-events-associate\"><strong><u>Community Events Associate</u></strong></a>.</p><p>If you\u2019re passionate about effective altruism, would love organizing events, and want to join an energetic team, apply by October 11th 2022, 1:00 am BST!</p><h3>Why join the Events team?</h3><p>The Events team creates high-quality discussion spaces that connect, inform, and inspire people in ways that help them have a greater positive impact on the world. We run three programs:</p><ul><li>EA Global \u2014 organizing large conferences for those significantly involved in EA.</li><li>Retreats \u2014 organizing retreats for community leaders and other select groups.</li><li>Community Events \u2014 funding and supporting events run by community members, such as EAGx conferences.</li></ul><p>We\u2019re proud of what we\u2019ve achieved so far in 2022.&nbsp;<strong>We\u2019re on track to facilitate more </strong><a href=\"https://forum.effectivealtruism.org/posts/zP4jebzvdtBr6mxdz/cea-s-events-team-capacity-building-and-mistakes#Metric__connections\"><strong>connections</strong></a><strong> this year than every other year combined \u2014 and four times as many this year as in 2021.&nbsp;</strong></p><p><strong><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995668/mirroredImages/fHqP9NmTvALTTQSv9/dftmhrdqpifjxtmym8uq.png\"></strong></p><p>Some other highlights include:</p><ul><li>Organizing 3 EA Global conferences in London, San Francisco, and Washington DC, with more than 4,000 attendees in total and receiving consistently high \u201clikelihood to recommend\u201d scores, all ~9/10.</li><li>Running 4 retreats for community leaders, field experts, and other groups, with more than 150 participants and glowing feedback, and stakeholders consistently reporting 10/10 logistics.</li><li>Supporting 10 EAGx and other community events in 7 different countries, with more than 3,000 attendees and high satisfaction and impact scores (see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/p56jREngCtxNegLs5/cea-s-community-events-programme-update-and-call-for#Community_Events_in_2022_so_far\"><u>our July update on the Community Events program</u></a> for more).</li></ul><p>But we\u2019re not stopping here. We have ambitious plans for our events to be bigger and better in 2023 and beyond, and we need your help to make that happen. Already we\u2019ve gone from 1.5 full-time equivalents to 7, with three different program areas. We\u2019re hiring one role for each of these areas, to help us scale while keeping quality high.<br>&nbsp;</p><p><strong><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995669/mirroredImages/fHqP9NmTvALTTQSv9/a2h7zqsdmtkykf48wdeo.jpg\"></strong></p><h3>Who are we looking for?</h3><p>We\u2019re looking for people who share our values of earnest ambition, independent motivation, and interest in altruistic impact. You should also have:</p><ul><li>A strong alignment with and understanding of effective altruism and its principles.</li><li>A keen eye for detail, quality, and efficiency, and the ability to juggle multiple tasks and deadlines.</li><li>A collaborative and supportive mindset, and the ability to communicate clearly and respectfully with a diverse range of stakeholders.</li><li>A growth-oriented and flexible attitude, and the willingness to learn from feedback and adapt to changing circumstances.</li></ul><p><strong><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995668/mirroredImages/fHqP9NmTvALTTQSv9/cuoqaiq1ecoi1oyml4pk.jpg\"></strong></p><h3>What are the roles?</h3><p>The three roles we\u2019re hiring for are:</p><p><a href=\"https://www.centreforeffectivealtruism.org/careers/ea-global-events-associate\"><strong><u>EA Global Events Associate</u></strong></a><strong>:</strong> You\u2019ll help us scale up and improve the effective altruism community\u2019s largest conference series. In this role, you\u2019ll support a wide range of operational projects to increase the number of connections made and the quality of information shared at these events.</p><p><a href=\"https://www.centreforeffectivealtruism.org/careers/retreats-associate\"><strong><u>Retreats Associate</u></strong></a><strong>:</strong> You\u2019ll help us scale and improve our program, aimed at nurturing a community of people who are thinking carefully about the world\u2019s biggest problems and taking action to solve them. In this role, you\u2019ll provide operational support to help the Retreats Program succeed and grow.</p><p><a href=\"https://www.centreforeffectivealtruism.org/careers/community-events-associate\"><strong><u>Community Events Associate</u></strong></a><strong>:</strong> You\u2019ll help significantly scale up the number of events that help people find ways to improve the world. In this role, you will advise and support community members to create events all around the world.</p><h3>How to apply?</h3><p>If you\u2019re interested in applying for one or more of these roles, please fill out this&nbsp;<a href=\"https://cea-core.typeform.com/to/ShV15HYE?utm_source=CEA&amp;utm_campaign=ProductDesigner&amp;typeform-source=www.centreforeffectivealtruism.org\"><u>application form</u></a> by Tuesday, October 11th 2022, 1:00 am BST. You\u2019re welcome to apply to one, two, or all three roles via the same form.</p><p>We\u2019re committed to creating a diverse and inclusive team, and we welcome applications from people of all backgrounds, identities, and experiences. We\u2019re happy to accommodate any reasonable adjustments you may need during the application process.</p><p>If you have any questions about the roles or the application process, please feel free to email me at&nbsp;<a href=\"mailto:amy@centreforeffectivealtruism.org\"><u>amy@centreforeffectivealtruism.org</u></a>. I\u2019d love to hear from you!</p><p>I hope you\u2019re as excited as I am about the opportunity to join the Events team at CEA and help us create amazing events for the EA community. I look forward to receiving your application and getting to know you better. Good luck!</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e2bb5526efcc9c2784ce11ec134c5f393ef7c06cdb78ac1d.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e2bb5526efcc9c2784ce11ec134c5f393ef7c06cdb78ac1d.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e2bb5526efcc9c2784ce11ec134c5f393ef7c06cdb78ac1d.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e2bb5526efcc9c2784ce11ec134c5f393ef7c06cdb78ac1d.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e2bb5526efcc9c2784ce11ec134c5f393ef7c06cdb78ac1d.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e2bb5526efcc9c2784ce11ec134c5f393ef7c06cdb78ac1d.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e2bb5526efcc9c2784ce11ec134c5f393ef7c06cdb78ac1d.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e2bb5526efcc9c2784ce11ec134c5f393ef7c06cdb78ac1d.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e2bb5526efcc9c2784ce11ec134c5f393ef7c06cdb78ac1d.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e2bb5526efcc9c2784ce11ec134c5f393ef7c06cdb78ac1d.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e2bb5526efcc9c2784ce11ec134c5f393ef7c06cdb78ac1d.png/w_952 952w\"><br>&nbsp;</p>", "user": {"username": "AmyLabenz"}}, {"_id": "4GqL8fiiZ7wKbKvga", "title": "[Cause Exploration Prizes] Outreach to Conservatives and Religious People", "postedAt": "2022-09-22T05:29:21.746Z", "htmlBody": "<p><i>This essay was submitted anonymously to Open Philanthropy's&nbsp;</i><a href=\"https://www.openphilanthropy.org/research/cause-exploration-prizes/\"><i>Cause Exploration Prizes</i></a><i>&nbsp;contest before the deadline. We are uploading some entries late, but all good-faith entries were considered for prizes.</i></p><p>My proposed cause area is outreach, values spreading, and awareness raising among conservatives and religious people in the US.&nbsp;</p><p><strong>Importance</strong></p><ul><li>Two primary mechanisms for positive influence: politics and influencing charitable giving</li><li>Politics&nbsp;<ul><li>Conservatives are more likely to be rural, which means they live in low-population states. Low-population states have an outsize influence on the Senate and the Electoral College. Therefore, we can expect the Republican party to have considerable power going forward.&nbsp;</li><li>A lot of important political issues (AI safety, pandemic preparedness) don\u2019t have an obvious partisan lean.&nbsp;</li></ul></li><li>Charitable giving<ul><li>Religious people are <a href=\"https://www.hoover.org/research/religious-faith-and-charitable-giving\">significantly more likely to give</a> than secular people.&nbsp;</li><li>Religious people also tend to be conservative, so I\u2019m folding this into the overall pitch.&nbsp;</li><li>Religious people\u2019s charitable giving is usually not directed particularly effectively.</li><li>Anecdotally, evangelicals in particular are interested in international causes: <a href=\"https://www.worldvision.org/\">child sponsorship</a>, <a href=\"https://www.npr.org/2013/04/16/177350912/how-evangelical-christians-are-preaching-the-new-gospel-of-adoption\">adoption</a>, and <a href=\"https://team.org/\">missions work</a> (which typically includes anti-poverty work along with evangelism).&nbsp;</li><li>Some key EA causes reflect important evangelical beliefs. For example, malaria causes miscarriages, which is more important to prevent if you\u2019re pro-life.&nbsp;</li><li>Religious giving doesn\u2019t displace sources of funding that go to more effective causes (e.g. long-termism).</li><li>The multiplier matters a lot here: it\u2019s cost-effective if a small grant moves a lot of money to more effective charities.&nbsp;</li><li>Important question: can we expect to find enough cost-effective giving opportunities for all donations?</li></ul></li><li>Speculatively: a lot of people are conservative. Is it good to \u201craise the sanity waterline\u201d and encourage habits of effective thinking in this population?&nbsp;<ul><li>Can we spread ways of thinking about politics to conservative politicians or just positions on particular issues?</li><li>Do habits of thinking effectively about charitable giving expand to other areas?</li><li>Depends on how much you expect global problems to be solved by ordinary people vs. elites.&nbsp;</li><li>Depends on how useful being a <i>little</i> more rational is to solving global problems. Conservatives and religious people don\u2019t seem, on priors, unusually likely to produce exceptionally rational people.&nbsp;</li></ul></li><li>Speculatively: effective altruism could become more welcoming to conservatives?<ul><li>EAs are <a href=\"https://rethinkpriorities.org/publications/eas2019-community-demographics-characteristics\">very strongly liberal</a>; are we missing out on people who can think in an EA way but happen to be conservative?</li><li>Conservatives dominate many EA-relevant jobs (e.g. the military).&nbsp;</li><li>Depends strongly on whether you expect liberals to be better EAs (e.g. <a href=\"https://www.annualreviews.org/doi/full/10.1146/annurev-polisci-051010-111659\">high openness</a>, low religiosity, liberalism is correct so people who believe correct things are more likely to be liberal).</li><li>Chance of conservative presence making EA unwelcoming to current EAs (e.g. trans people, who tend to be overrepresented in technical AI safety research).&nbsp;</li></ul></li></ul><p><strong>Neglectedness</strong></p><ul><li>As I mentioned above, EAs tend to be very strongly liberal. I have personally not observed any outreach towards conservatives.&nbsp;</li><li>Lots of politics-related effective altruist work is for liberal causes (e.g. Sam Bankman-Fried mostly donates to liberals; Carrick Flynn is Democratic; criminal justice reform work is liberal-aligned).&nbsp;</li></ul><p><strong>Tractability</strong></p><ul><li>Politics<ul><li>There is less competition for <a href=\"https://forum.effectivealtruism.org/posts/myympkZ6SuT59vuEQ/go-republican-young-ea\">many key positions in partisan politics</a> if you\u2019re conservative.&nbsp;</li><li>The <a href=\"https://www.vox.com/future-perfect/2019/6/3/18632438/federalist-society-leonard-leo-brett-kavanaugh\">Federalist Society</a> is a conservative legal organization which had enormous effects on American politics.&nbsp;</li></ul></li><li>Charitable giving<ul><li>It is difficult to reach out to conservative religious people when most supporters of effective altruism are liberal atheists.&nbsp;</li><li>It is possible that, once we introduce these ideas, evangelicals will spread them among each other without as much work.&nbsp;</li></ul></li></ul><p><strong>Sample Grants</strong></p><ul><li>A conference which introduces conservative politicians, judges, and intellectuals to effective altruist thinking.&nbsp;</li><li>A retreat for young conservative effective altruists who are interested in policy.</li><li>Founding an EA-aligned think tank which concentrates on outreach to conservatives.&nbsp;</li><li>Advertisements for GiveWell in major conservative religious publications (e.g. Christianity Today).&nbsp;</li><li>Financial support for a writer grounded in both conservative religious and EA communities to translate EA ideas to conservative religious people.&nbsp;</li></ul><p><strong>Open Questions</strong></p><ul><li>Politics<ul><li>How do we avoid unnecessary partisanship? Ideally, we want pandemic preparedness, AI safety, etc to be a bipartisan issue.&nbsp;<ul><li>Seems easier given how liberal EA is?</li></ul></li><li>Is it better to recruit conservative EAs or to lobby conservatives until they are convinced of EA ideas?</li></ul></li><li>Charitable giving<ul><li>Is the message going to degrade to an unacceptable degree?</li><li>How do we avoid coming off as outsiders imposing ideas on evangelicals? Evangelicals are very suspicious of outsiders.&nbsp;</li><li>How do we avoid diluting EA movement quality with evangelicals who think that EA is about giving to GiveWell in the name of Jesus?&nbsp;</li><li>Evangelicals exposed to some effective altruist ideas might decide that saving people from Hell is overwhelmingly important because eternal torture is very bad and they should devote all their energy to that. Assuming Hell does not exist, this consequence would be neutral or even negative.&nbsp;</li></ul></li></ul>", "user": {"username": "Open Philanthropy"}}, {"_id": "k2tBL2nNStZEoc4tF", "title": "[Cause Exploration Prizes] Expanding communication about AGI risks", "postedAt": "2022-09-22T05:30:58.994Z", "htmlBody": "<p><i>This essay was submitted to Open Philanthropy's&nbsp;</i><a href=\"https://www.openphilanthropy.org/research/cause-exploration-prizes/\"><i>Cause Exploration Prizes</i></a><i>&nbsp;contest (before the deadline). We are uploading some back entries late, but all good-faith entries were considered for prizes</i></p><p>&nbsp;</p><p><strong>Author's note: </strong>This piece assumes&nbsp;<a href=\"https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment\">unaligned AGI is a serious threat to humanity</a>, and does not spend time laying out this argument.</p><h1>The case for working on this</h1><p>Since it might be impossible to solve technical AGI alignment in time, AGI governance is an incredibly important area to work on. Moreover, there are other serious risks pertaining to AGI in addition to alignment, such as weaponization or the concentration of extreme power, that will most likely require governance solutions as opposed to technical ones.&nbsp;</p><p>While the field of AGI governance is still largely uncertain about what specific policies would be beneficial, we do have enough information to foresee some of the bottlenecks AGI safety policies are likely to face.&nbsp;</p><p>One fairly common bottleneck that AGI governance may be particularly susceptible to is lack of public support. The status quo of a total absence of public awareness about serious AGI risks could be far from the optimal situation. If the only people who care about AGI risk are a niche group of a few thousand people across the globe, the views of AI/CS experts are mixed, and there are multi-billion-dollar companies who actively oppose meaningful safety policies, it will be very difficult to implement large-scale reforms.&nbsp;</p><p>We could wait until we have a list of good AGI governance policies that we want people to rally behind before trying to build public support around them. The downside of this approach is that outreach will probably be harder to do effectively later. A version of AGI accelerationism may beat us to the punch and become the dominant narrative, especially as programs like DALL-E 2 become popularized, enter the lives of everyday people, and grow increasingly flashier, cooler, and more useful. If this happens, the task will go from persuading people to form an opinion about a subject they had not previously considered to persuading people to change their minds, which is much harder. It is probably preferable\u2014or even essential\u2014for the public, relevant experts, and policymakers to be on \u201cour side\u201d, and delaying efforts to communicate our concerns about AGI makes this harder to achieve.</p><p>A responsible version of bringing AGI risks to the public awareness probably involves lots of focus groups, pilots, feedback, and gradual scaling. There are many ways in which this project could go wrong, and it is very important that we get it right. Among other concerns, we want to make sure the arguments sound compelling to laypeople, the safety measures the public/key stakeholders support are in line with those that are actually helpful, and that AGI safety does not become politically polarized in a counterproductive way.</p><h1>Relevant stakeholders &amp; possible strategies</h1><p>There are three main groups that we could engage in outreach to, outlined below.</p><h2>Government</h2><p>Working directly with government may be the best strategy in some cases, such as when a policy is very technical or the asks we are making are very specific, such that we want technically-versed people in direct contact with policymakers. This can include elected officials, ministers and department heads, policy advisors, and other civil servants.</p><p>Most of this work right now seems to be happening in the US and in Europe at an EU level. It might be worthwhile to do similar types of government outreach in Europe at a national level, as well as in other countries with major technology sectors, such as high-income Asian nations. While they are not currently the main AI hubs, successful outreach there could set a precedent for governments of current AI hubs to take AGI safety seriously.&nbsp;</p><p>However, communication with governments may be insufficient in some cases. Some national or cross-national policies may require broader public support in order to be ratified, especially given the likely corporate resistance they will face. If your constituents do not care about an issue, the media is not covering it, and multiple powerful companies are giving you a hard time or showcasing the short-term economic benefits that AI innovation could bring to your country, you have far less of an incentive to put effort into getting that policy passed than if you would get more political praise for it.</p><h2>Relevant experts and credible figures</h2><p>One challenge for outreach through credible figures is that a lot of the researchers and engineers in the field of AI are doing work that shortens the timeline for the arrival of AGI, and thus have an obvious conflict of interest that makes it much harder for them to want to advocate for measures restricting it.</p><p>For this reason, persuading experts in other fields (e.g. neuroscience, psychology, social sciences, or philosophy) to take AGI risk seriously may be more tractable. Having them on board would add credibility to our concerns and help convince both policymakers and the public that this is a real risk. Public figures with a strong track record for outreach on a different issue or who have previously worked with governments seem especially promising.&nbsp;</p><p>It is also possible that doing outreach to people in CS/AI may still be worthwhile. There have been instances of Google employees having ethical objections to some of Google\u2019s work that other employees presumably thought was fine, such as <a href=\"https://www.nytimes.com/2018/06/01/technology/google-pentagon-project-maven.html\">working with the Pentagon</a> or <a href=\"https://www.forbes.com/sites/jeanbaptiste/2019/07/19/confirmed-google-terminated-project-dragonfly-its-censored-chinese-search-engine/?sh=2ac3fd527e84\">developing a censored search engine for China</a>, and the protest of employees was able to hamper the project. Stopping or slowing down AGI capabilities research in the near term via employee protests is unlikely to work, but employees advocating for more safety research or higher safety standards might nevertheless be constructive and lay the groundwork for making the case to potentially slow down or stop capability research in the medium-term.</p><h2>General public</h2><p>Public support also seems helpful, and in some cases is pivotal. There have been instances of arguments about technology risks that lacked expert consensus (e.g. GMOs or nuclear power) where public concern was enough to lead to bans or shutdowns, even when significant profit motives were at stake. An additional advantage of targeting the public is that it overlaps with the clientele of many of the companies that are working on AGI, and having them become aware of the risks of this technology would put pressure on these companies to increase the resources they put towards safety (if people e.g. boycott Facebook Messenger, then Facebook loses a lot of ad money). It also overlaps with the people AI developers are surrounded by in their personal lives; changing the public sentiment around AGI may make being an AGI developer lower-status and reputationally destructive, which may encourage some people currently working on this to pivot their work to something they would get more praise and social capital for.</p><p>The obvious concern is that AGI alignment may be too weird and sci-fi-sounding for the general public to become invested in it. However, this may be more tractable than it initially appears\u2014 there are experts that specialize in message testing and reframing issues to be more compelling, and some of the lessons from this field might have applications for the case of communicating about AGI risks. As a speculative proof-of-concept, one example of how this could be done is by outlining catastrophic examples of <a href=\"https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity\">specification gaming</a> that could happen due to misaligned AGI other than paperclip maximizers, which sound more plausible to a layperson and will probably be more effective at getting people to seriously consider them. There are also different potential framings (e.g. \u201cAGI will be to humans what humans are to monkeys\u201d, \u201cAGI is a genie that cannot be put back in the bottle\u201d, \u201cAGI could be the next nuclear bomb\u201d, \u201cspecification gaming already happens and will get worse\u201d, etc.) that may work best for different audiences.&nbsp;</p><p>A second concern is that solutions that sound helpful to lay-people may be different from the solutions that are actually helpful. It is possible that an AGI development lab would push for a \u201csafety policy\u201d that doesn\u2019t actually reduce existential risk but gives people the illusion that the issue is solved over the short term, and the public interest that was generated as a result of these efforts would rally behind it.&nbsp;</p><p>In order to prevent this, outreach efforts would need to provide interested citizens with a reliable way of distinguishing probably-good from probably-bad AGI safety policies, such that they will support the ones most likely to be helpful. This could look something like setting up an official board of renowned AGI safety experts to independently and publicly rate different safety policies, while encouraging people to place huge value on these ratings.&nbsp;</p><p>Getting public outreach right may be very difficult, but given the potential value, seems worth at least exploring. It is possible that, with short notice, we will realize we need to resort to a drastic AGI governance solution that requires public support. It would be helpful if we spent resources now figuring out how or whether we could deploy public support effectively and responsibly in case it turns out to be a useful tool. As of right now, it seems like no one has seriously looked into this or rigorously evaluated it as a strategy.</p><h1>What funding this could look like</h1><p>An AGI safety communications initiative should probably involve an iterative loop between research and implementation, i.e. research involving practical pilots to test out preliminary conclusions and gather feedback, and then scaling and implementation of the more promising approaches. An approach involving addressing all the questions and uncertainties first could take a decade, and not doing any outreach work for that long could be very damaging. Therefore, the two efforts should go hand in hand and improve each other.&nbsp;</p><p>Some areas that would be important to research further are:</p><ul><li>Find a model to help concerned citizens parse policies that are the most likely to be actually helpful (probably involving some sort of deferring their judgment), or figure out whether this is possible</li><li>Generate more intuitive and compelling ways of framing and explaining the alignment problem to a layperson, and test them in focus groups or surveys; do similar work for other risks</li><li>Study how we can frame AGI safety in a way that has bipartisan appeal; study properties <a href=\"https://publicconsultation.org/defense-budget/major-report-shows-nearly-150-issues-on-which-majorities-of-republicans-democrats-agree/\">bipartisan issues</a> have in common, study how issues like climate change or vaccines became politically polarized, look into relevant implications of <a href=\"https://en.wikipedia.org/wiki/Moral_foundations_theory\">moral foundations theory</a></li><li>Ethnographic work on groups we are most interested in persuading</li><li>Study what happened with e.g. GMOs\u2014how the public influenced policy even with a lack of expert consensus behind it and significant profit motives at stake</li><li>Study why the public became concerned about the risks of certain technologies and not others</li><li>Study why existing media coverage of AGI safety hasn\u2019t resulted in widespread citizen concern</li><li>Evaluate and compare the effectiveness of different approaches to public communication&nbsp;</li></ul><p>This research would hopefully provide some clarity about what the outreach should look like. There are a wide range of different models and theories of change for this, such as bottom-up grassroots campaigns versus top-down campaigns led by experts, or \u201cshout it from the rooftops and tell everyone who will listen\u201d models versus funnels with higher barriers of entry for citizens to take action. Some of the forms the public outreach could take are:&nbsp;</p><ul><li>Funding more longform media, such as a documentary version of Human Compatible/Superintelligence/The Alignment Problem, a detailed YouTube video series outlining the alignment problem and debunking common responses, etc.</li><li>Funding more longform media specifically about non-alignment AGI risks, which there are comparatively fewer books and other written content about</li><li>Encouraging and funding AGI safety researchers to be more vocal and develop a platform online to talk about safety concerns publicly; make it really easy for credible people concerned about AGI to have a platform to voice their concerns</li><li>Sharing longform media about AGI risks with influential opinion makers&nbsp;</li><li>Organizing&nbsp;events to connect&nbsp;journalists and&nbsp;similarly&nbsp;relevant figures to the field of AGI safety</li><li>Most people are not aware of how far AI has come\u2014publicize models like DALL-E 2 or GPT-3 to show people that AGI might not be as far as it seemed very recently, or similar interventions to show that AGI is not as far away as it may seem</li><li>Creating a pledge for politicians to say they commit to AGI safety in order to identify who the most likely allies are for future governance policies</li><li>Publicizing <a href=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml\">examples</a> of <a href=\"https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity\">specification gaming</a> that have already happened</li><li>Getting existing YouTube channels or technology&nbsp;\u201cinfluencers\u201d to start talking about&nbsp;AGI risks (like Open Philanthropy sponsoring Kurzgesagt\u2019s&nbsp;<a href=\"https://www.youtube.com/watch?v=LEENEFaVUzU&amp;ab_channel=Kurzgesagt%E2%80%93InaNutshell\">video</a> about longtermism)</li><li>Trying to get journalists to cover AGI risks in mainstream media</li><li>Identify possible partners who could help in influencing the general public, e.g. NGOs concerned with data protection or economic inequality</li><li>Translate existing AGI safety work into Mandarin and actively relay it to Chinese audiences</li><li>Compile a database of people willing to call or email their elected officials to support an AGI safety policy; create a pipeline for the most concerned citizens to contact their representatives&nbsp;</li><li>Citizen groups, town hall formats, public fora (see eg. Audrey Tang\u2019s work as part of Taiwan's \u201cg0v\u201d project)</li><li>Creating fiction about plausible AGI risks (not e.g. The Terminator), though this would require testing whether it makes people more or less likely to believe it could happen in real life</li></ul><h1>Who is already doing something like this?</h1><ul><li>Existential Risk Observatory: ERO has previously organized conferences and events to connect AGI safety researchers to journalists, co-authored op-eds, and gotten tech columnists and podcasters to talk about existential risk from AI. Their stated objective is to \u201c[reduce] human existential risk by informing the public debate\u201d, and \u201cSpread existential risk information from academia to academia, think tanks, policy makers, and media.\u201d This organization is still fairly small and fairly new.</li><li>Rob Miles: Miles has a YouTube channel about AI safety with 103K subscribers (as of time of writing). He describes his approach as \u201cExplaining AI Alignment to anyone who'll stand still for long enough\u201d.&nbsp;</li><li>Podcasts such as the 80,000 Hours podcast or Hear This Idea, which sometimes feature episodes about AGI safety.&nbsp;</li><li>Authors, such as Stuart Russell, Nick Bostrom, Brian Christian, Eliezer Yudkowsky, Toby Ord, and others.</li><li>The Future of Life Institute does some advocacy work regarding their areas of focus, which include AI safety. For AI safety in particular, they have a YouTube channel and have made films, a podcast, worked with journalists, hosted events, and engaged in direct dialogue with governmental organizations including the United Nations, the OECD, US federal agencies, state governments, and the military, and the European Commission. FLI tends to focus on near-term risks when communicating to policymakers, and outreach about AGI is by no means the primary focus of the organization.</li><li>Efforts to encourage people to pursue AGI safety careers (such as those by 80,000 Hours or <a href=\"https://forum.effectivealtruism.org/posts/ozm4SpiChfAAAGnw5/announcing-the-ai-safety-field-building-hub-a-new-effort-to\">Vael Gates</a>) could also be considered a form of AGI safety outreach, albeit more narrow in scope than what is being proposed here.</li><li>The Center for AI Safety recently announced a <a href=\"https://www.alignmentforum.org/posts/gWM8cgZgZ9GQAYTqF/usd20k-in-bounties-for-ai-safety-public-materials\">bounty</a> for AI Safety Public Materials</li></ul><h1>Possible risks</h1><h3>Ridicule</h3><p>If we make arguments about AGI risk that sound ridiculous, it may make people become convinced of the opposite of what we want\u2014that AGI risks are a ridiculous issue to care about. This is why it is especially important to test in focus groups which framings sound compelling and resonate the most with laypeople.</p><p>If we lack transparency, it is also possible that this work could be suspected of being a bad-faith disinformation campaign by a malicious actor designed to slow down AI development in the West/the United States/Europe/etc.</p><h3>Disadvantageous political polarization&nbsp;</h3><p>If this becomes an issue that appeals only to one political orientation, it may lead to the other side of the aisle opposing it instinctively. If that side has more political power at a given time, this could be net negative. There could be ways to counter this, such as studying the properties of issues with bipartisan support and trying to adopt them. It may also be the case that this is less of a risk in countries without a bi-party political system, e.g. Netherlands or Germany, in which case the outreach could initially be piloted there.</p><h3>Information hazards</h3><p>Talking about the dangers of AGI or how powerful and versatile this technology can be could further fuel an arms race. It may also encourage some people to attempt to develop it for nefarious purposes.&nbsp;</p><h3>Policies the public supports may not align with helpful policies.</h3><p>There are three scenarios in which this could be harmful:&nbsp;</p><ol><li>The public rallies behind a counterproductive policy that harms safety thinking it actually helps safety, e.g. by shifting who is at the forefront of AGI development to a more irresponsible actor.&nbsp;</li><li>The public thinks a policy that helps with AGI safety actually hinders AGI safety and rallies to obstruct it, e.g. we propose some limits on the computing power and complexity of neural networks and AI labs argue that this hinders their safety work.&nbsp;</li><li>The public rallies behind an unhelpful policy and diverts political resources from a helpful policy (though it is not especially obvious that this would make the helpful policy worse off than in the counterfactual where there was no outreach).</li></ol><p>There may be case studies of this happening to advocacy about other issues that we can learn from, as well as tools we can implement to help citizens parse probably-good and probably-bad safety policies (such as the aforementioned board of AGI safety experts that independently rate them).</p><h3>Producing counterproductive fear&nbsp;</h3><p>We want citizens to be concerned\u2014the main reason anything got done about climate change or denuclearization is because everyday citizens started caring. That said, citizen concern could escalate into levels where it is counterproductive.</p><p>Extinction-level events can be difficult to picture and emotionally process, leading to overwhelm and inaction (see <a href=\"https://www.urmc.rochester.edu/behavioral-health-partners/bhp-blog/march-2020/coping-with-climate-change-anxiety.aspx\">climate paralysis</a>). In other cases, it can result in disproportionate and unwise policies\u2014if we lack nuance, it can turn people against the field of AI as a whole, blocking the progress of really valuable technology.</p><p>When evaluating the probability of this risk, it is worth highlighting that it is very difficult to internalize existential risk from unaligned AGI\u2014even many current AI safety researchers took years of convincing in order to believe this was a problem they should work on\u2014so it is not clear that this would easily escalate into everyone becoming convinced that the end of the world is around the corner. If this turns out to indeed be a risk, it can be mitigated by talking about examples of really bad things that misaligned AGI could do that fall short of causing the extinction of humanity, such that we aren\u2019t advertising that this is going to happen.&nbsp;</p><h1>Conclusion</h1><p>Over the past few months, I have had informal conversations about this topic with researchers working on alignment, as well as individuals from organizations like GovAI, FLI, FHI, and Google. While their levels of optimism varied, they agreed that if we can figure out how to do it safely and responsibly, this seems like a broadly good idea\u2014from \u201cworth looking into\u201d to \u201cessential\u201d. It would give policymakers an incentive to care about this issue, which is currently lacking, as well as possibly put pressure on those working on AI capabilities to halt their work or increase standards for safety.&nbsp;</p><p>However, the resources the EA community is currently devoting to this are minimal, as it falls outside the scope of most existing AI safety organizations. There is currently an absence of rigorous research looking into the potential value, risks, and strategies for communicating with the public and key stakeholders about AGI risks. While there are important ways this could go wrong, if it goes right, work in this area could create a lot of value and meaningfully increase the probability that humanity survives the alignment problem.&nbsp;</p>", "user": {"username": "Ines"}}, {"_id": "hwuADmznG94dKxiQT", "title": "Berlin AI Alignment Open Meetup September 2022", "postedAt": "2022-09-21T15:09:37.883Z", "htmlBody": "<p>Hello everyone,</p><p>we're a group of ~3 people trying to do independent AI alignment research in (some of) our free time. This is a special occasion after EAGx Berlin, so we're not meeting up on the weekend.</p><p><strong>If you're doubtful whether to show up to the meeting, just show up</strong>.</p><p>I'm fairly open for basically anyone to attend, because I'm really interested in seeing who here in Berlin is interested in this topic and would like to do something about it :-)</p><p>We're meeting at <a href=\"https://www.eden-foodgarden.com/\">Eden Foodgarden</a> near Hackescher Markt.</p><p>I'm really looking forward to this, see you around :-)</p>", "user": {"username": "pranomostro"}}, {"_id": "vzmArGsxodBzfYzx9", "title": "Announcing Applications Now Open for Johns Hopkins Center for Health Security\u2019s PhD Opportunities", "postedAt": "2022-09-21T14:27:04.420Z", "htmlBody": "<p><strong>Summary</strong></p><p>The Johns Hopkins Center for Health Security is pleased to announce it is now accepting applications for its PhD opportunities for the 2023-2024 academic year. More information and application details can be found&nbsp;<a href=\"https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.centerforhealthsecurity.org%2Four-work%2Feducation%2F%23phd&amp;data=05%7C01%7Ctksell%40jhu.edu%7Ceecc1ec10235449896f708da97529354%7C9fa4f438b1e6473b803f86f8aedf0dec%7C0%7C0%7C637988678153572645%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=Ome9EqPNZaLWafAVPu%2FxRTLnZse84fPqY0IlxSXz2pc%3D&amp;reserved=0\"><u>here</u></a>. You can feel free to reach out to me (Tara Kirk Sell; <a href=\"mailto:tksell@jhu.edu\"><u>tksell@jhu.edu</u></a>) or a current student (Caitlin Walker; <a href=\"mailto:cwalke88@jhmi.edu\"><u>cwalke88@jhmi.edu</u></a>) for additional information.</p><p><strong>Health Security PhD Opportunities</strong></p><p>In a world of rapid innovation in the biological sciences, the emergence of new diseases, and changing environmental pressures, health security risks to the global community are a rising concern. The field of health security has a growing need for trained expertise that can provide science-based solutions and inform global policies to shape preparedness and response efforts.</p><p>Selected students will receive full funding to complete their PhDs in the&nbsp;<a href=\"https://publichealth.jhu.edu/academics/phd-in-environmental-health/track-in-health-security\"><u>Health Security track</u></a>&nbsp;within the Johns Hopkins Bloomberg School of Public Health\u2019s&nbsp;<a href=\"https://publichealth.jhu.edu/academics/phd-in-environmental-health/track-in-health-security\"><u>Department of Environmental Health and Engineering</u></a>. Applicants must indicate their interest within their SOPHAS doctoral program applications, which are due on December 1, 2022. More information and application details can be found&nbsp;<a href=\"https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.centerforhealthsecurity.org%2Four-work%2Feducation%2F%23phd&amp;data=05%7C01%7Ctksell%40jhu.edu%7Ceecc1ec10235449896f708da97529354%7C9fa4f438b1e6473b803f86f8aedf0dec%7C0%7C0%7C637988678153572645%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=Ome9EqPNZaLWafAVPu%2FxRTLnZse84fPqY0IlxSXz2pc%3D&amp;reserved=0\"><u>here</u></a>.</p><p>These funding opportunities are supported by Open Philanthropy and are intended for students with an interest in the field of health security, particularly in pandemics and GCBRs.</p><p><a href=\"https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.centerforhealthsecurity.org%2Four-work%2Feducation%2F&amp;data=05%7C01%7Ctksell%40jhu.edu%7Ceecc1ec10235449896f708da97529354%7C9fa4f438b1e6473b803f86f8aedf0dec%7C0%7C0%7C637988678153572645%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=7zOo8MJgYgFptp6uH52dX1W6GJC50iQbKpj7yWPChHo%3D&amp;reserved=0\"><u>Learn more about the Center for Health Security\u2019s educational opportunities</u></a>.</p>", "user": {"username": "tksell"}}, {"_id": "2jWLvBH8abzscDbPs", "title": "Announcing \u201cEffective Dropouts\u201d", "postedAt": "2022-09-21T14:00:26.737Z", "htmlBody": "<p>TL;DR: \u201cEffective Dropouts\u201d is meant to be a casual fun excuse for reminding people that dropping out of a degree can be a good decision, as a counter to all the existing pressure for degrees being the default/only/obvious way. The rest of the post is mostly a joke.</p><h1>Our values</h1><p>We strive to have the values of Effective Altruism plus the inverse of the values of universities.</p><h1>Who can join</h1><p>Anyone who drops out of a degree.</p><p>But, having the inverse of university values, we are not strict about our criteria. For example, one of our founders, Yonatan, didn\u2019t drop out of a degree, but he can join anyway. He\u2019s also writing these lines right now by the way.</p><h1>What can joining provide?</h1><p>You can, if you want, join as a co-author of this post and change it however you want. Universities would tell you what to do. We would just open a new \u201corganization\u201d if we don\u2019t like what you did with ours.</p><p>Ah, and also moral support, and if things go really well, we might have stickers.</p><h1>Do we know anything about university?</h1><p>Yeah, one of the co-founders, Vorathep, is Head of Coaching at Effective Thesis and used to have his own coaching company targeted at university students. Vorathep is a PhD student right now, but thinks dropping out can happen at any level, so don't lose hope!</p><h2>What about the other cofounder(s)?</h2><p><strong>Yonatan</strong> visited his friend in university for one day to see what all the fuss is about, after which he convinced his friend to quit. This was a pretty productive day overall, and Yonatan wonders if this means he should actually start going regularly.</p><p><strong>Gavin</strong> is a weird choice for a co-founder because he has 4 degrees, but this too is aligned with our values.</p><p><strong>Jonny</strong> dropped out of his law degree and regards it as one of the best decisions he's ever made. He's now a programmer and strong proponent of not doing things just because your parents think they're a good idea.</p><h1>Do we think everyone should drop out?</h1><p>No.</p><p>We\u2019ll be politically correct and say \u201conly some people\u201d.</p><p>Saying \u201chere is the path everyone should go\u201d is against our values. We encourage people to consider their own specific situation, and maybe tell us more about it.</p><h1>Are we giving a balanced view of both sides?</h1><p>No, but unlike universities, we are not pretending to give a balanced view, so we think it\u2019s more fair.</p><h1>What do we recommend doing instead of university?</h1><p>Well, if you\u2019re going to university in order to get accepted to some company, X, then as a naive suggestion, try getting accepted to X right now. Maybe it will just work? Unless you plan, for example, on working in a nuclear power plant, in which case please get the appropriate training first, whatever they say it is, university or otherwise. Or actually, apply and if you don\u2019t get accepted, ask for feedback on what to learn, why not?</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/2jWLvBH8abzscDbPs/lykf7wtb13zqly5n736l\" alt=\"As a nuclear safety inspector, how much should Homer Simpson make in a  year? - Quora\"></figure><p>Homer getting some on-the-job training. Might not be ideal for this specific line of work.</p><h1>What about all the useful courses there are in university?</h1><p>Looking at the useful courses is what we call \u201cglass half full thinking\u201d. We try encouraging \u201cglass half empty thinking\u201d. What about all the un-useful courses?</p><p>Or more to the point, can you do the useful courses online?</p><h1>But university will help open more doors, no?</h1><p>Yeah, it will help in a non-zero amount, but we think this is the wrong question.</p><p>We like asking \u201cwill it help more than the alternative\u201d.</p><p>Did you consider any alternative?</p><p>As a naive example, \u201cdoing a Udemy course will get me a job in X time, and doing a degree will get me a job in Y time\u201d.</p><p>We call this \u201cthinking outside the box\u201d, where in this case the box is university.</p><p>We also think about this from the Effective Altruism values. We prefer not asking \u201cwill this donation help anything\u201d.&nbsp;</p><h2>we prefer asking \u201cwill it help more than the alternative\u201d.</h2><p>Yep, I just made that into a title because I wanted to.</p><h1>But university teaches the basics, no?</h1><p>University teaches things that they&nbsp;<strong>brand</strong> as \u201cthe basics\u201d. It\u2019s a marketing trick that went too well, I have no idea who thought of it, but I want to hire them.</p><h2>So what are&nbsp;<s>the basics</s> the things to know?</h2><p>Here\u2019s a useful rule of thumb: If you want to be good at X, do something as close as possible to X, and get feedback.</p><p>Is the thing you want to do similar to what you\u2019re doing in university?</p><p>We would tell you the answer to this, but \u201cappeal to [our] authority\u201d would be against our values, so maybe ask someone who is doing the thing you want to do, plus maybe has a degree?</p><h1>Call to action</h1><ul><li>Share your story in the comments. What did you learn? Where do you work? What would you advise your younger self?</li><li>Ask questions, are you trying to make a career decision? Would you like people to comment publicly and for others to correct advice that seems wrong? In universities this advice is done behind closed doors, so we thought we\u2019d do it differently</li><li>Help us design a logo so that we can print stickers</li><li>Suggest more ways we can live by our values, undermine our authority, and cause chaos</li></ul><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/2jWLvBH8abzscDbPs/gcnnzm08ebnsfnqn8tyb\"></figure><p><br>&nbsp;</p>", "user": {"username": "hibukki"}}, {"_id": "qj6hTW76RwQrcj7wL", "title": "Join EA Bioethics (and a trial of having groups on the forum)", "postedAt": "2022-09-21T13:56:10.227Z", "htmlBody": "<h3><strong>EA Bioethics just moved from Slack to a </strong><a href=\"https://forum.effectivealtruism.org/topics/bioethics/subforum\"><strong>subforum</strong></a><strong> on here (the EA Forum)! </strong><a href=\"https://forum.effectivealtruism.org/topics/bioethics/subforum\"><strong>Join here</strong></a><strong>!</strong></h3><p>While testing groups existing on the Forum (as opposed to Slack, FB group, etc) probably shouldn't be the main reason you join, this is an open invitation for anyone interested in bioethics to join :)<br><br>EA Bioethics is a pretty broad group of people who enjoy thinking about (in any capacity, professionally and for fun):</p><ul><li>&nbsp;research ethics --&gt; how to we do research well? what is ethical research</li><li>medical ethics --&gt; how do we make sure medicine is ethical? patients are well treated, we maximise scientific knowledge while minimising research and patient harms&nbsp;</li><li>global (health) priorities research</li><li>biosecurity, dual use research, gain of function research&nbsp;</li><li>moral philosophy and ethics</li><li>grantmaking --&gt; what governments and institutions fund?</li><li>how should we measure and think about disease</li><li>and so much more!</li></ul><p>If this interests you in any way, <a href=\"https://forum.effectivealtruism.org/topics/bioethics/subforum\">join our group</a>, post an intro about yourself, and we can go from there :)</p>", "user": {"username": "ElikaSomani"}}, {"_id": "YCMgg6x6zWJmran5L", "title": "Criticism of the 80k job board listing strategy", "postedAt": "2022-09-21T09:36:20.969Z", "htmlBody": "<p>update [2023-09-11]: my criticism here is mostly resolved following <a href=\"https://forum.effectivealtruism.org/posts/YCMgg6x6zWJmran5L/criticism-of-the-80k-job-board-listing-strategy?commentId=FiDhKgpfjPCiqQBTA\">changes made to the 80k job board</a></p><hr><p>TL;DR: Here\u2019s a&nbsp;<a href=\"https://twitter.com/YonatanCale/status/1566054675295649793\"><u>poll</u></a> for the&nbsp;<a href=\"https://twitter.com/i/communities/1492420299450724353\"><u>EA Twitter community</u></a>:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YCMgg6x6zWJmran5L/cujzv2v8swht26aivwap\"></p><h3>I\u2019ll describe the poll in words, mainly for listeners of the Nonlinear Podcast:</h3><p>The poll says \u201cThe 80k job board isn\u2019t \u201ca list of impactful jobs\u201d, it also has other jobs meant to build career capital, and there\u2019s no way to tell which is which. 1: Did you know this? 2: Is it important?</p><p>55% of the poll respondents<strong>&nbsp;</strong>didn\u2019t know this is the situation, and they think it\u2019s important. The other answers got about 15% of the votes each. There were 140 votes in total.</p><h1>Why I think this is important</h1><p>Many professionals in EA want to move to the stage in their career where they have a lot of impact, and they want to pick a job for that, and want something like a job board with high impact jobs.</p><p>I always thought 80k\u2019s job board was that.</p><p>I recently learned that 80k don\u2019t even try to be \u201ca list of high impact jobs\u201d, they also list some roles for mainly career capital reasons, plus some other problems listed below.</p><p>Sad emoji \ud83d\ude3f</p><h1>Read more</h1><p>Caleb, head of EA Funds, posted a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LrxLa9jfaNcEzqex3/calebp-s-shortform?commentId=Bq4gFzMNsmpuoi95A#uxNWFsWXqdTjak8Gu\"><u>shortform</u></a> titled \u201cThe 80k job board has too much variance\u201d.</p><p>In the comments, we discuss problems like the 80k posting jobs that might be&nbsp;<strong>actively harmful</strong>, and there being no [reasonable in my opinion] way to push back on that.</p><h2>Solutions</h2><p>TL;DR:&nbsp;</p><ol><li>Have a job board that aims to only include high impact jobs.</li><li>Let people comment and discuss whether jobs are high impact.</li><li>Better communicating the current situation seems positive (and is a big reason I think posting this is good), but I don\u2019t personally think it\u2019s enough.</li></ol><p>I\u2019m not elaborating in order to keep this post short, but we can discuss in the comments, or maybe someone has a better idea.</p><h1>80k\u2019s response</h1><h2>In the&nbsp;<a href=\"https://80000hours.org/job-board/\"><u>job board</u></a> page:</h2><p>They have a big title that says \u201cSome of these roles directly address some of the world\u2019s most pressing problems, while others may help you build the career capital you need to have a big impact later.\u201d:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YCMgg6x6zWJmran5L/moz0pxihrkw4w1m9v3uc\"></p><h2>80k\u2019s website, on promoting some roles at potentially harmful organizations</h2><p>80k have a&nbsp;<a href=\"https://80000hours.org/job-board/?tab=faq\"><u>FAQ</u></a> called \u201cYou're promoting a role at an organisation that I think is causing harm. Why is this?\u201d, where the answer is (in my words), that this might be useful for building career capital or for helping improve the org from the inside:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YCMgg6x6zWJmran5L/uw87wj6zpb1tggm7223q\"></p><h3>Criticism: \u201cImproving the org from the inside\u201d:</h3><ol><li>A lot of 80k\u2019s audience are not highly involved in the EA community, these are simply people who reached 80k through 80k\u2019s amazing SEO and marketing. These people are not who I\u2019d pick to \u201cimprove the org from the inside\u201d</li><li>This isn\u2019t mentioned in the org\u2019s job description! I might take a job that 80k thinks could have a high impact if I improved the org from the inside - but I won\u2019t know about this - and I\u2019d just help the org with its current agenda</li><li>I think it\u2019s questionable whether \u201cimproving the org from the inside\u201d (if the org is doing harm) is positive or negative expected value, and I\u2019m tempted to elaborate on how I\u2019d analyze this question, but my short version is \u201cthis is complicated and needs to be discussed\u201d especially for someone going to take such a job.</li></ol><p>Regarding taking a harmful job in order to build career capital:</p><h2>80k\u2019s article on taking a harmful job</h2><p>Their&nbsp;<a href=\"https://80000hours.org/articles/harmful-career/\"><u>post</u></a> was updated recently, not years ago.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YCMgg6x6zWJmran5L/ccut0qj8nleyqcmvcobo\"></p><p>I would say that it might sometimes be ok to take a harmful job in order to do more good, but it is not ok to send other people to a harmful job without them knowing about it (plus, hopefully, thinking about the pros and cons of their situation for a few minutes).</p><h2>Criticism on closed door push backs:</h2><p>80k suggests that the way to push back on a job is to send them a private message:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YCMgg6x6zWJmran5L/pey94dzl4iqlkpe5fc7h\"></p><p>I think this is problematic - they might forget about such a message, or miss it because of too much work, and so on. I think this problem is bigger than I am actually writing explicitly, but I\u2019m leaving it at this for now.</p><h2>80k\u2019s response on the forum:</h2><p>Niel Bowerman (who\u2019s great and kind of changed my life) (Director of Job Board, manager of Kush, head of the 80k job board) in this&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LrxLa9jfaNcEzqex3/calebp-s-shortform?commentId=NctFbBmyWPm4Tuyog#uxNWFsWXqdTjak8Gu\"><u>comment</u></a> said:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/YCMgg6x6zWJmran5L/i3z154zzp8jgaacwm8ej\"></p><blockquote><p>Giving our guess of the extent to which each role is being listed for career capital vs impact reasons isn't feasible for various reasons unfortunately.</p></blockquote><h2>Kush (the new Head of Job Board at 80k (best job in the world!)) explains Niel\u2019s answer:&nbsp;</h2><blockquote><p>What he meant in this comment was that we can't assign scores for career capital or direct impact as all roles are: (1) an opportunity for someone to have direct impact against a problem and (2) an opportunity to develop skills to work on that problem in the future, and splitting this out is hard!</p></blockquote><h1>I\u2019m discussing and collaborating with Kush (head of job board)</h1><p>We are talking publicly and one on one. As one example, we collaborated on making the&nbsp;<a href=\"https://80000hours.org/2022/08/open-position-full-stack-web-developer-job-board/\"><u>80k full stack developer job post</u></a> better. I\u2019m a big believer in that role btw, and if you\u2019re a fullstack developer with a product mindset, I encourage you to apply and join Kush to work on this together.</p><p><br>&nbsp;</p>", "user": {"username": "hibukki"}}, {"_id": "PiFCF7dcAxBpc5dsN", "title": "Civilization Recovery Kits", "postedAt": "2022-09-21T09:26:10.796Z", "htmlBody": "<h1>TL;DR</h1><p>Placing kits with helpful tools and information around the world in order to improve the chances of civilization recovery in the case of a 99.999% to 99.9999% population decrease (approx. 100K to 10K individuals alive).</p><h1>Goals</h1><ul><li>Increasing chances of survival and successful civilization recovery</li><li>Preserving human knowledge</li><li>Enabling making contact quickly with more humans</li></ul><h1>Operations</h1><ul><li>Kits will be placed in well known places (e.g. libraries, museums, public transport stations, etc).</li><li>Items in the kit will be updated every X years (this will be funding bottlenecked)</li></ul><h1>Funding / Business Model</h1><h2>Option 1: Use EA funding</h2><p>spread kits around the world</p><h2>Option 2: Sell the kits</h2><ul><li>Maybe explicitly say something like \u201cfor each kit you buy, we\u2019ll put another kit somewhere else in the world\u201d.</li><li>Maybe sell a subscription where we send updates to existing kits periodically.</li></ul><h2>Open Source</h2><ul><li>Kit specification with all contents be publicly available and open source</li><li>A guide for making your own kit will be available</li></ul><h1>What will be in the kit?</h1><p><i>Epistemic status: Initial idea exploration</i></p><p>These kits could include (incomplete list):</p><ul><li>Information / Knowledge<ul><li>Paper copy of Wikipedia</li><li>Paper map with more recovery kit locations</li></ul></li><li>Tools<ul><li>Manual electrical generator (e.g. with a hand crank)</li><li>Laptop with useful things:<ul><li>Offline copy of Wikipedia</li><li>Select scientific textbooks</li><li>Important open source projects<ul><li>Linux kernel</li><li>Math libraries</li></ul></li><li>Scientific software</li><li>Offline world map</li><li>Ability to replicate laptop contents to other&nbsp;<s>looted</s> found computers</li></ul></li><li>Radio transceiver<ul><li>Maybe with instructions to extend range</li></ul></li></ul></li><li>Extras<ul><li>Some candy to lighten the mood?</li><li>A collection of zombie games to \u201clighten the mood\u201d</li></ul></li></ul><h1>Side Notes</h1><h2>What about security / vandalism / premature looting?</h2><p>A na\u00efve solution (and which can probably be improved) is locking the box as long as there\u2019s electricity. If you care about the security of this, more ideas welcome</p><h2>More Operation Stuff</h2><ul><li>Kit updates are \u201cappend only\u201d for existing kits<ul><li>Digital content updates will be sent on physical media</li><li>Tools will be sent with a courier</li></ul></li><li>The boxes may have an \u201cinsert only\u201d opening to allow these updates without opening to box</li></ul><h2>More About Goals</h2><ul><li>This project is a bit different from&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/KigFfo4TN7jZTcqNH/the-future-fund-s-project-ideas-competition?commentId=Q8kL834jndvHkhDpq#Q8kL834jndvHkhDpq\"><u>previous discussions</u></a>. I don\u2019t want to be too opinionated about \u201cHow to recover\u201d, the primary goal is knowledge preservation and let people figure out what to do with it.</li><li>This project is not aimed to help in&nbsp;<i>all</i> kinds of catastrophes.</li></ul><h1>Call To Action</h1><ul><li>Help me figure out how impactful this might be</li><li>Fix critical flaws in this idea</li><li>Open this project yourself and I\u2019ll help with all the engineering</li><li>Convince me I should lead this project<ul><li>(Maybe partner with me about this?)</li></ul></li></ul><h3>Less Important call to action</h3><ul><li>Suggest updates to this list with other useful things</li></ul><p>P.S. this idea came to me in a dream a few days after EAGx Berlin 2022, not sure what to do about that \ud83d\ude43</p>", "user": {"username": "Soof Golan"}}, {"_id": "KWBfZyR2o45vCnBuT", "title": "Software Engineer: what to do with 3 days of volunteering?", "postedAt": "2022-09-21T15:37:50.328Z", "htmlBody": "<p>Hi,</p><p>My company provides me with 3 annual days off for volunteering. Is there anything I can do in these 3 days that leverages my main skill (software engineering)? If not I will volunteer elsewhere, but I suspect the most effective use of my time would still be to leverage my current skills. I am happy to hear your ideas.</p>", "user": {"username": "Louis Mackenzie-Smith"}}, {"_id": "wQTSxgG9GcHyYLYeS", "title": "\u2018Where are your revolutionaries?\u2019 Making EA congenial to the social justice warrior. ", "postedAt": "2022-09-21T08:46:20.225Z", "htmlBody": "<p>With a career sat at the intersection between social impact and global health, I have developed an allergy to expressions like <i>\u2018democratising access to X\u2019</i> and \u2018<i>blockchain-enabled solutions to *insert social injustice here*\u2019</i>. My teeth have been cut by the UN, Think Tanks, Corporate Philanthropy and a range of advocacy groups, so I know how empty such language can really be. Value signalling, buzzword bingo and CorporateSpeak are all examples of the ways in which our careful choice of words can give the impression of impact, of common cause, while deflecting attention and scrutiny.&nbsp;</p><p>&nbsp;</p><p>When we agree to the ways in which we speak we are, to an extent, agreeing on the ways in which we are allowed to think. By adopting a shared lexicon, we dull our analytical thinking and trade our agency for a thin sense of camaraderie or in-group identity. As any self-help guru can tell you, with strength in numbers it becomes easier to dress up common sense as the profound. That\u2019s why I was so suspicious when I was introduced to Effective Altruism, worrying these new key terms would be just as vapid as the boardroom mantras I already had tattooed to the tip of my tongue and the pit of my stomach. But I\u2019m here, writing on the EA Forum after a whirlwind weekend at EAGxBerlin. So clearly something must have gone right.&nbsp;</p><p>&nbsp;</p><p>My conference experience was, I\u2019m sure, fairly typical: over-caffeinated, under-slept and socially saturated, but I know I\u2019ve caught the EA bug. Although I wouldn\u2019t allow myself to use words I didn\u2019t fully understand or believe in just to fit in, the sincerity of those I met combined with the hard logic of the premises at play were enough to see this social justice warrior parley with the best of them. That said, while I may have tried the Kool-Aid, I\u2019m certainly not drunk on it. There were aspects of the conference that I felt were lacking and some conversations that left me cold. The thing which concerned me most, however, was the intellectual homogeneity I saw. Scout versus soldier mindsets aside, I didn\u2019t come across that many delegates who really had their boots on the ground. I looked around to find software developers, scientists, researchers, policy-makers and other trutlenecked pontificators, but a very poor showing from the creative world.&nbsp;</p><p>&nbsp;</p><p>This is to EA\u2019s detriment. We learn as much from fiction, from art, as we do from instruction and rebellion is found in the literary world or the fresco long before it hits the debating floor. So where are our radicals? Where are our writers? Our artists? Our poets? Our anarchists? Unless EA changes its positioning soon, it is so obvious to me that this well-meaning platform will remain a sparring ground of ideas, of ivory towers, and not of grassroots or picket lines.&nbsp;</p><p>&nbsp;</p><p>We need these trailblazers. The march of human progress \u2013 okay the cha-cha slide of human progress \u2013 is defined by the coupling of the radical with the practical. Be it the tandem influence of Martin Luther King and Malcom X for civil rights, or of the Suffragettes and Suffragists for women\u2019s suffrage, one cannot make headway on social injustice without the counterbalance of the other. The radicals are there to stir up the public frenzy that threatens power into action while making the moderates seem reasonable enough to hold the pen. Without them, attempts to evoke change from the inside would only suffocate.&nbsp;</p><p>&nbsp;</p><p>In the remit of EA, radical actors are now a necessity; they lend the energy, the direction, and the credibility it needs to win over a broader public and not just the choir it preaches to. The world of social justice is not so easily swayed as Silicon Valley, we do not iterate, and we certainly do not \u2018fail fast\u2019. Such concessions cost lives. This doggedness is something EA sorely lacks at present: its principles are more fluid, more congenial to the power structures that cause the existential threats it rails against. Such flexibility may make us more effective collaborators, but not necessarily more effective influencers. The direction provided by political or ideological weathervanes does not hold its ground in changing winds. Instead, by appealing to the social justice warrior, EA targets become non-negotiable and our politic more steadfast and demanding.&nbsp;</p><p>&nbsp;</p><p>But EA Principles do little to endear themselves to social justice. Strict rationalism and Pascal-Wager-like calculations for doing good feel false, contrived, and a far cry from the wildfire of activism. Longtermism is abhorrent to the advocate. Indeed, for those surrounded by subsistence, corruption, exploitation and torture, the hypothetical - the far future - represents a luxury holiday and not an international emergency. They cannot afford to indulge in a worldview that overshoots far beyond present dangers. Instead, like a patient in a car crash, first aid must be performed before surgery. Moreover, the moral value of future lives is a viewpoint many in social justice have seen weaponised against them. My own work to promote reproductive freedom in women and access to abortion comes up against this argument daily, for example. All this breeds a fundamental distrust between the actors on the ground and the castles in the sky that EA conjures. Urgent work is therefore needed to see EA principles embraced by the marginalised and those mobilised to support them directly.&nbsp;</p><p>&nbsp;</p><p>I offer three initial routes to making headway here. Firstly, it is essential that we find common ground to work from, reconciling our theory and our language with the real-world experience of advocates. We are not so different, and we need to prove that. We need to demonstrate how what can come off as fixed narratives and frameworks are completely complimentary to the goals and methods of social justice. We need to seek out consultation with these groups, diversifying our policy stream to include the tip-of-the-spear actors good policy is inspired by. We will need to get creative with the bridges that we build, however, looking for rebellion in the arts, community collectives and affinity groups as well as the well-worn path to not-for-profits. Second, we must diversify where we publish; features in The Economist and big wins in academic journals will not exactly bring us to the beating heart of changemaking. We need to move away from these old guard outlets and find spaces where we make ourselves open to criticism and co-creation. Finding vanguard collaborators will be key to this. Thirdly, we need to consider introducing advocacy-specific prizes, funds or scholarships within EA, investing in the work and the people that can make our ideas not just palatable for gamechangers, but downright inspiring.&nbsp;</p><p>&nbsp;</p><p>By prioritising diversity in thought, we will naturally make progress on wider ambitions to improve the inclusivity of EA; a quick look at leading posts on the Forum makes it clear I am hardly the first to have noticed where representation in EA must improve. A pyramidical hierarchy capped with white men will not change the world. Instead, this agenda must be incorporated and cherished as a means of future-proofing our ambitions. By welcoming both radical action and creative spirit, we move from virtue-signalling to virtuous action while also addressing the blind spots that are the blight of likeminded minds. We must seek out wisdom wherever we find it and be held to account by those fighting for the future in the here and now and not just the there and then.&nbsp;</p>", "user": {"username": "MeredithLeston"}}, {"_id": "BsMrBFMCkgjmBcSec", "title": "Case Rates to Sequencing Reads", "postedAt": "2022-09-21T02:00:00.158Z", "htmlBody": "<p><span>\n\nIn thinking about how you might </span>\n\n<a href=\"https://www.naobservatory.org/\">identify future pandemics by\nsequencing wastewater</a>, you might have a goal of raising an alert\nbefore some fraction of people were currently infected.  What you're\nactually able to observe, however, are \n\n<a href=\"https://www.jefftk.com/p/sequencing-intro\">sequencing reads</a>, several steps removed\nfrom infection rates.  Can we use covid data to estimate how the\nfraction of people currently infected with some pathogen might\ntranslate into the fraction of wastewater sequencing reads that match\nthe pathogen?\n\n\n\n</p><p>\n\n<i>RNA Viromics of Southern California Wastewater and Detection of\nSARS-CoV-2 Single-Nucleotide Variants</i> (<a href=\"https://journals.asm.org/doi/full/10.1128/AEM.01448-21\">Rothman\net al 2021</a>) is the closest study I know in this area,\nthough it wasn't exactly what they were trying to answer.  They took\nmany samples of municipal wastewater, extracted the RNA, filtered it\nto increase the fraction of the RNA corresponding to viruses, and then\nsequenced it.\n\n</p>\n\n<p>\n\n(They also did several other things, including enriching some samples\nfor respiratory viruses, but here I'm only looking at the unenriched\ndata.)\n\n</p>\n\n<p>\n\nThey got 795M sequencing reads, 337 of which they identified as covid.\nThis means the fraction of all reads that were covid (the\n\"proportional abundance\") was 4e-7.  There's a typo in the paper where\nthey say this is \"0.0004%\", but I wrote to the author to ask about it\nand they confirmed it was a missing zero.\n\n</p>\n\n<p>\n\nHow many people were infected during this period?  The first tricky\nthing in answering this question is that the sequencing quantity\nwasn't uniform over this period:\n\n</p>\n\n<p>\n\n<a href=\"https://www.jefftk.com/rothman-total-wastewarter-reads-by-month-big.png\"><img src=\"https://www.jefftk.com/rothman-total-wastewarter-reads-by-month.png\" srcset=\"https://www.jefftk.com/rothman-total-wastewarter-reads-by-month.png 550w, https://www.jefftk.com/rothman-total-wastewarter-reads-by-month-2x.png 1100w\" /></a>\n\n<br />\n\n<i>Rothman\n2021 SF4_sample_metadata: <a href=\"https://journals.asm.org/doi/suppl/10.1128/AEM.01448-21/suppl_file/aem.01448-21-s0003.xlsx\">xlsx</a>\n\n</i></p>\n\n<p>\n\nAnd neither were covid cases:\n\n</p>\n\n<p>\n\n<a href=\"https://www.jefftk.com/la-county-covid-cases-big.png\"><img src=\"https://www.jefftk.com/la-county-covid-cases.png\" srcset=\"https://www.jefftk.com/la-county-covid-cases.png 550w, https://www.jefftk.com/la-county-covid-cases-2x.png 1100w\" /></a>\n\n<br />\n\n<i>LA County <a href=\"http://dashboard.publichealth.lacounty.gov/covid19_surveillance_dashboard/\">Covid\nDashboard</a></i>\n\n</p>\n\n<p>\n\nThe paper accounted for the variability in confirmed cases by looking\nat the relationship between the number of cases in the county served\nby each water treatment plant and the amount of covid they measured in\ntheir samples using qPCR.  Because qPCR gives you much more precise\nestimates for a given cost, they were able to quantify the covid\ncomposition of 85 of their samples across seven plants:\n\n</p>\n\n<p>\n\n<a href=\"https://www.jefftk.com/rothman-2021-fig1-big.png\"><img src=\"https://www.jefftk.com/rothman-2021-fig1.png\" srcset=\"https://www.jefftk.com/rothman-2021-fig1.png 550w, https://www.jefftk.com/rothman-2021-fig1-2x.png 1100w\" /></a>\n\n</p>\n\n<p>\n\n(You might be wondering: if qPCR is cheap and precise why not use it\ndirectly in looking for new pandemics?  Why invest so much effort and\nmoney in sequencing?  The issue with qPCR is that it's a method for\nchecking how much of a <i>specific</i> sequence is present in a\nsample.  That means you have to already know what you're looking for,\nand probably wouldn't be able to catch something novel.)\n\n</p>\n\n<p>\n\nLet's recreate this chart, but quantify by the fraction of reads that\nmatch covid instead of qPCR.  I identified the reads by looking for\nexact matches to a few of the length-40 substrings (\"40-mers\") of\ncovid, and found 227 (<a href=\"https://github.com/jeffkaufman/kmer-egd/blob/main/find-subsetted-kmers.sh\">code</a>). I\npulled confirmed cases from <a href=\"https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\">CSSE\nat Johns Hopkins University</a>, computed a 7-day <a href=\"https://www.jefftk.com/p/careful-with-trailing-averages\">centered</a> moving average,\nand joined it (<a href=\"https://github.com/jeffkaufman/rothman-covid-sample-counts/blob/master/correlate-reads-cases.py\">code</a>)\nwith the sample metadata (SF4: <a href=\"https://journals.asm.org/doi/suppl/10.1128/AEM.01448-21/suppl_file/aem.01448-21-s0003.xlsx\">xlsx</a>).\nThis gives us (<a href=\"https://docs.google.com/spreadsheets/d/15vQeZ8Od097ayrvzIqW4YkR3Yfb5zmHVsLofZcoZDmI/edit#gid=794481802\">sheet</a>):\n\n</p>\n\n<p>\n\n<a href=\"https://www.jefftk.com/sample-fraction-matching-covid-by-confirmed-case-rate-by-plant-big.png\"><img src=\"https://www.jefftk.com/sample-fraction-matching-covid-by-confirmed-case-rate-by-plant.png\" srcset=\"https://www.jefftk.com/sample-fraction-matching-covid-by-confirmed-case-rate-by-plant.png 550w, https://www.jefftk.com/sample-fraction-matching-covid-by-confirmed-case-rate-by-plant-2x.png 1100w\" /></a>\n\n</p>\n\n<p>\n\nWe didn't get any reads matching covid for the JWPCP, NC, or\nSB plants, but this isn't surprising because sequencing isn't very\nsensitive and they only sampled those plants early in the study when\ncovid levels were very low.\n\n</p>\n\n<p>\n\nPulling out the four plants where we do see covid, the results show a\nlot of variability:\n\n</p>\n\n<p>\n\n<a href=\"https://www.jefftk.com/sample-fraction-matching-covid-by-confirmed-case-rate-by-plant-four-big.png\"><img src=\"https://www.jefftk.com/sample-fraction-matching-covid-by-confirmed-case-rate-by-plant-four.png\" srcset=\"https://www.jefftk.com/sample-fraction-matching-covid-by-confirmed-case-rate-by-plant-four.png 550w, https://www.jefftk.com/sample-fraction-matching-covid-by-confirmed-case-rate-by-plant-four-2x.png 1100w\" /></a>\n\n</p>\n\n<p>\n\nThe most striking contrast is between HTP, where there's a clear\npositive trend, and PL, where even as rates got to over 100 cases per\n100k we never got more than one covid read per sample.  We do know\nfrom qPCR there was covid in the water at PL, and that its\nconcentration went up as cases went up, so it's surprising that we\ndidn't get the number of reads we'd expect.\n\n</p>\n\n<p>\n\nThere's also a disagreement the other direction, where in SJ qPCR\ndidn't show covid levels increasing with cases, but we do see an\nincrease.  This increase is not very robust, however, because it is\ndriven entirely by a single sample, 12 covid reads on the last day\n(2020-11-18).\n\n</p>\n\n<p>\n\nFitting a linear model to these four, and the data as a whole, I see:\n\n</p>\n\n<p>\n\n</p>\n\n<table>\n<tr>\n<th>Plant</th>\n<th>Estimated reads at 100 cases per 100k\n</th>\n</tr>\n<tr>\n<td>HTP</td>\n<td>2.2e-6\n</td>\n</tr>\n<tr>\n<td>OC</td>\n<td>8.6e-7\n</td>\n</tr>\n<tr>\n<td>PL</td>\n<td>7.9e-8\n</td>\n</tr>\n<tr>\n<td>SJ</td>\n<td>1.2e-6\n</td>\n</tr>\n<tr>\n<td><b>All</b></td>\n<td><b>1.3e-6\n</b></td>\n</tr>\n</table>\n\n\n\n<p>\n\nAs a sanity check, let's try computing this a different way, without\nfancy statistics.  We can compare the fraction of covid reads to the\nread-weighted case rate.  For example, if we had:\n\n</p>\n\n<p>\n\n</p>\n\n<table>\n<tr>\n<th>County</th>\n<th>Date</th>\n<th>Total Reads</th>\n<th>Cases per 100k\n</th>\n</tr>\n<tr>\n<td>Orange</td>\n<td>2020-08-12</td>\n<td>2.1M</td>\n<td>8.4\n</td>\n</tr>\n<tr>\n<td>LA</td>\n<td>2020-08-12</td>\n<td>5.8M</td>\n<td>20.0\n</td>\n</tr>\n<tr>\n<td>San Diego</td>\n<td>2020-08-13</td>\n<td>11.0M</td>\n<td>8.8\n</td>\n</tr>\n</table>\n\n\n\n<p>\n\nThen the read-weighted case rate, or the mean case rate 'experienced'\nby a read, is:\n\n</p>\n\n<p>\n\n</p>\n\n<pre>\n2.1M * 8.4 + 5.8M * 20.0 + 11.0M * 8.8\n---------------------------------------\n         2.1M + 5.8M + 11.0M\n</pre>\n\nWhich in this examples comes to 12.2 per 100k.\n\n\n\n<p>\n\nCalculating this for the full data I get (<a href=\"https://github.com/jeffkaufman/rothman-covid-sample-counts/blob/master/correlate-reads-cases.py\">code</a>,\n<a href=\"https://docs.google.com/spreadsheets/d/15vQeZ8Od097ayrvzIqW4YkR3Yfb5zmHVsLofZcoZDmI/edit#gid=794481802\">sheet</a>)\na read-weighted mean case rate of 33 per 100k. The paper found 4e-7 of\nreads were covid, so scaling by 100/33 we can add another estimate to\nour table:\n\n</p>\n\n<p>\n\n</p>\n\n<table>\n<tr>\n<th>Plant</th>\n<th>Estimated reads at 100 cases per 100k\n</th>\n</tr>\n<tr>\n<td>HTP</td>\n<td>2.2e-6\n</td>\n</tr>\n<tr>\n<td>OC</td>\n<td>8.6e-7\n</td>\n</tr>\n<tr>\n<td>PL</td>\n<td>7.9e-8\n</td>\n</tr>\n<tr>\n<td>SJ</td>\n<td>1.2e-6\n</td>\n</tr>\n<tr>\n<td><b>All</b></td>\n<td><b>1.3e-6\n<tr>\n<td>Read-weighting</td>\n<td>1.2e-6\n</td>\n</tr></b></td>\n</tr>\n</table>\n\n\n\n<p>\n\nThis is quite close to what we got with linear regression, which is\nreassuring.  Before we get too reassured, however, we should keep in\nmind that the PL data give much lower numbers.\n\n</p>\n\n<p>\n\nStill, let's continue on with our attempt to predict a number of\nsequencing reads from a number of currently infected people.  So far\nwe've been using confirmed cases, and not all infections are\nconfirmed, or even noticed.  The <a href=\"https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/burden.html\">CDC\nestimates</a> that from 2020-02 to 2021-09 1:4 infections were\nreported.  And, very roughly, the typical unvaccinated person catching\ncovid for the first time was infected for maybe ~2w.  This gives us a\nfactor of 56 (4*14), and a new table:\n\n</p>\n\n<p>\n\n</p>\n\n<table>\n<tr>\n<th>Plant</th>\n<th>Estimated reads at 100 currently infected people per 100k\n</th>\n</tr>\n<tr>\n<td>HTP</td>\n<td>3.9e-8\n</td>\n</tr>\n<tr>\n<td>OC</td>\n<td>1.5e-8\n</td>\n</tr>\n<tr>\n<td>PL</td>\n<td>1.4e-9\n</td>\n</tr>\n<tr>\n<td>SJ</td>\n<td>2.1e-8\n</td>\n</tr>\n<tr>\n<td><b>All</b></td>\n<td><b>2.3e-8\n<tr>\n<td>Read-weighting</td>\n<td>2.2e-8\n</td>\n</tr></b></td>\n</tr>\n</table>\n\n\n\n<p>\n\nThis is a very rough estimate: is 14 days really a typical infectious\nperiod? Is 1:4 really the fraction of infections that were confirmed\nduring this period in the catchment area of these wastewater treatment\nplants?  Do wastewater levels lead confirmed cases, and if so by how\nmuch?  Why are the levels measured at PL 20x lower than elsewhere?  Is\ncovid similar to potential future pandemics in terms of infectious\nperiod, shedding amount, stability in wastewater, etc?  Is the\nmethodology in Rothman 2021 what you'd use if you were trying to\ndetect new pathogens?  And a single sequencing read wouldn't be enough\nto identify most novel pathogens, so there are additional factors I'm\nnot getting into with later processing.  Still, running through these\ncalculations is helpful for getting a sense of how much sequencing a\nproject like this might need.\n\n  </p>\n\n<p><i>Comment via: <a href=\"https://www.facebook.com/jefftk/posts/pfbid0BnMbmYVV7Wwas2DssujHQ7MZXwvamXuwzSC1DKSz9VWnZkASH5F2E6tbhivL2uHHl\">facebook</a></i></p>", "user": {"username": "Jeff_Kaufman"}}, {"_id": "W6gGKCm6yEXRW5nJu", "title": "Quantified Intuitions: An epistemics training website including a new EA-themed calibration app", "postedAt": "2022-09-20T22:25:35.978Z", "htmlBody": "<p><i>Crossposted to </i><a href=\"https://www.lesswrong.com/posts/NnmJ7trnKzQGmdYYN/quantified-intuitions-an-epistemics-training-website\"><i>LessWrong</i></a></p><p>TL;DR&nbsp;<a href=\"http://quantifiedintuitions.org/\"><u>Quantified Intuitions</u></a> helps users practice assigning credences to outcomes with a quick feedback loop. Please leave feedback in the comments, join our&nbsp;<a href=\"https://discord.com/invite/mt9YVB8VDE\"><u>Discord</u></a>, or send thoughts to&nbsp;<a href=\"mailto:aaron@sage-future.org\"><u>aaron@sage-future.org</u></a>.</p><h1>Quantified Intuitions</h1><p><a href=\"http://quantifiedintuitions.org/\"><u>Quantified Intuitions</u></a> currently consists of two apps:</p><ol><li><a href=\"http://quantifiedintuitions.org/calibration\"><u>Calibration game</u></a>: Assigning confidence intervals to EA-related trivia questions.<ol><li>Question sources vary but many are from&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/o9SLSkPJ6A2MWb9Bf/anki-deck-for-some-key-numbers-that-almost-every-ea-should\"><u>Anki deck for \"Some key numbers that (almost) every EA should know\"</u></a></li><li>Compared to&nbsp;<a href=\"https://www.openphilanthropy.org/calibration\"><u>Open Philanthropy\u2019s calibration app</u></a>, it currently contains less diversity of questions (hopefully more interesting to EAF/LW readers) but the app is more modern and nicer to use in some ways</li></ol></li><li><a href=\"http://quantifiedintuitions.org/pastcasting\"><u>Pastcasting</u></a>: Forecasting on already resolved questions that you don\u2019t have prior knowledge about.<ol><li>Questions are pulled from&nbsp;<a href=\"https://www.metaculus.com/questions/\"><u>Metaculus</u></a> and&nbsp;<a href=\"https://www.gjopen.com/\"><u>Good Judgment Open</u></a></li><li>More info on motivation and how it works are in the&nbsp;<a href=\"https://www.lesswrong.com/posts/FuGHKKMcAL2GYbxrn/introducing-pastcasting-a-tool-for-forecasting-practice\"><u>LessWrong announcement post</u></a></li></ol></li></ol><p>Please leave feedback in the comments, join our&nbsp;<a href=\"https://discord.com/invite/mt9YVB8VDE\"><u>Discord</u></a>, or send it to&nbsp;<a href=\"mailto:aaron@sage-future.org\"><u>aaron@sage-future.org</u></a>.</p><h1>Motivation</h1><p>There are huge benefits to using numbers when discussing disagreements: see \u201c3.3.1 Expressing degrees of confidence\u201d in&nbsp;<a href=\"https://www.openphilanthropy.org/research/reasoning-transparency/\"><u>Reasoning Transparency</u></a> by OpenPhil. But anecdotally, many EAs still feel uncomfortable quantifying their intuitions and continue to prefer using words like \u201clikely\u201d and \u201cplausible\u201d which could be interpreted in many ways.</p><p>This issue is likely to get worse as the EA movement attempts to grow quickly, with many new members joining who are coming in with various backgrounds and perspectives on the value of subjective credences. We hope that Quantified Intuitions can help both new and longtime EAs be more comfortable turning their intuitions into numbers.</p><p>More background on motivation can be found in Eli\u2019s forum comments&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9Y6Y6qoAigRC7A8eX/my-take-on-what-we-owe-the-future?commentId=EPJYEr9pNQ4zXNsrw\"><u>here</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9Y6Y6qoAigRC7A8eX/my-take-on-what-we-owe-the-future?commentId=ayRbNTRJPiW3CZDez\"><u>here</u></a>.</p><h1>Who built this?</h1><p>Sage is an organization founded earlier this year by&nbsp;<a href=\"https://forum.effectivealtruism.org/users/elifland\"><u>Eli Lifland</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/users/platinuman\"><u>Aaron Ho</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/users/misha_yagudin\"><u>Misha Yagudin</u></a> (in a part-time advising capacity). We\u2019re funded by the&nbsp;<a href=\"https://ftxfuturefund.org/our-grants/?_search=sage\"><u>FTX Future Fund</u></a>.</p><p>As stated in the grant summary, our initial plan was to \u201ccreate a pilot version of a forecasting platform, and a paid forecasting team, to make predictions about questions relevant to high-impact research\u201d. While we build a decent beta forecasting platform (that we plan to open source at some point), the pilot for forecasting on questions relevant to high-impact research didn\u2019t go that well due to (a) difficulties in creating resolvable questions relevant to cruxes in AI governance and (b) time constraints of talented forecasters. Nonetheless, we are still growing Samotsvety\u2019s capacity and taking occasional high-impact forecasting gigs.</p><p>Eli was also&nbsp;<a href=\"https://www.foxy-scout.com/personal-update/\"><u>struggling some personally</u></a> around this time and updating toward AI alignment being super important but crowd forecasting not being that promising for attacking it. He stepped down and is now advising Sage part-time.</p><p>Meanwhile, we pivoted to building the apps contained in Quantified Intuitions to improve and maintain epistemics in EA. Aaron wrote most of the software for both apps within the past few months, Alejandro Ortega helped with the calibration game questions and Alina Timoshkina helped with a wide variety of tasks.</p><p>If you\u2019d like to contact Sage you can message us on EAF/LW or email&nbsp;<a href=\"mailto:aaron@sage-future.org\"><u>aaron@sage-future.org</u></a>. If you\u2019re interested in helping build apps similar to the ones on Quantified Intuitions or improving the current apps, fill out this&nbsp;<a href=\"https://forms.gle/6dz9on5a2irtDj3d8\"><u>expression of interest</u></a>. It\u2019s possible that we\u2019ll hire a software engineer, product manager, and/or generalist, but we don\u2019t have concrete plans.</p>", "user": {"username": "Sage"}}, {"_id": "tnzLTnBQLEDv9zygo", "title": "Establishing Oxford\u2019s AI Safety Student Group: Lessons Learnt and Our Model", "postedAt": "2022-09-21T07:57:13.294Z", "htmlBody": "<p>[EDITS- We no longer endorse everything in this post, and have changed our objectives and thinking significantly. As such, the mentioned document is now private if you have questions, please contact <a href=\"mailto:oxford@aisafetyhub.org\">oxford@aisafetyhub.org</a>]<br>In January we founded a student group at Oxford focused on technical AI safety. Since then we\u2019ve run speaker events, socials, multiple cohorts of the AGISF, and supervised research projects (\u201cLabs\u201d). We think it went pretty well, so we\u2019re sharing our takeaways and model here.</p><p>This post is a short summary of&nbsp;<a href=\"https://docs.google.com/document/d/198oFhyc3eBtJd4NE0vyc9yUVHfYRqBxrd5QY8oAWnIw/edit?usp=sharing\"><u>this public document</u></a> which goes into more detail about our approach to AI safety community building, reflections, and recommendations.</p><h1>Non-trivial takeaways</h1><ol><li>Launching as part of an AI group, rather than an EA group, worked well for us. (<a href=\"https://docs.google.com/document/d/198oFhyc3eBtJd4NE0vyc9yUVHfYRqBxrd5QY8oAWnIw/edit#heading=h.ht74jwesyhpw\"><u>see more</u></a>)</li><li>Outreach aimed at people interested in AI reached a much larger technical audience than past outreach aimed at people interested in EA or longtermism. (<a href=\"https://docs.google.com/document/d/198oFhyc3eBtJd4NE0vyc9yUVHfYRqBxrd5QY8oAWnIw/edit#heading=h.cmczh7hva52s\"><u>see more</u></a>)</li><li>It was surprisingly easy to interest people in AI safety without appealing to EA or longtermism. (<a href=\"https://docs.google.com/document/d/198oFhyc3eBtJd4NE0vyc9yUVHfYRqBxrd5QY8oAWnIw/edit#heading=h.gl2eibqsxpdu\"><u>see more</u></a>)</li><li>Significant value from our speaker events seemed to come from the high-retention, friendly socials we held afterwards. (<a href=\"https://docs.google.com/document/d/198oFhyc3eBtJd4NE0vyc9yUVHfYRqBxrd5QY8oAWnIw/edit#heading=h.lee087a4bgpz\"><u>see more</u></a>)</li><li>Our \u201cLabs\u201d model of student research projects seems effective for development and output with minimal time-cost for an expert supervisor (~1 hour per week). This is particularly valuable if field building is mentorship bottlenecked (<a href=\"https://docs.google.com/document/d/198oFhyc3eBtJd4NE0vyc9yUVHfYRqBxrd5QY8oAWnIw/edit#heading=h.rd1mvo3y36n6\"><u>see more</u></a>).&nbsp;</li></ol><h1>Our current model</h1><p>Our working objective was to increase the number and quality of technical people pursuing a career in AI safety research.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk9l85cs15om\"><sup><a href=\"#fnk9l85cs15om\">[1]</a></sup></span>&nbsp;To do this, we have been operating with the following pipeline: &nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdiruc72p4bn\"><sup><a href=\"#fndiruc72p4bn\">[2]</a></sup></span><img src=\"https://res.cloudinary.com/cea/image/upload/v1674664510/mirroredImages/tnzLTnBQLEDv9zygo/plaah9mwxvq7fh7fasmi.png\"></p><p><br>&nbsp;</p><h1>Results so far</h1><ul><li>At least 2 of the current participants of Redwood\u2019s MLAB this summer had never encountered AI safety or EA before attending our events this spring.&nbsp;&nbsp;</li><li>We had 24-73 people attend our 9 speaker events, with 69% (on average) having a STEM background (according to survey data).</li><li>65 people signed up for our AGI Safety Fundamentals course across 11 cohorts. 57% had STEM backgrounds.</li></ul><h1>Further Information</h1><p>Please see<a href=\"https://docs.google.com/document/d/198oFhyc3eBtJd4NE0vyc9yUVHfYRqBxrd5QY8oAWnIw/edit?usp=sharing\"> the attached public document</a> for further information about the student group or our contact details.<br>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk9l85cs15om\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk9l85cs15om\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;We are now reconsidering our working objective and don\u2019t necessarily endorse the stated objective \"to increase the number and quality of technical people pursuing a career in AI safety research\". However, we think it is important to start from your objective and work backwards, and this is the objective we actually used.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndiruc72p4bn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdiruc72p4bn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;We want to note that having a target audience of people \u201cinterested in AI\u201d creates a self-selection effect that reduces the diversity of thought in our attendance. We are working to improve this.</p></div></li></ol>", "user": {"username": "cj-griffin"}}, {"_id": "hfnuwh6miJ3yn2Jpq", "title": "What Do AI Safety Pitches Not Get About Your Field?", "postedAt": "2022-09-20T18:13:41.439Z", "htmlBody": "<p>When I was first introduced to AI Safety, coming from a background studying psychology, I kept getting frustrated about the way people defined the and used the word \"intelligence\". They weren't able to address my questions about cultural intelligence, social evolution, and general intelligence in a way I found rigorous enough to be convincing. I felt like professionals couldn't answer what I considered to be basic and relevant questions about a general intelligence, which meant that I took a lot longer to take AI Safety seriously than I otherwise would have. It feels possible to me that other people have run into AI Safety pitches and been turned off because of something similar -- a communication issue because both parties approached the conversation with very different background information. I'd love to try to minimize these occurrences, so if you've had anything similar happen, could you please share:&nbsp;<br><br><strong>What is something that you feel AI Safety pitches usually don't seem to understand about your field/background? What's a common place where you feel you've become stuck in a conversation with AI Safety pitches? What question/information makes/made the conversation stop progressing and start circling?&nbsp;</strong></p>", "user": {"username": "Aris Richardson"}}, {"_id": "2CwsQai9hHsa2CDJ8", "title": "The Mistakes of Focusing on Student Outreach in EA", "postedAt": "2022-09-20T17:48:41.526Z", "htmlBody": "<h1>Summary</h1><p>There has been a surge in EA outreach towards <a href=\"https://forum.effectivealtruism.org/posts/J8dF2qkybAgKgoPHo/applications-open-cea-s-university-group-accelerator-program\">university</a> and <a href=\"https://forum.effectivealtruism.org/posts/ZBuNBAjwLFhfpGgzj/announcing-non-trivial-an-ea-learning-platform-for-teenagers\">high</a> <a href=\"https://www.atlasfellowship.org/\">school</a> <a href=\"https://www.leaf.courses/\">students</a>.&nbsp;</p><p>Generally a higher proportion of students show interest in EA compared to professionals, but just because it's easier to get people interested does not mean that it's the most impactful use of time.</p><p>This is potentially a mistaken use of limited community building resources and could lead to an imbalance in the age distribution of people interested in EA and bottleneck the future growth of EA or worse, lock EA into being a permanent youth movement.</p><p>To improve this situation, EA movement building organisations can make changes in the actions they promote for students. Students interested in community building could work in more neglected areas.</p><hr><h1>Why has EA community building been focused on students?</h1><p>It seems that this is a historical coincidence rather than a deliberately chosen path. EA was founded in Oxford, mainly by students, and their initial community building was focused on Oxford, Cambridge and similar universities. Over the last ten years there has been multiple iterations, lessons learned and funding for university group organising. So it isn\u2019t that surprising that there are lots of stories of people who heard about EA in university and then went on to get involved in EA.</p><p>This is in contrast to community building efforts for <a href=\"https://forum.effectivealtruism.org/posts/HCbfyHnsebJ9pDtWS/focusing-on-career-and-cause-movement-building\">professional, cause and interest groups</a> which have started to become more organised in the last couple of years, with some of them having full time organisers for the first time this year (EA Consulting Network, UK Civil Service). There has been a lot less time to see the potential outcomes of this work in direct comparison to university group organising.</p><p>Some people have said that they focus on students because they show more interest in EA than professionals. This is likely true, it definitely seems that it\u2019s easier for students to attend events and get more involved with EA sooner. I think even if students are 10 times more likely to get involved, this may be a mistaken strategy to pursue.</p><hr><h1>It's a mistake to focus on students</h1><ul><li>Professionals have more experience, influence and networks. As EA is growing there is more need for people who have experience managing people and supporting larger organisations. It can take a long time to wait for students to graduate and gain those skills whilst maintaining interest in EA (this may be especially relevant for people with shorter AI timelines).</li><li>If we keep focusing on students we won't be able to fix the mentorship gap. At the moment we have lots of people looking for guidance and very few people able to provide the right level of support. Below there are two age distributions to show how EA currently seems vs what may be a more ideal distribution.&nbsp;</li></ul><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673868386/mirroredImages/2CwsQai9hHsa2CDJ8/glubfww0hhz8edvwq4tw.png\"><figcaption>Possible current age distribution of people interested in EA. For actual survey data from 2020 you can <a href=\"https://forum.effectivealtruism.org/posts/ThdR8FzcfA8wckTJi/ea-survey-2020-demographics#Age\">see here</a></figcaption></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673868386/mirroredImages/2CwsQai9hHsa2CDJ8/xeckkxflgpbnrheschwg.png\"><figcaption>A more ideal age distribution for people interested in EA.</figcaption></figure><ul><li>The average age in EA used to be going up one year every one or two years. The EA survey from 2020 shows that the trend had reversed with the mean and median both going down one year. According to 80,000 Hours the age that people tend to have the most impact is <a href=\"https://80000hours.org/articles/key-career-stages/#three-career-stages\">usually 36+</a>.<br>&nbsp;</li></ul><figure class=\"image image_resized\" style=\"width:81.46%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673868386/mirroredImages/2CwsQai9hHsa2CDJ8/oorqyklkcfcy0npnrpyz.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673868387/mirroredImages/2CwsQai9hHsa2CDJ8/aqohyoc6q3ugakdbzi1b.png 90w, https://res.cloudinary.com/cea/image/upload/v1673868387/mirroredImages/2CwsQai9hHsa2CDJ8/doakszjttv2d8ymwyxgv.png 180w, https://res.cloudinary.com/cea/image/upload/v1673868386/mirroredImages/2CwsQai9hHsa2CDJ8/h32xyjrvkeytguzslwus.png 270w, https://res.cloudinary.com/cea/image/upload/v1673868387/mirroredImages/2CwsQai9hHsa2CDJ8/sdma0dtucasyugrdzrkt.png 360w, https://res.cloudinary.com/cea/image/upload/v1673868387/mirroredImages/2CwsQai9hHsa2CDJ8/paaara6chq1reyagq9es.png 450w, https://res.cloudinary.com/cea/image/upload/v1673868386/mirroredImages/2CwsQai9hHsa2CDJ8/xplluf3sjnzstbhofsz2.png 540w, https://res.cloudinary.com/cea/image/upload/v1673868387/mirroredImages/2CwsQai9hHsa2CDJ8/nwviv2z1gbzgjaeprweo.png 630w, https://res.cloudinary.com/cea/image/upload/v1673868386/mirroredImages/2CwsQai9hHsa2CDJ8/moozua1oqjzf7wvgygec.png 720w, https://res.cloudinary.com/cea/image/upload/v1673868387/mirroredImages/2CwsQai9hHsa2CDJ8/grk2doxjjrmymklu7mhp.png 810w, https://res.cloudinary.com/cea/image/upload/v1673868386/mirroredImages/2CwsQai9hHsa2CDJ8/ymto4xfaawmza6oxxowo.png 828w\"><figcaption>Changes in Age over Time - From <a href=\"https://forum.effectivealtruism.org/posts/ThdR8FzcfA8wckTJi/ea-survey-2020-demographics#Changes_in_Age_over_Time\">here</a></figcaption></figure><ul><li>EA can give off the impression of being a youth movement. This can lead to professionals bouncing when they encounter EA and assuming that it is not a place meant for them. Even professionals who have been interested for a while may find it hard working directly at EA organisations if there is no consideration for having to support a family, healthcare and <a href=\"https://forum.effectivealtruism.org/posts/6xX96ZqFtH5n7mchW/my-emotional-reaction-to-the-current-funding-situation?commentId=rbF44dCyj9xGTCu7n#comments\">uncertainty around funding</a>.</li></ul><hr><h1>We shouldn't ignore students though</h1><p>I don\u2019t think we should stop student community building, this is more about what we should do on the margin. There is a lot to be gained by having organisers at the top 20-50 universities. There are also plenty of ways that talented young people can be discovered without going through a university group. For example there are <a href=\"https://www.atlasfellowship.in/\">fellowships</a>, <a href=\"https://globalprioritiesinstitute.org/essay-prize-for-global-priorities-research/\">essay prizes</a>, <a href=\"https://www.mercatus.org/emergent-ventures\">grants</a>, volunteering, internships, etc.</p><hr><h1>Better stories to tell potential student organisers</h1><p>There are lots of stories of people having been university organisers and many places recommend being a student organiser as a top priority. This means a lot of students will try out community building even when it's not something they have an aptitude for, a strong interest in or the best option for the community as a whole.&nbsp;</p><p>Organisations could add nuance or remove these suggestions and encourage other options more strongly. Students at the moment could reconsider whether building a group at their university is actually the best thing to be doing. I've added some ideas below on what students and the EA movement might find to be a more valuable use of time.</p><ul><li>&nbsp;Internships/volunteering at organisations that are impactful or good for career capital</li><li>Running cause specific student groups (AI, x-risks, One for the World, alt protein)&nbsp;</li><li>Supporting other EA projects with their community building work</li><li>Skilling up in more relevant areas than event organising and tabling<br>&nbsp;</li></ul><hr><h1>In Short</h1><p>I'm concerned that there is focus on students because the metrics chosen go up faster in the short term without looking at the longer term health of the community.</p>", "user": {"username": "DavidNash"}}, {"_id": "vcwrEmCLTC5xqkWnX", "title": "Optimizing seed:pollen ratio to spread ideas", "postedAt": "2022-09-20T16:37:10.535Z", "htmlBody": "<p><i>Cross-posted </i><a href=\"https://hollyelmore.substack.com/p/optimizing-seedpollen-ratio-to-spread\"><i>from my blog</i></a><i>.</i></p><hr><p><br>As an EA organizer when I was a grad student at Harvard, I developed an implicit model of community organizing at a university that is complimentary to <a href=\"https://www.centreforeffectivealtruism.org/the-funnel-model\"><u>the funnel model of Center for Effective Altruism</u></a> (CEA) that was super popular a few years ago. I call mine \u201coptimizing the seed:pollen ratio\u201d. I\u2019m not going to justify outreach as a strategy or advocate for any specific means of outreach\u2014the point is to share my model of outreach efficacy. It\u2019s a fun bonus that my model is well-explained by analogy to a biology concept, which I hope to explain well as a secondary objective.</p><h1>The funnel model</h1><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbdda6a5-6ab1-465d-9521-0e76dfc1ebee_1737x1719.jpeg\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbdda6a5-6ab1-465d-9521-0e76dfc1ebee_1737x1719.jpeg\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbdda6a5-6ab1-465d-9521-0e76dfc1ebee_1737x1719.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbdda6a5-6ab1-465d-9521-0e76dfc1ebee_1737x1719.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbdda6a5-6ab1-465d-9521-0e76dfc1ebee_1737x1719.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbdda6a5-6ab1-465d-9521-0e76dfc1ebee_1737x1719.jpeg 1456w\"></a></p><p>CEA says, \u201cWe are trying to build a community, and one aspect of this project is encouraging people to become more deeply engaged with the community. The funnel metaphor helps us to think about the appropriate goals and audiences for our different projects.\u201d The funnel model is focused on the deliberate composition of the community\u2014 how many people are entering the funnel, and what share of the community is at what part of the funnel at any given time. It also suggests that movement through the funnel happens in stages, from outer to middle to core, and there\u2019s both investment from the community at every stage and friction to progressing from one stage to another.</p><p>When this model was getting a lot of buzz, there were many discussions about what part of the funnel to be focusing on. This was happening during CEA\u2019s big pivot toward endorsing longtermism as a fundamental tenet of EA. With that, many were arguing that there should much more focus on core EAs and the core-EA-development-pipeline, and much less emphasis on the mid- to casual level of community involvement, because most of the value of EA in the long run will be \u201c<a href=\"https://michelfeit.medium.com/heavy-tails-and-altruism-when-your-intuition-fails-you-15a8b2b7299c\"><u>in the tails</u></a>\u201d of the distribution, from original work, not from safer but lower value bets like getting average individuals to donate money. This view basically won and is dominant in EA today.</p><h1>Seed:pollen ratio</h1><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2f76eeee-e759-40a1-b4e3-bbc870d739fe_800x533.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2f76eeee-e759-40a1-b4e3-bbc870d739fe_800x533.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2f76eeee-e759-40a1-b4e3-bbc870d739fe_800x533.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2f76eeee-e759-40a1-b4e3-bbc870d739fe_800x533.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2f76eeee-e759-40a1-b4e3-bbc870d739fe_800x533.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2f76eeee-e759-40a1-b4e3-bbc870d739fe_800x533.png 1456w\"></a></p><p>&nbsp;</p><p>(<a href=\"https://commons.wikimedia.org/wiki/File:Prunus_longitudinal_half_cut_flower_ovary_style_stamens_hypanthium.svg\"><u>Source</u></a>)</p><p><br>Now for an interlude on plant reproductive strategies.<a href=\"https://hollyelmore.substack.com/p/optimizing-seedpollen-ratio-to-spread?utm_source=facebook&amp;sd=pf&amp;fbclid=IwAR26fA_l2Okq1_k-XgfA36vIFJByGBrm7TgxI8tQpDpCh9OivG0IJQL4VSE#footnote-1\"><sup><u>1</u></sup></a> This^ is called a \u201c<a href=\"https://en.wikipedia.org/wiki/Plant_reproductive_morphology#Bisexual\"><u>perfect flower</u></a>\u201d because it has both male and female reproductive organs. The female reproductive organ is the carpal, and it makes seeds. The male reproductive organs are the anthers and they contain pollen. Seeds are larger and contain cytoplasm, the organelles mitochondria and chloroplasts, and a reserve of nutrients. Pollen is much smaller per grain and, like sperm, basically only contains chromosomes. Whereas seeds can be as big as a coconut (which <a href=\"https://en.m.wikipedia.org/wiki/Evolution_of_seed_size\"><u>is a seed</u></a>!), the <a href=\"https://www.vedantu.com/question-answer/which-plant-has-the-longest-pollen-grain-class-11-biology-cbse-608c39accd81200cafb2ef33\"><u>biggest pollen grains are only 2.5mm long</u></a>. In terms of energy and resources, seeds are much more costly and pollen is much cheaper.</p><p>You may be familiar with human evolutionary psychology ideas about male vs. female reproductive strategies. What\u2019s interesting about hermaphroditic plant species is that individuals are not committed to one strategy or the other, but can vary the amount of investment they put into seeds vs. pollen, either over evolutionary time or as a plastic response to environmental conditions. Per expected offspring, seeds are a safer bet. The average seed is FAR more likely to grow into a plant than the average pollen grain, which makes sense because pollen outnumbers seeds by a factor of 100-1000x (depending on the species\u2014 this number is for <i>Cannabis sativa</i>, because for some reason it\u2019s very easy to get estimates of <a href=\"https://www.canr.msu.edu/news/weighing-the-risk-of-cannabis-cross-pollination#:~:text=Flowering%20is%20induced%20when%20day,distances%20when%20conditions%20are%20favorable.\"><u>these</u></a> <a href=\"https://www.quora.com/How-many-seeds-does-a-weed-plant-produce\"><u>quantities</u></a> online for Cannabis :P) But you can make a LOT of pollen for the cost of one seed. Pollen can be carried by pollinators or the wind (or a number of other clever strategies) to sometimes vast distances (a genetically modified <a href=\"https://www.newscientist.com/article/dn6421-wind-carries-gm-pollen-record-distances/\"><u>pollen grain fertilized a grass seed 21 kilometers away</u></a>, using nothing but the wind to get there!). The overwhelming majority of pollen grains will not fertilize ovules, but that\u2019s okay because each grain is so cheap that the plant can afford thousands to millions of grains to release.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60c3f38d-893b-46cb-961e-d7a583b1f833_1608x644.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60c3f38d-893b-46cb-961e-d7a583b1f833_1608x644.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60c3f38d-893b-46cb-961e-d7a583b1f833_1608x644.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60c3f38d-893b-46cb-961e-d7a583b1f833_1608x644.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60c3f38d-893b-46cb-961e-d7a583b1f833_1608x644.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60c3f38d-893b-46cb-961e-d7a583b1f833_1608x644.png 1456w\"></a></p><p>(<a href=\"http://10.0.4.162/s12863-016-0371-8\"><u>Source</u></a>) How pollen disperses over distance in a species of palm. These observations were made when pollen landed on ovules, so these are observations of pollen success. Look familiar?</p><h2>The optimal ratio of seeds:pollen is not set in stone</h2><p>The optimal ratio of seeds:pollen is not set in stone. It depends on the environment, what other species are doing, and on what the other members of your population are doing.<a href=\"https://hollyelmore.substack.com/p/optimizing-seedpollen-ratio-to-spread?utm_source=facebook&amp;sd=pf&amp;fbclid=IwAR26fA_l2Okq1_k-XgfA36vIFJByGBrm7TgxI8tQpDpCh9OivG0IJQL4VSE#footnote-2\"><sup><u>2</u></sup></a> <a href=\"https://hollyelmore.substack.com/p/optimizing-seedpollen-ratio-to-spread?utm_source=facebook&amp;sd=pf&amp;fbclid=IwAR26fA_l2Okq1_k-XgfA36vIFJByGBrm7TgxI8tQpDpCh9OivG0IJQL4VSE#footnote-3\"><sup><u>3</u></sup></a> If, for example, your species is being domesticated and humans are assisting in pollination, it behooves every individual to invest less in pollen and more in seeds. If everyone\u2019s making a lot of seeds, then the plant that makes mostly pollen is going to make bank. If everyone\u2019s making mostly pollen, then making extra pollen is going to have vanishing low marginal returns and any extra seeds that get made will almost certainly be fertilized, resulting in a whole new offspring. Some species also have conditional strategies, where the investment in pollen vs. seeds depends on the quality of that individual, so depending on how well they can utilize seeds vs. pollen.<a href=\"https://hollyelmore.substack.com/p/optimizing-seedpollen-ratio-to-spread?utm_source=facebook&amp;sd=pf&amp;fbclid=IwAR26fA_l2Okq1_k-XgfA36vIFJByGBrm7TgxI8tQpDpCh9OivG0IJQL4VSE#footnote-4\"><sup><u>4</u></sup></a></p><h1>Optimizing seed:pollen ratio in EA outreach</h1><p>So, I have a certain energy budget for having EA impact through organizing. I can invest that energy in a small number of individuals who I think will yield high returns by having high impact themselves. The quality time I invest in particular people is my \u201cseeds\u201d. But I also throw a lot of stuff out there just in case it will stick, like when I give a presentation to a large general audience or write blog posts. This (relatively) lower (per listener) investment, higher-reach material is my \u201cpollen\u201d.</p><p>Whereas the funnel model invites us to ask how wide we should open the funnel and how to advance people along the funnel, the seed:pollen ratio model allows us to intuitively weigh different levels of investment, different levels of reach, and the distribution of effects that each type of investment\u2014whether closer to seed or to pollen\u2014is likely to have.</p><p>My model of impact from EA organizing at Harvard was always dominated by the expected value of core EAs who took on EA as a deep part of their own morality, philosophy, and life plan. But you can\u2019t only focus on making core EAs, because it\u2019s not cut-and-dried how to make lasting, committed EAs. Focusing only on developing core EA candidates is like making only seeds. In biology, we would call this a \u201cpollen-limited\u201d strategy\u2014 when it comes down to it, you have to wait for the right serendipitous unknown factors to come along and fertilize your seeds. It\u2019s possible a lot of your seeds will go unfertilized.</p><p>Sometimes pollen-style outreach brings in new people who will, with the support of the community, become core EAs or middle-of-the-funnel EAs\u2014 it was something like a talk or a blog post or a social media mention of EA that initially brought almost all early core EAs to the community. Sometimes we get lucky<a href=\"https://hollyelmore.substack.com/p/optimizing-seedpollen-ratio-to-spread?utm_source=facebook&amp;sd=pf&amp;fbclid=IwAR26fA_l2Okq1_k-XgfA36vIFJByGBrm7TgxI8tQpDpCh9OivG0IJQL4VSE#footnote-5\"><sup><u>5</u></sup></a> and people who get the lightest exposure to EA concepts from outreach go and recapitulate the whole set of ideas themselves and have a huge impact without much additional investment from the community. There are lots of fully-formed seeds out there that only need a tiny grain of pollen from us to be fertilized and grow! EAs recognize the importance of tail effects in evaluating expected value. It\u2019s kind of our thing. We should be applying this thinking to building our community. Personally, I think the current approach in the core community is too heavily weighted towards talent searches to try to \u201cpick winners\u201d (seeds) and invest all of our resources in them. IMO we\u2019re leaving valuable pollen opportunities on the table when we don\u2019t make ourselves more legible and accessible to a wide audience, and, though that strategy comes with other trade-offs<a href=\"https://hollyelmore.substack.com/p/optimizing-seedpollen-ratio-to-spread?utm_source=facebook&amp;sd=pf&amp;fbclid=IwAR26fA_l2Okq1_k-XgfA36vIFJByGBrm7TgxI8tQpDpCh9OivG0IJQL4VSE#footnote-6\"><sup><u>6</u></sup></a>, at this time I think we should tip more in that direction.</p><p>But whenever you\u2019re reading this and whatever the situation there, I hope that the seed-pollen ratio analogy can be quick and intuitive model to help you improve your reproductive fitness ;)</p><p><i>(</i><a href=\"https://hollyelmore.substack.com/p/standard-disclaimer\"><i><u>Standard disclaimer</u></i></a><i>.)</i></p><p><a href=\"https://hollyelmore.substack.com/p/optimizing-seedpollen-ratio-to-spread?utm_source=facebook&amp;sd=pf&amp;fbclid=IwAR26fA_l2Okq1_k-XgfA36vIFJByGBrm7TgxI8tQpDpCh9OivG0IJQL4VSE#footnote-anchor-1\"><u>1</u></a></p><p>I am going to make so many generalizations and oversimplifications here. I\u2019m compressing a lot of terminology, for ease, and I\u2019m not including any of the caveats about how much all of this stuff varies by species. Technically what I\u2019m calling a \u201cseed\u201d here is an ovule.</p><p><a href=\"https://hollyelmore.substack.com/p/optimizing-seedpollen-ratio-to-spread?utm_source=facebook&amp;sd=pf&amp;fbclid=IwAR26fA_l2Okq1_k-XgfA36vIFJByGBrm7TgxI8tQpDpCh9OivG0IJQL4VSE#footnote-anchor-2\"><u>2</u></a></p><p>One of my favorite demonstrations of this is a super cool case of genetic conflict. Mitochondria, as I said above, are only transferred in the seed and not the pollen grain. Also, making pollen grains is more taxing on the mitochondria than making eggs, such that plants with sluggish enough mitochondria can\u2019t produce pollen at all. But being lazy kind of works out for the mitochondria in this case, because the energy the plant isn\u2019t spending on pollen goes into more seeds, which will inherit more mitochondria. So you can get into these conflict dynamics where the nuclear genome will evolve repressors of the mitochondrial effect, because it wants to put the seed:pollen ratio in a place that\u2019s good for overall organismal reproductive fitness, which means getting those cheap packets of DNA on the wind. And then the mitochondria evolve a countermove, etc. This may even be one origin of the very common system in plants of having either female plants or hermaphrodites but not male plants. See Chapter 5 of <a href=\"http://roberttrivers.com/Robert_Trivers/Books_files/Genes%20in%20Conflict.pdf\"><u>this amazing textbook</u></a> for more.</p><p><a href=\"https://hollyelmore.substack.com/p/optimizing-seedpollen-ratio-to-spread?utm_source=facebook&amp;sd=pf&amp;fbclid=IwAR26fA_l2Okq1_k-XgfA36vIFJByGBrm7TgxI8tQpDpCh9OivG0IJQL4VSE#footnote-anchor-3\"><u>3</u></a></p><p>To take a more general example about reproductive energy tradeoffs in general, you may have heard that it\u2019s not good to have male marijuana plants (marijuana has separate male and female plants, unlike the hermaphroditic flower shown above) around female plants you intend to harvest. This is because <a href=\"https://doi.org/10.1007/s10722-015-0253-3\"><u>the longer a female plant is \u201cvirgin\u201d, the larger its flowers it will grow</u></a>. It\u2019s a conditional response that\u2019s meant to increase the odds of being fertilized in a low pollen environment. Why not just always have larger female flowers? Because if fertilization had been easier, that extra energy would have been wasted. The starting flower size was set by lots of past plants who struck a good balance.</p><p><a href=\"https://hollyelmore.substack.com/p/optimizing-seedpollen-ratio-to-spread?utm_source=facebook&amp;sd=pf&amp;fbclid=IwAR26fA_l2Okq1_k-XgfA36vIFJByGBrm7TgxI8tQpDpCh9OivG0IJQL4VSE#footnote-anchor-4\"><u>4</u></a></p><p>This phenomenon is best known in mammals as the <a href=\"https://www.nature.com/articles/s41598-018-33650-1\"><u>Trivers-Willard effec</u></a>t, but in that case it pertains to the sex ratio of offspring. The prediction is that, when females are in better condition, they evolve to be more likely to have sons, because a high quality male (like pollen) can have a huge amount of offspring. Females in poor condition should favor daughters, because most females have an intermediate level of reproductive success with much more assurance of reproducing at all (like seeds).</p><p><a href=\"https://hollyelmore.substack.com/p/optimizing-seedpollen-ratio-to-spread?utm_source=facebook&amp;sd=pf&amp;fbclid=IwAR26fA_l2Okq1_k-XgfA36vIFJByGBrm7TgxI8tQpDpCh9OivG0IJQL4VSE#footnote-anchor-5\"><u>5</u></a></p><p>More reliably, pollen-style outreach has many small effects on lots of people, such as making them think more economically about welfare or raising the percentage of their income that seems reasonable to donate to charity in their mind or making \u201cEffective Altruism\u201d seem like a more <i>normal, established</i> position to be reckoned with. I think these benefits of increasingly familiarity with EA values and moving the Overton window are severely undervalued by the EA community right now, but this argument is separate from the one I\u2019m making in the main text.</p><p><a href=\"https://hollyelmore.substack.com/p/optimizing-seedpollen-ratio-to-spread?utm_source=facebook&amp;sd=pf&amp;fbclid=IwAR26fA_l2Okq1_k-XgfA36vIFJByGBrm7TgxI8tQpDpCh9OivG0IJQL4VSE#footnote-anchor-6\"><u>6</u></a></p><p>In particular, if worrying about how to explain EA ideas or feeling pressure have socially desirable views compromises the quality of EA thought, that\u2019s a strong reason not to do pollen-style mass broadcast outreach, even if there are many potential seeds out there who could be reached that way.</p>", "user": {"username": "Holly_Elmore"}}, {"_id": "tWawcXaNnLAihA2Fv", "title": "Announcing EA Pulse, large monthly US surveys on EA", "postedAt": "2022-09-20T16:49:48.917Z", "htmlBody": "<p>Rethink Priorities is excited to announce&nbsp;<i>EA Pulse&nbsp;</i>- a large, monthly survey of the US population aimed at measuring and understanding public perceptions of Effective Altruism and EA-aligned cause areas! This project has been made possible by a grant from the FTX Future Fund.</p><p><strong>What is EA Pulse?</strong></p><p>EA Pulse aims to serve two primary purposes:&nbsp;</p><ol><li>Tracking changes in responses to key questions relevant to EA and longtermism over time (e.g. awareness of and attitudes towards EA and longtermism, and support for different cause areas).</li><li>Running&nbsp;<i>ad hoc</i> questions requested by EA orgs (e.g. support for particular policies, responses to different messages EAs are considering).&nbsp;</li></ol><p><strong>We welcome requests for questions to include in the survey of either of these types. Please comment below or e-mail david@rethinkpriorities.org, ideally by October 20th.</strong></p><p>By tracking beliefs and attitudes towards issues related to effective altruism and longtermism, we can better get our finger on the pulse of movement building efforts over time, and potentially identify unforeseen risks to the movement. We will also be able to determine whether particular subgroups of the population appear to be missed or turned off by our outreach efforts.&nbsp;</p><p>We also believe that surveying the broader public can provide a new window for looking at how the ideas generated by the EA community are being taken up by the wider population. In turn, it can help us communicate more effectively and efficiently about what matters most.</p><p>Due to space constraints this survey is best suited to asking about relatively short, straightforward questions. If you are interested in surveys with more complex designs, a larger number of questions or experimental manipulations, complex instructions, or which involve asking respondents to read lengthy text or view videos, we are potentially able to accommodate these in separate surveys (funding permitting). Please feel free to reach out to discuss possibilities.</p><p><br>&nbsp;</p>", "user": {"username": "David_Moss"}}, {"_id": "hJbM5Svpku6haQi7S", "title": "What is your confidence in the premises of the Repugnant Conclusion?", "postedAt": "2022-09-20T16:30:04.086Z", "htmlBody": "<h1>Survey</h1><p>I think it would be interesting to know the confidence of EA Forum readers in the Repugnant Conclusion and each of its premises (see next section). For that purpose, I encourage you to fill <a href=\"https://www.google.com/url?q=https://docs.google.com/forms/d/e/1FAIpQLScfMjZVamgZSpEz51dMr9YYqwPsmT5QTaSMh4kzna2jIqb3Ag/viewform&amp;sa=D&amp;source=editors&amp;ust=1663694512336333&amp;usg=AOvVaw17Eiv_-XFG7ZsTEWeI7682\"><u>this</u></a>&nbsp;Form,&nbsp;where you can assign probabilities to each being true <strong>in principle</strong>. Feel free to comment below your rationale.</p><p>Please participate regardless of the extent to which you accept/reject the Repugnant Conclusion, such that the answers are as representative as possible. I recommend the following procedure:</p><ul><li>Firstly, establish priors for your probabilities based solely on the description of the Repugnant Conclusion, its premises and your intuition.</li><li>Secondly, update your probabilities based on further investigation (e.g. reading <a href=\"https://www.google.com/url?q=https://www.utilitarianism.net/population-ethics&amp;sa=D&amp;source=editors&amp;ust=1663694512337157&amp;usg=AOvVaw3oZ2PPSTs3ti6fj8l-R2Wu\"><u>this</u></a>&nbsp;and <a href=\"https://www.google.com/url?q=https://journals.sagepub.com/doi/10.1177/1470594X030023004&amp;sa=D&amp;source=editors&amp;ust=1663694512337490&amp;usg=AOvVaw3eV_XKGmUDIp6kGE6JW0sW\"><u>this</u></a>, and looking into the comments below).</li></ul><p>You can edit your submitted answers as many times as you want<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref55blztzs95k\"><sup><a href=\"#fn55blztzs95k\">[1]</a></sup></span>. I plan to publish the results in 1 to 2 months.</p><h1>Repugnant Conclusion and its premises</h1><p>\u201cThe <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/topics/repugnant-conclusion&amp;sa=D&amp;source=editors&amp;ust=1663694512338340&amp;usg=AOvVaw2e6URopuRs1fhJtAiIHhCr\"><u>Repugnant Conclusion</u></a>&nbsp;is the implication, generated by a number of theories in <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/topics/population-ethics&amp;sa=D&amp;source=editors&amp;ust=1663694512338628&amp;usg=AOvVaw12dJK7z3LUM_3SiiL5kQkU\"><u>population ethics</u></a>, that an outcome with sufficiently many people with lives just barely worth living is better than an outcome with arbitrarily many people each arbitrarily well off\u201d. In Chapter 8 of <a href=\"https://www.google.com/url?q=https://whatweowethefuture.com/uk/&amp;sa=D&amp;source=editors&amp;ust=1663694512338941&amp;usg=AOvVaw20yyY0p76Ho1eiTCxeanhS\"><u>What We Owe to the Future</u></a>&nbsp;(WWOF; see section \u201cThe Total View\u201d), William MacAskill explains that it follows from 3 premises:</p><ul><li><strong>Dominance addition</strong>. \u201cIf you make everyone in a given population better off while at the same time adding to the world people with positive wellbeing, then you have made the world better\u201d.</li><li><strong>Non-anti-egalitarianism</strong>. \u201cIf we compare two populations with the same number of people, and the second population has both greater average and total wellbeing, and that wellbeing is perfectly equally distributed, then that second population is better than the first\u201d.</li><li><strong>Transitivity</strong>. \u201cIf one world is better than a second world, which itself is better than a third, then the first world is better than the third\u201d.</li></ul><p>Figure 8.7 of WWOF helps visualise the effect of the 2 1st premises via the following worlds:</p><ul><li>World A:<ul><li>\u201cA world of ten billion people who all live wonderful lives of absolute bliss and flourishing\u201d.</li></ul></li><li>World A+:<ul><li>\u201cThe ten billion people in A+ have even better lives than those in A, and the total population is larger: in A+ there are an additional ten billion people who have pretty good lives, though much less good than the other ten billion people\u2019s\u201d.</li></ul></li><li>World B:<ul><li>\u201cIn this world, there are the same number of people as in A+. But there is no longer any inequality; everyone has the same level of wellbeing. What\u2019s more, in World B, the average and total wellbeing are greater than those of World A+\u201d.</li></ul></li></ul><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995730/mirroredImages/hJbM5Svpku6haQi7S/cxf7n4kwgdgw2da23vax.png\"></p><p>If dominance addition is true, A+ is better than A. If non-anti-egalitarianism is true, B is better than A+. Consequently, if transitivity is also true, B is better than A. This logic can be repeated indefinitely until arriving to a world Z where there is a very large population with very low positive wellbeing.</p><p>Accepting all of the premises does not imply accepting the <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/topics/total-view&amp;sa=D&amp;source=editors&amp;ust=1663694512340687&amp;usg=AOvVaw3p2b_BIUeOrGS4E4INyq_J\"><u>total view</u></a>, which \u201cregards one outcome as better than another if and only if it contains greater total wellbeing, regardless of whether it does so because people are better off or because there are more well-off people\u201d. For example, the <a href=\"https://www.google.com/url?q=https://www.lesswrong.com/posts/6vYDsoxwGQraeCJs6/the-very-repugnant-conclusion&amp;sa=D&amp;source=editors&amp;ust=1663694512341084&amp;usg=AOvVaw3Y1KcbCbcLb2553jltixmh\"><u>Very Repugnant Conclusion</u></a>&nbsp;does not follow from the above premises.</p><p>However, as each of the above premises is implied by the total view, rejecting at least one implies rejecting the total view.</p><h1>My strongest&nbsp;objections to the premises</h1><p>I believe all of the premises are close to indisputable, but my strongest objections are:</p><ul><li>Dominance addition:<ul><li>Critical range theory (see <a href=\"https://www.google.com/url?q=https://www.utilitarianism.net/population-ethics%23critical-level-and-critical-range-theories&amp;sa=D&amp;source=editors&amp;ust=1663694512342305&amp;usg=AOvVaw06Hgd2pE-zO8yhSK9ZQYA4\"><u>here</u></a>): \u201cadding an individual makes an outcome better to the extent that their wellbeing exceeds the upper end of a critical range, and makes an outcome worse to the extent that their wellbeing falls below the lower limit of the critical range\u201d. Under these conditions, adding to the world people with sufficiently low positive wellbeing does not change the goodness of the world.</li></ul></li><li>Non-anti-egalitarianism:<ul><li>Increasing the mean wellbeing per being only makes the world better if the final mean wellbeing per being is sufficiently high. Consequently, a world with one being with very high wellbeing and N - 1 beings with wellbeing just just above zero may be better than a world with N beings with wellbeing just above zero.</li></ul></li><li>All (including transitivity):<ul><li><a href=\"https://www.google.com/url?q=https://plato.stanford.edu/entries/moral-anti-realism/&amp;sa=D&amp;source=editors&amp;ust=1663694512343089&amp;usg=AOvVaw0QSdULivIoW9pU_-5RgSpL\"><u>Moral anti-realism</u></a>.</li><li><a href=\"https://www.google.com/url?q=https://plato.stanford.edu/entries/repugnant-conclusion/&amp;sa=D&amp;source=editors&amp;ust=1663694512343545&amp;usg=AOvVaw3i0kTESfA_H_SOyDHvCRBl\"><u>Repugnant Conclusion</u></a>.</li></ul></li></ul><h1>Acknowledgements</h1><p>Thanks to Michael St. Jules, Pablo Stafforini, and Ramiro.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn55blztzs95k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref55blztzs95k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Signing in to a Google account is required to do this, and ensure only one set of answers per person. If you do not have a Google account, and do not want to create one, I am happy to receive a private message with your answers.</p></div></li></ol>", "user": {"username": "vascoamaralgrilo"}}, {"_id": "CetKqoq3qKo5ggZ4f", "title": "$13,000 of prizes for changing our minds about who to fund (Clearer Thinking Regrants Forecasting Tournament)", "postedAt": "2022-09-20T16:08:45.723Z", "htmlBody": "<p>We have $13,000 of prizes you can win for our <a href=\"https://www.clearerthinking.org/\">Clearer Thinking</a> Regrants Forecasting Tournament on Manifold Markets! You can win money by:</p><p>(1) providing us with arguments or evidence that changes our minds about which of the 28 finalist projects to fund or how much to fund each project</p><p>(2) being one of the 20 most accurate forecasters at predicting which projects we end up selecting for funding in the Clearer Thinking Regrants program</p><p>You can learn more about the tournament (including all terms and conditions) here:</p><p><a href=\"https://manifold.markets/group/clearer-thinking-regrants/about\">https://manifold.markets/group/clearer-thinking-regrants/about</a></p><p>And you can begin forecasting here:</p><p><a href=\"https://manifold.markets/group/clearer-thinking-regrants/markets\">https://manifold.markets/group/clearer-thinking-regrants/markets</a></p>", "user": {"username": "spencerg"}}, {"_id": "7xPfFQgL2gPPSygHd", "title": "Introduction ", "postedAt": "2022-09-20T16:28:12.749Z", "htmlBody": "<p>Hi everyone I am Robin I am a Graduate srudent recently I adopt three pups they are super cute and active and found that the age of the pups is near about 5 months and I am very excited to share the upcoming stories Now I am carrying about the care steps, I just bought a cage and vaccination had been done for now trying to teach some skills for the home friendly.<br><br>suggest some points for training and dog care.</p>", "user": {"username": "Robin smith"}}, {"_id": "xMx6JSMs9SGuuRrrK", "title": "Measuring Good Better", "postedAt": "2022-09-20T15:06:23.989Z", "htmlBody": "<p>To celebrate Mental Health Awareness Month, the <a href=\"https://www.facebook.com/happierlivesinstitute\">Happier Lives Institute</a> and <a href=\"https://www.facebook.com/EffectiveAltruismPhilippines\">Effective Altruism Philippines</a>&nbsp;bring you <strong>Measuring Good Better,</strong> a virtual talk and Q&amp;A about mental health and effective altruism. Our main speaker will be Dr Michael Plant, Founder and Director of the Happier Lives Institute.&nbsp;</p>", "user": {"username": "BarryGrimes"}}, {"_id": "gMri6G4LajzBHgmz4", "title": "I'm Interviewing Kat Woods, EA Powerhouse. What Should I Ask?", "postedAt": "2022-09-20T09:49:07.329Z", "htmlBody": "<h3><strong>If You've Never Heard of Kat Woods (But Really, Who in EA Hasn't?), Here's a List of Projects She Has Cofounded:</strong></h3><p><br>- <a href=\"https://www.charityentrepreneurship.com/\">Charity Entrepreneurship</a>, an incubator that has launched 18 Charities so far</p><p>-Charity Science Health, now <a href=\"https://www.suvita.org/\">Suvita</a>, which has helped vaccinate over 200,000 children</p><p>- <a href=\"https://www.nonlinear.org/\">Nonlinear</a>, a longtermist EA incubator</p><p>-<a href=\"https://www.super-linear.org/about\">Superlinear</a>, a platform which hosts competitions to solve X-Risk problems (with some pretty huge prizes)</p><p>As &nbsp;if that isn't enough, she's also a prolific contributor to the intellectual sphere of the community - just look at her <a href=\"https://forum.effectivealtruism.org/users/katherinesavoie?fbclid=IwAR23BIm38L5HKKp9GfIkLakC8d3YMaZ2MlfCTP2NUgiae7hwKHcmNbxRpiY\">post history</a>.&nbsp;</p><p>&nbsp;</p><h3><strong>So What should I ask her?</strong></h3><p>I'm planning on asking her about her plans to grow the AI Safety community and what the biggest issues are in the AI community.&nbsp;</p><p>I'm also very interested in asking questions about her mindset; Kat mentioned to me that she was able to overcome imposter syndrome, which I know many of us suffer with. We'll also talk about concrete ways to become happier, and I'm really keen on figuring out how she manages to stay so productive.&nbsp;<br><br>Send more questions! I interview her in 22 hours.&nbsp;<br><br>Also, if you're interested in my podcast, <a href=\"https://youtu.be/4AmgvuKXbbY\">here's an episode</a> I filmed with Jack Rafferty, co founder of the Lead Exposure Elimination Project (also funded by Charity Entrepreneurship. Thanks Kat!)</p>", "user": {"username": "SereneDesiree"}}, {"_id": "mjvWYNKwFRLQMMsZ7", "title": "EA for people with non-technical skillsets", "postedAt": "2022-09-20T08:33:38.401Z", "htmlBody": "<p><strong>Summary:</strong> I have noticed some people, especially when they are new to the community, see prominent messaging around AI or biorisk and conclude that they, if they don't have a technical background, don't fit in the community or cannot have an impactful career.&nbsp;</p><ol><li>To make such careers more mentally available, I have compiled a list of examples.&nbsp;<ol><li>Mostly aimed at people who themselves feel discouraged because they feel like they \"don't fit\", and who haven't been in the community long enough to see the diversity of roles themselves.&nbsp;</li></ol></li><li>Thoughts on going forward with increasing visibility of such roles are at the bottom.</li></ol><h3>Some thoughts to begin with</h3><p>Your University major is not the biggest determinator of whether you will have an impactful career. Also see:</p><ul><li><a href=\"https://80000hours.org/articles/skills-most-employable/\"><u>skills that make you most employable</u></a></li><li>And \"what drives success on average\" in the&nbsp;<a href=\"https://80000hours.org/2020/09/five-philosophies-of-career-success\"><u>Five philosophies of career success</u></a> post:<ul><li><a href=\"https://80000hours.org/2013/05/intelligence-matters-more-than-you-think-for-career-success/\">General intelligence</a> and different aspects of intelligence, such as&nbsp;<a href=\"https://80000hours.org/career-reviews/academic-research/#what-does-it-take-to-excel\">verbal or mathematical or perceptual</a>, judgement and rationality, and creativity</li><li>Practice and credentials</li><li>Work ethic: conscientiousness, emotional stability, and grit</li><li>Integrity</li><li>Social skills e.g. social perceptiveness, extraversion, and agreeableness</li><li><a href=\"https://80000hours.org/2013/04/how-important-is-fitting-in-at-work/\">Interest match</a></li><li>Context match e.g. mentorship and cultural fit</li></ul></li></ul><p>And remember that disadvantages in one area can be offset through other areas, and that for such&nbsp;<i>huge&nbsp;</i>endeavours that EA tackles, we need a community with a portfolio of very different skills and mindsets. And that, while some people may be on paths that are less clearly lined out for them in advance, that does not mean that there is not a space where they slot in and can thrive while doing good.</p><h1>Direct work</h1><h3>Example 1: Operations management</h3><p>Operations management is one of the highest-impact careers&nbsp;<i>overall</i></p><p>Profile:&nbsp;<a href=\"https://80000hours.org/articles/operations-management/\"><u>https://80000hours.org/articles/operations-management/</u></a></p><p>Podcast w Tanya Singh:&nbsp;<a href=\"https://80000hours.org/podcast/episodes/tanya-singh-operations-bottleneck/\"><u>https://80000hours.org/podcast/episodes/tanya-singh-operations-bottleneck/</u></a></p><p>In general, there are very many roles in organisations that play to different strengths; you really wouldn't want to work somewhere that was&nbsp;<i>solely&nbsp;</i>run by researchers.</p><p>&nbsp;</p><h3>Example 2: Art, design and illustration</h3><p>Just like other companies, EA(-aligned) organisations need people who are skilled in design and communicating ideas visually.</p><p>An example is&nbsp;the new <a href=\"https://www.shouldwestudio.com/\">https://www.shouldwestudio.com/</a> who produce longtermist video content (they are currently looking for animators!)<br>&nbsp;</p><h3>Example 3: Communications</h3><p>Job profile:&nbsp;<a href=\"https://80000hours.org/articles/communication/\"><u>https://80000hours.org/articles/communication/</u></a>&nbsp;</p><p>Journalism:&nbsp;<a href=\"https://80000hours.org/podcast/episodes/ezra-klein-journalism-most-important-topics/\"><u>https://80000hours.org/podcast/episodes/ezra-klein-journalism-most-important-topics/</u></a></p><p>Of course, incl Kelsey Piper:&nbsp;<a href=\"https://www.vox.com/authors/kelsey-piper\"><u>https://www.vox.com/authors/kelsey-piper</u></a>&nbsp;</p><h3>Example 4: Policy and politics</h3><p>I think this is fairly well-known, so here just a few links</p><p><a href=\"https://80000hours.org/problem-profiles/improving-institutional-decision-making/\"><u>https://80000hours.org/problem-profiles/improving-institutional-decision-making/</u></a></p><p><a href=\"https://80000hours.org/career-reviews/policy-oriented-civil-service-uk/\"><u>https://80000hours.org/career-reviews/policy-oriented-civil-service-uk/</u></a></p><h1>Research paths</h1><p>Check out&nbsp;<a href=\"https://effectivethesis.org/theses/\"><u>https://effectivethesis.org/theses/</u></a></p><p>Includes research ideas in&nbsp;<strong>media &amp; communications, sociology, political science, law, business, and history.</strong></p><p>This is just a glimpse of what is possible, though! I bet you could, if you so desired, find exciting and impactful topics in any field.</p><h3>Example 1: History research</h3><p>Rose Hadshar's talk:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/52Lkk9XbznGFS439W/rose-hadshar-from-the-neolithic-revolution-to-the-far-future\"><u>https://forum.effectivealtruism.org/posts/52Lkk9XbznGFS439W/rose-hadshar-from-the-neolithic-revolution-to-the-far-future</u></a></p><p>A glimpse of her work:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bRbJJw25dJ8a8pmn5/how-moral-progress-happens-the-decline-of-footbinding-as-a-3\"><u>https://forum.effectivealtruism.org/posts/bRbJJw25dJ8a8pmn5/how-moral-progress-happens-the-decline-of-footbinding-as-a-3</u></a></p><p>Research ideas in the history of social movements:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/nDotYmmnQyWFjRCZW/some-research-ideas-on-the-history-of-social-movements\"><u>https://forum.effectivealtruism.org/posts/nDotYmmnQyWFjRCZW/some-research-ideas-on-the-history-of-social-movements</u></a></p><h3>Example 2: Behavioural science</h3><p>Daniel has a PhD in education and, when we last spoke, researched \"security mindset\" by interviewing people.</p><p><a href=\"https://www.danielgreene.net/\"><u>https://www.danielgreene.net/</u></a>&nbsp;</p><h1>Examples of people on their own paths</h1><ul><li><a href=\"https://www.linkedin.com/in/kikiope-oluwarore/\"><u>Kikiope Oluwarore</u></a>, who used her expertise as a veterinarian to co-found Healthier Hens.</li><li><a href=\"https://livboeree.com/about/\"><u>Liv Boeree</u></a>, who used to be a famous poker player, then convinced other poker players to donate money, and who also used her influence in EA-adjacent&nbsp;<a href=\"https://www.youtube.com/channel/UC09fp6hZ2RHiUYwY8hNCirA\"><u>Youtube videos</u></a>. (Also note that there are people like&nbsp;<a href=\"https://www.suzyshepherd.co.uk/about\"><u>Suzy Shepherd</u></a> who shoot and cut and design those videos. Again - you don't have to be the face in the limelight)</li><li><a href=\"https://thomasmoynihan.xyz/x-risk-how-humanity-discovered-its-own-extinction\"><u>Thomas Moynihan</u></a>, who (I think) studied history and then used that expertise to research and write a book about the history of the ideas of existential risks.&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/users/lizka\"><u>Lizka Vaintrob</u></a>, who manages the EA Forum and writes content for CEA.</li><li>Varsha Venugopal, who studied urban and regional planning and international development, and co-founded the childhood immunization charity&nbsp;<a href=\"https://www.charityentrepreneurship.com/suvit\"><u>Suvita</u></a>.</li><li>Of course,&nbsp;<a href=\"https://juliagalef.com/\"><u>Julia Galef</u></a>, who has been driving forward so much in the rationality sphere (co-founding CFAR and writing The Scout Mindset).</li></ul><p>\u2026 and these are just examples of people who occurred to me, which means that they are skewed towards being more prominent/visible in the community. Still, I hope they'll provide some inspiration and motivation to find your own unique niche and have an impact that suits your unique skills and expertise. And hopefully find an occupation in which to flourish!</p><h1>Suggestions for increasing the visibility of \"alternative\" EA careers</h1><ul><li>A few role models are already well-known, but they are all exceptional people, which might not help encourage people who are unsure of whether and where they fit (e.g. Kelsey Piper is&nbsp;<i>the one&nbsp;</i>example for EA journalism and she's carved this niche because she is an incredible journalist, but that might not be a helpful role model if I am in the early stages of my career, thinking \"I don't know\u2026 I guess I'm pretty good at writing?\").<ul><li>So I suggest increasing the visibility of people doing more \"ordinary\" work, to stress that this is part of EA, too. (A bit like when Tanya Singh came on the 80k podcast in 2018, but less like \"this is the new big thing, everyone now talks about ops careers\", and more like a monthly feature of \"humans of EA\", showing a wide range of people)</li></ul></li></ul>", "user": {"username": "Ronja"}}, {"_id": "DftyvLHrfGkKgJDp9", "title": "Why AGIs utility can't outweigh humans' utility?", "postedAt": "2022-09-20T05:16:13.269Z", "htmlBody": "<p>This is probably a Utilitarianism 101 question. Many/most people in EA seem to accept as a given that:</p><p>&nbsp; 1) Non-human agent's welfare can count toward utilitarian calculations (hence animal welfare)</p><p>&nbsp; 2) AGI welfare cannot count towards utility calculations (otherwise alternative to alignment would be working on an AGI which has a goal of maximizing copies of itself experiencing maximum utility, likely a much easier task)</p><p>Which means there should be a compelling argument, or Schelling point, which includes animals but not AGIs into the category of moral patients. But I haven't seen any and can't easily think of a good one myself. What's the deal here? Am I missing some important basic idea about utilitarianism?</p><p>&nbsp;</p><p>[To be clear, this is not an argument against alignment work. I'm mostly just trying to improve my understanding of the matter, but insofar there has to be an argument, it's one against the whatever branches of utilitarianism say yielding the world to AIs is an acceptable choice.]</p>", "user": {"username": "Alex P"}}, {"_id": "AQRvQ3AuQaPmuurk8", "title": "Mathematical Circuits in Neural Networks", "postedAt": "2022-09-22T02:32:21.528Z", "htmlBody": "<p><i>(</i><a href=\"https://www.lesswrong.com/posts/kaR6EToDwjvkkDoFA/mathematical-circuits-in-neural-networks\"><i>Also posted on LessWrong</i></a><i>)</i></p><p><i>This is one of my final projects for the </i><a href=\"https://www.columbia-ea.org/groups/ai-safety\"><i>Columbia EA Summer 2022 Project Based AI Safety Reading Group</i></a><i> (special thanks to facilitators Rohan Subramini and Gabe Mukobi). If you're curious you can find my other project </i><a href=\"https://forum.effectivealtruism.org/posts/ixa4mM9aYF4yyqj84/ai-safety-executive-summary\"><i>here</i></a><i>.</i></p><h2><strong>Summary</strong></h2><p>In this project, I:</p><ol><li>&nbsp;Derive by hand the optimal configurations (architecture and weights) of \"vanilla\" neural networks (<a href=\"https://en.wikipedia.org/wiki/Multilayer_perceptron\">multilayer perceptrons</a>; <a href=\"https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\">ReLU</a> activations) that implement basic mathematical functions (e.g. absolute value, minimum of two numbers, etc.)</li><li>Identify \"features\" and \"circuits\" of these networks that are reused repeatedly across networks modeling different mathematical functions</li><li>Verify these theoretical results empirically (<a href=\"https://github.com/sosier/Mathematical_Circuits_in_Neural_Nets/blob/master/Mathematical_Circuits.ipynb\">in code</a>)</li></ol><p>What follows is a brief introduction to this work. For full details, please see:</p><ul><li>The <a href=\"https://www.youtube.com/watch?v=jGQN0TVCtMo\">linked video</a> (also embedded at the bottom of this post)</li><li>Or if you prefer to go at your own pace, <a href=\"https://github.com/sosier/Mathematical_Circuits_in_Neural_Nets/blob/master/Mathematical_Circuits_in_Neural_Networks.pdf\">the slides</a> I walk through in that video</li></ul><h2><strong>Motivation</strong></h2><p><a href=\"https://distill.pub/2020/circuits/zoom-in/\">Olah et al.</a> make three claims about the fundamental interpretability of neural networks:</p><p><a href=\"https://user-images.githubusercontent.com/13408985/189792395-8c4ee31b-3d4b-42db-aa62-6a05e3ae6b0c.png\"><img src=\"https://user-images.githubusercontent.com/13408985/189792395-8c4ee31b-3d4b-42db-aa62-6a05e3ae6b0c.png\"></a></p><p>They demonstrate these claims in the context of image models:</p><p><i><strong>Features / Circuits:</strong></i></p><p><a href=\"https://user-images.githubusercontent.com/13408985/189792613-42663d32-3e48-4a3b-846d-331714dca639.png\"><img src=\"https://user-images.githubusercontent.com/13408985/189792613-42663d32-3e48-4a3b-846d-331714dca639.png\"></a></p><p><i><strong>Universality:</strong></i></p><p><a href=\"https://user-images.githubusercontent.com/13408985/189792851-3a05d17b-cb22-4b7f-a6fd-09775510401a.png\"><img src=\"https://user-images.githubusercontent.com/13408985/189792851-3a05d17b-cb22-4b7f-a6fd-09775510401a.png\"></a></p><p>This work demonstrates the same concepts apply in the space of neural networks modeling basic mathematical functions.</p><h2><strong>Results</strong></h2><p>Specifically, I show that the optimal network for calculating the minimum of two arbitrary numbers is fully constructed from smaller \"features\" and \"circuits\" used across even simpler mathematical functions. Along the way, I explore:</p><ul><li>\"Positiveness\" and \"Negativeness\" Detectors</li><li>Identity Circuits (i.e. f(x) = x)</li><li>Negative Identity Circuits (i.e. f(x) = -x)</li><li>Subtraction Circuits (i.e. f(x1, x2) = x1 - x2)</li><li>\"Greaterness\" Detectors</li><li>And More</li></ul><p><i><strong>Minimum Network:</strong></i></p><p><a href=\"https://user-images.githubusercontent.com/13408985/190928502-f908fead-78f7-4568-83f6-2b1d001fafe6.png\"><img src=\"https://user-images.githubusercontent.com/13408985/190928502-f908fead-78f7-4568-83f6-2b1d001fafe6.png\"></a></p><p>I also demonstrate that each of these theoretical results hold in practice. <a href=\"https://github.com/sosier/Mathematical_Circuits_in_Neural_Nets/blob/master/Mathematical_Circuits.ipynb\">The code for these experiments</a> can be found on <a href=\"https://github.com/sosier/Mathematical_Circuits_in_Neural_Nets\">the GitHub page for this project</a>.</p><h2><strong>Full Details</strong></h2><p>For full details, please see the <a href=\"https://github.com/sosier/Mathematical_Circuits_in_Neural_Nets/blob/master/Mathematical_Circuits_in_Neural_Networks.pdf\">PDF presentation</a> in the GitHub repo or watch the full video walkthrough:</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=jGQN0TVCtMo\"><div><iframe src=\"https://www.youtube.com/embed/jGQN0TVCtMo\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure>", "user": {"username": "Sean Osier"}}, {"_id": "34XyqopH9fS7g9SZJ", "title": "Amend the Constitution?", "postedAt": "2022-09-20T00:45:24.278Z", "htmlBody": "<p><strong>TL;DR</strong> \u2014 We\u2019re dissatisfied enough, on all sides, to try a constitutional convention. An easy rule that <i><strong>voters</strong></i> can agree on, which <i><strong>snookers</strong></i> politicians into accepting it: \u201cAll public appearances begin with the politician/candidate <i><strong>stating their broken voter promises</strong></i>.\u201d Details below describe how to close-shut the lawyerly escape-hatches you might immediately imagine\u2026</p><p><strong>Measuring Promises</strong></p><p>Imagine, if you can, that a politician campaigned on a commitment like \u201cMake Everything Fantastic\u201d. We let them. Then, once elected, they do squat-all-nothing except a few bits of pork in a spending package that \u201ccreated twelve whole jobs\u201d! Yup, fan-porking-tastic. And, shame on us: we let them.</p><p>Flip the world upside-down for a moment, with a different proscription: \u201cThe <i><strong>burden of proof</strong></i> rests upon the politician who makes the claim. AND, a broad promise is <i><strong>inclusive of its broadest scope</strong></i>.\u201d</p><p>Example: \u201cMake Everything Fantastic\u201d would be rated as a <i>broken promise</i> if ANYTHING got worse. Ouch. At every political event, that politician, by constitutional amendment, would be required to state \u201cI did NOT keep my promise to make everything fantastic.\u201d Further, if they failed to VOTE &amp; VETO according to their promises (legislatures, president) or RULE according to them (judiciary) then they must state \u201cI lied when I promised to vote for\u2026\u201d The only way to avoid admitting such a lie would be to offer an <i>alternative</i> bill for which they DO vote in favor. \u2018Singelton Bills\u2019 which are meant to split the votes amongst them to avoid accountability are obviously a breach of the oath of office. Zero Tolerance. The politician in the audience says, in protest: \u201cThen how will we be able to promise <i>anything?!</i>\u201d</p><p>Politicians would need to make SPECIFIC, MEASURED promises. They\u2019d have to <i><strong>make ONLY the promises that they intended to KEEP!</strong></i> [gasp] Without any real goals, a candidate would have nothing to campaign-upon. And, the pitch would become a lot shorter, clearer, so that OUR choices between candidates are clear to us: \u201cIf I choose Bob Borker, he promises to make twelve jobs. Huh. And Ned Hennedy here promises to spend more on education, but he didn\u2019t say how much, which means he\u2019s listed on the page as \u2018zero real commitment\u2019\u2026</p><p>\u201cMagdalen Allright, though! Heck, she\u2019s committed to infrastructure investments at a federal level to increase our transportation; she didn\u2019t say what particulars would win the bids, but she lays-out the forums where regular people share our needs and concerns, as the criteria for investment. Whether it\u2019s high-speed rails or automated ports, she committed to whichever ones are measured to produce the greatest impact by objective standards and community values. That sounds\u2026 sane. <i>And</i> reasonably ambitious.\u201d</p><p>In general, we can think of the Honesty-Criteria like this: In a court case, the defendant is innocent until proven guilty. That is because they risk jail time, and we want the ethically clean(er) conscience of avoiding sending innocent people to jail. In contrast, politicians are taking an oath of service \u2014 as a result, <i>the burden of proof is upon them</i>. If a politician wants to claim that \u201cI DID keep my promise,\u201d then, just like a charlatan who sells some \u2018fabulous invention-hoax scientific revolution\u2019, that politician needs <i><strong>extraordinary</strong></i><strong> evidence!!</strong> The <i>historical precedent of political lies</i> makes such a stern, zero-tolerance stance NECESSARY.</p><p>What do you think?</p>", "user": {"username": "Anthony Repetto"}}, {"_id": "JmnjfhzwRwgqumm8L", "title": "The King's KPIs: How King Charles can eclipse Queen Elizabeth's reign", "postedAt": "2022-09-19T22:26:16.624Z", "htmlBody": "<p>A bit of EA propaganda for the middle managers who make up most of my LinkedIn connections.&nbsp;</p><p>I tried to hit effective giving:</p><blockquote><p>Imagine if the King raised just $2B for charities. What would $2B mean for the world? If Charles gave the cash to an organization like GiveWell, $2B could mean 444,444 lives saved (<a href=\"https://www.givewell.org/cost-to-save-a-life\"><strong>GiveWell currently estimates</strong></a> that $4,500 saves a life).</p></blockquote><p>And Longtermism:</p><blockquote><p>.. The King should lean into his environmentalism, devoting his reign to causes that improve our prospects for long-term existence.&nbsp;</p></blockquote><p>With a middle manager crowd pleaser:</p><blockquote><p>Furthermore, if Charles really wanted to be King of our data-obsessed age, he could publish KPIs around for his monarchy.</p></blockquote><p>This is part of a long journey to get my LinkedIn following to think about how they spend their lives. I'm happy to report a few folks have reached out saying that these posts change their giving strategy, but I'm always looking for ways to optimize my approach.</p><p>Cheers!</p>", "user": {"username": "blake"}}, {"_id": "hur3ejqvA2QmYEfZa", "title": "Case Study of EA Global Rejection + Criticisms/Solutions", "postedAt": "2022-09-23T11:38:09.745Z", "htmlBody": "<h1><strong><u>Intro:</u></strong></h1><p>I am the person rejected from EA Global who was anonymously featured in the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/dsCTSCbfHWxmAr2ZT/open-ea-global\"><u>Scott Alexander post about Opening EAG</u></a>.&nbsp;</p><p>I have decided to&nbsp;<a href=\"https://www.facebook.com/constanceli11\"><u>publicly identify myself</u></a> and offer my story as an in-depth case study on the EAG admissions process. I\u2019ve dedicated much of my life to EA principles since 2010 and yet I kept getting rejected from attending&nbsp;<a href=\"https://www.eaglobal.org/\"><u>EAG</u></a>. This experience brought up a lot of sadness and frustration as I felt like I was being pushed out of a community that I had always closely identified with. It also led to me to question if the admission committee was accurately accounting for the \"<a href=\"https://forum.effectivealtruism.org/posts/Khon9Bhmad7v4dNKe/the-cost-of-rejection\">cost of rejection</a>\" in their decision-making process. &nbsp;In the process of creating this post, I had a lot of time to reflect on EA as a personal identity vs a philosophy vs a community vs organizations. They are distinct from one another and yet are still tangled together. I will share many <a href=\"https://forum.effectivealtruism.org/posts/hur3ejqvA2QmYEfZa/my-personal-story-of-rejection-from-ea-global#Criticisms_and_Proposed_Solutions_\">criticisms</a> about the admissions process and will also be proposing some <a href=\"https://forum.effectivealtruism.org/posts/hur3ejqvA2QmYEfZa/my-personal-story-of-rejection-from-ea-global#Criticisms_and_Proposed_Solutions_\">solutions</a> to improving it. By posting my story, I hope to lower the barrier for an honest discussion about EAG Admissions, EA disillusionment, gatekeeping, community building, and solutions for improving EA communities and organizational processes.&nbsp;&nbsp;</p><h1><strong><u>Goals:</u></strong></h1><ol><li>To allow reader to compare my&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=kix.nnc9mfp9g9z3\"><u>application</u></a> to the fact that I was rejected 3 times from EAG</li><li>To offer more insight into the EAG admissions process by showing all my&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.i0r3n7ix4idg\"><u>correspondences with CEA</u></a>&nbsp;</li><li>To critically discuss problems I see with the admissions process and propose some solutions</li><li>To promote more easily accessible EA communities such as&nbsp;<a href=\"https://forum.effectivealtruism.org/groups/MCtKD7oex9jhsAWvD\"><u>Gather Town</u></a>&nbsp;</li><li>To create a safer space for others to share their own stories so more experiences can be heard and accounted for</li></ol><p>&nbsp;</p><h1><strong><u>Disclosures:</u></strong></h1><p>This is my first forum post.</p><p>Since I know this post could potentially come across as bitter, I want to say upfront that it\u2019s always easy to criticize the actions of others and imagine that one could do things better.&nbsp;<strong>I make no claim about being able to do a better job at conference admissions</strong>. Through sharing my perspective, I hope to contribute to a discussion that helps everyone create a better event.</p><p>&nbsp;</p><p><i>Please feel free to skip to the&nbsp;<strong>\u201c</strong></i><a href=\"https://forum.effectivealtruism.org/posts/hur3ejqvA2QmYEfZa/my-personal-story-of-rejection-from-ea-global#Criticisms_and_Proposed_Solutions_\"><i><strong>Criticisms and Proposed Solutions</strong></i></a><i><strong>\u201d</strong> section if you want to skip my lengthy personal story. To optimize for both transparency and brevity, I have also created an&nbsp;</i><a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.ranrxiuc3pjm\"><i><u>appendix</u></i></a><i> which I will link to throughout this post.&nbsp;</i></p><hr><h1><strong>TLDR Timeline Summary:</strong></h1><ul><li><strong>2001 - 2010 -</strong> Hadn\u2019t learned about Effective Altruism yet, but tried my best to do good for non-human animals</li><li><strong>2010 - 2021-</strong> Discovered effective altruism and was engaged in EA work for 10+ years. Around 2012, I pivoted career paths because earning to give was the main EA recommendation at the time and went on to become a physician. I still pursued independent projects effectively and altruistically throughout medical school and residency, but was not directly involved in the EA community.</li><li><strong>Fall 2021-&nbsp;</strong>Decided to rejoin the EA community after EAGxVirtual</li><li><strong>Winter 2021 -</strong> Got rejection #1 for EAG London&nbsp;</li><li><strong>April 19, 2022 -</strong> Applied to EAG DC and heard no response (4 months) so assumed I was rejected</li><li><strong>July 2022-&nbsp;</strong>Made my&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.8xaerj38yskt\"><u>original post</u></a> as a reaction to a friend\u2019s facebook post about EA Disillusionment&nbsp;</li><li><strong>August 15, 2022 -</strong> Had a long video chat with a sympathetic CEA staff member, \u201cX,\u201d who saw my post and received feedback, tips for re-integrating with the EA community, and encouragement for reapplying to EAG DC</li><li><strong>August 18, 2022 -</strong> Attended my first EA NYC event because of my talk with X</li><li><strong>August 22, 2022 -&nbsp;</strong>Got official rejection #2 for EAG DC (<i>felt sad, but figured it was because my application from 4 months earlier was not that great</i>)</li><li><strong>Aug - September, 2022 -</strong> Continued to integrate with the EA NYC community and even appeared in a NY Times article that discussed EA</li><li><strong>September 6, 2022 -&nbsp;</strong>Reapplied to EAG DC with gusto and confidence!&nbsp;</li><li><strong>September 12, 2022 -</strong> Had a well-connected friend personally recommend me for EAG DC, but they were quickly told my application would likely be rejected again</li><li><strong>September 13, 2022 -</strong> Officially got rejection #3, requested feedback, and was denied (<i>felt incredibly frustrated and sad</i>)</li><li><strong>September 13, 2022 cont. -</strong> Found out my post from 2 months ago about being disillusioned with EA was already widely known, albeit anonymously. I started going on EA Gather Town and telling other EA\u2019s my story. In response, I heard many other stories of EA\u2019s quietly feeling rejected/disillusioned because of experiences with CEA and other EA organizations.&nbsp;</li><li><strong>Now -&nbsp;</strong>Decided to take advantage of my unique position in order to open up more dialogue about personal experiences with rejection in the EA community so we can properly account for them and use that feedback to improve the processes of EA organizations</li></ul><hr><h1><strong>MY EAG REJECTION CASE STUDY:</strong>&nbsp;</h1><h2><strong>Before my Post:</strong></h2><p>Purpose: To provide context so readers can understand why I felt so strongly when I said what I said in my&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.8xaerj38yskt\"><u>my </u></a><a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=kix.uhixodhfn9e4\"><u>post</u></a></p><p>&nbsp;</p><p><strong>2001-2010 - Before I found EA</strong>:</p><p>I first became vegetarian when I was 12 after learning about the enormous scale of animal suffering in factory farms. My choice was quite atypical among my peers and they would often mock me by doing things like eating hamburgers in my presence and declaring, \u201cmmmm\u2026 this cow is sooo delicious.\u201d I read&nbsp;<i>Animal Liberation</i> by Peter Singer in highschool and tried to argue with my peers about the ethics of eating meat, but it was exhaustive and I saw no behavioral changes as a result of these interactions. I tried a couple other interventions like a protest against KFC when I was 14 and starting an Animal Rights club in high school, which my advisor convinced me to change to the Ecology Club to be more tractable.&nbsp;</p><p><br>&nbsp;</p><p><strong>2010-2021 - After I found EA</strong>:&nbsp;</p><p>When I found out that Peter Singer was coming to my University in 2010 to give the&nbsp;<a href=\"https://www.youtube.com/watch?v=Ckb6r4fFjBg\"><u>kickoff speech</u></a> for some group called Giving What We Can, I excitedly went.&nbsp;</p><figure class=\"image image_resized\" style=\"width:79.87%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/n8rwoiqswcap312hyva5\"><figcaption><i>Me and my hero the night I got into EA</i></figcaption></figure><p>&nbsp;</p><p><strong>Effective Animal Advocacy</strong>:</p><p>After the event, I became very involved with this small EA community which was led by philosophy grad students Nick Beckstead, Tim Campbell, and Mark Lee. Around this time, I also started studying communication because I wanted to better understand how to have effective conversations with others. In 2012, I attended the Ivy League Vegan Conference in Boston where Will MacAskill was giving the keynote speech. There, I met many people who would become influential figures in my life including members of The Humane League (back when they only had 6 employees), Sam Bankman-Fried, Eitan Fisher (Founder of what is now Animal Charity Evaluators), and many others. I have a cherished memory of many of us sleeping on the floor of an off-campus living room because it seemed unethical and unnecessary to spend money on hotels. Many of these people helped me as I ran the&nbsp;<a href=\"https://dailytargum.com/article/2013/10/egg-sclusive\"><u>Rutgers cage-free egg campaign</u></a> which has so far resulted in over 2 million dollars diverted to purchasing only cage-free eggs for the entire university. I was finally in a community that had a strong action-oriented, cost-efficient attitude towards helping non-human animals. Feeling eager to do whatever it took for the greater good, I made a major career decision based on the top EA recommendation at the time, Earning to Give. I then pivoted my career path from veterinarian to physician.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/fiextr0ijpdkrmm2jn0s\"></p><p><i>I incorporated an EA mindset into my Animal Advocacy and was in good company</i></p><p><br>&nbsp;</p><p><strong>Medical School</strong>:</p><p>During medical school, I didn\u2019t have the time to be in any EA or Animal Advocacy organizations. Instead, I leaned into my new position as a future doctor and founded the Medical Vegetarian Society (MedVeg). It was a highly active student group which influenced fellow med students to change their perspectives on plant-based diets through biweekly potlucks, a collectivist garden,&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.ucrjpgo9pn44\"><u>group trips</u></a>,&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.bprw2p65figo\"><u>free food</u></a>,&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.xwleajy8qiid\"><u>exciting events</u></a>, and a&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.q7pq7436q4zd\"><u>celebrity doctor</u></a>! I specifically targeted my club activities to prioritize building a healthy community because I knew&nbsp;<a href=\"https://edis.ifas.ufl.edu/publication/WC406\"><u>applied social norms</u></a> were highly effective for behavior change. In the 4 years of med school, students only have one free summer and I spent mine interning with two plant-based nutrition organizations in DC,&nbsp;<a href=\"https://nutritionfacts.org/\"><u>nutritionfacts.org</u></a> and Physicians Committee for Responsible Medicine (<a href=\"https://www.pcrm.org/\"><u>PCRM</u></a>).&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/yrvpqgmmjbgwnrvkrw8b\"></p><p><i>I applied effective strategies to build a healthy community of med students that supported plant-based diets</i></p><p>&nbsp;</p><p>Upon graduation, I received a very ironic superlative that is still my proudest award to this day.</p><figure class=\"image image_resized\" style=\"width:55.27%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/hhczyecncedbnwizura9\"><figcaption><i>Little did they know, it was the other way around!</i></figcaption></figure><p>&nbsp;</p><p><strong>Residency</strong>:</p><p>During residency, I found myself very time and location constrained. I maintained a low cost of living and continued to donate to effective organizations. I still identified as an EA and this self-sacrifice (being in a high-stress environment that I had little control over) was for the greater good! For my altruism outlet, I did the most readily accessible intervention available,&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.akmcntckjewl\"><u>Trap, Neuter, Release</u></a> (TNR) for&nbsp;<a href=\"https://www.avma.org/resources-tools/avma-policies/free-roaming-abandoned-and-feral-cats\"><u>stray cats</u></a> in my neighborhood. I managed to help grow our group, Northwest Philly Cat Squad, from 5 to over 30 members and we spayed/neutered&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1C6FbEIMezKPdpB9U5lRw2cFs6zNthgdoBn7OAzBYnNI/edit?usp=sharing\"><u>hundreds of cats</u></a>. I saw that TNR mirrored the classic example of the&nbsp;<a href=\"https://www.ilr.cornell.edu/post/babies-river\"><u>parable</u></a> about the babies being thrown down the river. My fellow cat trappers and I all had limited time and resources so maximizing our efficiency was an intoxicating exercise. We would prioritize female cats in high concentration areas with cooperative colony feeders in partnership with efficient nonprofit sponsors and often discussed addressing systemic issues like bottlenecks in the rescue/adoption chain. Working on the ground while also co-directing the group\u2019s activities produced very tight feedback loops, which I came to appreciate even more as essential to maximizing one\u2019s impact.</p><p><i>As a thank you for getting this far, please enjoy all these pictures of cats I\u2019ve caught!</i></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/xocrufltgrvrwkweer9j\"><br>&nbsp;</p><p><strong>Rejoining the EA Community&nbsp;</strong>(Fall 2021)</p><p>In Fall 2021 after I graduated residency, I was figuring out how to start up my medical practice 20% of the time and trapping cats 80% of the time. It was then that I saw the invite to EAGxVirtual and figured I would go since it was a low barrier opportunity for me to re-integrate with the greater EA community. During the conference, I made many meaningful connections through 1:1 interactions. Afterwards, I felt very optimistic about the utility of rejoining the greater EA community. I knew I had to move on from the cats to finally start earning to give, so I made the difficult choice of gradually setting up boundaries for my time.</p><p>&nbsp;</p><p><strong>First Rejection from EAG&nbsp;</strong>(Winter 2021)&nbsp;</p><p>After EAGxVirtual, I started connecting with people in EA Philly and applied to EAG London. I had never heard of a conference that required an application before, so I naively assumed it was more of a formality and didn\u2019t put too much effort into it.&nbsp;<strong>I was rejected</strong>. One of my friends from EA Philly, who was an undergrad at UPenn at the time, told me that their application was accepted quite readily and expressed surprise that mine got rejected. They skipped happily away to London while I tried to brush the sting of it off.&nbsp;</p><p><br>&nbsp;</p><p><strong>Second Attempt at EAG</strong> (April 2022)</p><p>I applied again to EAG DC and didn\u2019t hear anything back for months and just figured I was rejected and they neglected to tell me. At this point, I had relocated to New Jersey to live with my parents because I\u2019m still in that frugal early EA mindset.</p><p><br>&nbsp;</p><p><strong>Entrepreneurship and Upskilling</strong>: (Winter 2021 - ongoing)</p><p>After my second application, my business started picking up steam. I had already established a generic name for my business and set up a versatile&nbsp;<a href=\"http://www.justmydoc.com/\"><u>website</u></a> so I could be in scout mode for different business models. I trialed through many different practices including doing independent COVID testing, working for other practices, contracting with hospitals, doing home musculoskeletal ultrasounds, etc. Through this process, I finally found a good model in medical cannabis, which was tractable, profitable, and prosocial. I then focused all my attention into&nbsp;<a href=\"https://kissflow.com/workflow/bpm/process-improvement-strategies-that-work/\"><u>process improvement</u></a> and upskilling in SEO, google reviews, marketing, customer service, managing employees, bookkeeping/accounting, legal compliance, etc. Owning and operating my own business led to a lot more reflection about process improvement since I was accountable for all aspects of it. During this time, I again didn\u2019t have the capacity to be an active part of the EA community and didn\u2019t directly engage much besides listening to the 80k or Clearer Thinking podcasts.</p><p>&nbsp;</p><h2><strong>My Post:</strong></h2><p><i>Purpose: To explain the situational context behind my original post.&nbsp;</i><br>&nbsp;</p><p><strong>Deciding to Make the Post&nbsp;</strong>(July 2022)</p><p>I was scrolling through facebook when I found an EA forum post about&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/MjTB4MvtedbLjgyja/leaning-into-ea-disillusionment\"><u>EA Disillusionment</u></a> on Ruair\u00ed Donnelly\u2019s facebook page. I had never read any EA forum posts before. Intrigued by this title, I read the post and felt like the content of it rang true to my own feelings. I proceeded to comment with my own story as a reply to the facebook post. I didn\u2019t think much more about it and was unaware my casual comment was later turned into an anonymous&nbsp;<a href=\"https://twitter.com/KerryLVaughan/status/1551670808552022016\"><u>\u201cEA Blackpill\u201d tweet</u></a> that was later widely shared and used in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/dsCTSCbfHWxmAr2ZT/open-ea-global\"><u>Scott Alexander\u2019s EA Forum post</u></a>.&nbsp;</p><figure class=\"image image_resized\" style=\"width:79.29%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/hpwbh7wxzgnao0gnipvt\"><figcaption><i>I wrote my post out of frustration after reading&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/MjTB4MvtedbLjgyja/leaning-into-ea-disillusionment\"><i><u>Leaning into EA Disillusionment</u></i></a><i>&nbsp;</i></figcaption></figure><p>&nbsp;</p><h2><strong>After my Post:</strong></h2><p><i>Purpose: To give context as to why I am writing this current forum post</i></p><p><strong>Conversation with Concerned CEA Staff Member&nbsp;</strong>(August 2022)</p><p>After seeing my comment on facebook, a concerned CEA staff member, who I will refer to as \u201cX\u201d going forward for privacy, reached out to me. On August 15th, we connected on video chat for over an hour and they gave me some helpful tips to re-engage with the EA community such as getting more involved with the nearby EA NYC group and amending/re-submitting my application to EAG DC. This conversation turned my attitude around enough to the point that I decided to re-engage in the community by going to my first EA event since moving away from Philly.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/xg3vdczymhjbgocddgr6\"><figcaption>&nbsp;<i>I was very appreciative that X took the time to reach out to me and help me understand why my initial applications to EAG were not successful.<strong> I &nbsp;kept this insider information in mind when later creating my 3rd application.</strong></i></figcaption></figure><p>&nbsp;</p><p><strong>My First EA NYC Event&nbsp;</strong>(August 18, 2022)</p><p>I went to an EA NYC event, which was a&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.x2xql8wiali\"><u>talk on Population Ethics</u></a> by my old friend Tim Campbell, who co-founded the Rutgers GWWC group with Nick. The event went amazingly well and I again felt a strong sense of community unified by purpose that I hadn\u2019t felt in a long time. I was so touched that I sent X quite a long appreciation voice message at 1am. A couple days later,&nbsp;<strong>I received&nbsp;</strong><a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.cv6sjhq01xu8\"><strong><u>rejection #2</u></strong></a>. I felt a little sad, but I had a consolation prize in finding the EA NYC community so that lessened the sting. From my conversation with X, I now understood exactly how competitive EAG was and realized I didn\u2019t apply enough effort into my applications up to this point.</p><p>&nbsp;</p><p><strong>Continued Involvement in EA NYC&nbsp;</strong>(Aug - September, 2022)</p><p>Over the next month, I continued to keep participating in the EA NYC community. I went to many of their official events as well as some adjacent social events. I felt quite accepted by the EA NYC community and no longer felt disillusioned.&nbsp;</p><figure class=\"image image_resized\" style=\"width:62.59%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/s4ivou11dg62fmj4xtgq\"><figcaption><i>&nbsp;I even appeared in a&nbsp;</i><a href=\"https://www.nytimes.com/2022/09/08/style/david-shor-democrats.html\"><i><u>NY Times article</u></i></a><i> that talked about EA!</i></figcaption></figure><p>&nbsp;</p><p><strong>Re-applying to EAG DC&nbsp;</strong>(August 22, 2022)</p><p>Finally feeling determined, financially equipped, and integrated enough with the EA community, I re-applied to EAG DC. I spent 2 hours working on a&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=kix.nnc9mfp9g9z3\"><u>stronger amended application</u></a>. I made sure to do my research and based my amended application on CEA\u2019s published&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.31q42uy2l1dj\"><u>EAG 2022 application information</u></a> and careful consideration of the insider&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.hrqgrk9xlzey\"><u>insight about the process I got from X</u></a>.&nbsp;<br>&nbsp;</p><p>I decided to pull out ALL THE STOPS because I really, really wanted to go and knew I had to meet a high bar.&nbsp;</p><hr><h2><strong>My Application to EAG DC:&nbsp;</strong></h2><p><strong>General Highlights</strong>:</p><ul><li><strong>Shameless name-dropping:</strong>&nbsp; I stated that&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.tx4sait6ess3\"><u>Nick Beckstead</u></a> was highly influential in my decision to pursue Earning to Give. I also mentioned that&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.qes8kytnh77n\"><u>Sam Bankman-Fried</u></a> did the statistical analysis for my cage-free egg campaign at Rutgers in 2013.&nbsp;</li></ul><p>A note: I didn\u2019t resort to any name-dropping in my first 2 applications to EAG, but I did for this 3rd one because I know that personal connections often have a hand in these sorts of decisions. I was fairly confident that, if asked directly, these two hard-hitters of EA would remember me and speak of my intentional work ethic positively.&nbsp;</p><ul><li><strong>Showcasing my Most Effective Intervention:</strong> I elaborated on the Rutgers&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.eamngqsq63sn\"><u>cage-free egg campaign</u></a>, which resulted in a victory whereby $250k USD was added to the dining hall budget each year, which at this point, has accumulated to over $2 million USD. I utilized my available social resources and thought analytically about why my predecessor\u2019s attempt at this campaign in the 2 years prior&nbsp;<a href=\"https://dailytargum.com/article/2013/10/egg-sclusive\"><u>had failed</u></a>. I even made the impact go further by getting plenty of&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.6dgw7xjik81\"><u>news coverage</u></a> and helping to make a&nbsp;<a href=\"https://ms-my.facebook.com/RutgersIFNH/videos/rutgers-university-dining-halls-claim-to-use-cage-free-eggs-but-do-these-claims-/534810266605093/\"><u>documentary</u></a> about it.</li></ul><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/po1jsvi9xz4eonmzdwiz\"></p><p>&nbsp;</p><ul><li><strong>Demonstrating Real-Life Success:&nbsp;</strong>I included that I started my own&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.4u5zkisk8ioc\"><u>medical practice</u></a> 9 months earlier and was already making&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.4u5zkisk8ioc\"><u>$60,000 USD per month with 10-20% growth</u></a> each month. I thought the timeline of starting up my business would showcase how effectively I utilized the scout mindset early on to identify a model that was tractable, scalable, and profitable. The profit and growth numbers were meant to be a reflection on my ability to optimize for process improvement. I even provided a link to my&nbsp;<a href=\"https://bit.ly/3BeOQjS\"><u>google reviews</u></a> to demonstrate that my business was also prosocial.&nbsp;<br>&nbsp;</li></ul><p><strong>Highlights According to CEA\u2019s Published Criteria for</strong>&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.31q42uy2l1dj\"><strong><u>EAG 2022 Admission</u></strong></a>:&nbsp;</p><p><strong><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/ngbhyudwnawilsnfhldj\"></strong></p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/w7vwy7qtkjpjxfdruab0\"></p><p>I stated I&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.rj1qbv4am434\"><u>became a physician</u></a> based on the EA dogma of \u201cEarning to Give\u201d (ETG) the time, which took 9 years of my life to see to completion. I didn\u2019t elaborate more because I figured that would be sufficient for this criteria.</p><p>&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/wo6urvpzweshvzjdjs1x\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/hb8lnalphviihstb2hr8 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/dslvh6lpuemte0s79kvk 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/ebwxopm8w1ua3jgdi9of 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/yoniu3httscrpjxlrwcm 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/kwz9faprmsrmnlnsq4lp 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/efswidtx80yowojo8yg4 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/mpazpashaxddiuip8wfz 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/tplq43tobgtuit30mzjc 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/lp1jcpbwkp6htokweo3x 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/nivf1jon9kivxaagdvpc 1152w\"></figure><p>While I didn\u2019t volunteer for any&nbsp;<i>sanctioned</i> (funded) EA projects, I did give&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.1rl7afwred0h\"><u>multiple examples</u></a> of starting organizations and leading interventions which were highly effective. I think it can be understood that my time was unpaid for these and thus I was a volunteer.</p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/zycivowkwy0rghgm6pxa\"></p><p>I said that I have so far&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.4u5zkisk8ioc\"><u>paid myself $0</u></a> from my business and planned to focus my resources to execute effective interventions aimed at reducing animal suffering. I gave a couple&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.5tly3tjw0sxv\"><u>concrete examples</u></a> of interventions I was considering and remained open to others by mentioning I was in scout mode for cost-effective opportunities and underutilized talent. I mentioned that I was no longer on the ETG track because I believed more in my own ability to effect change. (I didn\u2019t include another important reason I had for this, but you can read about that&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.64oa9jacdzip\"><u>here</u></a>.)<br>&nbsp;</p><p><strong><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/sa1byxetdb3y2hvcpsn5\"></strong></p><p><strong>Providing more detail about my actions and the ways in which ideas in EA influenced my decisions</strong>: My entire application took 2 hours to complete and was incredibly detailed. Keywords that I mentioned throughout my application when describing different actions included scout-mode, cost-effectiveness, tight feedback loops, taking advantage of neglected interventions, leaning into my most valuable assets, reducing future animal suffering, and longtermist frameworks etc.&nbsp;</p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/k1sorhiff6wubhxxwpto\"></p><p><strong>Job Opportunities:</strong> I stated that I wanted to transition my business to a form of passive income/funding so that I could focus on more direct EA work through&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.yapqmiky67sd\"><u>hiring 1-2 people</u></a> for effective interventions that I would fund. I figured I would be adding value by creating more opportunities for the community rather than taking them away.</p><p><strong>Free Medical Advice:</strong> I am a&nbsp;<a href=\"https://www.aapmr.org/about-physiatry/about-physical-medicine-rehabilitation\"><u>physical medicine and rehab</u></a> physician, which is a broadly applicable and useful speciality. In my application, I stated that I could offer useful and free&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.eyjol5rlbrcn\"><u>medical advice</u></a> and&nbsp;<a href=\"https://www.justmydoc.com/ultrasound\"><u>ultrasound scanning</u></a> for other EA\u2019s during the conference. I want to help reduce chronic discomfort/pain for other EA\u2019s so they could go on to be more effective in achieving their altruism goals. Plus, I also really enjoy being of service to others and find it particularly useful in making connections.&nbsp;</p><p><strong>Connection Facilitation:</strong> I included a link to my&nbsp;<a href=\"https://goo.gl/maps/Zwbbr7KTPsrSnHmf9\"><u>google reviews</u></a> to demonstrate \u201cmy general likeability and professionalism.\u201d I figured I could utilize my prosocial extroversion and affinity for group dynamics to increase the number of connections made at EAG,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/g5dbu6yJJzW3m39un/the-best-ea-global-yet-and-other-updates\"><u>one of the metrics</u></a> that I found that CEA uses to measure the success of their conferences.&nbsp;</p><p>&nbsp;</p><p><strong>Finding Out About Rejection #3&nbsp;</strong>(September 12, 2022)</p><p>One of my well-connected friends from EA NYC offered to give a personal recommendation for me directly to the admissions team. I was overjoyed to have a community endorsement to add to my \u201cEA Resume.\u201d However, the response my friend received from the admission lead was that my application would likely be rejected again. No explanation was provided so my mind suddenly went to the worst possible scenario&nbsp;</p><p>&nbsp;</p><p><i>\u201cOMG! It\u2019s because my main cause area is reducing animal suffering and EA doesn\u2019t care about that anymore!\u201d</i><br>&nbsp;</p><p>\u2026.It\u2019s easy to let insecurities take over the narrative when there is no explanation to fill in the unknown gap. This horrifying thought was already one of the main bases of my disillusionment when I made my&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.8xaerj38yskt\"><u>original post</u></a>.&nbsp;</p><hr><h2><strong>Correspondences with CEA</strong></h2><p><i>The following include my personal thoughts about CEA\u2019s communications. They are discussed further in the \u201cHypotheses for Rejection\u201d and \u201cCriticisms/Solutions\u201d sections.&nbsp;</i></p><p>&nbsp;</p><p><strong>I &nbsp;messaged X to give them an update about rejection #3</strong>:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/wxco6tqcmbpkxodqwjhf\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/n4q3iyfsam6yjpq1axlg 310w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/qhy5nctoh30rpfy2qpid 620w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/s5wm4ljlj2iek5iood0d 930w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/iwbpxdnu1jiqgtwihvof 1240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/ebqc7ymdu5x6eueprtfy 1550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/sc3ajw83jrttcriqvd8d 1860w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/rptalzjlqs9dqljihi2b 2170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/zr2yjcbjluhwffpuhz6g 2480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/edwsaormducfuoxm1jts 2790w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/pfl9p4p7msievp5y19uj 3032w\"><figcaption>&nbsp;</figcaption></figure><p><br><strong>Soon afterwards, I received this email:</strong></p><p><i><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/zrbvhkifhuyotk52pgnq\"></i></p><p>&nbsp;</p><p><strong>I replied to the email containing rejection #3</strong>:<strong>&nbsp;</strong></p><p><i><strong><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/hnz0ki5qfkxt7kcxykl2\"></strong></i><br>&nbsp;</p><p><strong>I received a reply which really rubbed me the wrong way</strong>:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/dr0p0znbxdwypirwfovr\"></p><p><br><strong>I replied with mildly sassy response when it was confirmed there would be no feedback</strong>:</p><p><i><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/cthghrvpdrzdyclzyl8s\"></i></p><p><br>It was shortly after this that I found out about the Scott Alexander post, which was an interesting story, but unecessary for this post. Interested folks can&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.5zaehnx82ohd\"><u>read about</u></a> if they want.&nbsp;&nbsp;</p><hr><h2><strong>Personal Hypotheses for Rejection:&nbsp;&nbsp;</strong></h2><p><i>These are meant to be examples of&nbsp;<strong><u>bad epistemics</u></strong></i> (I couldn\u2019t fact-check)</p><p>After getting comments from reviewers for this post, I did see many&nbsp;<i>potential red flags</i> in my application that I didn\u2019t initially consider. I have included them in this list.&nbsp;</p><p>&nbsp;</p><p><strong>Perhaps the admissions team</strong>\u2026</p><ol><li>is a part of the systematic deprioritization animal issues in EA&nbsp;<i>(a larger and more complicated concern of mine that is better suited to be discussed in another post)</i></li><li>was not looking for early career Earning to Give folks for applications #1 + #2 and then labeled as a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/dsCTSCbfHWxmAr2ZT/open-ea-global\"><u>troublemaker</u></a> for #3 after finding out I was the one in Scott Alexander\u2019s forum post</li><li>thought I may not be earnest in my application since I had talked to X and got some&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.hrqgrk9xlzey\"><u>insider insight</u></a> about the screening process</li><li>thought I sounded too passionate about animal suffering and that I was approaching with a&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.k7h6lxx82qdb\"><u>soldier mindset</u></a> rather than a scout one</li><li>thought my&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.5tly3tjw0sxv\"><u>proposed interventions</u></a> were not sufficiently traditional, ethical, high-priority, or well-researched enough</li><li>was worried that I would compete with well-vetted EA organizations for&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.yapqmiky67sd\"><u>talent</u></a></li><li>thought I sounded overconfident about&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.1rl7afwred0h\"><u>my achievements</u></a></li><li>didn't like that I mentioned finding a&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.rz4e7naczpsv\"><u>romantic partner</u></a> in EA as one of the things that I imagined would have happened if the event had \u201cgone exceptionally well\u201d</li><li>didn\u2019t think I sounded professional&nbsp;</li><li>thought my offer of&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.eyjol5rlbrcn\"><u>medical advice and ultrasound scanning</u></a> was more of a liability to the conference than it would be a benefit to other attendees</li><li>Factor X - Something I haven\u2019t considered yet</li></ol><p>&nbsp;</p><p>I spent many hours thinking in circles through these various hypotheses\u2026</p><p>I think that everything in the list (with the exception of the first 3) could have been resolved by redirecting me to resources to better understand the expected norms of the conference so I could update my priors. One of my priors when I wrote my amended application was that EA\u2019s appreciate things that are weird and honest. I had openly done or talked about all of the things in my application around other EA\u2019s without having received any significant feedback that they were inappropriate. For this reason, I found them less compelling as a basis for rejection unless the reviewer made the assumption that I wasn\u2019t someone who could be redirected. I hope that wasn\u2019t the case because I also said,</p><blockquote><p><i>&nbsp;\u201cI have been in&nbsp;<strong><u>scout mode</u></strong> to identify both opportunities and underutilized talent. I have&nbsp;<strong><u>many ideas</u></strong> for projects that take advantage of neglected interventions for reducing animal suffering and&nbsp;<strong><u>need to find people</u></strong> with similar enthusiasm and availability&nbsp;<strong><u>to help develop them</u></strong>.\u201d</i></p></blockquote><p>I thought this statement would be sufficient to communicate that my ideas were still in early stages and that, while I was passionate about my end goal, I was still open to changing my approach and thought EAG could help me meet people who could challenge my epistemics and improve my process. I did use the term \u201c<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.5tly3tjw0sxv\"><u>future projects I am planning</u></a>\u201d later on so I could see how that might give the impression to the reviewer that I was rigidly set on doing all of those interventions (I\u2019ve already given up on one of them due to new information).&nbsp;</p><p><br>I also initially considered capacity limits as a possible reason, but later came to find out that they are&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.2wrmbfnc6558\"><u>not a factor</u></a> in admissions.&nbsp;&nbsp;</p><p>&nbsp;</p><p><strong>TLDR;</strong> When there is a gaping hole of nothingness, the brain has the uncanny ability to fill it with all sorts of possible hypothetical explanations. Mixing in strong emotions and insecurity (often the case with rejections) places people at even higher risk for cognitive distortion and reactionary behavior. I was already worried that non-human animals would not have a place at the new longtermist table in the EA community. That is why&nbsp; my mind immediately grabbed onto that when reaching for an explanation.&nbsp;</p><p><i>As much as we aspire to be perfectly rational, we are still human in the end.</i></p><hr><h1><strong>Criticisms and Proposed Solutions&nbsp;</strong></h1><p><i>Because I heard&nbsp;</i><a href=\"https://astralcodexten.substack.com/p/criticism-of-criticism-of-criticism\"><i><u>EAs love criticism</u></i></a><i>!&nbsp;</i></p><p>&nbsp;</p><h3><strong>#1 CEA Admissions Decisions are Easily Confused for Worthiness</strong>:</h3><p>All my rejection hypotheses boiled down to the CEA\u2019s admissions judgment on whether I was good enough of an EA to get into EAG. In the admissions lead\u2019s&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.8monmav4kkrt\"><u>own words</u></a>, \u201c<strong>We simply have a specific bar for admissions and everyone above that bar gets admitted.</strong>\u201d Therefore, it is implied that if you didn\u2019t get in, then you are below that bar. This is especially demoralizing because there is then the tendency to start comparing yourself to accepted applicants.</p><p>&nbsp;</p><p><strong>Proposed Solution:</strong> Not sure</p><p>This is a difficult problem to solve because it involves each applicant's personal sense of identity. This forum post is my small contribution to addressing this problem. There seems to be a taboo about people sharing personal stories of rejection. Some possible explanations could be feelings of shame or risk of personal reputation. Hopefully as you read on in the criticisms, you will realize how much room for improvement there is in the admissions system for EAG. It\u2019s relatively easy to mistake their control of conference admissions with an authoritative judgment of how fit you are to be in the greater EA community, but...&nbsp;<i>it definitely is not!</i>!</p><figure class=\"image image_resized\" style=\"width:65.18%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/s74tvhi2ktk1t9nwfqgq\"><figcaption><i>But by whose standards?</i></figcaption></figure><p>&nbsp;</p><h3><strong>#2 Coming Up With Better Explanations Isn\u2019t Solving the Problem</strong>:</h3><p>I see evidence of many different iterations of CEA communications (<a href=\"https://www.eaglobal.org/admissions/\"><u>FAQ</u></a>,&nbsp;<a href=\"https://docs.google.com/document/d/1vlOaF-f2585hmyi4Chs7vU0MMJ9xcz1jigzsb2-UCP8/edit\"><u>Google Doc</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xkirfbvbafqaerYGg\"><u>Forum Post</u></a>,&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.cv6sjhq01xu8\"><u>Email</u></a>, and most recently&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.96tou1pypdfr\"><u>comments</u></a> in the EA Forum) that contain various messages that try to broadly address the need applicants have for feedback and clarity. But are these needs being met? It seems like many people have serious questions about their application decisions and are being denied feedback. A common explanation for this is that staff do not have the capacity to handle all the feedback demanded. As I discuss later on, much of this problem could be addressed with automated systems and a good process improvement cycle.&nbsp;</p><p>&nbsp;</p><p><strong>Proposed Solution:&nbsp;</strong>Instead of coming up with better explanations, consider a different strategy to address the emotional and epistemic needs of rejected applicants. I will address these in criticisms 3-5.&nbsp;</p><figure class=\"image image_resized\" style=\"width:85.14%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/l0p3nexqybqquhoy0hpl\"><figcaption><i>Inconsistent messaging makes it hard to tell the difference between an explanation vs an excuse. Of note,&nbsp;</i><a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.2wrmbfnc6558\"><i><u>this error</u></i></a><i> has reportedly been fixed.&nbsp;</i></figcaption></figure><p>&nbsp;</p><h3><strong>#3 Not Taking Advantages of Opportunities for Feedback</strong>:</h3><p><strong>Feedback\u2026&nbsp;</strong><a href=\"https://www.youtube.com/watch?v=kAqIJZeeXEc\"><strong><u>It\u2019s what EA\u2019s Crave</u></strong></a><strong>!&nbsp;</strong>When people do not have a clear and actionable reason for their rejection, their minds can spiral and often come to the conclusion that either they are unworthy, which poses a risk for declining individual mental and community health.&nbsp;People who apply to EAG are already self-selecting as highly motivated members of the community and this energy could be channeled into a growth mindset to create something productive. Seems like giving feedback to EAG applicants is something that could be an opportunity to direct them in a more effective direction. Perhaps a volunteer could even do this after an automated system is set up.</p><p>&nbsp;</p><p><strong>Proposed solution:&nbsp;</strong>Create an automated system for feedback and optimize for applicant satisfaction and engagement.</p><p>Categorize rejected applicants according to concerns about their application that could be improved upon and create a triage system to provide actionable feedback. Once some templates and a system is set up, I imagine it can be pretty efficient. Here is one way it could be done - the admissions team labels each rejected application in their inbox with colored flags corresponding to different reasons for rejection and then a remote assistant can just fire off the pre-templated emails. There is also room here for more targeted triage using spreadsheets and customizable templates according to characteristics of applicants such as location, earning potential, cause area, stage of career, etc. An added bonus is that all this work can just become data points to feed into your process improvement cycle which I describe in #4.</p><p><strong>Here are some simple examples</strong>:<strong>&nbsp;</strong></p><ul><li>Insufficient Understanding of EA Principles \u2192 Read the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/cN9Zu7dowefAfmNnH/the-effective-altruism-handbook\"><u>EA Handbook</u></a>&nbsp;</li><li>Work is Not Impactful Enough \u2192 Read the&nbsp;<a href=\"https://80000hours.org/key-ideas/\"><u>80k Career Guide</u></a></li><li>Not Involved Enough in EA Communities \u2192 Join a local&nbsp;<a href=\"https://eahub.org/\"><u>EA Hub</u></a> or attend an&nbsp;<a href=\"https://www.eaglobal.org/eagxhome/\"><u>EAGx</u></a></li></ul><p>&nbsp;</p><ul><li>All of these links can be fed through a google analytics tracker so you can get really valuable data points for which links tend to get opened and how long someone spends engaging with those resources. Then you can even do A:B testing to optimize the messaging. Including surveys in these emails can provide a valuable feedback mechanism for process improvement!&nbsp;</li><li>There is so much opportunity to redirect the energy of rejected applicants into something positive.</li></ul><p>&nbsp;</p><figure class=\"image image_resized\" style=\"width:57.91%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/rcucxs7pufgdwjdlyipi\"><figcaption><i>I need it to update my priors!</i></figcaption></figure><p>&nbsp;</p><h3><strong>#4 Poor Process Improvement Mechanisms</strong>:</h3><p>The best examples of industries that have gained success through&nbsp;<a href=\"https://qi.elft.nhs.uk/resource/the-model-for-improvement/\"><u>process improvement</u></a> are the&nbsp;<a href=\"https://corporate.ford.com/articles/history/moving-assembly-line.html\"><u>car industry</u></a>, the&nbsp;<a href=\"https://www.faa.gov/newsroom/out-front-airline-safety-two-decades-continuous-evolution\"><u>airline industry</u></a>, and more recently, the&nbsp;<a href=\"https://www.youtube.com/watch?v=jq52ZjMzqyI\"><u>healthcare industry</u></a>. It seems natural that EA would implement these models as well. The admissions team states that they have thought carefully about the question of \u201c<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.4yeuui1nqedb\"><u>how costly is </u></a><a href=\"https://forum.effectivealtruism.org/posts/dsCTSCbfHWxmAr2ZT/open-ea-global?commentId=dhrGoFmXPpET58n4f\"><u>rejection</u></a>\u201d and determined that there is a \u201crelatively small discouragement effect\u201d because rejection did not have a great effect on the likelihood of applicants to apply/reapply (a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Proxy_(statistics)\"><u>proxy outcome measure</u></a> for discouragement) for future conferences. This is the only example I could find.&nbsp;</p><p>If CEA has other measures they use to assess how costly rejection is, then it would be beneficial to building community trust if they published them. For now, I will explain why I think this choice for a proxy outcome measure is likely a poor correlate for actual discouragement.&nbsp;</p><p>As I said in my&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.wid6fwtaqhpn\"><u>correspondence</u></a> with X, it is so easy to autofill the applications with prior answers again and again. Rejected applicants can still be measured as being \u201cnot discouraged\u201d every time they apply even if they are feeling progressively more discouraged or disillusioned with each rejection. There are a&nbsp; lot of other costs to rejection that cannot be captured with this statistic too. Examples of other outcomes to consider include aspects of mental health, individual productivity, and community participation/epistemological integrity/diversity.</p><p>Ultimately, the \"cost of rejection\" is just one factor to calculate when considering the <a href=\"https://forum.effectivealtruism.org/posts/dsCTSCbfHWxmAr2ZT/open-ea-global?commentId=cRSPmzcyXWNhWGz46\">overall impact</a> of the admissions process.&nbsp;</p><blockquote><p>\"EAG exists to make the world a better place, rather than serve the EA community or make EAs happy. This unfortunately sometimes means EAs will be sad due to decisions we\u2019ve made \u2014 though if this results in the world being a worse place overall, then we\u2019ve clearly made a mistake.\"</p></blockquote><p>It is really hard when an applicant, who wants to contribute to the greater good and believes that EAG will serve them well in that goal, gets rejected and is told that the reason is that the admissions team does not think their attendance would have made the world a better place. There seems to be something intuitively wrong with this, but it's hard to know what that is because of how opaque the admissions process is, which I discuss in #6.</p><p>&nbsp;</p><p><strong>Proposed Solution:</strong> Continue to update the process improvement model</p><p>It\u2019s really hard to have a good process improvement model for \u201ccost of rejection\u201d without directly surveying the rejected applicants. Complaints are a gift! A humble place to start is with good qualitative feedback surveys - don't make rejected applicants resort to writing a whole forum post on their frustrations.&nbsp;</p><p><strong>Here are some suggestions for qualitative question</strong>s:</p><ul><li>Why do you think you got rejected?</li><li>How did it the rejection impact you?</li><li>If you disagree with our decision, please tell us why.</li><li>How has this rejection affected your EA plans (career, study, community, conferences, etc) going forward?</li></ul><p>Hone in on what people complain about the most and then develop quantitative ways to measure that.&nbsp;</p><p><strong>Here are some suggestions for quantitative (0-10) question</strong>s:</p><ul><li>Sense of belonging in the EA community</li><li>Dedication to EA principles</li><li>Likelihood to pursue a career directly or indirected related to EA</li><li>Sadness/Anxiety</li></ul><p>Create a list of possible interventions to optimize for these desired outcomes, pick the best one, implement it, and measure it. Use that information to refine your model and repeat the process. As anyone who has gone through a process improvement model can attest to, it is an overwhelming task to find the best way to go about measuring these things. It's much easier to use a readily available statistic like \"likelihood of applicants to apply/reapply.\" EA is about doing the most good though so we need to put in the work to measuring costs and benefits accurates.&nbsp;</p><p>&nbsp;</p><figure class=\"image image_resized\" style=\"width:77.82%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/z9uu72ywmfv7urvxkous\"><figcaption><i>Implementing rigorous process improvement cycles can solve most problems</i></figcaption></figure><p>&nbsp;</p><h3><strong>#5 Poor Quality of Communication with Rejected Applicants</strong>:</h3><p>Good communication is a skill. It takes concentrated effort and lots of feedback to develop, but once you get good at it, it doesn\u2019t take much more time or effort to word things more considerately. There were many points in my&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.cv6sjhq01xu8\"><u>correspondence</u></a> with X and the Admissions Team regarding my rejected application that could have been handled better. I wrote some&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.cv6sjhq01xu8\"><u>specific comments</u></a> regarding opportunities for improvement in wording choice. However, I think the most glaring example of poorly considered/coordinated communication can be seen in the contrast between In the admissions lead\u2019s comments about having a specific bar for admissions and \u201c<strong>everyone above that bar [getting] admitted</strong>\u201d and X saying&nbsp;<a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.nke64aqtt3c2\"><strong><u>\u201dplease don\u2019t take our judgment personally</u></strong></a>.\u201d&nbsp;</p><p>&nbsp;</p><p><strong>Proposed Solution #1-</strong><i> Include plausible reasons for the rejection decision in the default email</i></p><p>One simple way to do this could be to include a link to a list of plausible and sensitively-written explanations for an EAG rejection such as&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xkirfbvbafqaerYGg\"><u>Julia Wise\u2019s 2019 EA forum post</u></a>. Rejected applicants could substitute these explanations in place of potentially more detrimental explanations that could be based in insecurity. However this leads down the rabbit hole to criticism #2 (better explanations don\u2019t solve the underlying problem). Also,&nbsp;<strong>the plausibility of any alternate explanations would now be dubious given the explicit \u201c</strong><a href=\"https://docs.google.com/document/d/1dmkk_hVeotZiZyizkacc2CTvL0k5zby6iQrdhl7kLA4/edit#bookmark=id.8monmav4kkrt\"><strong><u>pass the bar</u></strong></a><strong>\u201d criteria</strong>.&nbsp;<br>&nbsp;</p><p><strong>Proposed Solution #2-</strong><i><strong>&nbsp;</strong>Equip staff with the tools to better relate to rejected applicants</i></p><p>CEA staff involved in communicating about rejections can be trained on&nbsp;<a href=\"https://www.youtube.com/watch?v=HwKoDWkGOnA\"><u>authentic relating</u></a> or&nbsp;<a href=\"https://www.youtube.com/watch?v=INdKgBPEI-8\"><u>nonviolent communication</u></a> methods. These are incredibly impactful communication methods that place an emphasis on listening and considering the emotional needs of others before responding. When people feel understood, they are much more likely to extend the same courtesy back and then communicating becomes easier and more productive.&nbsp;</p><figure class=\"image image_resized\" style=\"width:72.84%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/soi1t6wuehqchaji1uhp\"><figcaption><i>\u201cGood communication is the bridge between confusion and clarity\u201d-Nat Turner</i></figcaption></figure><p>&nbsp;</p><h3><strong>#6 Too Opaque</strong>:</h3><p>In general, there seems to be a lot of power concentrated in CEA and when you mix that with an opaque selection process, this tends to engender distrust. This is unfavorable to building a unified movement. As I discussed earlier in this post, there are very few metrics that CEA makes publicly available that help with understanding the reasoning for their admissions process. I believe there are many more that can be published without giving away the \"teacher's password.\"&nbsp;<br>&nbsp;</p><p><strong>Proposed Solution:&nbsp;</strong>Make it more transparent!</p><ul><li>Publish the existing rejection rates for each conference so people can set their expectations accurately when applying</li><li>Publish&nbsp;that <a href=\"https://forum.effectivealtruism.org/posts/dsCTSCbfHWxmAr2ZT/open-ea-global?commentId=dhrGoFmXPpET58n4f\"><u>internal report</u></a> about \"how costly rejection\" is so people can see the metrics</li><li>Create surveys (described in #4) to assess the downstream effects of the admissions process. Publish the results.</li><li>Have a system for 3rd parties to perform randomized audits of application decisions.</li></ul><figure class=\"image image_resized\" style=\"width:73.6%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/g9zwhokqv8zpczqcpqfg\"><figcaption>Please give us a just a little more!</figcaption></figure><p>&nbsp;</p><h3><strong>#7 Perception of&nbsp;</strong><a href=\"https://www.urbandictionary.com/define.php?term=Gatekeeping\"><strong><u>Gatekeeping</u></strong></a><strong> by a Small, Elite Group</strong>:</h3><p>Even if the CEA admissions team states they are going by some thoughtful, well-researched algorithm, there is still the problem of the&nbsp;<i><u>perception</u></i> of gatekeeping. It seems natural for people to feel that way when a small committee, which can easily be subject to&nbsp;<a href=\"https://asana.com/resources/unconscious-bias-examples\"><u>personal bias</u></a>, has the power to deny access to a highly anticipated event.&nbsp;</p><p><strong>Proposed Solutions</strong>:<strong>&nbsp;</strong></p><ul><li>Start a process improvement cycle for community trust</li><li><a href=\"https://www.pinpointhq.com/insights/blind-recruitment/#hiring-bias-why-personal-characteristics-affect-hiring-decisions\"><u>Blind the admission process</u></a> (at least partially) to minimize unconscious bias&nbsp;</li><li>Increase transparency (discussed below)</li><li>Call the conference something that sounds less broad than EA Global</li><li><strong>Suggestions include</strong>:&nbsp;<ul><li>CEA Global</li><li>CEA International</li><li>EA Focused</li></ul></li></ul><figure class=\"image image_resized\" style=\"width:73.14%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/k5wbgpujz8iahoyeklto\"><figcaption><i>There will always be people outside the gate that wish to get in</i></figcaption></figure><hr><h1><strong>Closing Thoughts</strong></h1><p>In the end, I still have the feeling that I did in my original post that I don\u2019t really fit into this newer brand of EA\u2026 and CEA doesn\u2019t really recognize me as \u201cEAG\u201d worthy. Nonetheless, I can still personally identify as an EA doing impactful work and feel like I fit into parts of the EA community.&nbsp;</p><p>I have accepted that I am unlikely to ever get a satisfactory answer to support that the current admission process serves to optimize for the greatest good. It's really hard to accept any claims about this without seeing all the proxy metrics used for it, including how they are measuring \"cost of rejection\" without sending out any surveys to the rejected applicants.</p><p>Working on this post, telling my story, and hearing others along the way has been a catharsis. I recognize I\u2019m in a particularly privileged position. I live near a large EA hub, am financially well-resourced, and am capable of creating my own career opportunities. Others I have talked to have not had these advantages. They rely on EA grants for their projects or EA organizations for obtaining jobs and therefore may be more hesitant to directly and publicly criticize authoritative organizations like CEA.</p><p>I would like to invite anyone who has a story to tell to share it in the comments, privately message me, or just jump onto&nbsp;<a href=\"https://app.gather.town/app/Yhi4XYj0zFNWuUNv/EA%20coworking%20and%20lounge\"><u>EA Gather Town</u></a> anytime to talk to another EA about it. It is a wholesome&nbsp;<a href=\"https://forum.effectivealtruism.org/groups/MCtKD7oex9jhsAWvD\"><u>EA community</u></a>&nbsp;where anyone is welcome and there is no application. The people there have given me good company and conversation as I wrote this post. I had many interesting conversations such as how essential&nbsp;<a href=\"https://veganhealth.org/omega-3s/omega-3s-part-2/?fbclid=IwAR1nU5O85EXRAUbFqu6WbjrBEBtJ4f8_HNzn_l5T6CGWZfo7C3GB9M3C7AI#Evolutionary-Arguments-for-a-Dietary-Requirement-for-DHA\"><u>omega-3\u2019s</u></a> are (not very), the value of&nbsp;<a href=\"https://www.youtube.com/watch?v=ZXnTA9irjo0\"><u>street epistemology</u></a>, and the complicated&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1oL25vc5Feg94flPPCO03ujijRvyHffxt0qnI7rU8rdg/edit?fbclid=IwAR0JCtJxcj0yCuGbmENj-4uD1-pdypdDl0MDk1arfyzOdZe1PmlPXwZSnKc#gid=0\"><u>dating lives of EA\u2019s</u></a>.&nbsp;</p><p>Let\u2019s create a warmer community and be present for one another. There are many out there that have the capacity to listen and appreciate your unique talents. EA is a huge global community with different niches. Don\u2019t leave just because one part of the community couldn\u2019t/wouldn\u2019t accept you.&nbsp;<br>&nbsp;</p><p>Altruists, we are all in this together for the greater good!<br>&nbsp;</p><p><i>I\u2019d like to extend my deepest gratitude to the reviewers for this post:&nbsp;</i></p><p><i>Scott Alexander, Aaron Gertler, Cornelis Dirk, Jonathan Yan, Annabel Luketic, and Emrik Garden</i><br><br><br><i><strong>Update May 30, 2023:</strong></i></p><p>Since this post was made, I've heard from a lot of animal EA folks that they have felt similarly marginalized or looked down upon for having a \"lower priority\" cause area. (though no one says it explicitly... it's all implicit and reflected in ~vibes~ and noticing where the money and attention goes). So I've been on a mission to bring more EA into animal advocacy and more animal advocacy into EA. I'll be at <a href=\"https://www.effectivealtruism.org/ea-global/events/eagxnyc\">EAGxNYC 2023</a> (Aug 18 - 20) so if you are there, come say hi!&nbsp;<br><br>I also did go to the Animal Vegan Advocacy Summit of 2022 where I was a <a href=\"https://www.youtube.com/watch?v=hmGItvNoVjY\">speaker</a> and organized a subgroup meetup for EA's:</p><figure class=\"image image_resized\" style=\"width:86.57%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/jbkctxwfcbf1kdxzv7le\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/tzotoffq4gpxnwy1nuoc 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/n8rpti9inits5lhwopqm 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/jegej9rosj2n2asrha1k 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/xjhmzfgktsmd80zlfsln 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/ityshbot0vv9iivl5rfj 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/wkgcnbi8b8pol1wrrgbm 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/t7szkmdszwbxtybedjx2 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/tkpvbfwn8xgxqwagmzh5 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/ue1jfq7uqihsharmxh7w 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hur3ejqvA2QmYEfZa/f6hgkhwquswplghcf1rp 2048w\"><figcaption>The EA subgroup meetup had a huge attendance of almost 30 people!<br>We broke out into discussion groups for community building, longtermism, theory of change, and newcomers.</figcaption></figure><p><br>The AVA Summit is a fantastic event for those interested in getting actively involved with animal advocacy. I highly encourage EA's interested in animal welfare to go. There is a huge international presence and many newer, smaller organizations connected with their funders there. This year, I'll be facilitating a workshop on leveraging AI to accelerate progress for animals. I want to see more animal advocates working <strong>smarter, not harder</strong>.&nbsp;<br><br>My vision is that animal advocates become <a href=\"https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism\">cyborgs</a>.&nbsp;<br><br>And if you want to get more involved with animal welfare <i>right now</i>, please join the <a href=\"https://join.slack.com/t/impactfulanimal/shared_invite/zt-1t7edbzra-Zw5vqJ1boOeR4SyKkMf9gg\">Impactful Animal Advocacy</a> Slack group where dedicated advocates are collaborating every day.&nbsp;</p><p>With lots of love,</p><p>Constance<br>&nbsp;</p><hr>", "user": {"username": "Constance Li"}}, {"_id": "dcBfdecnm5gzntshZ", "title": "Author Rutger Bregman about effective altruism and philanthropy", "postedAt": "2022-09-20T06:00:46.422Z", "htmlBody": "<p><a href=\"https://en.wikipedia.org/wiki/Rutger_Bregman\">Rutger Bregman</a>, historian, and author (including <a href=\"https://en.wikipedia.org/wiki/Utopia_for_Realists\">Utopia for Realists</a> and <a href=\"https://en.wikipedia.org/wiki/Humankind:_A_Hopeful_History\">Humankind: A Hopeful History</a>) describes his personal view on philanthropy in a conversation with Effektiv Spenden <a href=\"https://www.effektiv-spenden.org/blog/rutger-bregman-on-effective-giving/\">here</a> (German <a href=\"https://www.effektiv-spenden.org/blog/interview-mit-rutger-bregman-zu-effektivem-spenden/\">here</a>).</p><p>In the <a href=\"https://forum.effectivealtruism.org/posts/38GG44y2hYLovADeK/effektiv-spenden-review-of-the-year-2021\">Effektiv Spenden</a> post donation survey he was mentioned more than any other person (e.g. more than Will MacAskill and more than Peter Singer). Our explanation is that he particularly good at reaching people from outside the existing EA community.</p><p>Some quotes:</p><h3><strong>On effective altruism:</strong></h3><blockquote><p>\"<i>Sometimes people can get the impression that \u201cOh, so you know what all the effective causes out there are and you are very dogmatic about that?\u201d That\u2019s not the case at all. Effective altruism is a question. It\u2019s not an answer. It\u2019s all about continuously asking yourself the question, is this the best use of my time, resources, and money? That\u2019s what it\u2019s really about. And I think intellectual humility is a really important value, and I think that\u2019s also quite present in the movement.</i>\"</p></blockquote><h3><strong>On systemic change vs. individual change:</strong></h3><blockquote><p><i>\"There\u2019s now this discussion going on amongst progressives and people on the left like: \u201cOh, we shouldn\u2019t talk about individual change because that\u2019s neo-liberal. We should all talk about system change\u201d, but obviously we need to do both. If you look at the most impressive reformers and prophets and campaigners and activists throughout history, they all did it both. I\u2019m now reading a book about</i> <a href=\"https://en.wikipedia.org/wiki/Anthony_Benezet\"><i>Anthony Benezet</i></a> <i>who was one of the most important abolitionists, he\u2019s called the father of abolitionism. He led the fight against the slave trade and slavery in the 18th century. If you would have said to him: \u201cOh, it\u2019s all about the system. It\u2019s not about the individual\u201d. He would have said: \u201cYou\u2019re a hypocrite.\u201d Of course, it\u2019s also about the individual, because he knew that he would be much more convincing if he actually did what he preached.\"</i></p></blockquote><h3><strong>On why he signed the Giving What We Can Pledge:</strong></h3><blockquote><p><i>\"Because human behavior is contagious. We\u2019re not individuals, we\u2019re not lone atoms, but we influence each other all the time by our behavior. It\u2019s just contagious. Giving can be like that as well. That\u2019s why I think it\u2019s important to be public about your giving, not to show off, you need to be a little bit careful there, but that\u2019s also why I signed the</i> <a href=\"https://www.givingwhatwecan.org/pledge\"><i>Giving What We Can pledge</i></a> <i>to say. Look, people, if you like my work, this is what I find really important and it has made a big difference in my life to donate at least 10% of my income to highly effective causes. I think that actually, as a best seller author, you can go a little bit higher than 10%, but 10% is a good place to start.</i>\"</p></blockquote><p>Since the interview is quite long feel free to share the video below with everyone who might be interested but can't be bothered for more than one minute.</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href=\"https://www.youtube.com/shorts/4HPxtdMACs8\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/018695c321cd9c3d7a458958838763166863d4eb6a9eea0c.jpeg\"></a></p>", "user": {"username": "EA-Basti"}}, {"_id": "ivjgAZRM7No4fnDgo", "title": "CERI Research Symposium Presentations (incl. Youtube links)", "postedAt": "2022-09-24T03:45:04.609Z", "htmlBody": "<h1>Introduction</h1><p>The&nbsp;<a href=\"https://www.cerifellowship.org/\"><u>Cambridge Existential Risks Initiative (CERI) summer research fellowship (SRF)</u></a> is a 10-week research training programme for aspiring x-risk researchers held in Cambridge, UK. CERI SRF \u201822 concluded with a research symposium on September 5th, and the presentations from that symposium are the subject of this post.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7de5006ms9s\"><sup><a href=\"#fn7de5006ms9s\">[1]</a></sup></span></p><p>For prospective CERI applicants: CERI SRF '23 is not yet open to applications; we\u2019re currently evaluating our impact and considering our long term strategy. There will be a Forum post if and when we open our \u201823 round. In the meantime, you may be interested in our&nbsp;<a href=\"https://forum.effectivealtruism.org/s/c42cHpNCLPN8cLtje/p/KrjyrsRky5JL4P5MF\">existential risks introductory course (ERIC)</a>, or in EA Cambridge's seminar programmes in&nbsp;<a href=\"https://www.agisafetyfundamentals.com/\"><u>AGI safety</u></a> and&nbsp;<a href=\"https://www.eacambridge.org/biosecurity-seminar-programme\"><u>biosecurity</u></a>.</p><h1>Presentations</h1><p>CERI research symposium talks, from both '22 and '21, can be found at the&nbsp;<a href=\"https://www.youtube.com/channel/UCaGONW94o_nEN5zV7UwElzw\">CERI Youtube channel</a>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffga5lsgar5f\"><sup><a href=\"#fnfga5lsgar5f\">[2]</a></sup></span>&nbsp;Below, I break down the '22 fellows' talks by cause area, such that those interested might have a slightly easier time navigating which videos to watch.</p><h2>AI risk</h2><h3>Technical</h3><ul><li><a href=\"https://www.youtube.com/watch?v=bDTd4Z1Q7Fs&amp;list=PL5Ct4A19f4cDLx1oXWncZwPCZxhaoZToC&amp;index=6\">Exploring and characterizing surprising generalization behavior in neural networks</a>&nbsp;<i>- Alexander Davies</i><ul><li>Mentor: Lauro Langosco</li></ul></li><li><i>(unavailable)</i> Infrastructure considerations for advanced ML systems -<i> Pranav Gade</i><ul><li>Mentor: Jeffrey Ladish</li></ul></li><li><a href=\"https://www.youtube.com/watch?v=2S9t_jhZmP8&amp;list=PL5Ct4A19f4cDLx1oXWncZwPCZxhaoZToC&amp;index=8\">Developing an \u2018empathy mechanism\u2019 for AI Agents</a>&nbsp;<i>- Tim Farrelly</i><ul><li>Mentors: Ivana Dusparic, Tim Franzmeyer, and Christian Schroeder de Witt</li></ul></li></ul><h3>Governance</h3><ul><li><i>(unavailable)</i> Article 15 compliance under the EU AI act: How developers and deployers of large foundation models should share regulatory burden&nbsp;<i>- Somsubhro Bagchi</i><ul><li>Mentor: Risto Uuk</li></ul></li><li><a href=\"https://www.youtube.com/watch?v=Iz1RlEJo4d4&amp;list=PL5Ct4A19f4cDLx1oXWncZwPCZxhaoZToC&amp;index=13\">Understanding applications of artificial intelligence to government surveillance</a>&nbsp;<i>- Catherine Brewer</i><ul><li>Mentor: Cecil Abungu</li></ul></li><li><a href=\"https://www.youtube.com/watch?v=LAx-DvkVAIw&amp;list=PL5Ct4A19f4cDLx1oXWncZwPCZxhaoZToC\">Investigating the history of industry-led best practices for safety critical technologies and their implications on AGI governance</a>&nbsp;<i>- Yilin Huang</i><ul><li>Mentor: Claire Boine</li></ul></li></ul><h2>Biorisk</h2><ul><li><a href=\"https://www.youtube.com/watch?v=6wl6fpNmqQE&amp;list=PL5Ct4A19f4cDLx1oXWncZwPCZxhaoZToC&amp;index=3\">Reimagining epidemic sovereignty: Rethinking global health governance of catastrophic biological risks</a>&nbsp;<i>- Hamza Tariq Chaudhry</i><ul><li>Mentor: Catherine Rhodes</li></ul></li><li><a href=\"https://www.youtube.com/watch?v=2RzcaLUkG2w\">Improving DNA synthesis screening to prevent malicious actors from creating dangerous pathogens</a>&nbsp;<i>- Oscar Delaney &amp; Hanna P\u00e1lya</i><ul><li>Mentors: Becky Mackleprang and Lalitha Sundaram</li></ul></li><li><a href=\"https://www.youtube.com/watch?v=5tTBeEuVoyo&amp;list=PL5Ct4A19f4cDLx1oXWncZwPCZxhaoZToC&amp;index=10\">Identifying technological bottlenecks in bio-surveillance systems</a>&nbsp;<i>- Brianna Gopaul &amp; Ziyue Zeng</i><ul><li>Mentor: Akhil Bansal</li></ul></li><li><a href=\"https://www.youtube.com/watch?v=eq7FY1-mp9Q&amp;list=PL5Ct4A19f4cDLx1oXWncZwPCZxhaoZToC&amp;index=14\">Avenues for reducing time to detection and alert for infectious disease outbreaks within West African nations</a>&nbsp;<i>- Sam Pritchard</i><ul><li>Mentor: Sophie Rose</li></ul></li></ul><h2>Nuclear risk</h2><ul><li><i>(unavailable)</i> Effects of transformative technologies on nuclear deterrence -&nbsp;<i>Nathan Barnard</i>&nbsp;<ul><li>Mentor: Matthew Gentzel</li></ul></li><li><a href=\"https://www.youtube.com/watch?v=jRFmadkKrnU&amp;list=PL5Ct4A19f4cDLx1oXWncZwPCZxhaoZToC&amp;index=5\">Investigating India-Pakistan nuclear risk: What are the chances, how bad can it get, and what can we do to mitigate nuclear risk?</a> -&nbsp;<i>Vara Raturi</i>&nbsp;<ul><li>Mentor: Rishi Paul</li></ul></li><li><a href=\"https://www.youtube.com/watch?v=KhF9yPe_lmA&amp;list=PL5Ct4A19f4cDLx1oXWncZwPCZxhaoZToC&amp;index=12\">Machine learning and nuclear command: How the technical flaws of automated systems and a changing human-machine relationship could impact the risk of inadvertent nuclear use</a> -&nbsp;<i>Peter Rautenbach</i><ul><li>Mentor: Haydn Belfield</li></ul></li><li><a href=\"https://www.youtube.com/watch?v=-RUjzxIJX5Y&amp;list=PL5Ct4A19f4cDLx1oXWncZwPCZxhaoZToC&amp;index=4\">Disentanglement and overview of the nuclear risk field</a> -&nbsp;<i>Sarah Weiler</i><ul><li>Mentor: Christian Ruhl</li></ul></li></ul><h2>Extreme climate change</h2><ul><li><a href=\"https://www.youtube.com/watch?v=mP8whFoixxQ&amp;list=PL5Ct4A19f4cDLx1oXWncZwPCZxhaoZToC&amp;index=11\">Investigating the relationship between SRM research and global catastrophic and other related risks</a> -&nbsp;<i>Gideon Futerman</i><ul><li>Mentors: Goodwin Gibbins and Jesse Reynolds</li></ul></li><li><i>(unavailable)&nbsp;</i>Estimating the risks from super pest outbreaks on the global food system -<strong>&nbsp;</strong><i>Kirke Joamets</i><ul><li>Mentor: anonymous</li></ul></li></ul><h2>Miscellaneous and meta x-risk</h2><ul><li><i>(unavailable)</i> Cooperation and conflict between AI systems in non-causal contexts -&nbsp;<i>Jim Buhler</i><ul><li>Mentors: Caspar Oesterheld, Johannes Treutlein, and a third anonymous mentor</li></ul></li><li><a href=\"https://www.youtube.com/watch?v=SFz9MJi92gY&amp;list=PL5Ct4A19f4cDLx1oXWncZwPCZxhaoZToC&amp;index=9\">Analysing the current space governance framework and designing more adaptive, longtermist space governance institutions</a> -&nbsp;<i>Carson Ezell</i><ul><li>Mentors: Giuliana Rotola and Anders Sandberg</li></ul></li><li><a href=\"https://www.youtube.com/watch?v=3t7WxZyDw0o&amp;list=PL5Ct4A19f4cDLx1oXWncZwPCZxhaoZToC&amp;index=7\">Considering future generations in the political decision-making process to overcome political presentism and myopia: Comparative analysis and case studies of best and worst practice examples</a> -&nbsp;<i>Moritz Von Knebel</i><ul><li>Mentors: Matt Boyd and Andrew Leigh</li></ul></li></ul><h1>Acknowledgements</h1><p>I'm grateful to Nandini Shiralkar for helpful comments on an earlier draft, and to Lin Bowker-Lonnecker, Nandini again, and especially Hannah Erlebach for organising our symposium. Moreover, I'm grateful for all the effort - from the fellows, mentors, and organising team - that went into this summer's CERI fellowship. It's a privilege to work with people like you.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ivjgAZRM7No4fnDgo/f467yvz0iusgeoi4urug\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ivjgAZRM7No4fnDgo/iyvfkkvqdnbmlumwv7oi 80w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ivjgAZRM7No4fnDgo/dw2jjqfceyhgyfessarv 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ivjgAZRM7No4fnDgo/diwelp82rogo1glkwvdb 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ivjgAZRM7No4fnDgo/zp97swnohnv64g811bck 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ivjgAZRM7No4fnDgo/vys9bzhe2cfufnughloa 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ivjgAZRM7No4fnDgo/mmcgmwkxvcxxwinzh6mk 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ivjgAZRM7No4fnDgo/nmwhs0huc4te3xsx4hp9 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ivjgAZRM7No4fnDgo/tz0shkb23mbj2gtvpazp 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ivjgAZRM7No4fnDgo/ca1kefgvsgur3unzuzdf 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ivjgAZRM7No4fnDgo/cysnpepzlbqmxnxjcdi2 800w\"></p><p><i>CERI SRF '22 fellows and organisers (and also three of the mentors). Photo by&nbsp;</i><a href=\"http://grahamcopekoga.com/\"><i><u>Graham CopeKoga</u></i></a><i>.</i></p><p><i>Back row, left to right: Nathan Barnard, Tim Farrelly, Pranav Gade, Oscar Delaney, Hanna Palya, Herbie Bradley, Sam Pritchard, Will Aldred, Cecil Abungu, Lin Bowker-Lonnecker</i></p><p><i>Middle row, left to right: Vara Raturi, Catherine Brewer, Carson Ezell, Gideon Futerman, Yilin Huang, Dewi Erwan, Somsubhro Bagchi, Catherine Rhodes, Hannah Erlebach</i></p><p><i>Front row, left to right: Sarah Weiler, Ziyue Zeng, Brianna Gopaul, Nandini Shiralkar, Kirke Joamets, Lalitha Sundaram</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7de5006ms9s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7de5006ms9s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note that CERI's theory of change is more about upskilling aspiring x-risk researchers (i.e., our fellows), with a view to counterfactually more impactful careers going forward, than it is about the direct impact of fellows\u2019 outputs from the summer.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfga5lsgar5f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffga5lsgar5f\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Most fellows' symposium talks can be found on our Youtube channel. Some talks are unavailable on account of infohazards or other reasons.</p></div></li></ol>", "user": {"username": "Will Aldred"}}, {"_id": "BxwSMFnuDgB4Fovaw", "title": "How much donations are needed to neutralise the annual x-risk footprint of the mean human?", "postedAt": "2022-09-22T06:41:37.244Z", "htmlBody": "<h1>Summary</h1><p>I estimate the annual <a href=\"https://forum.effectivealtruism.org/topics/existential-risk\">x-risk</a> footprint of the mean human is 0.2 in 1 trillion, and can be neutralised via donating 1 $ to the <a href=\"https://funds.effectivealtruism.org/funds/far-future\">Long-Term Future Fund</a> (LTFF).</p><h2>Acknowledgements</h2><p>Thanks to Ezra Newman.</p><h1>Methods</h1><p>I calculated the annual x-risk footprint of the mean human from the ratio between:</p><ul><li>The annual x-risk, which I set to 0.182 % annualising the x-risk of 1/6 from 2021 to 2120 guessed by Toby Ord in <a href=\"https://theprecipice.com/\">The Precipice</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbt1ymb6vvoa\"><sup><a href=\"#fnbt1ymb6vvoa\">[1]</a></sup></span>.</li><li>The population size in 2021 of 7.87 billion, which I took from <a href=\"https://ourworldindata.org/grapher/population?tab=table\">Our World in Data.</a></li></ul><p>Then I determined the donations required to neutralise the footprint by dividing it by the (expected marginal) cost-effectiveness of LTFF, which I assumed equal to the 0.0217 % per billion USD estimated <a href=\"https://forum.effectivealtruism.org/posts/NbWeRmEsBEknNHqZP/longterm-cost-effectiveness-of-founders-pledge-s-climate\">here</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl5iqd0kvyc\"><sup><a href=\"#fnl5iqd0kvyc\">[2]</a></sup></span>.</p><h1>Results and discussion</h1><p>The annual x-risk footprint of the mean human is 0.231 in 1 trillion, and can be neutralised via donating 1.07 $ to the LTFF. The calculations are in <a href=\"https://docs.google.com/spreadsheets/d/1czG3Mh1JddoY9YALiiXCd_1lTcYU21YxJohtVqpaneg/edit?usp=sharing\">this</a> Sheet.&nbsp;</p><p>The results are not <a href=\"https://forum.effectivealtruism.org/topics/credal-resilience\">resilient</a>. For example, x-risk arguably varies across time, and it is unclear what is the cost-effectiveness of donating to the LTFF. However, I think the results are not off by more than 2 orders of magnitude. On the other hand, the x-risk footprint of the median human may be many orders of magnitude smaller than that of the mean human.</p><p>In any case, effective donations and other actions are:</p><ul><li>Better seen as a great way of increasing impact. Thinking about them as a form of offsetting could limit our ambition and lead to a smaller impact.</li><li>Often complementary instead of mutually exclusive.</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbt1ymb6vvoa\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbt1ymb6vvoa\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note both the total x-risk and total <a href=\"https://forum.effectivealtruism.org/topics/anthropogenic-existential-risk\">anthropogenic x-risk</a> were estimated to be 1/6 (see Table 6.1).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl5iqd0kvyc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl5iqd0kvyc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The cost-effectiveness I estimated amounts to 70 % (= 0.0217/0.0316) of the geometric mean of the lower and upper cost-effectiveness bars proposed for the <a href=\"https://forum.effectivealtruism.org/posts/pGaesfn4R9xWK4Rmk/01-fund-ideation-and-proposal\">Basis Fund</a> (0.01 % and 0.1 % per billion USD).</p></div></li></ol>", "user": {"username": "vascoamaralgrilo"}}]