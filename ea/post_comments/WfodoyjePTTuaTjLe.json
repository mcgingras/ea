[{"_id": "NgKzcLDhtidf4xJkt", "postedAt": "2023-10-27T21:45:18.154Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Charlie -- thanks very much for this informative and valuable analysis.&nbsp;</p><p>Some EAs might react by thinking 'Well in many of these cases, the protesters were wrong, misguided, or irrational, so we feel like it's weird to learn from protests that were successful, but that addressed the wrong cause areas'. That was my first reaction, esp. regarding protests against nuclear power, GMOs, and geo-engineering (all of which I support, more or less).&nbsp;</p><p>So I think it's important to separate goals from strategies, and to learn effective strategies even from protest movements that may have had wrong-headed goals.</p><p>Indeed, taking that point seriously, it may be worth broadening our historical consideration of successful protest movements from anti-new-technology protests to other situations in which protesters succeeded in challenging entrenched corporate power and escaping from geopolitical arms races. There again, I think we should feel free to learn effective tactics even from movements with misguided goals.&nbsp;</p>", "parentCommentId": null, "user": {"username": "geoffreymiller"}}, {"_id": "syuyyKzAwLG9di7JR", "postedAt": "2023-10-28T11:40:07.360Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Hi Geoffrey, I appreciate that: thank you!</p><p>I agree with you that taking lessons from groups with goals you might object to seems counter-intuitive. (I might also add that protests against nuclear weapons programs, fossil fuels, and CFCs seemed to have had creditworthy aims.) However, I agree with you that we can learn effective strategies from groups with wrong-headed goals. Restricting the data to just groups we agree with would lose lessons about efficacy/messaging/allyship etc.</p><p>(There's also a broader question about whether this mixed reference class should make us worry about bad epistemics in AI activism community. <a href=\"https://forum.effectivealtruism.org/users/oscar-delaney?mention=user\">@Oscar Delaney</a> made a related comment in my <a href=\"https://forum.effectivealtruism.org/posts/6jxrzk99eEjsBxoMA/go-mobilize-lessons-from-gm-protests-for-pausing-ai\">other piece</a>. However, I am comparing groups on what circumstances they were in (facing similar geopolitical/corporate incentives), not epistemics.)&nbsp;</p><p>I also agree that widening the scope beyond anti-technology protests would be interesting!&nbsp;</p>", "parentCommentId": "NgKzcLDhtidf4xJkt", "user": {"username": "hptc123"}}, {"_id": "SA8sEcAEB5CAoDtKT", "postedAt": "2023-10-28T23:52:35.916Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>This is really valuable research, thank you for investigating and sharing!<br><br>My take from this is that there are people out there who are very good (and sometimes lucky) at organising effective protests. As <a href=\"https://forum.effectivealtruism.org/users/geoffreymiller?mention=user\">@Geoffrey Miller</a> comments below (above?), the protesters were not always right. But this is how the world works. We do not have a perfect world government. We can still learn from what they did that worked!<br><br>I believe that the world would be a much better place if EA's influenced more policy decisions. And if protesters were supporting EA-positions, it's highly likely that they'd will be at least mostly right!</p><p>So hoping that this work will help us figure out how to get more EA-aligned protesters.&nbsp;</p><p>I will be investigating a related area in my BlueDot Research Project, so may post something more on this later. But this article has already been a great help!<br><br>&nbsp;</p>", "parentCommentId": null, "user": {"username": "Denis "}}, {"_id": "RhBQA4AW7xEnQJD4c", "postedAt": "2023-10-29T13:49:46.367Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Hi Denis! Thank you for this. I agree that more EA influence on policy decisions would a good outcome. As I tried to set out in this piece, 'insiders' currently advising governments on AI policy would benefit from greater salience of AI as an issue, which protests could help bring about.&nbsp;</p><p>In terms of how we can get more EA-aligned protestors ... a really interesting question, and looking forward to seeing what you produce!&nbsp;</p><p>My initial thoughts: rational arguments about AI activism probably aren't necessary or sufficient for broader EA engagement. EAs aren't typically very ideological/political, and I think psychological factors (\"even though I want to protest, is this what serious EAs do?\") are strong motivators. I doubt many people seriously consider the efficacy/desirability of protests, before going on a protest. (I didn't, really). Once protests become more mainstream, I suspect more people will join. A rough-and-ready survey of EAs &amp; their reasons not to protest would be interesting. <a href=\"https://forum.effectivealtruism.org/users/gideon-futerman?mention=user\">@Gideon Futerman</a> mentioned this in passing.&nbsp;</p><p>Another constraint on more EAs at protests is a lack of funding. <a href=\"https://www.socialchangelab.org/_files/ugd/503ba4_b34f58804b344ff5af974dbfe64a3c25.pdf\">This is endemic to protest groups more generally</a>, and I think is also true for groups like PauseAI. I don't think there are any full-time organisers in the UK, for example.&nbsp;</p>", "parentCommentId": "SA8sEcAEB5CAoDtKT", "user": {"username": "hptc123"}}, {"_id": "PELFbeEMvPumnEJjj", "postedAt": "2023-10-30T04:24:57.843Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>I would caution against taking too strong stances on whether we support something or not without having rigorously assessed it according to EA epistemic standards. I feel this is particularly acute in the case of GMOs and geo-engineering where in the case of GMOs the OP highlighted the justice part of the protests where on the one hand there are powerful corporates and on the other sometimes poor farmers. In the case of geo-engineering in Sweden it should be noted that Sweden has not signed the declaration on the rights of indigenous peoples. Not heeding the needs and perspectives of such marginalized groups has in the past been linked to horrific outcomes and as EA has its origin in helping these very groups I would strongly caution about taking any strong stances here without very careful deliberation.<br><br>This also goes to another comment on this post about how \"the world would be better if EAs influence more policy decisions\" which makes me cautious about hubris from EAs in how we can \"fix everything\". One thing I like about EA is how we focus on very small targeted interventions, understanding them thoroughly before trying to make a change. History seem less seldom to judge harshly those who just were seeking to help people clearly in need with limited scope, whereas people or small groups with grand ideas of revolutionizing the world often seem to end up as history's antagonists.<br><br>Apologies for the rant, I actually came to this post to raise another point but felt a need to react to some of the sentiment in the other comments.</p>", "parentCommentId": "NgKzcLDhtidf4xJkt", "user": {"username": "Ulrik Horn"}}, {"_id": "GfPQhwaDCDdG6kQEv", "postedAt": "2023-10-30T04:33:01.356Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Thanks for the thorough analysis (you seem to have covered a lot of ground with little time!).<br><br>I have been interested in warning shots, albeit from biosecurity, but I think as you point out it could work quite well for AI too as long as take-off is not too fast. I am curious: Did you get a feeling that the movements you studied were able to make more of warning shots from having prepared and organized beforehand? Or could they quite effectively have impact by only retroactively responding to those events afterwards?&nbsp;<br><br>I found that for biosecurity, it seems possible that the <a href=\"https://forum.effectivealtruism.org/posts/gEZjkmLavbZrmtvmZ/is-the-risk-of-a-bioweapons-warning-shot-greater-than\">chance of a warning shot</a> is so high that it might be worthwhile preparing for it, especially if such preparations are likely to pay off. And to perhaps tie my comment a bit more closely to AI, it might be that the warning shot in bio is also a warning shot in AI, if the engineered pathogen was made possible by AI (there seems to be a possibility for quite a lot of <a href=\"https://forum.effectivealtruism.org/posts/ARwFCMpgTbmJ89hBP/ai-bio-cannot-be-half-of-ai-catastrophe-risk-right\">catastrophe risk overlap</a> between AI and bio).</p>", "parentCommentId": null, "user": {"username": "Ulrik Horn"}}, {"_id": "ZQnyFiQjDFk8RwLhF", "postedAt": "2023-10-30T16:18:05.645Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Ulrik - I agree with you that 'History seem less seldom to judge harshly those who just were seeking to help people clearly in need with limited scope, whereas people or small groups with grand ideas of revolutionizing the world often seem to end up as history's antagonists.'</p><p>It's important to note that the AI industry promoters who advocate rushing full speed ahead towards AGI are a typical example of 'small groups with grand ideas of revolutionizing the world', eg by promising that AGI will solve climate change, solve aging, create mass prosperity, eliminate the need to work, etc. They are the dreamy utopians who are willing to take huge risks and dangers on everybody else, to impose their vision of an ideal society.</p><p>The people advocating an AI Pause (like me) are focused on a 'very small targeted intervention' of the sort that you support: shut down the handful of companies, involving just a few thousand researchers, in just a few cities, who are rushing towards AGI.</p>", "parentCommentId": "PELFbeEMvPumnEJjj", "user": {"username": "geoffreymiller"}}, {"_id": "ChuuLMRF9tz3rjgft", "postedAt": "2023-10-30T16:25:06.708Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Charlie - I appreciate your point about the lack of funding for AI-related protests.</p><p>There seems to be a big double standard here.&nbsp;</p><p>Many EA organizations are happy to spend tens of millions of dollars on 'technical AI alignment work', or AI policy/governance work, in hopes that they will reduce AI extinction risk. Although, IMHO, both have a very low chance of actually slowing AGI development, or resulting in safe alignment (given that 'alignment with human values in general' seems impossible, in principle, given the diversity and heterogeneity of human values -- as starkly illustrated in recent news from the Middle East.)</p><p>But the same EA organizations aren't willing, yet, to spend even a few tens of thousands of dollars on 'Pause AI' protests that, IMO, would have a much higher chance of sparking public discourse, interest, and concern about AI risks.</p><p>Funding protests is a tried-and-true method for raising public awareness. Technical AI alignment work is not a tried-and-true method for making AI safe. If our goal is to reduce extinction risk, we may be misallocating resources in directions that might seem intellectually prestigious, but that aren't actually very effective in the real world of public opinion, social media, mainstream media, and democratic politics.</p>", "parentCommentId": "RhBQA4AW7xEnQJD4c", "user": {"username": "geoffreymiller"}}, {"_id": "sKckzYBEdcufbJq8B", "postedAt": "2023-10-30T17:56:15.704Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Yes I agree. Apologies for responding to the other comment on this post in the reply to your comment - I think that created unnecessary confusion.</p>\n", "parentCommentId": "ZQnyFiQjDFk8RwLhF", "user": {"username": "Ulrik Horn"}}, {"_id": "4xDagrRkQGAaiTvSL", "postedAt": "2023-10-30T18:48:16.812Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Ulrik - thanks; understood!</p>", "parentCommentId": "sKckzYBEdcufbJq8B", "user": {"username": "geoffreymiller"}}, {"_id": "Rep6JJ3pzGEaoRqid", "postedAt": "2023-10-30T20:53:57.065Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Lightspeed Grants and my smaller individual donors should get credit for funding me to work on advocacy which includes protests full-time! Sadly afaik that is the only EA/adjacent funding that has gone toward public advocacy for AI Safety.</p>\n", "parentCommentId": "ChuuLMRF9tz3rjgft", "user": {"username": "Holly_Elmore"}}, {"_id": "jvPp5WKim3L3ujtca", "postedAt": "2023-10-31T22:07:34.454Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Thank you <a href=\"https://forum.effectivealtruism.org/users/ulrik-horn?mention=user\">@Ulrik Horn</a>! I think warning shots may very well be important.&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/6jxrzk99eEjsBxoMA/go-mobilize-lessons-from-gm-protests-for-pausing-ai#Disclaimer_\">From my other piece</a>: building up organizations in anticipation of future \u2018trigger events\u2019 is vital for protests, so that they can mobilize and scale in response \u2013 the organizational factor which&nbsp;<a href=\"https://www.apollosurveys.org/social-change-and-protests/\"><u>experts thought was most important for protests</u></a>. I think the same is true for GMOs: pre-existing social movements were able to capitalise on trigger events of 1997/1998, in part, because of prior mobilisation starting in 1980s.&nbsp;</p><p>I also think that engineered pathogen event is a plausible warning shot for AI, though we should also broaden our scope of what could lead to public mobilisation. Lots of 'trigger events' for protest groups (e.g. Rosa Parks, Arab Spring) did not stem from warning shots, but cases of injustice. Similarly, there weren't any 'warning shots' which posed harm for GMOs. (I say more about this in other piece!)</p>", "parentCommentId": "GfPQhwaDCDdG6kQEv", "user": {"username": "hptc123"}}, {"_id": "yWAhrFiW7BYWaobcG", "postedAt": "2023-11-02T09:28:20.856Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Hey Charlie, great post--EAs tend to forget to look at the big picture, or when they do it's very skewed or simplistic (assertions that technology has always been for the best, etc). So it's good to get a detailed perspective on what worked and what did not.</p><p>I would simply add that there is a historical/cultural component that determines the chances of success for each protest that should not be forgotten. For example, in Sweden, a highly-functioning democracy, it's no surprise that the government would pay attention to the protests; I'm not sure this would work for example in Iran if people were pushing against nuclear. In Kazakhstan, the political climate at the time was freer than it is now, it was the end of the Soviet Union and there was a wind of freedom and detachment from the Soviet Union that made more things possible.&nbsp;</p><p>Adding this component by understanding the political dynamics and the level of freedom or responsiveness of the governments towards a bottom-up level of contest would probably help in advocating or not for protests and even put money into it. EAs might not be sold on activism but if they're shown that there's a decent possibility of impact they might change their minds.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Vaipan"}}, {"_id": "CCBJAfanc3MHWKCax", "postedAt": "2023-11-02T09:30:56.627Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Small remark regarding your the metric \"* 100% minus the probability that the given technological restraint would have occurred without protests\" (let's call the latter probability <em>x</em>): this seems to suggest that given the protests the probability became 100% while before it had been <em>x</em> and that hence the protests raised the probability from <em>x</em> to 100%. But the fact that the event eventually did occur does not mean at all that after the protests it had a probability of 100% of occurring. It could even have had the very same probability of occurring as before the protests, namely <em>x</em>, or even a smaller probability than that, if only <em>x&gt;0</em>.</p>\n<p>What you would actually want to compare here is the probability of occurring given no protests (<em>x</em>) and the probability of occurring given protests (which would have to be estimated separately).</p>\n<p>In short: your numbers overestimate the influence of protests by an unknown amount.</p>\n", "parentCommentId": null, "user": {"username": "Jobst Heitzig (vodle.it)"}}, {"_id": "YjXzjZfsJRZRDhNMD", "postedAt": "2023-11-03T16:16:11.822Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>That is a good point, thanks for that Jobst. I've made some edits in light of what you've said.&nbsp;</p>", "parentCommentId": "CCBJAfanc3MHWKCax", "user": {"username": "hptc123"}}, {"_id": "vEF5L3uKArcEA2DAS", "postedAt": "2023-11-03T16:30:34.543Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Hi Vaipan, I appreciate that!&nbsp;</p><p>I agree that political climate is definitely important. The presence of elite allies (Swedish Democrats, President Nazarbayev), and their responsiveness to changes in public opinion was likely important. I am confident the same is true for <a href=\"https://forum.effectivealtruism.org/posts/6jxrzk99eEjsBxoMA/go-mobilize-lessons-from-gm-protests-for-pausing-ai\">GM protests in 1990s in Europe</a>: decision-making was made by national governments, (who were more responsive to public perceptions than FDA in USA), and there were sympathetic Green Parties in coalition governments in France/Germany.&nbsp;</p><p>I agree that understanding these political dynamics for AI is vitally important \u2013 and I try to do so in the GM piece. One key reason to be pessimistic about AI protests is that there <a href=\"https://forum.effectivealtruism.org/posts/6jxrzk99eEjsBxoMA/go-mobilize-lessons-from-gm-protests-for-pausing-ai#I__Who_are_the_Political_Allies_\">aren't many elite political allies for a pause</a>. I think the most plausible TOCs for AI protests, for now, is about raising public awareness/shifting the Overton Window/etc., rather than actually achieving a pause.</p>", "parentCommentId": "yWAhrFiW7BYWaobcG", "user": {"username": "hptc123"}}, {"_id": "xpifipuAQ6CCHPwt6", "postedAt": "2023-11-09T16:31:47.705Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>First of all - great concept and great execution. Lots of interesting information and a lucid, well-supported conclusion.&nbsp;<br><br>My initial and I feel insufficiently addressed concern is that successful protests will of course be overlooked because in retrospect the technology they are protesting will seem \"obviously doomed\" or \"not the right technology\" etc. Additionally, successful protests are probably a lot shorter than the unsuccessful ones (which go on for years continuing to try to stop something that is never stopped). I'm not sure this is evidence that the successful protests \"don't count\" because they were \"too small.\"&nbsp;<br><br>I see you considered some of this in your (very interesting) section on \"other interesting examples of technological restraint\" but I would emphasize these and others like them are quite relevant as long as they obey your other constraints of similar-enough motivation and technology.&nbsp;<br><br>I was not able to come up with any examples to contribute. I feel there are a significant subset of things (pesticides, internet structural decisions, nonstandard incompatible designs?) that were attractive but would have resulted in a worse future had they not been successfully diverted.</p>", "parentCommentId": null, "user": {"username": "EcologyInterventions"}}, {"_id": "cnghhPrBCjtqMX6aR", "postedAt": "2023-11-09T22:49:13.157Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Thanks for writing this! I had one thought regarding how relevant saying no to some of the technologies you listed is to AGI.&nbsp;</p><p>In the case of nuclear weapons programs, the use of fossil fuels, CFCs, and GMOs, we actively used these technologies before we said no (FFs and GMOs we still use despite 'no', and nuclear weapons we have and could use at a moments notice). With AGI, once we start using them it might be too late. Geo-engineering experiments is the most applicable out of these, as we actually did say no before any (much?) testing was undertaken.</p>", "parentCommentId": null, "user": {"username": "MichaelDello"}}, {"_id": "y3WK5tnncBRww2gr2", "postedAt": "2023-11-19T09:36:01.934Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>FYI I was also confused by the probability metric, reading after your edits. I read it multiple times and couldn't get my head round it.</p>\n<p>\"Probability of event occurring given protests - Probability of event occurring without protests\"</p>\n<p>The former number should be higher than the latter (assuming you think that the protests increased the chance of it happening) and yet in every case, the first number you present is lower, e.g.:</p>\n<p>\"De-nuclearization in Kazakhstan in early 1990s (5-15%*)\"</p>\n<p>(Another reason it's confusing is that they read like ranges or confidence intervals or some such, and it's not until you get to the end of the list that you see a definition meaning something else.)</p>\n", "parentCommentId": "YjXzjZfsJRZRDhNMD", "user": {"username": "Jamie_Harris"}}, {"_id": "QtDWPCa5irMqjBXtY", "postedAt": "2023-11-21T12:42:59.968Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Thank you!&nbsp;</p><p>I think your point about hindsight bias is a good one. I think it is true of technological restraint <a href=\"https://verfassungsblog.de/paths-untaken/\">in general</a>: \"Often, in cases where a state decided against pursuing a strategically pivotal technology for reasons of risk, or cost, or (moral or risk) concerns, this can be mis-interpreted as a case where the technology probably was never viable.\"&nbsp;</p><p>I haven't discounted protests which were small \u2013 GMO campaigns and SAI advocacy were both small scale. The fact that unsuccessful protests are more prolonged might make them more psychologically available: e.g. Just Stop Oil campaigns. I'm slightly unsure what your point is here?</p><p>I also agree that other examples of restraint are also relevant \u2013 particularly if public pressure was involved (like for Operation Popeye, and Boeing 2707).</p>", "parentCommentId": "xpifipuAQ6CCHPwt6", "user": {"username": "hptc123"}}, {"_id": "TGa3BnzMbiHDGPLJh", "postedAt": "2023-11-21T12:56:51.436Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>I agree restraining AGI requires \"saying no\" prior to deployment. In this sense, it is more similar to geo-engineering than fossil fuels: there might be no 'fire alarm'/'warning shot' for either.&nbsp;</p><p>Though, the net-present value of AGI (as perceived by AI labs) still seems v high, evidenced by high investment in AGI firms. So, in this sense, it has similar commercial incentives for continued development as continued deployment of GMOs/fossil fuels/nuclear power. I think the GMO example might be the best as it both had strong profit incentives and no 'warning shots'.</p>", "parentCommentId": "cnghhPrBCjtqMX6aR", "user": {"username": "hptc123"}}, {"_id": "KL8CwLLHx3xijiwcr", "postedAt": "2023-11-21T13:03:55.747Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>Sorry that this is still confusing. 5-15 is the confidence interval/range for the counterfactual impact of protests, &nbsp;i.e. p(event occurs with protests) - p(event occurs without protests) = somewhere between 5 and 15. &nbsp;Rather than p(event occurs with protests) = 5, p(event occurs without protests) = 15, which wouldn't make sense.</p>", "parentCommentId": "y3WK5tnncBRww2gr2", "user": {"username": "hptc123"}}, {"_id": "8wyre9N9ubj6oqcst", "postedAt": "2023-12-12T10:25:22.096Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<blockquote>\n<p>Cancellation of prominent SAI geo-engineering experiments (30-60% influence* for SCOPEx, 10-30%* for cancellation of SPICE); De-nuclearization in Kazakhstan in early 1990s (5-15%<em>) and in Sweden in late 1960s (1-15%</em>); Reagan\u2019s move towards a \u2018nuclear freeze\u2019 in the 1980s, and subsequent international treaties (20-40%* for former, 5-15%* for latter); Changing UK\u2019s climate policies in 2019 (40-70%* for specific emissions reductions); Germany\u2019s phase-out of nuclear power from 2011 to 2023 (10-30%<em>); Domestic bans on CFCs in late 1970s (5-25%</em>), and influential for stricter international governance from 1990 (1-10%<em>); Europe\u2019s de-facto moratorium on GMOs in late 1990s (30-50%</em>)</p>\n</blockquote>\n<p>I would like to note that none of that had been met with corporations willing to spend potentially dozens of billions of dollars on lobbying, due to the availability of the capital and the enormous profits to be made; and none of these clearly stand out to policymakers as something uniquely important from the competitiveness perspective. AI is more like railroads; it\u2019d be great to make it more like CFCs in the eyes of policymakers, but for that, you need a clear scientific consensus on the existential threat from AI.</p>\n<p>It makes sense to study the effectiveness of protests and other activism in general and figure out good strategies to use; but due to the differences, I believe it is too easy to learn wrong lessons from restrictions on GMOs and nuclear and have a substantial net-negative impact.</p>\n<p>With AI, we want the policymakers to understand the problem and try to address it; incentivising them to address the public's concerns won\u2019t lead to the change we need.</p>\n<p>The messaging from AI activists should be clear but epistemically honest; we should ask the politicians to listen to the experts, we shouldn\u2019t ask them to listen to the loudest of our voices (as the loudest voices are likely to make claims that the policymakers will know to be incorrect or the representatives of AI companies will be able to explain why they\u2019re wrong, having the aura of science on their side).</p>\n<p>Studying protests in general is great; studying whatever led to the Montreal Protocol and the Vienna Convention is excellent; but we need to be clearly different from people who had protested railroads.</p>\n<p>\u2014</p>\n<p>Also, I\u2019m not sure there\u2019s an actual moratorium on GM crops in Europe- many GMO are officially imported and one GM crop (corn, I think?) is cultivated inside the EU</p>\n", "parentCommentId": null, "user": {"username": "Samin"}}, {"_id": "5KkNHPihAG7CpqZCo", "postedAt": "2024-02-02T19:35:20.092Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>I'm so sorry it's taken me so long to respond, Mikhail!</p><p>&lt;I would like to note that none of that had been met with corporations willing to spend potentially dozens of billions of dollars on lobbying&gt;</p><p>I don't think this is true, for GMOs, fossil fuels, or nuclear power. It's important total lobbying capacity/potential, from actual amount spent on lobbying.... Total annual <strong>t<u>otal technology lobbying </u></strong><u>is in the order hundreds of million: the amount allocated for AI lobbying is, by definition, less. This is a similar to total annual lobbying (or I suspect lower) than than biotechnology spending for GMOs. Annual</u> climate lobbying<a href=\"https://www.theguardian.com/business/2019/mar/22/top-oil-firms-spending-millions-lobbying-to-block-climate-change-policies-says-report\"><u> over \u00a3150 million per year</u></a><u> as I mentioned in my piece. The stakes are also high for nuclear power. As mentioned in my piece, legislation in Germany to</u> extend plant lifetimes in 2010 offered around&nbsp;<a href=\"https://www.economist.com/europe/2010/09/02/nuclear-power-um-maybe\"><u>\u20ac73 billion in extra profits</u></a> for energy companies, some firms sued for billions of Euros after Germany's reversal. (Though, I couldn't find an exact figure for nuclear lobbying). &nbsp;</p><p>&lt; none of these clearly stand out to policymakers as something uniquely important from the competitiveness perspective &gt;&nbsp;</p><p>I also feel this is too strong. Reagan's national security advisors were reluctant about his arms control efforts in 1980s because of<a href=\"https://tnsr.org/2018/05/ronald-reagan-and-the-cold-war-what-mattered-most/\"> national security concerns.</a> Some politicians in Sweden believed nuclear weapons were uniquely important for national security. If your point is that AI is more strategically important than these other examples, then I would agree with you. Though your phrasing is overly strong.&nbsp;</p><p>&lt; AI is more like railroads &gt;&nbsp;</p><p>I don't know if this is true ... I wonder how strategically important railroads were? I also wonder how profitable they were? Seems to be much more state involvement in railroads versus AI... Though, this could be an interesting case study project!</p><p>&lt; AI is more like CFCs in the eyes of policymakers, but for that, you need a clear scientific consensus on the existential threat from AI &gt;&nbsp;</p><p>I agree you need scientific input, but CFCs also saw widespread public mobilisation (as described in the piece).&nbsp;</p><p>&lt; incentivising them to address the public's concerns won\u2019t lead to the change we need &gt;</p><p>This seems quite confusing. Surely, this depends on what the public's concerns are?&nbsp;</p><p>&lt; the loudest voices are likely to make claims that the policymakers will know to be incorrect &gt;</p><p>This also seems confusing to me. If you believe that policymakers regularly sort the \"loudest voices\" from real scientists, in general, why do you think that regulations with \"substantial net-negative impact\" passed wrt GMOs/nuclear?&nbsp;</p><p>&lt; Also, I\u2019m not sure there\u2019s an actual moratorium on GM crops in Europe &gt;&nbsp;</p><p>Yes, with \"moratorium\" I'm referring to a de-facto moratorium on new approvals of GMOs 1999-2002. In general, though, Europe grows a lot less GMOs than other countries:&nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7553740/\"><u>0.1 million hectares</u></a> annually versus&nbsp;<a href=\"https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjnlfXW9YyCAxVIilwKHSYqN-UQFnoECA0QAw&amp;url=https%3A%2F%2Fgeneticliteracyproject.org%2F2019%2F09%2F05%2Fus-world-leader-in-gmo-research-cultivation-growing-75-million-hectares-in-2018%2F%23%3A~%3Atext%3DGenetic%2520Literacy%2520Project-%2CUS%2520world%2520leader%2520in%2520GMO%2520research%252C%2520cultivation%25E2%2580%2594growing%2C75%2520million%2520hectares%2520in%25202018%26text%3DThe%2520US%2520remains%2520as%2520the%2C%252Dbiotech%2520Applications%2520(ISAAA).&amp;usg=AOvVaw2rKQD43pSXMb2bIK65OTci&amp;opi=89978449\"><u>&gt;70 million hectares in US</u></a><u>. I wasn't aware</u> Europe imports GMOs from abroad.&nbsp;</p><p>&nbsp;</p><p><br>&nbsp;</p>", "parentCommentId": "8wyre9N9ubj6oqcst", "user": {"username": "hptc123"}}, {"_id": "oX8moR3exA4uLgvMZ", "postedAt": "2024-02-08T22:01:43.424Z", "postId": "WfodoyjePTTuaTjLe", "htmlBody": "<p>I appreciate the reply!</p>\n<blockquote>\n<p>CFCs also saw widespread public mobilisation</p>\n</blockquote>\n<p>Widespread public mobilisation certainly helps to get more policymakers on board with regulation, but the mobilisation has to be caused by a scientific consensus and be pointing at the scientific consensus</p>\n<blockquote>\n<p>If you believe that policymakers regularly sort the \"loudest voices\" from real scientists</p>\n</blockquote>\n<p>I was talking about the loudest of the activist voices: I\u2019m worried policymakers might hear the public is concerned about hugely beneficial technology instead the public being concerned about the technical reasons for doom and pointing at the scientists to listen to who can explain these reasons</p>\n<blockquote>\n<p>a de-facto moratorium on new approvals</p>\n</blockquote>\n<p>For the context, I want lots of narrow AI to be allowed; I also want things with potential to kill everyone to be prevented from being created, with no chance of someone getting through, anywhere in the planet.</p>\n<blockquote>\n<p>lobbying</p>\n</blockquote>\n<p>$957m in the US alone in 2023 on tech (less on AI): <a href=\"https://www.cnbc.com/2024/02/02/ai-lobbying-spikes-nearly-200percent-as-calls-for-regulation-surge.html\">https://www.cnbc.com/2024/02/02/ai-lobbying-spikes-nearly-200percent-as-calls-for-regulation-surge.html</a></p>\n", "parentCommentId": "5KkNHPihAG7CpqZCo", "user": {"username": "Samin"}}]