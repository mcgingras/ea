[{"_id": "rca3LDgjDjsEwfLHK", "postedAt": "2024-01-25T14:03:59.132Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Thanks for the post. I really appreciate this type of modeling exercise.</p><p>I've been thinking about this for a while, and there are some reflections it might be proper to share here. In summary, I'm afraid a lot of effort in x-risks might be misplaced. Let me share some tentative thoughts on this:</p><p>a) TBH, I'm not very concerned with precise values of point-estimates for the probability of human extinction. Because &nbsp;of anthropic bias, or the fact that this is necessarily a one-time event, the incredible values involved, and doubts about how to extrapolate from past events here, etc., So many degress of freedom, that I don't expect the uncertainties in question to be properly expressed. Thus, if the overall \"true\" x-risk is 1% or 0.00000001%, that doesn't make a lot of difference to me - at least in terms of policy recommendation.</p><p>I'm rather more concerned with odds ratios. If one says that every x-risk estimate is off by n orders of magnitude, I have nothing to reply; instead, I'm interested in knowing if, e.g., one specific type of risk is off, or if it makes human extinction 100 times more likely than the \"background rate of extinction\" (I hate this expression, because it suggests we are talking about frequencies).</p><p>b) So I have been wondering if, instead of trying to compute a causal chain leading from now to extinction, it'd be more useful to do backward reasoning instead: suppose that humanity is extinct (or reduced to a locked-in state) by 3000 CE (or any other period you choose); how likely is it that factor x figures in a causal chain leading to that?</p><p>When I try to consider this, I think that a messy unlucky narrative where many catastrophes concur is at least on a pair with a \"paperclip-max\" scenario. Thus, even though WW 3 would not wipe us out, it would make it way more likely that something else would destroy us afterwards. I'll someday try to properly model this.</p><p>Ofc, I admit that this type of reasoning \"makes\" x-risks less comparable with near-termist interventions - but I'm afraid that's just the way it is.</p><p>c) I suspect that some confusions might be due to Parfit's thought-experiment: because extinction would be much worse than an event that killed 99% of humanity, people often think about events that could wipe us out once and for all. But, in the real world, an event that killed 99% of humanity at once is way more likely than extinction at once, and the former would probably increase extinction risk in many orders of magnitude (specially if most survivors were confined to a state where they would be fragile against local catastrophes). The last human will possibly die of something quite ordinary.</p><p>d) There's an interesting philosophical discussion to be had about what \"the correct estimate of the probability of human extinction\" even means. It's certainly not an objective probability; so the grounds for saying that such an estimate is better than another might be something like that it converges towards what an ideal prediction market or <a href=\"https://intelligence.org/files/LogicalInduction.pdf\">logical inductor</a> would output. But then, I am quite puzzled about how such a mechanism could work for x-risks (how would one define prices? well, one could perhaps value lives with the statistical value of life, like <a href=\"https://www.nber.org/papers/w26068\">Martin &amp; Pyndick)</a>.</p>", "parentCommentId": null, "user": {"username": "Ramiro"}}, {"_id": "Pqjx29ZA9rxAP224i", "postedAt": "2024-01-25T14:37:26.184Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Something that surprised me a bit, but that is unlikely to affect your analysis:</p><blockquote><p>I used <a href=\"https://en.wikipedia.org/wiki/Correlates_of_War\"><u>Correlates of War</u></a>\u2019s <a href=\"https://ourworldindata.org/grapher/deaths-in-wars-by-type-correlates-of-war\"><u>data</u></a>&nbsp;on annual war deaths of combatants due to fighting, disease, and starvation. The dataset goes from 1816 to 2014, and excludes wars which caused less than 1 k deaths of combatants in a year.</p></blockquote><p>Actually, I'm not sure if this dataset is taking into account average estimates of excess deaths in Congo Wars (1996-2003, 1.5 million - 5.4 million) - and I'd like to check how it takes into account Latin American Wars of the 19th century.</p>", "parentCommentId": null, "user": {"username": "Ramiro"}}, {"_id": "9B6FcQFDyP2zHDPeJ", "postedAt": "2024-01-25T15:07:13.786Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Thanks for sharing your thoughts, Ramiro!</p><blockquote><p>a) TBH, I'm not very concerned with precise values of point-estimates for the probability of human extinction. Because &nbsp;of anthropic bias, or the fact that this is necessarily a one-time event, the incredible values involved, and doubts about how to extrapolate from past events here, etc., So many degress of freedom, that I don't expect the uncertainties in question to be properly expressed. Thus, if the overall \"true\" x-risk is 1% or 0.00000001%, that doesn't make a lot of difference to me - at least in terms of policy recommendation.</p></blockquote><p>On the one hand, I agree <a href=\"https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/#:~:text=The%20crucial%20characteristic%20of%20the,and%20robustness%20of%20the%20estimates.\">expected value estimates cannot be taken literally</a>. On the other, I think there is a massive difference between one's best guess for the annual extinction risk<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmxkhxxoiz6i\"><sup><a href=\"#fnmxkhxxoiz6i\">[1]</a></sup></span>&nbsp;being 1 % or 10^-10 (in policy and elsewhere). I guess you were not being literal? In terms of risk of personal death, that would be the difference between a non-Sherpa first-timer climbing Mount Everest<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefidyplv2gn4\"><sup><a href=\"#fnidyplv2gn4\">[2]</a></sup></span>&nbsp;(risky), and driving for 1 s<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl3kuhjzrfaj\"><sup><a href=\"#fnl3kuhjzrfaj\">[3]</a></sup></span>&nbsp;(not risky).</p><p>It is worth noting one of the upshorts of the post I linked above is that priors are important. I see my post as an illustration that priors for extinction risk are quite low, such that inside view estimates should be heavily moderated.</p><p>It may often not be desirable to prioritise based on point estimates, but there is a sense in which they are unavoidable. When one decides to prioritise A over B at the margin, one is implicitly relying on point estimates: \"expected marginal cost-effectiveness of A\" &gt; \"expected marginal cost-effectiveness of B\".</p><blockquote><p>I'm rather more concerned with odds ratios. If one says that every x-risk estimate is off by n orders of magnitude, I have nothing to reply; instead, I'm interested in knowing if, e.g., one specific type of risk is off, or if it makes human extinction 100 times more likely than the \"background rate of extinction\" (I hate this expression, because it suggests we are talking about frequencies).</p></blockquote><p>That makes a lot of sense if one is assessing interventions to decrease extinction risk. However, if the risk is sufficiently low, it will arguably be better to start relying on other metrics. So I think it is worth keeping track of the absolute risk for the purpose of <a href=\"https://forum.effectivealtruism.org/topics/cause-prioritization\">cause prioritisation</a>.</p><blockquote><p>b) So I have been wondering if, instead of trying to compute a causal chain leading from now to extinction, it'd be more useful to do backward reasoning instead: suppose that humanity is extinct (or reduced to a locked-in state) by 3000 CE (or any other period you choose); how likely is it that factor x figures in a causal chain leading to that?</p></blockquote><p><a href=\"https://en.wikipedia.org/wiki/Pre-mortem\">Pre-mortems</a> make sense. Yet, they also involve thinking about the causal chain. In contrast, my post takes an outside view approach without modelling the causal chain, which is also useful. Striking the right balance between inside and outside views is one of the <a href=\"https://goodjudgment.com/philip-tetlocks-10-commandments-of-superforecasting/\">Ten Commandments for Aspiring Superforecasters</a>.</p><blockquote><p>When I try to consider this, I think that a messy unlucky narrative where many catastrophes concur is at least on a pair with a \"paperclip-max\" scenario. Thus, even though WW 3 would not wipe us out, it would make it way more likely that something else would destroy us afterwards. I'll someday try to properly model this.</p></blockquote><p>I agree <a href=\"https://en.wikipedia.org/wiki/Cascade_effect\">cascade effects</a> are real, and that having a 2nd catastrophe conditional on 1 catastrophe will tend to be more likely than having the 1st catastrophe. Still, having 2 catastrophes will tend to be less likely than having 1, and I guess the risk of the 1st catastrophe will often be a good proxy for the overall risk.</p><blockquote><p>c) I suspect that some confusions might be due to Parfit's thought-experiment: because extinction would be much worse than an event that killed 99% of humanity, people often think about events that could wipe us out once and for all. But, in the real world, an event that killed 99% of humanity at once is way more likely than extinction at once, and the former would probably increase extinction risk in many orders of magnitude (specially if most survivors were confined to a state where they would be fragile against local catastrophes). The last human will possibly die of something quite ordinary.</p></blockquote><p>Relatedly, readers may want to check Luisa Rodriguez' <a href=\"https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would\">post</a> on the likelihood that <a href=\"https://forum.effectivealtruism.org/topics/civilizational-collapse\">civilizational collapse</a> would directly lead to human extinction. Nonetheless, at least following my methodology, which does not capture all relevant considerations, annual war deaths being 99 % of the global population is also astronomically unlikely for most best fit distributions. You can see this comparing my estimates for the probability of a 10 % and 100 % population loss.</p><blockquote><p>d) There's an interesting philosophical discussion to be had about what \"the correct estimate of the probability of human extinction\" even means. It's certainly not an objective probability; so the grounds for saying that such an estimate is better than another might be something like that it converges towards what an ideal prediction market or <a href=\"https://intelligence.org/files/LogicalInduction.pdf\">logical inductor</a> would output. But then, I am quite puzzled about how such a mechanism could work for x-risks (how would one define prices? well, one could perhaps value lives with the statistical value of life, like <a href=\"https://www.nber.org/papers/w26068\">Martin &amp; Pyndick)</a>.</p></blockquote><p>I would argue there is not a fundamental difference between <a href=\"https://www.tutorialspoint.com/subjective-probability-vs-objective-probability\">objective and subjective probabilities</a>. All probabilities are based on past empirical evidence and personal guesses to a certain extent. That being said, I think using heuristics like the ones you suggested can be useful to ground more subjective probabilities.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmxkhxxoiz6i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmxkhxxoiz6i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I <a href=\"https://forum.effectivealtruism.org/posts/9pSJj7c7dJziypYTL/stop-talking-about-p-doom?commentId=bK4WNLherjxNSpQHA\">prefer</a> focussing on extinction risk.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnidyplv2gn4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefidyplv2gn4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>According to <a href=\"https://www.thehindu.com/sci-tech/energy-and-environment/why-is-climbing-mount-everest-so-dangerous/article66904138.ece\">this</a> article, \"from 2006 to 2019, the&nbsp;death rate&nbsp;for first-time, non-Sherpa climbers was 0.5% for women and 1.1% for men\".</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl3kuhjzrfaj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl3kuhjzrfaj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>10^-10 corresponds to 10^-4 <a href=\"https://en.wikipedia.org/wiki/Micromort\">micromorts</a>, and driving \"370 km\" corresponds to 1 micromort. So 10^-10 respects driving for 0.037 km (370 km times 10^-4), which would take 1 s (= 0.037/100*60^2) at 100 km/h.</p></div></li></ol>", "parentCommentId": "rca3LDgjDjsEwfLHK", "user": {"username": "vascoamaralgrilo"}}, {"_id": "FDRcL9CHqYFhcJyeW", "postedAt": "2024-01-25T17:59:11.669Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Thanks for the note, Ramiro!</p><p>Global annual deaths of combatants from 1996 to 2003 were <a href=\"https://docs.google.com/spreadsheets/d/1bhqZRZ1KOndD3IqSin-TPtsqB9uk2Kvj8_FCB8vBnHs/edit#gid=0&amp;range=F183:F190\">59.8 k</a> according to Correlates of War, whereas the death tolls you mention would imply annual deaths of 431 k (= (1.5 + 5.4)/2*10^6/(2003 - 1996 + 1)) for the <a href=\"https://en.wikipedia.org/wiki/First_Congo_War\">Congo</a> <a href=\"https://en.wikipedia.org/wiki/Second_Congo_War\">Wars</a> alone. So it looks like the vast majority of deaths of the Congo Wars are being attributed to civilians.</p><p>I agree the above will not matter for the conclusions of my analysis. Based on the 2 estimates above, global deaths of combatants were 13.9 % (= 59.8/431) of all deaths in the Congo Wars, which much less than my central estimate of 50 %. However, I also used a pessimistic fraction of 10 % for the deaths of combatants as a fraction of total deaths (for all years, not just those of the Congo Wars), and still got astronomically low extinction risk.</p>", "parentCommentId": "Pqjx29ZA9rxAP224i", "user": {"username": "vascoamaralgrilo"}}, {"_id": "ee7Chfyx2coYwmznt", "postedAt": "2024-01-26T17:36:17.493Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>This is an interesting analysis that I haven't properly digested, so what I'm about to say might be missing something important, but something feels a bit strange about this type of approach to this type of question.</p><p>For example, couldn't I write a post titled \"Can AI cause human extinction? not on priors\" where I look at historical data on \"humans killed by machines\" (e.g. traffic accidents, factory accidents) as a fraction of the global population, show that it is tiny, and argue it's extremely unlikely that AI (another type of machine) will wipe us all out?</p><p>I think the mistake I'd be making here is lumping in AGI with cars, construction machinery, etc, into one single category. But then I imagine the people who worry about extinction from war are also imagining a kind of war which should belong in a different category to previous wars.</p><p>What's your take on this? Would the AI post be actually valid as well? Or is there an important difference I'm missing?</p>", "parentCommentId": null, "user": {"username": "tobycrisford"}}, {"_id": "BJfBYGZWnmhYfqHQi", "postedAt": "2024-01-27T08:36:50.726Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<blockquote><p>suppose that humanity is extinct (or reduced to a locked-in state) by 3000 CE (or any other period you choose); how likely is it that factor x figures in a causal chain leading to that?</p></blockquote><p>Perhaps not a direct answer to your question, but this reminded me of the <a href=\"https://possibleworldstree.com/\">Metaculus Ragnarok series</a>.</p>", "parentCommentId": "rca3LDgjDjsEwfLHK", "user": {"username": "Mo Nastri"}}, {"_id": "dB83FR2Te84troPwK", "postedAt": "2024-01-27T09:59:30.284Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Great reflection, Toby!</p><blockquote><p>I think the mistake I'd be making here is lumping in AGI with cars, construction machinery, etc, into one single category. But then I imagine the people who worry about extinction from war are also imagining a kind of war which should belong in a different category to previous wars.</p></blockquote><p>I agree that would be the mistake. I think one can use an analysis similar to mine to conclude, for example, that the probability of car accidents causing human extinction is astronomically low. However, I would say using past war deaths to estimate future war deaths is much more appropriate than using past car accident deaths to estimate the risk of human extinction due to advanced AI.</p><p>I assume increasing capability to cause damage is the main reason for people arguing that future wars would belong to a different category. Yet:</p><ul><li>I think war capabilities have been decreasing or not changing much in the last few decades:<ul><li>\"Nuclear risk has been decreasing. The estimated destroyable <a href=\"https://ourworldindata.org/grapher/estimated-destroyable-area-by-nuclear-weapons-deliverable-in-first-strike\">area</a> by nuclear weapons deliverable in a first strike has decreased 89.2 % (= 1 - 65.2/601) since its peak in 1962\" (see 1st graph below).</li><li>Military expenditure as a fraction of global GDP <a href=\"https://data.worldbank.org/indicator/MS.MIL.XPND.GD.ZS\">has</a> decreased from 1960 to 2000, and been fairly constant since then (see 2nd graph below).</li></ul></li><li>Taking a broader view, war capabilities do have been increasing, but there is not a clear trend in the deaths in conflicts as a fraction of the global population since 1400 (see last figure in the post).</li><li>Increases in the capability to cause damage are usually associated with increases in the capability to prevent damage, which I guess explains what I said just above, so one should not forecast future risk based on just one side alone.</li></ul><figure class=\"media\"><div data-oembed-url=\"https://ourworldindata.org/grapher/estimated-destroyable-area-by-nuclear-weapons-deliverable-in-first-strike\">\n\t\t\t\t\t<div data-owid-slug=\"estimated-destroyable-area-by-nuclear-weapons-deliverable-in-first-strike\" class=\"owid-preview\">\n\t\t\t\t\t\t<iframe src=\"https://ourworldindata.org/grapher/estimated-destroyable-area-by-nuclear-weapons-deliverable-in-first-strike\">\n\t\t\t\t\t</iframe></div>\n\t\t\t\t</div></figure><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/ved7dn0d2v9r1o2r48sz\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/c6ldlkt4yfjjcf9ovudc 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/gyaabexr9reoqylg5edx 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/rljkmns0fq8aj2qry3lo 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/ytt8ct4znokacwnpxpzo 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/s7f5x0gugsxgicqkewim 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/oymev6mjuy7tpxpnmaaf 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/zgevkxlftcphazszswvw 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/fuednoshin1lpszsfouo 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/bauyipwxqq6yaq78qipx 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/ampmbfby3anb2w8qc3um 1957w\"></p><p>Something like deaths in car accidents does not capture the relevant factors which can lead to AI causing human extinction. I think the best <a href=\"https://forecasting.quarto.pub/book/base-rates.html?ref=bounded-regret.ghost.io\">reference class</a> for this is looking into how species have gone extinct in the past. Jacob Steinhardt did an <a href=\"https://bounded-regret.ghost.io/base-rates-of-catastrophes/\">analysis</a> which has some relevant insights:</p><blockquote><p>Thus, in general most species extinctions are caused by:</p><ul><li>A second species which the original species has not had a chance to adapt to. This second species must also not be reliant on the original species to propagate itself.</li><li>A catastrophic natural disaster or climate event. [As a side note, I believe we have pretty good reasons to think this point applies much more weakly to humans than animals.]</li><li>Habitat destruction or ecosystem disruption caused by one of the two sources above.</li></ul></blockquote><p>Advanced AI is much more analogous to a new species than e.g. cars, so per the 1st point it makes sense that extinction risk from advanced AI is much higher than from e.g. cars. I would still claim deaths in past wars and <a href=\"https://forum.effectivealtruism.org/posts/M6zAzCBAsBxem7Lu4/can-a-terrorist-attack-cause-human-extinction-not-on-priors\">terrorist attacks</a> provide a strong basis for arguing that humans will not go extinct via an AI war or terrorist attack. However, the 1st point alludes to what seems to me to be the greatest risk from AI, <a href=\"https://arxiv.org/pdf/2303.16200.pdf\">natural selection favoring AIs over humans</a>.</p><p>I should note I do not consider humans being outcompeted by AI as necessarily bad. I endorse expected total hedonistic <a href=\"https://utilitarianism.net/\">utilitarianism</a> (ETHU), and humans <a href=\"https://forum.effectivealtruism.org/posts/rLLRo9C4efeJMYWFM/welfare-ranges-per-calorie-consumption\">are</a> unlikely to be the most efficient way of increasing welfare longterm. At the same time, minimising nearterm extinction risk from AI is an arguably helpful heuristic to align AI with ETHU.</p>", "parentCommentId": "ee7Chfyx2coYwmznt", "user": {"username": "vascoamaralgrilo"}}, {"_id": "duBuYbEzps7SdW7W2", "postedAt": "2024-01-28T00:29:11.973Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Perhaps I misunderstand the situation, but it seems like methodology around how to analyze tails of distributions will dominate the estimates at the current scale. Then, we should take the expectation over our corresponding uncertainty and we end up with a vastly higher estimate.<br><br>Another way to put this is that median (or geometric mean) seem like the wrong aggregation methods in this regime and the right aggregation method is more like arithmetic mean (though perhaps slightly less aggressive than this).</p>", "parentCommentId": null, "user": {"username": "Ryan Greenblatt"}}, {"_id": "vKdkdbC3hi5JJJ8Xq", "postedAt": "2024-01-28T00:39:02.682Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>It seems to me like a pretty relevant comparison would be to the probabilty that an asteroid impact causes &gt;1 billion deaths.</p><p>As in, run this exact methodology using deaths due to asteroid impacts per year over the past 200 years as the dataset (we could also use injuries instead of deaths).<br><br>My understanding is that this would result in predicting an astronomically, astronomically low probability of &gt;1 billion deaths.<br><br>So either:<br><br>- Updating up massively on such astronomically low priors is common in cases where we have other arguments at hand (in the asteroid case it would be various other data sources on asteriod collisions, in the war case it would be arguments related to bioweapons or similar)<br>- This methodology provides a very bad prior for asteroids.<br><br>Yes, asteroids are slightly cherry picked, but when talking about probabilities of 10^-13 this amount of cherry picking doesn't matter.</p>", "parentCommentId": "dB83FR2Te84troPwK", "user": {"username": "Ryan Greenblatt"}}, {"_id": "2GLAqdLpKuKYqps3X", "postedAt": "2024-01-28T14:39:05.787Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Thanks for the comment, Ryan!</p><p>You are right that using the mean could lead to very different conclusions. For war deaths of combatants equal to 50 % of war deaths (my best guess), the annual probability of a war causing human extinction is:</p><ul><li>For the top 10 best fit distributions, 0 using the median, but 1.09*10^-14 using the mean.</li><li>For the top 100 best fit distributions, 6.36*10^-14 using the median, but 0.132 % using the mean.</li></ul><p>However, I did not use the mean because it is not resistant to outliers<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftm74m8hmugb\"><sup><a href=\"#fntm74m8hmugb\">[1]</a></sup></span>. Relatedly, I <a href=\"https://forum.effectivealtruism.org/posts/JxakgiGiJ3egodfb3/intermediate-report-on-abrupt-sunlight-reduction-scenarios?commentId=iRCGvh6vxJ2ktJgDR\">commented</a> that:</p><blockquote><p>In general, my view is more that it feels overconfident to ignore predictions, and using the mean does this when samples differ a lot among them. To illustrate, if I am trying to aggregate N probabilities, 10 %, 1 %, 0.1 %, ..., and 10^-N, for N = 9:</p><ul><li>The probability corresponding to the geometric mean of odds is 0.0152 % (= 1/(1 + (1/9)^(-(1 + 7)/2*7/7))), which is 1.52 times the median of 0.01 %.</li><li>The mean is 1.59 % (= 0.1*(1 - 0.1^7)/(1 - 0.1)/7), i.e. 159 times the median.</li></ul><p>I think the mean is implausible because:</p><ul><li>Ignoring the 4 to 5 lowest predictions among only 7 seems unjustifiable, and using the mean is equivalent to using the probability corresponding to the geometric mean of odds putting 0 weight in the 4 to 5 lowest predictions, which would lead to 0.894 % (= 1/(1 + (1/9)^(-(1 + 5)/2*5/7))) to 4.15 % (= 1/(1 + (1/9)^(-(1 + 4)/2*4/7))).&nbsp;</li><li>Ignoring the 3 lowest and 3 highest predictions among 7 seems justifiable, and would lead to the median, whereas the mean is 159 times the median.</li></ul></blockquote><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntm74m8hmugb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftm74m8hmugb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I have added to the post:</p><blockquote><p>[I did not use:] The mean because it is not resistant to outliers.</p></blockquote><p>&nbsp;</p></div></li></ol>", "parentCommentId": "duBuYbEzps7SdW7W2", "user": {"username": "vascoamaralgrilo"}}, {"_id": "Rf9bdqgRPyNkxdrXp", "postedAt": "2024-01-28T15:53:42.330Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Thanks for the suggestion, Ryan!</p><p>In general, extrapolations based on a given dataset become less reliable as one tries to predict events which are increasingly far away from the region for which there is data. Therefore my methodology is way more applicable to estimate the annual risk of human extinction from wars than from asteroids:</p><ul><li>My maximum annual war deaths as a fraction of the global population is 0.300 %<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvtijomurjjs\"><sup><a href=\"#fnvtijomurjjs\">[1]</a></sup></span>&nbsp;(= 1.50*10^-3/0.5), which is just 2.5 orders of magnitude away from extinction.</li><li>I guess the maximum annual deaths from asteroids in the past 200 years were less than 800, i.e. less than 10^-7 (= 800/(8*10^9)) of the global population, which is at least 7 orders of magnitude away from extinction.</li></ul><p>In any case, annual extinction risk from asteroids and comets is astronomically low based on inside view models. The results of Table 1 of <a href=\"https://www.sciencedirect.com/science/article/pii/S0016328722000337\">Salotti 2022</a> suggest it is 2.2*10^-14 (= 2.2*10^-12/100), which is around 1/3 of my best guess prior for wars of 6.36*10^-14.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Rf9bdqgRPyNkxdrXp/hxl6lmsl03v3djkpebao\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Rf9bdqgRPyNkxdrXp/ndguahmaqemvidb4z2q3 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Rf9bdqgRPyNkxdrXp/bkfefmw9klczcfblu7fo 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Rf9bdqgRPyNkxdrXp/abe8zcrpyewaykdyxg4d 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Rf9bdqgRPyNkxdrXp/lay1yt9opqebzwyjdfqc 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Rf9bdqgRPyNkxdrXp/lbwrgcvxd7p23gaqlsmt 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Rf9bdqgRPyNkxdrXp/holzid1hezhgvpajeow7 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Rf9bdqgRPyNkxdrXp/bgcpdi2x5w8kmg4yb6yy 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Rf9bdqgRPyNkxdrXp/erwtfy9x8moggou7cbgk 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Rf9bdqgRPyNkxdrXp/hggtrp7rkwdwmnwbgwli 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Rf9bdqgRPyNkxdrXp/ndbs5fanemy2spivvgxq 1930w\"></figure><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvtijomurjjs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvtijomurjjs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For my best guess of war deaths of combatants equal to 50 % of total deaths.</p></div></li></ol>", "parentCommentId": "vKdkdbC3hi5JJJ8Xq", "user": {"username": "vascoamaralgrilo"}}, {"_id": "AQzQES552apvXjgci", "postedAt": "2024-01-28T19:13:49.007Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Let me briefly try to reply or clarify this:</p><blockquote><p>I think there is a massive difference between one's best guess for the annual extinction risk<a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors?commentId=9B6FcQFDyP2zHDPeJ#fnmxkhxxoiz6i\"><sup>[1]</sup></a>&nbsp;being 1 % or 10^-10 (in policy and elsewhere). I guess you were not being literal? In terms of risk of personal death, that would be the difference between a non-Sherpa first-timer climbing Mount Everest<a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors?commentId=9B6FcQFDyP2zHDPeJ#fnidyplv2gn4\"><sup>[2]</sup></a>&nbsp;(risky), and driving for 1 s<a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors?commentId=9B6FcQFDyP2zHDPeJ#fnl3kuhjzrfaj\"><sup>[3]</sup></a>&nbsp;(not risky).</p></blockquote><p>I did say that I'm not very concerned with the absolute values of precise point-estimates, and more interested in proportional changes and in relative probabilities; allow me to explain:</p><p>First, as a rule of thumb, <i>coeteris paribus</i>, a decrease in the avg x-risk implies an increase in the expected duration of human survival - so yielding a proportionally higher expected value for reducing x-risk. I think this can be inferred from Thorstad's toy model in&nbsp;<a href=\"https://philarchive.org/rec/THOERP\">Existential risk pessimism and the time of perils</a>. So, if something reduces x-risk by 100x, I'm assuming it doesn't make much difference, from my POV, if the prior x-risk is 1% or 10^-10 - because I'm assuming that EV will stay the same. This is not always true; I should have clarified this.</p><p>Second, it's not that I don't see any difference between \"1%\" vs. \"10^-10\"; I just don't take sentences of the type \u201cthe probability of p is 10^-14\u201d at face value. For me, the reference for such measures might be quite ambiguous without additional information - in the excerpt I quoted above, you do provide that when you say that this difference would correspond to the distance between the risk of death for Everest climbing vs. driving for 1s \u2013 which, btw, are extrapolated from frequencies (according to the footnotes you provided).</p><p>Now, it looks like you say that, given your best estimate, the probability of extinction due to war is <i>really approximately like&nbsp;</i>picking a certain number from a lottery with 10^14 possibilities, or the probability of tossing a <i>fair coin</i> 46-47 times and getting only heads; it\u2019s just that, because it\u2019s not resilient, there are many things that could make you significantly update your model (unlike the case of the lottery and the fair coin). I do have something like a philosophical problem with that, which is unimportant; but I think it might result in a practical problem, which might be important. So...</p><p>It reminds me of a paper by the epistemologist <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/meta.12142\">Duncan Pritchard</a>, where he supposes that a bomb will explode if (i) in a lottery, a specific number out of 14 million is withdrawn, or if ( (ii) a conjunction of bizarre events (eg., the spontaneous pronouncement of a certain Polish sentence during the Queen's next speech, the victory of an underdog at the Grand National...) occurs, with an assigned probability of 1 in 14 million. Pritchard concludes that, though both conditions are equiprobable, we consider the latter to be a lesser risk because it is \"modally farther away\", in a \"more distant world\"; I think that's a terrible solution: people usually prefer to toss a fair coin rather than a coin they know is biased (but whose precise bias they ignore), even though both scenarios have the same \u201cmodal distance\u201d. Instead, the problem is, I think, that reducing our assessment to a point-estimate might fail to convey our uncertainty regarding the differences in both information sets \u2013 and one of the goals of subjective probabilities is actually to provide a measurement of uncertainty (and the expectation of surprise). That\u2019s why, when I\u2019m talking about very different things, I prefer statements like \u201cboth probability distributions have the same mean\u201d to claims such as \u201cboth events have the same probability\u201d.</p><p>Finally, I admit that the financial crisis of 2008 might have made me a bit too skeptical of sophisticated models yielding precise estimates with astronomically tiny odds, when applied to events that require no farfetched assumptions - particularly if minor correations are neglected, and if underestimating the probability of a hazard might make people more lenient regarding it (and so unnecessarily make it more likely). I'm not sure how epistemically sound my behavior is; and I want to emphasize that this skepticism is not quite applicable to your analysis - as you make clear that your probabilities are not <i>resilient</i>, and point out the main caveats involved (particularly that, e.g., a lot depends on what type of distribution is a better fit for predicting war casualties, or on what role tech plays).</p>", "parentCommentId": "9B6FcQFDyP2zHDPeJ", "user": {"username": "Ramiro"}}, {"_id": "wPHy6s2xXoh6BAcXm", "postedAt": "2024-01-28T19:29:24.486Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>I think there are probably cases where you want to do tail analysis and where doing something more like arithmetic mean produces much better estimates. I quickly tried to construct a toy model of this (but I failed).<br><br>In particular, I think if you have 10 possible models of tail behavior, your prior is 10% on each, and you don't update much between which fitting models to use based on seeing the data you have (due to limited data from the tail), then I think the right aggregation model is going to be arithmetic mean (or something close to this based on the amount of update).<br><br>The fact that the mean isn't robust to outliers is actually the right property in the case: indeed low probabilites in the tail are dominated by outliers. (See work by Nassim Taleb for instance.)</p>", "parentCommentId": "2GLAqdLpKuKYqps3X", "user": {"username": "Ryan Greenblatt"}}, {"_id": "D9a99frZrXaZH4ZBx", "postedAt": "2024-01-28T19:32:57.456Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>(Yeah, the asteroids comparison is perhaps more relevant to the post on terrorist attacks.)&nbsp;</p>", "parentCommentId": "Rf9bdqgRPyNkxdrXp", "user": {"username": "Ryan Greenblatt"}}, {"_id": "ovtvxf6CTpcung2TX", "postedAt": "2024-01-29T09:08:49.409Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Agreed. At the same time, my methodology resulting in an astronomically low extinction risk from asteroids would arguably still be qualitatively in agreement with the results of <a href=\"https://www.sciencedirect.com/science/article/pii/S0016328722000337\">Salotti 2022</a>.</p>", "parentCommentId": "D9a99frZrXaZH4ZBx", "user": {"username": "vascoamaralgrilo"}}, {"_id": "GyDBhQQnMyFxsdQcM", "postedAt": "2024-01-29T10:34:35.182Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<blockquote><p>In particular, I think if you have 10 possible models of tail behavior, your prior is 10% on each, and you don't update much between which fitting models to use based on seeing the data you have (due to limited data from the tail), then I think the right aggregation model is going to be arithmetic mean (or something close to this based on the amount of update).</p></blockquote><p>There is a sense in which I agree with the above in theory, because I think the models are mutually incompatible. The annual war deaths as a fraction of the global population cannot simultaneously follow e.g. a Pareto and lognormal distribution. However, I would say the median or other method which does not overweight extremely high predictions is better in practice. For example:</p><ul><li>The weighted/unweighted median <a href=\"https://forum.effectivealtruism.org/posts/sMjcjnnpoAQCcedL2/when-pooling-forecasts-use-the-geometric-mean-of-odds?commentId=HNtKuSWwcyB5gdeE3\">performed</a> better than the weighted/unweighted mean on Metaculus' questions.</li><li><a href=\"https://samotsvety.org/track-record/\">Samotsvety</a> <a href=\"https://forum.effectivealtruism.org/posts/2nDTrDPZJBEerZGrk/samotsvety-nuclear-risk-update-october-2022#A_brief_note_on_the_aggregation_method\">aggregated</a> predictions differing a lot between them from 7 forecasters<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdqpqh547i2j\"><sup><a href=\"#fndqpqh547i2j\">[1]</a></sup></span>&nbsp;using the geometric mean after removing the lowest and highest values.<ul><li>The geometric mean, like the median, does not overweight extremely high predictions.</li><li>The more one removes extreme predictions before using the geometric mean, the closer it gets to the median.</li><li>A priori, it seems sensible to use an aggregation method that one of the most accomplished forecasting groups uses.</li></ul></li></ul><blockquote><p>The fact that the mean isn't robust to outliers is actually the right property in the case: indeed low probabilites in the tail are dominated by outliers. (See work by Nassim Taleb for instance.)</p></blockquote><p>I said \"I did not use the mean because it is not resistant to outliers\", but I meant \"because it <a href=\"https://forum.effectivealtruism.org/s/hjiBqAJNKhfJFq7kf/p/sMjcjnnpoAQCcedL2#The_arithmetic_mean_of_probabilities_ignores_information_from_extreme_predictions\">ignores</a> information from extremely low predictions\" (I have updated the post):</p><blockquote><h3><strong>The arithmetic mean of probabilities ignores information from extreme predictions</strong></h3><p>The arithmetic mean of probabilities ignores extreme predictions in favor of tamer results, to the extent that even large changes to individual predictions will barely be reflected in the aggregate prediction.</p><p>As an illustrative example, consider an outsider expert and an insider expert on a topic, who are eliciting predictions about an event. &nbsp;The outsider expert is reasonably uncertain about the event, and each of them assigns a probability of around 10% to the event. The insider has priviledged information about the event, and assigns to it a very low probability.</p><p>Ideally, we would like the aggregate probability to be reasonably sensitive to the strength of the evidence provided by the insider expert - if the insider assigns a probability of 1 in 1000 the outcome should be meaningfully different than if the insider assigns a probability of 1 in 10,000 <a href=\"https://forum.effectivealtruism.org/posts/sMjcjnnpoAQCcedL2/when-pooling-forecasts-use-the-geometric-average-of-odds#Footnotes\">[9]</a>.</p><p>The arithmetic mean of probabilities does not achieve this - in both cases the pooled probability is around&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(10\\% + 1/1,000)/2 \\approx (10\\% + 1/10,000)/2 \\approx 5.00\\%\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">000</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u2248</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">000</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u2248</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5.00</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>. The uncertain prediction has effectively overwritten the information in the more precise prediction.</p><p>The geometric mean of odds works better in this situation. We have that&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"[(1:9) \\times (1:999)]^{1/2} \\approx 1:95\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">9</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">999</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u2248</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">95</span></span></span></span></span></span></span>, while&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"[(1:9) \\times (1:9999)]^{1/2} \\approx 1:300\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">9</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">9999</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u2248</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">300</span></span></span></span></span></span></span>. Those correspond respectively to probabilities of 1.04% and 0.33% - showing the greater sensitivity to the evidence the insider brings to the table.</p><p>See <a href=\"https://faculty.wharton.upenn.edu/wp-content/uploads/2015/07/2015---two-reasons-to-make-aggregated-probability-forecasts_1.pdf\">(Baron et al, 2014)</a> for more discussion on the distortive effects of the arithmetic mean of probabilities and other aggregates.</p></blockquote><p>I have <a href=\"https://forum.effectivealtruism.org/posts/acREnv2Z5h4Fr5NWz/my-current-best-guess-on-how-to-aggregate-forecasts?commentId=RufbbgGcrX9gtmtd6\">asked</a> Jaime Sevilla to share his thoughts. Thanks for raising this important point!</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndqpqh547i2j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdqpqh547i2j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For the question \"What is the unconditional probability of London being hit with a nuclear weapon in October?\", the 7 forecasts were 0.01, 0.00056, 0.001251, 10^-8, 0.000144, 0.0012, and 0.001. The largest of these is 1 M (= 0.01/10^-8) times the smallest.</p></div></li></ol>", "parentCommentId": "wPHy6s2xXoh6BAcXm", "user": {"username": "vascoamaralgrilo"}}, {"_id": "pey3X2kf7inxpk6bk", "postedAt": "2024-01-29T12:49:56.496Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Thanks for clarifying!</p><blockquote><p>First, as a rule of thumb, <i>coeteris paribus</i>, a decrease in the avg x-risk implies an increase in the expected duration of human survival - so yielding a proportionally higher expected value for reducing x-risk. I think this can be inferred from Thorstad's toy model in&nbsp;<a href=\"https://philarchive.org/rec/THOERP\">Existential risk pessimism and the time of perils</a>. So, if something reduces x-risk by 100x, I'm assuming it doesn't make much difference, from my POV, if the prior x-risk is 1% or 10^-10 - because I'm assuming that EV will stay the same. This is not always true; I should have clarified this.</p></blockquote><p>I think you mean that the expected value of the future will not change much if one decreases the nearterm annual existential risk without decreasing the longterm annual existential risk.</p>", "parentCommentId": "AQzQES552apvXjgci", "user": {"username": "vascoamaralgrilo"}}, {"_id": "g6KCuurh8MuD7Ai7e", "postedAt": "2024-01-29T20:13:13.803Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Thanks for the detailed reply and for asking Jaime Sevilla!<br><br>FWIW on the Samotsvety nuclear forecasts, I'm pretty intuitively scared by that aggregation methodology and spread of numbers (as people discussed in comments on that post).</p>", "parentCommentId": "GyDBhQQnMyFxsdQcM", "user": {"username": "Ryan Greenblatt"}}, {"_id": "HXZapswLYtDcWz9zD", "postedAt": "2024-01-30T01:33:50.781Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>You are welcome!</p><blockquote><p>Thanks for the detailed reply and for asking Jaime Sevilla!</p></blockquote><p>For reference, here are Jaime's <a href=\"https://forum.effectivealtruism.org/posts/acREnv2Z5h4Fr5NWz/my-current-best-guess-on-how-to-aggregate-forecasts?commentId=LH6BCCjF6TPxPzZrB\">thoughts</a>:</p><blockquote><p>Interesting case. I can see the intuitive case for the median.</p><p>I think the mean is more appropriate - in this case, what this is telling you is that your uncertainty is dominated by the possibility of a fat tail, and the priority is ruling it out.</p></blockquote><p>I am still standing by the median. I think using a weighted mean could be reasonable, but not a simple one. Even if all distributions should have the same weight on priors, and the update to the weights based on the fit to the data is pretty negligible, I would say one should put less weight on predictions further away from the median. This effect can be simply captured by aggregating the predictions using the median<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7dq5ku1auu5\"><sup><a href=\"#fn7dq5ku1auu5\">[1]</a></sup></span>.</p><p>I have now <a href=\"https://docs.google.com/spreadsheets/d/1bhqZRZ1KOndD3IqSin-TPtsqB9uk2Kvj8_FCB8vBnHs/edit#gid=531904533&amp;range=B1:B2\">done</a> a graph illustrating how the mean ignores information from extremely low predictions:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/ricwcqzris15oodcbj49\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/xhbpgmkfgqyjk6x1vwpq 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/uovmjnwmoftvyldne2fu 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/exid47pzbghbclvqb4wt 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/l3utrho6c62vlnidtfmd 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/nf6jcm0sgv6lbnx3zwma 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/kmh7pimlkyqbmheh4tgo 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/akf35aazyxx7qqbhktxe 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/e7zm6wscrdlsav3hu3x2 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/hmad0ny2blcfpxmz70i5 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/mvyjqss6k5xgb6ogypgj 1200w\"></figure><p>More importantly, I have now <a href=\"https://docs.google.com/spreadsheets/d/1bhqZRZ1KOndD3IqSin-TPtsqB9uk2Kvj8_FCB8vBnHs/edit#gid=1478327613&amp;range=A18\">noticed</a> the high annual probabilities of extinction are associated with arguably unreasonably high probability of extinction conditional on an annual population loss of at least 10 %.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/lt9fec6ekr7xtcqz92gp\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/jfcqc0cvl9a3emmth9q2 130w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/xjfla2u1uquqdnutmlbu 260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/ekmq1374hslbpr8japlw 390w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/qfergcjliyvxsw8w6utd 520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/odrqebkefcokqvqfxsqy 650w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/nyn9rstqdhfeehqny5tf 780w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/rsih8atwf3k2f29jdpfl 910w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/ewdourub4ajnfgx8uek2 1040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/b4eqjg6wvug1i9mg3mg4 1170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HXZapswLYtDcWz9zD/jtcjlwi1uqh1qcymobkk 1201w\"></figure><p>For the annual probability of a war causing human extinction to be at least 0.0122 %, which is similar to the \"0.0124 %/year\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref96xa9rvetj7\"><sup><a href=\"#fn96xa9rvetj7\">[2]</a></sup></span>&nbsp;I inferred for Stephen's results, the probability of a war causing human extinction conditional on it causing an annual population loss of at least 10 % has to be at least 14.8 %. I see this as super high.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7dq5ku1auu5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7dq5ku1auu5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Or, if there were no null predictions, the geometric mean or probability linked to the geometric mean of the odds.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn96xa9rvetj7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref96xa9rvetj7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I have added this to the post now. Previously, I only had Stephen's extinction risk per war.</p></div></li></ol>", "parentCommentId": "g6KCuurh8MuD7Ai7e", "user": {"username": "vascoamaralgrilo"}}, {"_id": "BdP5DoDZ4emj2wddg", "postedAt": "2024-01-30T08:10:24.021Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>\"I inferred for Stephen's results, the probability of a war causing human extinction conditional on it causing an annual population loss of at least 10 % has to be at least 14.8 %.\"</p><p>This is interesting! I hadn't thought about it that way and find this framing intuitively compelling.&nbsp;</p><p>That does seem high to me, though perhaps not ludicrously high. Past events have probably killed at least 10% of the global population, WWII was within an order of magnitude of that, and we've increased out warmaking capacity since then. So I think it would be reasonable to put that annual chance of a war killing at least 10% of the global population at at least 1%.&nbsp;</p><p>That could give some insight into the extinction tail, perhaps implying that my estimate was about 10x too high. That would still make it importantly wrong, but less egregiously than the many orders of magnitude you estimate in the main post?</p>", "parentCommentId": "HXZapswLYtDcWz9zD", "user": {"username": "Stephen Clare"}}, {"_id": "GohGBeg43zgpJcdDa", "postedAt": "2024-01-30T12:30:03.716Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Thanks for jumping in, Stephen!</p><blockquote><p>That does seem high to me, though perhaps not ludicrously high. Past events have probably killed at least 10% of the global population, WWII was within an order of magnitude of that, and we've increased out warmaking capacity since then. So I think it would be reasonable to put that annual chance of a war killing at least 10% of the global population at at least 1%.</p></blockquote><p>Note the 14.8 % I mentioned in my last comment refers to \"the probability of a war causing human extinction conditional on it causing an annual population loss of at least 10 %\", not to the annual probability of a war causing a population loss of 10 %. I think 14.8 % for the former is super high<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcuhgh5men1u\"><sup><a href=\"#fncuhgh5men1u\">[1]</a></sup></span>, but I should note the Metaculus' community might find it reasonable:</p><ul><li>It is predicting:<ul><li>A <a href=\"https://www.metaculus.com/questions/1585/ragnar%25C3%25B6k-question-series-if-a-nuclear-catastrophe-occurs-will-it-reduce-the-human-population-by-95-or-more/\">5 %</a> chance of a nuclear catastrophe causing a 95 % population loss conditional on it causing a population loss of at least 10 %.</li><li>A <a href=\"https://www.metaculus.com/questions/2514/ragnar%25C3%25B6k-question-series-if-a-global-biological-catastrophe-occurs-will-it-reduce-the-human-population-by-95-or-more/\">10 %</a> chance of a bio catastrophe causing a 95 % population loss conditional on it causing a population loss of at least 10 %.</li></ul></li><li>I think a nuclear or bio catastrophe causing a 95 % population loss would still be far from causing extinction, so I could still belive the above suggest the probability of a nuclear or bio war causing extinction conditional on it causing a population loss of at least 10 % is much lower than 5 % and 10 %, and therefore much lower than 14.8 % too.</li><li>However, the Metaculus' community may find extinction is fairly likely conditional on a 95 % population loss.</li></ul><blockquote><p>That could give some insight into the extinction tail, perhaps implying that my estimate was about 10x too high. That would still make it importantly wrong, but less egregiously than the many orders of magnitude you estimate in the main post?</p></blockquote><p>Note \"the probability of a war causing human extinction conditional on it causing an annual population loss of at least 10 %\" increases quite superlinearly with the annual probability of a war causing human extinction (see graph in my last comment). So this will be too high by more than 1 OOM if the 14.8 % I mentioned is high by 1 OOM. To be precise, for the best fit distribution with a \"probability of a war causing human extinction conditional on it causing an annual population loss of at least 10 %\" of 1.44 %, which is roughly 1 OOM below 14.8 %, the annual probability of a war causing human extinction is 3.41*10^-7, i.e. 2.56 (= log10(1.24*10^-4/(3.41*10^-7))) OOMs lower. In reality, I suspect 14.8 % is high by many OOMs, so an astronomically low prior still seems reasonable to me.</p><p>I have just finished a <a href=\"https://docs.google.com/document/d/1rr68EOgP6t3XOaAwPlwlAl4zPK2_uEz644IGEEihGbQ/edit?usp=sharing\">draft</a> where I get an insive view estimate of 5.53*10^-10 for the nearterm annual probability of human extinction from nuclear war, which is not too far from the best guess prior I present in the post of 6.36*10^-14. Comments are welcome, but no worries if you have other priorities! Update: I have now <a href=\"https://forum.effectivealtruism.org/posts/6KNSCxsTAh7wCoHko/nuclear-war-tail-risk-has-been-exaggerated\">published</a> the post.</p><p>I understand war extinction risk may be majorly driven by AI and bio risk rather than nuclear war. However, I have sense this is informed to a significant extent by Toby's <a href=\"https://forum.effectivealtruism.org/posts/Z5KZ2cui8WDjyF6gJ/some-thoughts-on-toby-ord-s-existential-risk-estimates\">estimates</a> for existential risk given in <a href=\"https://theprecipice.com/\">The Precipice</a>, whereas I have found them consistently much higher than my estimates for extinction risk for the matters I have investigated. For example, in the draft I linked above, I say it is plausible extinction risk from nuclear war is similar to that from asteroids and comets.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncuhgh5men1u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcuhgh5men1u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I updated \"quite high\" in my last comment to \"super high\".</p></div></li></ol>", "parentCommentId": "BdP5DoDZ4emj2wddg", "user": {"username": "vascoamaralgrilo"}}, {"_id": "bidQBbMfXLXscJYo9", "postedAt": "2024-01-30T15:47:27.014Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Thanks again for this post, Vasco, and for sharing it with me for discussion beforehand. I really appreciate your work on this question. It's super valuable to have more people thinking deeply about these issues and this post is a significant contribution.</p><p>The headline of my response is I think you're pointing in the right direction and the estimates I gave in my original post are too high. But I think you're overshooting and the probabilities you give here seem too low.</p><p>I have a couple of points to expand on; please do feel free to respond to each in individual comments to facilitate better discussion!</p><p>To summarize, my points are:</p><ol><li>I think you're right that my earlier estimates were too high; but I think this way overcorrects the other way.</li><li>There are some issues with using the historical war data</li><li>I'm still a bit confused and uneasy about your choice to use proportion killed per <i>year</i> rather than proportion or total killed per <i>war</i>.</li><li>I think your preferred estimate is so infinitesimally small that something must be going wrong.</li></ol><p><strong>First, you're very likely right that my earlier estimates were too high.</strong> Although I still put some credence in a power law model, I think I should have incorporated more model uncertainty, and noted that other models would imply (much) lower chances of extinction-level wars.&nbsp;</p><p>I think <a href=\"https://forum.effectivealtruism.org/users/ryan-greenblatt?mention=user\">@Ryan Greenblatt</a> has made good points in other comments so won't belabour this point other than to add that I think some method of using the mean, or geometric mean, rather than median seems reasonable to me when we face this degree of model uncertainty.</p><p>One other minor point here: a reason I still like the power law fit is that there's at least some theoretical support for this distribution (as Bear wrote about in <i>Only the Dead</i>). Whereas I haven't seen arguments that connect other potential fits to the theory of the underlying data generating process. This is pretty speculative and uncertain, but is another reason why I don't want to throw away the power law entirely yet.</p><p><strong>Second, I'm &nbsp;still skeptical that the historical war data is the \"right\" prior to use.</strong> It may be \"a\" prior but your title might be overstating things. This is related to Aaron's point you quote in footnote 9, about assuming wars are IID over time. I think maybe we can assume they're I (independent), but not that they're ID (identically distributed) over time.&nbsp;</p><p>I think we can be pretty confident that WWII was so much larger than other wars not just randomly, but in fact because globalization<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflef7ehyvw9\"><sup><a href=\"#fnlef7ehyvw9\">[1]</a></sup></span>&nbsp;and new technologies like machine guns and bombs shifted the distribution of potential war outcomes. And I think similarly that distribution has shifted again since. Cf. my discussion of war-making capacity <a href=\"https://forum.effectivealtruism.org/posts/mBM4y2CjfYef4DGcd/modelling-great-power-conflict-as-an-existential-risk-factor#Will_future_weapons_be_even_worse_\">here</a>. Obviously past war size isn't completely irrelevant to the potential size of current wars, but I do think not adjusting for this shift at all likely biases your estimate down.</p><p><strong>Third, I'm still uneasy about your choice to use annual proportion of population killed rather than number of deaths per war.</strong> This is just very rare in the IR world. I don't know enough about how the COW data is created to assess it properly. Maybe one problem here is that it just clearly breaks the IID assumption. If we're modelling each year as a draw, then since major wars last more than a year the probabilities of subsequent draws are clearly dependent on previous draws. Whereas if we just model each war as a whole as a draw (either in terms of gross deaths or in terms of deaths as a proportion of world population), then we're at least closer to an IID world. Not sure about this, but it feels like it also biases your estimate down.</p><p><strong>Finally, I'm a bit suspicious of infinitesimal probabilities due to the strength they give the prior.</strong> They imply we'd need enormously strong evidence to update much at all in a way that seems unreasonable to me.&nbsp;</p><p>Let's take your preferred estimate of an annual probability of \"6.36*10^-14\". That's a 1 in 15,723,270,440,252 chance. That is, 1 in 15 trillion years.</p><p>I look around at the world and I see a nuclear-armed state fighting against a NATO-backed ally in Ukraine; I see conflict once again spreading throughout the Middle East; I see the US arming and <i>perhaps</i> preparing to defend Taiwan against China, which is governed by a leader who claims to consider reunification both inevitable and an existential issue for his nation.&nbsp;</p><p>And I see nuclear arsenals that still top 12,000 warheads and growing; I see ongoing bioweapons research powered by ever-more-capable biotechnologies; and I see obvious military interest in developing AI systems and autonomous weapons.</p><p>This does not seem like a situation that only leads to total existential destruction once every 15 trillion years.</p><p>I know you're only talking about the prior, but your preferred estimate implies we'd need a galactically-enormous update to get to a posterior probability of war x-risk that seems reasonable. So I think something might be going wrong. Cf. some of Joe's discussion of settling on infinitesimal priors <a href=\"https://www.lesswrong.com/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk\">here</a>.</p><p>All that said, let me reiterate that <strong>I really appreciate this work!</strong></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlef7ehyvw9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflef7ehyvw9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>What I mean here is that we should adjust somewhat for the fact that world wars are even possible nowadays. WWII was fought across three or four continents; that just couldn't have happened before the 1900s. But about 1/3 of the COW dataset is for pre-1900 wars.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Stephen Clare"}}, {"_id": "ZRhnJtFG5rLZcAgHg", "postedAt": "2024-01-30T19:34:56.809Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Thanks for all the feedback, and early work on the topic, Stephen! I will reply to your points in different comments as you suggested.</p><blockquote><p><strong>First, you're very likely right that my earlier estimates were too high.</strong> Although I still put some credence in a power law model, I think I should have incorporated more model uncertainty, and noted that other models would imply (much) lower chances of extinction-level wars.&nbsp;</p></blockquote><p>To be fair, you and Rani had a section on <a href=\"https://forum.effectivealtruism.org/posts/PyZCqLrDTJrQofEf7/how-bad-could-a-war-get#Breaking_the_law\">breaking the [power] law</a> where you say other distributions would fit the data well (although you did not discuss the implications for tail risk):</p><blockquote><p>First, and most importantly,&nbsp;<strong>only two papers in the review also check whether other distributions might fit the same data.</strong>&nbsp;<a href=\"https://arxiv.org/pdf/0706.1062.pdf\"><u>Clauset, Shalizi, and Newman (2009)</u></a> consider four other distributions,<a href=\"https://forum.effectivealtruism.org/posts/PyZCqLrDTJrQofEf7/how-bad-could-a-war-get#fn9goblfdkit8\"><sup>[3]</sup></a>&nbsp;while&nbsp;<a href=\"https://www.tandfonline.com/doi/full/10.1080/10242694.2015.1025486\"><u>Rafael Gonz\u00e1lez-Val (2015)</u></a> also considers a lognormal fit. <strong>Both papers find that alternative distributions also fit the Correlates of War data well.</strong> In fact, when Clauset, Shalizi, and Newman compare the fit of the different distributions, they find no reason to prefer the power law.<a href=\"https://forum.effectivealtruism.org/posts/PyZCqLrDTJrQofEf7/how-bad-could-a-war-get#fnvwbt23seeds\"><sup>[4]</sup></a></p></blockquote><p>With respect to the below, I encourage readers to check the respective <a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors?commentId=duBuYbEzps7SdW7W2\">thread</a> for context.</p><blockquote><p>I think <a href=\"https://forum.effectivealtruism.org/users/ryan-greenblatt?mention=user\">@Ryan Greenblatt</a> has made good points in other comments so won't belabour this point other than to add that I think some method of using the mean, or geometric mean, rather than median seems reasonable to me when we face this degree of model uncertainty.</p></blockquote><p>As I explained in the <a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors?commentId=duBuYbEzps7SdW7W2\">thread</a>, I do not think a simple mean is appropriate. That being said, the mean could also lead to astronomically low extinction risk. With the methodology I followed, one has to look into at least 34 distributions for the mean not to be astronomically low. I have just obtained the following graph in <a href=\"https://docs.google.com/spreadsheets/d/1bhqZRZ1KOndD3IqSin-TPtsqB9uk2Kvj8_FCB8vBnHs/edit#gid=531904533&amp;range=A1:A2\">this</a> tab:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZRhnJtFG5rLZcAgHg/trqy7fcdxj03ro1s0czb\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZRhnJtFG5rLZcAgHg/c7xygvj34kghsp33kryx 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZRhnJtFG5rLZcAgHg/yudufxc7btxxsfstjtjg 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZRhnJtFG5rLZcAgHg/dakrptupvhwsigjv7adm 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZRhnJtFG5rLZcAgHg/vgak4vdgodcw6bdulvdt 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZRhnJtFG5rLZcAgHg/lwlajn7javpcpsjludfy 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZRhnJtFG5rLZcAgHg/htyfstm0omyu3pvas9yr 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZRhnJtFG5rLZcAgHg/cnlutolcmlpegzqtc0if 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZRhnJtFG5rLZcAgHg/b6qpb6ra6ibtkju0olrh 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZRhnJtFG5rLZcAgHg/hv3pywhjh4ovi2qpf4bw 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ZRhnJtFG5rLZcAgHg/v4dj1c29yqn6agbz4etc 1200w\"></figure><p>You suggested using the geometric mean, but it is always 0 given the null annual extinction risk for the top distribution, so it does not show up in the above graph. The median only is non-null for at least 84 distributions. I looked into all the 111 types of <a href=\"https://docs.scipy.org/doc/scipy/reference/stats.html%23probability-distributions\"><u>distributions</u></a>&nbsp;available in SciPy, since I wanted to minimise cherry-picking as much as possible, but typical analyses only study 1 or a few. So it would have been easy to miss on noticing that the mean could lead to a much higher extinction risk.</p><p>Incidentally, the steep increase in the red line of the graph above illustrates one worry I have about using the mean I had alluded to in the <a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors?commentId=duBuYbEzps7SdW7W2\">thread</a>. The simple mean is not resistant to outliers, in the sense that these are overweighted<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1vc77saufc8j\"><sup><a href=\"#fn1vc77saufc8j\">[1]</a></sup></span>. I have a strong intuition that, given 33 models outputting an annual extinction risk between 0 and 9.07*10^-14, with mean 1.15*10^-13 among them, one should not update upwards by 8 OOMs to an extinction risk of 6.45*10^-6 after integrating a 34th model outputting an annual extinction risk of <a href=\"https://docs.google.com/spreadsheets/d/1bhqZRZ1KOndD3IqSin-TPtsqB9uk2Kvj8_FCB8vBnHs/edit#gid=47587321&amp;range=M40\">0.0219 %</a> (similar to yours of 0.0124 %). Under these conditions, I think one should put way less weight on the 34th model (maybe roughly no weight?). As Holden Karnofsky discusses in the post <a href=\"https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">Why we can\u2019t take expected value estimates literally (even when they\u2019re unbiased)</a>:</p><blockquote><p>An EEV [explicit expected value] approach to this situation [analogous to using the simple mean in our case] might say, \u201cEven if there\u2019s a 99.99% chance that the estimate [of high extinction risk] is completely wrong and that the value of Action A is 0, there\u2019s still an 0.01% probability that Action A has a value of X. Thus, overall Action A has an expected value of at least 0.0001X; the greater X is, the greater this value is, and if X is great enough [if there are a few models outputting a high enough extinction risk] then, then you should take Action A unless you\u2019re willing to bet at enormous odds that the framework is wrong.\u201d</p><p>However, the same formula discussed above indicates that Action X actually has an expected value \u2013 after the Bayesian adjustment \u2013 of X/(X^2+1), or <i>just under 1/X</i>. In this framework, <i>the greater X is [the higher the extinction risk of a poorly calibrated model], the <strong>lower</strong> the expected value of Action A [the lower the product between the weight a poorly calibrated model should receive and its high extinction risk].</i> This syncs well with my intuitions: if someone threatened to harm one person unless you gave them $10, this ought to carry more weight (because it is more plausible in the face of the \u201cprior\u201d of life experience) than if they threatened to harm 100 people, which in turn ought to carry more weight than if they threatened to harm 3^^^3 people (I\u2019m using 3^^^3 here as a <a href=\"http://lesswrong.com/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">representation of an unimaginably huge number</a>).</p></blockquote><p>Ideally, one would do more research to find how much weight each distribution should receive. In the absence of that, I think using the median is a simple way to adequately weight outliers.</p><blockquote><p>One other minor point here: a reason I still like the power law fit is that there's at least some theoretical support for this distribution (as Bear wrote about in <i>Only the Dead</i>).</p></blockquote><p>The worry here is that the theoretical support for using a power law breaks at some point. According to a power law, the probability p1 of at least 8 billion deaths conditional on 800 M deaths is the same as the probability p2 of 80 billion deaths conditional on 8 billion deaths. However, p1 is low<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1d4ih6xztm7\"><sup><a href=\"#fn1d4ih6xztm7\">[2]</a></sup></span>&nbsp;whereas p2 is 0.</p><blockquote><p>Whereas I haven't seen arguments that connect other potential fits to the theory of the underlying data generating process.</p></blockquote><p>There is this argument I mentioned in the post:</p><blockquote><p>In addition, according to extreme value theory (<a href=\"https://en.wikipedia.org/wiki/Extreme_value_theory\"><u>EVT</u></a>), the right tail <a href=\"https://www.openphilanthropy.org/research/geomagnetic-storms-using-extreme-value-theory-to-gauge-the-risk/\"><u>should</u></a>&nbsp;follow a <a href=\"https://en.wikipedia.org/wiki/Generalized_Pareto_distribution\"><u>generalised Pareto</u></a><a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors#fndrtzvuenji5\"><sup>[10]</sup></a>, and the respective best fit distribution resulted in an extinction risk of exactly 0<a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors#fnbm12h8dlhwo\"><sup>[11]</sup></a>&nbsp;(R^2 of 99.8 %). Like I <a href=\"https://forum.effectivealtruism.org/posts/aSzxoj7irC5jNHceB/how-likely-is-world-war-iii?commentId%3DvRYHa7X4yxPuTdxvz\"><u>anticipated</u></a>, the best fit Pareto (power law) resulted in a higher risk, 0.0122 % (R^2 of 99.7 %), i.e. 98.4 % (= 1.22*10^-4/(1.24*10^-4)) of <u>Stephen\u2019s</u>&nbsp;0.0124 %. Such remarkable agreement means the extinction risk for the best fit Pareto is essentially the same regardless of whether it is fitted to the top 10 % logarithm of the annual war deaths of combatants as a fraction of the global population (as I did), or to the war deaths of combatants per war (as implied by Stephen using Bear\u2019s estimates). I guess this qualitatively generalises to other types of distributions. In any case, I&nbsp;<a href=\"https://docs.google.com/document/d/14_5GnGMD_hwGjEITRLuBeV4sfRG1tI5xmAb1bqW1hEc/edit#heading=h.jqgyd0hhwqld\"><u>would</u></a> rather follow my approach.</p></blockquote><p>I should note I have just updated in the post the part of the sentence above after \"i.e.\". Previously, I was comparing the annual war extinction risk of my best fit power law with your extinction risk per war under the \"constant risk hypothesis\". Now I am making the correct comparison with your annual war extinction risk.</p><blockquote><p>This is pretty speculative and uncertain, but is another reason why I don't want to throw away the power law entirely yet.</p></blockquote><p>Just to clarify, I am still accounting for the results of the power law in my best guess. However, since I am using the median to aggregate the various estimates of the extinction risk, I get astronomically low extinction risk even accounting for distributions predicting super high values.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1vc77saufc8j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1vc77saufc8j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I have added this point to the post.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1d4ih6xztm7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1d4ih6xztm7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I would argue not just low, but super low.</p></div></li></ol>", "parentCommentId": "bidQBbMfXLXscJYo9", "user": {"username": "vascoamaralgrilo"}}, {"_id": "FZuooyftAP2ST5sxQ", "postedAt": "2024-01-30T23:44:19.867Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Thanks Vasco! I'll come back to this to respond in a bit more depth next week (this is a busy week).</p><p>In the meantime, curious what you make of my point that setting a prior that gives only a 1 in 15 trillion chance of experiencing an extinction-level war in any given year seems wrong?</p>", "parentCommentId": "ZRhnJtFG5rLZcAgHg", "user": {"username": "Stephen Clare"}}, {"_id": "2Kt9siq4PgwyoNDXg", "postedAt": "2024-01-31T00:05:45.870Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<blockquote><p><strong>Second, I'm &nbsp;still skeptical that the historical war data is the \"right\" prior to use.</strong> It may be \"a\" prior but your title might be overstating things. This is related to Aaron's point you quote in footnote 9, about assuming wars are IID over time. I think maybe we can assume they're I (independent), but not that they're ID (identically distributed) over time.</p></blockquote><p>Historical war deaths seem to me like the most natural prior to assess future war deaths. I guess you consider it a decent prior too, as you relied on historical war data to get your extinction risk, but maybe you have a better reference class in mind?</p><p>Aron's point about annual war deaths not being IID over time does not have a clear impact on my estimate for the annual extinction risk. If one thinks war deaths have been decreasing/increasing, then one should update towards a lower/higher extinction risk. However:</p><ul><li>There is not an obvious trend in the past 600 years (see last graph in the post).</li><li>My impression is that there is lots of debate in the literature, and that the honest conclusion is that we do not have enough data to establish a clear trend.</li></ul><p>I think Aron's paper (<a href=\"https://www.science.org/doi/10.1126/sciadv.aao3580\"><u>Clauset 2018</u></a>) agrees with the above:</p><blockquote><p>Since 1945, there have been relatively few large interstate wars, especially compared to the preceding 30 years, which included both World Wars. This pattern, sometimes called the long peace, is highly controversial. Does it represent an enduring trend caused by a genuine change in the underlying conflict-generating processes? Or is it consistent with a highly variable but otherwise stable system of conflict? Using the empirical distributions of interstate war sizes and onset times from 1823 to 2003, we parameterize stationary models of conflict generation that can distinguish trends from statistical fluctuations in the statistics of war. These models indicate that both the long peace and the period of great violence that preceded it are not statistically uncommon patterns in realistic but stationary conflict time series.</p></blockquote><p>I think there is also another point Aron was referring to in <a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors#fn3ki69cyqac9\">footnote 9</a> (emphasis mine):</p><blockquote><p>you have a deeper assumption that is quite questionable, which is whether events are plausibly iid [<a href=\"https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables\"><u>independent and identically distributed</u></a>] over such a long time scale. This is where the deep theoretical understanding from the literature on war is useful, and in my 2018 paper [<a href=\"https://www.science.org/doi/10.1126/sciadv.aao3580\"><u>Clauset 2018</u></a>], my Discussion section delves into the implications of that understanding <strong>for making such long term </strong>and large-size<strong> extrapolations</strong>.</p></blockquote><p>Relevant context for what I highlighted above:</p><blockquote><p><a href=\"https://www.science.org/doi/10.1126/sciadv.aao3580\"><u>Clauset 2018</u></a>&nbsp;did estimate a 50 % probability of a war causing 1 billion battle deaths<a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors#fnalywhv9w3e\"><sup>[15]</sup></a>&nbsp;in the next 1,339 years (see \u201cThe long view\u201d), which is close to my pessimistic scenario [see post for explanation]</p></blockquote><p>I think Aron had the above in mind, and therefore was worried about assuming wars are IID over a long time, because this affects how much time it would take in expectation for a war to cause extinction. However, in my post I am estimating this time, but rather the nearterm annual probability of a war causing extinction, which does not rely on assumptions about whether wars will be IID over a long time horizon. I alluded to this in footnote 9:</p><blockquote><p>Assuming wars are IID over a long time scale would be problematic if one wanted to estimate the time until a war caused human extinction, but I do not think it is an issue to estimate the nearterm annual extinction risk.</p></blockquote><p>It is possible you missed this part, because it was not in the early versions of the draft.</p><blockquote><p>I think we can be pretty confident that WWII was so much larger than other wars not just randomly, but in fact because globalization<a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors#fnlef7ehyvw9\"><sup>[1]</sup></a>&nbsp;and new technologies like machine guns and bombs shifted the distribution of potential war outcomes.</p></blockquote><p>Some thoughts on the above:</p><ul><li>What directy matters to assess the annual probability of a war causing human extinction is not war deaths, but annual war deaths as a fraction of the global population. For instance, one can have increasing war deaths with constant annual probability of a war causing human extinction if wars become increasinly long and population increases. Hopefully not, but it is possible wars in the far future will routinely wipe out e.g. trillions of digital minds while not posing any meaningful risk of wiping out all digital minds due to the existence of a huge population.</li><li>It is unclear to me whether globalisation makes wars larger. For example, globalisation is associated with an expansion of international trade, and this can explain the \"durable peace hypothesis\" (see <a href=\"https://www.pnas.org/doi/full/10.1073/pnas.1520970112\">Jackson 2015</a>).</li><li>In agreement with <a href=\"https://en.wikipedia.org/wiki/Deterrence_theory\">deterrence theory</a>, I believe greater potential to cause damage may result in less expected damage, although I am personally not convinced of this.</li><li>Even if globalisation makes wars larger, it could make them less frequent too, such that the expected annual damage decreases, and so does the annual probability of one causing extinction.</li></ul><blockquote><p>And I think similarly that distribution has shifted again since. Cf. my discussion of war-making capacity <a href=\"https://forum.effectivealtruism.org/posts/mBM4y2CjfYef4DGcd/modelling-great-power-conflict-as-an-existential-risk-factor#Will_future_weapons_be_even_worse_\">here</a>. Obviously past war size isn't completely irrelevant to the potential size of current wars, but I do think not adjusting for this shift at all likely biases your estimate down.</p></blockquote><p>Related to the above, I <a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors?commentId=dB83FR2Te84troPwK\">commented</a> that:</p><blockquote><p>I assume increasing capability to cause damage is the main reason for people arguing that future wars would belong to a different category. Yet:</p><ul><li>I think war capabilities have been decreasing or not changing much in the last few decades:<ul><li>\"Nuclear risk has been decreasing. The estimated destroyable <a href=\"https://ourworldindata.org/grapher/estimated-destroyable-area-by-nuclear-weapons-deliverable-in-first-strike\">area</a> by nuclear weapons deliverable in a first strike has decreased 89.2 % (= 1 - 65.2/601) since its peak in 1962\" (see 1st graph below).</li><li>Military expenditure as a fraction of global GDP <a href=\"https://data.worldbank.org/indicator/MS.MIL.XPND.GD.ZS\">has</a> decreased from 1960 to 2000, and been fairly constant since then (see 2nd graph below).</li></ul></li><li>Taking a broader view, war capabilities do have been increasing, but there is not a clear trend in the deaths in conflicts as a fraction of the global population since 1400 (see last figure in the post).</li><li>Increases in the capability to cause damage are usually associated with increases in the capability to prevent damage, which I guess explains what I said just above, so one should not forecast future risk based on just one side alone.</li></ul></blockquote><blockquote><figure class=\"media\"><div data-oembed-url=\"https://ourworldindata.org/grapher/estimated-destroyable-area-by-nuclear-weapons-deliverable-in-first-strike\">\n\t\t\t\t\t<div data-owid-slug=\"estimated-destroyable-area-by-nuclear-weapons-deliverable-in-first-strike\" class=\"owid-preview\">\n\t\t\t\t\t\t<iframe src=\"https://ourworldindata.org/grapher/estimated-destroyable-area-by-nuclear-weapons-deliverable-in-first-strike\">\n\t\t\t\t\t</iframe></div>\n\t\t\t\t</div></figure><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/ved7dn0d2v9r1o2r48sz\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/c6ldlkt4yfjjcf9ovudc 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/gyaabexr9reoqylg5edx 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/rljkmns0fq8aj2qry3lo 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/ytt8ct4znokacwnpxpzo 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/s7f5x0gugsxgicqkewim 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/oymev6mjuy7tpxpnmaaf 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/zgevkxlftcphazszswvw 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/fuednoshin1lpszsfouo 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/bauyipwxqq6yaq78qipx 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/dB83FR2Te84troPwK/ampmbfby3anb2w8qc3um 1957w\"></p></blockquote>", "parentCommentId": "bidQBbMfXLXscJYo9", "user": {"username": "vascoamaralgrilo"}}, {"_id": "kLzXQjrPw6nJzueRw", "postedAt": "2024-01-31T00:39:45.991Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<blockquote><p><strong>Third, I'm still uneasy about your choice to use annual proportion of population killed rather than number of deaths per war.</strong> This is just very rare in the IR world.</p></blockquote><p>Looking into annual war deaths as a fraction of the global population is relevant to estimate extinction risk, but the <a href=\"https://en.wikipedia.org/wiki/International_relations\">international relations</a> world is not focussing on this. For reference, here is what I said about this matter in the post:</p><blockquote><p>Stephen commented I had better follow the typical approach of modelling war deaths, instead of annual war deaths as a fraction of the global population, and then getting the probability of human extinction from the chance of war deaths being at least as large as the global population. I think my approach is more appropriate, especially to estimate tail risk. There is human extinction if and only if annual war deaths as a fraction of the global population are at least 1. In contrast, war deaths as a fraction of the global population in the year the war started being at least 1 does not imply human extinction. Consider a war lasting for the next 100 years totalling 8 billion deaths. The war deaths as a fraction of the global population in the year the war started would be 100 %, which means such a war would imply human extinction under the typical approach. Nevertheless, this would only be the case if no humans were born in the next 100 years, and new births are not negligible. In fact, the global population increased thanks to these during the years with the most annual war deaths of combatants in the data I used:</p><ul><li>From 1914 to 1918 (years of <a href=\"https://en.wikipedia.org/wiki/World_War_I\"><u>World War 1</u></a>), they were 9.28 M, 0.510 % (= 9.28/(1.82*10^3)) of the global population in 1914, but the global population increased 2.20 % (= 1.86/1.82 - 1) during this period.</li><li>From 1939 to 1945 (years of <a href=\"https://en.wikipedia.org/wiki/World_War_II\"><u>World War 2</u></a>), they were 17.8 M, 0.784 % (= 17.8/(2.27*10^3)) of the global population in 1939, but the global population increased 4.85 % (= 2.38/2.27 - 1) during this period.</li></ul></blockquote><p>Do you have any thoughts on the above?</p><blockquote><p>I don't know enough about how the COW data is created to assess it properly. Maybe one problem here is that it just clearly breaks the IID assumption. If we're modelling each year as a draw, then since major wars last more than a year the probabilities of subsequent draws are clearly dependent on previous draws. Whereas if we just model each war as a whole as a draw (either in terms of gross deaths or in terms of deaths as a proportion of world population), then we're at least closer to an IID world. Not sure about this, but it feels like it also biases your estimate down.</p></blockquote><p>It is unclear to me whether this is a major issue, because both methodolies lead to essentially the same annual war extinction risk for a power law:</p><blockquote><p>Like I <a href=\"https://forum.effectivealtruism.org/posts/aSzxoj7irC5jNHceB/how-likely-is-world-war-iii?commentId%3DvRYHa7X4yxPuTdxvz\"><u>anticipated</u></a>, the best fit Pareto (power law) resulted in a higher risk, 0.0122 % (R^2 of 99.7 %), i.e. 98.4 % (= 1.22*10^-4/(1.24*10^-4)) of <u>Stephen\u2019s</u>&nbsp;0.0124 %. Such remarkable agreement means the extinction risk for the best fit Pareto is essentially the same regardless of whether it is fitted to the top 10 % logarithm of the annual war deaths of combatants as a fraction of the global population (as I did), or to the war deaths of combatants per war (as implied by Stephen using Bear\u2019s estimates). I guess this qualitatively generalises to other types of distributions. In any case, I&nbsp;<a href=\"https://docs.google.com/document/d/14_5GnGMD_hwGjEITRLuBeV4sfRG1tI5xmAb1bqW1hEc/edit#heading=h.jqgyd0hhwqld\"><u>would</u></a> rather follow my approach.</p></blockquote>", "parentCommentId": "bidQBbMfXLXscJYo9", "user": {"username": "vascoamaralgrilo"}}, {"_id": "jaYSkxFHG36S5aCda", "postedAt": "2024-01-31T00:43:58.183Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>You are welcome, Stephen!</p><blockquote><p>I'll come back to this to respond in a bit more depth next week (this is a busy week).</p></blockquote><p>No worries, and thanks for still managing to make an in-depth comment!</p><blockquote><p>In the meantime, curious what you make of my point that setting a prior that gives only a 1 in 15 trillion chance of experiencing an extinction-level war in any given year seems wrong?</p></blockquote><p>I only managed to reply to 3 of your points yesterday and during this evening, but I plan to address that 4th one still today.</p>", "parentCommentId": "FZuooyftAP2ST5sxQ", "user": {"username": "vascoamaralgrilo"}}, {"_id": "PLtxJoE5CMvMp8QdG", "postedAt": "2024-01-31T11:42:21.326Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<blockquote><p><strong>Finally, I'm a bit suspicious of infinitesimal probabilities due to the strength they give the prior.</strong> They imply we'd need enormously strong evidence to update much at all in a way that seems unreasonable to me.</p><p>[...]</p><p>Cf. some of Joe's discussion of settling on infinitesimal priors <a href=\"https://www.lesswrong.com/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk\">here</a>.</p></blockquote><p>I think there is a potential misunderstanding here. Joe Carlsmith's<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefja5hgln7o1n\"><sup><a href=\"#fnja5hgln7o1n\">[1]</a></sup></span>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/3KAuAS2shyDwnjzNa/predictable-updating-about-ai-risk#6__Constraints_on_future_worrying\">discussion</a> on the contraints on future updating apply to one's best guess. In contrast, my astronomically low best guess prior is not supposed to be neither my current best guess nor a preliminary best guess from which one should formally update towards one's best guess. That being said, historical war deaths seem to me like the most natural prior to assess future war deaths, so I see some merit in using my astronomically low best guess prior as a preliminary best guess.</p><p>I also agree with Joe that an astronomically low annual AI extinction risk (e.g. 6.36*10^-14) would not make sense (see <a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors?commentId=ee7Chfyx2coYwmznt\">this</a> somewhat related thread). However, I would think about the possibility of AI killing all humans in the context of <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\">AI risk</a>, not <a href=\"https://80000hours.org/problem-profiles/great-power-conflict/\">great power war</a>.</p><blockquote><p>Let's take your preferred estimate of an annual probability of \"6.36*10^-14\". That's a 1 in 15,723,270,440,252 chance. That is, 1 in 15 trillion years.</p></blockquote><blockquote><p>I look around at the world and I see a nuclear-armed state fighting against a NATO-backed ally in Ukraine; I see conflict once again spreading throughout the Middle East; I see the US arming and <i>perhaps</i> preparing to defend Taiwan against China, which is governed by a leader who claims to consider reunification both inevitable and an existential issue for his nation.&nbsp;</p><p>And I see nuclear arsenals that still top 12,000 warheads and growing; I see ongoing bioweapons research powered by ever-more-capable biotechnologies; and I see obvious military interest in developing AI systems and autonomous weapons.</p></blockquote><blockquote><p>This does not seem like a situation that only leads to total existential destruction once every 15 trillion years.</p></blockquote><p>I feel like the sentiment you are expressing describing current events and trends would also have applied in the past, and today to risks which you might consider overly low. On the one hand, I appreciate a probability like 6.36*10^-14 intuitively feels way too small. On the other, humans are not designed to intuitively/directly assess the probability of rare events in a reliable way. These involve many steps, and therefore give rise to <a href=\"https://en.wikipedia.org/wiki/Scope_neglect\">scope neglect</a>.&nbsp;</p><p>As a side note, I do not think there is an evolutionary incentive for an individual human to accurately distinguishing between an extinction risk of 10^-14 and 0.01 %, because both are negligible in comparison with the annual risk of death 1 % (for a life expectancy of 100 years). Relatedly, I mentioned in the post that:</p><blockquote><p>In general, I suspect there is a tendency to give probabilities between 1 % and 99 % for events whose mechanics we do not understand well [e.g. extinction conditional on a war larger than World War 2], given this range encompasses the vast majority (98 %) of the available linear space (from 0 to 1), and events in everyday life one cares about are not that extreme. However, the available logarithmic space is infinitely vast, so there is margin for such guesses to be major overestimates. In the context of tail risk, subjective guesses can easily fail to adequately account for the faster decay of the tail distribution as severity approaches the maximum.</p></blockquote><p>In addition, I guess my astronomically low annual war extinction risk feels like an extreme value to many because they have in the back of their minds Toby's <a href=\"https://forum.effectivealtruism.org/posts/Z5KZ2cui8WDjyF6gJ/some-thoughts-on-toby-ord-s-existential-risk-estimates\"><u>guesses</u></a>&nbsp;for the existential risk between 2021 and 2120 given in <a href=\"https://theprecipice.com/\"><u>The Precipice</u></a>.&nbsp;The guess was 0.1 % for nuclear war, which respects an annual existential risk of around 10^-5, way larger than the estimates for annual war extinction risk I present in my post. Toby does not mechanistically explain how he got his guesses, but I do not think he used quantitative models to derive them. So I think they may well be prone to scope neglect. In terms of Toby's guesses, I also mentioned in the post that:</p><blockquote><p>In general, I agree with David Thorstad that Toby Ord\u2019s <a href=\"https://forum.effectivealtruism.org/posts/Z5KZ2cui8WDjyF6gJ/some-thoughts-on-toby-ord-s-existential-risk-estimates\"><u>guesses</u></a>&nbsp;for the existential risk between 2021 and 2120 given in <a href=\"https://theprecipice.com/\"><u>The Precipice</u></a>&nbsp;are very high (e.g. 0.1 % for nuclear war). In the realm of the more anthropogenic <a href=\"https://forum.effectivealtruism.org/topics/ai-risk\"><u>AI</u></a>, <a href=\"https://forum.effectivealtruism.org/topics/biosecurity-and-pandemics\"><u>bio</u></a>&nbsp;and <a href=\"https://forum.effectivealtruism.org/topics/nuclear-warfare-1\"><u>nuclear</u></a>&nbsp;risk, I personally think underweighting the <a href=\"https://forum.effectivealtruism.org/topics/inside-vs-outside-view\"><u>outside view</u></a>&nbsp;is a major reason leading to overly high risk. I encourage readers to check David\u2019s series <a href=\"https://ineffectivealtruismblog.com/category/exaggerating-the-risks/\"><u>exaggerating the risks</u></a>, which includes subseries on <a href=\"https://forum.effectivealtruism.org/topics/climate-change\"><u>climate</u></a>, <a href=\"https://forum.effectivealtruism.org/topics/ai-risk\"><u>AI</u></a>&nbsp;and <a href=\"https://forum.effectivealtruism.org/topics/biosecurity-and-pandemics\"><u>bio</u></a>&nbsp;risk.</p></blockquote><p>To give an example that is not discussed by David, <a href=\"https://www.sciencedirect.com/science/article/pii/S0016328722000337\"><u>Salotti 2022</u></a> estimated the extinction risk per century from asteroids and comets is 2.2*10^-12 (see Table 1), which is 6 (= log10(10^-6/(2.2*10^-12)) orders of magnitude lower than Toby Ord\u2019s guess for the existential risk. The concept of existential risk is quite vague, but I do not think one can say existential risk from asteroids and comets is 6 orders of magnitude higher than extinction risk from these:</p><ul><li>There have been 5&nbsp;<a href=\"https://en.wikipedia.org/wiki/Extinction_event\"><u>mass extinctions</u></a>, and the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Volcanic_winter\"><u>impact winter</u></a> involved in the last one, which played a role in the extinction of the dinosaurs,&nbsp;<a href=\"https://www.scientificamerican.com/article/how-mammals-conquered-the-world-after-the-asteroid-apocalypse/\"><u>may</u></a> well have contributed to the emergence of mammals, and ultimately humans.</li><li>It is possible a species better than humans at steering the future would have evolved given fewer mass extinctions, or in the absence of the last one in particular, but this is unclear. So I would say the above is some evidence that existential risk may even be lower than extinction risk.</li></ul><blockquote><p>I know you're only talking about the prior, but your preferred estimate implies we'd need a galactically-enormous update to get to a posterior probability of war x-risk that seems reasonable. So I think something might be going wrong.&nbsp;</p></blockquote><p>The methodology I followed in my analysis is quite similar to yours. The major differences are that:</p><ul><li>I fitted distributions to the top 10 % logarithm of the annual war deaths of combatants as a fraction of the global population, whereas you relied on an extinction risk per war from Bear obtained by fitting a power law to war deaths of combatants per war. As I <a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors?commentId=kLzXQjrPw6nJzueRw\">commented</a>, it is unclear to me whether this is a major issue, but I prefer my approach.</li><li>I dealt with 111 types distributions, whereas you focussed on 1.<ul><li>For the distribution you used (a Pareto), I got an annual probability of a war causing human extinction of <a href=\"https://docs.google.com/spreadsheets/d/1bhqZRZ1KOndD3IqSin-TPtsqB9uk2Kvj8_FCB8vBnHs/edit#gid=47587321&amp;range=M51\">0.0122 %</a>, which is very similar to the 0.0124 %/year respecting your estimate of 0.95 % over 77 years.</li><li>Aggregating the results of the top 100 distributions, I got 6.36*10^-14.</li></ul></li></ul><p>You might be thinking something along the lines of:</p><ul><li>Given no fundamental flaw in my methodology, one should updated towards an astronomically low war extinction risk.</li><li>Given a fundamental flaw in my methodology, one should updated towards a war extinction risk e.g. 10 % as high as your 0.0124 %/year, i.e. 0.00124 %/year.</li></ul><p>However, given the similarities between our methodologies, I think there is a high chance that any fundamental flaw in my methodology would affect yours too. So, given a fundamental flaw in mine, I would mostly believe that neither my best guess prior nor your best guess could be trusted. So, to the extent your best guess for the war extinction is informed by your methodology, I would not use it as a prior given a fundamental flaw in my methodology. In this case, one would have to come up with a better methodology rather than multiplying your annual war extinction risk by e.g. 10 %.</p><p>I also feel like updating on your prior via multiplication by something like 10 % would be quite arbitrary because my estimates for the annual war extinction risk are all over the map. Across all 111 distributions, and 3 values for the deaths of combatants as a fraction of the total deaths (10 %, 50 % and 90%) I studied, I got estimates for the annual probability of a war causing human extinction from 0 to <a href=\"https://docs.google.com/spreadsheets/d/1bhqZRZ1KOndD3IqSin-TPtsqB9uk2Kvj8_FCB8vBnHs/edit#gid=471548181&amp;range=M107\">8.84 %</a>. Considering just my best guess of war deaths of combatants equal to 50 % of the total deaths, the annual probability of a war causing human extinction still ranges from 0 to <a href=\"https://docs.google.com/spreadsheets/d/1bhqZRZ1KOndD3IqSin-TPtsqB9uk2Kvj8_FCB8vBnHs/edit#gid=47587321&amp;range=M93\">2.95 %</a>. Given such wide ranges, I would instead update towards a state of greater <a href=\"https://forum.effectivealtruism.org/topics/cluelessness\">cluelessness</a> or less <a href=\"https://forum.effectivealtruism.org/topics/credal-resilience\">resilience</a>. In turn, these would imply a greater need for a better methodology, and more research on quantifying the risk of war in general.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnja5hgln7o1n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefja5hgln7o1n\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I like use the full name on the 1st occasion a name is mentioned, and then just the 1st name afterwards.</p></div></li></ol>", "parentCommentId": "bidQBbMfXLXscJYo9", "user": {"username": "vascoamaralgrilo"}}, {"_id": "WEjuiP6sGdpuBPA5h", "postedAt": "2024-02-05T18:12:11.413Z", "postId": "TwpoedzMpmy7k7NKH", "htmlBody": "<p>Thanks for this response!</p>", "parentCommentId": "dB83FR2Te84troPwK", "user": {"username": "tobycrisford"}}]