[{"_id": "ngMJ377zFaj7mXn3J", "postedAt": "2023-10-11T03:30:30.038Z", "postId": "ARwFCMpgTbmJ89hBP", "htmlBody": "<p>Just an observation from the few, excellent and disparate answers so far: It seems the answer to how much of AI risk is the overlap with bio runs the gamut from almost nothing to almost everything. I guess in line with previous thinking on estimates around which there is uncertainty, this to me points at a few things:</p><ul><li>It increases the value of trying to analyse this in more detail. Especially as I pointed out in the OP that it could have big, action-relevant implications.</li><li>We can not write off any significant degree of overlap. With a risk mindset it means we should all be concerned this overlap could be large, even if we personally think the overlap is small.</li><li>Similarly it also means we cannot expect bio defense to do all the job of AI safety, even if one personally believes the overlap is near complete.</li><li>For people using AI+bio as an example of ways AI could go wrong, for now it seems fair to continue to use this example perhaps and when appropriate adding that it is unclear how much of AI risk is AI+bio - it could be a lot.</li></ul>", "parentCommentId": null, "user": {"username": "Ulrik Horn"}}]