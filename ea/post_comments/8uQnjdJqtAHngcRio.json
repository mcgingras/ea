[{"_id": "yqXYABPkWSYSiySaL", "postedAt": "2023-10-05T16:03:14.653Z", "postId": "8uQnjdJqtAHngcRio", "htmlBody": "<p>Hello Jack, I'm honoured you've written a review of my review! Thanks also for giving me sight of this before you posted. I don't think I can give a quick satisfactory reply to this, and I don't plan to get into a long back and forth. So, I'll make a few points to provide some more context on what I wrote. [I wrote the remarks below based on the original draft I was sent. I haven't carefully reread the post above to check for differences, so there may be a mismatch if the post has been updated]</p><p>First, the piece you're referring to is a book review in an academic philosophy journal. I'm writing primarily for other philosophers who I can expect to have lots of background knowledge (which means I don't need to provide it myself).</p><p>Second, book reviews are, by design, very short. You're even discouraged from referencing things outside the text you're reviewing. The word limit was 1,500 words - I think my review may even be shorter than your review of my review! - so the aim is just to give a brief overview and make a few comments.</p><p>Third, the thrust of my article is that MacAskill makes a disquietingly polemical, one-sided case for longtermism. My objective was to point this out and deliberately give <i>the other side </i>so that, once readers have read both they are, hopefully, left with a balanced view. I didn't seek to, and couldn't possibly hope to, given a balanced argument that refutes longtermism in a few pages. I merely explain why, in my opinion, the case for it in the book is unconvincing. Hence, I'd have lots of sympathy with your comments if I'd written a full-length article, or a whole book, challenging longtermism.</p><p>Fourth, I'm not sure why you think I've misrepresented MacAskill (do you mean 'misunderstood'?). In the part you quote, I am (I think?) making my own assessment, not stating MacAskill's view at all. What's more, I don't believe MacAskill and I disagree about the importance of the intuition of neutrality for longtermism. I only observe that accepting that intuition would <i>weaken </i>the case - I <i>do not claim</i> there is <i>no </i>case for longtermism if you accept it. Specifically, you quote MacAskill saying:</p><blockquote><p>[if you endorse the intuition of neutrality] you wouldn\u2019t regard the absence of future generations in itself as a moral loss.</p></blockquote><p>But the <i>cause du jour</i> of longtermism is preventing existential risks <i>in order that</i> many future happy generations exist. If one accepts the intuition of neutrality that would reduce/remove the good of doing <i>that</i>. Hence, it does present a severe challenge to longtermism in practice - especially if you want to claim, as MacAskill does, that longtermism changes the priorities.</p><p>Finally, on whether 'many' philosophers are sympathetic to person-affecting views. In my experience of floating around seminar rooms, it seems to be a view of the large minority of discussants (indeed, it seems far more popular than totalism). Further, it's taken as a default, or starting position, which is why other philosophers have strenuously argued against it; there is little need to argue against views that no one holds! I don't think we should assess philosophical truth 'by the numbers', ie polling people, rather than by arguments, particularly when those you poll aren't familiar with the arguments. (If we took such an approach, utilitiarianism would be conclusively 'proved' false.). That said, off the top of my head, philosophers who have written sympathetically about person-affecting views include <a href=\"https://academic.oup.com/edited-volume/41281/chapter-abstract/351601098\">Bader</a>, Narveson (two classic articles <a href=\"https://academic.oup.com/mind/article-abstract/LXXVI/301/62/1187592\">here </a>and <a href=\"https://www.jstor.org/stable/27902295\">here</a>), Roberts (especially <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1755-2567.2011.01117.x\">here</a>, but she's written on it <a href=\"https://philpeople.org/profiles/melinda-roberts/publications\">a few times</a>), Frick (<a href=\"https://philpapers.org/rec/FRICRA-4\">here </a>and in <a href=\"https://philpapers.org/rec/FRIMPH\">his thesis</a>), <a href=\"https://link.springer.com/chapter/10.1007/978-1-4020-5697-0_1\">Heyd</a>, <a href=\"https://global.oup.com/academic/product/the-non-identity-problem-and-the-ethics-of-future-people-9780199682935?cc=us&amp;lang=en&amp;\">Boonin</a>, Temkin (<a href=\"https://www.raco.cat/index.php/LEAP/article/view/297557/386539\">here </a>and probably elsewhere). There are not 'many' philosophers in the world, and population ethics is a small field, so this is a non-trivial number of authors! For an overview of the non-identity problem in particular, see the <a href=\"https://plato.stanford.edu/archives/win2015/entries/nonidentity-problem/\">SEP</a>.</p>", "parentCommentId": null, "user": {"username": "MichaelPlant"}}, {"_id": "dWFkuqZEsWwoAXC8K", "postedAt": "2023-10-05T20:21:06.962Z", "postId": "8uQnjdJqtAHngcRio", "htmlBody": "<p>Thanks for this reply Michael! I'll do a few replies and understand that you don't want to get in a long back and forth so will understand if you don't reply further.</p><p>Firstly, the following is all very useful background so I appreciate these clarifications:</p><blockquote><p>First, the piece you're referring to is a book review in an academic philosophy journal. I'm writing primarily for other philosophers who I can expect to have lots of background knowledge (which means I don't need to provide it myself).</p><p>Second, book reviews are, by design, very short. You're even discouraged from referencing things outside the text you're reviewing. The word limit was 1,500 words - I think my review may even be shorter than your review of my review! - so the aim is just to give a brief overview and make a few comments.</p><p>Third, the thrust of my article is that MacAskill makes a disquietingly polemical, one-sided case for longtermism. My objective was to point this out and deliberately give <i>the other side </i>so that, once readers have read both they are, hopefully, left with a balanced view.</p></blockquote><p>In light of this I think the wording \"Plant presents a very one-sided analysis of the non-identity problem\" is an unfair criticism. I'm still happy I wrote that section because I wanted to defend longtermism from your attack, but I should have framed it differently.</p>", "parentCommentId": "yqXYABPkWSYSiySaL", "user": {"username": "jackmalde"}}, {"_id": "ovZ7BoHkyyj2t55Pw", "postedAt": "2023-10-05T20:30:14.179Z", "postId": "8uQnjdJqtAHngcRio", "htmlBody": "<p>I lean towards thinking the following is unfair.</p><blockquote><p>Third, the thrust of my article is that MacAskill makes a disquietingly polemical, one-sided case for longtermism.</p></blockquote><p>If one were just to read WWOTF they would come away with an understanding of:</p><ul><li>The intuition of neutrality - what it is, the fact that some people hold it, <strong>the fact that if you accept it you shouldn't care about losing future generations.</strong></li><li>The non-identity problem - what it is and why <strong>some see it as an argument against being able to improve the future.</strong></li><li>The repugnant conclusion - what it is, how some find it repugnant and <strong>why it is an argument against total utilitarianism</strong>.</li></ul><p>This is all Will explaining the <i>'other side'</i>. Sure he's one-sided in the sense that he also explains why he disagrees with these arguments, but that seems fine to me. He's not writing a textbook. What would have been an issue is if he had, say, just explained total utilitarianism without also explaining the repugnant conclusion, the intuition of neutrality or the non-identity problem.</p><p>Regarding the \"polemical\" description. I'm not really sure what you're getting at. Merriam-Webster defines a polemic as \"an aggressive controversialist\". Do you think Will was aggressive? As I say he presents 'the other side' while also explaining why he disagrees with it. I'm not really seeing an issue here.</p>", "parentCommentId": "yqXYABPkWSYSiySaL", "user": {"username": "jackmalde"}}, {"_id": "eHvgEhDvBuzGj9cC8", "postedAt": "2023-10-05T20:44:58.697Z", "postId": "8uQnjdJqtAHngcRio", "htmlBody": "<blockquote><p>Fourth, I'm not sure why you think I've misrepresented MacAskill (do you mean 'misunderstood'?). In the part you quote, I am (I think?) making my own assessment, not stating MacAskill's view at all.</p></blockquote><p>You say the following in the summary of the book section (bold part added by me):</p><blockquote><p>If correct, this <strong>[the intuition of neutrality]</strong> would present a severe challenge to longtermism</p></blockquote><p>By including it in the 'summary' section I think you implicitly present this as a view Will espoused in the book - and I don't agree that he did.</p><blockquote><p>But the <i>cause du jour</i> of longtermism is preventing existential risks <i>in order that</i> many future happy generations exist. If one accepts the intuition of neutrality that would reduce/remove the good of doing <i>that</i>. Hence, it does present a severe challenge to longtermism in practice - especially if you want to claim, as MacAskill does, that longtermism changes the priorities.</p></blockquote><p>Sure, people talk about avoiding extinction quite a bit, but that isn't the only reason to care about existential risk, as I explain in my post. For example, you can want to prevent existential risks that involve locking-in bad states of the world in which we continue to exist e.g. an authoritarian state such as China using powerful AI to control the world.&nbsp;</p><p>One could say reducing x-risk from AI is the <i>cause du jour</i> of the longtermist community. The key point is that <strong>reducing x-risk from AI is still a valid priority (for longtermist reasons) if one accepts the intuition of neutrality.</strong></p><p>Accepting the intuition of neutrality would involve some re-prioritization within the longtermist community - say moving resources away from x-risks that are solely extinction risks (like biorisks?) and towards x-risks that are more (like s-risks from misaligned AI or digital sentience). I simply don't think accepting the intuition of neutrality is a \"severe\" challenge for longtermism, and I think it is clear Will doesn't think so either (e.g. see <a href=\"https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2/\">this</a>).</p>", "parentCommentId": "yqXYABPkWSYSiySaL", "user": {"username": "jackmalde"}}, {"_id": "df2ccLAvBQszm7nwz", "postedAt": "2023-10-05T20:49:31.555Z", "postId": "8uQnjdJqtAHngcRio", "htmlBody": "<blockquote><p>That said, off the top of my head, philosophers who have written sympathetically about person-affecting views include <a href=\"https://academic.oup.com/edited-volume/41281/chapter-abstract/351601098\">Bader</a>, Narveson (two classic articles <a href=\"https://academic.oup.com/mind/article-abstract/LXXVI/301/62/1187592\">here </a>and <a href=\"https://www.jstor.org/stable/27902295\">here</a>), Roberts (especially <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1755-2567.2011.01117.x\">here</a>, but she's written on it <a href=\"https://philpeople.org/profiles/melinda-roberts/publications\">a few times</a>), Frick (<a href=\"https://philpapers.org/rec/FRICRA-4\">here </a>and in <a href=\"https://philpapers.org/rec/FRIMPH\">his thesis</a>), <a href=\"https://link.springer.com/chapter/10.1007/978-1-4020-5697-0_1\">Heyd</a>, <a href=\"https://global.oup.com/academic/product/the-non-identity-problem-and-the-ethics-of-future-people-9780199682935?cc=us&amp;lang=en&amp;\">Boonin</a>, Temkin (<a href=\"https://www.raco.cat/index.php/LEAP/article/view/297557/386539\">here </a>and probably elsewhere). There are not 'many' philosophers in the world, and population ethics is a small field, so this is a non-trivial number of authors! For an overview of the non-identity problem in particular, see the <a href=\"https://plato.stanford.edu/archives/win2015/entries/nonidentity-problem/\">SEP</a>.</p></blockquote><p>I agree we should be more swayed by arguments than numbers - I feel like it was you who played the numbers game first so I thought I'd play along a bit.</p><p>FYI I did reference that SEP article in my post and it says (emphasis mine):</p><blockquote><p>Since the nonidentity problem became well-known through the work of Derek Parfit, James Woodward and Gregory Kavka in the early 1980s, most philosophers have accepted it as showing that at least one of the aforementioned intuitions must be false. <strong>Here, the most frequently identified culprit is intuition (1), that is, the person-based intuition itself.</strong></p></blockquote>", "parentCommentId": "yqXYABPkWSYSiySaL", "user": {"username": "jackmalde"}}, {"_id": "fPTBNzZDyXiJavZ2H", "postedAt": "2023-10-06T05:30:07.881Z", "postId": "8uQnjdJqtAHngcRio", "htmlBody": "<p>Thanks for the reply and I think you make a lot of good arguments, I'm not sure where I sit on this issue!</p>\n<p>I found your last paragraph a little disturbing, because even given the truth of long termism, some of these ideas seem like they wouldn't necessarily serve the present or the future particularly well. Would 100 percent of philosophies working on the question of the far future really be the best way to improve the field, with other important philosophical professions neglected? Widespread surveillance has already proven to be too unpalatable to most westerners and impractical even if it did prevent someone making a bioweapon. Personally I think even given longtermism, most people should be working on fixing existing issues (governance, suffering, climate change) as fixing things now will also get the future. Perhaps 100 to 1000x the current number off other working on longtermist causes would be ideal, but I think gearing the whole engine of the world towards longtermism might well be counterproductive.</p>\n<p>\"Virtually the whole machine learning community would be working on the technical problem of AI alignment. Governments would have departments for reducing existential risk / for future generations. 100% of philosophers would be working on the question \u201chow can we best improve the far future\u201d. We would save a lot more than we do and mitigate climate change far more than we do. We might even have widespread surveillance to ensure we don\u2019t destroy ourselves (to be clear I am personally unsure if this would be required/desirable). We would have everyone on earth working together to improve the far future instead of what we have now - countries working against each other to come out on top.\"</p>\n", "parentCommentId": null, "user": {"username": "NickLaing"}}, {"_id": "ok5TWme97DReHoArD", "postedAt": "2023-10-06T12:31:55.890Z", "postId": "8uQnjdJqtAHngcRio", "htmlBody": "<p>Hey, thanks for you comment! To be honest my addendum is a bit speculative and I haven't thought about a huge amount. I think I may have been a little extreme and that factoring moral uncertainty would soften some of what I said.</p><blockquote><p>Would 100 percent of philosophies working on the question of the far future really be the best way to improve the field, with other important philosophical professions neglected?</p></blockquote><p>When you say \"improve the field\" I'm not sure what you mean. Personally I don't think there is intrinsic value in philosophical progress, only instrumental value. It seems desirable for the philosophy field to reorient in a way that focuses on improving the world as much as possible, and that is likely to mean at least some fields entirely or nearly die out (e.g. aesthetics?, philosophy of religion?). I suspect a lot of fields would continue if we were to focus on improving the future though, as most of them have some useful role to play. The specific questions philosophers work on within those fields would change quite a bit though.&nbsp;</p><blockquote><p>Widespread surveillance has already proven to be too unpalatable to most westerners and impractical even if it did prevent someone making a bioweapon.</p></blockquote><p>I tried to express agnosticism on if this would be desirable and I am very sympathetic to arguments it wouldn't be.</p><blockquote><p>Personally I think even given longtermism, most people should be working on fixing existing issues (governance, suffering, climate change) as fixing things now will also get the future.</p></blockquote><p>I did mention the importance of alleviating suffering and tackling climate change in my post. I'm not sure if we disagree as much as you think we do. Governance is a bit vague, but many forms of governance can easily be justified on longtermist grounds (as can climate change).</p><blockquote><p>I think gearing the whole engine of the world towards longtermism might well be counterproductive</p></blockquote><p>It is possible that \"obsessing\" about the far future is counterproductive. At that point we would be justified in obsessing less. However, we would be obsessing less <strong>on longtermist grounds</strong>.</p>", "parentCommentId": "fPTBNzZDyXiJavZ2H", "user": {"username": "jackmalde"}}, {"_id": "KJiaWsYxT7Xu9qDcq", "postedAt": "2023-10-06T15:43:20.248Z", "postId": "8uQnjdJqtAHngcRio", "htmlBody": "<p>I think this is another point where you're missing context. It's kind of a quirk of academic language, but \"polemical\" is usually used in contrast to analytical in texts like these - meaning that the work in question is more argumentative/persuasive than analytical or explicative, which I honestly think is a very apt description of WWTF.&nbsp;</p>", "parentCommentId": "ovZ7BoHkyyj2t55Pw", "user": {"username": "tugbazsen"}}, {"_id": "CJnL4Ms9mL6f3h3DY", "postedAt": "2023-10-06T15:56:17.370Z", "postId": "8uQnjdJqtAHngcRio", "htmlBody": "<p>OK. I think Will intended WWOTF to be a persuasive piece so I\u2019m not sure if this is a valid criticism. He wasn\u2019t writing a textbook.</p>\n", "parentCommentId": "KJiaWsYxT7Xu9qDcq", "user": {"username": "jackmalde"}}, {"_id": "GNrn24o3wdnAFazK6", "postedAt": "2023-10-06T16:26:55.450Z", "postId": "8uQnjdJqtAHngcRio", "htmlBody": "<p>Book reviews are meant to be informative and critiques aren't always meant to be negative, so I don't know why you're framing it as an attack on WWTF or MacAskill. Knowing the tone of a work is valuable information for someone reading a book review.</p><p>On a personal note, I'll say that I also agree with the \"disquieting\" portion of \"disquietingly polemical\" - I had the sense that WWTF presented longtermism and caring about future generations as a kind of foregone conclusion and moral imperative rather than something to be curious about and think deeply on, but I prefer these kinds of books to be more proactive in very strongly establishing the opposing viewpoints, so it's probably more irksome to me than it would be to others. He wasn't writing a textbook and it's prerogative to write something that's an outright manifesto if he so chooses, but that doesn't make pointing out the tone an unvalid critique.</p>", "parentCommentId": "CJnL4Ms9mL6f3h3DY", "user": {"username": "tugbazsen"}}, {"_id": "E7joazpH5zfaTk8hH", "postedAt": "2023-10-06T16:50:47.034Z", "postId": "8uQnjdJqtAHngcRio", "htmlBody": "<p>I'm not sure I have framed the review as an attack? I don't think it is. I have no problem with Michael writing the review, I just disagree with the points he made.</p><p>It was a while since I read the book in its entirety, but I will just leave a quote from the introduction which to me doesn't read as \"disquietingly polemical\" (bold emphasis mine):</p><blockquote><p><strong>For those who want to dig deeper into some of my claims, I have compiled extensive supplementary materials</strong>, including special reports I commissioned as background research, and made them available at whatweowethefuture.com. Despite the work done so far, <strong>I believe we have only scratched the surface of longtermism and its implications; there is much still to learn.</strong></p><p><strong>If I\u2019m right</strong>, then we face a huge responsibility. Relative to everyone who could come after us, we are a tiny minority. Yet we hold the entire future in our hands. Everyday ethics rarely grapples with such a scale. We need to build a moral worldview that takes seriously what\u2019s at stake.</p></blockquote>", "parentCommentId": "GNrn24o3wdnAFazK6", "user": {"username": "jackmalde"}}, {"_id": "8c6wgtcMPveGHcAbC", "postedAt": "2023-10-06T17:23:15.337Z", "postId": "8uQnjdJqtAHngcRio", "htmlBody": "<p>The general tone of your comments + the line \"I'm still happy I wrote that section because I wanted to defend longtermism from your attack\" in one comment gives me the impression that you are, but I'm fully willing to accept that it's just the lack of emotive expressiveness in text.&nbsp;</p><p>Yes, MacAskill does have these explicit lines at certain points (I'd argue that this is the bare minimum, but it's a problem I have with a large swathe of academic and particularly pop-philosophy texts and as I said it's in some measure a matter of personal preference), but the overall tone of the text and the way he engages with counterarguments and positions still came off as polemical to me. I admittedly hold seminal texts - which WWTF is obviously intended to be - up to particularly high standards in this regard, which I think is fair but completely understand if others disagree. To be clear, I think that this also weakens the argumentation overall rather than just being a lopsided defense or a matter of tone. I think the points raised here about the intuition of neutrality are an good example of this; a more robust engagement with the intuition of neutrality and its implications for longtermism could help specify longtermism and it's different strains to make it less of an amorphous moral imperative to \"think/care about future generations\" and a more easily operationalized and intellectually/analytically robust moral philosophy since it would create room for a deeper discussion of how longtermist approaches that prioritize the existence of future people differ from longtermist approaches that view the benefits for future people as secondary.</p>", "parentCommentId": "E7joazpH5zfaTk8hH", "user": {"username": "tugbazsen"}}, {"_id": "8kSdEJd8zyPRLZMFT", "postedAt": "2023-10-06T18:19:43.360Z", "postId": "8uQnjdJqtAHngcRio", "htmlBody": "<p>Ah ok I actually used the word \u201cattack\u201d. I probably shouldn\u2019t have, I feel no animosity at all towards Michael. I love debating these topics and engaging with arguments. I wish he\u2019d had more room to expand on his person-affecting leanings. In a sense he is \u201cattacking\u201d longtermism but in a way that I welcome and enjoy responding to.</p>\n<p>I happen to think the level of attention Will gave to population ethics and the concepts of the non-identity problem, repugnant conclusion, and person-affecting intuition is fairly admirable for a book intended for a general non-philosophical audience. As I say, if you read the book you do understand why these three things can be seen as undermining longtermism. Saying up front that he has more material to engage with on his website seems great to me.</p>\n", "parentCommentId": "8c6wgtcMPveGHcAbC", "user": {"username": "jackmalde"}}, {"_id": "g2XCkbMFxJTeEuxPa", "postedAt": "2023-10-06T19:54:21.837Z", "postId": "8uQnjdJqtAHngcRio", "htmlBody": "<p>I think this is confused. WWOTF is obviously both aiming to be persuasive and coming from a place of academic analytical philosophical rigour. Many philosophers write books that are both, e.g. Down Girl by Kate Manne or The Right to Sex by Amia Srinivasan. I don't think a purely persuasive book would have so many citations.&nbsp;<br>.</p>", "parentCommentId": "CJnL4Ms9mL6f3h3DY", "user": {"username": "zichenghuang"}}]