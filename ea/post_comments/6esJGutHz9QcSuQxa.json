[{"_id": "Exa8EnDaBnbCWRvuj", "postedAt": "2022-11-06T03:43:24.416Z", "postId": "6esJGutHz9QcSuQxa", "htmlBody": "<p>I have thought a few times that maybe a safer route to AGI would be to learn as much as we can about the most moral and trustworthy humans we can find and try to build on that foundation/architecture. I'm not sure how that would work with existing convenient methods of machine learning.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Peter Gebauer"}}, {"_id": "awgye75fMHE5rCGTj", "postedAt": "2022-11-07T01:48:56.259Z", "postId": "6esJGutHz9QcSuQxa", "htmlBody": "<p>I find it a bit irritating and slightly misleading that this post lists several authors, (some of them very famous in EA), who have not actually written the submission. May I suggest to only list  one account (eg ketanrama) as the author of the post?</p>\n", "parentCommentId": null, "user": {"username": "harfe"}}, {"_id": "6SLSp7mMrHMyaJv3N", "postedAt": "2022-11-07T02:00:06.506Z", "postId": "6esJGutHz9QcSuQxa", "htmlBody": "<p>I find this submission very low on detail in the places that matter, namely the anthropomorphic AGI itself. It is not clear how this could be build, or why it is more realistic that such an AGI gets build than other AGIs.</p>\n<blockquote>\n<p>and educated and reared much like a human child, in a caring and supportive environment.</p>\n</blockquote>\n<p>How would this look like? Why would the AGI respond to this like a well-behaved human child?</p>\n<blockquote>\n<p>Its value system would be, like that of humans, dynamic, high dimensional, and to some degree ineffable.</p>\n</blockquote>\n<p>Would it have inconsistent values? How do you know there won't be any mesaoptimization?</p>\n", "parentCommentId": null, "user": {"username": "harfe"}}, {"_id": "f7u5YqS9Z34XDmgwa", "postedAt": "2022-11-07T14:32:57.897Z", "postId": "6esJGutHz9QcSuQxa", "htmlBody": "<p>I have some discussion of this area in general and one of David Jilk\u2019s papers in particular at my post <a href=\"https://www.lesswrong.com/posts/Sd4QvG4ZyjynZuHGt/intro-to-brain-like-agi-safety-12-two-paths-forward\">Two paths forward: \u201cControlled AGI\u201d and \u201cSocial-instinct AGI\u201d</a>.</p><p>In short, it seems to me that if you buy into this post, then the next step should be to <a href=\"https://www.lesswrong.com/posts/5F5Tz3u6kJbTNMqsb/intro-to-brain-like-agi-safety-13-symbol-grounding-and-human\">figure out how human social instincts work</a>, not just qualitatively but in enough detail to write it into AGI source code.</p><p>I claim that this is an open problem, involving things like circuits in the hypothalamus and neuropeptide receptors in the striatum. And it\u2019s the main thing that I\u2019m working on myself.</p><p>Additionally, there are several very good reasons to work on the human social instincts problem, even if you don\u2019t buy into other parts of David Jilk\u2019s assertions here.</p><p>Additionally, figuring out human social instincts is (I claim) (at least mostly) orthogonal to work that accelerates AGI timelines, and therefore we should all be able to rally around it as a good idea.</p><p>Whether we should <i>also </i>try to accelerate anthropomorphic AGI timelines, e.g. by studying the learning algorithms in the neocortex, is bound to be a much more divisive question. I claim that on balance, it\u2019s mostly a very bad idea, with certain exceptions including closed (and not-intended-to-be-published) research projects by safety/alignment-concerned people. [I\u2019m stating this opinion without justifying it.]</p>", "parentCommentId": null, "user": {"username": "steve2152"}}, {"_id": "n8RiWhTDE3K8mbjCv", "postedAt": "2022-11-08T12:18:37.548Z", "postId": "6esJGutHz9QcSuQxa", "htmlBody": "<p>Yes, maybe a better option would be to have a separate account \"Future Fund AI Worldview Prize submissions\". Or even create an account for the author that they can later claim if they wish (but make it clear in the bio, and at the top of the post, that it is a place-holder account in the mean time).</p>", "parentCommentId": "awgye75fMHE5rCGTj", "user": {"username": "Greg_Colbourn"}}, {"_id": "jC8EBqJvvAaYAQ8qn", "postedAt": "2023-02-17T19:53:06.035Z", "postId": "6esJGutHz9QcSuQxa", "htmlBody": "<p>The problem with \"anthropomorphic AI\" approaches is</p><ol><li>The human mind is complicated and poorly understood.</li><li>Safety degrades fast with respect to errors.&nbsp;</li></ol><p>Lets say you are fairly successful. You produce an AI that is really really close to the human mind in the space of all possible minds. A mind that wouldn't be particularly out of place at a mental institution. They can produce paranoid ravings about the shapeshifting lizard conspiracy millions of times faster than any biological human.&nbsp;</p><p>Ok, so you make them a bit smarter. The paranoid conspiricies get more complicated and somewhat more plausible. &nbsp;But at some points, they are sane enough to attempt AI research and produce useful results. Their alignment plan is totally insane.</p><p>In order to be useful, anthropomorphic AI needs to not only make AI that thinks similarly to humans. They need to be able to target the more rational, smart and ethical portion of mind space.&nbsp;</p><p>Humans can chuck the odd insane person out of the AI labs. Sane people are more common and tend to think faster. A team of humans can stop any one of their number crowning themselves as world king.&nbsp;</p><p>In reality, I think your anthropomorphic AI approach gets an arguably kind of humanlike in some ways AI that takes over the world. Because it didn't resemble the right parts of the right humans in the right ways closely enough in the places where it matters.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Donald Hobson"}}]