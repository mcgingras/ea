[{"_id": "jdn3FaPkYAzACg64e", "postedAt": "2024-02-29T15:10:51.480Z", "postId": "bqhnbcRMeDdLcMedo", "htmlBody": "<p>This is a really interesting post, thank you for making it. I've <a href=\"https://forum.effectivealtruism.org/posts/wex8972nRsSjW8CHk/compliance-monitoring-as-an-impactful-mechanism-of-ai-safety\">written about similar internal safety methods before</a>, as well as writing some longer comments on this topic on other people's posts <a href=\"https://forum.effectivealtruism.org/posts/7vcEq6nGojvhSTKo2/corporate-governance-for-frontier-ai-labs-a-research-agenda\">elsewhere</a>, but I am very interested in the different angle of approach detailed in your post.<br><br>I wonder what windfalls would look like across various organisational types? A big division would be between private sector and public sector, and I would also be interested in seeing how this is different for monopsonies or more complex market sectors.&nbsp;</p><p><a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4155549\">This research</a> showed that public sector organisations (at least in some sub-types) had a very specific set of concerns and desires for AI tools compared to what we've seen elsewhere. In a more anecdotal vein, in my work across both private and public sectors I have seen this pattern quite strongly represented. I would be interested to hear your thoughts about ways that alignment windfalls/taxes as described in your post could impact or be impacted by that side of things - the difference in what each sector considers taxes and windfalls when profit is not the main objective?</p>", "parentCommentId": null, "user": {"username": "AI Law"}}, {"_id": "XwP4zfgJDjoGaH3QL", "postedAt": "2024-02-29T17:49:37.924Z", "postId": "bqhnbcRMeDdLcMedo", "htmlBody": "<p>Another potential windfall I just thought of: the kind of AI scientist system discussed by Bengio in <a href=\"https://slideslive.com/39014230/towards-quantitative-safety-guarantees-and-alignment\">this talk</a> (<a href=\"https://yoshuabengio.org/2023/05/07/ai-scientists-safe-and-useful-ai/\">older writeup</a>). The idea is to build a non-agentic system that uses foundation models and amortized Bayesian inference to create and do inference on compositional and interpretable world models. One way this would be used is for high-quality estimates of p(harm|action) in the context of online monitoring of AI systems, but if it could work it would likely have other profitable use cases as well.</p>", "parentCommentId": null, "user": {"username": "stuhlmueller"}}, {"_id": "qdEBqjHg4NycfAwK2", "postedAt": "2024-02-29T20:42:04.566Z", "postId": "bqhnbcRMeDdLcMedo", "htmlBody": "<p><strong>Executive summary</strong>: Discovering ways for companies to align AI systems with social good that also improve profitability reduces AI risk by shaping companies' incentives.</p><p><strong>Key points</strong>:</p><ol><li>Alignment taxes like robustness tradeoffs incur costs for safety, while alignment windfalls like human feedback also create value.</li><li>Startups optimize greedily within the landscape of taxes and windfalls they know of to maximize returns.</li><li>We can shape this landscape via regulation, public awareness, recruiting appeals.</li><li>But companies' knowledge of the full landscape is limited, so promoting windfalls specifically also guides development.</li><li>\"Factored cognition\" improves transparency and trust while solving problems, an example of a windfall.</li><li>Discovering and advocating for more windfalls makes AI safety intrinsically economically rational.</li></ol><p>&nbsp;</p><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}]