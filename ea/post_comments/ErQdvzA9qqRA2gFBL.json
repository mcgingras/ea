[{"_id": "DBrcos4euMyXeu323", "postedAt": "2023-09-06T21:34:58.621Z", "postId": "ErQdvzA9qqRA2gFBL", "htmlBody": "<p><strong>Executive summary</strong>: The report discusses key factors enabling scaling of AI systems, including costs, hardware, parallelization techniques, data availability, and energy usage. Significant investments have been made by major companies recently, suggesting state-of-the-art models likely cost around half a billion dollars. GPU capabilities beyond raw compute power drive their high cost. Data limitations seem surmountable through alternative sources and techniques. Energy needs could pose engineering challenges in the near future, while also enabling satellite detection of major training runs.<br><br><strong>Key points</strong>:</p><ol><li>Frontier AI models likely cost around $500 million currently, with 80% spent on hardware and 20% on operating costs. GPUs are about 70% of hardware costs.</li><li>Communication bandwidth, not just compute power, drives the high price of ML GPUs relative to gaming GPUs.</li><li>Parallelization techniques each have limitations that constrain scaling, especially communication costs.</li><li>Private data, multimodal training, and other techniques can supplement natural language data.</li><li>Energy needs may soon require gigawatt-scale supercomputers, posing engineering challenges but enabling satellite detection.</li><li>FLOPs are an unreliable metric for ML hardware capabilities due to specialization like lower precision numbers. More stable metrics could improve forecasting and regulation.</li></ol><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}]