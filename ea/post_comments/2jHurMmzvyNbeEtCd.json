[{"_id": "gfueaeb9JBrtyXwxq", "postedAt": "2023-03-22T19:17:39.078Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>I agree with your claim that lognormal distributions are a better choice than normal. However this doesn't explain whether another distribution might be better (especially in cases where data is scarce, such as the number of inhabitable planets).</p><p>For example, the power law distribution has some theoretical arguments in its favour and also has a significantly higher kurtosis, meaning there is a much fatter tail.</p>", "parentCommentId": null, "user": {"username": "Sanjay"}}, {"_id": "EXbKX7yJYRw6LFPRv", "postedAt": "2023-03-23T00:37:01.588Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>Neat post, and nice to see squiggle in the wild.</p><p>Some points</p><blockquote><p>Suppose I have several point-estimates for the fuel efficiency of my car - I can easily take a weighted average of these to make an aggregate point estimate, but it\u2019s not clear how I could turn them into an interval estimate without a heavy dose of personal bias.</p></blockquote><p>You could create a mixture distribution, you could fit a lognormal whose x% confidence interval is the range expressed by the points you've already found, you could use your subjective judgment to come up with a distribution which could fit it, you could use kernel density estimation (https://en.wikipedia.org/wiki/Kernel_density_estimation).&nbsp;</p><hr><p>In your number of habitable planets estimate, you have a planetsPerHabitablePlanet estimate. This is an interesting decomposition. I would have looked at the fraction of planets which are habitable, and probably fit a beta distribution to it, given that we know that the fraction is between 0 and 1. This seems a bit like a matter of personal taste, though.</p>", "parentCommentId": null, "user": {"username": "NunoSempere"}}, {"_id": "puKQeg22jpBjkowms", "postedAt": "2023-03-23T13:55:32.441Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>Thanks. I'll read up on the power law dist and at the very least put a disclaimer in: I'm only checking which is better out of normal/lognormal.</p>", "parentCommentId": "gfueaeb9JBrtyXwxq", "user": {"username": "Stan Pinsent"}}, {"_id": "soK6cYun5yFoNHpPn", "postedAt": "2023-03-23T14:28:41.823Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>Cool. To be clear, I think if anyone was reading your piece with any level of care or attention, it would be clear that you were comparing normal and lognormal, and not making any stronger claims than that.</p>", "parentCommentId": "puKQeg22jpBjkowms", "user": {"username": "Sanjay"}}, {"_id": "jJfCFKy2mTs9vtc9c", "postedAt": "2023-03-31T07:25:18.577Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>Greater remark, Sanjay! Great piece, Stan!</p><p>Related to which type of distribution is better, in <a href=\"https://80000hours.org/podcast/episodes/david-roodman-becoming-a-world-class-researcher/\">this</a> episode of The 80,000 Hours Podcast (search for \"So you mentioned, kind of, the fat tail-ness of the distribution.\"), David Roodman suggests using the <a href=\"https://en.wikipedia.org/wiki/Generalized_Pareto_distribution\">generalised Pareto distribution</a> (GPD) to model the right tail (which often drives the expected value). David mentions the right tails of normal, lognormal and power law distributions are particular cases of the GDP:</p><blockquote><p><strong>Robert Wiblin:</strong> This kind of log-normal or normal curve, or power law, are they all special cases of this generalized family [GPD]?</p><p><strong>David Roodman:</strong> Their tails are.</p></blockquote><p>So, fitting the right-tail empirical data to a GPD is arguably better than assuming (or fitting the data to) one particular type of distribution:</p><blockquote><p>[David Roodman:]</p><p>So what you can do is you can take a data set like, all geomagnetic disturbances since 1957, and then look at the [inaudible 00:59:09] say, 300 biggest ones. What\u2019s the right tail of the distribution? And then ask which member of the generalized Pareto family fits that data the best? And then once you\u2019ve got a curve that you know \u2026 you know for theoretical reasons is a good choice, you can extrapolate it farther to the right and say, \u201cWhat\u2019s a million year storm look like?\u201d</p><p>And one also has to be careful about out of sample extrapolations. But I think it\u2019s more grounded in theory, this is, to use the generalized Pareto family, because it is analogous to using the normal family when constructing usual standard errors. Than, to, for example, assume that geomagnetic storms follow a power law, which was done in one of the papers that reached the popular press. So there was a Washington Post story some years ago that said the chance of a Carrington-size storm was like 12% per decade. But that was assuming a power law, which has a very fat tail. When I looked at the data, I just felt that that \u2026 and allowed the data to choose within a larger and theoretically motivated family. It did not, the model fit did not gravitate towards the power law.</p></blockquote>", "parentCommentId": "puKQeg22jpBjkowms", "user": {"username": "vascoamaralgrilo"}}, {"_id": "4jYvGf4aw7Er4CrKw", "postedAt": "2023-03-31T08:05:37.651Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>Nice post, Stan!</p><blockquote><p>Point estimates are fine for multiplication, lossy for division</p></blockquote><p>I think one caveat here is that, if we want to obtain an expected value as output, the input point estimates should refer to the mean instead of the median. They are the same or similar for non-heavy-tailed distributions (like uniform or normal), but could differ a lot for heavy-tailed ones (like exponential or lognormal). When setting a lognormal to a point estimate, I think people often use the geometric mean between 2 percentiles (e.g. 5th and 95th percentiles), which corresponds to the median, not mean. Using the median in this case will underestimate the expected value, because it equals (see <a href=\"https://en.wikipedia.org/wiki/Log-normal_distribution\">here</a>):</p><ul><li>E(X) = Median(X)*e^(sigma^2/2), where sigma^2 is the variance of log(X).</li></ul><p><a href=\"https://docs.google.com/document/d/1XjOwv-ma9qkz-In5qXlL8X1pQEsL5HXs-PSHQ58lQi0/edit\">Here</a> you mention that this \"lognormal mean\" can lead to extreme results, but I think that is a feature as long as we think the lognormal is modelling the right tail correctly. If we do not think so, we can still use the mean of:</p><ul><li>Truncated lognormal distribution.</li><li>Minimum between a lognormal distribution and a maximum value (after which we think the lognormal no longer models the right tail well).</li></ul><blockquote><p><strong>Interval estimates are prone to personal bias.&nbsp;</strong>It\u2019s easy to create an interval estimate intuitively. When objectivity is important and the evidence base is sparse, point estimates are easier to form and are more transparent.</p></blockquote><p>In my mind:</p><ul><li>Being objective is about faithfully representing the information we have about reality, even if that means being more uncertain.</li><li>The evidence base being sparse suggests we are uncertain about what reality actually looks like, which means a faithful representation of it will more easily be achieved by intervals, not point estimates. For example, I think using interval estimates in the Drake equation in much more important that in the cost-effectiveness analyses of GiveWell's top charities.</li><li>One compromise to achieve transparency while mainting the benefits of interval estimates is using pessimistic, realistic and optimistic point estimates. One the one hand, this may result in wider intervals because the product between 2 5th percentiles is rarer than a 5th percentile, so the pessimistic final estimate will be more pessimistic than its inputs. On the other hand, we can think as the wider intervals as accounting for structural uncertainty of the model.</li></ul>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "9RwCH63NwhkaDtmEC", "postedAt": "2023-04-01T17:04:14.613Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>Thanks for your feedback, Vasco. It's led me to make extensive changes to the post:</p><ul><li>More analysis on the pros/cons of modelling with distributions. I argue that sometimes it's good that the crudeness of point-estimate work reflects the crudeness of the evidence available. Interval-estimate work is more honest about uncertainty, but runs the risk of encouraging overconfidence in the final distribution.</li><li>I include the lognormal mean in my analysis of means. You have convinced me that the sensitivity of lognormal means to heavy right tails is a strength, not a weakness! But the lognormal mean appears to be sensitive to the size of the confidence interval you use to calculate it - which means subjective methods are required to pick the size, introducing bias.</li></ul><p>Overall I agree that interval estimation is better suited to the Drake equation than to GiveWell CEAs. But I'd summarise my reasons as follows:</p><ul><li>The Drake Equation really seeks to ask \"how likely is it that we have intelligent alien neighbours?\", but point-estimate methods answer the question \"what is the expected number of intelligent alien neighbours?\". With such high variability the expected number is virtually useless, but the <i>distribution </i>of this number allows us to estimate the number of alien neighbours. GiveWell CEAs probably have much less variation and hence a point-estimate answer is relatively more useful</li><li>Reliable research on the numbers that go into the Drake equation often doesn't exist, so it's not too bad to \"make up\" interval estimates to go into it. We know much more about the charities GiveWell studies, so made-up distributions (even those informed by reliable point-estimates) are much less permissible.</li></ul><p>Thanks again, and do let me know what you think!</p>", "parentCommentId": "4jYvGf4aw7Er4CrKw", "user": {"username": "Stan Pinsent"}}, {"_id": "d7nt3sRFXjDoeZuSR", "postedAt": "2023-04-01T18:43:39.612Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>Nice, thanks for the update!</p><blockquote><p>You have convinced me that the sensitivity of lognormal means to heavy right tails is a strength, not a weakness!</p></blockquote><p>Yes, but only as long as we think the heavy right tail is being accurately modelled! Jaime Sevilla has <a href=\"https://forum.effectivealtruism.org/posts/acREnv2Z5h4Fr5NWz/my-current-best-guess-on-how-to-aggregate-forecasts\">this</a> post on which methods to use to aggregate forecasts.</p><blockquote><p>Interval-estimate work is more honest about uncertainty, but runs the risk of encouraging overconfidence in the final distribution.</p></blockquote><p>I think it is worth flagging that risk, but I would say:</p><ul><li>In general, if a given method is more accurate, it seems reasonable to follow that method everything else equal.</li><li>One can always warn about not overweighting results estimated with intervals.</li><li>Intuitively, there seems to be much higher risk of being overconfident about a point estimate than about a mean estimated with intervals together with a confidence interval. For example, regarding Toby Ord's best guess given in Table 6.1 of The Precipice for the existential risk from nuclear war between 2021 and 2120, I think it is easier to be overconfident about A than B:<ul><li>A. <strong>0.1 %</strong>.</li><li>B. <strong>0.1 % (90 % confidence interval, 0.03 % to 0.3 %)</strong>. Toby mentions that:<ul><li>\"There is significant uncertainty remaining in these estimates and they should be treated as representing the right order of magnitude\u2014each could easily be a factor of 3 higher or lower\".</li></ul></li></ul></li></ul><blockquote><p>But the lognormal mean appears to be sensitive to the size of the confidence interval you use to calculate it - which means subjective methods are required to pick the size, introducing bias.</p></blockquote><p>Yes, for the same median, the wider the interval, the greater the mean. If one is having a hard time linking 2 given estimates to a confidence interval, one can try the narrowest and widest reasonable intervals, and see if the lognormal mean will vary a lot.</p><blockquote><p>We know much more about the charities GiveWell studies, so made-up distributions (even those informed by reliable point-estimates) are much less permissible.</p></blockquote><p>I think people with knowledge about GiveWell's cost-effectiveness analyses would be able to come up with reasonable distributions. A point estimate is equivalent to assigning probability 1 to that estimate, and 0 to all other outcomes, so it is easy to come up with something better (although it may well not be worth the effort).</p>", "parentCommentId": "9RwCH63NwhkaDtmEC", "user": {"username": "vascoamaralgrilo"}}, {"_id": "7GpSAvaXL22chu4vr", "postedAt": "2023-04-03T19:08:27.330Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>Thanks again!</p><p>I think I have been trying to portray the point-estimate/interval-estimate trade-off as a difficult decision, but probably interval estimates are the obvious choice in most cases.</p><p>So I've re-done the \"Should we always use interval estimates?\" section to be less about pros/cons and more about exploring the importance of communicating uncertainty in your results. I have used the Ord example you mentioned.</p>", "parentCommentId": "d7nt3sRFXjDoeZuSR", "user": {"username": "Stan Pinsent"}}, {"_id": "77qu5bPLkEp7snfy7", "postedAt": "2023-04-03T21:19:16.074Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>Makes sense, thanks!</p>", "parentCommentId": "7GpSAvaXL22chu4vr", "user": {"username": "vascoamaralgrilo"}}, {"_id": "j2LwMGTvQiS2uSe52", "postedAt": "2023-04-15T11:13:55.242Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>Hi Stan,</p><blockquote><h2><strong>Point estimates are fine for multiplication, lossy for division</strong></h2></blockquote><p>One way of getting around this is transforming all divisions into multiplications. For example, one can calculate E(X/Y) from E(X)*E(1/Y) (assuming independence), instead of using E(X)/E(Y). Computing E(1/Y) will require using <a href=\"https://www.getguesstimate.com/\">Guesstimate</a> or similar, but then the mean can be used in a spreadsheet without having to run a full Monte Carlo simulation, which would take longer.&nbsp;</p><p>I am not sure, but I think a similar approach can be followed for most estimates. For example, one can use Guesstimate to obtain E(X^alpha) or E(log(X)), instead of using E(X)^alpha or log(E(X)).&nbsp;</p><p>Using intervals is still useful to get ranges for the outputs in a principled way, but I wonder whether the expected value alone is enough. I think expected utility is all that matters, so there is a sense in which the expected value captures all the relevant information.</p><p>I suppose I have been using interval estimates because I think they can inform how much the expected value might change in response to new information, which is useful to know. However, I am not confident uncertainty, which is what can be directly observed from the outputted intervals, is a good proxy for <a href=\"https://forum.effectivealtruism.org/topics/credal-resilience\">resilience</a>.</p><p>I think I have come to believe assessing resilience doing a sensitivity analysis with point estimates derived from distributions is usually better than trying to evaluate it based on the uncertainty of the final result.</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "jwL94JogmtkeZHY2x", "postedAt": "2023-04-15T16:38:37.632Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>Note that 1/Y is generally not well defined when Y's range contains 0, and it's messy when it approaches it, and when both X and Y contains both positive and negative parts. My preferred solution is to either look at Xs and Ys that are both positive, or to look at the joint pdf of X and Y, rather than the sum.</p>", "parentCommentId": "j2LwMGTvQiS2uSe52", "user": {"username": "NunoSempere"}}, {"_id": "8GQPbktnku6RwuRhK", "postedAt": "2023-04-15T20:59:10.593Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>Hi Nu\u00f1o,</p><p>Nice points!</p><blockquote><p>Note that 1/Y is generally not well defined when Y's range contains 0, and it's messy when it approaches it</p></blockquote><p>I agree. Just one note, I think a distribution for Y which encompasses 0 cannot be correct, because it would lead to infinities, which <a href=\"https://forum.effectivealtruism.org/posts/weBqHK7J32FE5oozd/why-i-am-happy-to-reject-the-possibility-of-infinite-worlds\">I am happy to reject</a>. Can you give some examples in which Y (i.e. a distribution in the denominator) is defined such that it could not be zero, but you still found messiness?</p><blockquote><p>when both X and Y contains both positive and negative parts</p></blockquote><p>For this case, one can get point estimates from:</p><ul><li>E(X) = P(X &gt; 0)*E(X | X &gt; 0) + P(X &lt; 0)*E(X | X &lt; 0).</li><li>E(1/Y) = P(Y &gt; 0)*E(1/Y | Y &gt; 0) + P(Y &lt; 0)*E(1/Y | Y &lt; 0).</li></ul>", "parentCommentId": "jwL94JogmtkeZHY2x", "user": {"username": "vascoamaralgrilo"}}, {"_id": "HuvqQkvYdLjqT6oB3", "postedAt": "2023-04-16T17:25:37.203Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<blockquote><p>Just one note, I think a distribution for Y which encompasses 0 cannot be correct</p></blockquote><p>This may not buy you enough. E.g., sometimes you may want to calculate the $/life saved, where life saved is a distribution which could be 0.</p>", "parentCommentId": "8GQPbktnku6RwuRhK", "user": {"username": "NunoSempere"}}, {"_id": "rBLRfMgtGEWJguNKX", "postedAt": "2023-04-16T20:51:00.408Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>I think that in practice you (almost) always want to calculate <a href=\"https://forum.effectivealtruism.org/posts/SesLZfeYsqjRxM6gq/probability-distributions-of-cost-effectiveness-can-be\">lives/$, not $/life</a>, and the cost is practically never zero</p>", "parentCommentId": "HuvqQkvYdLjqT6oB3", "user": {"username": "Lorenzo Buonanno"}}, {"_id": "cjdhHjj287RbwJB6H", "postedAt": "2023-04-17T06:47:59.846Z", "postId": "2jHurMmzvyNbeEtCd", "htmlBody": "<p>Hi Lorenzo,</p><blockquote><p>I think that in practice you (almost) always want to calculate <a href=\"https://forum.effectivealtruism.org/posts/SesLZfeYsqjRxM6gq/probability-distributions-of-cost-effectiveness-can-be\">lives/$, not $/life</a></p></blockquote><p>Yes, I prefer to calculate the cost-effectiveness in terms of benefits per unit cost. This way, the expected cost-effectiveness can be multiplied by the cost to obtain the expected benefits. In contrast, the cost cannot be divided by the expected cost per unit benefit to obtain the expected benefits.</p><p>Another advantage of benefits per unit cost is that they always increase with the goodness of the intervention, whereas the cost per unit benefit has a more confusing relationship (when it can be both positive and negative).</p><blockquote><p>the cost is practically never zero</p></blockquote><p>Yes, I do not think the cost can be zero. Even if the monetary cost is zero, there are always time costs.</p>", "parentCommentId": "rBLRfMgtGEWJguNKX", "user": {"username": "vascoamaralgrilo"}}]