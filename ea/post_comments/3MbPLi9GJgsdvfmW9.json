[{"_id": "oeAaaT6zhjQmrrkmL", "postedAt": "2018-01-14T12:01:42.745Z", "postId": "3MbPLi9GJgsdvfmW9", "htmlBody": "<p>I haven't read the whole paper yet, so forgive me if I miss some of the major points by just commenting on this post.</p>\n<p>The image seems to imply that non-aligned AI would only extinguish human life on Earth. How do you figure that? It seems that an AI could extinguish all the rest of life on Earth too, even including itself in the process. [edit: this has since been corrected in the blog post]</p>\n<p>For example, you could have an AI system that has the objective of performing some task X, before time Y, without leaving Earth, and then harvests all locally available resources in order to perform that task, before eventually running out of energy and switching off. This would seem to extinguish all life on Earth by any definition.</p>\n<p>We could also discuss whether AI might extinguish all civilizations in the visible universe. This also seems possible. One reason for this is that humans might be the only civilization in the universe.</p>\n", "parentCommentId": null, "user": {"username": "RyanCarey"}}, {"_id": "zR8frTz3h3He5jbak", "postedAt": "2018-01-14T12:26:40.145Z", "postId": "3MbPLi9GJgsdvfmW9", "htmlBody": "<p>No, in the paper we clearly said that non-alaigned AI is the risk to the whole universe in the worst case scenario. </p>\n", "parentCommentId": "oeAaaT6zhjQmrrkmL", "user": {"username": "turchin"}}, {"_id": "z2jpxxv6hrjkawNgy", "postedAt": "2018-01-14T13:51:43.157Z", "postId": "3MbPLi9GJgsdvfmW9", "htmlBody": "<p>Also, the image above indicates AI would likely destroy all life on earth, not only human life. </p>\n", "parentCommentId": "oeAaaT6zhjQmrrkmL", "user": {"username": "Khorton"}}, {"_id": "NhEmNaK4yPXyBv7yr", "postedAt": "2018-01-14T14:16:40.617Z", "postId": "3MbPLi9GJgsdvfmW9", "htmlBody": "<p>It is hard to encapsulate this all into a simple scale, but we wanted to recognize that false vacuum decay that would destroy the Universe at light speed would be worse than bad AI, at least if you think the future will be net positive. Bad AI could be constrained by a more powerful civilization.</p>\n", "parentCommentId": "oeAaaT6zhjQmrrkmL", "user": {"username": "Denkenberger"}}, {"_id": "RgN9sEGgmvyNuz9Nm", "postedAt": "2018-01-14T14:20:48.114Z", "postId": "3MbPLi9GJgsdvfmW9", "htmlBody": "<p>In the article AI is destroying all life on earth, but on the previous version of the image in this blog post the image was somewhat redesign to better visibility and  AI risk jumped to the kill all humans. I corrected the image now, so it is the same as in the article,  - so the previous comment was valid. </p>\n<p>Will the AI be able to destroy other civilizations in the universe depends on the fact if these civilizations will create their own AI before intelligence explosion wave from us arrive to them. </p>\n<p>So AI will kill only potential and young civilizations in the universe, but not mature civilizations.</p>\n<p>But it is not the case for false vacuum decay wave which will kill everything (according to our current understanding of AI and vacuum).</p>\n", "parentCommentId": "z2jpxxv6hrjkawNgy", "user": {"username": "turchin"}}, {"_id": "ogniLpmzYLzHwghny", "postedAt": "2018-01-14T18:21:37.445Z", "postId": "3MbPLi9GJgsdvfmW9", "htmlBody": "<p>This seems like a good project and I found the 2-axis picture helpful. The only bit that stood out was global warming. I'm not sure how you're defining it but my sense is that global warming of some sort seems pretty likely to be a problem in the next 100 years. If you mean a particularly severe form of global warming, it might help to have a more expressive term like &quot;runaway climate change&quot; or &quot;severe climate change&quot; and possible also a term for a more moderate form that appears in another box.</p>\n", "parentCommentId": null, "user": {"username": "MichaelPlant"}}, {"_id": "3zfhBDGRRk2XcYacg", "postedAt": "2018-01-14T22:05:30.487Z", "postId": "3MbPLi9GJgsdvfmW9", "htmlBody": "<p>Surely, there are two types of global warming.</p>\n<p>I think that risks of runaway global warming are underestimated, but there is very small scientific literature to support the idea. </p>\n<p>If we take accumulated tall from smaller effects of the long-term global warming of 2-6C, it could be easily calculated as a very larger number, but to be regarded as a global catastrophe, it probably should be more like a one-time event, or many other things will be also a global catastrophe, like cancer etc.</p>\n", "parentCommentId": "ogniLpmzYLzHwghny", "user": {"username": "turchin"}}, {"_id": "gRqBx2boBwcWtq4B5", "postedAt": "2018-02-27T00:27:42.682Z", "postId": "3MbPLi9GJgsdvfmW9", "htmlBody": "<p>Modeling global catastrophic risks as single events makes sense for a lot of purposes. In terms of preventive actions to be taken, and how resources from, e.g., governments should be divvied up between risks, insuring against one-off catastrophes are important. I think it makes sense to exclude climate change itself from the model, and include potentially catastrophic events which may result from runaway climate change, like worldwide famines. </p>\n<p>Another way to model global catastrophic risks is to model their rising profile as a function of time, and past which critical milestones will the scale or degree of the risk irreversibly escalate. This way for the purposes of x-risk studies we can have a model which incorporates both the risk climate change poses over time, versus the development of emerging technologies like AI and genetic engineering over the same period of time.</p>\n", "parentCommentId": "3zfhBDGRRk2XcYacg", "user": {"username": "Evan_Gaensbauer"}}]