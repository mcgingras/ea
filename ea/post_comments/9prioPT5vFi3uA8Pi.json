[{"_id": "mR42i7opt7g59NTMo", "postedAt": "2024-02-26T13:40:51.218Z", "postId": "9prioPT5vFi3uA8Pi", "htmlBody": "<p><strong>Executive summary</strong>: This appendix provides additional details and clarifications on topics covered in <a href=\"https://forum.effectivealtruism.org/posts/JGazpLa3Gvvter4JW/cooperating-with-aliens-and-distant-agis-an-ecl-explainer-1\">the main \"ECL explainer\" post.</a></p><p><strong>Key points</strong>:</p><ol><li>The post discusses the difference between an agent's total acausal influence and their relevant acausal influence - the part they can strategically use. The latter may be much smaller.</li><li>It elaborates on the potentially vast spatial and temporal size of the universe, and the concept of Everett branches, to emphasize the vast number of agents that may exist.</li><li>It argues there may be large gains from cooperation due to diminishing returns, comparative advantage, and combinability of values. However, the magnitude is uncertain.</li><li>It explains why acausally influencing agents to harm us via cooperation is unlikely. Direct harm seems implausible, while harm through opportunity cost depends on the relative strength of influence over different agents.</li><li>It notes that whether ECL is worthwhile depends on the ratio of influence over agents with shared versus different values, and the relative benefits of cooperation. The post is overall positively disposed towards ECL.</li></ol><p>&nbsp;</p><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}]