[{"_id": "JfJ7zCqcK5EqWAnHz", "postedAt": "2024-01-15T21:19:56.477Z", "postId": "LsfPqfn8diuBwRYat", "htmlBody": "<p>I generally agree with this critique.</p><p>A while back I wrote about an <a href=\"https://forum.effectivealtruism.org/posts/EqtGTmxrahdt3LKky/org-proposal-effective-foundations\">idea for an org that focuses on redirecting US private foundation grants toward more effective causes</a>. Got a lot of feedback, and the consensus was that existing private foundations just aren't tractable. And I tend to agree with that.</p><p>But I have been working on a research paper where we interview private foundation grantmakers to try to better understand how they operate and the information used in their decision making. One of the takeaways is that <a href=\"https://www.trustbasedphilanthropy.org/\">trust-based philanthropy</a> has had HUGE influence on private foundation grantmaking, despite being very new (every participant we interviewed indicated their foundation had implemented at least some trust based philanthropy practices).&nbsp;</p><p>This got me thinking - has EA had any influence? Not a single participant indicated that EA had influenced their grantmaking, and I would say that 75% were neutral and 25% were <i>openly hostile </i>to the idea of EA influencing their grantmaking.&nbsp;</p><p>I think EA would benefit from conversations around how to sell EA ideas to these other groups. I think it would require what some would view as \"watering down\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpwlzmr6pkj\"><sup><a href=\"#fnpwlzmr6pkj\">[1]</a></sup></span>&nbsp;of EA principles, but could substantially increase the overall impact of EA. Definitely interesting to think about what aspects of EA could be compromised before it ceases to be EA at all.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpwlzmr6pkj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpwlzmr6pkj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For example, most US private foundations are severely constrained by the original founder's intent, such as spending funds in X geographic area. Could these foundations be persuaded and made more effective through a version of EA that encourages effective giving, given existing foundation constraints?</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Kyle Smith"}}, {"_id": "SpraxqmxPghAcF8nQ", "postedAt": "2024-01-16T11:30:28.500Z", "postId": "LsfPqfn8diuBwRYat", "htmlBody": "<p>What about the <a href=\"https://forum.effectivealtruism.org/topics/effective-institutions-project?sortedBy=magic\">Effective Institutions Project</a> (<a href=\"https://effectiveinstitutionsproject.org/\">website</a>)? While they haven't posted on the EA Forum in a while, I remember the <a href=\"https://forum.effectivealtruism.org/posts/ttpSEgE3by7AAhQ7w/improving-institutional-decision-making-which-institutions-a#Case_studies\">case studies</a> from their \"which institutions? a framework\" writeup and their <a href=\"https://forum.effectivealtruism.org/posts/tNq65hopAmX3YeKoL/a-landscape-analysis-of-institutional-improvement\">landscape analysis of institutional improvement opportunities</a> (Amazon, the Chinese Communist Party\u2019s Politburo, DeepMind, Meta, OpenAI, the Executive Office of the US President, and the US National Security Council make the top 10 in both their neartermist and longtermist rankings; Google, the State Council of China, and the World Health Organization round out their neartermist list and Alphabet, the European Union, and the US Congress round out their longtermist one).&nbsp;</p>", "parentCommentId": null, "user": {"username": "Mo Nastri"}}, {"_id": "RNpnoeD9vmoAsRfso", "postedAt": "2024-01-16T15:01:36.380Z", "postId": "LsfPqfn8diuBwRYat", "htmlBody": "<p>I agree. Involving other actors forces us to examine deeply EA's weirdness and unappealing behaviours, and brings a ton of experience, network, and amplifies impact.&nbsp;</p><p>This is something that I have been seriously thinking about when organizing big projects, especially when it comes to determine the goals of a conference and the actors that we choose to invite. Specifically in a theme such as AI safety, where safety concerns should be propelled and advertised in policy among other non-EA policy actors.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Vaipan"}}, {"_id": "SZd8zEqWeaPSswwry", "postedAt": "2024-01-16T17:15:12.595Z", "postId": "LsfPqfn8diuBwRYat", "htmlBody": "<p>left some comments on the doc \u2014 i overall agree with this critique, but would like to see a bit more on your thoughts driving the research you've already done.</p>", "parentCommentId": null, "user": {"username": "Saul"}}]