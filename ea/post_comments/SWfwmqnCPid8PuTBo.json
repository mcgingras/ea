[{"_id": "oCasjkNkQrWPE2DBT", "postedAt": "2023-09-24T08:16:11.013Z", "postId": "SWfwmqnCPid8PuTBo", "htmlBody": "<p>There are also big incentive gradients <i>within</i> longtermism:</p><ul><li>To work on AI experiments rather than AI theory (higher salary, better locations, more job security)</li><li>To work for a grantmaker rather than a grantee (for job security), and</li><li>To work for an AI capabilities company rather than outside (higher salary)</li></ul>", "parentCommentId": null, "user": {"username": "RyanCarey"}}, {"_id": "pD3bstbZMBKZ5S9kW", "postedAt": "2023-09-24T11:18:30.651Z", "postId": "SWfwmqnCPid8PuTBo", "htmlBody": "<blockquote>\n<p>Biosecurity is a well-established field outside of EA, and there are many excellent upskilling opportunities outside the movement (e.g. pursuing George Mason Global Biodefense Masters, joining professional societies like ABSA, engaging with the UN and WHO)</p>\n</blockquote>\n<p>While there are people in the broader biosecurity field doing good work, my impression is this is the exception. There's a ton of work done without a threat model or with what I (and I think most people who thought about it for a bit from an EA perspective) would say is a threat model that neglects the ways the world has been changing and is likely to continue to change. I don't see EAs preferring to join EA biosecurity groups over other groups in the biosecurity field as something that commonly puts them in less impactful roles.</p>\n", "parentCommentId": null, "user": {"username": "Jeff_Kaufman"}}, {"_id": "QFyGwXJzcz8LAsjmp", "postedAt": "2023-09-24T11:20:38.400Z", "postId": "SWfwmqnCPid8PuTBo", "htmlBody": "<ul>\n<li>To work in AI instead of other areas (higher salary, topic is shiny)</li>\n</ul>\n<p>(Disclosure: I decided to work in biorisk and not AI)</p>\n", "parentCommentId": "oCasjkNkQrWPE2DBT", "user": {"username": "Jeff_Kaufman"}}, {"_id": "7KfLxXPcLEZzsB6uj", "postedAt": "2023-09-24T15:18:01.091Z", "postId": "SWfwmqnCPid8PuTBo", "htmlBody": "<p>Wow very well put. This is the one that scares me the most out of these three, and I think there could be more exploring to be done as to first, how strong an incentive this might be, and then how that incentive can change people's view on their job and AI</p>\n<p>\"To work for an AI capabilities company rather than outside (higher salary)\"</p>\n<p>I know it's a side note not directly related to the original question, but I would be interested to see data comparing</p>\n<ol>\n<li>\n<p>Safety researchers' pdoom who work for AI capabilities companies vs. Those who work for independent safety orgs (this might have been done already)</p>\n</li>\n<li>\n<p>What proportion of AI safety people who started working for capabilities orgs have moved on over time (I would call it defected) to working more on capabilities than alignment.</p>\n</li>\n</ol>\n", "parentCommentId": "oCasjkNkQrWPE2DBT", "user": {"username": "NickLaing"}}, {"_id": "EJdNyhZWh4M7SGHmQ", "postedAt": "2023-09-24T20:03:06.806Z", "postId": "SWfwmqnCPid8PuTBo", "htmlBody": "<p>These are all great points. I was planning to add this into the main post, but I don't think it ended up in the final draft - so thanks for raising this!&nbsp;</p>", "parentCommentId": "oCasjkNkQrWPE2DBT", "user": {"username": "vaidehi_agarwalla"}}]