[{"_id": "tKs9jsw5ePCtCTtjo", "postedAt": "2024-02-13T23:18:20.109Z", "postId": "3SvCxuB46so7ynWmx", "htmlBody": "<p>Another argument for asymmetric preference views (including antifrustrationism) and preference-affecting views over total symmetric preference views is that the total symmetric views are actually pretty intrapersonally alienating or illiberal in principle, and possibly in practice in the future with more advanced tech or when we can reprogram artificially conscious beings.</p>\n<p>Do you care a lot about your family or other goals? Nope! I can make you care and approve way more about having a green cube and your new life centered on green cubes, abandoning your family and goals. You'll be way better off. Even if you disprefer the prospect now, I'll make sure you'll be way more grateful after with your new preferences. The gain will outweigh the loss.</p>\n<p>Basically, if you can manipulate someone's mind to have additional preferences that you ensure are satisfied, as long as the extra satisfaction exceeds the frustration from involuntarily manipulating them, it\u2019s better for them than leaving them alone.</p>\n<p>Asymmetric and preference-affecting views seem much less vulnerable to this, as long as we count as bad the frustration involved in manipulating or eliminating preferences, including preferences against certain kinds of manipulation and elimination. For example, killing someone in their sleep and therefore eliminating all their preferences has to still typically be bad for someone who would disprefer it, even if they don\u2019t find out. The killing both frustrates and eliminates their preferences basically simultaneously, but we assume the frustration is still bad. And new satisfied preferences wouldn't make up for the frustration on these views.</p>\n<p>This is the problem of replacement/replaceability, applied intrapersonally to preferences and desires.</p>\n", "parentCommentId": null, "user": {"username": "MichaelStJules"}}, {"_id": "q5HhtwPjGDPrEHtcC", "postedAt": "2024-02-14T00:33:27.554Z", "postId": "3SvCxuB46so7ynWmx", "htmlBody": "<p>Related to your point 1 :&nbsp;<br><br>I think one concrete complexity-increasing ingredient that many (but not all) people would want in a utopia is for one's interactions with other minds to be authentic \u2013 that is, they want the right kind of \"<a href=\"https://www.lesswrong.com/posts/r7f58E8A85xLgWuqG/contact-with-reality\">contact with reality</a>.\"<br><br>So, something that would already seem significantly suboptimal (to some people at least) is lots of private experience machines where everyone is living a varied and happy life, but everyone's life in the experience machines follows pretty much the same template and other characters in one's simulation aren't genuine, in the sense that they don't exist independently of one's interaction with them (meaning that your simulation is solipsistic and other characters in your simulation may be computed to be the most exciting response to you, but their memories from \"off-screen time\" are fake). So, while this scenario would already be a step upwards from \"rats on heroin\"/\"brains in a vat with their pleasure hotspots wire-headed,\" it's still probably not the type of utopia many of us would find ideal. Instead, as social creatures who value meaning, we'd want worlds (whether simulated/virtual or not doesn't seem to matter) where the interactions we have with other minds are genuine. That these other minds wouldn't just be characters programmed to react to us, but real minds with real memories and \"real\" (as far as this is a coherent concept) choices. Utopian world setups that allow for this sort of \"contact with reality\" presumably cannot be packed <i>too tightly</i> with sentient minds.<br><br>By contrast, things seem different for dystopias, which can be packed tightly. For dystopias, it matters less whether they are repetitive, whether they're lacking in options/freedom, or whether they have solipsistic aspects to them. (If anything, those features can make a particular dystopia more horrifying.)</p><p>To summarize, here's an excerpt from <a href=\"https://forum.effectivealtruism.org/posts/8yaQ6i3oaFLprsFyb/ai-alignment-researchers-may-have-a-comparative-advantage-in#Normative_reasons_for_dedicating_some_attention_to_s_risks\">my post</a> on alignment researchers arguably having a comparative advantage in reducing s-risks:</p><blockquote><p><strong>Asymmetries between utopia and dystopia.</strong> It seems that we can \u201cpack\u201d more bad things into dystopia than we can \u201cpack\u201d good things into utopia. Many people presumably value freedom, autonomy, some kind of \u201c<a href=\"https://www.lesswrong.com/posts/r7f58E8A85xLgWuqG/contact-with-reality\">contact with reality</a>.\u201d The <i>opposites</i> of these values are easier to implement and easier to stack together: dystopia can be repetitive, solipsistic, lacking in options/freedom, etc. For these reasons, it feels like there\u2019s at least&nbsp;<i>some type of</i> asymmetry between good things and bad things \u2013&nbsp;even if someone were to otherwise see them as completely symmetric.</p></blockquote>", "parentCommentId": null, "user": {"username": "Lukas_Gloor"}}, {"_id": "Tmfj7naENxwkTFbwh", "postedAt": "2024-02-14T06:01:17.133Z", "postId": "3SvCxuB46so7ynWmx", "htmlBody": "<p>I don't understand the point about the complexity of value being greater than the complexity of suffering (or disvalue). Can you possibly motivate the intuition here? It seems to me like I can reverse the complex valuable things that you name, and I get their \"suffering equivalents\" e.g. (e.g. friendship -&gt; hostility, happiness -&gt; sadness, love -&gt; hate ... etc.), and they don't feel significantly less complicated.&nbsp;<br><br>I don't know exactly what it means for these things to be less complex; I'm imagining something like writing a Python program that simulates the behaviour of two robots in a way that is recognisable to many people as \"friends\" or \"enemies\" and measuring at the length of the program.</p>", "parentCommentId": null, "user": {"username": "calebp"}}, {"_id": "FHZwThCga3xJLsx4Y", "postedAt": "2024-02-14T14:52:10.769Z", "postId": "3SvCxuB46so7ynWmx", "htmlBody": "<p><strong>Executive summary</strong>: The post argues that several common philosophical views - including the complexity of value, moral uncertainty, and caring about preference satisfaction - actually imply putting more priority on avoiding catastrophic suffering (s-risk) than maximizing positive value.</p><p><strong>Key points:</strong></p><ol><li>Complexity of value but simplicity of suffering implies suffering is more energy-efficient, so worst cases outweigh best cases.</li><li>Moral uncertainty leans towards suffering being worse than happiness is good.</li><li>Caring about preferences suggests anti-frustrationism, which weighs dissatisfaction more than satisfaction.</li><li>These undermine common appeals to symmetry as a reason not to focus on s-risk.</li><li>They suggest moral views often point to more concern for avoiding the worst rather than maximizing the best.</li><li>None definitively say s-risk should dominate, but shift priority towards avoiding downside risks.</li></ol><p>&nbsp;</p><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}, {"_id": "wpo5zhZBouut4HwvM", "postedAt": "2024-02-14T21:40:55.690Z", "postId": "3SvCxuB46so7ynWmx", "htmlBody": "<p>I have two points regarding point 2. Firstly, what matters is the relationship between the expected happiness and the expected suffering, not the best happiness and the worst suffering. There is no particular reason that these relationships should be the same. It may be that the worst suffering outweighs the best happiness, and also that the expected happiness outweighs the expected suffering.<br><br>Secondly, why do you think people would skew towards the suffering dominating? My intuition is that the expected happiness will generally dominate. I've noticed there are a subset of EAs who seem to have an obsession with suffering, and the related position of anti-natalism, but I do not think EAs are representative of the broader population in this regard, and I do not think this subset of EAs are epistemically justified.</p>", "parentCommentId": null, "user": {"username": "River"}}, {"_id": "KpMHttJojFLLSoJMM", "postedAt": "2024-02-15T02:32:10.257Z", "postId": "3SvCxuB46so7ynWmx", "htmlBody": "<p>It's not that there aren't similarly complex reverses, it's that there's a type of bad that basically everyone agrees can be extremely bad, i.e. extreme suffering, and there's no (or much less) consensus on a good with similar complexity and that can be as good as extreme suffering is bad. For example, many would discount pleasure/joy on the basis of false beliefs, like being happy that your partner loves you when they actually don't, whether because they just happen not to love you and are deceiving you, or because they're a simulation with no feelings at all. Extreme suffering wouldn't get discounted (much) if it were based on inaccurate beliefs.</p><p>A torturous solipsistic experience machine is very bad, but a happy solipsistic experience machine might not be very good at all, if people's desires aren't actually being satisfied and they're only deceived into believing they are.</p>", "parentCommentId": "Tmfj7naENxwkTFbwh", "user": {"username": "MichaelStJules"}}, {"_id": "BcTaBWf82qxeDwKKb", "postedAt": "2024-02-19T20:22:39.845Z", "postId": "3SvCxuB46so7ynWmx", "htmlBody": "<p>Yes, you're totally right that I was just speaking about the range and not the expectation! That's part of the reason why I said none of the points I made are decisive for working on s-risk. I was only providing arguments against the position that the range is symmetric, which I often see people take.</p>", "parentCommentId": "wpo5zhZBouut4HwvM", "user": {"username": "Chi"}}]