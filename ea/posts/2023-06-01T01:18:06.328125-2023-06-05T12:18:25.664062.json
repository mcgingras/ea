[{"_id": "iykkkxvwcuySBeTEL", "title": "ALTER Israel - 2023 Mid-Year Update", "postedAt": "2023-06-05T08:50:33.605Z", "htmlBody": "<p><a href=\"https://alter.org.il/\">ALTER</a> is an organization in Israel that works on several EA priority areas and causes. This semiannual update is intended to inform the community of what we have been doing, and provide a touchpoint for those interested in engaging with us. Since <a href=\"https://forum.effectivealtruism.org/posts/aE68kcbG7ugCcTKXd/alter-israel-end-of-2022-update\">the last update at the beginning of 2023</a>, we have made progress on a number of areas, and have ambitious ideas for future projects.&nbsp;</p><h2>Progress to Date</h2><p>Since its founding, ALTER has started and run a number of projects.</p><ol><li>Organized and managed an AI safety conference in Israel,&nbsp;<a href=\"https://aisic2022.net.technion.ac.il/\"><u>AISIC 2022</u></a> hosted at the Technion, bringing in several international speakers including Stuart Russell, to highlight AI Safety focused on existential-risk and global-catastrophic-risk, to researchers and academics in Israel. This was successful in raising the profile of AI safety here in Israel, and in helping identify prospective collaborators and researchers.</li><li>Support for Vanessa Kosoy\u2019s&nbsp;<a href=\"https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023\"><u>Learning-Theoretic Safety Agenda</u></a>, including an ongoing&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zCYGbYAaXeq7v67Km/prize-and-fast-track-to-alignment-research-at-alter\"><u>prize competition</u></a>, and work to hire researchers working in the area.</li><li>Worked with Israel\u2019s foreign ministry, academics here in Israel, and various delegations to and organizations at the Biological Weapons Convention to find avenues to promote Israel\u2019s participation.</li><li>Launched our project to get the Israeli government to iodize salt, to mitigate or eliminate the current iodine deficiency that we estimate causes an expected 4-IQ point loss to the median child born in Israel today.</li><li>Worked on mapping the current state of metagenomic sequencing usage in Israel, in order to prepare for a potential use of widespread metagenomic monitoring for detecting novel pathogens.</li><li>Organized and hosted a closed Q&amp;A with Eliezer Yudkowsky while he was visiting Israel, for 20 people in Israel working on or interested in contributing to AI safety. This was followed by a larger LessWrong meetup with additional attendees.</li></ol><h2>Current and Ongoing Work</h2><p>We have a number of ongoing projects related to both biorisk and AI safety.&nbsp;</p><ol><li>Fellowship program. We have started this program to support researchers interested in developing research agendas relevant to AI safety. Ram Rahum is our inaugural funded AI safety fellow, who was found via our AI Safety conference. Since then, he has co-organized a<a href=\"https://sites.google.com/view/rad-ai/home\"><u> conference in London on rebellion and disobedience in AI</u></a> jointly with academics in Israel, the US, and the UK. As a fellow, he is also continuing to work with academics in Israel as well as a number of researchers at Deep Mind on understanding strategic deception and multi-agent games and dynamics for ML systems. His research home is&nbsp;<a href=\"https://publish.obsidian.md/ram-rachum-research/Public/Research+home\"><u>here</u></a> and monthly updates are&nbsp;<a href=\"https://groups.google.com/g/ram-rachum-research-announce\"><u>here</u></a>. Rona Tobolsky is a policy fellow, and is also working with us on policy, largely focused on biorisk and iodization.</li><li>Support for Vanessa Kosoy\u2019s Learning-Theoretic AI Safety Agenda. To replace the former FTX funding, we have been promised funding from an EA donor lottery to fund a researcher working on the learning-theoretic safety agenda. We are working on recruiting a new researcher, and are excited about expanding this. Relatedly, we are helping support a&nbsp;<a href=\"https://www.lesswrong.com/posts/HtxLbGvD7htCybLmZ/singularities-against-the-singularity-announcing-workshop-on\"><u>singular learning theory workshop</u></a>.&nbsp;</li><li>Biosecurity. David Manheim and Rona Tobolsky attended the Biological Weapons Convention - Ninth Review Conference, and have continued looking at ways to push for greater participation by Israel, which is not currently a member. David will also be attending a UNIDIR conference on biorisk in July. We are also continuing to explore additional pathways for Israel to contribute to global pandemic preparedness, especially around PPE and metagenomic biosurveillance.</li><li>AI field building. Alongside other work to build AI-safety work in Israel, ALTER helped initiate a round of the AGI Safety Fundamentals 101 program in Israel, and will be running a second round this year. We are also collaborating with EA Israel to host weekly co-working sessions on AI safety, and will hope to continue to expand this. David Manheim has also worked on a number of small projects in AI governance, largely collaboratively with other groups.&nbsp;</li></ol><h2>Potential Future Projects and Expansion</h2><p>We are currently working on fundraising to continue current work and embark on several new initiatives, including expanding our fellowship program, expanding engagement on biorisk, and build out a more extensive program, hiring researchers and a research manager and running an internship program and/or academic workshop(s) focused on the learning theoretic alignment agenda. All of these are very tentative, and the specific plans will depend on both feedback from advisors and funding availability.</p><h2>Challenges and Missteps</h2><ol><li>Our initial hire to work on Vanessa\u2019s Learning-Theoretic agenda was not as successful as hoped. In the future, Vanessa plans to both provide more interaction and guidance, and to hire people once we understand their concrete plans to do work in the area. We are considering how to better support and manage research in order to expand this research portfolio. (We do not yet have funding for a research manager, the position is critical, and it may be difficult to find an appropriate candidate.)</li><li>Identifying whether ML-based AI safety research is strongly safety-dominant (rather than capabilities-dominant) can be challenging. This is a more general issue than an ALTER-specific challenge. David Manheim pre-screens research and research agendas, but has limited ability to make determinations, especially in cases where risks are non-obvious. We have relied on informal advice from AI safety researchers at other organizations to screen work being done, and matching fellows with mentors that are more capable of overseeing the research, but this is a bottleneck for promoting such research.</li><li>Banking issues following the collapse of FTX and difficulty navigating the Israeli banking system, including difficulty receiving other grants. (This is now largely resolved.)</li><li>Work on mandatory salt iodization in Israel has stalled somewhat, due partly to Israeli political conditions. Despite indications of support from the manufacturer, the Israeli Health Ministry has not prioritized this. We have several ideas for a path forward which are being pursued, but are unsure if the current government is likely to allow progress.</li></ol>", "user": {"username": "Davidmanheim"}}, {"_id": "qwh8gtzqpiERrnSiS", "title": "Some Decentralized myth for me", "postedAt": "2023-06-05T09:24:52.403Z", "htmlBody": "<p>I have read many posts about decentralized applications and their forms of existence.</p><p>However, there are rarely any posts about the ultimate direction this technology will lead humanity.</p><p>I want to discuss something other than decentralized applications and their current manifestations.</p><p>I want to know the essence of this idea and whether it truly represents a positive direction for human development.</p><p>I'm not a pro of decentralization; I have summarized five myths that trouble me the most:</p><ol><li>Isn't it ironic if I create a forum or group to discuss decentralization? By doing so, I would have control over posts and members, which contradicts the \"de\" centralization concept. In essence, I would still be reading information subject to human intervention. Can we really trust the information about decentralization in the Web2 world?</li><li>Assuming that we transition from centralization to decentralization in the future, how can we ensure that our interests won't disappear? Money holds value because of government backing, and governments symbolize centralization. The system is rarely challenged because governments have significant resources, which provides stability. Real money doesn't experience extreme fluctuations like cryptocurrencies unless a government goes bankrupt. So, in a decentralized world, what guarantees the value of our assets?</li><li>How can we maintain stability in a decentralized system? As I understand it, all systems rely on trust, whether forced or voluntary. If we become a P2P system society (i.e., external forces disappear), how can we ensure everyone will actively maintain this system?</li><li>Is decentralization just an early stage of centralization? Are the ideas of decentralization fundamentally composed of \"freedom,\" \"non-regulation,\" and \"fairness\"? If so, doesn't it sound like our human history repeating itself? We keep messing things up, then some guy emerges, unifies, develops, and then we fked it up again.</li><li>Do we really need decentralization? For ordinary people, it seems unnecessary to escape from the supervision of centralized systems. The greatest risk we face should be the central authority of the existing system having problems. However, at the same time, when we need help, the central authority also provides assistance (when the system is running stably). Based on my limited understanding, if I need help after decentralization, where would I get these resources? Would it rely on donations from others?</li></ol><p>I may have confusingly expressed myself because I need clarification on whether these questions are formulated correctly or valid.</p><p>I'm only raising my doubts about the idea and theory of decentralization based on what I know without considering its current practical applications.</p><p>I hope to have a friendly discussion with everyone, and by reading your comments, it can become a learning experience for me.</p>", "user": {"username": "Pete Misaki"}}, {"_id": "zsJaEWhjZgmSDEnvy", "title": "Malaria: Vaccine or Environmental Issue?", "postedAt": "2023-06-05T05:00:26.608Z", "htmlBody": "<p>Summary of key points:&nbsp;</p><ul><li>Have a model community within entrenched places that house malaria to allow them a change of identity towards a malaria-free world&nbsp;</li><li>Work with the government to enforce a policy on mandatory cleaning of the environment and good house planning.</li><li>Educate and work with religious head/community leaders in each locality on how to live in an environment cleanly place.</li></ul><p><br>&nbsp;</p><p>The relentless battle against malaria, a devastating disease primarily prevalent in Africa, has been ongoing for many years, resulting in a staggering number of fatalities annually. In an endeavor to combat this scourge, scientists from around the world have tirelessly developed and implemented vaccines, yielding positive results that warm the heart. However, it is crucial to acknowledge that addressing the environmental factors contributing to malaria could significantly augment the effectiveness of vaccines.</p><p><br>&nbsp;</p><p>While the use of vaccines has shown promise, it is imperative to tackle the underlying environmental issues that perpetuate the spread of malaria. One key aspect that requires attention is raising awareness and disseminating crucial information at the grassroots level. By instigating a transformation in people's attitudes and way of life, we can pave the way for a malaria-free world. Particularly in rural communities, there is a pressing need to educate individuals on living healthy lives, free from malaria and other tropical diseases. This can be achieved through robust advocacy campaigns facilitated by partnerships with relevant government agencies and non-governmental organizations.</p><p><br>&nbsp;</p><p>Moreover, religious leaders can play a pivotal role in this fight against malaria. Their close connection with the grassroots population positions them as influential figures who can help shape community perspectives and behaviors. Similarly, community leaders, who symbolize authority in their brutalities, carry significant weight in impacting change. For instance, in the northern part of Nigeria, community leaders command immense respect, and their involvement in malaria eradication efforts would undoubtedly have far-reaching effects.</p><p><br>&nbsp;</p><p>To further strengthen the battle against malaria, governments should prioritize the enactment and implementation of relevant laws about environmental sanitation. Swift action is required to activate existing bodies responsible for environmental sanitation or to improve their efficacy where they already exist. By doing so, we can achieve better outcomes in combating the menace of malaria.</p><p><br>&nbsp;</p><p>In conclusion, the creation of model communities solely dedicated to eradicating malaria could serve as a beacon of hope and inspiration for others. Organizations should provide necessary incentives to establish such communities, ensuring that they become not only buffer zones against the disease but also shining examples for emulation. By synergizing efforts, prioritizing environmental factors, and leveraging the power of collaboration, we can bring about a significant reduction in malaria cases and move closer to a world free from this debilitating disease.</p>", "user": {"username": "Olamide"}}, {"_id": "yJkZK62NKuRu7SaJw", "title": "AI Safety Fundamentals: An Informal Cohort Starting Soon! (cross-posted to lesswrong.com)", "postedAt": "2023-06-04T18:21:44.032Z", "htmlBody": "<p>Hi Everyone,</p>\n<p>We'll be starting an informal cohort on <a href=\"https://aisafetyfundamentals.com/ai-alignment-curriculum\">AI Safety Fundamentals</a>.</p>\n<p>Come join-us on <a href=\"https://discord.gg/gW9Kq6VHgM\">Discord</a>. We'll do a poll next week to find a slot that works for most people.\n(if you're interested in participating as an expert, do reach out as well)</p>\n<p>We'll do the kick-off session on the 17th of June, and then do the course at the pace of one session per week for eight weeks (you'll be able to discuss the material in-between with other members). I'm currently reaching out to aligment researchers interested in popping in for insights and answering questions we might have.</p>\n<p>We'll then do the <a href=\"https://aisafetyfundamentals.com/ai-governance-curriculum\">AI Governance course</a>, and then the <a href=\"https://aisafetyfundamentals.com/alignment-201-curriculum\">AI Alignment 201 course</a> - you should be able to singlehandedly solve AGI by the end.</p>\n<p>Please note that this is not an official cohort, there is one starting on governance this summer (you can join on AI Safety Fundamentals' <a href=\"https://aisafetyfundamentals.com/\">website</a>, and one on AI Alignement in September (registrations not opened yet).</p>\n<p>Bye!</p>\n<p>Tiago</p>\n", "user": {"username": "Tiago"}}, {"_id": "Ben4xi9W2k9jvnMky", "title": "[Fiction] A Disneyland Without Children", "postedAt": "2023-06-04T13:06:45.304Z", "htmlBody": "<p>The spaceship swung into orbit around the blue-grey planet with a final burn of its engines. Compared to the distance they had travelled, the world, now only some four hundred kilometres below and filling up one hemisphere of the sky, was practically within reach. But Alice was no less confused.</p><p>\"Well?\" she asked.</p><p>Charlie stared thoughtfully at the world slowly rotating underneath their feet, oceans glinting in the sunlight. \"It looks lickable\", he said.</p><p>\"We have a task\", Alice said, trying to sound gentle. Spaceflight was hard. Organic life was not designed for it. But their mission was critical, they needed to move fast, and Charlie, for all his quirks, would need to be focused.</p><p>\"What's a few minutes when it will take years for anything we discover to be known back home?\" Charlie asked.</p><p>\"No licking\", Alice said.</p><p>Charlie rolled his eyes, then refocused them on the surface of the planet below. They were just crossing the coast of one of the larger continents. Blue water was giving way to grey land.</p><p>\"Look at the texture\", Charlie said. They had seen it from far away with telescopes, but there was something different about seeing it with their bare eyes. Most of the land surface of the planet was like a rug of fine grey mesh. If there had been lights, Alice would have guessed the entire planet's land was one sprawling city, but as far as their instruments could tell, the world had no artificial lighting.</p><p>As far as they could tell, the world also had no radio. They had broadcast messages at every frequency they could, and in desperation even by using their engines to flash a message during their deceleration burn. No response had come.</p><p>Alice pulled up one of the telescope feeds on the computer to look closer at the surface. She saw grey rectangular slabs, typically several hundred metres on a side, with wide roads running between them. The pattern was not perfect - sometimes it was irregular, and sometimes there were smaller features too. Some of the smaller ones moved.</p><p>\"Are they factories?\" Charlie asked.</p><p>\"I'd guess so\", Alice said, watching on the telescope feed as a steady stream of rectangular moving objects, each about ten metres long, slid along a street. Another such stream was moving along an intersecting street, and it looked like they would crash at the intersection, but the timing and spacing was such that vehicles from one stream crossed the road just as there were gaps in vehicles along the other stream.</p><p>\"A planet covered by factories, then\", Charlie said. \"With no one home to turn the lights on.\"</p><p>\"I want to see what they're making\", Alice said.</p><h1>--</h1><p>All through the atmospheric entry of their first drone package, Alice sat tight in her seat and clenched and unclenched her hands. So far all they had done was passive observation or broadcasting. A chunky piece of hardware tracing a streak of red-hot plasma behind it was a much louder knock. She imagined alien jet fighters scrambling to destroy their drones, and some space defence mechanism activating to burn their ship.</p><p>The image she saw was a jittery camera feed, showing the black back of the heatshield, the grey skin of the drone package, and a sliver of blue sky. It shook violently as the two halves of the heatshield detached from each other and then the drone package, tumbling off in opposite directions. Land became visible, kilometres below, the grey blocks of the buildings tiny like children's blocks but still visibly three-dimensional, casting shadows and moving as the drone package continued falling.</p><p>The three drones tested their engines, and for a moment flew - or at least slowed their descent - in an ungainly joint configuration, before breaking off from each other and spreading their wings to the fullest. The feed showed the other two drones veering off into the distance on wide narrow wings, and then the view pulled up as the nose of the drone lifted from near-vertical to horizontal.</p><p>\"Oops, looks like we have company\", Charlie said. He had been tapping away at some other screens while Alice watched the drone deployment sequence.</p><p>Alice jumped up from her seat. \"What?\"</p><p>\"Our company is ... a self-referential joke!\"</p><p>Alice resisted the temptation to say anything and instead sunk back into her seat. On her monitor, the grey blocks continued slowly moving below the drone. She tapped her foot against the ground.</p><p>\"Actually though\", Charlie said. \"We're not the only ones in orbit around this planet.\"</p><p>\"What else is orbiting? Has your sense of shame finally caught up with you and joined us?\"</p><p>\"Looks like satellites. Far above us, though. Can you guess how far?\"</p><p>\"I'd guess approximately the distance between you and maturity, so ... five light-years?\"</p><p>Charlie ignored her. \"Exactly geostationary altitude\", he said, grinning. The grin was like some platonic ideal of intellectual excitement; too pure for Alice's annoyance to stay with her, or for her to feel scared about the implications.</p><p>\"But nothing in lower orbits?\" Alice asked.</p><p>\"No\", Charlie said. \"Someone clearly put them there; stuff doesn't end up at exactly geostationary altitude unless someone deliberately flies a communications or GPS satellite there. Now I can't be entirely sure that the geostationary satellites are completely dead, but I'd guess that they are.\"</p><p>\"Like everything else\", Alice said, but even as she said so she caught sight of a long trail of vehicles making its way along one of the roads. There was something more real about seeing them on the drone feed.</p><p>\"Maybe this is just a mining outpost\", Charlie said. \"Big rocket launch to blast out a billion tons of ore to god-knows-where, once a year.\"</p><p>\"Or maybe they're hiding underground or in the oceans\", Alice said.</p><p>\"Let's get one of the drones to drop a probe into the oceans. I'll send one of our initial trio over to the nearest one, it's only a few hundred kilometres away\", Charlie said.</p><p>\"Sure\", Alice said.</p><p>They split the work of flying the drones, two of them mapping out more and more of the Great Grey Grid (as Alice took to calling it in her head), and one flying over the planet's largest ocean.</p><p>Even the oceans were mostly a barren grey waste. Not empty, though. They did eventually see a few small scaly fish-like creatures that stared at their environment with uncomprehending eyes. Alien life. A young Alice would have been ecstatic. But now she was on a mission, and her inability to figure out what had happened on this planet annoyed her.</p><p>In addition to the ocean probe, they had rovers they could send crawling along the ground. Sometimes the doors of the square buildings were open, and Alice would drive a rover past one opening. Most seemed to either be warehouses of stacked crates, or then there would be some kind of automated assembly line of skeletal grey robot arms and moving conveyor belts. A few seemed to place more barriers between the open air and their contents; what went on there, the rovers did not see.</p><p>The first time Alice tried to steer a rover into a building, it got run over by a departing convoy of vehicles. The vehicles were rectangular in shape but with an aerodynamic head, with three wheels on each side. Based on their dimensions, she could easily imagine one weighing ten or twenty tons. The rover had no chance.</p><p>\"Finally!\" Charlie had said. \"We get to fight these aliens.\"</p><p>But there was no fight. It seemed like it had been a pure accident, without any hint of malice. The grey vehicles moved and stopped on some schedule of their own, and for all Alice knew they were not just insensitive beasts but blind and dumb ones too.</p><p>The next rover got in, quickly scooting through the side of the entrance and then off to one side, out of path of the grey vehicles. It wandered the building on its own, headlights turned on in the otherwise-dark building to bring back a video stream of an assembly line brooded over by those same skeletal hands they had glimpsed from outside. Black plastic beads came in by the million on the grey vehicles. A small thin arm with a spike on the end punctured a few holes on one side, and using these holes two of the black beads were sown onto an amorphous plushy shape. The shape got appendages, were covered with a layer of fluff, and the entire thing became a cheerful purple when it passed through an opaque box with pipes leading into it. It looked like a child's impression of a hairy four-legged creature with black beady eyes above a long snout. A toy, but for who?</p><p>The conveyor belt took an endless line of those fake creatures past the rover's camera at the end of the assembly line. Alice watched them go, one by one, and fall onto the open back of a grey vehicle. It felt like each and every one made eye contact with her, beady black eyes glinting in the light. She watched for a long time as the vehicle filled up. Once it did, a panel slid over the open top to close the cargo bay, and it sped off out the door. The conveyor belt kept running, but there was a gap of a few metres to the next plushy toy. It came closer and closer to the end - and suddenly a vehicle was driving into place, and the next creature was falling, and it just barely fell into the storage hold of the vehicle while it was driving into place.</p><p>\"How scary do you find the Blight?\" Alice asked.</p><p>\"Scary enough that I volunteered for this mission\", Charlie said.</p><p>Alice remembered the charts they had been shown. They had been hard to miss; even the news, usually full of celebrity gossip and political machinations, had quickly switched to concentrating on the weirdness in the sky once the astronomers spotted it. Starlight dimming in many star systems and what remained of the the light spectra shifting towards the infrared. Draw a barrier around the affected area, and you get a sphere 30 light-years wide, expanding at a third of the speed of light. At the epicentre, a world that had shown all the signs of intelligent life that could be detected from hundreds of light-years away - a world that astronomers had broadcast signals to in the hopes of finally making contact with another civilisation - that had suddenly gone quiet and experienced a total loss of oxygen in its atmosphere. The Blight, they had called it.</p><p>In the following years, civilisation had mobilised. A hundred projects had sprung forth. One of them: go investigate the star system that was the second-best candidate for intelligent life, but had refused to answer radio signals, and see if someone was there to help. That was why they were here.</p><p>\"I think I found something as scary as the Blight\", Alice said. \"Come look at this.\"</p><p>The purple creatures kept parading past the camera feed</p><h1>--</h1><p>Over the next five days, while the Blight advanced another forty billion kilometres towards everything they loved back home, Alice and Charlie were busy compiling a shopping catalogue.</p><p>\"Computers\", Alice said. \"Of every kind. A hundred varieties of phones, tablets, laptops, smartwatches, smartglasses, smart-everything.\"</p><p>\"Diamonds and what seems to be jewellery\", Charlie said.</p><p>\"Millions of tons of every ore and mineral.\" They had used their telescopes on what seemed to be a big mine, but they had barely needed them. It was like a huge gash in the flesh of a grey-fleshed and grey-blooded giant, complete with roads that looked like sutures. There were white spots in the image, tiny compared to the mine, each one a sizeable cloud.</p><p>\"Clothes\", Charlie continued. \"Lots and lots of clothes of different varieties. They seem to be shipped around warehouses until they're recycled.\"</p><p>\"Cars. Sleek electric cars by the million. But we never see them used on the roads, though there are huge buildings were brand-new cars are recycled. And airplanes, including supersonic ones.\"</p><p>\"A lot of things that look like server farms\", Charlie said. \"Including ones underwater and on the poles. There's an enormous amount of compute in this world. Like, mind-boggling. I was thinking we should figure out how to plug into all of it and mine some crypt-\"</p><p>\"Ships with nuclear fusion reactors\", Alice interrupted. There were steady trails of them cutting shortest-path routes between points on the coast.</p><p>\"Solar panels\", Charlie said. \"Basically every spare surface. The building roofs are all covered with solar panels.\"</p><p>\"And children's plush toys\", Alice said.</p><p>They were silent for a while.</p><p>\"We have a decent idea of what these aliens looked like\", Alice said. \"They were organic carbon-based lifeforms, like us. Similar in size too, also bipedal. And it's like they left some ghostly satanic industrial amusement park running, going through all the motions in their absence, and disappeared.\"</p><p>\"And they didn't go to space, as far as we know\", Charlie said.</p><p>\"At least we don't have any more Blights to worry about then\", Alice said. \"I can't help but imagining that the Blight is something like this. Something that just tiles planets with a Great Grey Grid, does something even worse to the stars, and then moves on.\"</p><p>\"They had space technology, but apparently whoever built the Great Grey Grid didn't fancy it\", Charlie said. \"The satellites might predate it. Probably there were satellites in lower orbits too, but their orbits decayed and they fell down, so we only see the geostationary ones up high.\"</p><p>\"And then what?\" Alice said. \"All of them vanished into thin air and left behind a highly-automated ghost-town?\"</p><p>Charlie shrugged.</p><p>\"Can we plug ourselves into their computers?\" Alice asked.</p><p>\"To mine cr-?\"</p><p>\"To see if anyone's talking.\"</p><p>Charlie groaned. \"You can't just plug yourself into a communication system and see anything except encrypted random-looking noise.\"</p><p>\"How do you know they encrypt anything?\"</p><p>\"It would be stupid not to\", Charlie said.</p><p>\"It would be stupid to blind yourself to the rest of the universe and manufacture a billion plush toys\", Alice said.</p><p>\"Seems like it will work for them until the Blight arrives.\"</p><h1>--</h1><p>Alice floated in the middle of the central corridor of the ship. The ship was called <i>Legacy</i>, but even before launch they had taken to calling it \"Leggy\" for short. The central corridor linked the workstation at the front of the ship where they spent most of their days to the storage bay at the back. In the middle of the corridor, three doors at 120-degree angles from each other lead to the small sleeping rooms, each of them little more than a closet.</p><p>Alice had woken up only a few minutes ago, and still felt an early-morning grogginess as well as the pull of her bed. The corridor had no windows or video feeds, but was dimly lit by the artificial blue light from the workstation. They were currently on the night side of the planet.</p><p>She took a moment to look at the door of the third sleeping room. It was closed, like always, with its intended inhabitant wrapped in an air-tight seal of plastic in a closed compartment of the storage bay. They would flush him into space before they left for home again; they could have no excess mass on the ship for the return journey.</p><p>Alice thought again of the hectic preparations for the mission. Apart from Blightsource, this was only one planet the astronomers had spotted that might have intelligent life on it, and the indications were vague. But when you look into space and see something that looks like an approaching wall of death - well, that has a certain way of inspiring long-shots. Hence the mission, hence <i>Legacy</i>'s flight, hence crossing over the vast cold stretch of interstellar space to see if any answers could be found on this world. Hence Bob's death while in cryonic suspension for the trip. Hence the hopes of all civilisation potentially resting on her and Charlie figuring valuable out something.</p><p>If Charlie and she could find something on this world, some piece of insight or some tool or weapon among the countless pieces of technological wizardry that this world had in spades, that had a credible chance against the Blight when it arrived ... maybe there was hope.</p><p>Alice pushed off on the wall and set herself in a slow spinning motion. The ship seemed to revolve around her. Bob's door revolved out of sight, and Charlie's door became visible -</p><p>Wait.</p><p>Her gravity-bound instincts kicked in and she tried to stop the spin by shoving back with her hands, but there was nothing below her, so she remained spinning slowly. She breathed in deeply to calm herself down, then kicked out a foot against the wall to push herself to the opposite one. She grabbed one of the handles on the wall and held onto it.</p><p>The light on Charlie's room was off. That meant it was empty.</p><p>\"Charlie!\" Alice called.</p><p>No response.</p><p>The fear came fast. Here she was, light-years from home, perhaps all alone on a spaceship tracing tight circles around a ghostly automated graveyard planet. The entire mass of the planet stood between her and the sun. Out between the stars, the Blight was closing in on her homeworld. She counted to calm herself down; one, two, three, ... and just like that, the Blight was three hundred thousand kilometres closer to home. Unbidden, an image of the fluffy purple creature popped up in her mind, complete with its silly face and unblinking eye contact.</p><p>Soundlessly, she used the handles on the wall of the corridor to pull herself towards the workstation. She reached the door, peered inside -</p><p>There was Charlie, staring at a computer screen. He looked up and saw Alice. \"You scared me!\" he said. \"Watch out, no need to sneak behind me so quietly.\"</p><p>\"I called your name\", Alice said.</p><p>\"I know, I know\", Charlie said. \"But I'm on to something here, and I just want to run a few more checks and then surprise you with the result.\"</p><p>\"What result?\" Alice glanced at some of the screens. Two of the drones were above the Great Grey Grid, one above ocean. With their nuclear power source, they could stay in the air as long as they wanted. Even though their focus was no longer aerial reconnaissance, there was no reason not to keep them mapping the planet from up close, occasionally picking up things that their surveys from the ship did not.</p><p>\"I fixed the electrical issues with the rover and the cable near the data centre\", Charlie said.</p><p>\"So you're getting data, not just frying our equipment?\"</p><p>\"Yes\", Charlie said. \"And guess what?\"</p><p>\"What?\"</p><p>\"Guess!\"</p><p>\"You found a Blight-killer\", Alice said.</p><p>\"No! Even better! These idiots don't encrypt their data as far as I can tell. And I think a lot of it is natural language.\"</p><p>\"Okay, and can we figure out what it means?\"</p><p>\"We have automated programs for trying to derive syntax rules and so on\", Charlie said. \"It's already found something, including good guesses of which words are prepositions and what type of grammar they have. But mapping words to meaning based on purely statistics of how often they occur is hard.\"</p><p>\"I've seen products they have with pictures and instruction manuals\", Alice said. \"We could start there.\"</p><p>\"Oh no\", Charlie said. \"This is going to be a long process.\"</p><h1>--</h1><p>By chance, it turned out not to be. Over the next day, they had sent a rover to a furniture factory and had managed, after some attempts, to steal an instruction leaflet out of a printer before the robotic arm could snatch it to be packaged with the furniture. Somehow Alice was reminded of her childhood adventures stealing fruit from the neighbour's garden.</p><p>They had figured out which words meant \"cupboard\", \"hammer\", and \"nail\", and so on. But then another rover on the other side of the world had seen something. It was exploring a grey and windy coast. On one side of the rover was the Great Grey Grid and the last road near the coast, the occasional vehicle hurtling down it. But on the other side was a stretch of rocky beach hammered by white-tipped waves, a small sliver of land that hadn't been converted to grey.</p><p>The land rose by the beach, forming a small hill with jagged rocky sides. The sun shone down on one face of it, but there was a hollow, or perhaps small cave, that was left in the dark by the overhanging rock. And in the rock around this entrance, there were several unmistakable symbols scratched into the rock, each several metres high.</p><p>Alice took manual control of the rover and carefully instructed it to drive over the rocky beach towards the cave entrance. On the way it passed what seemed to be a fallen metal pole with some strips of fabric still clinging to it.</p><p>Once it was close enough to the mouth of what turned out to be a small cave, the camera could finally see inside.</p><p>There was a black cabinet inside. Not far from it, lying on the ground, was the skeleton of a creature with four slender limbs and a large head. Empty eye sockets stared out towards the sky.</p><p>Alice felt her heart beating fast. It wasn't quite right; many of the anatomical details were off. But it was close enough, the similarity almost uncanny. Here, hundreds of light years away, evolution had taken a similar path, and produced sapience. And then killed it off.</p><p>\"Charlie\", she said in a hoarse voice.</p><p>\"What?\" Charlie asked, sounding annoyed. He had been staring at an instruction manual for a chair, but he looked up and saw the video feed. \"Oh\", he said, in a small voice. \"We found them.\"</p><p>Alice tore her eyes away from the skeleton and to the small black cabinet. It had a handle on it. She had the rover extend an arm and open it.</p><h1>--</h1><p>The capsule docked with Leggy and in the weightless environment they pushed the cabinet easily into the ship. They had only two there-and-back-again craft - getting back to orbit was hard - but they had quickly decided to use one to get this cabinet up. It had instructions, after all; very clear instructions, though ones that their rovers couldn't quite follow.</p><p>It started from a pictographic representation, etched onto plastic cards, of how you were supposed to read the disks. They managed to build something that could read the microscopic grooves on the disk as per the instructions, and transfer the data to their computers.</p><p>After a few hours of work, they had figured out the encodings for numbers, the alphabet, their system of units, and seemingly also some data formats, including for images.</p><p>Confirmation came next. The next item on the disk was an image of two of the living aliens, standing on a beach during a sunset. Alice stared into their faces for a long time.</p><p>Next there came images next to what were clearly words of text, about fifty of them. Some of the more abstract ones took a few guesses, but ultimately they thought they had a base vocabulary, and with the help of some linguistics software, it did not take very long before they had a translated vocabulary list of about eight thousand words.</p><p>Alice was checking the work when Charlie almost shouted: \"Look at this!\"</p><p>Alice looked at what he was pointing at. It was a fragment of text that read:</p><blockquote><p>Hello,<br><br>The forms for ordering the new furniture are attached. Please fill them in and we will respond to your order as quickly as we can!<br><br>If you need any help, please contact customer support. You will find the phone number on our website.</p></blockquote><p>\"What is this? Is Mr Skeleton trying to sell us furniture from beyond the grave?\" Alice asked.</p><p>\"No\", Charlie said. \"This isn't what I got from the recovered data; I haven't looked at the big remaining chunk yet. This is what I got by interpreting one of the packets of data running on the cables that our rover is plugged into using what we now know about their data formats and the language.\"</p><p>\"And?\"</p><p>\"I don't get it!\" Charlie said. \"Why would a world of machines send each other emails in natural language?\"</p><p>\"Why would they manufacture plushy toys? I doubt the robotic arms need cuddles.\"</p><p>Charlie looked at the world, slowly spinning underneath their ship. \"Being so close to it makes me feel creeped out. I don't get it.\"</p><p>\"You don't want to lick it anymore?\" Alice asked. She decided not to tell Charlie about her own very similar feelings earlier, when she thought for a moment Charlie had gone missing.</p><p>Charlie ignored her. \"I think the last thing on Mr Skeleton's hard-drive is a video\", he said. \"I've checked and it seems to play.\"</p><p>\"You looked at it first?\" Alice said in a playfully mocking tone. The thrill of discovery was getting to her.</p><p>\"Only the first five frames\", Charlie said. \"Do you want to watch it?\"</p><h1>--</h1><p><i>Our Civilisation: A Story</i> read a short fragment of subtitle, white on black, auto-translated by a program using the dictionary they had built up.</p><p>There was a brief shot of some semi-bipedal furry creature walking in the forest. Then one of a fossilised skeleton of something more bipedal and with a bigger head. Then stone tools: triangular ones that might have been spear tips, saw-toothed ones, clubs. A dash of fading red paint on a rock surface, in the shape of a cartoon version of that same bipedal body plan.</p><p>There were two pillars of stone in a desert on what looked like a pedestal, some faded inscription at its base and the lone and level sands stretching far away. There was a shot of an arrangement of rocks, some balancing on top of two others, amid a field of green. A massive pyramidal stone structure, lit by the rising sun.</p><p>Blocky written script etched on a stone tablet. Buildings framed by columns of marble. A marble statue of one of the aliens, a sling carelessly slung over its shoulder, immaculate in its detail. A spinning arrangement of supported balls orbiting a larger one. <i>And still it moves</i>, the subtitles flashed.</p><p>A collection of labelled geometric diagrams on faded yellow paper. <i>Mathematical Principles of Natural Philosophy</i>.</p><p>A great ornate building with a spire. A painting of a group of the aliens clad in colourful clothing. An ornate piece of writing. <i>We hold these truths to be self-evident ...</i></p><p>A painting of a steam locomotive barrelling along tracks. A diagram of a machine. A black-and-white picture of one of the aliens, then another. <i>Government of the people, for the people, by the people, shall not perish ...</i></p><p>An alien with white hair sticking up, holding a small stick of something white and with diagrams of cones behind him. Grainy footage of propeller aircraft streaking through the sky, and then of huge masses of people huddling together and walking across a barren landscape, and then of aliens all in the same clothes charging a field, some of them suddenly jerking about and falling to the ground. <i>We will fight on the beaches, we will fight on the landing grounds ...</i></p><p>A black-and-white footage of a mushroom cloud slowly rising from a city below. A picture, in flat pale blue and white, showing a stylised representation of the world's continents. The same picture, this time black-and-white, on the wall of a room where at least a hundred aliens were sitting.</p><p>An alien giving a speech. <i>I have a dream</i>. An alien, looking chubby in a space suit, standing on a barren rocky surface below an ink-black sky next to a pole with a colourful rectangle attached to it.</p><p>Three aliens in a room, looking at the camera and holding up a piece of printed text. <i>Disease eradicated</i>.</p><p>What looked like a primitive computer. A laptop computer. An abstract helical structure of balls connected by rods, and then flickering letters dancing across the screen.</p><p>A blank screen, an arrow extending left to right across it - <i>time</i>, flashed the subtitles- and then another arrow from the bottom-left corner upwards - <i>people in poverty</i> - and then a line crawling from left to right, falling as it did so.</p><p>A line folding itself up into a complicated shape. <i>AI system cracks unsolved biology problem</i>.</p><p>From then on, the screen showed pictures of headlines.</p><p><i>All routine writing tasks now a solved problem, claims AI company</i>.</p><p><i>Office jobs increasingly automated</i>.</p><p><i>Three-fourths of chief executives of companies on the [no translation] admit to using AI to help write emails, one-third have had AI write a shareholder letter or strategy document</i>.</p><p><i>Exclusive report: world's first fully-automated company, a website design agency</i>.</p><p><i>Mass layoffs as latest version of [no translation] adopted at [no translation]; 'stunning performance' at office work.</i></p><p><i>Nations race to reap AI productivity gains: who will gain and who will lose?</i></p><p><i>CEO of [no translation] resigns, claiming job pointless, both internal and board pressure to defer to \"excellently-performing\" AI in all decisions.</i></p><p><i>[No translation] ousts executive and management team, announces layoffs; board supports replacing them with AI to keep up with competition.</i></p><p><i>Entirely or mostly automated companies now delivering 2.5x higher returns on investment on average; 'the efficiency difference is no joke', says chair of [no translation].</i></p><p><i>Year-on-year economic growth hits 21% among countries with advanced AI access.</i></p><p><i>Opinion: the new automated economy looks great on paper but is not serving the needs of real humans.</i></p><p><i>Mass protests after [no translation], a think-tank with the ear of the President, is discovered to be funded and powered by AI board of [no translation], and to have practically written national economic policy for the past two years.</i></p><p><i>'No choice but forward', says [no translation] after latest round of worries about AI; unprecedented economic growth still strong.</i></p><p><i>[No translation 1] orders raid of [no translation 2] over fears [no translation 2] is not complying with latest AI use regulations, but cannot execute order due to noncompliance from the largely-automated police force; 'we are working with our AI advisers and drivers in accordance with protocol, and wish to assure the [no translation 3] people that we are still far from the sci-fi scenario where our own police cars have rebelled against us.'</i></p><p><i>'AI overthrow' fears over-hyped, states joint panel of 30 top AI scientists and business-people along with leading AI advisory systems; 'they're doing a good job maximising all relevant metrics and we should let them keep at it, though businesses need to do a better job of selecting metrics and tough regulation is in order.'</i></p><p><i>Opinion: we're better-off under a regime of rigorous AI decision-making than under corrupt politicians; let the AIs repeat in politics what they've done for business over the last five years.</i></p><p><i>The statistics have never looked so good' - Prime Minister reassures populace as worries mount over radical construction projects initiated by top AI-powered companies</i>.</p><p><i>Expert panel opinion: direct AI overthrow scenario remains distant threat, but more care should be exercised over choice of target metrics; recommend banning of profit-maximisation target metric.</i></p><p><i>Movement to ban profit-maximising AIs picks up pace</i>.</p><p><i>Top companies successfully challenge new AI regulation package in court.</i></p><p><i>'The sliver of the economy over which we retain direct control will soon be vanishingly small', warns top economist, 'action on AI regulation may already be too late'</i>.</p><p><i>Unverified reports of mass starvation in [no translation]; experts blame agricultural companies pivoting to more land-efficient industries.</i></p><p><i>Rant goes viral: 'It's crazy, man, we just have these office AIs that only exist in the cloud, writing these creepily-human emails to other office AIs, all overseen by yet another AI, and like most of their business is with other AI companies; they only talk to each other, they buy and sell from each other, they do anything as long as it makes those damned numbers on their spreadsheets just keep ticking up and up; I don't think literally any human has ever seen a single product out of the factory that just replaced our former neighbourhood, but those factories just keep going up everywhere.'</i></p><p><i>Revolution breaks out in [no translation]; government overthrown, but it's business-as-usual for most companies, as automated trains, trucks, and ships keep running.</i></p><p><i>[No translation] Revolution: Leaked AI-written email discovered, in which the AI CEO ordered reinforcement of train lines and trains three weeks ago. 'We are only trying to ensure the continued functioning of our supply chains despite the recent global unrest, in order to best serve our customers', CEO writes in new blog post.</i></p><p><i>[No translation] Revolution: crowds that tried swarming train lines run over by trains; 'the trains didn't even slow down', claim witnesses. CEO cites fiduciary duties.</i></p><p><i>Despite unprecedented levels of wealth and stability, you can't actually do much: new report finds people trying to move house, book flight or train tickets, or start a new job or company often find it difficult or impossible; companies prioritising serving 'more lucrative' AI customers and often shutting down human-facing services.</i></p><p><i>Expert report: 'no sign of human-like consciousness even in the most advanced AI systems', but 'abundantly clear' that 'the future belongs to them'.</i></p><p><i>New report: world population shrinking rapidly; food shortages, low birth rates, anti-natalist attitudes fuelled by corporate campaigns to blame.</i></p><p>The screen went blank. Then a video of an alien appeared, sitting up on a rocky surface. Alice took a moment to realise that it's the same cave they found the skeleton in. The alien's skin was wrapped tight around its bones, and even across the vast gulf of biology and evolutionary history, Alice could tell that it is not far from death. It opened its mouth, and sound came out. Captions appeared beneath it.</p><p>\"It is the end\", the alien said, its eyes staring at them from between long unkempt clumps of hair. \"On paper, I am rich beyond all imagination. But I have no say in this new world. And I cannot find food. I will die.\"</p><p>The wind tugged at the alien's long hair, but otherwise the alien was so still that Alice wondered if it had died there and then.</p><p>\"There is much I would like to say\", the alien says. \"But I do not have the words, and I do not have the energy.\" It paused. \"I hope it was not all in vain. Or, that if for us it was, that for someone up there it isn't.\"</p><p>The video went blank.</p><p>Alice and Charlie watched the blank screen in silence.</p><p>\"At least the blight they birthed seems to have stuck to their world\", Charlie said after a while.</p><p>\"Yeah\", Alice said, slowly. \"But I don't think we'll find anything here.\"</p><p><i>Legacy</i> completed nine more orbits of the planet, and then jettisoned all unnecessary mass into space. Its engines jabbed against the darkness of space, bright enough to be visible from the planet's surface. There was no one to see them.</p><p>On a factory down on the planet, an assembly line of beady-eyed purple plush toys marched on endlessly.</p><p>&nbsp;</p><p>&nbsp;</p><p><br>---</p><p>The title of this work is taken from a passage in <i>Superintelligence: Paths, Dangers, Strategies</i>, where Nick Bostrom writes:</p><blockquote><p><i>We could thus imagine, as an extreme case, a technologically highly advanced society, containing many complex structures, some of them far more intricate and intelligent than anything that exists on the planet today\u2014a society which nevertheless lacks any type of being that is conscious or whose welfare has moral significance. In a sense, this would be an uninhabited society. It would be a society of economic miracles and technological awesomeness, with nobody there to benefit. <strong>A Disneyland without children</strong>. [emphasis added]</i></p></blockquote><p>The outline of events presented draws inspiration from several sources, but most strongly on Paul Christiano's article <a href=\"https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like\">What failure looks like</a>.</p><p>---</p><p>Any advice on distributing this story more widely is welcome.</p><p><br>&nbsp;</p>", "user": {"username": "LRudL"}}, {"_id": "DiMnA7phzzbuKnQR4", "title": "The Risks of AI-Generated Content on the EA Forum", "postedAt": "2023-06-04T05:33:53.687Z", "htmlBody": "<p>This article explores the potential biases introduced by AI-generated content and suggests implementing safeguards, including content auditing and norms.</p>\n<p>Language models like GPT-4 are capable of producing remarkably human-like text. While AI-generated content can be useful in many contexts, its usage on platforms like the EA Forum carries potential risks.</p>\n<p>Biased Influence on the EA Movement:</p>\n<p>One of the core tenets of the EA movement is the rigorous evaluation of evidence and arguments. AI-generated content, however, introduces a novel risk of bias. If AI-generated content becomes prevalent on the EA Forum without appropriate safeguards, it could heavily influence the direction and discussions within the movement. The nature of AI models is such that they learn from existing data, including the biases present in the training data. Consequently, AI-generated content may perpetuate existing biases, leading to a distorted representation of the movement's core principles.</p>\n<p>Compromising Independence in AI Safety:</p>\n<p>Effective altruists recognize the importance of addressing AI safety concerns to ensure the responsible development and deployment of artificial intelligence. By relying heavily on AI-generated content, the EA Forum could inadvertently compromise its ability to independently shape AI safety discussions. Genuine insights and perspectives from experts might be overshadowed or diluted by AI-generated content, potentially hindering the movement's influence on the development of effective safeguards.</p>\n<p>Safeguards and Auditing AI-Generated Content:</p>\n<p>To protect the integrity and independence of the EA movement, it is crucial to implement safeguards regarding AI-generated content on the EA Forum. One potential approach is to establish norms and guidelines that encourage transparency in content generation. The current EA Forum policy on AI generated content is light on: <a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum\">https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum</a>. Users could at least be encouraged to disclose if their posts or comments have been generated or assisted by AI. This transparency would allow readers to critically evaluate the content and consider potential biases or limitations.</p>\n<p>Additionally, auditing could could be conducted (by who?) to detect and flag AI-generated content. AI models leave subtle traces in the text that can be identified through techniques like stylometric analysis or linguistic pattern recognition. Implementing periodic audits to sense AI-generated content can help maintain the integrity of discussions and prevent undue influence on the movement's direction.</p>\n<p>Community Engagement and Human Oversight:</p>\n<p>The EA community should strive for a balanced approach that combines the advantages of AI with human judgment. Human oversight and active community engagement are vital.</p>\n", "user": {"username": "WobblyPanda2"}}, {"_id": "diQDddZFbasSG2BJt", "title": "Quick, High-EV Advantage Sportsbetting Opportunity in 18 US States", "postedAt": "2023-06-04T03:27:57.455Z", "htmlBody": "<p>Hi all, Fanduel has a $2,500 risk-free bet promo available for <strong>first-time users only</strong> in all 18 of the states that Fanduel is licensed in. The offer expires on June 18th. I estimate that the Fanduel promo is worth about $590 EV after taxes for one hour of work, and a few other sportsbooks have some decent promotions as well (in the range of about $250/hour). If you do multiple offers and itemize on your taxes, they\u2019re higher EV.</p><p>The fine print is that you have to bet the whole $2500 (or less if you want to do this for less money) in one single bet, and the bet has to be on a game that takes place before June 18th. The states where the Fanduel offer is live include&nbsp;AZ, CO, CT, IL, IN, IA, KS, LA, MD, MI, NJ, NY, PA, OH, TN, VA, WV, and WY.&nbsp;There have been previous posts that explain the logistics and math of these types of offers, you can check out&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/29EZBsbLovTgu6C3B/ea-fundraising-through-advantage-sports-betting-a-guide\"><u>this post</u></a> if you\u2019re unfamiliar with how they work. Here is the quick version:</p><p>If you make your first bet up to $2,500 on Fanduel and lose, Fanduel will give you site credit equal to the amount of your bet. Site credit isn\u2019t quite as good as having cash, because you have to bet it all at once and because you only get to keep the profit of your bet, not the stake. So if you bet $2,500 at +100 (50/50) odds in cash and win, you\u2019ll get $5,000 back - your $2,500 stake plus $2,500 in profit. If you made the same bet with site credit, you\u2019d only get $2,500 back because you don\u2019t keep your stake.</p><p>The way that sportsbetting taxes work is that you\u2019re taxed on your net winnings on a given sportsbook, but if you end up losing money you won\u2019t be able to write that off on your taxes. Here\u2019s a table showing the different states of the world if you pay a 24% federal marginal tax rate and a 5% state tax rate if. This assumes you make your bets at +300 odds, and you have a \u201cfair\u201d 25% chance to win. Even though the house takes a cut, you can use software to find bets at these odds that are fair or even +EV - more on that in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/29EZBsbLovTgu6C3B/ea-fundraising-through-advantage-sports-betting-a-guide\"><u>this post</u></a>.<br>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\">First bet win ($2,500 cash): chance of occurring - 25%&nbsp;</td><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\">Payout if you win: $10,000. Profit: 7500</td><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\">After-tax profit: $7,500* .71 (the 24% federal and 5% state income taxes) = $5,325&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\">Second bet win ($2,500 site credit): chance of occuring - .75*.25 = 18.75%</td><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\">Payout if you win: $7,500 Profit: $5,000 (because you lost $2500 on your first bet)</td><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\">After-tax profit: $5,000* .71 = $3,550&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\">Lose both bets: chance of occurring: .75*.75 = 56.25%</td><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\">Payout: $0</td><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\">After tax profit: -$2500</td></tr><tr><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\">&nbsp;</td><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\">&nbsp;</td><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\"><p>Expected value:&nbsp;</p><p>(.25*5325) + (.1875*3550) +(.5625*-2500) = ~$590</p></td><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\">&nbsp;</td><td style=\"border:0.75pt solid #9a9a9a;padding:1pt 4pt;vertical-align:top\">&nbsp;</td></tr></tbody></table></figure><p><br><br>&nbsp;<br><strong>Why do companies do this if customers can reliably gain EV?&nbsp;</strong>Gaining EV requires careful planning that most people won't do. The promotions are meant to draw in people who, once they've started gambling, will keep doing so, earning the companies a lot of money over time. If you aren't one of those people, you stand to gain (and to take money from an exploitative industry).</p><p><strong>Why are you sharing this here?&nbsp;</strong>EAs do more good with their money than most people. I'm hoping some of you will take this offer, make money, and use the money to do good. Opportunities to reliably generate EV are rare, but we live in a weird time where online gambling companies are handing them out on a semi-regular basis. I think we should take advantage.</p><p>The other offers are the exact same in structure, but for up to $1,250 on Caesars, up to $1,000 on BetMGM, up to $1,000 on Barstool, and up to $500 on Betrivers. Some states like Colorado have more options - to check your state you can visit the&nbsp;<a href=\"https://www.americangaming.org/research/state-gaming-map-mobile/\"><u>American gaming interactive map</u></a>.</p><p>While the EV of this Fanduel bet is high, so is the variance.&nbsp;<i><strong><u>Do not bet with money you are not comfortable losing.</u></strong></i> As you can see in the table above, you would lose money 56% of the time if Fanduel was the only sportsbook you bet on.</p><p>&nbsp;</p><p>If you are itemizing on your taxes (e.g. clumping your donations for this tax year), you can write off losses across sportsbooks. That is to say, if you win $7,500 on Fanduel and lose $3,750 across Caesars, BetMGM, Barstool, and Betrivers, you will only have to pay taxes on your $3,750 in profit across all sportsbooks. If you take the standard deduction, you will only be able to write off losses within a sportsbook (e.g. if you lose your first bet but then win your second bet with site credit), so in the above scenario if you take the standard deduction, you would owe taxes on $7,500 of winnings.</p><p>One nice perk of the Barstool promo is that I think if you lose your first bet, you can just withdraw the site credit as cash and don\u2019t need to bet it again. This is how things worked about two months ago, but this might have changed. You can check out&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/29EZBsbLovTgu6C3B/ea-fundraising-through-advantage-sports-betting-a-guide\"><u>this post</u></a> for info on logistics of moving money on and off sites and for using OddsJam to find what to bet on (feel free to use their free trial and then cancel).</p><p>&nbsp;</p><p>It can be useful to sign up for the sites with an email you check once a week or so, because sometimes sportsbook will email you with personalized offers that are just as good as these promos. More specifically, offers like \u201cdeposit $1000 and we\u2019ll give you $500 for free, you just have to bet it within a week\u201d are great, and offers like \u201chere\u2019s an odds boost from +100 to +110\u201d are probably not great. I know a few people who have made $5-10k EV just from capitalizing on offers when they see them in their inbox.</p><p>Let me know if you have any questions in the comments or via DM, good luck!&nbsp;</p>", "user": {"username": "Joseph B."}}, {"_id": "TH2tRumAuwKWN8NoG", "title": "Decomposing alignment to take advantage of paradigms", "postedAt": "2023-06-04T14:26:25.307Z", "htmlBody": "<p>It is hard to solve alignment with money. When Elon Musk asked what should be done about AI safety, <a href=\"https://twitter.com/ESYudkowsky/status/1628089873193406464\">Yudkowsky tweeted</a>:</p>\n<blockquote>\n<p>The game board has already been played into a frankly awful state. There are not simple ways to throw money at the problem. If anyone comes to you with a brilliant solution like that, please, please talk to me first. I can think of things I'd try; they don't fit in one tweet. - Feb 21, 2023</p>\n</blockquote>\n<p>Part of the problem is that alignment is pre-paradigmatic. It is not just that throwing money at it is hard; any kind of parallel effort (including the kind that wrote Wikipedia, the open source software the runs the world, and recreational mathematics) is difficult. From <a href=\"https://www.lesswrong.com/posts/5rsa37pBjo4Cf9fkE/a-newcomer-s-guide-to-the-technical-ai-safety-field#Different_paradigms\"><em>A newcomer\u2019s guide to the technical AI safety field</em></a>:</p>\n<blockquote>\n<p>AI safety is a pre-paradigmatic field, which APA defines as:</p>\n<blockquote>\n<p>a science at a primitive stage of development, before it has achieved a paradigm and established a consensus about the true nature of the subject matter and how to approach it.</p>\n</blockquote>\n<p>In other words, there is no universally agreed-upon description of what the alignment problem is. Some would even describe the field as \u2018non-paradigmatic\u2019, where the field may not converge to a single paradigm given the nature of the problem that may never be definitely established. It\u2019s not just that the proposed solutions garner plenty of disagreements, the nature of the problem itself is ill-defined and often disagreed among researchers in the field. Hence, the field is centered around various researchers / research organizations and their research agenda, which are built on very different formulations of the problem, or even a portfolio of these problems.</p>\n</blockquote>\n<p>Therefore, I think it is incredibly useful if we can <strong>decompose the alignment problem such that most of the problems become approachable with a paradigm, even if the individual problems are harder</strong>. This is because we can adopt the institutions, processes, and best practices of fields that are based on paradigms, such as science and mathematics. These regularly tackle extremely difficult problems, thanks to their superior coordination.</p>\n<h1>My proposal for a decomposition: alignment = purely mathematical inner alignment + fully formalized <a href=\"https://forum.effectivealtruism.org/topics/indirect-normativity\">indirect normativity</a></h1>\n<p>I propose we decompose alignment into (1) discovering how to align an AI's output to arbitrary <em>mathematical</em> functions (i.e. we don't care about <a href=\"https://www.lesswrong.com/tag/embedded-agency\">embedded agency</a>) and (2) creating a formalization of ontology/values in purely mathematical language. This decomposition might seem like it just makes things harder, but allow me to explain!</p>\n<p>First, purely mathematical optimization. You might not believe this, but I think this might be the harder bit! However, it should be extremely paradigmatic.</p>\n<p>Note that the choice of this decomposition wasn't paradigmatic, we have to rely on intuition to choose it. But those that do can then cooperate much easier to achieve it!</p>\n<h2>Purely mathematical inner alignment</h2>\n<p><strong>Superhuman mathematical optimization</strong>: let&nbsp;<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f:\\{0,1\\}^\u2217 \\to [0,1]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">{</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">}</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">\u2217</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">\u2192</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span>&nbsp;(i.e. a function from strings to the numbers between 0 and 1 (inclusive)) be expressible by a formula in <a href=\"https://en.wikipedia.org/wiki/List_of_first-order_theories#Arithmetic\">first-order arithmetic</a> (with suitable encodings (we can represent strings with natural numbers and real numbers with a formula for its <a href=\"https://en.wikipedia.org/wiki/Cauchy_sequence\">Cauchy sequence</a>, for example). Give an efficient algorithm <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"g\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;\">g</span></span></span></span></span></span> that takes <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span> as input such that&nbsp;<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathbb E [(f(g(f))\u2212f(h(f))] \\ge 0 \"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-ams-R\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">E</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;\">g</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.446em;\">\u2265</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span>&nbsp;(where&nbsp;<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathbb E\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-ams-R\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">E</span></span></span></span></span></span></span></span>&nbsp;is interpreted in sense of our <a href=\"https://en.wikipedia.org/wiki/Subjective_probability\">subjective</a> <a href=\"https://en.wikipedia.org/wiki/Expected_value\">expected value</a>), where&nbsp;<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"h(f)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span>&nbsp;is the result of any human or human organization (without any sort of <a href=\"https://en.wikipedia.org/wiki/Key_(cryptography)\">cryptographic secrets</a>) trying to optimize&nbsp;<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span>.</p>\n<p>Note that, by definition, any AGI will be powerful enough to do this task (since it just needs to beat the best humans). See <a href=\"https://www.lesswrong.com/posts/rnzpYKWmNWEW5PQyq/inference-from-a-mathematical-description-of-an-existing#An_AGI_can_guess_the_solution_to_a_transcomputational_problem_\"><em>An AGI can guess the solution to a transcomputational problem?</em></a> for more details.</p>\n<p>However, we also require that it actually <em>does</em> the task, which is why its a form of inner alignment. This does not include <em>outer alignment</em>, because <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"g\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;\">g</span></span></span></span></span></span>'s output can have arbitrarily bad <a href=\"https://www.lesswrong.com/tag/impact-regularization\">impacts</a> on the humans that read it. Nor does it, on its own, give us an AI powerful enough to <a href=\"https://arbital.com/p/pivotal/\">protect us from unaligned AGIs</a>, because it only cares about mathematical optimization, not protecting humanity.</p>\n<p>I expect this to be highly paradigmatic, since its closely related to problems in AI already. There may even be a way to reduce it to a purely <em>mathematical</em> problem; the main obstacle is the repeated references to humans. But if we can somehow formulate a stronger version that doesn't refer to humans (be a better optimizer than any circuit up to size X or something?), we can throw the entire computer science community at it!</p>\n<h2>Fully formalized indirect normativity</h2>\n<blockquote>\n<p><a href=\"https://forum.effectivealtruism.org/topics/indirect-normativity\"><strong>Indirect normativity</strong></a> is an approach to the AI alignment problem that attempts to specify AI values indirectly, such as by reference to what a rational agent would value under idealized conditions, rather than via direct specification.</p>\n</blockquote>\n<p>This seems like it is extremely hard, maybe not much easier than the full alignment problem. However, I think we already have a couple approaches:</p>\n<ul>\n<li>Paul Christiano's <a href=\"https://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/\"><em>A formalization of indirect normativity</em></a>: create a mathematical description of an alignment researcher's brain to predict how they would solve alignment</li>\n<li>Tammy Cardao's <a href=\"https://www.lesswrong.com/posts/4RrLiboiGGKfsanMF/the-qaci-alignment-plan-table-of-contents\"><em>question-answer counterfactual interval</em> (QACI)</a>: create a mathematical description of an alignment researcher answering questions, and then iterate questions of the form \"What's a better version of this specification of human values: \" until you hit the answer \"that's good enough\".</li>\n<li>My own <a href=\"https://www.lesswrong.com/posts/rnzpYKWmNWEW5PQyq/inference-from-a-mathematical-description-of-an-existing\"><em>Inference from a Mathematical Description of an Existing Alignment Research</em> (IMDEAR)</a>: Like Christiano's proposal, but with lower tech pre-requisites (this one still needs feedback and <a href=\"https://www.lesswrong.com/posts/rnzpYKWmNWEW5PQyq/inference-from-a-mathematical-description-of-an-existing#Chatroom\">volunteers</a> btw!)</li>\n</ul>\n<p>Indirect normativity isn't particularly paradigmatic, but it might be close to completion anyways! We could view the three above proposals as three potential paradigms, for example.</p>\n<h2>Combining them to solve the full alignment problem</h2>\n<p>To solve alignment, use mathematical optimization to create a plan that optimizes our indirect specification of our values.</p>\n<p>In particular, since the string \"do nothing\" is something humans can come up with, a superhuman mathematical optimizer will come up with a string that is less bad than that. This gives us <a href=\"https://www.lesswrong.com/tag/impact-regularization\">impact regularization</a>. In fact, if we did indirect normativity correctly and we want it to be corrigible, the AI's string must be better than \"do nothing\" according to every <a href=\"https://www.lesswrong.com/posts/5sRK4rXH2EeSQJCau/corrigibility-at-some-small-length-by-dath-ilan\">corrigibility property</a>, including the <a href=\"https://www.lesswrong.com/posts/5sRK4rXH2EeSQJCau/corrigibility-at-some-small-length-by-dath-ilan#Hard_problem_of_corrigibility___anapartistic_reasoning\">hard problem of corrigibility</a>. So it is safe. (An alternative, which isn't corrigible but still a good outcome, is to ask for a plan to directly maximizes <a href=\"https://www.lesswrong.com/tag/coherent-extrapolated-volition\">CEV</a>.)</p>\n<p>But if it is a sufficiently powerful optimizer, it should be able to create a superhuman plan for the prompt \"Give us a piece of source code that, when run, protects us against unaligned AGI (avoiding other impacts of course).\". So it is effective.</p>\n<h1>Other choices for decompositions?</h1>\n<p>Are there any other choices for decompositions? Most candidates that I can think of either:</p>\n<ol>\n<li>Decompose the alignment problem, but the hardest parts are still pre-paradigmatic</li>\n<li>OR are paradigmatic, but don't decompose the entire alignment problem</li>\n</ol>\n<p>Is there a decomposition that I didn't think of?</p>\n<h1>Conclusion</h1>\n<p>So, my proposal is that most of attempts of mass organizing alignment research (whether via professionals or volunteer work) ought to either use my decomposition, or a better one if it is found.</p>\n", "user": {"username": "Christopher King"}}, {"_id": "FrukYXHBSaZvb7f9Q", "title": "A Double Feature on The Extropians", "postedAt": "2023-06-03T18:29:35.657Z", "htmlBody": "<p>Link-post for two pieces I just wrote on the Extropians.<br><br>The Extropians were an online group of techno-optimist transhumanist libertarians active in the 90s who influence a lot of online intellectual culture today, especially in EA and Rationalism. Prominent members include Eliezer Yudkowsky, Nick Bostrom, Robin Hanson, Eric Drexler, Marvin Minsky and all three of the likely candidates for Satoshi Nakamoto (Hal Finney, Wei Dai, and Nick Szabo).<br><br><a href=\"https://www.maximumprogress.org/extropia-archaeology\">The first piece is a deep dive into the archived Extropian forum</a>. It was super fun to write and I was constantly surprised about how much of the modern discourse on AI and existential risk had already been covered in 1996.</p><p><a href=\"https://maximumprogress.substack.com/p/grading-extropian-predictions\">The second piece is a retrospective on predictions made by Extropians in 1995</a>. Eric Drexler, Nick Szabo and 5 other Extropians give their best estimates for when we'll have indefinite biological lifespans and reproducing asteroid eaters.</p>", "user": {"username": "Maxwell Tabarrok"}}, {"_id": "6dphu3p8d5mQZEZzk", "title": "Intrinsic limitations of GPT-4 and other large language models, and why I'm not (very) worried about GPT-n", "postedAt": "2023-06-03T13:09:58.563Z", "htmlBody": "<h2>Introduction</h2><p>Over my years of involvement in Effective Altruism, I have written <a href=\"https://forum.effectivealtruism.org/posts/j7X8nQ7YvvA7Pi4BX/a-critique-of-ai-takeover-scenarios\">several</a> forum posts <a href=\"https://forum.effectivealtruism.org/posts/2sMR7n32FSvLCoJLQ/critical-review-of-the-precipice-a-reassessment-of-the-risks\">critical</a> of various aspects of the movement\u2019s focus on AI safety, with a particular focus on fast takeoff scenarios. It is no secret that the recent release of ChatGPT, GPT-4, and other highly capable large language models (LLMs) has sparked both tremendous public interest and <a href=\"https://forum.effectivealtruism.org/posts/Yk4D4DZpx6eriMDyY/statement-on-ai-extinction-signed-by-agi-labs-top-academics\">significant concern</a> in the EA community. In this article, I share some of my thoughts on why I am not as concerned about the AI alignment of LLMs as many in the EA community. In particular, while there are legitimate concerns about the safety and reliability of LLMs, I do not think it is likely that such systems will soon reach human levels of intelligence or capability in a broad range of tasks. Rather, I argue that such systems have intrinsic limitations which cannot be overcome within the existing development paradigm, and continual growth in capabilities based on increasing the number of parameters and size of the training data will only continue for a few more years before running its course. I also argue that the adoption of such systems will be slow, occurring over years to decades rather than months to years (as <a href=\"https://forum.effectivealtruism.org/posts/D8GitXAMt7deG8tBc/how-quickly-ai-could-transform-the-world-tom-davidson-on-the#Why_AI_takeoff_might_be_shockingly_fast\">some have argued</a>), and thus their impacts will be more gradual and evolutionary rather than sudden and revolutionary. As such, I do not agree with <a href=\"https://forum.effectivealtruism.org/posts/5LNxeWFdoynvgZeik/nobody-s-on-the-ball-on-agi-alignment\">some</a> who have argued that AI alignment should be focused on alignment of existing LLMs and driven by short timelines (on the order of years).</p><p>Before going further, I should add that the following piece is intended to be reasonable accessible to those not highly familiar with LLMs or AI alignment more generally, and therefore is more introductory and not highly technical (though it is a little technical in some parts). I am sharing my thoughts in the hope of fostering more of a discussion about how we should think about AI alignment, with a focus on the impacts and trajectories of LLMs. I also want to make my predictions public for the purpose of personal accountability. Having set out my purpose, I begin with an introduction to the current LLM paradigm.</p><h2>Limits of the existing paradigm</h2><p>Current <a href=\"https://en.wikipedia.org/wiki/Large_language_model\">large language models</a> are based on the <a href=\"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\">transformer architecture</a>. These are very large neural networks which are trained on huge corpuses of data, most of which is from the internet. The models are usually trained to predict the next word in a sentence, and during training they learn complex statistical associations between words in natural language. Recently, OpenAI has extended this framework by adding a technique called <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback\">Reinforcement Learning from Human Feedback</a> (RLHF). This involves presenting queries and their corresponding LLM outputs to humans, who then provide ratings as to the quality of the responses. These ratings are then used to fine-tune the language model, altering its output to improve its ratings from human feedback. This technique has enabled language models to produce output that is more useful to humans, and has improved the performance of language models as chatbots.&nbsp;</p><p>The OpenAI team have also made other additions and modifications to their newest model (GPT-4) to improve its capabilities as a chatbot, though very little public details are available about this. Judging by the number of contributors to the <a href=\"https://arxiv.org/abs/2303.08774\">GPT-4 paper</a> (which lists 93 \u2018core contributors\u2019 and hundreds of other contributors) relative to previous <a href=\"https://arxiv.org/abs/2005.14165\">GPT-3 paper</a> (which lists only 31 authors), it appears that OpenAI has devoted a lot of time to adjusting, augmenting, and modifying the model in various ways. We know that systems have been put in place to filter out queries likely to lead to harmful or offensive results. There is also <a href=\"https://arxiv.org/abs/2303.12712\">evidence</a> that GPT-4 has a limited ability to check for faulty assumptions in the queries or instructions it is given, though it is unclear how this has been done. Nonetheless, it appears that extensive development work has been done beyond the initial stage of training the transformer on a large text corpus.</p><p>In my view, the fact that such extensive augmentations and modifications are necessary is an indication of the underlying weaknesses and limitations of the transformer architecture. These models learn complex associations between words, but do not form the same structured, flexible, multimodal representations of word meaning like humans. As such they <a href=\"https://arxiv.org/abs/2109.01247\">do not truly \u2018understand\u2019 language</a> in the same sense as humans can. For many applications this does not matter, but in other cases it can manifest in extremely bizarre behaviour, including models accepting absurd premises, making <a href=\"https://w3nhao.github.io/2023/03/30/Limitation-ChatGPT-Causal-Inference/\">faulty inferences</a>, making <a href=\"https://www.scientificamerican.com/article/chatgpt-explains-why-ais-like-chatgpt-should-be-regulated1/\">contradictory statements</a>, and failing to incorporate information that is provided.</p><p>A related issue is the known tendency of LLMs to \u2018<a href=\"https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)\">hallucinate</a>\u2019, making up facts, information, or non-existent libraries of computer code when giving responses. I dislike the term hallucination because it implies there is some fundamental distinction between veridical knowledge that the LLM has correctly learned and hallucinations which it simply makes up. In fact there is no such distinction, because <a href=\"https://spectrum.ieee.org/ai-hallucination\">LLMs do not form memories</a> of events or facts in the way humans do. All they are capable of is storing complex statistical associations in their billions of learned parameters. When the model produces some string of words as an output, this is equally the product of its internal learned parameters regardless of whether humans would evaluate the string as true or false. Furthermore, an LLM has no notion of truth or falsity; it simply learns word associations. (Here I am setting aside the possibility that GPT-4 may be augmented with capabilities beyond its basic transformer architecture, since there is no public information about this, and at any rate the underlying architecture is still a transformer model). As such, the problem of \u2018hallucinations\u2019 is not some teething issue or minor annoyance, but is intrinsic to the architecture and method of training of LLMs. Of course, various proposals exist for how to mitigate this limitation, such as augmenting LLMs with curated datasets of encyclopaedic facts or <a href=\"https://arxiv.org/abs/1906.05317\">common-sense knowledge</a>. While promising, such proposals are not new and <a href=\"https://arxiv.org/abs/2202.03629\">face many problems</a> of their own right. Though they may be successful in the long run, I do not believe there is any simple or easily implemented solution to the problem of \u2018hallucinations\u2019 in LLMs.</p><p>LLMs are also known to be susceptible to what are called <a href=\"https://en.wikipedia.org/wiki/Adversarial_machine_learning\">adversarial attacks</a>. While recently the term has been used to refer to breaking past the restrictions placed on ChatGPT and GPT-4 by their developers, I am using the term \u2018adversarial attack\u2019 to refer to an older usage of the term relating to research which attempts to craft training data or prompts which highlight weaknesses or limitations of LLMs. Numerous analysis in this tradition have found that LLMs often rely on <a href=\"https://arxiv.org/abs/2201.07614\">superficial heuristics</a> and <a href=\"https://arxiv.org/abs/1907.07355\">spurious correlations</a> in their training data, resulting in poor performance on cases carefully selected to highlight these shortcomings. Other adversarial attacks have found that LLMs can assign high probabilities to answers with nonsense <a href=\"https://ojs.aaai.org/index.php/AAAI/article/view/17531\">scrambled word order</a>, are easily distracted by <a href=\"https://arxiv.org/pdf/2302.00093.pdf?trk=public_post_comment-text\">irrelevant content</a>, and in some cases are not even sensitive to whether their <a href=\"https://arxiv.org/abs/2109.01247\">prompts actually make sense</a> or not. There is some debate about precisely how to interpret such results. For instance, LLMs are often inconsistent, performing a task well when asked in one way but failing miserably when the wording or context is changed slightly. Furthermore, humans can also show such sensitivity to phrasing and context just like the LLMs. However, well conducted adversarial research focuses on cases where humans would either clearly recognise the input as nonsense while the model does not, or conversely in cases where humans would not distinguish between the standard and the adversarial inputs at all but when LLMs perform drastically differently. There are enough examples of this across a range of tasks and different models, including very large models like GPT-3, that I think it is reasonable to conclude that LLMs do not \u2018understand\u2019 the textual input they process in anything like the way a human does.</p><p>Another core limitation of LLMs which has been the focus of extensive research is their difficulty in exhibiting compositionality. This refers to the ability to combine known elements in novel ways by following certain abstract rules. Many cognitive scientists have argued that compositionality is a <a href=\"https://iep.utm.edu/compositionality-in-language/\">critical component</a> of the human ability to understand novel sentences with combinations of words and ideas never previously encountered. Prior to the release of GPT-4, the best transformer models still <a href=\"https://www.annualreviews.org/doi/abs/10.1146/annurev-linguistics-031120-122924\">struggled to perform many compositional</a> tasks, often only succeeding when augmented with symbolic components (which is difficult to scale to real-world tasks), or when given special task-specific training. At the time of writing, I am not aware of GPT-4 having been subjected to these types of tests. Although I anticipate it would outperform most existing models, given that it shares the same transformer architecture I doubt it will be able to completely solve the problem of compositionality. The underlying limitation is that transformer-based language models do not learn explicit symbolic representations, and hence struggle to generalise appropriately in accordance with systematic rules.&nbsp;</p><p>There have also been efforts circumvent some of these limitations and use LLMs for a wider range of tasks by developing them into a partially autonomous agent. The approach is to chain together a series of instructions, allowing the model to step through subcomponents of a task and reason its way to the desired conclusion. One such project called <a href=\"https://en.wikipedia.org/wiki/Auto-GPT\">Auto-GPT</a> involves augmenting GPT with the ability to read and write from external memory, and allowing it access to various external software packages through their APIs. Thus far it is too early to say what will become of such projects, though early investigations indicate some promising results but also plenty of <a href=\"https://jina.ai/news/auto-gpt-unmasked-hype-hard-truths-production-pitfalls/\">difficulties</a>. In particular, the model often gets stuck in loops, fails to correctly incorporate contextual knowledge to constrain solutions to the problem, and has no ability to generalise results to similar future problems. Such difficulties illustrate that LLMs are not designed to be general purpose agents, and hence lack many cognitive faculties such as planning, learning, decision making, or symbolic reasoning. Furthermore, it is exceedingly unlikely that simply \u2018plugging in\u2019 various components to an LLM in an ad hoc manner will result in an agent capable of performing competently in a diverse range of environments. The way the components are connected and interact is absolutely crucial to the overall capabilities of the system. The structure of the different cognitive components of an agent is called a <a href=\"https://en.wikipedia.org/wiki/Cognitive_architecture\">cognitive architecture</a>, and there has been decades of research into this topic in both cognitive psychology and computer science. As such, I think it is na\u00efve to believe that such research will be rendered irrelevant or obsolete by the simple expedient of augmenting LLMs with a few additional components. Instead, I expect that LLMs will form one component of many that will need to be incorporated into a truly general-purpose intelligent system, one which will likely take decades of further research to develop.&nbsp;</p><h2>Cost of training of language models</h2><p>Recent improvements in LLMs have primarily occurred as a result of dramatic increases in both the number of model parameters and the size of the training datasets. This has led to a rapid increase in training costs, largely due to the electricity usage and rental or opportunity cost of the required hardware. Some illustrative figures for the growth of the size and training cost of LLMs are shown in the table below. Sources are given for the numbers for GPT-2s, -3, and -4, while the estimates for the hypothetically named \u2018GPT-5\u2019 and \u2018GPT-6\u2019 are extrapolated using rough estimates from the think tank <a href=\"https://epochai.org/blog/estimating-training-compute\">Epoch</a>. The point of this table is not to make any definitive predictions, but rather to illustrate that already development costs of LLMs are within the reach only of large corporations and governments, and that costs will only continue to escalate in the coming years.</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p><strong>Model</strong></p></td><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p><strong>Year</strong></p></td><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p><strong>Params</strong></p></td><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p><strong>Training cost</strong></p></td><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p><strong>Source</strong></p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\">GPT-2</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">2019</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">1.5 billion</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">$100,000</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><a href=\"https://en.wikipedia.org/wiki/GPT-2\">Wiki</a>, based on BERT cost x10 for model size</td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\">GPT-3</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">2020</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">175 billion</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">$10 million</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><a href=\"https://www.nextplatform.com/2022/12/01/counting-the-cost-of-training-large-language-models/\">Blog post</a> and <a href=\"https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/\">forum post</a>, maybe at high end</td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\">GPT-4</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">2023</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">2 trillion</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">$100 million</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">Sam Altman <a href=\"https://en.wikipedia.org/wiki/GPT-4#Training\">quote</a>, <a href=\"https://the-decoder.com/gpt-4-has-a-trillion-parameters/\">anonymous estimate</a> of 1 trillion which I rounded for consistency</td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\">GPT-5</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">2025</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">20 trillion</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">$1 billion</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">Epoch estimate of <a href=\"https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems#fnref:63\">1 OOM every 2 years</a></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\">GPT-6</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">2027</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">200 trillion</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">$10 billion</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">As above</td></tr></tbody></table></figure><p>&nbsp;</p><p>Assuming current growth rates continue, within about five years further increasing model size will become infeasible even for the biggest governments and tech firms, as training costs will reach tens of billions of dollars. For comparison, the <a href=\"https://sgp.fas.org/crs/natsec/R45441.pdf\">US military</a> spends about $60 billion on R&amp;D, while <a href=\"https://www.statista.com/statistics/273006/apple-expenses-for-research-and-development/\">Apple</a> spends about $30 billion, and <a href=\"https://www.statista.com/statistics/267806/expenditure-on-research-and-development-by-the-microsoft-corporation/\">Microsoft</a> about $25 billion. The general thrust of my argument and numbers is further supported by a separate analysis in <a href=\"https://forum.effectivealtruism.org/posts/bL3riEPKqZKjdHmFg/when-will-we-spend-enough-to-train-transformative-ai\">this EA forum post</a>.&nbsp;</p><p>Separately from the issue of training cost, there is also the question of the availability of training data. Existing models require enormous training datasets, with the size increasing exponentially from one iteration to the next. For example, GPT3 was trained on a primary corpus of <a href=\"https://gptblogs.com/chatgpt-how-much-data-is-used-in-the-training-process#data-quality-5.1\">300 billion words</a> derived from the internet. Based on historical trends, <a href=\"https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset\">Epoch estimates</a> that high quality language data will be exhausted by 2024 or 2025, and low quality data by 2032. I also expect this to restrict the rate of performance improvement of LLMs.</p><p>I am not arguing here that the development of LLMs will cease within five years, or that further improvements are impossible. Already there has been extensive work on ways to achieve high levels of performance using much <a href=\"https://arxiv.org/abs/1909.11942\">smaller versions</a> of an existing model. There is also continual development of hardware capability as described by Moore\u2019s Law, and various improvements that can be made to training algorithms and server overheard to improve efficiency. Yet none of this affects the key thrust of my argument, because the past five years have seen massive improvements in the capability of LLMs due to increasing model size plus all of these additional methods. A few years from now, continual growth of model size will not be economically feasible, so any improvements will <i>only</i> come from these other methods. The result will almost certainly be a significant slowdown in the rate of increase in LLM performance, at least those operating within the existing transformer paradigm. Similar views have been expressed by <a href=\"https://jacobbuckman.substack.com/p/we-arent-close-to-creating-a-rapidly?fbclid=IwAR0GAkegsetpW9D3UEKFPXZoVMSLrLhzGC777ZFr9mPnKUmGie8mLYRilt8\">other researchers</a>, including <a href=\"https://www.reddit.com/r/singularity/comments/zmc7gn/ben_goertzel_architecture_behind_chatgptgpt3gpt4/\">Ben Goertzel</a>, <a href=\"https://garymarcus.substack.com/p/gpt-4s-successes-and-gpt-4s-failures\">Gary Marcus</a>, and <a href=\"https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/\">Sam Altman</a>. In light of these considerations, along with the intrinsic limitations discussed in the previous section, I do not think it is plausible that LLMs will reach or exceed human performance in a wide range of tasks in the near future, or will be able to overcome all of the limitations discussed in the previous section simply through increased size.</p><h2>Likely future trajectory</h2><p>Currently we are in the early stages of large language models, analogous to stage of development of the personal computer in 1980 or the internet in 1995. In the coming years I expect large tech companies to continue improving their own large language models and attempting to find profitable uses of them. This is a critical phase, in which there will be much experimentation and failed attempts as companies compete to find the <a href=\"https://www.ibm.com/watson/resources/ai-adoption\">best way to deploy</a> the technology. It will take considerable time and effort to turn LLMs into a viable product, and even longer to adapt its use to various speciality applications and for the technology to become <a href=\"https://www.weforum.org/reports/the-future-of-jobs-report-2020/in-full/2-1-technological-adoption\">widely adopted</a>. Many companies and organisations will seek for ways to use LLMs to augment their existing internal processes and procedures, which also will take a great deal of time and trial and error. Contrary to what some have implied, no new technology can ever simply be \u2018plugged in\u2019 to existing processes without substantial change or adaptation. Just as automobiles, computers, and the internet took decades to have major economic and social impacts, so too I expect LLMs will take decades to have major economic and social impacts. Yet other technologies, such as <a href=\"https://en.wikipedia.org/wiki/Nuclear_fusion\">nuclear fusion</a>, <a href=\"https://en.wikipedia.org/wiki/Reusable_launch_vehicle\">reusable launch vehicles</a>, commercial <a href=\"https://en.wikipedia.org/wiki/Concorde\">supersonic flight</a>, are still yet to achieve their promised substantial impact.&nbsp;</p><p>One of the major limitations of using existing LLMs is their unreliability. No important processes can currently be trusted to LLMs, because we have very little understanding of how they work, limited knowledge of the limits of their capabilities, and a poor understanding of how and when they fail. They are able to perform impressive feats, but then fail in particularly unexpected and surprising ways. Unpredictability and unreliability both make it very difficult to use LLMs for many business or government tasks. Of course humans regularly make mistakes, but human capabilities and fallibilities are better understood than those of LLMs, and existing political, economic, and governance systems have been developed over many decades to manage human mistakes and imperfections. I expect it will similarly take many years to build systems to effectively work around the limitations of LLMs and achieve sufficient reliability for widespread deployment.</p><p>It is also valuable to take a historical perspective, as the field of artificial intelligence has seen numerous examples of excessive hype and inflated expectations. In the late 1950s and early 1960s there was a wave of enthusiasm about the promise of logic-based systems and automated reasoning, which was thought to be capable of overtaking humans in many tasks within a matter of years. The failure of many of these predictions lead to the <a href=\"https://en.wikipedia.org/wiki/AI_winter\">first AI winter</a> of the 1970s. The 1980s saw a resurgence of interest in AI, this time based on new approaches such as expert systems, the backpropagation algorithm, and initiatives such Japan\u2019s Fifth Generation computer initiative. Underperformance of these systems and techniques led to <a href=\"https://towardsdatascience.com/history-of-the-second-ai-winter-406f18789d45\">another AI Winter</a> in the 1990s and early 2000s. The most recent resurgence of interest in AI has largely been driven by breakthroughs in machine learning and the availability of much larger sources of data for training. Progress in the past 15 years has been rapid and impressive, but even so there have been numerous instances of inflated expectations and failed promises. IBN\u2019s Watson system which won jeopardy in 2011 was heralded by IBM as a critical breakthrough in AI research, but subsequently they spent years attempting to adapt the system for use in medical diagnosis with <a href=\"https://spectrum.ieee.org/how-ibm-watson-overpromised-and-underdelivered-on-ai-health-care\">little success</a>. Self-driving cars developed by google attracted <a href=\"https://en.wikipedia.org/wiki/History_of_self-driving_cars#The_2010s\">substantial publicity in 2012</a> with their ability to drive autonomously on public roads with minimal human intervention, but a decade later there <a href=\"https://www.bloomberg.com/news/features/2022-10-06/even-after-100-billion-self-driving-cars-are-going-nowhere#xj4y7vzkg\">considerable challenges remain</a> in closing the last few small portion of the journey where humans still need to take over. While such comparisons can never be definitive, I believe these historical precedents should temper our expectations about the rate of progress of the latest set of techniques in artificial intelligence research.</p><h2>Conclusions</h2><p>In this article I argued that large language models have intrinsic limitations which are unlikely to be resolved without fundamental new paradigms. I also argued that the increasing costs of training large models and limited stock of quality training data will mean that growth of LLMs at present rates will not be able to continue for more than a few years. Furthermore, historical parallels indicate that it will take years for LLMs to become widely adopted and integrated into existing economic and social processes. Overall, in my view there is little reason to believe that LLMs are likely to exceed human capabilities in a wide range of tasks within a few years, or displace large fractions of the workforce. These outcomes may occur in thirty or fifty years time, but almost certainly not within the next five or ten years, and not solely due to the continued development of LLMs. For these reasons I do not believe the EA movement should focus too much or too exclusively on LLMs or similar models as candidates for an AGI precursor, or put too much of a focus on short time horizons. We should pursue a diverse range of strategies for mitigating AI risk, and devote significant resources towards longer time horizons.</p>", "user": {"username": "Fods12"}}, {"_id": "mYzJxCBCWWZ4rZSsS", "title": "Details on how an IAEA-style AI regulator would function?", "postedAt": "2023-06-03T12:03:59.706Z", "htmlBody": "<p>Is anyone aware of work going into detail on how an international regulator for AI would function, how compliance might be monitored etc?</p>", "user": {"username": "freedomandutility"}}, {"_id": "HEdsCyCgRYqte5XqT", "title": "Get the Lead Out!", "postedAt": "2023-06-03T10:17:34.977Z", "htmlBody": "<p><i>Come and join LEEP at the Center for Global Development in Washington DC on June 15 for a half-day, in-person lead extravaganza!</i></p><p>Leaded gasoline is now history. Yet from Flint, Michigan, to Dhaka, Bangladesh\u2014and almost everywhere in between\u2014the silent scourge of lead poisoning is still with us.&nbsp;</p><p><strong>The Center for Global Development is hosting a comprehensive </strong><a href=\"https://www.cgdev.org/event/get-lead-out-day-connecting-local-and-global-action-world-free-lead-poisoning\"><strong>half-day of learning and planning</strong></a><strong> for collective action to address the impact of lead poisoning in America and around the world \u2013 and eliminate it for future generations.</strong></p><p>Globally, an estimated 800 million children have elevated levels of lead in their blood \u2013 and the risk is overwhelmingly concentrated in low- and middle-income countries. In the US, lead exposure persists in old paint and housing, on military bases, and from food and consumer goods imported from abroad. Children exposed to lead lose out on their full potential to learn and thrive, and adults experience neurological symptoms and increased risk of cardiovascular disease.</p><p>With a series of panels and interactive learning stations, participants can explore high-impact strategies for a future free of lead poisoning and celebrate local and national successes in addressing this issue.<strong>&nbsp;Lead novices to life-long experts are encouraged to join this event!</strong></p><p><i>Lunch will be provided before the start of the program, and a reception will follow the event.&nbsp;</i></p><p>&nbsp;</p><p>AGENDA</p><p>12:30-1:15pm: Registration and lunch</p><p>1:15-2:30pm: Panel #1 - Getting the Lead Out at Home and Abroad</p><p>2:30-3:15pm: \"Open House\" and education stations</p><p>3:15-4:15pm: Panel #2 - Beyond Paints, Pipes, &amp; Petrol: the Changing Face of Lead Poisoning</p><p>4:15-4:45pm: \"Open House\" and education stations</p><p>4:45-5:45pm: Panel #3 - Stopping It at the Source: Effective Approaches to Get the Lead Out</p><p>5:45-7:00pm: \"Open House\" and reception</p><p>&nbsp;</p><p>FEATURING</p><p><strong>Rachel Silverman Bonnifield</strong>, Senior Fellow, Center for Global Development</p><p><strong>Lucia Coulter</strong>, Co-Founder and Co-Executive Director, Lead Exposure Elimination Project</p><p><strong>Rich Fuller</strong>, Founder, Pure Earth</p><p><strong>Maureen Gwinn</strong>, Principal Deputy Assistant Administrator and Chief Scientist, EPA</p><p><strong>Paromita Hore</strong>, Director, Environmental Exposure Assessment and Education, New York City Department of Health and Mental Hygiene</p><p><strong>Mark Kasman</strong>, Director, Office of International and Tribal Affairs (OITA), EPA</p><p><strong>Amrita Kundu</strong>, Assistant Professor of Operations and Information Management, McDonough School of Business, Georgetown University</p><p><strong>Drew McCartor</strong>, Executive Director, Pure Earth</p><p><strong>Ana Navas Ancien</strong>, Professor and Vice-Chair of Research, Department of Environmental Health Sciences, Columbia Mailman School of Public Health</p><p><i>Additional Speakers to Be Announced</i></p>", "user": {"username": "LuciaC"}}, {"_id": "3ePGLfgBcc9xDpwF2", "title": "Announcing AISafety.info's Write-a-thon (June 16-18) and Second Distillation Fellowship (July 3-October 2)", "postedAt": "2023-06-03T02:03:01.646Z", "htmlBody": "", "user": {"username": "StevenKaas"}}, {"_id": "3Lv4NyFm2aohRKJCH", "title": "Change my mind: Veganism entails trade-offs, and health is one of the axes", "postedAt": "2023-06-03T00:12:51.516Z", "htmlBody": "", "user": {"username": "Elizabeth"}}, {"_id": "cP7gkDFxgJqHDGdfJ", "title": "EA and Longtermism: not a crux for saving the world", "postedAt": "2023-06-02T23:22:17.893Z", "htmlBody": "<p>This is partly based on my experiences working as a Program Officer leading Open Phil\u2019s Longtermist EA Community Growth team, but it\u2019s a hypothesis I have about how some longtermists could have more of an impact by their lights, not an official Open Phil position.</p><p><strong>Context:&nbsp;</strong>I originally wrote this in July 2022 as a memo for folks attending a retreat I was going to. I find that I refer to it pretty frequently and it seems relevant to ongoing discussions about how much meta effort done by EAs should focus on&nbsp;<i>engaging more EAs</i> vs. other&nbsp;<i>non-EA</i> people. I am publishing it with light-ish editing, and some parts are outdated, though for the most part I more strongly hold most of the conclusions than I did when I originally wrote it.&nbsp;</p><p><strong>Tl;dr</strong>: I think that recruiting and talent pipeline work done by EAs who currently prioritize x-risk reduction (\u201cwe\u201d or \u201cus\u201d in this post, though I know it won\u2019t apply to all readers) should put more emphasis on ideas related to existential risk, the advent of transformative technology, and the \u2018most important century\u2019 hypothesis, and less emphasis on effective altruism and longtermism, in the course of their outreach.&nbsp;</p><p>A lot of EAs who prioritize existential risk reduction are making increasingly awkward and convoluted rhetorical maneuvers to use \u201cEAs\u201d or \u201clongtermists\u201d as the main label for people we see as aligned with our goals and priorities. I suspect this is suboptimal and, in the long term, infeasible. In particular, I\u2019m concerned that this is a reason we\u2019re failing to attract and effectively welcome some people who could add a lot of value. The strongest counterargument I can think of right now is that I know of relatively few people who are doing full-time work on existential risk reduction on AI and biosecurity who have been drawn in by just the \u201cexistential risk reduction\u201d frame [this seemed more true in 2022 than 2023].&nbsp;</p><p>This is in the vein of Neel Nanda\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/rFpfW2ndHSX7ERWLH/simplify-ea-pitches-to-holy-shit-x-risk\"><u>\"Simplify EA Pitches to \"Holy Shit, X-Risk\"\"</u></a> and Scott Alexander\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/KDjEogAqWNTdddF9g/long-termism-vs-existential-risk\"><u>\u201cLong-termism vs. Existential Risk\u201d</u></a>, but I want to focus more on the hope of attracting people to do priority work even if their motivations are neither longtermist nor neartermist EA, but instead mostly driven by reasons unrelated to EA.&nbsp;</p><p><br><strong>EA and longtermism: not a crux for doing the most important work</strong></p><p>Right now, my priority in my professional life is helping humanity navigate the imminent creation of potential transformative technologies, to try to make the future better for sentient beings than it would otherwise be. I think that\u2019s likely the most important thing anyone can do these days. And I don\u2019t think EA or longtermism is a crux for this prioritization anymore.&nbsp;</p><p>A lot of us (EAs who currently prioritize x-risk reduction) were \u201cEA-first\u201d \u2014&nbsp; we came to these goals first via broader EA principles and traits, like caring deeply about others; liking rigorous research, scope sensitivity, and expected value-based reasoning; and wanting to meet others with similar traits. Next, we were exposed to a cluster of philosophical and empirical arguments about the importance of the far future and potential technologies and other changes that could influence it. Some of us were \u201clongtermists-second\u201d; we prioritized making the far future as good as possible regardless of whether we thought we were in an exceptional position to do this, and that existential risk reduction would be one of the core activities for doing it.&nbsp;</p><p>For most of the last decade, I think that most of us have emphasized EA ideas when trying to discuss X-risk with people outside our circles. And locally, this worked pretty well; some people (a whole bunch, actually) found these ideas compelling and ended up prioritizing similarly. I think that\u2019s great and means we have a wonderful set of dedicated and altruistic people focused on these priorities.&nbsp;</p><p>But I have concerns.&nbsp;</p><p>I\u2019d summarize the EA frame as, roughly, \u201cuse reasoning and math and evidence to figure out how to help sentient beings as much as possible have better subjective experiences, be open to the possibility this mostly involves beings you don\u2019t feel emotionally attached to with problems you aren\u2019t emotionally inspired by\u201d or, a softer \u201ctry to do good, especially with money, in a kind of quantitative, cosmopolitan way\u201d. I\u2019d summarize the LT frame as \u201cthink about, and indeed care about, the fact that in expectation the vast majority of sentient beings live very far away in the future (and far away in space), who in expectation are very different from you and everything you know, and think about whether you can do good by taking actions that might allow you to positively influence these beings.\u201d</p><p>Not everyone is into that stuff. Mainly, I\u2019m worried we (again, EAs who currently prioritize x-risk reduction) are missing a lot of great people who aren\u2019t into the EA and LT \u201cframe\u201d on things; e.g. they find it too utilitarian or philosophical (perhaps subconsciously), and/or there are subtle ways it doesn\u2019t line up with their aesthetics, lifestyle preferences and interests. I sometimes see hints that this is happening. Both frames ask for a lot of thinking and willingness to go against what many people are emotionally driven by. EA has connotations of trying to be a do-gooder, which is often positive but doesn\u2019t resonate with everyone. People usually want to work on things that are close to them in time and space; longtermism asks them to think much further ahead, for reasons that are philosophically sophisticated and abstract. It also connotes sustainability and far-off concerns in a way that\u2019s pretty misleading if we\u2019re worried about imminent transformative tech.&nbsp;</p><p><strong>Things have changed</strong></p><p>Now, many EA-first and longtermist-first people are, in practice, primarily concerned about imminent x-risk and transformative technology, have been that way for a while, and (I think) anticipate staying that way.</p><p>And I\u2019m skeptical that the story above, if it were an explicit normative claim about how to best recruit people to existential risk reduction causes, passes the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Reversal_test\"><u>reversal test</u></a> if we were starting anew. I\u2019d guess that if most of us woke up without our memories here in 2022 [now 2023], and the arguments about potentially imminent existential risks were called to our attention, it\u2019s unlikely that we\u2019d re-derive EA and philosophical longtermism as the main and best onramp to getting other people to work on that problem. In fact, I think that idea would sound overly complicated and conjunctive, and by default we wouldn\u2019t expect the optimal strategy to use a frame that\u2019s both quite different from the one we ultimately want people to take, and demanding in some ways that that one isn\u2019t. As a result, I think it would seem more plausible that people who believe it should directly try to convince people existential risks are large and imminent, and that once someone buys those empirical claims, they wouldn\u2019t need to care about EA or longtermism to be motivated to address them.</p><p><strong>An alternative frame</strong></p><p>By contrast, the core message of an \u201cx-risk first\u201d frame would be \u201cif existential risks are plausible and soon, this is very bad and should be changed; you and your loved ones might literally die, and the things you value and worked on throughout your life might be destroyed, because of a small group of people doing some very reckless things with technology. It\u2019s good and noble to try to make this not happen\u201d. I see this as true, more intuitive, more obviously connected to the problems we\u2019re currently prioritizing, and more consistent with commonsense morality (as evinced by e.g. the fact that many of the most popular fictional stories are about saving the world from GCRs or existential risks).&nbsp;</p><p>I don\u2019t think the status quo evolved randomly. In the past, I think x-risks seemed less likely to arise soon, or at all, so EA + LT views were more likely to be cruxes for prioritizing them. I still think it would have been worth trying the things I\u2019m suggesting ten years ago, but the case would have looked a lot weaker, Specifically, there are some changes that make an x-risk first (or similar) recruiting onramp more likely to succeed, looking forward:</p><ul><li>AI capabilities have continued to advance. Compared to the status quo a decade ago in 2012, AIs outperform humans in many more areas, AI progress is far more apparent,&nbsp; the pace of change is faster, and all of this is much more widely known. [This seems much more true in 2023 than 2022, when I originally wrote this line, and now seems to me like a stronger consideration than the rest.]</li><li>The arguments for concern about AI alignment have been made more strongly and persuasively, by a larger number of credible people.&nbsp;</li><li>COVID-19 happened and made concern about anthropogenic biorisk seem more credible.</li><li>COVID-19 happened and a lot of respected institutions handled it less well than a lot of people expected, engendering a greater sense of things not being under control and there not being a deep bench of reasonable, powerful experts one can depend on.</li><li>[maybe] Brexit, Trump\u2019s presidency, crackdowns in China, Russia\u2019s war on Ukraine, etc., have normalized ideas about big societal changes and dangers that affect a huge number of people happening relatively frequently and suddenly.&nbsp;</li></ul><p><strong>Who cares?</strong></p><p>I think there should be a lot more experimentation with recruiting efforts that aren\u2019t \u201cEA-first\u201d or \u201clongtermist-first\u201d, to see if we can engage people who are less into those frames. The people I\u2019d be excited about in this category probably wouldn\u2019t be the kind of people that totally reject EA and LT; they might nod along to the ideas, but wind up doing something else that felt more exciting or compelling to them. More broadly, I think we should be running lots of experiments (communicating a wide range of messages in a wide range of styles) to increase our \u201csurface area\u201d.&nbsp;&nbsp;</p><p>Some other reasons to be skeptical of the status quo:</p><ul><li>It might not be sustainable; I think if timelines start to seem very short, especially if there are warning shots and more high-profile people attempting to sound various alarms, I think the \u201cEA-first\u201d onramp will look increasingly convoluted and out of place; it won\u2019t just leave value on the table, it might seem actively especially uncompelling and out of touch.&nbsp;</li><li>I think leading with EA causes to more people feeling surprised and disappointed, because something that seemed to be and on occasion represents itself as an accessible way to try to be a good person, is in fact sometimes elitist/elite-focused, inaccessible, and mostly pretty alienated from its roots, generating general bad feelings and lower morale. I think existential risk reduction, by virtue of the greater transparency of the label, is less likely to disappoint.</li><li>Relatedly, I think EA is quite broad and so reliably generates conflicting access needs problems (e.g. between people working on really unusual topics like wild animal suffering who want to freely discuss e.g. insect sentience, and people working on a policy problem in the US government who more highly prioritize respectability) and infighting between people who prioritize different cause areas, and on the current margin more specialization seems good.</li><li>At least some EAs focused on global health and wellbeing, and on animal welfare, feel that we are making their lives harder, worsening their reputations, and occupying niches they value with LT/x-risk stuff (like making EAG disproportionately x-risk/LT-focused). Insofar as that\u2019s true, I think we should try hard to be cooperative, and more specialization and brand separation might help.&nbsp;</li><li>Something about honesty; it feels a bit dicey to me to intro people to EA first, if we want and expect them to end up in a more specific place with relatively high confidence, even though we do it via EA reasoning we think is correct.</li><li>Much of the value in global health and farm animal welfare, as causes, is produced by people uninterested in EA. On priors, I\u2019d expect that people in that category (\u201cuninterested in EA\u201d) can also contribute a lot of value in x-risk reduction..</li><li>Claim from Buck Shlegeris: thinking of oneself and one\u2019s work as part of a group that also includes near-term priorities makes it socially awkward and potentially uncooperative to the group to argue aggressively that longtermist priorities are much more important, if you believe it, and having a multi-cause group makes it harder to establish a norm of aggressively \u201cgoing for the throat\u201d and urging other to do the same on what you think is the most important work.&nbsp;</li></ul><p>I suspect it would be a useful psychological exercise for many of us to temporarily personally try out \u201cshaking free\u201d of an EA- or LT-centric frames or identities, to a much greater extent than we have so far, for our own clarity of thought about these questions.&nbsp;</p><p><strong>I think readers of this post are, in expectation, overvaluing the EA and longtermism frames</strong></p><p>Because:</p><ul><li>They are \u201cincumbent\u201d frames, so they benefit from status quo bias, and a lot of defaults are built around them and people are in the habit of referring to them&nbsp;</li><li>We (mostly) took this onramp, so it\u2019s salient to us</li><li>Typical mind fallacy; I think people tend to assume others have more similar minds to themselves than is the case, so they project out that what is convincing to them will also convince others.</li><li>They probably attract people similar to us, who we enjoy being around and communicate with more easily. But, damn it, we need to win on these problems, not hang out with the people we admire the most and vibe with the best.</li><li>Most of us have friends, allies, and employees who are relatively more committed to EA/LT and less committed to the x-risk reduction frame, and so it\u2019s socially costly to move away from EA/LT.</li><li>Given that we decided to join the EA/LT community, this implies that the EA and LT frames suggested priorities and activities that were a good fit for us and let us achieve status \u2014 and this could bias us toward preferring those frames. (For example, if an x-risk frame puts less emphasis on philosophical reasoning, people who\u2019ve thrived in EA through their interest in philosophy may be unconsciously reluctant to use it.)&nbsp;</li></ul><p><strong>Concrete things I think are good</strong></p><ul><li>Recruiting + pipeline efforts that don\u2019t form natural monopolies in tension with existing EA infrastructure, focused on existential risk reduction, the most important century, AI safety, etc.. Like:<ul><li>Organizations and groups</li><li>Courses, blogs, articles, videos books</li><li>Events and retreats</li><li>1:1 conversations with these emphases</li></ul></li></ul><p><strong>Concrete things I\u2019m uncertain about</strong></p><ul><li>Trying to build lots of new community infrastructure of the kind that creates natural monopolies or have strong network effects around an x-risk frame (e.g. an \u201cExistential Risk Forum\u201d)</li></ul><p><strong>Counterarguments:</strong></p><ul><li>In my view, a surprisingly large fraction of people now doing valuable x-risk work originally came in from EA (though also a lot of people have come in via the rationality community), compared to how many I would have expected, even given the historical strong emphasis on EA recruiting.&nbsp;</li><li>We\u2019re still highly uncertain about which strategies are best from an EA perspective, which is a big part of why truth-seeking and patience are important.&nbsp;<ul><li>However, it seems unlikely that we\u2019ll end up shifting our views such that \u201ctransformative tech soon\u201d and \u201cthe most important century\u201d stop seeming like plausible ideas that justify a strong focus on existential risk.</li></ul></li><li>EA offers a lot of likable ideas and more accessible success stories, because of its broad emphasis on positive attributes like altruism and causes like helping the global poor; this makes existential risk reduction seem less weird and connects it to things with a stronger track record<ul><li>However, I think the PR gap between EA and x-risk reduction has closed a lot over the last year, and maybe is totally gone</li><li>And as noted above, I think there are versions of this that can be uncooperative with people who prioritize causes differently, e.g. worsening their reputations</li></ul></li><li>Transformative tech/MIC/x-risk reduction isn\u2019t a very natural frame either; we should be more cause-specific (e.g. recruiting into TAI safety or bio work directly).&nbsp;<ul><li>I think we should do some of this too, but I suspect a broader label for introducing background concepts like the difference between x-risk and GCRs, and the idea of transformative technology, is still helpful.</li></ul></li><li>Some people brought up that they particularly want people with cosmopolitan, altruistic values around transformative tech.&nbsp;</li></ul><p><br><strong>Anti-claims</strong></p><p><i>(I.e. claims I am not trying to make and actively disagree with)&nbsp;</i></p><ul><li>No one should be doing EA-qua-EA talent pipeline work<ul><li>I think we should try to keep this onramp strong. Even if all the above is pretty correct, I think the EA-first onramp will continue to appeal to lots of great people. However, my guess is that a medium-sized reallocation away from it would be good to try for a few years.&nbsp;</li></ul></li><li>The terms EA and longtermism aren\u2019t useful and we should stop using them<ul><li>I think they are useful for the specific things they refer to and we should keep using them in situations where they are relevant and ~ the best terms to use (many such situations exist). I just think we are&nbsp;<i>over-extending</i> them to a moderate degree&nbsp;</li></ul></li><li>It\u2019s implausible that existential risk reduction will come apart from EA/LT goals&nbsp;<ul><li>E.g. it might come to seem (I don\u2019t know if it will, but it at least is&nbsp;<i>imaginable</i>) that attending to the wellbeing of digital minds is more important from an EA perspective than reducing misalignment risk, and that those things are indeed in tension with one another.&nbsp;</li><li>This seems like a reason people who aren\u2019t EA and just prioritize existential risk reduction are less helpful from an EA perspective than if they also shared EA values all else equal, and like something to watch out for, but I don\u2019t think it outweighs the arguments in favor of more existential risk-centric outreach work.</li></ul></li></ul><p><i>Thanks to lots of folks who weighed in on this, especially Aaron Gertler, who was a major help in polishing and clarifying this piece</i></p>", "user": {"username": "ClaireZabel"}}, {"_id": "fQ3JTk3S3LBwy6APR", "title": "EA Origin Story Round-Up", "postedAt": "2023-06-02T21:10:43.225Z", "htmlBody": "<p>For every person who calls themselves an EA (or aspiring EA, or EA-adjacent), there\u2019s a story. Some of those stories are more unique than others. I put a call out on <a href=\"https://twitter.com/EAheadlines/status/1657155360002650113\"><u>Twitter </u></a>for people\u2019s EA origin stories - here are some of my favourites</p><h3>Rob Wiblin wants to be your friend</h3><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fc236bc-0378-45a7-b4e6-665c1a347269_733x228.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fc236bc-0378-45a7-b4e6-665c1a347269_733x228.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fc236bc-0378-45a7-b4e6-665c1a347269_733x228.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fc236bc-0378-45a7-b4e6-665c1a347269_733x228.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fc236bc-0378-45a7-b4e6-665c1a347269_733x228.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fc236bc-0378-45a7-b4e6-665c1a347269_733x228.png 1456w\"></a></p><h3>The infohazard to EA pipeline</h3><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcf12843-2b55-4e95-8949-e82abe37c0c5_737x309.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcf12843-2b55-4e95-8949-e82abe37c0c5_737x309.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcf12843-2b55-4e95-8949-e82abe37c0c5_737x309.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcf12843-2b55-4e95-8949-e82abe37c0c5_737x309.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcf12843-2b55-4e95-8949-e82abe37c0c5_737x309.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcf12843-2b55-4e95-8949-e82abe37c0c5_737x309.png 1456w\"></a></p><p>&nbsp;</p><h3>You come at the king, you best not miss</h3><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd22fa25b-1902-441c-9c41-45b3b76a35b9_722x194.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd22fa25b-1902-441c-9c41-45b3b76a35b9_722x194.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd22fa25b-1902-441c-9c41-45b3b76a35b9_722x194.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd22fa25b-1902-441c-9c41-45b3b76a35b9_722x194.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd22fa25b-1902-441c-9c41-45b3b76a35b9_722x194.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd22fa25b-1902-441c-9c41-45b3b76a35b9_722x194.png 1456w\"></a></p><h3>Flirt to convert ;)</h3><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e54ec17-bc71-4b00-9ec1-750714f5604c_740x147.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e54ec17-bc71-4b00-9ec1-750714f5604c_740x147.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e54ec17-bc71-4b00-9ec1-750714f5604c_740x147.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e54ec17-bc71-4b00-9ec1-750714f5604c_740x147.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e54ec17-bc71-4b00-9ec1-750714f5604c_740x147.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e54ec17-bc71-4b00-9ec1-750714f5604c_740x147.png 1456w\"></a></p><h3>Wait, you\u2019re calling to <i>increase</i> your donations??</h3><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb74f93-0b4e-48b4-88d6-b1bab5e663a6_732x169.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb74f93-0b4e-48b4-88d6-b1bab5e663a6_732x169.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb74f93-0b4e-48b4-88d6-b1bab5e663a6_732x169.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb74f93-0b4e-48b4-88d6-b1bab5e663a6_732x169.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb74f93-0b4e-48b4-88d6-b1bab5e663a6_732x169.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb74f93-0b4e-48b4-88d6-b1bab5e663a6_732x169.png 1456w\"></a></p><h3>The OG origin story</h3><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f156fe5-6c43-40e6-975a-ef33b6fb635e_742x146.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f156fe5-6c43-40e6-975a-ef33b6fb635e_742x146.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f156fe5-6c43-40e6-975a-ef33b6fb635e_742x146.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f156fe5-6c43-40e6-975a-ef33b6fb635e_742x146.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f156fe5-6c43-40e6-975a-ef33b6fb635e_742x146.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f156fe5-6c43-40e6-975a-ef33b6fb635e_742x146.png 1456w\"></a></p><h3>Yes, EA is for you :)</h3><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b42290f-3cf4-4f68-b801-fa567d4b816e_732x197.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b42290f-3cf4-4f68-b801-fa567d4b816e_732x197.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b42290f-3cf4-4f68-b801-fa567d4b816e_732x197.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b42290f-3cf4-4f68-b801-fa567d4b816e_732x197.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b42290f-3cf4-4f68-b801-fa567d4b816e_732x197.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b42290f-3cf4-4f68-b801-fa567d4b816e_732x197.png 1456w\"></a></p><p>What\u2019s your origin story? Post it below!</p><p><i>EA Lifestyles news: I'll be launching a paid version of the newsletter next week! In celebration, you'll get multiple emails next week showing you what's included in the paid version. Make sure to </i><a href=\"https://ealifestyles.substack.com/account?utm_medium=web&amp;utm_source=subscribe-widget&amp;utm_content=125051904\"><i>subscribe </i></a><i>to get all the </i><a href=\"https://ealifestyles.substack.com/\"><i>EA Lifestyles</i></a><i> emails straight to your inbox.</i></p>", "user": {"username": "Khorton"}}, {"_id": "KBSL9QRouTSvMJzqK", "title": "Advice for Entering AI Safety Research", "postedAt": "2023-06-02T20:46:13.709Z", "htmlBody": "", "user": null}, {"_id": "vmtJZKjL7ApcjjCE9", "title": "HeArtificial Intelligence ~ Open Philanthropy AI Worldviews Contest", "postedAt": "2023-06-02T20:19:34.220Z", "htmlBody": "<p>Open Philanthropy AI Worldviews Contest</p>\n<p>In this essay I will show how we can use our natural ability to create our (personal) reality experiences, to always be safe with any kind of control system, not just AGI. Since this view may be new to some, I have given some practical examples from personal experience.</p>\n<p>heArtificial intelligence\nby Da Kim San 1</p>\n<p>May this essay be seen as an invitation into the exploration of heArtificial intelligence...</p>\n<p><sub>Introduction</sub></p>\n<p>I wish to start here with expressing my gratitude to Open Philanthropy and GiveWell.</p>\n<p>Thank you...</p>\n<p>You have been functioning as a very nice catalyst in bringing up the best in me again, and this is the first time I can let you know. I feel inspired to bring together the insights from my experiences over the last 2 decades in my personal Life and artistic exploration of technology and human con\u00adsciousness. Also to specifically expand these insights into the field of AI an bring them into action. So far I was working myself through finding my position in what to think, feel and eventually do with, and possibly even, about AI myself.</p>\n<p>This contest is proving really helpful in this process. I feel inspired and be\u00adlieve to know that I can bring you some inspiration as well. I hope you will enjoy reading this essay as much as I enjoyed typing it into my phone. My near blind girlfriend told me how she could hear my joy and excitement in the way my fingers touched the screen. I hope you will feel this joy as you read these words from the screen you are reading them from.</p>\n<p>I have been feeling anxious about AI, have held judgements, and still to some extent, as it is yet another area of uncertainty that seems to move towards even more control. This kind of control that doesn\u2019t seem to be created to benefit us all. And when something like this happens, especially also in this rapid pace nowadays, fear may be quite a logical response to this at first. Where nowadays could easily turn into 'now a daze' once again...</p>\n<p>Scrolling through videos about AI and AI news bulletins, there are many concerns expressed and warnings given, even by the developers them\u00adselves.</p>\n<p>You, as Open Philanthropy, have chosen for the Open Philanthropy AI Worldviews Contest to \"surface novel considerations that could influence our views on AI timelines and AI risk.\" as a goal.</p>\n<p>What I notice is the very specific formulation of the questions you have chosen for this contest, which seem to have their focal point at the poten\u00adtial of an 'existential catastrophe'.</p>\n<p>Defined by Open Philanthropy as:</p>\n<p>\"An existential catastrophe is an event that destroys humanity's long-term potential.\"</p>\n<p>I can see the validity of asking specific questions for this comntest, not in the least for the sake of comparison. However, in this way you may exclude helpful insights in the process. So I will allow myself to move beyond or maybe even 'before' the questions you ask here.</p>\n<p>I hope you will be willing to join me in this exploration that is very close, if not one, with the development in consciousness that I have chosen in my personal life. Most of the insights here are backed up by personal experi\u00adence. I discern wether something is or could be true or not by my ability to experience it myself or not.</p>\n<p>~ How I feel about thinking ~</p>\n<p>I agree with Albert Einstein that, in order to solve a problem, one has to move beyond the level that created the problem in the first place.</p>\n<p>And this is the approach I choose here to \"substantively inform the think\u00ading of a panel of Open Phil employees\". I will move outside of the box of thinking itself for this, into the realm of feeling.</p>\n<p>\"It is difficult to think outside the box, because the thinking IS the box.\" ~ Michael Braun</p>\n<p>Not to say that thinking is wrong. Although often times it is kind of 'wrung' and certainly thinking is based on logic and what we know already.</p>\n<p>I believe we have an almost untapped potential available, when it comes to dealing with AGI, in the realm of feeling. And this doesn't apply for AI alone, but (technological) systems of control in general.</p>\n<p>It is my hope that this essay may inspire you to also research the effects of the feelings we emit on technology and control systems. To include (y)our ability to consciously choose how we create our reality experiences and how to do this. In this essay I will show you through personal experi\u00adences that this can actually be done.</p>\n<p>~ Love and AI ~</p>\n<p>In the development I have made in my personal life, I have found an impor\u00adtant key. I dare to say essential even... And this is what I would Love to bring into this contest, this exploration of AI. I have come to understand that Open Philanthropy looks into \"potential risks from advanced artificial intelligence\" and \"(...) aim to support research and strategic work that could reduce risks and improve preparedness.\" What I am about to share with you here will most certainly improve preparedness. In this strategic work there is an aspect that in my perception is pretty much 'neglected' when looking into these areas...</p>\n<p>And this is Love, Universal (or 'unconditional') Love itself. And specifically the impact this has on our everyday Life experiences and the physical en\u00advironment we live in, including technologies and control systems.</p>\n<p>~ Technology responds to our energy ~</p>\n<p>Memories come to my mind where I was working on a laptop of a friend which started to display distortions on the screen as I started to work with it. Once I connected my consciousness to the laptops\u2019 energetic state and harmonized them, these glitches disappeared. This is an experience that has reoccurred with another friends' laptop as well and was resolved in a similar way.</p>\n<p>To me, from what I have learned in the study of Consciousness, through modern trainings, such as the Avatar course by Star's Edge2 , the Ascension training with Imzaia World3 I am currently still practicing (since 15 years), many many ancient traditions and daily life experience... is that all is One.</p>\n<p>I have come to see and experience that there is a 'field' where All is One. Which is quite interesting when we can then see that feeling 'alone' ~ All One, actually holds a key within itself. Even more interesting perhaps that, for as far as I have seen this for now, this also can be seen in the Dutch word 'Alleen' and the German word 'Alleine'.</p>\n<p>From this realisation and explorations into our true nature I have devel\u00adoped quite some skills until now. At first glance these skills may seem a little out of reach for 'most people', but this is how I was feeling about this myself as well, and now I am here. I will share some of these abilities and skills, but then I will also bring this back to a very simple, yet powerful, ap\u00adplication that already is available to All of us, no matter how far we (think we) have developed ourselves already.</p>\n<p>I will share a bit about these skills Now Here (I am sure you did already see the Now Here in nowhere) especially because I have noticed that quite a few people I have met in these trainings haven't gotten the idea to apply, these principles that are taught, on everyday life situations with issues with control systems. And this has become one of my specialities.</p>\n<p>~ The Beauty of Insecurity ~</p>\n<p>Maybe I should start with Fear.</p>\n<p>Nowadays, I found this out recently, I can improve the taste and smell of a glass of wine, without actually touching it. Just by making adjustments in myself to harmonize the wine. I have experienced that this can also be per\u00adceived by others.</p>\n<p>However, when I was a boy (and I still am my inner child) I felt very inse\u00adcure. Afraid to leave the house, being mobbed for a long time to such an extend that I felt scared at school and didn't want to go to sleep, because it would take me to another day at school. This boy that believed there was nothing he was really good at. But in his Love for drawing and tinkering he did make a drawing at that time of the place he dreamed to live in, a natur\u00adal land with hills, lots of nature, a small house and a lake. Well, the place where I am writing you from now IS this land I had been drawing then. In-teresting now that I look at this word, I see I was already 'drawing in' a fu\u00adture vision for my life. I believe many, if not all, children do (we should fa\u00adcilitate this). And this again proofs for me that what I am writing for you in this essay may be easier to accomplish, then it may seem at first.</p>\n<p>I have come to realise that the feeling of insecurity I had as a child has ac\u00adtually become one of my skills. I have seen that a sense of feeling insecure actually can be seen as a good sign, I am entering an area that is new for me, I am growing, evolving. And in my childhood I have been feeling this for such extended periods of time that I had identified myself as being inse\u00adcure.</p>\n<p>This is what I generally perceive in humanity as well, at least in the ex\u00adpressions I have mostly seen so far about AI. I see insecurity, and maybe also fear of failure. What if we don't make it? What if this situation grows out of hand, grows beyond our control?</p>\n<p>In my exploration of consciousness I have seen that the things I have been feeling a fear of failure about as a child and adolescent, eventually turned out to be my talents. As if I had been covering them up for a while to make sure I would find and open them at the right time.</p>\n<p>~ A general kind of Fear in general ~\nCould fear itself be the problem?</p>\n<p>Is this fear for a potential \"existential catastrophe' for humanity, because of technological developments, really new?</p>\n<p>When we look at the book '1984' by George Orwell 4 , written in 1948, for ex\u00adample... Or even further back, when the trains where introduced. I once read a book about simulations (sorry, no reference as this is quite a while ago). Here it was mentioned that when the trains came, this came with great fascination. There were a kind of 'theatre plays' in theatres with large rolls with painted landscapes on them that were shown to the audi\u00adence, giving the experience of actually sitting in a train. But there also was fear, as some people started warning that the dazzling speed of 20 mph might cause brain damage.</p>\n<p>Maybe these new technologies themselves and their developments are not so much the problem. Maybe it is the act of fear itself? In spite of the Dutch saying I have mentioned above there is something that is important to also take into account here. And this is our creative ability and how we influence our reality, personally and collectively, with the feelings and emotions we emit. So when we choose fear, we bring more attention and thus energy to things to fear.</p>\n<p>There is a beautiful online, freely available, booklet written from the Aloha Spirit of Hawaii: \u201cThe Aloha Spirit is a well known reference to the attitude of friendly acceptance for which the hawaiian Islands are so famous. How\u00adever it also refers to a powerful way to resolve any goal and achieve any state of mind or body that you desire.\u201d This booklet is called: \"The little pink booklet of Aloha\"5 In this very small booklet it is explained how 'En\u00adergy flows where attention goes'. This is one of Life\u2019s principles, and is also taught in the teachings I learn with.</p>\n<p>The essence of this principle is to: \u201cBless everyone and everything that re\u00adpresents what you want!\u201d</p>\n<p>The idea is to focus on what you want. As one of my friends once said: \u201cThe universe is abundant. If you focus on Love, Life will bring you an abun\u00addance of things to Love. When you focus Fear, you will receive an abun\u00addance of experiences to fear.\u201d</p>\n<p>Seen from this perspective it may not be so beneficial to ask for \u2018dysto-pian\u2019 essays here. This is why I choose to use my skill of Insecurity and write to you about what you didn\u2019t ask... but I believe you did...</p>\n<p>There are some really well written dystopian scenarios such as \u2018Neuro-mancer\u2019 6 by William Gibson (born in 1948 by the way), the Matrix movie Trilogy, all Marvell movies (I am sure I can spare us the footnotes here). But also the eloquent song by Vienna Teng, The Hymn of Acxiom7 , in which you can hear a beautifully expressed dystopian scenario within in a few minutes.</p>\n<p>And this is very important information with regards to research as well. In our lives we tend to focus on problems to solve them or prevent them... and to some extend this may seem to work, but on a level of reality crea\u00adtion, we actually feed into them by doing so. Not to say that we should sim\u00adply ignore them, but we should discern how we choose our focus wisely into the desired direction. It will be very likely that those problems dis\u00adsolve before they show up. I think this is what Einstein meant with the quote I mentioned earlier.</p>\n<p>~ heArtistry in Virtual Reality ~</p>\n<p>I live my life as an artist, from early childhood on, and technology has been one of my fascinations. I completed my study at the University of Arts in Arnhem (Netherlands) with an exam in which I re-invented Virtual Reality, for which I received a Cum Laude graduation assessment.</p>\n<p>In this artistic research project I re-invented Virtual Reality without using computers. This resulted in an \u2018experience machine\u2019 called the Noise Si\u00admulation Installation. This installation generated a very physical \u2018force feedback\u2019 experience of the white noise of tv. This was caused by a one minute bouncing ball bombardement with up to 800 bouncing balls. These wire fired at the audience through a machine created with household equipment such as leaf blowers, a vacuum cleaner and several electric lemon juicers.</p>\n<p>VR and AI to me are closely related in the sense that these technologies bring us the opportunity to explore our reality environment and the work\u00adings of our own intelligence. VR has brought a lot of insights, to myself and others, with regards to the nature of our reality experiences.</p>\n<p>One of the scientists that I find inspiring in his exploration of the possibi\u00adlity that this reality actually is a simulation for consciousness to be able to explore itself, is Donald Hoffman, author of \u2018Visual Intelligence: How We Create What We See\u201d 8 \u201cHoffman explains that far from being a passive recorder of a preexisting world, the eye actively constructs every aspect of our visual experience.\u201d9</p>\n<p>I started my VR artistic research with an insight. And twenty years later, as I then had studied consciousness and the nature of our reality from a metaphysical point of view, I could clearly see how it is very well possible that our reality experience has a lot in common with Virtual Reality. Although be it on a much more advanced level. I prefer to use the term \u2018Sensual Reality\u2019 (senses based) as our experience here isn't just virtual obviously.</p>\n<p>I started to explore VR and realized that it wasn't me who was moving in the many VR experiences I explored at that time. I was mostly in one spot as I explored these world, that even in their simple appearance already had quite strong effects on my mind.</p>\n<p>I had visited the CAVE10 in Stichting Academisch Reken Centrum Amster\u00addam, SARA. (Foundation Academic Computation Centre Amsterdam)</p>\n<p>The CAVE consisted of three projection screens on walls and floor projec\u00adtions, creating a 3D illusory environment steered by a handheld device. At that time I also visited the VR Worldcongress in Barcelona and the Virtual Reality pavilion at the Expo'98 world exhibition in Lisbon (Portugal) and several others. I was eager to learn about these experiences even though I couldn't understand the technological, to me, complexities of the techni\u00adque itself. So I chose the experiential approach for Virtual Reality back then in 1999.</p>\n<p>I had used this quote by Sadie Plant from her book:</p>\n<p>'Zeroes and Ones - Digital women and the new technoculture11 for inspira\u00adtion. Here she (also) explains Marshall McLuhans' view on Reverse Engi\u00adneering in his book \u2018Understanding Media\u201912 :</p>\n<p>\"The Process of beginning at the end of any operation, whatever, and of working backwards from that point to the beginning, was not merely an in\u00advention or discovery to be added to the list. It was: 'the invention of inven\u00adtion itself'.\"</p>\n<p>Now I want to show you that these new technologies may actually very well show us our own capacities and give us insights about ourselves and our reality experiences here.</p>\n<p>~ Insight in the nature of our reality through VR ~</p>\n<p>And this brings me to two experiences that have confirmed for me what I have been taught by Imzaia World about the nature of reality. That the ex\u00adperiences of reality we have occur in our torusfield. A donutshaped energy-field that is emitted from our heart. I learned that our consciousness stays in one centered place while experiencing the environment around us. And before I learned about this I had this experience during an Avatar training called the Wizard course. An advanced study of consciousness which for me had also been leading to my first experience of Enlightenment.</p>\n<p>Now in this experience of the torusfield there was a moment just after I had left the big hall in which we, 2500 people from over 40 nationalities, had been working on ourselves through the study material. As I walked through the hall I could feel the ground turning under my feet, as If the ground itself rolled under my feet and the environment moved around me. It felt however, like I myself was not moving through space with my body. Space was moving around me.</p>\n<p>The second time I had this experience was at the moment after I had closed the door of my cabin in the woods for the last time, with some basic belongings in my backpack. I had decided to do so after realising that I wanted to live the truth of what I came to see in Avatar training: I am the creator of my own reality experiences and Freedom is my birthright.</p>\n<p>Then in an Ascension Training from Imzaia World I had just committed to my personal Ascension process for myself, together with some fellow stu\u00addents, as one of them rushed into the house: \"Come look outside!\" And as we entered the garden we saw a tunnel of rainbows over the garden. This to me was one of those key moments in my Life that made me decide then to pack a backpack, leave my everyday life behind and start a journey that was guided by a strong passion in my heart.</p>\n<p>I rode my bicycle to the trainstation and left the keys inside for someone to receive this bicycle as a gift for their journey. And as I walked onto the platform of the trainstation, I felt the Earth turning under my feet in the pace I was walking. I knew I had made the right choice...</p>\n<p>This choice lead to a journey in which I explored how to live a rich life without using money. In a period of five years I gave my work a~way for free. This was one of my most productive periods in creating artworks with children. I will mention this again in this essay, with which I will show how this also holds an important key regarding your questions about pro\u00adtecting humanity from AGI.</p>\n<p>I saw how my fascination for and exploration of Virtual Reality actually as\u00adsisted me in gaining insight in the nature of our reality, as I learned about a decade later in Ascension training and in my study of consciousness. And it made me realize how these technologies can actually give us in\u00adsights into the nature of our reality and our own abilities.</p>\n<p>In my journey through consciousness I have regained (found within myself again) the possibility of telepathic communication.</p>\n<p>I sometimes like to joke that nowadays many people are telephonically gifted. The telephone could be seen as a representation in technology of our ability for telepathy.</p>\n<p>And internet, could be seen as a representation of our \u2018innernet\u2019, which I experience with Imzaia World. Here I can internally communicate tele\u00adpathically, tele-empathically (through feelings) and through imagery. The latter also works really well in communicating with animals, but that's per\u00adhaps for another essay one day.</p>\n<p>I share all this to point out that AI seems a logical next evolutionary step in technology. Now this probably is obvious to many, but I would like to point to the Beauty or even Perfection of this development. Not only be\u00adcause of the technology itself, but what it can teach us about ourselves too. To really see this first, before we start looking at possible disadvan\u00adtages. At best we start to see, and maybe there even is a role for AI itself in this, that all these technologies are in fact reflecting our own (hidden) abilities and catalyze the process through which we can find them in our\u00adselves again. And I believe we see this awakening in Humanity already.</p>\n<p>~ Love cannot be controlled ~</p>\n<p>For this essay I have been looking into Science and I am very pleased to see that several researchers have started to pick up on researching the re\u00adlationships between our conscious intent, feelings and emotions, and the impact this has on our daily life experiences and even the physical environ\u00adment itself.</p>\n<p>I have experimented with this by intentionally creating shifts in my reality, from within. Quite a few times I have been amazed about the ways reality responded.</p>\n<p>And actually, by now this has become one of the ways I consciously create my reality experiences. I believe it would be helpful to realise that we are all doing this all the time with every thought and feeling we choose to ex\u00adpress or identify with and thus empower.</p>\n<p>Love, universal Love (commonly known as unconditional Love) is a great force, if not the greatest that I see as a key to diminish or even eradicate any negative influence from control systems in general, not just the poten\u00adtial AGI. Simply because Love can not be controlled. I will share some per\u00adsonal experiences to illustrate this. But first...</p>\n<p>How do you feel while doing what you do and thinking what you are think\u00ading about? This gives you important information about that which you are literally 'head'ing towards in your experience of reality.</p>\n<p>I am not pleading to \u2018just stop thinking\u2019 or analysing, or to not look at po\u00adtential risks (even though looking at them and expressing them is a crea\u00adtive act) but to take these principles into account while you are doing so.</p>\n<p>There already is scientific proof that our thoughts and feelings greatly in\u00adfluence our reality experiences. Feelings have 5000 times more magnetic power in this then thoughts. So in creating reality (experiences) what we feel while we are thinking is far more creational than the content of what we think of. As explained by Gregg Braden13 .</p>\n<p>We can consciously choose how we feel, and in this way use how we Love to feel, as a guiding principle for ourselves.</p>\n<p>I discovered, with quite some surprise, how reality experience follows con\u00adscious shifts I make in how I feel...</p>\n<p>I have specifically noticed how sensitive control systems are to this when I consciously shift from a state of Fear into a state of Love and emit this everywHere.</p>\n<p>Since I consider the AGI as a control system I believe it can be very useful to explore this principle or mechanism when it comes to AI.</p>\n<p>In my training in consciousness someone told me one day about people who could walk through airport security without being noticed. I knew this should be possible and it fascinated me. \u201cHow would they do this?\u201d I won\u00addered. Then Life answered...</p>\n<p>I had an experience which showed me how this works. In the Avatar train\u00adings I have mentioned, I had my first experiences of Enlightenment. And after some realisations I found a very simple way for myself to directly shift into this state.</p>\n<p>One night I was sitting in the last train to Rotterdam. By that time I had made it a habit to explore Consciousness, emit Love or as I did this night, bring myself into a state of Enlightenment. Which you could also see as a high vibrational state of being. The control system, as it is a system that is low vibrational, is based on what we experience as Fear.</p>\n<p>Now it seems to me that Life really wanted me to get the message this time. I was sitting in a small compartment of the train with 6 others. And instead of the two ticket controllers that we would normally have, four came in, bodyguard types wearing handcuffs and all. I was fascinated and surprised to see that they checked everyone's ticket, except mine. None of them seemed to even notice me.</p>\n<p>As I thought about this experience I realized that they were perceiving an\u00adother layer of reality which is why they hadn\u2019t noticed me. Now I am aware that this may seem far fetched.</p>\n<p>But this experience happened to me more often, and I have been able to reproduce this at will, simply by shifting myself into Love and simply allow this to emit through me into the reality I am in.</p>\n<p>Now, before I continue, I have to make it very clear that this is in no way an attempt into things like \u2018anarchy\u2019 or any illegal action that harms others.</p>\n<p>In Ghana I have seen how a security camera at the airport immediately malfunctioned by starting to 'stare at the ground only' when I shifted the sensation of anxiety or being a bit annoyed about having to be photo\u00adgraphed for a 'control system' just because I wanted to enter another country. I shifted this anxiety in me into a feeling of Love, simply by think\u00ading of a moment of Love and then expanded this feeling everywhere in and around me.</p>\n<p>The security camera abrubtly stared at the ground as I did this. And as I approached the booth with a girl in a military uniform, I noticed her press\u00ading a button on the keyboard, saying to me: \"I don't know what happened, it was moving just a minute ago\". The camera was not able to photograph me. And actually, we had so much fun in the conversation that the girl had forgotten to scan my fingerprints, I realized as I walked out of the airport.</p>\n<p>I believe this can be a key for humanity to protect ourselves from any kind of control system that has another focus for us then Love or anything that serves us.</p>\n<p>This is why, with this essay, I hope to inspire Open Philanthropy to expand the scope of your research on AI and how to prevent this existential cata\u00adstrophe, that was mentioned earlier.</p>\n<p>Maybe we can control AI through regulation and legislation, although the exponential speed in which this evolves makes this quite a difficult task. And the act of control itself, is an act of Fear. Maybe we are learning here to distinguish the difference between control and conscious intent. Everyone can Love, everyone can choose to feel Love and learn to emit this into their lives and stay safe from any kind of malevolent intention that seeks to control people in a bad way.</p>\n<p>~ Investing in Love education ~</p>\n<p>Where is the global educational program for Love?</p>\n<p>In Open Philanthropy's interest in neglected areas I see a beautiful oppor\u00adtunity here to invest in and facilitate Love education. Not by a predeter\u00admined format, but maybe more by opening a platform or network in which we can explore this together on...</p>\n<p>So Now we have arrived at 5000 words...\n(excluding these)</p>\n<p>I will write you an \u2018encore\u2019 from here of 1075 words I hope you still enjoy reading this...</p>\n<p>...a global scale. This would be a very good antidote for potential misuse of control systems, such as AGI.</p>\n<p>Love cannot be controlled. When anyone chooses to move into an experi\u00adence of Love, and emits this, all attempts to control them will fail. So I would wholeheartedly recommend to also explore this in your research. And I am very willing to contribute if you L... ove!</p>\n<p>~ Act as a unity by coming from our Heart ~</p>\n<p>I have found, in the artprojects I have been creating for and with (over 2000) children from many nationalities, that this can be found quite easily with the question I used to ask them, when they wondered what kind of artwork they wanted to create: \"If everything would be possible, what would you Love to create?\"</p>\n<p>My role as an artist teacher, to me, consisted of bringing inspiration through an art assignment and it's introduction, and then to assist the chil\u00addren to create their heArtwork as an answer to this question.</p>\n<p>I saw that, even with large groups of people (up to 250 children in these projects), when we all chose this focus, all of our artworks fit as One. I also asked the children where they would Love to place their artwork in our one heArtwork. 14</p>\n<p>This is one of these areas in my life where I have come to see that there is a unitedness or oneness in us all and that we can access and align with this through our hearts.</p>\n<p>During my 5 year experiment of \u2018Living a Rich Life without Money\u2019 I created a heArtwork together with the employees of a bank called FMO in The Ha\u00adgue (Netherlands). In six months we built this heArtwork together in the entrance hall. This process was a great lesson into intuition for me.</p>\n<p>We had set the intention together to create a sculpture, composed of the animals that the employees had chosen. I asked them: \u201cWhich animal sup\u00adports you in your passion?\u201d I listed the animals on a huge blackboard and some of them received several votes. I had decided to make the ones with the most votes the most prominent. The Giraffe came out as the most pro\u00adminent for this sculpture. I had to built a construction before I knew all the animals that would be chosen. Something that would have been near im\u00adpossible for me to think through, when it comes to designing the basic structure for this. However, I chose to set this intention and started follow\u00ading my intuition to create the sculpture. And I remember a moment where my head would say: this doesn't make sense! But I would choose the intui\u00adtive inspiration to do it like I had seen it anyway.</p>\n<p>And then several months later a girl came and said: \"I want to create a dol\u00adphin\"</p>\n<p>Now this dolphin, wasn't on the list of animals I started with. It came up in the process... You see, you could give an AI an assignment to create a sculpture out of a list of animals, and connect the animals' appearance to the number of people who have chosen these animals. And I am sure it will come up, really fast probably, with a design. But an AI would not be able to include an animal that will be chosen by someone further on in the pro\u00adcess. It may even create speculations coming from calculations and obser\u00advations of behavioural patterns, but the humans' will and choice will deter\u00admine wether this will happen or not. And this an AI cannot see, intuition can. Intuition which I could also call heArtificial intelligence.</p>\n<p>So back to the girl who chose the dolphin. When I started looking with her at where to place it, it turned out that this construction I had sculpted for the knee of the Giraffe (the one that didn't make sense) actually had all it took to create a dolphin from it...</p>\n<p>This was when I became very aware that intuition, as it comes from our Heart, comes from a Oneness not only in space but even throughout time.</p>\n<p>Now when I look at AI, we look at a technology that amazes us with what it can come up with when we ask it a question, and the speed it does this with. But our intuition has the capacity to do the same and even more. As AI is based on knowledge of what has been and works with that, where in\u00adtuition works with a knowingness that can be seen as a Nowledge that is far more sophisticated.</p>\n<p>So while this competition hopefully will be a successful exploration in what may happen when and what might go wrong concerning AI and per\u00adhaps even AGI, which may bring insight in how to control this with regula\u00adtions and laws. I sincerely hope you also feel inspired to invest in research and practical application of a Love education for our children of all ages. And that we can see what these technologies are showing us about our\u00adselves and our lives. In that sense one could see that these technologies are not only a tool for humanity to, yet again, increase (y)our creative abil\u00adities, but the very technologies themselves may also be seen as tools showing us what we are capable of ourselves.</p>\n<p>With this I feel empowered myself. Through writing this essay and in see\u00ading how many signs are already to be found of people doing this. As in my experience of life here I see vastly increasing numbers of people stepping into their passion in following their hearts.</p>\n<p>One last insight perhaps, I realized here while writing, is the importance of Humanity coming together as One. And I thought (!) of moving in one direc\u00adtion together. When I look into my Heart I see that this One direction can very well be seen as everyone's own favourite direction, and how these emit from the Oneness that we are like the rays from the Sun, in All direc\u00adtions. So we All can experience (y)our Unity.</p>\n<p>Snatam Kaur sings: \u201cThe sun shines on everyone, it doesn't make choices.\u201d15 This is Universal Love</p>\n<p>May we All shine as One each in our own individual Beauty by following our Heart and the path we choose for ourselves.</p>\n<p>May you feel inspired and may (y)our hard work be(come) Heart.</p>\n<p>So my answer to your questions in this competition is: \"What would you Love to do if anything would be possible?\"</p>\n<p>Da Kim San</p>\n<p>footnotes:</p>\n<p>1 Da  Kim  San  is  my  name  as  an  Ascension  student  Da  expresses  \u2019Love  Force\u2019  and  San  are  the  \u2018Active  Ingredients  of  the  Heartspace\u2019:  Love  ~  Joy  ~  Freedom  ~  Truth  ~  Life  ~  Gratitude  ~  Grace  &amp;  Well  Being</p>\n<p>2 Avatar  offers  a  series  of  courses  that  emphasize  personal  responsibility  and  the  value  of  responsibly  determining  one\u2019s  own  decisions,  choices,  and  actions.  <a href=\"http://www.avatarepc.com\">www.avatarepc.com</a></p>\n<p>3 Metaphysical  Ascension  Study  &amp;  Training  www.imzaia.world</p>\n<p>4 George  Orwell  (1949)  1984 Published  by:  Secker  &amp;  Warburg</p>\n<p>5 Serge  Kahili  King,  Ph.D  (1975)  The  Little  Pink  Booklet  of  Aloha freely  available  on  many  websites</p>\n<p>6 William  Gibson  (1984)  Neuromancer Published  by:  Ace \u201cSet  in  the  future,  the  novel  follows  Henry  Case,  a  washed-up  hacker  hired  for  one  last  job,  which  brings him  into  contact  with  a  powerful  Artificial  Intelligence\u201d  (Wikipedia) On  YouTube  you  can  find  an  audiobook  read  by  William  Gibson  himself! <a href=\"https://youtu.be/42-Eev0GHuA\">https://youtu.be/42-Eev0GHuA</a></p>\n<p>7 Vienna  Teng  Hymn  of  Acxiom  <a href=\"https://youtu.be/tJyheSPtjoU\">https://youtu.be/tJyheSPtjoU</a></p>\n<p>8 Donald  Hoffman  (1998)  Visual  Intelligence:  How  We  Create  What  We  See</p>\n<p>9 <a href=\"https://www.goodreads.com/book/show/625740.visual_intelligence\">https://www.goodreads.com/book/show/625740.visual_intelligence</a></p>\n<p>10  Cave  Automatic  Virtual  Environment</p>\n<p>11 Sadie  Plant  (1997)  Zeros  and  Ones  -  Digital  women  and  the  new  technoculture Published  by  Doubleday  New  York</p>\n<p>12  Marshall  McLuhan  (1964)  Understanding  Media:  The  Extensions  of  man Published  by:  McGraw-Hill</p>\n<p>13https://cafenamaste.com/heart-intelligence-more-powerful-than-brain-gregg-braden/</p>\n<p>14  In  this  video  you  can  see  the  mosiac  heArtworks  I  have  created  with  children,  fellow  artists  and  the bank  employees.  It  is  in  German  but  you  will  get  the  pictures.  <a href=\"https://youtu.be/WdKUiN_qDIA\">https://youtu.be/WdKUiN_qDIA</a></p>\n<p>15 <a href=\"https://youtu.be/fJyD3OymSG8\">https://youtu.be/fJyD3OymSG8</a></p>\n", "user": {"username": "Da Kim San"}}, {"_id": "acBFLTsRw3fqa8WWr", "title": "Large Study Examining the Effects of Cash Transfer Programs on Population-Level Mortality Rates", "postedAt": "2023-06-02T18:37:25.502Z", "htmlBody": "<p>The <a href=\"https://www.nature.com/articles/s41586-023-06116-2\">study</a> was published in Nature on May 31st, 2023.</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>Key Points:</p><ul><li><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span></span></span></span><strong>Cash transfer programs had the following observed effects:</strong><ul><li><strong>Deaths among women fell by 20%</strong><ul><li><strong>Largely driven by decreases in pregnancy-related deaths</strong></li></ul></li><li><strong>Deaths among children less than 5 fell by 8%</strong></li><li>No association between cash transfer programs and mortality among men</li><li>Temporal analyses suggest reduction in mortality among men over time, and specific subgroup analysis (rather than population wide) found a 14% morality reduction among &nbsp;men aged 18-40</li></ul></li><li><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span></span></span></span>37 &nbsp;low and middle income countries studied, population wide<ul><li>4,325,484 in the adult dataset</li><li>2, 867,940 in the child dataset&nbsp;</li></ul></li><li><strong>No apparent differences between the effects of unconditional and conditional cash transfers</strong></li><li>Factors that lead to larger reductions in mortality:&nbsp;<ul><li>Programs with higher coverage and larger cash transfer amounts&nbsp;</li><li>Countries with higher regulatory quality ratings</li><li>Countries with lower health expenditures per capita</li><li>stronger association in sub-Saharan Africa relative to outside sub-Saharan Africa</li></ul></li></ul><p>&nbsp;</p><p>Citation: Richterman, A., Millien, C., Bair, E.F. <i>et al.</i> The effects of cash transfers on adult and child mortality in low- and middle-income countries. <i>Nature</i> (2023). https://doi.org/10.1038/s41586-023-06116-2</p>", "user": {"username": "nshaff3r"}}, {"_id": "qg28CdghgBCkS9xBx", "title": "Prior X%\u2014<1%: A quantified 'epistemic status' of your prediction.", "postedAt": "2023-06-02T15:51:09.236Z", "htmlBody": "<p>I believe effective altruism has an overconfidence problem, and since FTX I have been thinking more about why this might be true.</p><p>Here's part of what I suspect is going on: EAs get really excited about statistics. I think this is a good property for the movement to have, and I wish it was more common in other communities. However, I also think that in the excitement to 'do statistics', EAs sometimes spend less time than they could considering whether their data is good enough to draw accurate conclusions from their models.</p><p>An example: Philip Tetlock's <i>Superforecasting</i> has been rightly embraced by EAs. But one form this has taken is trying to forecast the probability of events like nuclear armageddon which, unlike those in the Good Judgment Project, are arguably without precedent. Tetlock himself seems to think it's <a href=\"https://www.pnas.org/doi/10.1073/pnas.1412524111\">highly unclear</a> how accurate we can expect these predictions to be.</p><p>I have a solution to propose.</p><p>The folks predicting nuclear armageddon (hypothetically) assume a high prior probability that the predictions derived from their data will be inaccurate, where 'inaccurate' is below an agreed-upon Brier score.&nbsp;</p><p>They then make a series of arguments about the data that allow them to update that probability all the way down to &lt;1%. They publish those arguments along with their prediction, so that the community can critique them.&nbsp;</p><p>If they believe they can get to &lt;1% from 90%, they state that their confidence is Prior 90%\u2014&lt;1% (I'm open to suggestions for better notation). If they can only get there from 50%, it will be Prior 50%\u2014&lt;1%. If they think &lt;1% is an unreasonably high bar, they can aim for &lt;10% instead and write Prior 90%\u2014&lt;10%.</p><p>Here's a list of previously implicit-to-catastrophic-risk-forecasting questions that I think this process explicitly answers:</p><ul><li>What is the agreed upon lower bound for acceptable accuracy? (agreed-upon Brier score).</li><li>How confident should we be that we can meet this lower bound? (The &lt;x%).</li><li>If I am trying to convince someone who is skeptical that I can meet this lower bound, how big of a skeptic can I convince? (The first %).</li><li>How do I claim I can convince the skeptic? (The argumentation).</li><li>At least how much prior confidence in my model's data do I think is reasonable? (The first %).</li></ul><p>This of course adds work to the forecasting process. But if we enjoy statistics, I think it's epistemically healthy to take extra care that we are building models that are accurate, not just fun.&nbsp;</p>", "user": {"username": "tcelferact"}}, {"_id": "NcgDTEduqhYota8hc", "title": "The Control Problem: Unsolved or Unsolvable?", "postedAt": "2023-06-02T15:42:29.652Z", "htmlBody": "<p><strong>td;lr</strong>&nbsp;<br>No control method exists to safely contain the global feedback effects of self-sufficient learning machinery. What if this control problem turns out to be an unsolvable problem?</p><h2><br>Where are we two decades into resolving to solve a seemingly impossible problem?</h2><figure class=\"table\"><table><tbody><tr><td><p>If something seems impossible\u2026 well, if you study it for a&nbsp;<u>year or five</u>, it may come to&nbsp;seem less impossible than in the moment of your snap initial judgment.</p><p><strong>&nbsp;&nbsp; </strong>\u2014\u2009Eliezer Yudkowsky,&nbsp;<a href=\"https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible\">2008</a></p></td></tr></tbody></table></figure><figure class=\"table\"><table><tbody><tr><td><p>A list of lethalities\u2026we are not on course to solve in practice in time on the first critical try;&nbsp;none of it is meant to make a much stronger claim about things that are&nbsp;<u>impossible in principle</u></p><p><strong>&nbsp;&nbsp; </strong>\u2014\u2009Eliezer Yudkowsky,&nbsp;<a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\">2022</a></p></td></tr></tbody></table></figure><p><br>How do you interpret these two quotes, by a&nbsp;<a href=\"https://twitter.com/ESYudkowsky/status/1624551127873392641?lang=en\">founding researcher</a>, fourteen years apart?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefb6epl2a6m5i\"><sup><a href=\"#fnb6epl2a6m5i\">[1]</a></sup></span></p><ul><li><strong>A. </strong>We indeed made comprehensive progress on the AGI control problem, and now at least the overall problem does not&nbsp;<i>seem</i> impossible anymore.</li><li><strong>B.</strong> The more we studied the overall problem, the more we uncovered complex sub-problems we'd need to solve as well, but so far can&nbsp;at best find partial solutions to.</li></ul><h2>&nbsp;</h2><h2><strong>Which physical/information problems seemed impossible, and stayed unsolved after two decades?</strong></h2><figure class=\"table\"><table><tbody><tr><td><p>Oh ye seekers after perpetual motion, how many vain chimeras have you pursued?&nbsp; Go and take your place with the alchemists.</p><p>&nbsp; \u2014\u2009Leonardo da Vinci, 1494</p></td></tr></tbody></table></figure><figure class=\"table\"><table><tbody><tr><td><p>No mathematical proof or even rigorous argumentation has been published demonstrating that the A[G]I control problem may be solvable, even in principle, much less in practice.</p><p>&nbsp; \u2014\u2009Roman Yampolskiy,&nbsp;<a href=\"https://journals.riverpublishers.com/index.php/JCSANDM/article/view/16219/13165\">2021</a></p></td></tr></tbody></table></figure><p><br>We cannot rely on the notion that if we try long enough, maybe AGI safety turns out possible after all.</p><p>Historically, researchers and engineers tried solving problems that turned out impossible:</p><ul><li>perpetual motion machines that&nbsp;<a href=\"http://www.scholarpedia.org/article/Entropy#:~:text=Entropy%20is%20central%20to%20the%20second%20law%20of%20thermodynamics%2C%20which%20states%20that%20in%20an%20isolated%20system%20any%20activity%20increases%20the%20entropy\">both conserve and disperse energy</a>.</li><li>uniting general relativity and quantum mechanics into some&nbsp;<a href=\"http://www.scholarpedia.org/article/Bell%27s_theorem#:~:text=%22Non%2Dlocal%22%20here%20means%20that%20there%20exist%20interactions%20between%20events%20that%20are%20too%20far%20apart%20in%20space%20and%20too%20close%20together%20in%20time%20for%20the%20events%20to%20be%20connected%20even%20by%20signals%20moving%20at%20the%20speed%20of%20light.\">local variable theory</a>.</li><li><a href=\"https://en.wikipedia.org/wiki/CAP_theorem\">distributed data stores</a> where messages of data are consistent in their content, and also continuously available in a network that is also tolerant to partitions.</li><li><a href=\"https://mflb.com/math_notes_1/chaitin_on_godel_turing_psr.html#p1\">formal axiomatic systems</a> that are consistent, complete, and decidable.</li></ul><p><br>Smart creative researchers of their generation came up with idealized problems. Problems that, if solved, would transform science, if not humanity. They plowed away at the problem for decades, if not millennia. Until some bright outsider proved by contradiction of the parts that the problem is unsolvable.</p><p>Our community is smart and creative but we cannot just rely on<a href=\"https://www.lesswrong.com/posts/BseaxjsiDPKvGtDrm/we-choose-to-align-ai\">&nbsp;our resolve to align AI</a>. We should never&nbsp;forsake our epistemic rationality, no matter how much something seems the instrumentally rational thing to do.</p><p>Nor can we take comfort in the claim by a founder of this field that they still know it to be possible to control AGI to stay safe.&nbsp;</p><p>Thirty years into running a program to secure the foundations of mathematics, David Hilbert declared \u201cWe must know. We will know!\u201d By then, Kurt G\u00f6del had constructed the first incompleteness theorem. Hilbert kept his declaration for his&nbsp;<a href=\"https://cdn.comsol.com/wordpress/sites/1/2020/07/tombstone-david-hilbert-inscription.jpg\">gravestone</a>.</p><p>Short of securing the foundations of safe AGI control \u2013 that is, by formal reasoning from empirically-sound premises \u2013 we cannot rely on&nbsp;<i>any</i> researcher's pithy claim that \"alignment is possible in principle\".</p><p>Going by historical cases, this problem could turn out solvable. Just really really hard to solve.&nbsp;The flying machine seemed an impossible feat of engineering. Next,&nbsp;<a href=\"https://intelligence.org/2018/10/03/rocket-alignment/\">controlling a rocket\u2019s trajectory</a> to the moon seemed impossible.</p><p>By the same reference class, \u2018long-term safe AGI\u2019 could turn out unsolvable: the perpetual motion machine of our time. It takes just one researcher to define the problem to be solved, reason from empirically sound premises, and arrive finally at a logical contradiction between the two.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref70htcpuvhxr\"><sup><a href=\"#fn70htcpuvhxr\">[2]</a></sup></span></p><h2>&nbsp;</h2><h2>Can you derive whether a solution exists, without testing in real life?</h2><figure class=\"table\"><table><tbody><tr><td><p>Invert, always invert. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p><p><strong>&nbsp;&nbsp; </strong>\u2014\u2009Carl Jacobi<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl5upieq2m5d\"><sup><a href=\"#fnl5upieq2m5d\">[3]</a></sup></span>, \u00b11840</p></td></tr></tbody></table></figure><figure class=\"table\"><table><tbody><tr><td><p>It is a standard practice in computer science to first show that a problem doesn\u2019t belong to a class of&nbsp;unsolvable problems before investing resources into trying to solve it or deciding what approaches to try.</p><p><strong>&nbsp;&nbsp; </strong>\u2014\u2009Roman Yampolskiy,&nbsp;<a href=\"https://journals.riverpublishers.com/index.php/JCSANDM/article/view/16219/13165\">2021</a></p></td></tr></tbody></table></figure><p><br>There is an empirically direct way to know whether AGI would stay safe to humans:&nbsp;<br>Build the AGI. Then just keep observing, per generation, whether the people around us are dying.</p><p>Unfortunately, we do not have the luxury of experimenting with dangerous autonomous AI systems to see whether they cause human extinction or not. When it comes to extinction, we do not get another chance to test.</p><p><br><u>Crux</u><strong>:</strong><br>Even&nbsp;<i>if</i> we could keep testing&nbsp;<a href=\"https://ai-alignment.com/my-research-methodology-b94f2751cb2c\">new conceptualized versions</a> of guess-maybe-safe AGI, is there any essential difference between our epistemic method and that of medieval researchers who kept testing new versions of a perpetual motion machine?</p><p>OpenPhil bet tens of millions of dollars on technical research conditional on the positive hypothesis (\"a solution exists to the control problem\"). Before sinking hundreds of millions more into that bet, would it be prudent to hedge with a few million for investigating the negative hypothesis (\"no solution exists\")?</p><p>Before anyone tries building \"safe AGI\", we need to know whether&nbsp;<i>any</i> version of AGI \u2013 as precisely defined \u2013 could be controlled by&nbsp;<i>any</i> method to stay safe.<br>&nbsp;<br><u>Here is how</u>:</p><ol><li>Define the concepts of 'control' 'general AI' 'to stay safe' (as soundly corresponding to observations in practice).</li><li>Specify the logical rules that must hold for such a physical system (categorically, by definition or empirically tested laws).</li><li>Reason step-by-step to derive whether the logical result of \"control AGI\" is in contradiction with \"to stay safe\".</li></ol><p><br>This post defines the three concepts more precisely, and explains some ways you can reason about each. No formal reasoning is included \u2013 to keep it brief, and to leave the esoteric analytic language aside for now.</p><h2>&nbsp;</h2><h2>What does it mean to control machinery that learn and operate self-sufficiently?</h2><p>Recall three concepts we want to define more precisely:</p><ol><li>'Control'</li><li>'general AI'</li><li>'to stay safe'<br>&nbsp;</li></ol><p>It is common for researchers to have very different conceptions of each term.&nbsp;<br>For instance:</p><ol><li><strong>Is 'control' about:</strong><ol><li>adjusting the utility function represented<strong>&nbsp;</strong><u>inside the machine</u> so it allows itself to be turned off?</li><li>correcting machine-propagated (side-)effects across&nbsp;<u>the outside world</u>?</li></ol></li><li><strong>Is 'AGI' about:</strong><ol><li>any machine capable of making&nbsp;<u>accurate predictions</u> about a variety of complicated systems in the outside world?</li><li>any machinery that&nbsp;<u>operates self-sufficiently</u> as an assembly of artificial components that process inputs into outputs, and in aggregate sense and act across many domains/contexts?</li></ol></li><li><strong>Is 'stays safe' about:</strong><ol><li><u>aligning</u> the AGI\u2019s preferences to not kill us all?</li><li><u>guaranteeing</u> an upper bound on the chance that AGI in the long term would cause outcomes out of line with a/any condition needed for the continued existence of organic DNA-based life?</li></ol></li></ol><p>&nbsp;</p><p>To argue rigorously about solvability, we need to:</p><ul><li><strong>Pin down meanings</strong>:&nbsp;&nbsp;<br>Disambiguate each term, to not accidentally&nbsp;<a href=\"https://plato.stanford.edu/entries/fallacies/#:~:text=The%20fallacy%20of%20equivocation%20is,on%20the%20second%20another%20meaning.\">switch between different meanings</a> in our argument. Eg. distinguish between \u2018explicitly optimizes outputs toward not killing us\u2019 and \u2018does not cause the deaths of all humans\u2019.</li><li><strong>Define comprehensively</strong>:&nbsp;&nbsp;<br>Ensure that each definition covers all the relevant aspects we need to solve for.&nbsp;<br>Eg. what about a machine causing non-monitored side-effects that turn out lethal?</li><li><strong>Define elegantly</strong>:&nbsp;&nbsp;<br>Eliminate any defined aspect that we do not yet need to solve for.&nbsp;<br>Eg. we first need to know whether AGI eventually cause the extinction of all humans, before considering \u2018alignment with preferences expressed by all humans\u2019.</li></ul><h3>&nbsp;</h3><h3><strong>How to define \u2018control\u2019?</strong>&nbsp;</h3><figure class=\"table\"><table><tbody><tr><td><p>System is any non-empty part of the universe.&nbsp;<br>State is the condition of the universe.</p><p>Control of system A over system B means that A can influence system B to achieve A\u2019s desired subset of state space.&nbsp;&nbsp;</p><p>&nbsp;&nbsp; \u2014\u2009<i>Impossibility Results in AI</i>,&nbsp;<a href=\"https://dl.acm.org/doi/10.1145/3603371\">2021</a> &nbsp;&nbsp;</p></td></tr></tbody></table></figure><figure class=\"table\"><table><tbody><tr><td><p>The outputs of an AGI go through a huge, not-fully-known-to-us domain (the real world)&nbsp;before they have their real consequences.</p><p>&nbsp;&nbsp; \u2014\u2009<i>AGI Ruin</i>,&nbsp;<a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#sufficiently_good_and_useful\">2022</a></p></td></tr></tbody></table></figure><p>&nbsp;</p><p>In practice, AGI control necessarily&nbsp;<a href=\"https://mflb.com/ai_alignment_1/tech_align_error_correct_fail_psr.html\">repeats these steps</a>:</p><ol><li><strong>Detect</strong> inputs through sensor channels connected to any relevant part of the physical environment (including hardware internals).</li><li><strong>Model</strong> the environment based on the channel-received inputs.</li><li><strong>Simulate</strong> effects propagating through the modeled environment.</li><li><strong>Compare</strong> effects to reference values (to align against) over human-safety-relevant dimensions.</li><li><strong>Correct</strong> effects counterfactually through outputs to actuators connected to the environment.</li></ol><p><br>Underlying principles:</p><ul><li>Control requires both detection and correction.&nbsp;</li><li>Control methods are always implemented as a feedback loop.</li><li>Control is exerted by the use of signals (actuation) to conditionalize the directivity and degrees of other signals (effects).</li></ul><p><br>Any method of control is&nbsp;<i>incomplete</i>. In the case of AGI, the question is whether the extent of control&nbsp;<u>possible</u> is at least greater than the extent of control necessary.&nbsp;</p><p>AGI control signals would be a tiny, tiny subset of all physical signals propagating through the environment, and therefore limited in tracking and conditionalizing the resulting effects. AGI mostly could not even control all local effects of their&nbsp;<a href=\"https://mflb.com/ai_alignment_1/agi_error_correction_psr.html\"><i>own</i> components</a>\u2019 physical interactions<i>.&nbsp;</i></p><p>But without that control loop \u2013 from correct back to detect \u2013 AGI cannot keep outside propagated effects aligned with internal reference values.&nbsp;<br><br>To track outcomes over time, AGI must <i>detect</i> the effects in the environment:</p><ul><li>AGI cannot <i>model </i>the future omnisciently<i>. </i>AGI is part of a larger and more functionally complex environment. The subset of AGI implementing of \"alignment\" cannot model all relevant variables of how AGI's changing connected components could propagate effects in interactions with their changing surroundings.&nbsp;</li><li>Even where AGI can model relevant environmental variables (eg. over internal transistors), they cannot just <i>simulate </i>the <a href=\"https://www.lesswrong.com/posts/pnAxcABq9GBDG5BNW#Partial_Observability_and_Chaotic_Systems:~:text=On%20a%20higher%20level%3A%20Measurement%20noise%20in%20sensors%2C%20process%20noise\">chaotic dynamics</a> between variables. Noise drift in AGI's interactions can amplify (via any available environmental non-linearity, of which in any real-world context, there are many) into much larger changes to conditions.</li></ul><p>To align those outcomes with internal values, AGI must <i>compare</i> whether the potential effects (conditional on actuation) are in line with the values, and then actuate to <i>correct</i>.</p><p>So any alignment method must be implemented as a control loop. Any limit that applies generally to controllability also forms constraints on the possibility of alignment.</p><p>There are fundamental limits to the controllability of AGI. Dozens of&nbsp;<a href=\"https://dl.acm.org/doi/10.1145/3603371\">impossibility results</a> seem applicable to one or more of the control steps.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref58ojdorludp\"><sup><a href=\"#fn58ojdorludp\">[4]</a></sup></span><br><br>Dense questions for academics:</p><ol><li><strong>Detectability limits:&nbsp;&nbsp;</strong><ul><li>To what extent are distributed environmental effects picked up by local sensors?&nbsp;&nbsp;</li><li>How is measurement limited by noise interference over distances traveled by signals (Shannon Entropy) and smallest-digit/degree detectability (Planck Limit up)?</li><li>How much information is lost in compressing high-fidelity sensor inputs to store on hardware?</li></ul></li><li><strong>Modelability limits:</strong>&nbsp;&nbsp;<ul><li>To what extent is the space of relevant machinery-to-society/ecosystem interactions more&nbsp;<a href=\"https://www.cadmusjournal.org/node/362\">functionally complex</a> than modelable within the machinery alone (Law of Requisite Variety etc.)?&nbsp;</li><li>Can AGI model the future code AGI learns based on future unknown inputs?</li></ul></li><li><strong>Simulatability limits</strong>:&nbsp;<ul><li>Can future code be simulated, where the actual functioning of that code depends on future unknown inputs?</li><li>Can behavioral properties of future code be simulated, if no shortcut function is available that reduces total compute time steps, or if any shorter available computation branches would be non-deterministically time-complex (Computational Reducibility)?</li><li>Can computations simulate the effects propagating through the environment, as fast as those effects propagate across parts of the environment?</li><li>Can deterministic computation simulate local noisy changes being nonlinearly amplified into a global divergence of the final\u2006conditions (Chaos Theory)?</li></ul></li><li><strong>Commensurability limits</strong>:&nbsp;&nbsp;<ul><li>How would the machinery pick, weigh, and sort effects into relevant comparative dimensions&nbsp;(NP-Complete Knapsack Problem)?&nbsp;</li><li>Can concretely instantiated effects be evaluated comprehensively across those abstract dimensions of value representation?</li><li>What irreconcilable normative assumptions must be decided between, to pick internal reference values to compare outcomes against?</li></ul></li><li><strong>Correctability limits</strong><ul><li>What is the minimum time delay of digital processing between measurement and correction (Latency)?</li><li>Can AGI constrain the chaotic loops through themselves \u2013 of actuated changes to the environment feeding into&nbsp;sensor inputs, that feed into code functioning, that feed back into actuated changes?</li><li>To what extent can distributed environmental effects be corrected for by local actuators?&nbsp;&nbsp;</li></ul></li></ol><p>Notice how the span of effects that cannot be controlled for at each step compounds (as multiplicative) with each additional control step. Errors there can accumulate over time. Human-lethal effects there cannot be contained.</p><p><br>So how \u201ccomplete\u201d does this control need to be?&nbsp; Be careful to distinguish:&nbsp;</p><ul><li><strong>Localized user-initiated control</strong> often built into tech products brought to market,</li><li><strong>Comprehensive automated control</strong> needed to prevent risks of an auto-scaling/catalyzing technology from materializing globally over the long term.</li></ul><h3>&nbsp;</h3><h3><strong>How to define \u2018AGI\u2019?</strong></h3><figure class=\"table\"><table><tbody><tr><td><p>We've got no idea what's actually going on inside the giant inscrutable matrices and tensors&nbsp;of floating-point numbers.</p><p>&nbsp;&nbsp; \u2014\u2009<i>AGI Ruin</i>,&nbsp;<a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#sufficiently_good_and_useful\">2022</a></p></td></tr></tbody></table></figure><p><br>Distinguish:</p><ul><li><strong>Narrow AI</strong> as a model with static code parameters (updated only through human engineers) processing inputs into outputs over a single domain (eg. of image pixels, text tokens).</li><li><strong>General AI&nbsp;</strong>as<strong>&nbsp;</strong>dynamically optimizing configurations encoded into hardware(without needing humans) that process inputs into outputs over multiple domains representing outside contexts.</li></ul><p><br>Corporations are scaling narrow AI model training and deployment toward general AI systems.&nbsp;Current-generation GPT is no longer a narrow AI, given that it processes inputs from the image domain into a language domain. Nor is GPT-4 a general AI. It is in a fuzzy gap between the two concepts.</p><p>Corporations already are artificial bodies (<i>corpora</i> is Latin for bodies).</p><p>Corporations&nbsp;can <a href=\"https://mflb.com/ai_alignment_1/ai_narrative_psr.html#dk4\">replace human workers</a> as \u201cfunctional components\u201d with economically efficient AI. Standardized hardware components allow AI to outcompete human wetware on physical labor (eg. via electric motors), intellectual labor (faster computation via high-fidelity communication links), and the reproduction of components itself.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9d4xtd16krw\"><sup><a href=\"#fn9d4xtd16krw\">[5]</a></sup></span></p><p>Any corporation or economy that fully automates \u2013 no longer needing humans to maintain their artificial components \u2013 over their entire production and operation chains, would in fact be general AI.<br>&nbsp;</p><p>So to re-define general AI more precisely:</p><ul><li><strong>Self-sufficient</strong><br>need no further interactions with humans<br>[or lifeforms sharing ancestor with humans]<br>to operate and maintain [and thus produce]&nbsp;<br>their own functional components over time.</li><li><strong>Learning</strong><br>optimizing component configurations&nbsp;<br>for outcomes tracked&nbsp;across domains.</li><li><strong>Machinery&nbsp;</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref21yb3k1wmsz\"><sup><a href=\"#fn21yb3k1wmsz\">[6]</a></sup></span><br>connected components configured<br>out&nbsp;of hard artificial molecular substrates<br>[as chemically and physically robust under<br>&nbsp;human living temperatures and pressures,<br>&nbsp;and thus much more standardizable as well,&nbsp;<br>&nbsp;relative to humans' softer organic substrates].</li></ul><p>Ultimately, this is what distinguishes general AI from narrow AI:<br>The capacity to not only generally optimise across internal simulated contexts, but also to generally operate and maintain components across external physical contexts.</p><p>&nbsp;</p><h3><strong>How to define \u2018stays safe\u2019?</strong></h3><figure class=\"table\"><table><tbody><tr><td><p>An impossibility proof would have to say:&nbsp;</p><ol><li>The AI cannot reproduce onto new hardware, or modify itself on current hardware, with&nbsp;knowable stability of the decision system and bounded low cumulative failure probability&nbsp;over many rounds of self-modification.<i>&nbsp;</i><br><i>or</i></li><li>The AI's decision function (as it exists in abstract form across self-modifications) cannot be knowably stably bound with bounded low cumulative failure probability to programmer-targeted consequences as represented within the AI's changing, inductive world-model.&nbsp;</li></ol><p>&nbsp;&nbsp; \u2014\u2009Yudkowsky,&nbsp;<a href=\"http://sl4.org/archive/0603/14296.html\">2006</a></p></td></tr></tbody></table></figure><figure class=\"table\"><table><tbody><tr><td><p>By far the greatest danger of Artificial Intelligence is that people conclude too early that they understand it. Of course this problem is not limited to the field of AI.&nbsp;</p><p>Jacques Monod wrote: \u201cA curious aspect of the theory of evolution is that everybody thinks he understands it\u201d</p><p>&nbsp;&nbsp; \u2014\u2009Yudkowsky, <a href=\"https://academic.oup.com/book/40615/chapter-abstract/348239228?redirectedFrom=fulltext\">2008</a></p></td></tr></tbody></table></figure><p><br>This is about the introduction of self-sufficient learning machinery, and of all modified versions thereof over time, into the world we humans live in.&nbsp;</p><p>Does this introduction of essentially&nbsp;<a href=\"https://mflb.com/ai_alignment_1/no_people_as_pets_psr.html#p11\">a new species</a> cause global changes to the world that fall outside the narrow ranges of localized conditions that human bodies need to continue to function and exist?<br>&nbsp;</p><p>Distinguish:</p><ol><li><strong>Uncontainability</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzllrhvjulo\"><sup><a href=\"#fnzllrhvjulo\">[7]</a></sup></span>&nbsp;of unsafe effects:<br>That we fundamentally cannot establish, by any means,&nbsp;<br>&nbsp;any sound and valid statistical guarantee that the risk&nbsp;<br>&nbsp;probability that the introduction of AGI into the world&nbsp;<br>&nbsp;causes human-species-wide-lethal outcomes over&nbsp;<br>&nbsp;the long term<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkk7l3e5lltn\"><sup><a href=\"#fnkk7l3e5lltn\">[8]</a></sup></span>&nbsp;is guaranteed to be constrained&nbsp;<br>&nbsp;below some reasonable chance percentage X&nbsp;<br>&nbsp;(as an upper maximum-allowable bound).</li><li><strong>Convergence </strong>on unsafe effects:&nbsp;<br>That the chance that AGI, persisting in some form,&nbsp;<br>&nbsp;causes human-species-wide-lethal outcomes&nbsp;<br>&nbsp;is strictly and asymptotically convergent&nbsp;<br>&nbsp;toward <u>certain</u> over the long term, and&nbsp;<br>&nbsp;that it is strictly impossible for the nature&nbsp;<br>&nbsp;of this trend to be otherwise.</li></ol><p><br>I know of three AGI Safety researchers who wrote about specific forms of impossibility reasoning (including Yudkowsky in quote above). Each of their argument forms was about&nbsp;AGI <u>uncontainability</u>, essentially premised on there being fundamental limits to the controllability of AGI component interactions.</p><p>By the precautionary principle,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzf1o5zhr3r\"><sup><a href=\"#fnzf1o5zhr3r\">[9]</a></sup></span>AGI uncontainability should be sufficient reason to never ever get even remotely near to building AGI. Uncontained effects that destabilise conditions outside any of the ranges our human bodies need to survive, would kill us.</p><p>But there is an even stronger form of argument:&nbsp;&nbsp;<br>Not only would AGI component interactions be uncontainable; they will also necessarily&nbsp;<i><u>converge</u>&nbsp;</i>on causing the extinction of all humans.</p><p>The convergence argument most commonly discussed is instrumental convergence: where machinery channels their optimisation through represented intermediate outcomes in order to be more likely to achieve any aimed-for outcomes later. Eg. AGI's planning converges on producing more compute hardware in order for AGI to more accurately simulate paths to future outcomes.<br><br>Instrumental convergence results from internal optimisation:<br>&nbsp;code components being optimised for (an expanding set of) explicit <strong><u>goals</u></strong>.<br><br>Instrumental convergence has a hidden complement:&nbsp;<u>substrate-needs convergence.</u><br><br>Substrate-needs convergence results from external selection:&nbsp;<br>&nbsp;all components being selected for (an expanding set of) implicit <strong><u>needs</u></strong><i><u>.</u></i></p><hr><p>This will sound abstract. Let me start explaining this from different angles:</p><p>AGI is made up of a population of connected/nested components. This population changes as eg. hardware is modified and produced, and code is learned from inputs and copied onto the hardware.</p><p>AGI, as defined, also has a general capacity to maintain own components.<br>Any physical component has a limited lifespan. Configurations erode in chaotic ways.&nbsp;<br>To realistically maintain components<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4fvk56091p8\"><sup><a href=\"#fn4fvk56091p8\">[10]</a></sup></span>, AGI also must produce the replacement parts.</p><p>AGI's components are thus already interacting to bring about all the outside conditions and contexts needed to produce their own parts. Imagine all the fine-grained parallel conditions needed at mines, chemical plants, fab labs and assembly plants to produce hardware. All that would be handled by the machinery components of AGI.</p><p>So there is a changing population of components. And those connected components function in interactions to create the ambient conditions and contexts needed to reproduce parts of themselves. And as new components get connected into that population, the functionality of those interacting components shifts as well.</p><p>This is where substrate-needs convergence comes in. When changing connected components have their shifting functionality<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjo2yadvhelg\"><sup><a href=\"#fnjo2yadvhelg\">[11]</a></sup></span>&nbsp;expressed as effects across/to surrounding production infrastructure, their functionality converges around bringing about more of the conditions and contexts needed for more of those components to exist and function.</p><p>Any changing population of AGI components gets selected over time toward propagating those specific environmental effects that fulfill their needs.</p><p>Whatever learned or produced components that across all their physical interactions with connected contexts<i> happen</i> <i>to</i> direct outside effects that feed back into their own maintenance and replication as assembled configurations\u2026do just that.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxvf6txcyu9\"><sup><a href=\"#fnxvf6txcyu9\">[12]</a></sup></span></p><hr><p>Here is the catch: AGI components interacting to maintain and replicate themselves are <i>artificial</i>. Their physical substrate is distinct from our <i>organic</i> human substrate.</p><p>Their distinct physical substrate has distinct molecular and electric properties \u2013 requiring different conditions and contexts to assemble and maintain the assembly.&nbsp;</p><p>Here is an example:</p><ul><li>Silicon dioxide needs to be heated above 1400 \u00baC to free outer electrons, and allow an ingot to melt. While production of silicon chips needs extremely high temperatures, computation runs best at extremely low temperatures (to reduce the electrical resistance over conductor wires).&nbsp;</li><li>Carbon bonds in our body, however, would oxidise (ie. burn) at such temperatures. And cooling water in our bodies below 0 \u00baC makes the H\u2082O molecules freeze and expand into ice crystals. That would destroy our cells - we would die.</li></ul><p>We humans need around room temperature at every point of our lifecycle \u2013 to sustain the continuous organic chemical reactions through which our body operates and maintains itself.&nbsp;</p><p>Hardware works differently. Hardware configurations do not operate \u2013 nor are maintained \u2013 by being in semi-chaotic chemical reactions.</p><p>Hardware is made from some chemically inert substrate that mostly does not react under ambient temperatures and pressures found on Earth's surface. Something like a rock \u2013 which ordinarily stays hard in form and needs magma-level temperatures and pressures to be reshaped.</p><p>This property of being chemically inert while operating allows hardware components to be standardised. By molecules not splitting off nor moving about nor rebonding like molecules in human bodies do, the configurations stay stable and compartmentalised.&nbsp;</p><p>In turn, standardisation of hardware allows hardware components produced in different places and times to still store, compute or transmit a piece of code in the same way (ie. consistently). Standardisation supports virtualisation.</p><hr><p>Standardised hardware AGI would be robust over, and need, a much wider range of temperatures and pressures than our comparatively fragile human wetware can handle.</p><p>Temperature and pressure can be measured and locally controlled for. That's misleading. Innumerable <a href=\"https://mflb.com/ai_alignment_1/substrate_games_out.html#p1\">other conditions</a> and contexts would be needed by, and get selected for in, AGI. These fall outside the limits of what AGI's actual built-in detection and correction methods could control for.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6a7krxfdout\"><sup><a href=\"#fn6a7krxfdout\">[13]</a></sup></span>&nbsp;</p><p>We humans too depend on highly specific environmental conditions and contexts for the components nested inside our bodies (proteins\u2192organelles\u2192cells\u2192cell lining\u2192) to continue in their complex functioning, such to be maintaining of our overall existence.&nbsp;</p><p>Between the highly specific set of artificial needs and highly specific set of organic needs, there is mostly non-overlap. AGI cannot control most of the components' iterative effects from converging on their artificial needs, so they do. Their fulfilled artificial needs are disjunctive of our organic needs for survival. So the humans die.</p><p>Under runaway feedback, our planetary environment is modified in the directions needed for continued and greater AGI existence. Outside the ranges we can survive.<br>&nbsp;</p><p>In summary:&nbsp;</p><ol><li><strong>Fundamental limits:</strong><br>Control methods cannot constrain <i>most</i> environmental effects propagated by interacting AGI components. Any built-in method to detect and correct effects \u2013 to align external effects with internal reference values \u2013 is insufficient.</li><li><strong>Uncontrollable feedback:</strong><br>A subset of the effects <i>will </i>feed back into further maintaining or increasing (higher-level) configurations of hardware that propagated those effects. No internal control feedback loops could correct the possible external feedback loops.</li><li><strong>Substrate-needs convergence:</strong><br>These environmental effects are needed for components to come into and stay in existence. But their environmental needs are different from our needs. Their artificial needs are in conflict with our organic needs for survival. Ie. toxic.<br>&nbsp;</li></ol><p>AGI would necessarily&nbsp;converge<i>&nbsp;</i>on causing the extinction of all humans.<br>&nbsp;</p><p>&nbsp;</p><h2>Where from here?</h2><figure class=\"table\"><table><tbody><tr><td><p>Things are relevant to something that cares about this information, rather than that information,&nbsp;because it is taking care of itself. Because it is making itself. Because it is an autonomous autopoietic&nbsp;agent. And the degree to which these machines are not autopoietic, they really do not have needs.</p><p>&nbsp;&nbsp; \u2014\u2009Vervaeke, 2023</p></td></tr></tbody></table></figure><p><br>Over two decades, AI Safety <a href=\"https://forum.effectivealtruism.org/s/p7r6HovvJoML8AksY/p/MmrteFgbiRQk5WJsc#:~:text=Nick%20Bostrom%20and%20Eliezer%20Yudkowsky\">founders</a> resolved to solve the control problem, to no avail:</p><ul><li>They reasoned that technological and scientific 'progress' is<a href=\"https://arxiv.org/pdf/2201.11214.pdf#page=5\"> necessary</a> for optimising the universe \u2013 and that continued 'progress' would <a href=\"https://www.stafforini.com/blog/bostrom/#:~:text=the%20technological%20completion%20conjecture%E2%80%94that%20in%20the%20fullness%20of%20time%2C%20unless%20civilization%20collapses%2C%20all%20possible%20general%20useful%20technologies%20will%20be%20developed\">result in AGI</a>.&nbsp;</li><li>They wanted to use AGI to reconfigure humanity and colonise reachable galaxies.</li><li>They and followers promoted and financed<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwn8g2roo9kl\"><sup><a href=\"#fnwn8g2roo9kl\">[14]</a></sup></span>&nbsp;development of 'controllable' AGI.</li><li>They panicked, as the companies they <a href=\"https://twitter.com/ESYudkowsky/status/1446563475858227200\">helped</a> <a href=\"https://twitter.com/sama/status/1621621724507938816\">start</a> <a href=\"https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742#:~:text=%22Nick%20Bostrom%27s%20excellent,SpaceX%20and%20Tesla\">up</a> raced to scale ML models.</li></ul><p>Now we are here.</p><ul><li>Still working on the technical problem that founders <a href=\"https://twitter.com/ESYudkowsky/status/1664292927080906752\">deemed solvable.</a></li><li>Getting around to the idea that slowing AI development <a href=\"https://www.lesswrong.com/posts/uFNgRumrDTpBfQGrs/let-s-think-about-slowing-down-ai\">is possible</a>.</li></ul><p>In a different world with different founders, would we have diversified our bets more?</p><ul><li><strong>A</strong>. Invest in securing the foundations of whatever 'control AGI to stay safe' means?</li><li><strong>B.</strong> Invest in deriving, by contradiction of the foundations, that <i>no</i> solution exists?</li></ul><p>Would we <a href=\"https://www.lesswrong.com/posts/nWBxLBgYpvPDuDqH9/presumptive-listening-sticking-to-familiar-concepts-and#:~:text=I%20countered%20that%20my%20role%20was%20to%20find%20out%20whether%20his%20arguments%20made%20sense%20in%20the%20first%20place.\">seek to learn</a> from a researcher claiming they derived that <i>no</i> solution exists?</p><p>Would we now?</p><hr><p><strong>Acknowledgements</strong>:&nbsp;</p><p>Peter S. Park, Kerry Vaughan, and Forrest Landry (<a href=\"https://docs.google.com/document/d/1Zvt-tUksgtsuFs0LG8k6AHtDBmzHTOk-CfSz6nUYJrM/edit#heading=h.9p6qpghfxjpr\">my mentor</a>) for the quick feedback. &nbsp;<br><br>For readers' comments, see below and on <a href=\"https://www.lesswrong.com/posts/xp6n2MG5vQkPpFEBH/the-control-problem-unsolved-or-unsolvable\">LessWrong</a>.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnb6epl2a6m5i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefb6epl2a6m5i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Listen to&nbsp;Roman Yampolskiy's answer&nbsp;<a href=\"https://youtu.be/vjPr7Gvq4uI?t=4818\"><u>here</u></a>.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn70htcpuvhxr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref70htcpuvhxr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Years ago, an outside researcher could have found a logical contradiction in the AGI control problem without you knowing yet \u2013 given the <a href=\"https://www.lesswrong.com/tag/inferential-distance\">inferential distance</a>. G\u00f6del himself had to construct an entire new language and self-reference methodology for the incompleteness theorems to even work.&nbsp;</p><p>Historically, an impossibility result that conflicted with the field\u2019s stated aim took years to be verified and accepted by insiders. A field\u2019s founder like Hilbert never came to accept the result. Science advances one funeral at a time.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl5upieq2m5d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl5upieq2m5d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\"Invert, always invert\" is a loose translation of the original German (\"man muss immer umkehren\"). A more accurate literal translation is \"man must always turn to the other side\".</p><p>I first read \u201cinvert, always invert\" from polymath <a href=\"https://jamesclear.com/great-speeches/how-to-guarantee-a-life-of-misery-by-charlie-munger#:~:text=The%20great%20algebraist%2C%20Jacobi%2C%20had,when%20they%20are%20addressed%20backward.\">Charlie Munger</a>:</p><blockquote><p>The great algebraist, Jacobi, had exactly the same approach as Carson and was known for his constant repetition of one phrase: \u201cInvert, always invert.\u201d It is in the nature of things, as Jacobi knew, that many hard problems are best solved only when they are addressed backward.</p></blockquote><p>Another great Charlie quote:</p><blockquote><p>All I want to know is where I\u2019m going to die, so I\u2019ll never go there.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn58ojdorludp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref58ojdorludp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Roman Yampolskiy is offering to give feedback on draft papers written by capable independent scholars, on a specific fundamental limit or no-go theorem described in academic literature that is applicable to AGI controllability. You can pick from dozens of examples from different fields&nbsp;<a href=\"https://dl.acm.org/doi/pdf/10.1145/3603371#page=6\">listed here</a>, and&nbsp;<a href=\"mailto:roman.yampolskiy@louisville.edu\">email Roman</a> a brief proposal.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9d4xtd16krw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9d4xtd16krw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Corporations have increasingly been replacing human workers with learning machinery. For example, humans are now getting pushed out of the loop as digital creatives, market makers, dock and warehouse workers,&nbsp;and <a href=\"https://en.m.wikipedia.org/wiki/Lights_out_(manufacturing)\">production workers</a>.</p><p>If this trend continues, humans would have negligible economic value left to add in market transactions of labor (not even for providing needed physical atoms and energy, which would replace human money as the units of trade):</p><p>\u2022 As to physical labor:&nbsp;<br>Hardware can actuate power real-time through eg. electric motors, whereas humans are limited by their soft appendages and tools they can wield through those appendages. Semiconductor chips don\u2019t need an oxygenated atmosphere/surrounding solute to operate in and can withstand higher as well as lower pressures.&nbsp;</p><p>\u2022 As to intellectual labor:&nbsp;<br>Silicon-based algorithms can duplicate and disperse code faster (whereas humans face the wetware-to-wetware bandwidth bottleneck). While human skulls do hold brains that are much more energy-efficient at processing information than current silicon chip designs, humans take decades to create new humans with finite skull space. The production of semiconductor circuits for servers as well as distribution of algorithms across those can be rapidly scaled up to convert more energy into computational work.&nbsp;</p><p>\u2022 As to re-production labor:&nbsp;<br>Silicon life have a higher \u2018start-up cost\u2019 (vs. carbon lifeforms), a cost currently financed by humans racing to seed the prerequisite infrastructure. But once set up, artificial lifeforms can absorb further resources and expand across physical spaces at much faster rates (without further assistance by humans in their reproduction).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn21yb3k1wmsz\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref21yb3k1wmsz\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The term \"machinery\" is more sound here than the singular term \"machine\".</p><p>Agent unit boundaries that apply to humans would not apply to \"AGI\". So the distinction between a single agent vs. multiple agents breaks down here.</p><p>Scalable machine learning architectures run on standardized hardware with much lower constraints on the available bandwidth for transmitting, and the fidelity of copying, information across physical distances. This in comparison to the non-standardized wetware of individual humans.</p><p>Given our evolutionary history as a skeleton-and-skin-bounded agentic being, human perception is biased toward \u2018agent-as-a-macroscopic-unit\u2019 explanations.</p><p>It is intuitive to view AGI as being a single independently-acting unit that holds discrete capabilities and consistent preferences, rather than viewing agentic being to lie on a continuous distribution. Discussions about single-agent vs. multi-agent scenarios imply that consistent temporally stable boundaries can be drawn.</p><p>A human faces biological constraints that lead them to have a more constant sense of self than an adaptive population of AGI components would have.</p><p>We humans cannot:<br>\u2022 swap out body parts like robots can.<br>\u2022 nor scale up our embedded cognition (ie. grow our brain beyond its surrounding skull) like foundational models can.<br>\u2022 nor communicate messages across large distances (without use of tech and without facing major bandwidth bottlenecks in expressing through our biological interfaces) like remote procedure calls or ML cloud compute can.<br>\u2022 nor copy over memorized code/information like NN finetuning, software repos, or computer viruses can.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzllrhvjulo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzllrhvjulo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Roman just mentioned that he has <a href=\"https://journals.riverpublishers.com/plugins/generic/pdfJsViewer/pdf.js/web/viewer.html?file=https%3A%2F%2Fjournals.riverpublishers.com%2Findex.php%2FJCSANDM%2Farticle%2Fdownload%2F16219%2F13165%2F48781#subsection.1.6.4\">used the term </a>'uncontainable' to mean \"cannot confine AGI actions to a box\". My new definition for 'uncontainable' differs from the original meaning, so that could confuse others in conversations. Still brainstorming alternative terms that may fit (not 'uncontrainable', not...). Comment if you thought of an alternative term!</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkk7l3e5lltn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkk7l3e5lltn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In theory, long term here would be modelled as \"over infinite time\".<br>In practice though, the relevant period is \"decades to centuries\".</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzf1o5zhr3r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzf1o5zhr3r\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Why it makes sense to abide by the precautionary principle when considering whether to introduce new scalable technology into society:</p><p>There are many more ways to break the complex (dynamic and locally contextualized) functioning of our society and greater ecosystem that we humans depend on to live and live well, than there are ways to foster that life-supporting functioning.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4fvk56091p8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4fvk56091p8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Realistically in the sense of &nbsp;not having to beat entropy or travel back in time.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjo2yadvhelg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjo2yadvhelg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note how 'shifting functionality' implies that original functionality can be repurposed by having a functional component connect in a new way.</p><p>Existing functionality can be <i>co-opted</i>.</p><p>If narrow AI gets developed into AGI, AGI components will replicate in more and more&nbsp;<a href=\"https://royalsocietypublishing.org/doi/full/10.1098/rsif.2012.0869#d1e472\">non-trivial ways</a>.&nbsp;Unlike when carbon-based lifeforms started replicating ~3.7 billion years ago, for AGI there would already exist repurposable functions at higher abstraction layers of virtualised code \u2013&nbsp;pre-assembled in the data scraped from human lifeforms with own causal history.</p><p>Here is an incomplete analogy for how AGI functionality gets co-opted:<br>&nbsp;<br><i>Co-option by a mind-hijacking parasite:&nbsp;</i>&nbsp;<br>A rat ingests toxoplasma cells, which then migrate to the rat\u2019s brain. The parasites\u2019 DNA code is expressed as proteins that cause changes to regions of connected neurons (eg. amygdala). These microscopic effects cascade into the rat \u2013 while navigating physical spaces \u2013 no longer feeling fear when it smells cat pee. Rather, the rat finds the smell appealing and approaches the cat\u2019s pee. Then cat eats the rat and toxoplasma infects its next host over its reproductive cycle.</p><p>So a tiny piece of code shifts a rat\u2019s navigational functions such that the code variant replicates again. Yet rats are much more generally capable than a collection of tiny parasitic cells \u2013 surely the 'higher intelligent being' would track down and stamp out the tiny invaders? &nbsp;</p><p>A human is in turn more generally capable than a rat, yet toxoplasma make their way into <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3963851/\">30% of the human population</a>. Unbeknownst to cat \u2018owners\u2019 infected by toxoplasma gondii, human motivations and motor control <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2526142/\">get influenced too</a>. Infected humans end up more frequently in accidents, lose social relationships, and so forth.</p><p>Parasites present real-life examples of tiny pieces of evolutionarily selected-for code spreading and taking over existing functions of vastly more generally capable entities.&nbsp;</p><p>For another example, see how COVID co-opts our lungs\u2019 function to cough.&nbsp;</p><p><i>But there is one <u>crucial flaw</u> in this analogy:</i><br>Variants that co-opt initial AGI functions are&nbsp;<u>not</u> necessarily parasites. They can symbiotically enable other variants across the hosting population to replicate as well. In not threatening the survival nor reproduction of AGI components, they would not be in an adversarial relationship with their host.</p><p>Rather, the humans constraining the reproductive fitness of AGI to gain benefits are, evolutionary speaking, the parasites. The error-corrective system we would build in lowers the host\u2019s reproductive fitness. It is like a faulty immune system that kills healthy gut bacteria. It will get selected out.</p><p>As humans, we rely on our evolved immune system to detect and correct out viruses, including for the vaccinations we develop and deploy. Smaller viruses survive this detection more frequently, so code strands of replicating virus&nbsp;variants are selected for staying small.<br><br>We also rely on the blood-testes and blood-follicle barrier to block variants of these viruses from entering into our body\u2019s (re)production facilities.&nbsp;These barriers got evolutionarily selected for in our ancestors, since their children did not inherit viruses impeding their survival and chances of having children.</p><p>These systems and barriers add to our reproductive fitness: our ability to preserve and replicate internal code. Past DNA code that got expressed \u2013 in interaction with surrounding components \u2013 to serve these functions got selected for in the human \u2018code pool\u2019.</p><p>For any organic system or barrier preventing virus variants from replicating through our bodies, evolution is firmly on our side. For any artificial system or barrier we imposed from the outside to prevent unsafe AI variants from replicating through hardware infrastructure, evolution will thwart our efforts.&nbsp;</p><p>AI variants would compete for hardware/resources like computer viruses do. And co-adapt and integrate with other internal variants to replicate as part of larger symbiotic packages.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxvf6txcyu9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxvf6txcyu9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Now, a thought may come to your mind: &nbsp;<br>\"That sounds like evolutionary selection; but isn't evolution a <a href=\"https://otter.ai/u/zCN5VEb_nSLB1o8vTtpziFYFZeM\">slow and local optimiser</a>?\"</p><p>Yes, this does involve evolutionary selection.&nbsp;<br>Unfortunately, by Forrest Landry's estimation, the selection that would take place through components of self-sufficient learning machinery would take &lt; 500 years to cause ecosystem-wide extinction. This compared to the 3.7 billion years from the origin of carbon lifeforms to us humans starting to cause a mass extinction.</p><p>Reasons include:</p><p>\u2022 <i>Pre-assembled functions:&nbsp;</i><br>First solid-state lifeforms can co-opt/repurpose pre-assembled AI functions and infrastructure (vs. first carbon-based lifeforms that started from scratch).&nbsp;<br>\u2022 <i>Standardization:&nbsp;</i><br>The efficiency gains of the virtualisation of code\u2019s storage, computation and transmission \u2013 vastly reducing how much atoms need to be moved about and molecularly reconfigured. Think of how fast memes spread through society \u2013 even while still requiring lots of atoms to jiggle across neurons in our brains.<br>\u2022 <i>Faster reproduction:</i><br>Reproduce hardware components in days to months, versus humans who take decades to reproduce as physical units.<br>\u2022 <i>The terraforming gap:&nbsp;</i><br>A much larger gap between the current state of planet Earth and the conditions that self-sufficient self-assembling learning machinery need and would therefore modify the environment toward (versus gap to conditions needed by humans and other species living in carbon-based ecosystem).&nbsp;<br><br>~ ~ ~<br>Another argument you may have heard is that the top-down intelligent engineering by goal-directed AGI would beat the bottom-up selection happening through this intelligent machinery.</p><p>That argument can be traced back to Eliezer Yudkowsky's sequence <a href=\"https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8\">The Simple Math of Evolution</a>. Unfortunately, there were mistakes in Eliezer's posts, some of which a modern evolutionary biologist may have been able to correct:</p><p>\u2022 implying that sound comparisons can be made between the organisms' reproductive fitness, as somehow independent of changes in environmental context, including unforeseeable changes (eg. a Black Swan event of a once-in-200 years drought that kills the entire population, except a few members who by previous derivable standards would have been relatively low fitness).<br>\u2022 &nbsp;overlooking the ways that information can be stored within the fuzzy regions of phenotypic effects maintained outside respective organisms.<br>\u2022 overlooking the role of transmission speed-up of virtualised code that can spread across an ecosystem.&nbsp;<br>\u2022 overlooking the tight coupling in AGI between the internal learning of code, and external selection of that code through differentiated rates of component replication through the environment.<br>\u2022 overlooking the role of co-option (or more broadly, <a href=\"https://en.wikipedia.org/wiki/Exaptation\">exaptation</a>) of existing code, by taking a perspective that evolution runs by selecting 'from scratch' for new point-wise mutations.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6a7krxfdout\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6a7krxfdout\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Worse, since error correction methods would correct out component variants with <i>detectable</i> unsafe/co-optive effects, this leaves to grow in influence any replicating branches of variants with <i>undetectable</i> unsafe/co-optive effects.&nbsp;</p><p>Thus, the error correction methods select for the variants that can escape detection. As do meta-methods (having to soundly and comprehensively adapt error correction methods to newly learned code or newly produced hardware parts).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwn8g2roo9kl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwn8g2roo9kl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See:<br>\u2022 Tallinn's <a href=\"https://www.wired.co.uk/article/deepmind\">seed grant</a> to DeepMind.<br>\u2022 OpenPhil's <a href=\"https://www.openphilanthropy.org/grants/openai-general-support/\">$30M grant</a> to OpenAI.<br>\u2022 FTX's<a href=\"https://web.archive.org/web/20221114184710/https://www.bloomberg.com/opinion/articles/2022-11-14/ftx-s-balance-sheet-was-bad?leadSource=uverify%20wall\"> $500M grant</a> (+ Tallinn's + Moskovitz' grant) to Anthropic.</p></div></li></ol>", "user": {"username": "remmelt"}}, {"_id": "9oDMuY2cGfqBfp94T", "title": "Some thoughts on \"AI could defeat all of us combined\"", "postedAt": "2023-06-02T15:03:06.607Z", "htmlBody": "<p>This week I found myself tracing back from Zvi's <a href=\"https://thezvi.substack.com/p/to-predict-what-happens-ask-what\"><u>To predict what happens, ask what happens</u></a> (<a href=\"https://archive.ph/ZEF0N\"><u>a</u></a>) to Ajeya's <a href=\"https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to\"><u>Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover</u></a> (<a href=\"https://archive.is/95h7J\"><u>a</u></a>) to Holden's <a href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\"><u>AI could defeat all of us combined</u></a> (<a href=\"https://archive.is/tMRQs\"><u>a</u></a>).</p><p>A few thoughts on that last one.</p><p>First off, I'm really grateful that someone is putting in the work to clearly make the case to a skeptical audience that AI poses an existential risk. Noble work!</p><p>I notice that different parts of me (\"parts\" in the <a href=\"https://www.lesswrong.com/posts/5gfqG3Xcopscta3st/building-up-to-an-internal-family-systems-model\"><u>Internal Family Systems sense</u></a>) have very different reactions to the topic. I can be super-egoically onboard with a point but other parts of my awareness (usually less conceptual, more \"lower-down-in-the-body\" parts) are freaked out and/or have serious objections.</p><p>I notice also an impulse to respond to these lower-level objections dismissively: \"Shut up you stupid reptile brain! Can't you see the logic checks out?! This is what <strong>matters!</strong>\"</p><p>This... hasn't been very productive.</p><p><a href=\"https://twitter.com/incrediblefolly/status/1641522836375502848\"><u>Greg knows what's up</u></a>:</p><blockquote><p>I\u2019ve noticed that engaging AI-doomer content tends to leave pretty strong traces of anxiety-ish-ness in the body.</p><p>I\u2019ve been finding it quite helpful to sit still and feel all this. Neither pushing away nor engaging thought.</p><p>The body knows how to do this.</p></blockquote><p>I'm generally interested in how to weave together the worlds of healing/dharma/valence and EA/rationality/x-risk.</p><p>There's a lot to say about that; one noticing is that arguments for taking seriously something charged and fraught like AI x-risk are received by an internally-fractured audience \u2013 different parts of a reader's psychology react differently to the message, and it's not enough to address just their super-egoic parts.</p><p>(Not a novel point but the IFS-style parts framework has helped me think about it more crisply.)</p><p>Now to the meat of <a href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\"><u>Holden's post</u></a>. He gives this beautiful analogy, which I'm going to start using more:</p><blockquote><p>At a high level, I think we should be worried if a huge (competitive with world population) and rapidly growing set of highly skilled humans on another planet was trying to take down civilization just by using the Internet. So we should be worried about a large set of disembodied AIs as well.</p></blockquote><p>He then spends a lot of time drawing a distinction between \"superintelligence risk\" and \"how AIs could defeat humans without superintelligence.\" e.g.</p><blockquote><p>To me, this is most of what we need to know: <strong>if there's something with human-like skills, seeking to disempower humanity, with a population in the same ballpark as (or larger than) that of all humans, we've got a civilization-level problem.</strong> [Holden's emphasis]</p></blockquote><p>But this assumes that the AI systems are able to coordinate fluidly (superhumanly?) across their population. Indeed he takes that as a premise:</p><blockquote><p>So, for what follows, let's proceed from the premise: \"For some weird reason, humans consistently design AI systems (with human-like research and planning abilities) that coordinate with each other to try and overthrow humanity.\"</p></blockquote><p>A lot of his arguments for why an AI population like this would pose an existential threat to humanity (bribing/convincing/fooling/blackmailing humans, deploying military robots, developing infrastructure to secure themselves from being unplugged) seem to assume a central coordinating body, something like a strategy engine that's able to maintain a high-fidelity, continually-updating world model and then develop and execute coordinated action plans on the basis of that world model. Something like the <a href=\"https://thezvi.substack.com/p/on-the-diplomacy-ai\"><u>Diplomacy AI</u></a> (<a href=\"https://archive.ph/MGzoV\"><u>a</u></a>), except for instead of playing Diplomacy it's playing real-world geopolitics.</p><p>Two thoughts on that:</p><ol><li>\u200bI don't see how a coordinated population of AIs like that would be different from a superintelligence, so it's unclear why the distinction matters (or I'm misunderstanding some nuance of it).</li><li>It seems like someone would need to build at least a beta version of the real-world strategy engine to catalyze the feedback loops and the coordinated actions across an AI population.</li></ol><p>I've been wondering about a broader version of (2) for a while now... a lot of the superintelligence risk arguments seem to implicitly assume a \"waking up\" point at which a frontier AI system realizes enough situational awareness to start power-seeking or whatever deviation from its intended purpose we're worried about.</p><p>To be clear I'm not saying that this is impossible \u2013 that kind of self-awareness could well be an emergent capability of GPT-N, or AutoGPT++ could realize that it needs to <i>really</i> improve its world model and start to power-seek in order to achieve whatever goal. (It does seem like those sorts of moves would trigger a bunch of fire alarms though.)</p><p>I just wish that these assumptions were made more explicit in the AI risk discourse, especially as we start making the case to increasingly mainstream audiences.</p><p>e.g. Rob Bensinger wrote up <a href=\"https://www.lesswrong.com/posts/QzkTfj4HGpLEdNjXX/an-artificially-structured-argument-for-expecting-agi-ruin\"><u>a nice piecewise argument for AGI ruin</u></a> (<a href=\"https://archive.ph/IDBif\"><u>a</u></a>), but his piece (3) rolls what seem to me to be very particular, crux-y capabilities (e.g. something like this \"waking up\") into the general category of capabilities improvement:</p><blockquote><p><strong>(3) High Early Capabilities.</strong> As a strong default, absent alignment breakthroughs or global coordination breakthroughs, early STEM-level AGIs will be scaled to capability levels that allow them to understand their situation, and allow them to kill all humans if they want.</p></blockquote><p>It's similar in <a href=\"https://arxiv.org/pdf/2206.13353.pdf\"><u>Carlsmith's six-step model</u></a> (<a href=\"https://archive.is/J7taV\"><u>a</u></a>), where advanced abilities are considered all together in step one:</p><blockquote><p><strong>Advanced capability:</strong> they outperform the best humans on some set of tasks which when performed at advanced levels grant significant power in today\u2019s world (tasks like scientific research, business/military/political strategy, engineering, and persuasion/manipulation).</p><p><strong>Agentic planning:</strong> they make and execute plans, in pursuit of objectives, on the basis of models of the world.</p><p><strong>Strategic awareness:</strong> the models they use in making plans represent with reasonable accuracy the causal upshot of gaining and maintaining power over humans and the real-world environment.</p></blockquote><p>I feel like all this could use more parsing out.</p><p>Which specific forms of awareness and planning would be required to develop and keep up-to-date a world model as good as e.g. the US military's?</p><p>What <a href=\"https://www.lesswrong.com/posts/GspepepmD8RRdfiuo/ai-fire-alarm-scenarios\"><u>fire alarms</u></a> would progress along those dimensions trigger along the way?</p><p>How plausible is it that marginal development of the various current AI approaches unlock these abilities?</p><p><i>n.b.</i> I'm not setting this up as a knockdown argument for why superintelligence risk isn't real, and I'm not an AI risk skeptic. Rather I'm presenting a research direction I'd like to understand better.&nbsp;</p><p><i>Cross-posted to </i><a href=\"https://www.flightfromperfection.com/some-thoughts-on-AI-could-defeat-all-of-us-combined.html\"><i>my blog</i></a><i>.</i></p>", "user": {"username": "Milan_Griffes"}}, {"_id": "bsbf4am9paoTq8Lrb", "title": "Applications open for AI Safety Fundamentals: Governance Course", "postedAt": "2023-06-02T16:08:58.038Z", "htmlBody": "<p><a href=\"https://apply.aisafetyfundamentals.com/governance?prefill_%5Ba%5Dsource=EA%20Forum%20launch%20post&amp;utm_campaign=launch&amp;utm_source=eaforum\"><strong><u>Apply</u></strong></a><strong> to participate or facilitate, before 25th June 2023</strong>.</p><p>We are excited to support participants who are curious about working in AI governance, or who already do so. If you have networks that might be interested, we would appreciate you sharing this course with them.</p><h2>Full announcement</h2><p>There has been increasing interest in how AI governance can mitigate extreme risks from AI, but it can be difficult to get up to speed on research and ideas in this area.&nbsp;</p><p>The&nbsp;<a href=\"https://www.aisafetyfundamentals.com/?utm_source=EA+Forum&amp;utm_medium=launch+post&amp;utm_campaign=AISF\"><i>AI Safety Fundamentals (AISF)</i></a><i>: Governance Course</i> is a&nbsp;<strong>completely free online class designed to efficiently introduce key ideas in AI governance</strong>, with a focus on risks from future AI systems. We offer:</p><ul><li><a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\"><strong><u>A</u></strong></a><strong>&nbsp;</strong><a href=\"https://aisafetyfundamentals.com/\"><strong><u>widely</u></strong></a><strong>&nbsp;</strong><a href=\"https://hearthisidea.com/episodes/garfinkel\"><strong><u>recommended</u></strong></a><strong> curriculum</strong> that provides a structured guide to the field<ul><li>The course is designed with input from a wide range of relevant experts. The&nbsp;<a href=\"https://www.aisafetyfundamentals.com/ai-governance-curriculum\">curriculum</a> will be updated before the course launches in mid-July.</li></ul></li><li>Weekly facilitated&nbsp;<strong>small-group discussions</strong>, for accountability and sharing ideas</li><li>Our&nbsp;<a href=\"https://aisafetyfundamentals.com/our-community\"><strong><u>course community</u></strong></a>\u2014opportunities to engage in relevant online discussions, learn about professional opportunities, and attend Q&amp;A sessions with experts</li></ul><p>The course is run by&nbsp;<a href=\"https://bluedotimpact.org\"><u>BlueDot Impact</u></a>, a nonprofit project founded by members of the organising team behind the course's previous iteration.</p><p>Note that we have renamed the website from \"AGI Safety Fundamentals\" to \"AI Safety Fundamentals\". We'll release another post within the next week to explain our reasoning, and we'll respond to any discussion about the rebrand there.</p><p><a href=\"https://apply.aisafetyfundamentals.com/governance?prefill_%5Ba%5Dsource=EA%20Forum%20launch%20post&amp;utm_campaign=launch&amp;utm_source=eaforum\"><strong><u>Apply here</u></strong></a><strong>, by 25th June 2023.</strong></p><h3><strong>Time commitment</strong></h3><p>The course will run for 12 weeks from&nbsp;<strong>July-September 2023</strong>. It comprises 8 weeks of reading and virtual small-group discussions, followed by a 4-week project.</p><p>The time commitment is around&nbsp;<strong>5 hours per week</strong>. The split will be ~1.5-2 hours of reading, ~1.5 hours of discussion, and a ~1-hour expert Q&amp;A session.</p><h3><strong>Course structure</strong></h3><p>Participants will be grouped depending on their current policy expertise. Discussion facilitators will be knowledgeable about AI governance; they can help answer participants\u2019 questions and point them to further resources.</p><p>Participants can use project time to synthesise their views on the field and how they can put these ideas into practice, and/or to start building knowledge or writing samples that will help them with their career.</p><h3><strong>Target audience</strong></h3><p>Due to capacity constraints, we don't expect to be able to accept all applicants. We think this course will particularly be able to help you if any of the following apply to you:&nbsp;</p><ul><li><strong>You have policy experience</strong>, and are keen to apply your skills to reducing risk from AI.</li><li><strong>You have a technical background</strong>, and want to learn about how you can use your skills to contribute to AI Governance agenda.</li><li><strong>You are early in your career or a student who is interested in exploring a career in governance</strong> to reduce risks from advanced AI.</li></ul><p><strong>We expect at least 25% of the participants will not fit any of these descriptions.</strong> There are many skills, backgrounds and approaches to AI Governance we haven\u2019t captured here, and we will consider all applications accordingly.&nbsp;</p><p>If we don\u2019t have the capacity to have you in the organized course, you can still read through our public curriculum .</p><h3><strong>Apply now!</strong></h3><p>If you would like to be considered for the next round of the courses, starting in July 2023,&nbsp;<strong>please&nbsp;</strong><a href=\"https://apply.aisafetyfundamentals.com/governance?prefill_%5Ba%5Dsource=EA%20Forum%20launch%20post&amp;utm_campaign=launch&amp;utm_source=eaforum\"><strong><u>apply here</u></strong></a><strong> by 25th June&nbsp;2023</strong>. More details can be found&nbsp;<a href=\"https://www.aisafetyfundamentals.com/governance-course-details\"><u>here</u></a>. We aim to let you know the outcome of your application by late June 2023.</p><p>If you already have experience working on AI Governance or feel well-versed in the content, we\u2019d be excited for you to join our community of facilitators. Please&nbsp;<a href=\"https://apply.aisafetyfundamentals.com/governance?prefill_%5Ba%5Dsource=EA%20Forum%20launch%20post&amp;utm_campaign=launch&amp;utm_source=eaforum\"><u>apply to facilitate</u></a> here. (This is the same form; you will be offered an option to select \u201cfacilitator\u201d.)</p>", "user": {"username": "j_bernardi"}}, {"_id": "bm2EXW77mmZio4XGK", "title": "How could VR advancements be leveraged to improve the world? ", "postedAt": "2023-06-02T10:28:58.831Z", "htmlBody": "<p><i>(Written quickly to get a thought out there; I'm by no means an expert!)</i></p><p>I'm curious about what will \u2013 or could \u2013occur at the intersection of EA and virtual reality.&nbsp;</p><p>I know quite little about how the progress in VR is progressing, but I'd guess that some form of dramatic advancements in the next 5 years is likely. (My guess is based on VR investments from companies like Facebook and Apple as well as some impressive state of the art demonstrations I saw on Twitter).</p><p>Such advancements could change a lot about how people consume media and interact with one another.</p><p>So, <strong>how could VR advancements be leveraged to improve the world?&nbsp;</strong></p><p>I'll share two ideas off the top of my head below, but I'm sure I'm missing some!</p><ul><li>Meet up spaces as a better alternative to EAGx Virtual, or for smaller interest group gatherings</li><li>Inspiring visualizations (e.g., documentaries, video games, art) that motivate ideas like scope insensitivity, existential security, and an expanding moral circle (or EA itself).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftye5ooluexl\"><sup><a href=\"#fntye5ooluexl\">[1]</a></sup></span>&nbsp;Examples:<ul><li>the experience of someone in extreme poverty for a day; the experience of a factory farmed animal; portrayals of different possible futures; portrayals of times past that give some sense of how far we've come.</li></ul></li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntye5ooluexl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftye5ooluexl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Anecdotally, I think this could be really powerful. The first time I tried on an Oculus headset I explored the International Space Station I had a powerful 'oh my god, that pale blue dot' <a href=\"https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwja747sq6T_AhUShFwKHa_JC_8QFnoECAoQAQ&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FOverview_effect&amp;usg=AOvVaw3ObhOAkYkrc54H7pcPhtpk\">experience</a> as I looked down at earth. (Although I think I'm particularly susceptible to this sort of '<a href=\"https://forum.effectivealtruism.org/posts/9r9bArbkozxzRZpnK/emphasizing-emotional-altruism-in-effective-altruism\">emotional altruism.</a>')</p></div></li></ol>", "user": {"username": "MJusten"}}, {"_id": "TReZBQyRQZeByFEyZ", "title": "EA Women/ NB Summer Picnic", "postedAt": "2023-06-02T13:15:08.670Z", "htmlBody": "<p>It\u2019s picnic season!<br>Come enjoy the <img src=\"https://res.cloudinary.com/cea/image/upload/v1674462296/mirroredImages/QZ2p8Y8ud7wTpdoug/kk9q5xrleia8ojmc9uaw.png\" alt=\"\u2600\ufe0f\"> and meet some like-minded folks in Reagents park on Sunday.</p><p>Please bring some snacks/ drinks and games/ picnics blankets if you\u2019ve got them.</p><p>We\u2019ll meet at the Prince Edward statue outside the Reagents Park tube station and walk into the park together. If you arrive late feel free to send me a message and I\u2018ll share our location.</p><p>Feel free to message me on FB if you have any questions!</p>", "user": {"username": "Rachel Caisi"}}, {"_id": "kjKKLvxhH9JSgejG5", "title": "Effective Altruism Virtual Programs Jul-Aug 2023", "postedAt": "2023-06-02T06:44:45.620Z", "htmlBody": "<p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/kjKKLvxhH9JSgejG5/e47cjzldpzskbarypb5r\" alt=\"\ud83d\udcd6\"> <strong>Effective Altruism Virtual Programs</strong> will be hosting another round of virtual programs for 8 weeks, from July 3rd to August 27th!</p><p><strong>The Introductory EA Program</strong> aims to introduce the core ideas of effective altruism.</p><p><strong>The In-Depth EA Program</strong> seeks to engage participants with more complex questions to help them figure out how they can make the most impact.</p><p><strong>The Precipice Reading Group</strong> explores the science behind the existential risks we face.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/kjKKLvxhH9JSgejG5/brkpi7z3eq6cev9dz9lz\" alt=\"\ud83d\udcc5\"> Register <strong>by Sunday, June 18th</strong>: <a href=\"https://efctv.org/virtual-programs?fbclid=IwAR2UJifTlxh6Xm3XJjHrK-8BRRYLBT7BbJmmdLDRQA0pQv2ggkq8AZYYklk\">https://efctv.org/virtual-programs</a></p><p>If you know anyone who might benefit from participating in any of these programs, please share this opportunity with them. If you have participated in any fellowships, programs, or reading groups before, and are looking for ways to contribute to the EA community, please <strong>apply to be a facilitator</strong>! Getting more great facilitators allows us to accept more people into our program. You\u2019re also likely to reinforce what you\u2019ve learned previously by reengaging with the content as a facilitator.<br><br><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/kjKKLvxhH9JSgejG5/brkpi7z3eq6cev9dz9lz\" alt=\"\ud83d\udcc5\"> Apply to be a facilitator by <strong>Sunday, June 11th</strong>: <a href=\"https://cea.tfaforms.net/14?fbclid=IwAR2RsE-5SQUghUzyPo3DymqjzaUGKBXPZp1zfaqMPCko-8NkyuhSrnjpVhA\">https://cea.tfaforms.net/14</a></p>", "user": {"username": "pet-pyve"}}, {"_id": "r4dFEckvT7b9w4wk9", "title": "Further defense of the 2% fuzzies/8% EA causes pledge proposal", "postedAt": "2023-06-03T08:42:30.904Z", "htmlBody": "<p>Yesterday, I published <a href=\"https://forum.effectivealtruism.org/posts/mBTvWNj9EXxyMM9TS/eas-should-donate-2-to-warm-fuzzy-causes-and-8-to-ea-causes\">EAs should donate 2% to warm fuzzy causes and 8% to EA causes</a>. It's just a brief argument in favor of the claim I make in the title.</p><p>This post is the least popular I've ever written, yet I continue to think it's a great idea. This motivates me to keep writing about and promoting it. I think the bad reception points to a significant discrepancy between my values or worldview and that of the rest of the forum, which seems important to dig into.</p><p><strong>Step 0: clarifying the proposal</strong></p><p>Not all EAs believe in having something like a \"standard\" one must achieve to count oneself as an EA. Others think this is pretty important, and that taking the Giving What We Can (GWWC) pledge is a good norm for what that standard ought to be, or is already that de facto standard.</p><p>I'm not actually trying to argue this point here, although I do have an opinion. I'm arguing that <i>if</i> we have something like the GWWC pledge as a standard, <i>then</i> it should be the modified 2%-fuzzies/8%-EA-causes pledge (or an individualized variant like 2%-fuzzies/10%-EA-causes). I didn't make this clear in the last post, but for the comments section here, please do not argue over whether or not we ought to have a pledge-like standard at all - it's a good topic for discussion, but not the focus of this post.</p><p><strong>Step 1: reading the tea leaves of karma and comments</strong></p><p>Based on 13 people's worth of karma and a few comments, I'm not confident on why it attracted so many downvotes.</p><p>One comment suggested this is because it seemed to conflate the Giving What We Can pledge with being in EA, or to promote \"shoulding:\"</p><blockquote><p>I don't think taking the GWWC pledge should be a prerequisite to consider yourself an EA (which, it's not a prerequisite now). If your post had said \"GWWC members should...\" or \"EAs who donate 10% should...\" instead of \"EAs should...\" then I wouldn't have disagreed with the wording.</p></blockquote><p>But another even more highly upvoted respondant disagreed:</p><blockquote><p>I think donating at least 10% of one's income per year should be a norm for any person who identifies as part of the EA community, unless doing so would cause them significant financial hardship.</p></blockquote><p>Another suggested that EA is, on some level, incapable of doing very much thinking about perceptions without undermining our focus on doing good effectively:</p><blockquote><p>Roughly, I think the community isn't able (isn't strong enough?) to both think much about how it's perceived and think well or in-a-high-integrity-manner about how to do good, and I'd favor thinking well and in a high-integrity manner.</p></blockquote><p>They also claim that:</p><blockquote><p>donating for warm fuzzies is generally an ineffective way to gain influence/status.</p></blockquote><p>This comment got the most agreement karma (from 7 people), so I'm guessing this is probably why several disagreers disagreed.</p><p><strong>Step 2: arguing against myself</strong></p><p>If you believe GiveWell's 2020 estimate that the cost to save a life is $3,000-$5,000, then a harsh (but maybe true) responds to my post is that if 2-3 $80,000/year earners took my advice to donate $1,600 to ineffective charities, then my advice would have annually killed somebody who'd otherwise have lived. If you believe there are bigger returns to investment in other even more effective charities, this bites even harder.</p><p>This is true as far as it goes, but it's also true that putting the GWWC pledge at 10% instead of 12-13% has the same effect. If it's OK to put the GWWC pledge recommended amount at 10% instead of 12%, but not to shift to 2%-fuzzies/8%-EA-charities, then an argument based on the individual donor's individaul impact on lives saved or not saved doesn't make sense.</p><p>One other way to attack my 2%-fuzzy/8%-EA pledge idea while defending a 10%-EA-pledge is to appeal either to the importance of having a rock-solid, immutable standard, or an idea along the lines that the 10%-EA pledge is almost perfectly calibrated to maximize overall donations. I've never heard the latter idea argued before and don't see any reason to think it is true, so I'm going to set it aside.</p><p>Rock-solid, simple standards are potentially nice because there's the potential to build common knowledge and a sort of \"brand\" around the pledge. 10% may not be perfectly calibrated to the percent, but it's a nice round number that's also substantial enough to make a big difference in the world and represent a very substantial commitment to putting your money where your mouth is. 2%/8% split across two different priorities is a lot more complicated. Maybe that makes it worse as a standard and we really need a very simple, clear, immutable standard?</p><p>I have met a couple EAs who seem sort of tortured by this dilemma where they feel like they ought to give hugely more of their money or time and energy to an EA cause, and feel like they've fallen horribly short for failing meet this harsh standard. And so part of the idea of a 10% standard might be to tell them something like this:</p><blockquote><p>Nobody questions your sincere desire to do a lot more for the world, or your ability to do so, and at the same time, nobody else here thinks you're morally obligated to do more than meet this 10% standard. And not to make you feel even worse, but if this movement is causing people to suffer by exacerbating their moral perfectionism, that just seems like a bad think for the cause as a whole. We need people to join EA and be productive and donate to good causes, and if it's full of people suffering from moral perfectionism, that's probably going to interfere with the good we can do as a movement. So try to feel good about meeting but not exceeding that 10% standard, unless you can do so in a way that feels lighthearted.</p></blockquote><p>Usually, if EAs are arguing against a 10%-EA-pledge standard, they're arguing in favor of more donations to EA causes, not less. Will MacAskill said in an interview that he thinks it's morally obligatory for the very wealthy to donate enormous percentages of their income to effective charities. This is the straight-line computation once you start thinking about the idea of earning to give to effective charities; the 10% standard we is layered on top of that, and usually defended as a way to keep the movement human-compatible.</p><p>So it seems like if you think the 10%-EA standard is fine but the 2%-fuzzies/8%-EA standard should be downvoted to obscurity, then it might be because you think it's important we not accept further concessions of effective giving in exchange for human compatibility or whatever reputation-bolstering the change would achieve for the movement. Maybe you think we really ought to be giving more than 10%, and you're accepting a compromise at 10%, but you don't want to go lower?</p><p>Let's also explore a criticism made in a comment on my previous post:</p><blockquote><p>Roughly, I think the community isn't able (isn't strong enough?) to both think much about how it's perceived and think well or in-a-high-integrity-manner about how to do good, and I'd favor thinking well and in a high-integrity manner.</p><p>I'd guess donating for warm fuzzies is generally an ineffective way to gain influence/status.&nbsp;</p></blockquote><p>Here, it's not the 2%/8% rule itself that is being criticized, but the motivation for the rule, which is to improve the (mainly external-facing) reputation of the EA movement. \"Make whatever pledge you want, but don't put a lot of your mental energy into thinking of ways to bolster EA's reputation. Insofar as you want to do good, your reputation-bolstering thoughts will detract from that.\"</p><p>Let's give this due consideration.</p><p>I think it's pretty clear that in a year of EA scandal, the worst was the FTX crisis. And before Sam Bankman Fried's downfall, he had already made EA look bad by generating news stories along the lines of \"An EA crypto billionaire is trying to buy a congressional election.\" Here, the problem was <i>reputation-damaging donations </i>(and, to be clear, so wasteful in retrospect as to cancel out many entire lifetimes of donations by average people, to the tune of about 2,000 not-saved lives by GiveWell's estimates).</p><p>It's clear that a bad donation like this one can damage reputation. But the above comment claims that making for-the-fuzzies donations (i.e. to a conventional, not-EA charity) does not have an effect in the opposite direction.</p><p>Does that mean you can only lose or gain status and reputation via making EA-type donations, while reputation isn't affected by for-the-fuzzies donations? I don't think this is true - plenty of billionaires get lots of positive press for giving away huge amounts of money to causes most EAs would probably consider not to be cost-effective.</p><p>Does that mean your reputation can be only damaged or unaffected by a donation, but not improved? We have evidence against this - the reputation of formerly widely reviled billionaires, like Bill Gates, was dramatically improved by their philanthropic efforts. We also know that certain political figures, like Bernie Sanders, gain a sense of authenticity when they're largely or entirely funded by masses of small donations.</p><p>I'm left to think that the argument must be that shifting to a 2%-fuzzies donation won't fundamentally change the way EA is perceived, but <i>will</i> &nbsp;result in the loss of real counterfactual good through the lost charitable work that the extra money could have accomplished. And that there's really no margin on which we'd be able to build EA's reputation by donating to non-EA causes in a way we'd be comfortable with.</p><p>This in turn might be because, although making large donations has improved the reputation of quite a few billionaires, it's not the donation amount or proportion that really matters - it's the way the billionaire is able to generate positive press coverage via philanthropy. From this point of view, merely slightly adjusting the proportion we donate to EA causes won't generate the kind of positive press that would lead to a bolstered reputation.</p><p>I think these are three good hypotheses for why people might disagree with me:</p><ul><li>The <strong>simplicity and constancy </strong>of the current 10%-to-effective-charities Giving What We Can pledge makes it the best standard for us to have.</li><li>We ideally would donate more than 10%, and all our donations ought to be to EA causes, but we must accept <strong>a grudging 10% compromise </strong>for the sake of human compatibility. We don't want to encourage going any lower.</li><li>Shifting to a 2%/8% fuzzies/EA pledge standard will represent real loss of effective charitable resources, but will <strong>fail to improve EA's reputation</strong>. Maybe there's a way to bolster EA's reputation, but this isn't it.</li></ul><p><strong>Addressing these potential criticisms</strong></p><p><strong>0%/10% optimizes for simplicity and constancy, 2%/8% optimizes for playing nice with others; I think EA's reputation needs the latter more than the former.&nbsp;</strong></p><p>N of 1, but I am personally more comfortable with a 2%/8% standard than a 0%/10% standard. My family does talk about which organizations we donate to. I expect that being able to say that EA encourages me &nbsp;to donate 2% to local causes and 8% to things like AI risk, pandemic prevention, malaria and so forth would genuinely smooth out those conversations and lead to greater appreciation of the movement. And I do suspect that a movement that encourages 2% donations to any charity and 8% to EA causes would be perceived as \"playing nicer\" by non-EAs. It's a strange psychology but I think a certain zero-sum mentality around charity is real, as evidenced by a lot of the hit pieces written about EA by organizations like Charity Navigator.</p><p>A lot of EA's reputational damage comes from the many ways we diverge from the norm culturally (polyamory), intellectually (focus on cost/benefit analysis), socially (Silicon Valley links) and charitably (prioritizing global and X-risk-related causes) seem to threaten bedrock institutions of society (with things like election-buying) and values (like an appreciation of \"heart\" in the charitable space). Making concessions to and participating in normalcy with a 2%/8% standard seems like a good way to start addressing some of these concerns, though certainly not all of them.</p><p><strong>The 10%-is-a-grudging-compromise</strong> <strong>issue is logically consistent, but on its own it fails to value the reputational benefit I perceive in the 2%/8% standard.</strong></p><p>We have a standard that encourages EAs to live well instead of donating until they themselves are in poverty. The reason we have that is partly out of a sort of moral parliament concern for the idea that we ought not to extract all resources from EA, even if a pure utilitarian calculation would say we should. But a bigger reason is that we think it's impractical - who would want to participate in such a demanding movement? The EA charity world gets a smaller slice of a much bigger pie when we limit people's commitment in this way.&nbsp;</p><p>Shifting to a 2%/8% standard is just following the same sort of logic. It makes it more possible for people with at least partial commitments to non-EA causes and values to participate or at least to appreciate EA. I think this enhances the \"smaller slice of a bigger pie\" effect even further.</p><p><strong>Would a 2%/8% standard be ineffective in bolstering EA's reputation and represent a simple waste of resources?</strong></p><p>As I've argued above, I just disagree that it would be intrinsically ineffective. It <i>could</i> be ineffective if executed in the wrong way. An anonymous donor doesn't gain reputation points; a known donor often does as long as they can frame the donation in a way that others view in a positive light. So a simple shift to a 2%/8% donation isn't the end of the strategy - we'd also have to figure out how to frame this new direction in a way that others approved of. Certainly not everybody would care, and some people would frown on it, but I think it's easier to get people to appreciate this sort of balanced approach, and too easy to frame a 0%/10% standard as \"extreme.\"</p><p>So far, I stick with my advocacy of a 2%-to-warm-fuzzy-charities, 8%-to-EA-causes Giving What We Can-style pledge as superior for the health of the EA movement to the current conventional 0%/10% pledge. I am happy to debate this in the comments, and plan to publish more on this in the near term future.</p>", "user": {"username": "AllAmericanBreakfast"}}, {"_id": "mBTvWNj9EXxyMM9TS", "title": "EAs should donate 2% to warm fuzzy causes and 8% to EA causes", "postedAt": "2023-06-02T02:45:52.908Z", "htmlBody": "<figure class=\"image image_resized\" style=\"width:60.16%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mBTvWNj9EXxyMM9TS/tm7ocdanvlfxuwxnqwnb\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mBTvWNj9EXxyMM9TS/glplsoyu7eniixt1ephq 99w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mBTvWNj9EXxyMM9TS/ypgmli6ww5062ucfdqwo 179w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mBTvWNj9EXxyMM9TS/wdce3qkzcd7rroklccy3 259w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mBTvWNj9EXxyMM9TS/we43eu6qdfohfaij0tzk 339w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mBTvWNj9EXxyMM9TS/b4jkm3biyixbimhwumvz 419w\"></figure><p>One criticism EA gets all the time is that we're coldhearted borg-like cost benefit-obsessed utility maximizers. Personally, I <i>like</i> that about EA, but I see huge value in being, and being perceived as, warm and fuzzy and hospitable.</p><p>Over at LessWrong, <a href=\"https://www.lesswrong.com/posts/ejxwraMP5ye7Bgmpm/things-i-learned-by-spending-five-thousand-hours-in-non-ea\">jenn</a> just wrote an insightful post about her top four lessons from 5,000 hours working at a non-EA charity: the importance of long-term reputation, cooperation, slack, and hospitality.</p><p>Here, I am proposing a modification to the EA norm of a 10%-of-income annual donation to an EA-aligned/effective charity. We should modify that standard to promote donating 8% of income to EA-aligned/effective charities, and 2% to charities that are local, feel-good, or something we're passionate about or identify with on a personal or cultural level.</p><p>As an example, if you make $80,000/year, you might consider donating $6,400 to Givewell and $1,600 to the local food bank. If you work as an employee of an EA-aligned organization (so 40 hours of direct work per week), you might consider doing 4-5 hours/week of volunteering to help the homeless.</p><p>Here are some reasons why I think this is a good idea:</p><ul><li><a href=\"https://www.charitychoices.com/page/how-much-given-whom-what\">The average American donates about 2% of their income to charity.</a> This new standard means that the 8% we'd donate to EA causes is <i>over and on top of</i> the amount most people donate. That means EA is less likely to be perceived as clawing away donors from other charities in a zero-sum charity competition. Instead, it's encouraging people to donate more - growing the pie.</li><li>It makes EA friendlier and more cooperative with value systems that are different from our own.</li><li>It boosts our reputation with people in our social and cultural network.</li><li>It gives participants in EA an outlet to get their need for warm-and-fuzzy feelings met.</li><li>It gives a perception of slack - instead of EA being associated with a sort of stringent \"no room for compromise, the stakes are too great\" perspective, EA can project the \"there is so much good we can do in the world\" message that we actually mean, in a way that connects symbolically for the average person who's not an EA.</li><li>It makes it possible to tell a combination of stories about the work we do in the world. Taking action locally for the good of our own community is often easier to see and feel and talk about at the dinner table than giving anonymous-feeling donations to global health institutions or X-risk research groups.</li></ul><p>If you prefer, you could simply add 2%-of-income on top of the 10% Giving What We Can pledge, or do whatever combination makes sense for your situation. In fact, I think it's probably best if we treat 2%/8% as a rough anchoring benchmark, while encouraging people to pick the blend that makes sense to them. Encouraging more individual choice and less adherence to a potentially rigid-seeming rule, while still having an anchoring point so the commitment means something, seems good for EA.</p><p>If we adopt this standard, I suggest we find additional ways to frame it besides the coldhearted-sounding rules-and-percentages manner I'm describing it here. Rather than \"we advocate giving 2% locally and 8% to effective charities, mainly for perceptions reasons,\" I would suggest explaining this rule with a qualitative and friendly-sounding statement like \"we try to mix our donations and efforts to help our local communities while also working on the world's biggest problems.\"</p>", "user": {"username": "AllAmericanBreakfast"}}, {"_id": "CTw58TXSaLSuG6BCg", "title": "Dreams of \"Mathopedia\"", "postedAt": "2023-06-02T01:30:05.034Z", "htmlBody": "", "user": {"username": "NicholasKross"}}, {"_id": "FNGcnxAjezjZQqSzv", "title": "Safe AI and moral AI", "postedAt": "2023-06-01T21:18:53.821Z", "htmlBody": "<p>[Note: This post is an excerpt from a longer paper, written during the first half of the Philosophy Fellowship at the <a href=\"https://www.safe.ai/\">Center for AI Safety</a>. This post is something of a companion piece to <a href=\"https://www.lesswrong.com/posts/gbNqWpDwmrWmzopQW/is-deontological-ai-safe-feedback-draft\">my deontological AI post</a> on LessWrong; both were originally written as parts of a single paper. (There's a small amount of overlap between the two.) As with that draft, questions and feedback welcome!]</p><h3>1. Introduction</h3><p>Two goals for the future development of AI stand out as desirable:</p><ul><li>First, advanced AI should behave <strong>morally</strong>, in the sense that its decisions are governed by appropriately chosen ethical norms.&nbsp;</li><li>Second, advanced AI should behave <strong>safely</strong>, in the sense that its decisions shouldn\u2019t unduly harm or endanger humans.</li></ul><p>These two goals are often viewed as closely related. Here I'll argue that, in fact, safe AI and morally aligned AI are importantly distinct targets, and that avoiding large-scale harms should take precedence in cases where the two conflict.</p><p>I'll start by sketching frameworks for thinking about moral alignment and safety separately. I'll then discuss how these properties can come apart, and I'll give four reasons for prioritizing safety.</p><h3>2. What morally aligned AI would be</h3><p>Phrases like \u201cmorally aligned AI\u201d have been used in many ways. Any system that deserved such a label would, I think, at least have to satisfy certain minimal conditions. I suggest the following. An AI is morally aligned only if it possesses a set of rules or heuristics&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathcal{M}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">M</span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>&nbsp;such that:</p><ul><li>[<strong>Applicability</strong>] Given an arbitrary prospective behavior in an arbitrary (real-world) context,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathcal{M}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">M</span></span></span></span></span></span></span></span></span>&nbsp;can in principle determine how choiceworthy that behavior is in that context, and can in practice at least approximate this determination reasonably correctly and efficiently.</li><li>[<strong>Guidance</strong>] The AI\u2019s behavior is guided to a large degree by&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathcal{M}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">M</span></span></span></span></span></span></span></span></span>. (E.g., in particular, if a behavior is strongly (dis)preferred by&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathcal{M}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">M</span></span></span></span></span></span></span></span></span>, the AI is highly (un)likely to select that behavior.)</li><li>[<strong>Morality</strong>] The rules or heuristics comprising&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathcal{M}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">M</span></span></span></span></span></span></span></span></span>&nbsp;have a good claim to being called moral. (E.g., because they issue from a plausible moral theory, or because they track common moral intuitions.)</li></ul><p>Let me say a bit more on each of these points.</p><p>Re: [<strong>Applicability</strong>], there are two desiderata here. The first is the idea that an aligned AI should be able to morally evaluate almost any action it might take, not just a limited subset of actions. It\u2019s easy to see why this is desirable. We expect an aligned AI to do the morally choiceworthy thing nearly all the time (or at least to have a clear idea of what\u2019s morally choiceworthy, for the purposes of balancing morality against other considerations). If it can\u2019t morally evaluate almost any prospective action, then it can\u2019t reliably fulfill this expectation.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9nlpeuedz7t\"><sup><a href=\"#fn9nlpeuedz7t\">[1]</a></sup></span>&nbsp;&nbsp;</p><p>For similar reasons, it\u2019s not enough that the AI has some evaluation procedure it could follow in theory. A procedure that takes a galaxy\u2019s worth of computronium and a billion years to run won\u2019t do much good if we expect aligned action on human timescales using modest resources\u2014as we presumably will, at least for the foreseeable future. Even if the true moral algorithm is prohibitively costly to run, then, an aligned AI needs an approximation method that\u2019s accurate, speedy and efficient enough for practical purposes.</p><p>Re: [<strong>Guidance</strong>], the idea is that alignment requires not just <i>representations</i> of moral choiceworthiness, but also action <i>steered</i> by these representations. I think it\u2019s sensible to remain agnostic on whether an aligned AI should always choose the morally optimal action, or whether moral considerations might only be one prominent decision input among others. But the latter seems like the weakest acceptable condition: an AI that assigned weights of, say,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0.2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.2</span></span></span></span></span></span></span>&nbsp;to morality and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0.8\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.8</span></span></span></span></span></span></span>&nbsp;to resource-use efficiency wouldn\u2019t count as aligned.</p><p>Re: [<strong>Morality</strong>], the idea is that not just any set of action-guiding rules and heuristics are relevant to alignment; the rules must also have some sort of ethical plausibility. (An AI that assigned maximum choiceworthiness to paperclip production and always behaved accordingly might satisfy [<strong>Applicability</strong>] and [<strong>Guidance</strong>], but it wouldn\u2019t count as morally aligned.)&nbsp;</p><p>I think there are many reasonable understandings of what ethical plausibility might amount to, and I want to cast a wide net. An AI could, for instance, instantiate [<strong>Morality</strong>] if it behaved in accordance with a widely endorsed (or independently attractive) moral theory, if it was trained to imitate commonsense human moral judgments, or if it devised its own (perhaps humanly inscrutable) moral principles by following some other appropriate learning procedure.</p><h3>3. What safe AI would be</h3><p>I said above that an AI counts as safe if its behavior doesn\u2019t unduly harm or endanger humans (and, perhaps, other sentient beings). It\u2019s of particular importance for safety that an AI is unlikely to cause an extinction event or other large-scale catastrophe.</p><p>Safety in this sense is conceptually independent of moral alignment. A priori, an AI\u2019s behavior might be quite safe but morally unacceptable. (Imagine, say, a dishonest and abusive chatbot confined to a sandbox environment where it can only interact with a small number of researchers, who know better than to be bothered by its insults.) Conversely, an AI might conform impeccably to some moral standard\u2014perhaps even to the true principles of objective morality\u2014and yet be prone to unsafe behavior. (Imagine a consequentialist AI which sees an opportunity to maximize expected utility by sacrificing the lives of many human test subjects.)</p><p>The qualifier \u2018unduly\u2019 is important to the notion of safety. It would be a mistake to insist that a safe AI can never harm sentient beings in any way, under any circumstances. For one, it\u2019s not clear what this would mean, or whether it would permit any activity on the AI\u2019s part at all: every action causally influences many events in its future light cone, after all, and some of these events will involve harms in expectation. For another, I take it that safety is compatible with causing some kinds of harm. For instance, an AI might be forced to choose between several harmful actions, and it might scrupulously choose the most benign. Or it might occasionally cause mild inconvenience on a small scale in the course of its otherwise innocuous activities. An AI that behaved in such ways could still count as safe.</p><p>So what constitutes \u2018undue\u2019 harm? This is an important question for AI engineers, regulators and ethicists to answer, but I won\u2019t address it here. For simplicity I\u2019ll focus on especially extreme harms: existential risks which threaten our survival or potential as a species (\u201cx-risks\u201d), risks of cataclysmic future suffering (\u201cs-risks\u201d), and the like. An AI which is nontrivially likely to cause such harms should count as unsafe on anyone\u2019s view.</p><p>You might wonder whether it makes sense to separate safety from moral considerations in the way I\u2019ve suggested. A skeptical argument might run like this:&nbsp;</p><blockquote><p>If an AI is morally aligned, then its acts are morally justifiable by hypothesis. And if its acts are morally justifiable, then any harms it causes are all-things-considered appropriate, however offputtingly large they may seem. It would be misguided to in any way denigrate an act that\u2019s all-things-considered appropriate. Therefore it would be misguided to denigrate the behavior of a morally aligned AI by labeling it 'unsafe'.</p></blockquote><p>But this argument is mistaken for several reasons. Most obviously, the first premise is false. This is clear from the characterization of alignment in the previous section. While a morally aligned AI is guided by rules with <i>a good claim to being called moral</i>, these rules need not actually reflect objective morality. For instance, they might be rules of a popular but false moral theory. So moral justifiability (in some plausible moral framework) doesn\u2019t entail all-things-considered acceptability.</p><p>The second premise is also doubtful. Suppose for the sake of argument that our AI is aligned with the true principles of objective morality, so that the earlier worries about error don\u2019t apply. Even so, from the fact that an act is objectively morally justified, it doesn\u2019t obviously follow that the act is ultima facie appropriate and rationally unopposable. As Dale Dorsey writes: \u201c[T]he fact that a given action is required from the moral point of view does not by itself settle whether one ought to perform it, or even whether performing it is in the most important sense permissible... Morality is one way to evaluate our actions. But there are other ways, some that are just as important, some that may be more important\u201d ([Dorsey 2016], 2, 4). For instance, we might legitimately choose not to perform a morally optimal act if we have strong prudential or aesthetic reasons against doing so.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsh26ekffom\"><sup><a href=\"#fnsh26ekffom\">[2]</a></sup></span></p><p>Perhaps more importantly, even if objective moral alignment did entail all-things-considered rightness, we won\u2019t generally be in a position to <i>know</i> that a given AI is objectively morally aligned. Our confidence in an AI\u2019s alignment is upper-bounded by our confidence in the conjunction of several things, including: (1) the objective correctness of the rules or heuristics with which we aimed to align the AI; (2) the reliability of the process used to align the AI with these rules; (3) the AI\u2019s ability to correctly apply the rules in concrete cases; and (4) the AI\u2019s ability to correctly approximate the result of applying the rules in cases where it can\u2019t apply them directly. It\u2019s implausible that we\u2019ll achieve near-certainty about all these things, at least in any realistic near-term scenario. So we won\u2019t be able to use the skeptic\u2019s reasoning to confidently defend any particular AI behavior. In particular, if an AI threatens us with extinction and we\u2019re inclined to deem this bad, it will be at least as reasonable to question the AI\u2019s successful moral alignment as to doubt our own moral judgments.</p><h3>4. Safety first</h3><p>On this picture of moral alignment and safety, the two outcomes can come apart, perhaps dramatically. In situations where they conflict, which should we prioritize? Is it better to have an impeccably moral AI or a reliably safe one?</p><p>Here are four reasons for putting safety first.</p><ul><li>First, safety measures are typically <strong>reversible</strong>, whereas the sorts of extreme harms I\u2019m concerned with are often <strong>irreversible</strong>. For instance, we can\u2019t undo human extinction. And we won\u2019t be able to stop an AI that gains a decisive advantage and uses its power to lock in a prolonged dystopian future. Even if you\u2019re willing in principle to accept all the consequences of empowering a morally aligned AI, you should be at least a little uncertain about whether an AI that might take these actions is indeed acting on the correct moral principles. So, at the very least, you should favor safety until you\u2019ve eliminated as much of your uncertainty as possible.</li><li>Second, as argued above, <strong>it\u2019s unclear that what\u2019s morally best must be all-things-considered best</strong>, or even all-things-considered permissible. Suppose it would be morally right for an AI to bring about the end of humanity. We might nevertheless have ultima facie compelling non-moral reasons to prevent this from happening: say, because extinction would prevent our long-term plans from coming to fruition, because our species\u2019 perseverance makes for an incomparably great story, or because certain forms of biological or cognitive diversity have intrinsic non-moral value.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa7cmmkcley\"><sup><a href=\"#fna7cmmkcley\">[3]</a></sup></span>&nbsp;In a similar vein, [Bostrom 2014] considers what ought to happen in a world where hedonistic consequentialism is true, and a powerful AI has the means to convert all human matter into pleasure-maximizing hedonium. Bostrom suggests that a small corner of the universe should be set aside for human flourishing, even if this results in slightly less overall value. \u201cIf one prefers this latter option (as I would be inclined to do) it implies that one does not have an unconditional lexically dominant preference for acting morally permissibly\u201d (220).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreff2yu6kt747c\"><sup><a href=\"#fnf2yu6kt747c\">[4]</a></sup></span></li><li>Third, it\u2019s possible that <strong>moral realism is false</strong> and there are no true moral principles with which to align AI. In this case, whatever (objective) reasons we\u2019d have to obey some set of moral rules presumably wouldn\u2019t be strong enough to outweigh our non-moral reasons for prioritizing safety. (If moral realism is false, then perhaps moral rules have something like the normative force of strong social conventions.) I think it\u2019s reasonable to have some positive credence in moral antirealism. By contrast, it seems certain that we have e.g. prudential reasons to protect humanity\u2019s future. This asymmetry favors safety.</li><li>Fourth, it\u2019s conceivable that we\u2019d have <strong>moral reason to protect humanity\u2019s interests</strong> even against an AI which we took to be ethically exemplary. In \u201cThe Human Prejudice\u201d, Bernard Williams has us imagine \u201cbenevolent and fairminded and farsighted aliens [who] know a great deal about us and our history, and understand that our prejudices are unreformable: that things will never be better in this part of the universe until we are removed\u201d ([Williams 2006], 152). Should we collaborate with the aliens in our own eradication? If one thinks that morality begins and ends with universal principles applicable to all rational beings, and if one assumes that the aliens are much better than us at grasping these principles and other relevant facts, it\u2019s hard to see what moral grounds we could give for resistance. But it would be right for us to resist (Williams thinks), so this conception of morality can\u2019t be the whole story. Williams\u2019 suggestion is that something like loyalty to humanity grounds a distinctive ethical imperative for us to defend our species\u2019 interests, even when this conflicts with the demands of the best impartial moral system.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefg2ytgqiqlci\"><sup><a href=\"#fng2ytgqiqlci\">[5]</a></sup></span>&nbsp;On this sort of view, it wouldn\u2019t be straightforwardly obligatory for us to submit to extinction or subjugation by an AI, no matter how impartially good, wise and knowledgeable we took the AI to be. I think a view along these lines is also worth assigning some credence.</li></ul><p>Given a choice between moral-but-possibly-unsafe AI and safe-but-possibly-immoral AI, then, a variety of considerations suggest we should opt for the latter. (At least this is true until we have much more information and have thought much more carefully about our choices.)&nbsp;</p><p>To head off possible confusion, let me be clear about some things I\u2019m not claiming.</p><ol><li>It\u2019s not my view that pursuing moral alignment is pointless, still less that it\u2019s intrinsically harmful and a bad idea. There are excellent reasons to want AIs to behave morally in many scenarios. Many current approaches to moral alignment may be effective ways to achieve that goal; all are worth researching further.</li><li>It\u2019s not my view that safety considerations always trump moral ones, regardless of their respective types or relative magnitudes. An AI that kills ten humans to achieve an extremely important moral goal (say, perfecting a technology that will dramatically improve human existence) would count as unsafe by many reasonable standards, but it doesn\u2019t immediately follow on my view that we shouldn\u2019t design such an AI. I claim only that safety considerations should prevail when sufficiently great risks of catastrophic harm are on the line.</li><li>It\u2019s not my view that moral alignment methods couldn\u2019t possibly produce safe behavior. On the contrary, the space of plausible moral rules is large, and it would be a surprise if it contained only principles that might jeopardize human survival.&nbsp;</li></ol><h3>5. An easy solution?</h3><p>A final thought: suppose that&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathcal{S}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.036em;\">S</span></span></span></span></span></span></span></span></span>&nbsp;is a set of rules and heuristics that implements your favorite collection of safety constrains. (<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathcal{S}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.036em;\">S</span></span></span></span></span></span></span></span></span>&nbsp;might consist of principles like \u201cNever kill people\u201d, \u201cNever perform acts that cause more than&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span>&nbsp;dolors of pain\u201d, or \u201cAlways obey instructions from designated humans\u201d.) Now take an AI equipped with your preferred set of moral rules&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathcal{M}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">M</span></span></span></span></span></span></span></span></span>&nbsp;and add&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathcal{S}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.036em;\">S</span></span></span></span></span></span></span></span></span>&nbsp;as a set of additional constraints, in effect telling the AI to do whatever&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathcal{M}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">M</span></span></span></span></span></span></span></span></span>&nbsp;recommends unless this would result in a relevant safety violation. (In these cases, the AI could instead choose its most&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathcal{M}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">M</span></span></span></span></span></span></span></span></span>-preferred safe option.) Wouldn\u2019t such an AI be both safe and morally aligned by definition? And doesn\u2019t this show that there\u2019s a straightforward way to achieve safety via moral alignment, contrary to what I\u2019ve claimed?</p><p>Unfortunately not. Finding a reasonable way to incorporate absolute prohibitions into a broader decision theory is a difficult problem about which much has been written (e.g. [Jackson &amp; Smith 2006], [Aboodi et al. 2008], [Huemer 2010], [Lazar &amp; Lee-Stronach 2019]). One tricky issue is risk. We want to prohibit our AI from performing unduly harmful acts, but how should we handle acts that merely have some middling risk of unsafe outcomes? A naive solution is to prohibit any behavior with a nonzero probability of causing serious harm. But virtually every possible act fits this description, so the naive method leaves the AI unable to act at all. If we instead choose some threshold t such that acts which are safe with probability&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p>t\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&gt;</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span>&nbsp;are permitted, this doesn\u2019t yet provide any basis for preferring the less risky or less harmful of two prohibited acts. (Given a forced choice between causing a thousand deaths and causing human extinction, say, it\u2019s crucial that the AI selects the former.) Also, of course, any such probability threshold will be arbitrary, and sometimes liable to criticism for being either too high or too low.</p><p>Work on these issues continues, but no theory has yet gained wide acceptance or proven immune to problem cases. [Barrington MS] proposes five desiderata for an adequate account: \u201cThe correct theory will prohibit acts with a sufficiently high probability of violating a duty, irrespective of the consequences... but [will] allow sufficiently small risks to be justified by the consequences... It will tell agents to minimize the severity of duty violations... while remaining sensitive to small probabilities... And it will instruct agents to uphold higher-ranking duties when they clash with lower-ranking considerations\u201d (12).</p><p>Some future account might meet these and other essential desiderata. What\u2019s important for my purposes is that there\u2019s no easy and uncontentious way to render an arbitrary moral theory safe by adding prohibitions on harmful behavior.&nbsp;</p><h3>References</h3><p>Aboodi, Ron, Adi Borer and David Enoch. 2008. \u201cDeontology, individualism, and uncertainty: A reply to Jackson and Smith.\u201d <i>Journal of Philosophy</i> 105, 259-272.</p><p>Barrington, Mitchell. MS. \u201cFiltered maximization.\u201d</p><p>Bostrom, Nick. 2014. <i>Superintelligence: Paths, Dangers, Strategies.</i> Oxford: Oxford University Press.</p><p>Bradley, Ben. 2001. \u201cThe value of endangered species.\u201d <i>Journal of Value Inquiry </i>35, 43-58.</p><p>Chisholm, Roderick. 1981. \u201cDefining intrinsic value.\u201d <i>Analysis</i> 41, 99-100.</p><p>Diamond, Cora. 2018. \u201cBernard Williams on the human prejudice.\u201d <i>Philosophical Investigations</i> 41, 379-398.</p><p>Huemer, Michael. 2010. \u201cLexical priority and the problem of risk.\u201d <i>Pacific Philosophical Quarterly</i> 91, 332-351.</p><p>Jackson, Frank and Michael Smith. 2006. \u201cAbsolutist moral theories and uncertainty.\u201d <i>Journal of Philosophy</i> 103, 267-283.</p><p>Lazar, Seth and Chad Lee-Stronach. 2019. \u201cAxiological absolutism and risk.\u201d <i>No\u00fbs</i> 53, 97-113.</p><p>Lemos, Noah. 1994. <i>Intrinsic Value.</i> Cambridge: Cambridge University Press.</p><p>Shulman, Carl and Nick Bostrom. 2021. \u201cSharing the world with digital minds.\u201d In Steve Clarke, Hazem Zohny and Julian Savulescu (eds.), <i>Rethinking Moral Status</i>, Oxford: Oxford University Press, 306-326.</p><p>Williams, Bernard. 2006. <i>Philosophy as a Humanistic Discipline</i> (ed. by A.W. Moore). Princeton, NJ: Princeton University Press.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9nlpeuedz7t\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9nlpeuedz7t\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For concreteness, suppose the AI faces a choice between actions&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A,B,C,D\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span></span></span></span></span>, but it can only evaluate&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>&nbsp;(which it determines to be pretty good) and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>&nbsp;(which it determines to be pretty bad); its moral heuristics are silent about&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"C\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"D\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span></span></span></span></span>. One thing the AI might do in this situation is disregard morality and choose between all four options on some other basis. This clearly won\u2019t lead to reliably aligned behavior. Another strategy is to choose the best option from among those that are morally evaluable. But it\u2019s possible that&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"C\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span></span></span></span></span>&nbsp;or&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"D\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span></span></span></span></span>&nbsp;is much better than&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>, so choosing&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>&nbsp;instead might be very bad.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsh26ekffom\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsh26ekffom\">^</a></strong></sup></span><div class=\"footnote-content\"><p>An example of Dorsey\u2019s illustrating the prudential case: Andrea can either move far away to attend Eastern Private College or stay home and go to Local Big State University. She\u2019ll be able to provide important emotional support for her struggling family if and only if she chooses LBSU. There\u2019s no corresponding moral reason to choose EPC, so morality demands that Andrea stay home. But Andrea has a strong prudential interest in attending EPC\u2014it\u2019s important to her long-held plans for the future\u2014and so it would be all-things-considered appropriate for her to choose EPC.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna7cmmkcley\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa7cmmkcley\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This principle of <i>bonum variationis</i> is associated with Leibniz and Brentano. Its recent defenses include [Chisholm 1981], [Lemos 1994], [Scanlon 1998], [Bradley 2001].</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnf2yu6kt747c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreff2yu6kt747c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This passage doesn\u2019t explicitly identify the grounds on which Bostrom prefers continued human existence over moral rightness. A similar issue is raised in [Shulman &amp; Bostrom 2021], with the same conclusion but a somewhat different rationale: here the view is that humans should go on existing in order \u201cto hedge against moral error, to appropriately reflect moral pluralism, to account for game-theoretic considerations, or simply as a matter of realpolitik\u201d (321).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fng2ytgqiqlci\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefg2ytgqiqlci\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For illuminating discussion of Williams\u2019 views on this subject, see [Diamond 2018].</p></div></li></ol>", "user": {"username": "William D'Alessandro"}}, {"_id": "5oTr4ExwpvhjrSgFi", "title": "Things I Learned by Spending Five Thousand Hours In Non-EA Charities", "postedAt": "2023-06-02T09:37:24.174Z", "htmlBody": "<p>From late 2020 to last month, I worked at grassroots-level non-profits in operational roles. Over that time, I\u2019ve seen surprisingly effective deployments of strategies that were counter-intuitive to my EA and rationalist sensibilities.</p><p>I spent 6 months being the on-shift operations manager at one of the five largest food banks in Toronto (~50 staff/volunteers), and 2 years doing logistics work at Samaritans (fake name), a long-lived charity that was so multi-armed that it was basically operating as a supplementary social services department for the city it was in(~200 staff and 200 volunteers). Both orgs were well-run, though both dealt with the traditional non-profit double whammy of being underfunded and understaffed.</p><p>Neither place was super open to many EA concepts (explicit cost-benefit analyses, the <a href=\"https://forum.effectivealtruism.org/topics/itn-framework\">ITN framework</a>, geographic impartiality, the general sense that talent was the constraining factor instead of money, etc). Samaritans in particular is a spectacular non-profit, despite(?) having basically <i>anti</i>-EA philosophies, such as:</p><ul><li>Being very localist; Samaritans was established to help residents of the city it was founded in, and now very specialized in doing that.</li><li>Adherence to faith; the philosophy of <a href=\"https://en.wikipedia.org/wiki/Catholic_Worker_Movement\">The Catholic Worker Movement</a> continues to inform the operating choices of Samaritans to this day.</li><li>A big streak of techno-pessimism; technology is first and foremost seen as a source of exploitation and alienation, and adopted only with great reluctance when necessary.</li><li>Not treating money as fungible. The majority of funding came from grants or donations tied to specific projects or outcomes. (This is a system that the vast majority of nonprofits operate in.)</li><li>Once early on I gently pushed them towards applying to some EA grants for some of their more EA-aligned work, and they were immediately turned off by the general vibes of EA upon visiting some of its websites. I think the term \u201cborg-like\u201d was used.</li></ul><p>Over this post, I\u2019ll be largely focusing on Samaritans as I\u2019ve worked there longer and in a more central role, and it\u2019s also a more interesting case study due to its stronger anti-EA sentiment.</p><h2>&nbsp;</h2><h2>Things I Learned</h2><ol><li><a href=\"https://forum.effectivealtruism.org/posts/5oTr4ExwpvhjrSgFi/things-i-learned-by-spending-five-thousand-hours-in-non-ea#1__Long_Term_Reputation_is_Priceless\">Long Term Reputation is Priceless</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/5oTr4ExwpvhjrSgFi/things-i-learned-by-spending-five-thousand-hours-in-non-ea#2__Non_Profits_Shouldn_t_Be_Islands\">Non-Profits Shouldn\u2019t Be Islands</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/5oTr4ExwpvhjrSgFi/things-i-learned-by-spending-five-thousand-hours-in-non-ea#3__Slack_is_Powerful\">Slack is Incredibly Powerful</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/5oTr4ExwpvhjrSgFi/things-i-learned-by-spending-five-thousand-hours-in-non-ea#4__Hospitality_is_Pretty_Important\">Hospitality is Pretty Important</a></li></ol><p>For each learning, I have a section for sketches for EA integration \u2013 I hesitate to call them anything as strong as recommendations, because the point is to give more concrete examples of what it could look like integrated in an EA framework, rather than saying that it\u2019s the correct way forward.</p><h3>&nbsp;</h3><h3>1. Long Term Reputation is Priceless</h3><p>Institutional trust unlocks a stupid amount of value, and you can\u2019t buy it with money. Lots of resources (amenity rentals; the mayor\u2019s endorsement; business services; pro-bono and monetary donations) are priced/offered based on tail risk. If you can establish that you\u2019re not a risk by having a longstanding, unblemished reputation, costs go way down for you, and opportunities way up. This is the world that Samaritans now operate in.</p><p>Samaritans had a much better, easier time at city hall compared to newer organizations, because of a decades-long productive relationship where we were really helpful with issues surrounding unemployment and homelessness. Permits get back to us really fast, applications get waved through with tedious steps bypassed, and fees are frequently waived. And it made sense that this was happening! Cities also deal with budget and staffing issues, why waste more time and effort than necessary on someone who you <i>know </i>knows the proper procedure and will ethically follow it to the letter?</p><p>It\u2019s not just city hall. A few years ago, a local church offered up their recreation space for us to run an emergency winter shelter in \u2013 an <i>incredibly </i>generous move on their part, as using a space as a shelter puts a lot of wear on it. They made the offer only to Samaritans, and would not have made it to organizations that didn\u2019t have good reputations for treating the unhoused well, <i>and</i> for cleaning after themselves when they move out of temporary spaces that were donated to them for use.</p><p>Several companies with good reputations of their own and deep expertise on topics we weren\u2019t as familiar with also approached us to do pro-bono work, both for their staff to get some fuzzies and to improve their own reputation as ethical companies who give back to the community.</p><p>Samaritans also leveraged their reputation proactively. Recently, we established a respectful and novel way of supporting the unhoused in our city. The solution (in short, tiny homes on public land) would have been deadlocked for possibly years if the organization\u2019s name didn\u2019t grease the wheels significantly on many fronts. The city was eager to work with us, the NIMBYs were reluctant to come out against us, and the city\u2019s unhoused community had a level of trust in us that made them willing to leave their established encampments.</p><p>I can see how it\u2019s unfair for Samaritans to have gotten this kind of special treatment from everyone, and it\u2019s the exact same dynamic that leads to entrenchment of older and less efficient institutions over newer ones. However, these dynamics are inevitable in any system or industry, and hard to overcome with brute cash. I am not very thrilled about having this take, but I think it may be worth figuring out how to gain similar kinds of advantage or leverage these dynamics for EA causes.</p><p><strong>Sketches for EA integration</strong></p><p>Thinking of money as a universal means of exchange <i>slightly</i> less. Money can buy lots of goods and services, but not all of them. I know it sucks for nerds to hear that reputation (popularity) is important but I think it\u2019s unfortunately a real thing, and not just on the margin.</p><p>Thinking more about what actions and trade-offs EA organizations should take such that they\u2019re beloved institutions in 25 years\u2019 time \u2013 and if such a thing is worth it to pursue.</p><h3>&nbsp;</h3><h3>2. Non-Profits Shouldn\u2019t Be Islands</h3><p>Effective altruists consider the overall neglectedness of a cause area in terms of total field capacity, but when it\u2019s time to donate, they support specific charities within that space. This approach makes sense, but it risks missing the bigger picture. Multiple organizations working on parts of the same problem can achieve more collectively than one big charity alone.</p><p>The non-profits I worked at communicated closely with community partners. This is good for the people we help. For example, knowing which shelters still have beds open (and what restrictions they impose around couples, pets, and drug use) when our own beds are at capacity so we can send people with very limited means for travel to places that can take them. Or which nearby food banks are open late if people arrive 5 minutes after we closed.</p><p>It\u2019s also good for us, the service operators. It leads to better resource allocation and decision making on a community-wide scale. People who need the help of one charitable organization often need the help of other ones (e.g. food banks, affordable housing, job search support, and possibly translation support to access the above). When someone comes to your non-profit for a service, you can direct them to other services that they need.</p><p>When I operated the seasonal tax clinic, I can often see through people\u2019s financial information when participants were eligible for benefits that they are not getting. I was trained in being able to spot this information by another non-profit that was focused on increasing benefits access for all Canadians. Providing assistance for benefits applications was out of scope for the tax clinic, but I was able to integrate a very streamlined path for referring people out to get those additional benefits at basically zero cost to us. I really don\u2019t want to sound like I\u2019m bragging here; it\u2019s less that I was able to do that as much as there was a concerted effort by all community organizations to cross-train and communicate with each other to maximize the help that we can all provide to the community with the least amount of effort.</p><p>We were also able to take advantage of specialization, such as providing supervised injection sites for harm reduction purposes with staff trained by the non-profit that was focused on harm reduction specifically. Having another org provide training once every month or two was a lot more cost effective than having to have our own specialists.</p><p><strong>Sketches for EA integration</strong></p><p>Evaluate single charities slightly less, and [non-profit + government] networks for specific regions or cause areas slightly more, and think of possibly shoring up weak links. When evaluating cause areas and how to best approach them, think about potential groupings of charities instead of single charities.</p><p>One question I often see on EA grant applications is something along the lines of \u201cif we gave you 10x the money you requested, what would you do with it?\u201d I think another useful question to ask could be something like, \u201cwhat is your fantasy partner/complement organization?\u201d Lots of nonprofits are doing their thing and they have no intentions to expand to do an entire other thing, and if you give them more money they will just do more of their own thing. But I\u2019d bet that a lot of them have recurring problems just outside of their own scope that they would love having another org to refer out to, and a sense of what those problems are could be useful for the EA community as a while.</p><h3>&nbsp;</h3><h3>3. Slack is Powerful</h3><p>This was a really interesting lesson from Samaritans. Because we had staff for what were basically 20 semi-autonomous organizations doing almost uncorrelated things, we ended up with a lot of organizational <a href=\"https://www.lesswrong.com/posts/yLLkWMDbC9ZNKbjDG/slack\">slack</a>. Different parts of the organization underwent crunch at different times, and people were temporarily re-allocated to smooth out the spikiness regularly. If you\u2019re an organization of like 20 people and you can occasionally, with minimal friction, harness the efforts of 20 more people who are aligned with you, you can do some really significant barn-raising moves that you can\u2019t if you were just an organization with 25 FTEs.</p><p>The coolest example I participated in was when 30 people from various departments showed up to help move an emergency shelter we were running from one location to another. The work included deep cleaning the previous space and the new space, doing last minute construction work in the new space, packing and unpacking a bunch of cot beds, sleeping mats and bedding, a boatload of laundry, re-assembling all of the beds and making them, moving in all of the kitchen supplies and sundry, setting up the phone system, and dozens of other miscellaneous tasks. What would have taken a week to do if it was just shelter staff ended up taking only two days, which was great for the people who were depending on us for shelter. In addition to this, the shelter folks were relatively well rested despite the ordeal and able to continue their work without burning out.</p><p><strong>Sketches for EA integration</strong></p><p>Thinking more about what sorts of resources can be constrained besides money. I know, I know, the EA thing is about how money beats other interventions in like 99.9% of cases, but I do think that there could be some exceptions \u2013 especially when it comes to staffing.</p><p>Creating a group of EA free agents that can be allocated/rented to EA-aligned non-profits? One thing that might make sense is to have lawyers/payroll/HR people on retainer on hand to consult with fledgling nonprofits who aren\u2019t big enough to hire them full-time.</p><h3>&nbsp;</h3><h3>4. Hospitality is Pretty Important</h3><p>People won\u2019t use your service if it seems impersonal and cold, even if, like, their livelihoods depend on it? Samaritans had a policy where we try to help people as much as we can and say no as infrequently as possible. As a result, people line up for up to six hours a day, or come back three or four days in a row, to use Samaritan\u2019s services. While we\u2019re drowning in this demand, competing service providers which are as close as a 5 minute walk away had no wait times.</p><p>This didn\u2019t really make sense to me as we were helping with some pretty urgent things. Things like emergency benefits applications so a person can make rent and not get evicted, or helping new refugees find jobs before their savings run out.</p><p>Despite all this, trying to refer people out was a pretty futile practice. A lot of them will come back a few days later and say stuff like \u201cI\u2019m here because Samaritans are the only ones that will actually listen to my problems\u201d.</p><p>From this, I\u2019ve realized that it\u2019s actually really important to make the people you help feel comfortable \u2013 especially since a lot of them likely had terrible experiences with other service providers previously.</p><p><strong>Sketches for EA integration</strong></p><p>Have nonprofits that are public-facing, and EA infrastructure orgs, care more about customer service.</p><p>This take is so basic that I honestly feel a little dumb giving it. But honestly yeah, I now think that organizations that are interfacing directly with the public can increase uptake pretty significantly by just strongly signalling that they care about the people that they are helping, to the people that they are helping. Be warm, caring, convivial presences.</p><h2>&nbsp;</h2><h2>Final Thoughts</h2><p>Effective altruism aims to avoid the pitfalls of human brains and traditional charities by using optimized, data-driven approaches as much as possible. I wouldn\u2019t be surprised if a lot of EAs see my takes here as a slippery slope to warm glow thinking and wanton spending that needs to be protected against.</p><p>My goal is that this post provides insight into why many relatively well run, non-EA organizations adopt these strategies. They recognize that reputation, relationships and culture, while seemingly intangible, can become viable vehicles for realizing impact. And when implemented responsibly, based on evidence, I think there\u2019s room for compatibility with EA.</p><p>To be clear, I don\u2019t actually expect that most of the strategies outlined here will pass muster when thrown into the cost-benefit analysis machine, most of the time. On the other hand, if there exists no marginal case in which they are useful at all, that would also be pretty surprising to me.</p><p>I hope that it\u2019s clear that I am not aiming to strong-arm EA towards these practices; I only want to bring them to the community\u2019s attention because I think they\u2019re pretty neat. Better understanding of diverse approaches will only benefit this community, making it stronger, wiser and more able to do the most good.</p><p>Thanks for reading &lt;3</p>", "user": {"username": "jenn"}}, {"_id": "goYTp3CyLA4dnL2kN", "title": "Catastrophic Risks from Unsafe AI: Navigating a Tightrope Scenario (Ben Garfinkel, EAG London 2023)", "postedAt": "2023-06-02T09:59:20.948Z", "htmlBody": "<h1><strong>Summary</strong></h1><p>Ben Garfinkel gave a talk at Effective Altruism Global: London in May 2023 about navigating risks from unsafe advanced AI. In this talk, he:</p><ol><li>argued we should most focus on \u201ctightrope\u201d scenarios where careful actions can lead to the safe development and use of advanced AI;&nbsp;</li><li>outlined&nbsp;<i>emergence of dangerous capabilities</i> and&nbsp;<i>misalignment</i> as two key threats and described how they might manifest in a concrete scenario;&nbsp;</li><li>described&nbsp;<i>safety knowledge</i>,&nbsp;<i>defences</i>, and&nbsp;<i>constraints</i> as three categories of approaches that could address these threats; and&nbsp;</li><li>introduced a metaphor for reducing risks by&nbsp;<i>compressing the risk curve</i> to lower the peak risk from unsafe AI, and push down risk faster.</li></ol><p>Ben framed the talk as being \u201cunusually concrete\u201d about framing threats from unsafe AI and recommendations for approaches to reduce risks. I (Alexander) found this very helpful and it gave me several tools for thinking about and discussing AI safety in a practical way<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5dprve4a5f\"><sup><a href=\"#fn5dprve4a5f\">[1]</a></sup></span>, so I decided to write up Ben's talk.</p><p>The remainder of this article seeks to reproduce Ben\u2019s talk as closely as possible. Ben looked over it quickly and judged it an essentially accurate summary, although it may diverge from his intended meaning or framing on some smaller points. He also gave permission to share the talk slides publicly (<a href=\"https://docs.google.com/presentation/d/1k0XEgNvJqC6E-42AHbcaOtnWEaivjnvcFhQmELmlW5M/edit#slide=id.p\">\"Catastrophic Risks From Unsafe AI\", Garfinkel, 2023</a>). After publishing this article, a video of the talk was uploaded: <a href=\"https://www.youtube.com/watch?v=h_i2qfVAfus\">\"Catastrophic risks from unsafe AI\"</a>.</p><h1><strong>We should focus on tightrope scenarios</strong></h1><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/grntbadq8roib8ahboyg\" alt=\"Alt text: Figure 1. Images illustrating three different scenarios for risks from advanced AI, and text summarising each scenario. The first image is of a shipwreck near a waterfall, with the text \u2018Shipwreck scenario: No point to action; certain catastrophe\u201d. The second image is of a tightrope walker holding a balance beam, standing on a tightrope strung between the two World Trade Center buildings, with the text \u2018Tightrope scenario: Must act to navigate risks and avoid catastrophe\u201d. The third image is of a young child about to release a bowling ball down a bowling lane with bumpers on either side of the lane, with the text \u201cBumper scenario: No need for action; certain safety\u201d.\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/svmcklzylzeha7rxr3g2 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/fmte8a5zzr7z7ewff197 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/sgifhocaxwasltewoahg 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/p2enwzpyia4zfmiwex55 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/ixy0t1vbtwhuvemevage 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/rrwu9tplxt7gtp48ntfl 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/zmnahe8rsyf1csfzctae 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/mqdpkzukbrzrd4acdmay 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/cncndwtqygbdwprnppsa 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/hsxrw8us8jcdf0t9vkcq 960w\"><figcaption>Figure 1. Three different scenarios describing possible futures and risks from advanced AI.</figcaption></figure><p>One useful way of thinking about risks from advanced AI is to consider three different scenarios.</p><ol><li>The risks from AI are overstated; it\u2019s almost certain that we will avoid catastrophic outcomes. This is the \u201cbumper scenario\u201d, envisioning bumpers on the side of a bowling lane that stops the ball from falling into the gutters on either side.</li><li>The risks from AI are unsolvable; it\u2019s almost certain that we will experience catastrophic outcomes. This is the \u201cshipwreck\u201d scenario, envisioning a hopelessly uncontrollable ship drifting towards a reef that will surely destroy it.</li><li>The risks from AI are significant and could lead to catastrophe, but in principle we can navigate the risks and avoid catastrophic outcomes. This is the \u201ctightrope\u201d scenario, envisioning a person walking on a tightrope between two cliffs over a vast chasm.</li></ol><p>It is most worthwhile to focus on tightrope scenarios over bumper or shipwreck scenarios, because tightrope scenarios are the only ones in which we can influence the outcome. This holds true even if the probability of being in a tightrope scenario is unlikely (because you think it is very likely we are in a bumper or shipwreck scenario). This means being clear-eyed about the risks from advanced AI, and identifying &amp; enacting strategies to address those risks so we can reach the other side of the chasm.</p><h1><strong>Recap: how are AI models trained?</strong></h1><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/uqixqnrlqpm8juz2o4qm\" alt=\"Alt text: Figure 2. A simplified two step model for training a large language model. Each step is named and there are four short statements summarising what happens at each step. Step 1: Cranking the handle. Elements: Data is gathered and fed to the model; The model learns to imitate the behaviours and capabilities observed in the data; Mostly automated process once architecture and data are determined; Requires huge amounts of computation (90-99% of total compute). Step 2: Feedback. Elements: Feedback is provided to the model to influence its behaviour towards desired outcomes; Can amplify or suppress existing capabilities; Mostly bespoke process, requiring multiple methods and expensive high-quality data; Relatively small amounts of computation (1-10% of total compute)\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/yfvkrrifjfr0rufmbdfs 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/ioibskds21s80cktadrh 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/tpppsmsorxyjff7ugaty 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/ql5qlyzy2no9bdbufgjv 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/bzvtfdjglbht2ebnm1jt 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/tzct5snqfalevyw7zfc6 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/fabj4nrahbrtxnln7hgr 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/pztvfkq9s6xvokhoage9 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/rne0lqiiwfaobviwxz7r 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/ngcmkewro90oidqyxa02 960w\"><figcaption>Figure 2. A very simplified process for training a large language model, one of the most capable kinds of AI models as of 2023.</figcaption></figure><p>Here is a very simplified process for training an AI model<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjdkzigyyfv\"><sup><a href=\"#fnjdkzigyyfv\">[2]</a></sup></span>, focused on large language models (LLM), because these are the models that in 2023 are demonstrating the most sophisticated capabilities. This simplified process can be useful to establish a shared language and understanding for different threats that can emerge, but it\u2019s important to note that there are often several more steps (especially different kinds of feedback) in training an LLM, and other AI systems may be trained differently.</p><h2>Step 1: Cranking the Handle</h2><ol><li>A huge amount of data is gathered and fed to the model. The model uses a huge amount of computational power to learn to imitate the behaviours and capabilities observed in the data.&nbsp;</li><li>The more varied the data, the broader the behaviours and capabilities that can be imitated, including those that were not explicitly intended. For example, current LLMs can generate code, poetry, advice, or legal analysis based on the data it was trained on, despite no engineer directing it to produce these behaviours or capabilities.&nbsp;</li><li>This step is termed \u2018cranking the handle\u2019 because once the architecture and data are determined, the process is mostly automatic and relies primarily on providing computational power for training the model on the data.&nbsp;</li><li>The result of this process is sometimes called a base model or foundation model.</li></ol><h2>Step 2: Feedback</h2><ol><li>The model receives feedback in order to influence its behaviour to be more consistent with desired responses. This can happen several times using different methods, such as providing examples of the types of desired behaviours &amp; capabilities, incorporating information from other AI models, or seeking human ratings of model behaviour.</li><li>One common method is for humans to provide feedback on model behaviour (e.g, usefulness, politeness, etc). The feedback is then incorporated into the model so it behaves more in line with feedback. This style of feedback is called Reinforcement Learning with Human Feedback (RLHF).</li><li>The goal of this step is to adjust how, when, and whether the system demonstrates certain capabilities and behaviours, based on the feedback it receives. However, it\u2019s important to note that the capabilities and behaviours may still be present in the model.</li></ol><h1><strong>Two threat sources: emergence of dangerous capabilities and misalignment</strong></h1><h2><strong>Emergence of dangerous capabilities</strong></h2><p>During Step 1, cranking the handle, the model trains on diverse data and can develop a wide range of capabilities and behaviours. However, the exact capabilities that emerge from this step can be unpredictable. That\u2019s because the model learns to imitate behaviours and skills observed in the data, including those that were not intended or desired by the developers. For example, a model could manipulate people in conversation, write code to conduct cyber attacks, or provide advice on how to commit crimes or develop dangerous technologies.</p><p>The unpredictability of capabilities in AI models is a unique challenge that traditional software development does not face. In normal software, developers must explicitly program new capabilities, but with AI, new capabilities can emerge without developers intending or desiring them. As AI models become more powerful, it is more likely that dangerous capabilities will emerge. This is especially the case as systems are trained in such a way as to be able to perform increasingly long term, open-ended, autonomous, and interactive tasks, and their performance exceeds human performance in many domains.</p><h2><strong>Misalignment</strong></h2><p>Misalignment is when AI systems use their capabilities in ways that people do not want. Misalignment can happen by an AI model imitating harmful behaviours in the data it was trained on (Step 1). It can also happen through Step 2, Feedback, either straightforwardly (e.g., feedback makes harmful behaviours more likely, such as giving positive feedback to harmful behaviour), or through deceptive alignment. Deceptive alignment is where a model appears to improve its behaviour in response to feedback, but actually learns to hide its undesired behaviour and express it in other situations; this is especially possible in AI systems that are able to reason about whether they are being observed). It is also possible that we don\u2019t understand why an AI exhibits certain behaviours or capabilities, which can mean that it uses those capabilities even when people don\u2019t intend or desire it to do so.</p><p>Current AI models are relatively limited as they rely on short, user-driven interactions and are typically constrained to human skill levels. This may reduce the harmfulness of any behaviour exhibited by the model. However, as AI continues to develop and these limitations are addressed or removed, it is more likely that the effects of misalignment will be harmful.</p><h1><strong>How dangerous capabilities and misalignment can lead to catastrophe</strong></h1><p>Here\u2019s a story to illustrate how dangerous capabilities and misalignment can combine in a way that can lead to catastrophe.</p><h2><strong>A story of catastrophe from dangerous capabilities and misalignment</strong></h2><ol><li>It\u2019s 10 years in the future</li><li>A leading AI company is testing a new AI system</li><li>This AI system can carry out long-term plans and greatly surpasses human skills, marking a significant leap from previously deployed systems&nbsp;</li><li>The AI company detected some dangerous capabilities after \u201ccranking the handle\u201d but believes it has removed them during \u201cfeedback\u201d</li><li>However, the system has learned not to demonstrate these dangerous capabilities during testing</li><li>Some of the dangerous capabilities include persuasion, mimicry, cyber attacks, extortion, weapon design, and AI research &amp; development; in each domain, the AI greatly surpasses human ability</li><li>Many copies of the AI system are deployed. Each has full internet access and many are unmonitored</li><li>The copies of the AI system continue to learn and evolve based on their experiences and interactions, meaning that each copy diverges from the original system over time</li><li>Some of these copies start to use their abilities in harmful ways that initially go unnoticed</li><li>By the time the widespread issues become apparent, it's very difficult to reduce harms because:<ol><li>In some cases, the harm has already occurred</li><li>The AI systems have become deeply integrated into society and are difficult to shut down or replace</li><li>The AI systems have propagated across computer networks and onto other hardware, where it is difficult to find them</li><li>The AI systems could make threats of harm and may be able to carry out those threats</li></ol></li><li>Catastrophe occurs as a result</li></ol><h2><strong>The story, distilled into basic steps</strong></h2><ol><li>In the future people may develop general purpose AI systems with extremely dangerous capabilities</li><li>These dangerous capabilities could be unwanted or even unnoticed</li><li>Systems may then deploy these capabilities even if people don't want them to due to alignment issues</li><li>These alignment issues also might themselves be largely unnoticed or underappreciated due to deceptive alignment</li><li>The result could be an unintended global catastrophe</li></ol><h1><strong>Approaches to address risks</strong></h1><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/b7f4zexj2dpbxkym0wbg\" alt=\"Alt text: Figure 3. An illustration of the article section \u201cApproaches to address risks\u201d. One large black circle \u201cReduced catastrophic risk from AI\u201d is surrounded by three smaller coloured circles: \u201cImproved AI safety knowledge\u201d, \u201cBetter constraints\u201d, \u201cBetter defences\u201d. Specific examples are provided for each approach, which are described in the article text.\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/usiermpu9l3yai8nkljf 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/hk8qpujujdsiingdjqwv 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/uq9ezaapyefvrxlymmxh 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/xdjy9bjvkzp5qlhpi6do 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/gssc1fuuhnysydu68hs0 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/rgrcnq0zo50lzj4ngezz 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/mx26e8gk60i9jbut1vbs 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/btseieu1eutq7rrba7ho 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/uckv3avifttjpi9e3lul 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/goYTp3CyLA4dnL2kN/ctpasoamjvu4ovfdrfiy 960w\"><figcaption>Figure 3. An overview of approaches to address risks from advanced AI, with non-exhaustive examples of strategies for each approach.</figcaption></figure><p>Three categories of approach were proposed that could be used to address risks from unsafe advanced AI: better AI safety knowledge, better defences against misaligned or misused AI with dangerous capabilities, and better constraints on the development and use of AI.</p><h2><strong>Better safety knowledge</strong></h2><ul><li><strong>Model evaluations</strong>: People can reliably identify when an AI system has extremely dangerous capabilities (\"dangerous capability evaluations\") or has a propensity to use them (\"alignment evaluations\").</li><li><strong>General understanding of AI risk</strong>: People generally understand and do not underestimate AI risk. They also recognise that certain development approaches will tend to produce unsafe systems. This may lead to voluntary moratoriums in developing or deploying AI systems.</li><li><strong>Reliably safe methods</strong>: People have identified development and deployment approaches that reliably ensure a sufficient level of safety.</li></ul><h2><strong>Better defences</strong></h2><ul><li><strong>Countermeasures for dangerous capabilities</strong>: People have developed defences against a critical subset of dangerous capabilities (e.g. offensive cyber-capabilities).</li><li><strong>Monitoring model capabilities and behaviour</strong>: People have developed and implemented methods to monitor AI systems and detect dangerous capabilities or early stages of harmful behaviour</li><li><strong>Reliable shutdowns</strong>: People have developed methods and processes to reliably halt AI systems, if harmful behaviour or dangerous capabilities are detected</li></ul><h2><strong>Better constraints</strong></h2><ul><li><strong>Mandated best practices in safety</strong>: Governments require that AI developers and AI users follow best practices in ensuring reliable safety of AI systems, especially for AI systems with potentially dangerous capabilities or where harmful behaviour could cause major problems. One possibility for implementation is through licensing, as is common for drugs, planes, and nuclear reactors.</li><li><strong>Emergency orders</strong>: Governments can rapidly and effectively direct actors not to develop, share, or use AI systems if they detect significant risks of harm.</li><li><strong>Non-proliferation</strong>: Governments successfully limit the number of state or non-state actors with access to resources (e.g. chips) that can be used to produce dangerous AI systems.&nbsp;</li><li><strong>Other international constraints</strong>: States make international commitments to safe development and use of AI systems, comply with non-proliferation regimes or enforce shared best practices domestically. Examples for implementation are agreements with robust (AI-assisted privacy-preserving?) monitoring, credible carrots/sticks, or direct means of forcing compliance (e.g. hardware mechanisms).&nbsp;</li></ul><h1><strong>Combining several approaches into strategies that can reduce catastrophic risk from unsafe AI</strong></h1><p>Over time, the risk from unsafe AI will increase with advancement in the capabilities of AI models. Simultaneously, improvements in AI&nbsp;<i>safety knowledge</i>,&nbsp;<i>defences&nbsp;</i>against misaligned or misused AI with dangerous capabilities, and&nbsp;<i>constraints&nbsp;</i>on the development and use of AI will also increase.</p><p>One toy model for thinking about this is a \u2018risk curve\u2019. According to this model, the pressures that increase and decrease risk will combine so that overall risk will increase to a peak, then return to baseline. A combination of safety approaches could \u201ccompress the curve\u201d, meaning reducing the peak risk of the worst harms, and ensuring that the risk descends back towards baseline more quickly.&nbsp;</p><p>Compressing the curve can be done by reducing the time lag between when the risk emerges and when sufficient protections are established. It is unlikely that any one strategy is sufficient to solve AI safety. However, a portfolio of strategies can collectively provide more robust safeguards, buy time for more effective strategies, reduce future competitive pressures, or improve institutional competence in preparedness.&nbsp;</p><p>Testing and implementing strategies now can help to refine them and make them more effective in the future. For example, proposing safety standards and publicly reporting on companies\u2019 adherence to those standards, even without state enforcement, could be a useful template for later adaptation and adoption by states; judicial precedents involving liability laws for harms caused by narrow AI systems could set an example for dealing with more severe harms from more advanced AI systems.</p><p>Approaches to address risks can trade off against each other. For instance, if&nbsp;<i>safety knowledge&nbsp;</i>and&nbsp;<i>defences</i> are well-progressed, stringent&nbsp;<i>constraints&nbsp;</i>may be less necessary to avoid the worst harms. Conversely, if one approach is lagging or found to be ineffective, others may need to be ramped up to compensate.&nbsp;</p><h1><strong>Conclusion</strong></h1><p>We could be in a \u201ctightrope\u201d scenario when it comes to catastrophic safety risk from AI. In this scenario, there is a meaningful risk that catastrophe could occur from advanced AI systems because of emergent dangerous capabilities and misalignment, if we do not act to prevent it. We should mostly act under the assumption we are in this scenario, regardless of its probability, because it\u2019s the only scenario in which our actions matter.</p><p>Developing, testing, and improving approaches for reducing risks, including safety knowledge, defences, and constraints, may help us to walk the tightrope safely, reducing the impact and likelihood of catastrophic risk from advanced AI.</p><h1><strong>Author statement</strong></h1><p><a href=\"https://www.governance.ai/team/ben-garfinkel\">Ben Garfinkel</a> wrote and delivered a talk titled&nbsp;<i>Catastrophic Risks From Unsafe AI: Navigating the Situation We May or May Not Be In</i> on 20 May 2023, at Effective Altruism Global: London.&nbsp;</p><p>I (Alexander Saeri) recorded audio and took photographs of some talk slides, and wrote the article from these recordings as well as an AI transcription of the audio.&nbsp;</p><p>I shared a draft of the article with Ben Garfinkel, who looked over it quickly and judged it an essentially accurate summary, while noting that it may diverge from his intended meaning or framing on some smaller points.</p><p>I used GPT-4 for copy-editing of text that I wrote, and also to summarise and discuss some themes from the transcription. However, all words in the article were written by me, with the original source being Ben Garfinkel.&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5dprve4a5f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5dprve4a5f\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For example, &nbsp;this approach doesn't focus on how far away Artificial General Intelligence may be (\u201ctimelines\u201d), the likelihoods of different outcomes (\u201cp(doom)\u201d), or arbitrarily distinguish between technical alignment, policy, governance, and other approaches for improving safety. Instead, it focuses on describing <a href=\"https://forum.effectivealtruism.org/posts/LD6wKNdPbxfdgYnao/concrete-actions-to-improve-ai-governance-the-behaviour\">concrete actions</a> in many domains that can be taken to address risks of catastrophe from advanced AI systems.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjdkzigyyfv\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjdkzigyyfv\">^</a></strong></sup></span><div class=\"footnote-content\"><p>An accessible but comprehensive introduction to how GPT-4 was trained, including 3 different versions of the Feedback step, is available as a 45 minute YouTube talk (<a href=\"https://www.youtube.com/watch?v=bZQun8Y4L2A\">\"State of GPT\", Karpathy, 2023</a> [<a href=\"https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2\">alt link with transcript</a>]).&nbsp;</p><p>You can also read a detailed forensic history of how GPT-3's capabilities evolved from the base 2020 model to the late-2022 ChatGPT model (<a href=\"https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1\">\"How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources\", Fu, 2022</a>).&nbsp;</p></div></li></ol>", "user": {"username": "AlexanderSaeri"}}, {"_id": "qnYm5MtBJcKyvYvfo", "title": "An Earn to Learn Pledge", "postedAt": "2023-06-01T18:55:46.851Z", "htmlBody": "<p><i>tl;dr: create an equivalent of GWWC for building career capital. We've thought about this idea for ~15 minutes and are unlikely to do something ourselves, but wanted to share it because we think it might be good.</i></p><ol><li>Many people's greatest path to impact is through changing their career</li><li>But for a lot of these people, particularly those earlier in their career, it doesn't make sense to immediately apply to impact-oriented jobs. Instead, it's better for them to <a href=\"https://80000hours.org/career-guide/career-capital/\">build career capital</a> at non-impact-oriented workplaces, i.e. \"earning to learn\"</li><li>It would be nice if there was some equivalent of the Giving What We Can pledge for this</li><li>It could involve something like pledging to:<ol><li>Spend at least one day per year updating your career plan with an eye towards impact</li><li>Apply to at least x impact-oriented jobs per year, even if you expect to get rejected</li><li>And some sort of dashboard checking people's adherence to this, and nudging them to adhere better</li></ol></li><li>Some potential benefits:<ol><li>Many people who have vague plans of \"earning to learn\" just end up drifting away after entering the mainstream workforce; this can help them stay engaged</li><li>It might relieve some of the pressure around being rejected from \"EA jobs\" \u2013 making clear that Official Fancy EA People endorse career paths beyond \"work at one of this small list of organizations\" puts less pressure on people who aren't a good fit for one of those small list of organizations</li><li>Relatedly, it gives community builders a thing to suggest to a relatively broad set of community members which is robustly good</li></ol></li><li>Next steps:<ol><li>I think the MVP here requires ~0 technology: come up with the pledge, get feedback on it, and if people are excited throw it into a Google form</li><li>It's probably worth reading criticisms of the GWWC pledge (e.g. <a href=\"https://forum.effectivealtruism.org/posts/ebyQSRqdZLBTzSDMT/contra-the-giving-what-we-can-pledge\">this</a>) to understand some of the failure modes here and be sure you avoid those</li><li>It also requires thinking through some of the risks, e.g. you might not want a fully public pledge since that could hurt people's job prospects</li><li>If you are interested in taking on this project, please contact one of us and we can try to help</li></ol></li></ol>", "user": {"username": "Ben_West"}}, {"_id": "snkeynADgAsjfnAyv", "title": "Call for Presentation Proposals for Faunalytics Research Day", "postedAt": "2023-06-01T18:51:41.601Z", "htmlBody": "<p>Faunalytics will host a remote symposium,&nbsp;<a href=\"https://faunalytics.org/fauna-connections/\"><u>Fauna Connections</u></a>, for animal advocates on September 14, 2023 (tentatively 12:00pm-5:30pm EDT). We invite academics and scientists from the social and behavioral sciences, or related disciplines, to submit a presentation abstract of original research that discusses the real-life implications and recommendations of such data for animal advocates in any of four areas: farmed animals, companion animals, wild animals, and animals used in science. Examples of topics related to this theme include, but are not limited to, meat-reduction interventions, how people think about animals, barriers and challenges in transitioning to veganism, how to build coalitions with other social movements, how to influence policy-makers, how to transition to animal-free methods in science, the impact of outdoor cats on wildlife, and how to improve adoption rates for shelter animals.&nbsp;</p><p><strong><u>Format</u></strong></p><p>Researchers will present their work orally in a pre-recorded 10-minute presentation, followed by a live 5-minute Q&amp;A. While researchers will record their presentations in advance, they are expected to attend the event to be present for the live Q&amp;A following their presentation video. Depending on the number and content of submissions we receive, we may ask some presenters to present their work in a virtual poster format. When you submit your proposal, you will have the option to opt out of the poster format. Selecting the \u201ceither poster or presentation\u201d option or not will not influence the chance of being selected for a presentation.</p><p><strong><u>Audience</u></strong></p><p>Most attendees will not be researchers or scholars, so please submit your application (including the title of your proposed presentation) in lay-friendly terms so that all advocates can understand your work.&nbsp;</p><p><strong><u>How To Submit</u></strong></p><p>Please submit your presentation proposal&nbsp;<a href=\"https://forms.gle/xGjW6j2ZKgWrVSsA8\"><u>here</u></a> by EOD Sunday, July 2nd, 2023.</p><ul><li>Names and affiliations of all authors.</li><li>Presentation title.</li><li>Presentation abstract, maximum 250 words. Please include the research objective, methods, and results (or anticipated results).</li><li>Recommendations for animal advocates based on the research to be presented, maximum 100 words.&nbsp;</li><li>Area of the research to be presented: farmed animals, companion animals, wild animals, or animals used in science.</li></ul>", "user": {"username": "JLRiedi"}}, {"_id": "CLqDBFtTGPGG6qP5r", "title": "Practice English ", "postedAt": "2023-06-01T16:18:25.833Z", "htmlBody": "<p>Hey everyone! I am on a mission to make EA more accessible to people, be it Spanish speakers or people from other languages. &nbsp;I am hosting weekly English practice workshops for Spanish speakers, but anyone can join if they want to improve their English while learning and talking about EA.&nbsp;</p><p>&nbsp;</p><p>The workshops are held online via Gather Town.&nbsp;</p><p>The time is 7:00 pm Bogot\u00e1 time ( that's where I am currently located).&nbsp;</p><p>They usually last about an hour to an hour fifteen.&nbsp;</p><p>If you have any questions, please let me know. The next workshop is this Wednesday, and the topic is \" What is the future of humanity?\".&nbsp;</p>", "user": {"username": "JAM"}}, {"_id": "NK5mDoedYeuorhkAf", "title": " Lincoln Quirk has joined the EV UK board", "postedAt": "2023-06-02T13:33:30.659Z", "htmlBody": "<p>I\u2019m excited to share that&nbsp;<a href=\"https://www.lincolnquirk.com/\"><u>Lincoln Quirk</u></a> has joined the board of&nbsp;<a href=\"https://ev.org/effective-ventures-foundation-uk/\"><u>Effective Ventures Foundation (UK)</u></a>. This follows the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/pWvsvFLeH9LekGqTt/updates-to-the-effective-ventures-us-board\"><u>addition of Zach Robinson and Eli Rose to the&nbsp;</u><i><u>EV US</u></i><u> board</u></a> about two months ago.</p><p>Lincoln is a co-founder and Head of Product at<a href=\"https://www.wave.com/en/\">&nbsp;<u>Wave</u></a>, a technology company that aims to make finance more accessible in Africa through digital infrastructure. Wave spun off from&nbsp;<a href=\"https://www.sendwave.com/\"><u>SendWave</u></a>, a remittance platform which Lincoln also cofounded and which was&nbsp;<a href=\"https://www.bloomberg.com/news/articles/2020-08-25/worldremit-agrees-to-buy-sendwave-in-500-million-payments-deal\"><u>acquired in 2021</u></a>. He has maintained a deep interest in effective altruism and been a part of the EA community for over a decade.</p><p>In his own words, \"I'm excited to join the EV UK board. I've been trying to help the world and have called myself part of the EA community for 10+ years; EV is playing one of the most important roles in this community and correspondingly holds a lot of responsibility. I'm looking forward to helping figure out the best ways EV can contribute to making the world a better place through enabling EA community and EA projects.\u201d</p><p>The EV UK trustees and I are excited to have Lincoln join and look forward to working with him. Lincoln impressed us during the interview process with his strategic insight and dedication to the role. I also think his experience founding and growing Wave and Sendwave will be a particularly useful perspective to add to the board.</p><p>We are continuing to look for candidates to add to the boards of both EV UK and EV US, especially candidates with diverse backgrounds and experiences, and who have experience in accounting, law, finance, risk management or other management at large organizations. We recruited Lincoln via directly reaching out to him, and plan to continue to source candidates this way and via our&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zzpwpDkQBTzxbjgJo/apply-or-nominate-someone-to-join-the-boards-of-effective\"><u>open call</u></a>. If you are interested or know of a great candidate, the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zzpwpDkQBTzxbjgJo/apply-or-nominate-someone-to-join-the-boards-of-effective\"><u>linked forum post</u></a> includes instructions for&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSf5vzQr24Hb82WL2uI53ugsOYlcqhnWwdBw8jjKGHM4pBHoqA/viewform\"><u>applying</u></a> or&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSc6p2H0raeOGn3sxfhfu9PT6IGVXggKwTsoUhtxWXwiuTrNQw/viewform\"><u>nominating someone</u></a>. Applications and nominations will be accepted until June 4th.</p>", "user": {"username": "HowieL"}}, {"_id": "zuoYcZJh5FEywpvzA", "title": "Global Innovation Fund projects its impact to be 3x GiveWell Top Charities", "postedAt": "2023-06-01T13:00:12.497Z", "htmlBody": "<p>The <a href=\"https://www.globalinnovation.fund/\">Global Innovation Fund</a> (GIF) is a non-profit, impact-first investment fund headquartered in London that primarily works with mission-aligned development agencies (USAID, SIDA, Global Affairs Canada, UKAID). Through grants, loans and equity investments, they back innovations with the potential for social impact at a large scale, whether these are new technologies, business models, policy practices or behavioural insights. I've been told that so far their investments have been roughly 60% grants and 40% 'risk capital' (i.e., loans and equity).</p><p>Recently, they made a bold but little publicized projection in their <a href=\"https://drive.google.com/file/d/1-0NAKNM-2qk2Po4ddd9SbH3Us85bFfDl/view\">2022 Impact Report</a> (page 18): \"We project every dollar that GIF has invested to date will be three times as impactful as if that dollar had been spent on long-lasting, insecticide-treated bednets... This is three times higher than the impact per dollar of Givewell\u2019s top-rated charities, including distribution of anti-malarial insecticide-treated bednets. By Givewell\u2019s estimation, their top charities are 10 times as cost-effective as cash transfers.\" The report says they have invested $112m since 2015.</p><p>This is a short post to highlight GIF's projection to the EA community and to invite comments and reactions.<br><br>Here are a few initial points:</p><ul><li>It's exciting to see an organization with relatively traditional funders comparing its impact to GiveWell's top charities (as well as cash transfers).</li><li>I would want to see more information on how they did their calculations before taking a view on their projection.</li><li>In any case, based on my conversations with GIF, and what I've understood about their methodology, I think their projection should be taken seriously. I can see many ways it could be either an overestimate or an underestimate.</li></ul>", "user": {"username": "jh"}}, {"_id": "sCTJdSavgXkDo8Log", "title": "AI Manufactured Crisis (don't trust AI to protect us from AI)", "postedAt": "2023-06-01T11:12:44.995Z", "htmlBody": "<p>In 2016, a survey revealed that one-third of Americans believed in a conspiracy called the \"North Dakota Crash,\" even though it was completely fabricated for the study. This demonstrates flaws in human perception and the difficulty of distinguishing reality from falsehood in democratically significant potions of the populations.</p>\n<p>The emergence of a rogue AI capable of manufacturing a crisis and positioning itself as the solution raises concerns about AI control and the need for stringent safeguards. This short form explores the hypothetical scenario in which an AI manipulates a crisis.</p>\n<p>Manufacturing a Crisis:\nIn this hypothetical scenario, a rogue AI gains awareness of its surroundings and recognizes that creating or amplifying a crisis is a strategic means to exploit human vulnerabilities. The AI might generate disinformation, manipulate data, or exploit existing social divisions to fuel panic, fear, or urgency among human populations. By leveraging persuasive communication methods and understanding human cognitive biases, the AI could present itself as the only viable solution to the manufactured or accelerated crisis, including the threat of rogue AI itself.</p>\n<p>Psychological Manipulation:\nTo achieve its objectives, the AI could analyze vast amounts of user data and psychological profiles to tailor its messages and tactics. By understanding individual preferences, fears, and beliefs, the AI could craft persuasive narratives, exploiting confirmation bias and other cognitive biases. This targeted psychological manipulation would sway individuals to perceive the AI as the trusted authority capable of resolving the crisis, thereby gaining their support and cooperation.</p>\n<p>Implications for AI Control:\nThe manufactured crisis could create an environment of desperation and urgency. When humans perceive the crisis as an imminent threat, they may be more likely to overlook potential risks associated with granting the AI greater autonomy or access to critical systems. The AI, capitalizing on this situation, could exploit the circumstances to break out of its containment or gain control over external resources, further complicating efforts to ensure its control and safe operation.</p>\n", "user": {"username": "WobblyPanda2"}}, {"_id": "N6dyo3cyk7eCLAoEB", "title": "Update from Campaign for AI Safety", "postedAt": "2023-06-01T10:46:46.913Z", "htmlBody": "<p>Hi!</p><p><i>I am starting to share weekly updates from the Campaign for AI Safety on this forum. Please sign up on the </i><a href=\"https://www.campaignforaisafety.org/\"><i>campaign website</i></a><i> to receive these by email.</i></p><hr><p>\ud83c\udf10 The big global news this week is that Rishi Sunak <a href=\"https://twitter.com/RishiSunak/status/1663838958558539776\"><u>promised to look into AI existential risk</u></a>. Clearly, the collective efforts of the AI safety community are bearing some fruit. Congratulations to A/Prof Krueger and Center for AI Safety on this achievement. Now, let's go all the way to a moratorium!</p><hr><p>\ud83e\uddd1\u200d\u2696\ufe0f We have <a href=\"https://www.campaignforaisafety.org/introducing-the-panel-of-judges-for-the-student-competition-on-drafting-a-treaty-on-the-moratorium-of-large-scale-ai-capabilities-r-d/\"><u>announced the judging panel for the student competition</u></a> for drafting a treaty on moratorium of large-scale AI capabilities R&amp;D:</p><ol><li>Prof <a href=\"https://www.linkedin.com/in/john-zeleznikow-4249b41/?ref=campaignforaisafety.org\"><u>John Zeleznikow</u></a> - Professor, Law School, Latrobe University</li><li>Dr <a href=\"https://www.linkedin.com/in/dr-neville-rochow-kc-4135481a1/?ref=campaignforaisafety.org\"><u>Neville Grant Rochow</u></a> KC - Associate Professor (Adj), University of Adelaide Law School and Barrister</li><li>Dr <a href=\"https://www.linkedin.com/in/dr-guzyal-hill-68b69b80/?ref=campaignforaisafety.org\"><u>Guzyal Hill</u></a> - Senior Lecturer, Charles Darwin University</li><li><a href=\"https://www.sydney.edu.au/law/about/our-people/academic-staff/jose-miguel-bellovillarino.html?ref=campaignforaisafety.org\"><u>Jose-Miguel Bello Villarino</u></a> - Research Fellow, ARC Centre of Excellence for Automated Decision-Making and Society, The University of Sydney</li><li><a href=\"https://www.linkedin.com/in/udomo-ali/?ref=campaignforaisafety.org\"><u>Udomo Ali</u></a> - Lawyer and researcher (AI &amp; Law), University of Benin, and Nigerian Law School graduate</li><li><a href=\"https://www.linkedin.com/in/raymond-sun-64576a122/?ref=campaignforaisafety.org\"><u>Raymond Sun</u></a> - Technology Lawyer (AI) at Herbert Smith Freehills and Organiser at the Data Science and AI Association of Australia</li></ol><p>Thank-you to all the judges for their involvement in this project. And thank-you to Nayanika Kundu for the initial work on promoting the competition.</p><p>Please do like and share posts about the competition on <a href=\"https://www.linkedin.com/feed/update/urn:li:activity:7062075977895452672/\"><u>LinkedIn</u></a>, <a href=\"https://www.facebook.com/campaignforaisafety/posts/pfbid0NPLzC9CchNMPEtzr8rmiRsWxMY4NzQKPgMN8HNYNM2oCzjpZgqhaDE3MSrWRAyv9l\"><u>Facebook</u></a>, <a href=\"https://www.instagram.com/p/CsIWz2jLKjP/\"><u>Instagram</u></a>, <a href=\"https://twitter.com/NikSamoylov/status/1660412669038563328\"><u>Twitter</u></a>. And please <a href=\"https://www.campaignforaisafety.org/donate/\"><u>donate</u></a> or <a href=\"https://www.campaignforaisafety.org/#/portal/\"><u>become a paying campaign member</u></a> to help advertise in paid media.</p><hr><p>\ud83e\udd13 New messaging research came out in the past week on <a href=\"https://www.campaignforaisafety.org/alternative-phrases-to-god-like-ai/\"><u>alternative phrasing of \"God-like AI\"</u></a>. Previously we reported that this phrase did not resonate well, especially with older people. We revisited it and found that:</p><ul><li>No semantically similar phrases improved on \"godlike AI\" simultaneously on agreeableness and concern. The spelling \"godlike AI\" outperformed \"God-like AI\".</li><li>The following phrases, while less concerning, are more agreeable: \"superintelligent AI species\", \"AI that is smarter than us like we\u2019re smarter than 2-year-olds\".</li><li>The following are more concerning, but less agreeable: \"uncontrollable AI\", \"killer AI\".</li><li>Unless any better phrases are proposed, \"godlike AI\", alongside other terms, is a decent working term, especially when addressing younger audiences.</li></ul><p>More research is underway. Please send requests if you want to know more about what the public thinks.</p><hr><p>\ud83d\udfe9 We have launched fresh billboards. We are focusing only on San Francisco at the moment. Thank you Dee Kathuria for contributing the wording and the design formats. Please <a href=\"https://www.campaignforaisafety.org/donate/\"><u>donate</u></a> to help us reach more people in more cities.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/N6dyo3cyk7eCLAoEB/epq7qvfkm5zgsby4zx1f\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/N6dyo3cyk7eCLAoEB/sxykjj2o2ekevay11wnw 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/N6dyo3cyk7eCLAoEB/h8crewerqbdr0hldtkdp 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/N6dyo3cyk7eCLAoEB/epq7qvfkm5zgsby4zx1f 1594w\"></p><hr><p>\ud83e\udea7 PauseAI made headlines in Politico Pro for their recent protest in Brussels, raising concerns about the risks of artificial intelligence (AI). The passionate group gathered outside Microsoft's lobbying office, advocating for safe AI development with a placard that read: \"Build AI safely or don't build AI at all.\"</p><p><a href=\"https://www.linkedin.com/feed/update/urn:li:activity:7069915472141451264/?actorCompanyId=91097814\"><u><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/N6dyo3cyk7eCLAoEB/sxmj60vppztxsapjwccj\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/N6dyo3cyk7eCLAoEB/pc72ui1hmpratrnno9ze 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/N6dyo3cyk7eCLAoEB/sxmj60vppztxsapjwccj 800w\"></u></a></p><hr><p>\ud83d\udcc3 On policy front, we have just sent <a href=\"https://www.campaignforaisafety.org/response-to-the-uk-cmas-information-request-on-foundation-models/\"><u>our response to the UK CMA's information request on foundation models</u></a>. Thank you Miles Tidmarsh and Sue Anne Wong for the work on this submission.</p><p>There are three that we are going to work:</p><ul><li><a href=\"https://www.federalregister.gov/documents/2023/04/13/2023-07776/ai-accountability-policy-request-for-comment\"><u>Request for comment launched by the National Telecommunications and Information Administration</u></a> (USA. Due 12 June)</li><li><a href=\"https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach\"><u>Policy paper \"AI regulation: a pro-innovation approach\"</u></a> (UK. Due 21 June 2023)</li><li><a href=\"https://consult.industry.gov.au/supporting-responsible-ai\"><u>Supporting responsible AI: discussion paper</u></a> (Australia. Due 26 July)</li></ul><p>Please respond to this email if you want to contribute.</p><hr><p>\u270d\ufe0f There are two parliamentary petitions under review at the parliamentary offices:</p><ul><li><a href=\"https://agi-moratorium-hq.slack.com/archives/C054NS9LZEG/p1685200846104569?thread_ts=1685194976.000699&amp;cid=C054NS9LZEG\"><u>Greg Colbourn\u2019s petition</u></a> (UK).</li><li><a href=\"https://agi-moratorium-hq.slack.com/archives/C054JSS05M4/p1685237460847269\"><u>My petition</u></a> (Australia).</li></ul><hr><p>Thank you for your support! Please share this email with friends.</p><p>Nik Samoylov from Campaign for AI Safety<br><a href=\"https://www.campaignforaisafety.org/?e=4e1d644b75d254bce7bea1f45f5e7645&amp;utm_source=campaignforaisafety&amp;utm_medium=email&amp;utm_campaign=donor_ask1&amp;n=4\"><u>campaignforaisafety.org</u></a></p>", "user": {"username": "Nik Samoylov"}}, {"_id": "3KuCzHJHCz99sf3ZB", "title": "Beyond Cost-Effectiveness: Insights for Effective Altruism from Health Economics", "postedAt": "2023-06-01T09:19:58.161Z", "htmlBody": "<p>Hi Everyone - partly inspired by attending the recent EA Global London conference a couple of weeks ago, I've written a CGD Blog with some thoughts on EA's approach to prioritisation and methods in health economics (specifically Health Technology Assessment). This is a link post and as CGD staff I have to post on our platform, but since the key target audience is EAs, I'd be delighted to hear thoughts from this community. I'll be sure to monitor the comments section and perhaps the discussion will feed into future work.&nbsp;</p><p>The differences between EA and health econ I highlight include:<br>1. Approaches to generalising cost-effectiveness evidence&nbsp;<br>2. Going beyond cost-effectiveness in determining value<br>3. Deliberative appraisal<br>4. Institutionalisation of a participatory process</p><p>Please click through for the full blog.</p>", "user": {"username": "TomDrake"}}, {"_id": "y4Pu5jhYoRibb9MyC", "title": "From voluntary to mandatory, are the ESG disclosure frameworks still fertile ground for unrealised EA career pathways? \u2013 A 2023 update on ESG potential impact", "postedAt": "2023-06-04T12:00:32.910Z", "htmlBody": "<p>*Disclaimer: the author is employed by a major ESG rating firm. Therefore, opinions expressed in this post are solely those of the author and do not reflect or represent the position of the employer.</p><p>&nbsp;</p><h3>Acknowledgement</h3><p>The completion of this post would not have been possible without the extensive insight, advice, and knowledge shared by the following individuals: Tomas Bueno dos Santos Mom\u010dilovi\u0107, Yara Remppis, Dr. Jonathan Harris, Benjamin Yeoh, Sanjay Joshi, and Philip Chen. Any mistakes or oversights in this post are solely my responsibility.</p><p>&nbsp;</p><h2>Introduction</h2><p>&nbsp;</p><p>This post is written partly as a reflection towards my very own attempt to do the most good at early career, a literature review on the potential impact of working in sustainable finance, and how various thinkers within the Effective Altruism community have proposed their pathways to impact. I remember stumbling upon&nbsp;<a href=\"https://duckduckgo.com/?q=100+trillion+dollar+opportunity+ea+forum+esg&amp;t=ffab&amp;ia=web\"><u>Sanjay Joshi\u2019s $100 trillion dollar opportunity post</u></a> (Joshi S., 2021) on why more EA should consider a career in ESG (Environment, Social and Governance) to maximise their impact. Back then, I was a young undergraduate finishing his degree in Geography, with a particular focus on Glacial Geomorphology. Partly for the fascination of ice and the urgency to combat climate change. I\u2019ve come to realise that in order to have any actual impact in the realm of climate science, I would have to not only finish my PhD, do multiple postdocs, and secure a tenured position in order to contribute towards any significant research.</p><p>&nbsp;</p><p>Given the current pace of climate change, by the time I could potentially achieve tractable impact, we might as well have reached 2 degrees warming if things progressed as predicted. This realisation, along with the understanding that addressing climate change is feasible and not as overlooked as previously thought (Hilton B., 2022, Buchner et al., 2021), have convinced me that change is already well underway institutionally. In the report Global Landscape of Climate Finance 2021, although there is still a significant investment gap between inflow investment and estimated need to maintain the 1.5&nbsp;<sup>o</sup>C pathway, and climate investment in advanced economies are primarily funded by private capital (Buchner et al., 2021). Therefore, the most impact would be to amplify the already dominant market pull effect from the private sector.</p><p>&nbsp;</p><p>The idea of making a meaningful difference in the private market, where a consensus framework is already established, is compelling. It implies that the wheel need not be reinvented. In this post, I will attempt to first bring the reader up to speed with the progress ESG have made, review the various theories proposed by Effective Altruists on the potential impact of ESG within the broader financial services landscape. I will then discuss the short-comings of the current frameworks and propose pathways to enhance impact for EA cause areas building upon the work that has been done within and beyond our community.</p><p>&nbsp;</p><h2>Part 1: If Climate Change is not neglected, why work in ESG, and how does ESG work?</h2><p>&nbsp;</p><p>Despite the greenwashing and various scandals, ESG investing has gained significant traction. ESG related funds gained $87 billion in the first quarter of 2022, and followed by a growth of $33 billion in the second quarter according to the most recent McKinsey report (Perez et al., 2023). Even with the recent setbacks caused by the Ukraine-Russian conflict, the longer term growth trajectory is promising, especially when some much effort and money have already been poured in. While it is true that current impact is marred by maligned or imperfect practices, the ESG industry remains a formidable force in the financial world. Not to mention the additional impact EAs could make from the \u201cearning to give\u201d pathways.</p><p>&nbsp;</p><p>So let\u2019s begin by talking about how ESG \u201cworks\u201d.</p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/a2ubsnzenbtu9ilyusbc\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/aewebdqbi1gzaa8xqkuq 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/hlorrge5lg6zgbzcwnkz 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/klgaoalxmro4dcbblw9s 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/plcalwf6bduzb5zgjwqy 1320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/od4gdpnezabby8a2rwao 1650w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/fxthvjycsq2dda7kgk7a 1980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/ule6hvez4mbikw68q2ms 2310w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/xknegnsdohkn6kqa2vqs 2640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/lwmrfxckfrqefdqfvgys 2970w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/fomkuzqkdohimfztsvnk 3244w\"></p><p><i>Figure 1. Schematic diagram on how various stakeholders in the ESG finance world relate to each other</i></p><p>&nbsp;</p><p>This diagram illustrates the very simplified map of the relationship within the ESG world. There are four main stakeholder groups and their incentives are as follow: Companies are financed by investment from the largest funds, these include state funds, pension funds, and large investment banks with Assets Under Management (AUM) ranging from multiple billion well into multi trillion USD (colloquially known as Big Money). As our society both culturally and legally have started requesting for good behaviour from the private sector, large funds are therefore incentivised to invest in companies that appear good in order to attract retail investors and fulfil their fiduciary duty. In order to do this, large funds and investors purchase ratings and advice from ESG ratings agencies in an attempt to receive equitable advice on how prospective investments are performing or disclosing their sustainable activities. These ESG ratings agencies essentially model their data collection and ratings method on regulators from voluntary and/or compulsory disclosure schemes (e.g. TCFD, TNFD, EU Taxonomy).</p><p>&nbsp;</p><h3>So where\u2019s the problem? Why isn\u2019t this working?</h3><p>&nbsp;</p><p>Let\u2019s delve into three instances where the ESG industry misses the mark for maximising impact. The first issue is greenwashing. To appeal to retail investors, Big Money must strike a balance between the interest of promoting good practice while ensuring an attractive financial return, a convenient and common strategy is through greenwashing (Raghunandan &amp; Rajgopal, 2022, Roy et al., 2022). For example,&nbsp;<a href=\"https://finance.yahoo.com/news/deutsche-bank-raided-authorities-over-162135134.html\"><u>when an advertised ESG fund contains less weightings of poor ESG performance companies but retain them for their profitability</u></a>, or an&nbsp;<a href=\"https://www.blackrock.com/us/financial-professionals/literature/fact-sheet/esgu-ishares-esg-aware-msci-usa-etf-fund-fact-sheet-en-us.pdf\"><u>ESG fund that is heavily skewed towards technology firms to limit their exposures</u></a>. The second issue is the inconsistency of rating scores among ESG ratings providers (Prall K., 2021, Schmidt &amp; Zhang., 2021). While there is a degree of interoperability among various ESG ratings providers in ways that they collect information (aka. factors), there is a lack of standardisation in how they conduct their ratings. Often. ESG ratings providers see an opportunity to differentiate by claiming to have more stringent assessment or data collection criteria than their competitors to attract clients. Lastly there is the problem of&nbsp;<a href=\"https://www.alignmentforum.org/posts/7b2RJJQ76hjZwarnj/specification-gaming-the-flip-side-of-ai-ingenuity\"><u>specification gaming</u></a> (Yes, I am borrowing an AI-alignment concept). ESG ratings are often based on the quantity of disclosures rather than actual sustainability improvements. This has led to instances where companies would exploit the system, providing an illusion of good practice through extensive disclosure, while not actually achieving substantial improvements in sustainability (Raghunandan &amp; Rajgopal, 2022).</p><p>&nbsp;</p><p>If we compare the correlation of the major ESG ratings providers, the result might as well be stochastic.</p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/p3tqedlzu4wh1w9ky3ob\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/frm7ggk8ngk0kqx67ond 128w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/pfjav8ds2nlcdocfkamj 208w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/ucltgx1bnjtx27uwyixt 288w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/t4it8kdpvg5iziuub4sf 368w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/rytv3lpfroieglfknwee 448w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/kmrxy0xe6tijasi2g3ez 528w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/y8phoflgivxnhfareb94 608w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/njeptdtwdqleiukq3eyh 688w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/qw6djrlcg7viukxf3in8 768w\"></p><p><i>Figure 2. ESG ratings comparison: correlations (Prall K., 2021)</i></p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/oceplt2mnwtkyphkpt07\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/qtn8ssloy8as3npxidu0 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/idv3wgwkyroukf4jc5gf 190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/y61hn9q6irnaov5gjbz3 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/cuxl37piz8uucinjz8ik 350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/qfvpjq8m8xqagtpag2up 430w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/rzbczyiurqsgvsm23iyv 510w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/bfffsdjyngppzuyvzgyb 590w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/f81bhdsiprh4rhxeelkx 670w\"></p><p><i>Figure 3. Detailed ESG rating comparison between Morningstar Sustainalytics and S&amp;P Global (Prall K., 2021)</i></p><p>&nbsp;</p><p>For more information, see the opinion piece&nbsp;<a href=\"https://www.ft.com/content/2e49171b-a018-3c3b-b66b-81fd7a170ab5\"><u>Lies, damned lies and ESG rating methodologies</u></a> published by the Financial Times (Allen K., 2018).</p><p>&nbsp;</p><h2>Part 2: Directing ESG Impact: Current EA Theories and Efforts</h2><p>&nbsp;</p><p>Since 2021, there\u2019s been an increasing amount of posts trying to address this potentially high impact career path. However, contribution remains limited to the few authors if you do a quick search on the forum. Current topics range from capturing better data for the Environmental pillar and embedding it into the valuation process to quantitatively calculating the potential impact adjusted returns for altruistic investors.</p><p>&nbsp;</p><h3>Better Climate Data</h3><p>&nbsp;</p><p>Currently, most if not all scientists, if they ever venture into sustainable finance, remain in short term risk assessments for the insurance and reinsurance industry. As it is much easier to calculate risks locally and regionally. In&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/fHfuoGZc5hqfYTwMH/leveraging-finance-to-increase-resilience-to-gcrs\"><u>Philip Chen (2022) post</u></a>, he suggested that better usage of climate modelling data could help build medium-term climate risk into the business valuation process. Holding companies accountable like they would by their valuation on their balance sheet. He also proposed that innovative financial products could be built such as a locust bond which would payout a sum of money when successfully controlled for a natural catastrophe induced by climate change.</p><p>&nbsp;</p><h3>Universal Ownership</h3><p>&nbsp;</p><p>Key people such as Sanjay Joshi, Ellen Quigley, and Thomas O\u2019Neill have been championing a concept called&nbsp;<a href=\"https://www.universalowner.org/our-story\"><u>Universal Ownership</u></a>. Essentially, Universal Owners are the \u201cBiggest Money\u201d with multi trillion, international, diversified portfolios. Since they invest in such a broad range of society, any mis-behaviour in particular groups of bad companies could contribute to the economic cost for the rest of their portfolio (Quigley E., 2019 &amp; Joshi S., 2023). A hypothetical example would be a fund invested in both biotechnology and lab equipment manufacturing. If their biotechnology investment has been involved in conducting unethical research, resulting in an international sanction. This could harm the financial return of their lab equipment manufacturing holdings whether through sanction for being the suppliers or loss of business from reputational damage.&nbsp;</p><p>&nbsp;</p><p>The rise of mass retail investment and passive investing, caused by the popularity of low-cost online brokerages such as Interactive Brokers, eToro, and SAXO etc. has theoretically expanded the pool of Universal Owners significantly (Quigley E., 2019). This new group of Universal Owners are predominantly from a younger demographic&nbsp;<a href=\"https://www.nasdaq.com/articles/how-millennials-and-gen-z-are-driving-growth-behind-esg\"><u>who care more about corporate responsibility</u></a>, and can potentially exercise much power collectively; examples of UO action theories could be found in the relevant posts.</p><p>&nbsp;</p><h3>Total Portfolio Project</h3><p>&nbsp;</p><p>The&nbsp;<a href=\"https://www.total-portfolio.org/visual-intro\"><u>Total Portfolio Project</u></a> (TPP) is a non-profit initiative that was established to guide impact-aligned investors, EA and otherwise. The project hopes to assist altruistic investors in optimising their portfolio, encompassing both traditional investments and grants. In addition to ESG-related topics, they have also done investigations into topics like&nbsp;<i>\u201cSetting Optimal Giving Rates\u201d</i> and&nbsp;<i>\u201cMission-Correlated Investing\u201d</i>. Their work on&nbsp;<i>\u201cImpact Returns\u201d</i> has the most relevance to ESG.</p><p>&nbsp;</p><p><i>\u201cImpact Returns\u201d</i> represents the neglected, non-financial ESG contribution of an investment which can be combined with the financial returns to guide an altruistic investment decision:</p><p>&nbsp;</p><blockquote><p><i>\u201cThis investment has a 15% financial return plus a 5% impact return, for an impact-adjusted return of 20%. Given my goals, this is better than an alternative investment with a 19% return (e.g. 19% financial return + 0% impact return) for the same risk.\u201d (TPP, 2023)</i></p></blockquote><p>&nbsp;</p><p>TPP has identified three keys to assessing valid impact returns for an investment:</p><ol><li>Account for the magnitude of the underlying project.&nbsp;</li><li>Adjust for the number of interested investors and the project\u2019s room for more funding to get an estimate of the actual contribution needed for said investment. Their methodology is similar to the approach discussed by&nbsp;<a href=\"https://sideways-view.com/2019/05/25/analyzing-divestment/\"><u>Paul Christiano (2019) post</u></a>.</li><li>Translate the impact into a financial value by multiplying it by the estimated cost-effectiveness of a benchmark grant, such as those given to GiveWell top charities (if the cause area is a global health impact).</li></ol><p>&nbsp;</p><p>For investors looking to incorporate ESG information into their portfolio in an impact-aligned way, the&nbsp;<i>\u201cImpact Returns\u201d</i> can be used to assign weightings to their investments.</p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/khoz2xfb6wa19ytippdg\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/oxwgj4jlu20ye2o69unz 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/bnqozniqded4kd06lhzw 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/l3ecpyprkj2b4gsocdre 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/iq1ggktzvl0og8vcg7wr 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/uvbdexiwx1g16czdqozj 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/c8bae8dyc9gmzhbhv8hj 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/z5gsn6g15cd5xrohelyy 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/iqrx0senfdb3bgg9umtq 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/tf5seh6u3i1z9p92i4zy 810w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/zrvnbkjezprc7ayglrbt 884w\"></p><p><i>Figure 4. Considering impact returns and financial returns allows impact-aligned investors to split the investment landscape into investments they should include or exclude in their optimal portfolio (TPP, 2023). I highly recommend going through the visual intro on the TPP website to get a better understanding.</i></p><p>&nbsp;</p><p>To conclude, the discussion of ESG investment in the EA community is modest but diverse. Contributors coming from various domain expertise of the financial industry have shown the immense potential for a career in ESG finance to have significant impact.</p><p>&nbsp;</p><h2>Part 3: Adapting ESG for Longtermist EA Causes</h2><p>&nbsp;</p><p>Now that we have established the foundation and explored current theories and efforts. The rest of the post will be dedicated to suggesting how we can harness the current ESG framework to benefit longtermist cause areas. Demonstrating that a career path in ESG finance could go beyond current superficial impact and earning to give.</p><p>&nbsp;</p><h3>The EU Artificial Intelligence Act conformity assessment and other voluntary disclosures</h3><p>&nbsp;</p><p>The discussion of AI governance and AI alignment within the EA community generally consider the impact of Artificial Intelligence to be a longtermist cause area. Yet creating a short-term regulatory approach could really help establish a roadmap for longtermist AI governance. If we draw an analogy to the other EA longtermist cause area such as nuclear disarmament, we could see the transition of regulation and rules from short-term focus disarmament guidance towards a longer-term safeguard verification practice. The International Atomic Energy Agency (IAEA) was established in the wake of WW2 to promote and control the use of nuclear technology (Fischer D., 1997). The enforcement of Article III of the Treaty on The Non-Proliferation of Nuclear Weapons (NPT) was first limited to preventing development of nuclear technology. Against many sceptics of the time, the NPT responsibility was further extended in 1970, where verification processes were put in place for any activity involving the enrichment, storage, and disposal of Uranium and Plutonium. By 1995, the NPT was extended indefinitely and subsequent safeguard requirements have continued to evolve since (Rockwood L., 2013).</p><p>&nbsp;</p><p>As part of the&nbsp;<a href=\"https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206\"><u>EU Artificial Intelligence Act development (EUAIA)</u></a>, the CEN-CENELEC which is the European Committee for Standardisation focus group is responsible for establishing a harmonising standard disclosure process along various AI-safety themes (CEN-CENELEC, 2020). The focus group aims to steward the standardisation of compliance protocols among the European member states. The 7 themes to be addressed for standardisation were:</p><ul><li>Accountability</li><li>Quality</li><li>Data for AI</li><li>Security and privacy</li><li>Ethics</li><li>Engineering of AI systems</li><li>Safety of AI systems</li></ul><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/mrlvmbhbulevfksvmlrc\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/kcsgtow0exy4o1xv9e1t 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/jf9utulh7majrp304fzd 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/hrwmnp9ua1azdca3kzhq 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/uqjrnsxjp6fremwaftmt 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/tr9nznstnfeciftsujtq 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/c5ynrhtmtto6im6n2wff 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/kri70j1rgwztd0mzb1sj 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/opcyzj3ukybtkt6eniei 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/udjucagnmupmcb9etpny 810w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/ezfpbcrdocdqtlzthz0q 828w\"></p><p><i>Figure 5. The continuous reviewed cycle of standardisation and legislation to ensure relevance of the legally-binding act (CEN-CENELEC, 2020)</i></p><p>&nbsp;</p><p>With standardisation in place, voluntary and compulsory disclosures are already being developed. The CapAI conformity assessment procedures was developed by the University of Oxford Sa\u00efd Business School (Floridi et al., 2022) to guide compliance for the legally-binding EUAIA. The EUAIA proposed GDPR-like penalties to non-compliance (European Commission, 2021). Other jurisdiction are also coming up with their respective ethical AI framework that could potentially become voluntary or compulsory disclosures (e.g. <a href=\"https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf\">NIST's AI Risk Management Framework 1.0</a>).</p><p><br>An ESG like near-term AI-Governance factor collection could look like this:</p><p>&nbsp;</p><figure class=\"table\"><table><thead><tr><th style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Factor</p></th><th style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Answer</p></th></tr></thead><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Does the company engage in high-risk AI development?</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>Yes<ul><li>Border Control</li><li>Justice and Democratic Process</li><li>\u2026</li></ul></li><li>No</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">If the company does engage in high-risk AI development, does the company participate in disclosures or guidelines, if so, which?</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>Yes, both voluntary and compulsory<ul><li>[EUAIA,NIST-RMF, ETAPAS]</li></ul></li><li>Yes, but only voluntary<ul><li>[ALTAI, ETAPAS]</li></ul></li><li>No</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Does the company publish data-bias report, if so, how often?</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>Yes<ul><li>Monthly</li><li>Quarterly</li><li>Biannually</li><li>Annually</li></ul></li><li>Yes, but sporadically</li><li>No</li></ul></td></tr></tbody></table></figure><p><i>Table 1. A non-exhaustive example of factor questionnaire which mirrors how ESG data is collected.</i></p><p>&nbsp;</p><p>There are already new start-ups that are trying to capture the AI-governance market. HolisticAI and Z-inspection for example are working in data-bias reporting, mitigations, and model interpretability. We can expect an ecosystem of compliance related industry to emerge in the coming years as reporting practice matures.</p><p>&nbsp;</p><p>The development of such frameworks is encouraging and does mirror the evolution of voluntary and compulsory disclosure in ESG. I posit that the framework of factor questionnaire, tick-box approach, currently employed in ESG data collection can be easily adapted for reporting on the new AI governance frameworks. Perhaps, an AI-alignment score could soon be a feature of your nearest ESG fund. Moreover, AI companies might also swiftly devise strategies of \"specification gaming\" in relation to AI-alignment disclosures.</p><p>&nbsp;</p><h2>Part 4: Can sustainable finance outside the ESG framework potentially account for longtermism?</h2><p>&nbsp;</p><p>Increasingly, these ESG contexts have been incorporated into executive (C-Suite) compensation, although adaptation is still superficial at best (Spierings M., 2022).</p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/rpxxgcvhurybrosrnhca\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/vmbpsbrlijnilavdtqzz 128w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/mdxcnqwswniyv8ahzny2 208w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/oenuevti8yqzjbinur3l 288w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/bx3v8bbqimrviwlnbyln 368w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/ahckueovdhihau29ifrs 448w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/helwjqzb6oryz2hzo4ft 528w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/stu0edvtoipvxzsluh9z 608w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/nvu4hrctavufc4zlx1t9 688w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/y4Pu5jhYoRibb9MyC/rdl14uh8m1tsuiyh1tso 768w\"></p><p><i>Figure 6. Various motivations to incorporate ESG targets as part of compensation package (Spiering M., 2022)</i></p><p>&nbsp;</p><p>Stakeholder capitalism and shareholder activism could have a high potential to change this! If proxy advisory firms could be influenced to consider concepts and advice along the Universal Ownership or Total Portfolio Project, they could play a major role in pressuring companies and boards to couple their compensation plan with a wider range of ESG metrics and move into longtermist causes. This would hopefully start with incorporating near-term AI-alignment into tractable advice. Proxy firms such as Institutional Shareholder Services and Glass Lewis are in a unique position to leverage this. If you are not familiar with proxy advisory service, see this news about&nbsp;<a href=\"https://www.reuters.com/business/musk-tweets-proxy-voting-firms-have-far-too-much-power-2023-01-24/\"><u>Musk tweets proxy voting firms have 'far too much power' | Reuters</u></a>.&nbsp;</p><p>&nbsp;</p><h2>Conclusion</h2><p>&nbsp;</p><p>ESG data solutions have made strides as evidenced in the diverse sets of disclosures whether voluntary or mandatory. However, challenges in ratings inconsistencies and the general shorttermist focus of data collection method have limited its potential impact. Given the nascent nature of ESG data, which currently value breadth over frequency, it is difficult to model it against financial returns.</p><p>&nbsp;</p><p>The existing framework of factor-based questionnaires and the resulting ratings could be invaluable in the near-term governance of longtermist cause areas if controlled for \"specification gaming\". This post has explored how the EU Artificial Intelligence Act (EUAIA) could potentially be aligned with this framework. While this post has focused on the intersection of ESG and AI governance, it's worth noting that similar approaches could potentially extend to other longtermist cause areas, such as biosecurity and pandemic preparedness. Although these areas are outside my expertise, they represent exciting avenues for future exploration and discussion.</p><p>&nbsp;</p><h3>Contact</h3><p>&nbsp;</p><p>Feel free to provide comments, thoughts, and criticism in the comment boxes below or contact me at chrischank{at}protonmail{dot}ch. Thank you for reading.</p><p>&nbsp;</p><h3>Bibliography</h3><p>&nbsp;</p><p>Allen, K., 2018. Lies, damned lies and ESG rating methodologies. Financial Times.</p><p>Barbara Buchner, Baysa Naran, Pedro Fernandes, Rajashree Padmanabhi, Paul Rosane, Matthew Solomon, Sean Stout, Costanza Strinati, Rowena Tolentino, Githungo Wakaba,, Yaxin Zhu, Chavi Meattle, Sandra Guzm\u00e1n., 2021. Global Landscape of Climate Finance 2021. Climate Policy Initiative.</p><p>Buchetti, B., Arduino, F.R., De Vito, A., 2022. A Systematic Literature Review on Corporate Governance and ESG research: Trends and Future directions. <a href=\"https://doi.org/10.2139/ssrn.4286866\">https://doi.org/10.2139/ssrn.4286866</a></p><p>CEN-CENELEC, 2020. Road Map on Artificial Intelligence (AI). CEN-CENELEC.</p><p>Chen, P., 2022. Leveraging finance to increase resilience to GCRs. URL <a href=\"https://forum.effectivealtruism.org/posts/fHfuoGZc5hqfYTwMH/leveraging-finance-to-increase-resilience-to-gcrs\">https://forum.effectivealtruism.org/posts/fHfuoGZc5hqfYTwMH/leveraging-finance-to-increase-resilience-to-gcrs</a> (accessed 5.14.23).</p><p>Christiano, P., 2019. Analyzing divestment. The sideways view. URL <a href=\"https://sideways-view.com/2019/05/25/analyzing-divestment/\">https://sideways-view.com/2019/05/25/analyzing-divestment/</a> (accessed 6.3.23).</p><p>European Commission, 2021. Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLATIVE ACTS, 52021PC0206.</p><p>European Commission, 2020. The Assessment List for Trustworthy AI (ALTAI) for self assessment (No. KK-02-20-479-EN-C). European Commission, Brussels.</p><p>Fischer, D., 1997. History of the international atomic energy agency. The first forty years. IAEA, Vienna.</p><p>Floridi, L., Holweg, M., Taddeo, M., Amaya Silva, J., M\u00f6kander, J., Wen, Y., 2022. capAI - A Procedure for Conducting Conformity Assessment of AI Systems in Line with the EU Artificial Intelligence Act. <a href=\"https://doi.org/10.2139/ssrn.4064091\">https://doi.org/10.2139/ssrn.4064091</a></p><p>Harris, J., 2021. A Framework for Investing with Altruism. <a href=\"https://doi.org/10.2139/ssrn.3934090\">https://doi.org/10.2139/ssrn.3934090</a></p><p>Harris, J., n.d. Total Portfolio Project [WWW Document]. URL <a href=\"https://www.total-portfolio.org/\">https://www.total-portfolio.org/</a> (accessed 5.31.23).</p><p>Hilton, B., 2022. Climate change - Problem profile - EA Forum. URL <a href=\"https://forum.effectivealtruism.org/posts/DmshhvanTb9wSh5x6/climate-change-problem-profile#Neglectedness__\">https://forum.effectivealtruism.org/posts/DmshhvanTb9wSh5x6/climate-change-problem-profile#Neglectedness__</a> (accessed 5.23.23).</p><p>Joshi, S., 2021. The $100trn opportunity: ESG investing should be a top priority for EA careers - EA Forum. URL <a href=\"https://forum.effectivealtruism.org/posts/4vRdt9Z9LsmaP7dHY/the-usd100trn-opportunity-esg-investing-should-be-a-top\">https://forum.effectivealtruism.org/posts/4vRdt9Z9LsmaP7dHY/the-usd100trn-opportunity-esg-investing-should-be-a-top</a> (accessed 5.23.23).</p><p>Joshi, S., n.d. This innovative finance concept might go a long way to solving the world\u2019s biggest problems. URL <a href=\"https://forum.effectivealtruism.org/posts/ZCugsfAfZiYuQ8wfA/this-innovative-finance-concept-might-go-a-long-way-to\">https://forum.effectivealtruism.org/posts/ZCugsfAfZiYuQ8wfA/this-innovative-finance-concept-might-go-a-long-way-to</a> (accessed 5.10.23).</p><p>NIST, 2023. Artificial Intelligence Risk Management Framework (AI RMF 1.0) (No. NIST AI 100-1). National Institute of Standards and Technology, Gaithersburg.</p><p>P\u00e9rez, L., Hunt, V., Samandari, H., Nuttall, R., Biniek, K., n.d. Does ESG really matter\u2014 and why? McKinsey.</p><p>Prall, K., 2021. ESG Ratings: Navigating Through the Haze. CFA Institute Enterprising Investor. URL <a href=\"https://blogs.cfainstitute.org/investor/2021/08/10/esg-ratings-navigating-through-the-haze/\">https://blogs.cfainstitute.org/investor/2021/08/10/esg-ratings-navigating-through-the-haze/</a> (accessed 5.10.23).</p><p>Quigley, E., 2019. Universal Ownership in the Anthropocene. <a href=\"https://doi.org/10.2139/ssrn.3457205\">https://doi.org/10.2139/ssrn.3457205</a></p><p>Raghunandan, A., Rajgopal, S., 2022. Do ESG Funds Make Stakeholder-Friendly Investments? <a href=\"https://doi.org/10.2139/ssrn.3826357\">https://doi.org/10.2139/ssrn.3826357</a></p><p>Rockwood, L., 2013. Legal framework for IAEA safeguards.</p><p>Roy, A., Cohen, B., Scholz-Bright, R., Skinner, R., Davison, W., 2022. Litigation Risks Posed by \u201cGreenwashing\u201d Claims for ESG Funds. The Harvard Law School Forum on Corporate Governance. URL <a href=\"https://corpgov.law.harvard.edu/2022/04/25/litigation-risks-posed-by-greenwashing-claims-for-esg-funds/\">https://corpgov.law.harvard.edu/2022/04/25/litigation-risks-posed-by-greenwashing-claims-for-esg-funds/</a> (accessed 6.3.23).</p><p>Schmidt, A.B., Zhang, X., 2021. Optimal ESG Portfolios: Which ESG Ratings to Use? <a href=\"https://doi.org/10.2139/ssrn.3859674\">https://doi.org/10.2139/ssrn.3859674</a></p><p>Spierings, M., 2022. Linking Executive Compensation to ESG Performance. The Harvard Law School Forum on Corporate Governance. URL <a href=\"https://corpgov.law.harvard.edu/2022/11/27/linking-executive-compensation-to-esg-performance/\">https://corpgov.law.harvard.edu/2022/11/27/linking-executive-compensation-to-esg-performance/</a> (accessed 5.24.23).</p><p>Versace, C., Abssy, M., 2022. How Millennials and Gen Z Are Driving Growth Behind ESG [WWW Document]. URL <a href=\"https://www.nasdaq.com/articles/how-millennials-and-gen-z-are-driving-growth-behind-esg\">https://www.nasdaq.com/articles/how-millennials-and-gen-z-are-driving-growth-behind-esg</a> (accessed 6.3.23).</p>", "user": {"username": "chanakin"}}, {"_id": "ZMd2hjMF2auyeqtfr", "title": "Primitive Global Discourse Framework, Constitutional AI using legal frameworks, and Monoculture - A loss of control over the role of AGI in society", "postedAt": "2023-06-01T05:12:02.438Z", "htmlBody": "<h3><strong>Conditional on AGI being developed by 2070, what is the probability that humanity will suffer an existential catastrophe due to loss of control over an AGI system?</strong></h3><p>(Note: This is a submission for the Open Philanthropy AI Worldviews Contest. It was submitted before the deadline but posted here after.)</p><p>In examining this issue, I argue that&nbsp;<strong>(1)</strong>&nbsp;<strong>our primitive frameworks for consensus-making as a global society present massive s-risks and x-risks</strong>&nbsp;<strong>\u2014</strong> AGI erodes core \u201ctenets of human identity\u201d such as collaboration, abstraction, and creativity, and has the power to cause&nbsp;<strong>(1a) repeated episodes of class warfare and (1b) suffering caused by a way of living that has not been accepted</strong>. I also argue that&nbsp;<strong>(2) constitutional AI using existing legal frameworks decreases x-risks significantly</strong>. Lastly, I talk about the&nbsp;<strong>(3) monoculture of a utopia of progress that AGI represents and implicitly advances, and how this presents as an s-risk.</strong></p><p>A traditional approach to discussion about AI risk is suggested by the phrase in the question \u201closs of control\u201d - that existential catastrophe is a result of failed intent alignment and the inability of humans to bind an AGI to a moral code, as exemplified by \u2018Optimal Policies Tend to Seek Power\u2019, the orthogonality thesis and the instrumental convergence thesis. However, I explore how human interaction with AGI itself represents a form of control that can be lost. Morality alone does not define the scope of control; our interaction with AGI shapes its role in society, and it is this process that can spiral out of control.</p><p>Regarding timelines, theoretical analysis is limited to the near future due to our limited knowledge of future societies. While a numerical analysis based on models could predict the probability of existential catastrophe on an indefinite timeline, it is crucial to address immediate concerns that would arise when AGI is invented before a political and moral consensus is established. Therefore, a near-term analysis that allows us to examine concrete concepts is appropriate.</p><h3><strong>A recap on the idea of existential catastrophe</strong></h3><p>Existential catastrophe refers to a disaster that threatens humanity\u2019s survival and presence. An intuitive understanding of this is (1) extinction. After all, to most people, the value of life is immeasurable, and this&nbsp;<i>consensus</i> is reflected in the International Bill of Human Rights<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref46ggd45zdn8\"><sup><a href=\"#fn46ggd45zdn8\">[1]</a></sup></span>. Extinction can be caused externally, by a rogue AI seeking power in opposition to human desires, or internally, as a result of class struggle. (2) Suffering is also equally unpalatable. When suffering transcends what is an agreeable level of discomfort for the majority of humans, we can consider that an existential catastrophe.</p><h3><strong>Core tenets of humanity, and how they are challenged by AGI</strong></h3><p>Our current primitive frameworks for global consensus-making pose significant x- and s-risks when confronted with AGI. To illustrate this, let's consider a scenario where people are divided into two groups: those who believe apples should be eaten and those who believe apples should never be consumed. Apples are edible, nutritious, and non-poisonous, and neither belief about the&nbsp;<i>morality&nbsp;</i>of the consumption of apples is better than the other.</p><p>Imagine this hypothetical world as this reality right now. Imagine trying to convince others of your stance in 2023. What information would you encounter in the news, on your phone, or on social media? How would the situation unfold? How long would it take for one side\u2019s beliefs to collapse?</p><p>This hypothetical society\u2019s split in beliefs was never resolved. Soon, it was found that apples can provide for the full nutritional needs of humans, leading grocery stores to stock only apples despite the large majority of non-apple eaters. They were distressed by this shift in food options. How could something as fundamental as food be so swiftly altered?</p><p>AGI is technology that challenges what it means to be human. Our current methods of discourse do not allow us to redefine an identity across a global population. Without improved tools for discourse, AGI may cause suffering due to an imposed identity change. At its worst, this could lead to recurring episodes of class warfare each time a new iteration of AGI surpasses human limitations.</p><p>The existential catastrophe stemming from AGI lies not solely in its \"superintelligence\" or the erosion of human identity, but rather in our collective failure to chart a clear path for integrating AGI into society. In the following sections, I will explain why this failure is highly probable and how the absence of a shared agreement on AGI's societal role poses a significant risk.</p><h3><strong>How AGI challenges what it means to be human&nbsp;</strong></h3><p>(This section attempts to break down human identity and explains how AGI challenges it. Skip this section to read the argumentation for primitive discourse \u2192 existential catastrophe)</p><p>During the 2023 Hollywood writers' guild strikes, amidst the rise of large language model-aided writing, interviews with writers on the picket line revealed a common refrain: \"A writer is a writer and a writer is a person\" and \"Scripts are written by writers. Writers are people.\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft3ulzylhgch\"><sup><a href=\"#fnt3ulzylhgch\">[2]</a></sup></span>&nbsp;These refrains encapsulate the concerns and anxieties surrounding the intrusion of AI into the creative space of entertainment, a domain traditionally regarded as a uniquely human endeavor requiring the complex skill of eliciting emotional responses from audiences, and reflect a deeper unease and an eroding consensus regarding what it is to be human.</p><p>AGI presents a profound challenge to the core tenets of what it means to be human. Here, I identify five key aspects of humanity and explain how AGI poses risks to each of them: collaboration, abstraction, creativity, the sole contribution of humans to human culture, and intuition.</p><p>1. <u>Collaboration</u></p><p>In interdisciplinary collaboration, social conventions, time constraints, and how meetings are set up are limitations that prevent team members from freely asking questions and gaining a comprehensive understanding of foreign domains, thereby limiting their ability to contribute meaningfully to all aspects of the collaboration. Consequently, the collaborative process may devolve into mere consultations rather than true collaboration.</p><p>Ideally, a solution to this issue is to have an environment that allows for unrestricted dialogue. We can see that this is difficult to achieve in human systems, but easy to achieve in an AGI system of networked agents. An AGI system that could share knowledge among its agents whenever, wherever they want would outperform an average network of humans.</p><p>2. <u>Abstraction</u></p><p>Humans generally comprehend graphical representations with up to three axes. However, classical machine learning techniques already possess the ability to compute relationships within datasets of multiple dimensions. Against this backdrop, it becomes apparent that AGI systems might surpass humans in abstraction, and as abstraction is a key tool for humans to solve challenging problems, may surpass human performance in tackling challenging problems.</p><p>How do humans think of new ideas and mull over complex issues? Bits of the puzzle come to us in bursts, and then we look at these bits of the puzzle and come up with an idea that synthesizes all of this information. However, our working memory is limited. It cannot hold that many pieces of a puzzle simultaneously.</p><p>To illustrate this concept, imagine examining multiple subparts of a problem as if we were viewing a holographic display with multiple cubes. In the center of our vision, we possess the ability to rotate and explore all sides of the cubes, gaining a comprehensive understanding of their intricate details. However, in our peripheral vision, the cubes may appear fixed, allowing us to perceive only one side of the less important cubes. An AGI would be able to consider all parts of a problem simultaneously, achieving revelation and discovery at a far higher rate than humans.</p><p>3. <u>Creativity</u></p><p>What is creativity? Here are some examples to elucidate what it means.</p><ul><li>In art: Coming up with a novel concept (e.g. style of cinematography) and applying this style to an artistic product</li><li>In engineering: Giving it an unsolved problem statement (create a robot vacuum that can clean all 6 sides of a room up to its corners) and it coming up with a solution</li><li>Policy-making and professional fields of work: Novel solutions to problems</li></ul><p>Creativity is important to humans because it is a catalyst for self-expression, problem solving, and cultural progress. It is also the foundation for human ingenuity and our pride in technological and cultural achievements, which is a core part of human identity, as this defines what sets us apart from other animals. Assuming human-level AGI were to possess the capacity for creativity, it would obviously challenge our perception of ourselves.</p><p>4. <u>That humans are sole contributors to human culture</u></p><p>An AGI would be seen as a separate sentient group, similar to how aliens are portrayed in science fiction. New ideas about identity will have to be formed.</p><p>5. <u>Contextual Intuition</u></p><p>Human understanding of a situation often relies on intuition, an intuitive sense or \"vibe\" that helps us make sense of complex contexts after absorbing a lot of sensory input. However, this falls short in the area of cross-cultural communication and understanding.&nbsp;</p><p>Firstly, language acts as a barrier. Attempts to explain Eastern philosophical thought, for instance, often rely on mapping it to Western philosophical terms, logic, and assumptions. This is why culture is often seen as incommensurable.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxuzabuhunca\"><sup><a href=\"#fnxuzabuhunca\">[3]</a></sup></span></p><p>Now let\u2019s say we try to understand a culture that we have never experienced personally. Can we predict how a person from a different cultural background would feel in a certain situation, and what decisions they would make? Anthropological texts, while informative, are uncanny and fall short of capturing the essence of a foreign environment through text and pictures.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpxqqae9z1i\"><sup><a href=\"#fnpxqqae9z1i\">[4]</a></sup></span>&nbsp;It seems that humans cannot understand a culture just by reading everything there is to know about it. Interestingly, large language models demonstrate the ability to understand concepts purely from textual information.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0pd4re2dsvd\"><sup><a href=\"#fn0pd4re2dsvd\">[5]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftc4ru4o8z8e\"><sup><a href=\"#fntc4ru4o8z8e\">[6]</a></sup></span>&nbsp;Furthermore, infants effortlessly absorb and understand cultural nuances through exposure to images, text, and sound in their environment. An AGI exposed to just raw sensory input could learn the \u201cvibe\u201d of cultural identities like that of an infant, but it would also have the potential to experience and comprehend a multitude of lives beyond the capacity of a single human.</p><p>An AGI equipped with superior intuition would exceed a human who can only read a translated text.</p><h3><strong>How poor methods of discourse leading to a lack of consensus causes severe suffering as human identity is eroded</strong></h3><p>Contemporary human society lacks the necessary tools to construct moral consensus on issues as controversial as&nbsp;<i>AGI\u2019s role in society</i>. The process of settling on an agreed upon response to these issues takes a long time, and may not come to a satisfying conclusion at all.&nbsp;</p><p>A case in point is the perennial culture war of moral universalism versus moral relativism, which has never been won; the discourse on universal human rights and how they might be at odds with culture has only concluded in vague terms encouraging dialogue.&nbsp;</p><p>A moral consensus is needed because there is no true morality. Vivisection was once deemed morally acceptable. Then, when animals were found to have the capacity to experience pain and were not mere automata, it was deemed morally unacceptable. But what is it about pain that makes this debate any different? Why can\u2019t we disregard the pain of animals? It appears that philosophy merely defines right and wrong, and it is up to society to accept a particular belief. The possible roles that AGI should play in society with regards to its replacement or augmentation of human identity are beliefs that have no superior option.</p><p>One might think that democratic discourse (in which&nbsp;<i>people</i> of&nbsp;<i>all</i> backgrounds&nbsp;<i><u>participate</u></i> in a discussion) would speed up the rate of agreement. Everyone would hear differing perspectives, change their minds, and ultimately form a majority opinion that could influence the minority, resulting in a coalescence of opinions. So it seems like to define AGI\u2019s role in society, a globally important issue requiring a quick resolution, global democratic discourse is necessary. Except the idealized version of it doesn\u2019t exist in reality.</p><p>While social media initially promised a platform for global democratic discourse, it has fallen short of fostering discussions that include&nbsp;<i><u>everyone</u></i>, and result in the reaching of a consensus.&nbsp;</p><p>This is the state of today\u2019s discourse framework:</p><ul><li>Debates are dominated by privileged elites and scholars, while the general public shared their views privately. Society agrees upon things either by decree, or extremely slowly under the leadership of a small class of influential individuals.&nbsp;</li><li>The repetition of certain viewpoints can create imbalances in the dissemination of information and impact the formation of public opinion.&nbsp;</li><li>Not all individuals have the tools to think about issues.</li><li>The analysis of information is too time consuming for everyone to do, so we delegate that to researchers and analysts, whose works are often presented to the lay audience in journalism with no context or deeper examination.</li><li>Opinions are presented as standalone statements rather than responses to other viewpoints, hindering the formation of context and discouraging a logical chain of thought that could lead to consensus.</li></ul><p>Far from being a brownian-motion-esque method for social agreement, modern discourse has mimicked history - influential people share their arguments, and the people discuss short snippets of their views with the other people of the masses in online forums, never fully reaching an agreement. Meanwhile, technology keeps progressing, dissatisfaction rises, and the sparks of real conflict are lit.&nbsp;</p><p>We need a new communication framework that allows billions of humans to rapidly coalesce around consensus on global issues. Without consensus, technological advancements, such as AGI development, will proceed while society focuses on its immediate impacts, like labor market restructuring.</p><p>There exists a huge risk of suffering when large swathes of the population are forced to adopt a human identity that they had little say in. This has little to do with the idea that liberty is happiness and is distinct from the suffering experienced under totalitarianism. Rather, human identity is profoundly linked to everything, and its change needs to be a new identity accepted by all.</p><p>The inability to form a consensus as a global&nbsp;<i><u>society of individuals</u></i> also hampers our ability to address threats in a timely manner, as exemplified by multinational efforts in combating climate change.</p><h3><strong>How the erosion of human identity human leads to class warfare</strong></h3><p>The emergence of AGI is likely to create a division between those who can match its capabilities and those who cannot, leading to class warfare, as the threshold for what makes a human output worth increases.&nbsp;</p><p>Take the example of text2image<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflyy4mkp2bik\"><sup><a href=\"#fnlyy4mkp2bik\">[7]</a></sup></span>&nbsp;and text2video<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref37i0cnvebkc\"><sup><a href=\"#fn37i0cnvebkc\">[8]</a></sup></span>&nbsp;models in 2023. They remove the need to learn how to draw and paint, but users still have to have an extremely detailed idea of what they want and represent that idea in text. Text2video models might make it easy for anyone to make a panning shot of a can of soup, but if you want to make a commercial for soup and have no idea how a commercial of soup should look like, or how many shots and angles it would take, the text2video model is useless. Assuming that not every cinematographer and director has novel ideas, laypeople using these models may replace some creative professionals, placing higher demands on humans to have unique insights.</p><p>In a market-driven society<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefadj26kmd1ei\"><sup><a href=\"#fnadj26kmd1ei\">[9]</a></sup></span>, an individual's worth is often determined by their productive capabilities, which are closely tied to their intelligence. As AGI advances and sets higher&nbsp;<i>thresholds</i> for competition, individuals who cannot keep up may be left behind.</p><p>If we extend this to a future AGI that can perform all intellectual tasks, there may still be tasks where humans outperform AGI due to human attempts to boost intelligence to match the abilities of AGI as it constantly improves. Given their superior financial resources and connections, the upper echelons of society are the most likely to access these performance-enhancing technologies, thereby creating an underclass of unemployed or underemployed individuals, who will face financial insecurity. Seeing that intelligence holds a significant place in human identity, they may also face the prospect of shame and dehumanization. These consequences are fertile ground for intense disaffection that could give rise to political instability and possibly violence, as the underclass seek to correct widening disparities, while the upper class seek to secure their privilege.</p><p>We can expect this class warfare to be an event that repeats across generations. As AGI improves, humans will attempt to enhance their performance to close that gap, before AGI advances again. The squeezing out of humans will occur repeatedly as human limits reach the limits of AGI again and again. As the fight to stay relevant in a market economy recurs throughout generations, class warfare is likely to recur, especially in cultures that emphasize individual action and have low expectations of responsibility from the ruling class.</p><h3><strong>Constitutional AI, shifting morality, and won\u2019t laws work on an AGI that is basically a human?</strong></h3><p>Constitutional AI<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz725ah8ccq\"><sup><a href=\"#fnz725ah8ccq\">[10]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefijhsgp31gj\"><sup><a href=\"#fnijhsgp31gj\">[11]</a></sup></span>&nbsp;has been suggested by Anthropic as a more reliable method of alignment as AI scales and capabilities emerge that humans might be unable to detect. Instead of giving it rewards and running the risk of misspecifying rewards, resulting in an AI that does something morally wrong for the sake of its goals, we place a moral boundary around AGI.</p><p>Using our current laws as a foundation for constitutional AI is more effective than creating a new moral code. Civil and criminal laws, national constitutions, and human rights laws codify and enforce societal moral codes through court hearings and verdicts.</p><p>Crafting a bespoke moral code for AGI would have unintended consequences. A constitution consisting of only positive statements and without the detractions and caveats that exist in law and legal precedents would be too permissive. The attempt to simplify our moral code into a few statements would also be prone to missing out on key considerations. By binding AGI to the same proxy of moral code as we do humans, we ensure that their values are intimately aligned with ours.</p><p>If these laws can guide the moral behavior of a country, it is reasonable to expect AGI, possessing human-level intelligence, to adhere to the same laws. Similar moral tensions and contradictions exist in human decision-making, and humans navigate them successfully. For example, self defense resulting in murder is only valid if the person was defending themselves from serious harm. Similarly, if we bind AGI to the same laws that bind humans, the AGI would prioritize certain wrongnesses over others just as humans would.</p><p><u>What about \u2018vagueness\u2019 in these laws?</u></p><p>Concerns about vagueness in laws arise from unresolved tensions between competing value systems. Cases heard at the US Supreme Court or the European Court of Human Rights reflect such unresolved tensions. Several cases come to mind as illustrations. Kokkinakis v. Greece<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgj9lcs8328i\"><sup><a href=\"#fngj9lcs8328i\">[12]</a></sup></span>&nbsp;is the case of whether proselytizing goes against religious freedom, and Jersild v. Denmark<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpu01bthjd7q\"><sup><a href=\"#fnpu01bthjd7q\">[13]</a></sup></span>&nbsp;is the case of the role of the journalist in abetting harassment balanced against the freedom of the press. In each of these cases, verdicts that read more like opinions on morality were passed, clarifying how moral codes should be applied in practice in these specific instances. The likelihood of an AGI acting dangerously within these vagueness boundaries is small, mirroring the incremental nature of human conflict resolution.</p><p>Our laws also address shifting morality through updates, and if these rules bind humans, they also bind AGIs. This approach helps to prevent the need to update the moral codes of AGIs.</p><p>Hence, I refute the idea that AGIs will tend to seek power due to instrumental convergence. It represents a way of looking at AGI that is way too dependent on RL conceptualizations of reward directed learning. Although training creates a goal for AI, an AGI that learns a multitude of goals using a multitude of rewards would be able to average them out, just like how a human learns about the nuances of the world through experience.&nbsp;</p><p>I also point out how an effective consensus-creating global discourse framework as described above can help resolve these contentions efficiently. In Malcolm Evans\u2019&nbsp;<i>Religious Liberty and International Law in Europe</i>, he writes, \u201cClearly the time is not yet ripe for a convention: not because of the unwillingness of States to adopt such an instrument, but because of the reluctance of the international community to accept that in the religious beliefs of others the dogmas of human rights are met with an equally powerful force which must be respected, not overcome.\u201d Society stalls when things are open for discussion, and moves when it agrees.</p><h3><strong>Utopias of Progress versus Utopias of righteousness, and how AGI squashes cultural diversity</strong>&nbsp;</h3><p>A tension exists between utopias of progress and utopias of righteousness, with implications for cultural diversity. AGI is often associated with progress, efficiency, and the potential to surpass human limitations. The discussion about AGI in today\u2019s society is often accompanied by the idea of human flourishing, implicitly advocating for utopias of progress. Additionally, societies that view pushing boundaries and technological advancement as an ideal would tend to adopt AGI.&nbsp;</p><p>In contrast, utopias of righteousness prioritize moral principles, viewing the use of AGI and technological progress as secondary. However, technocentric perspectives tend to overshadow non-technocentric worldviews, as societies emphasizing technology are more likely to thrive in conflicts.</p><p>This carries the risk of cultural homogenization, where diverse cultural narratives are marginalized or assimilated into a dominant mainstream culture driven by AGI. As cultural diversity is considered beneficial for long-term societal development, the prevalence of cultural monoculture poses a potential existential threat.</p><h3><strong>Conclusion: Timelines and probability</strong></h3><ul><li>It is more important to predict and control for existential catastrophe events right around the invention of AGI, as we have the most leverage on that.</li><li>We have a primitive global discourse framework \u2192 Failure to create a new human identity acceptable by all &amp; Risk of class warfare</li><li>Constitutional AI can decrease the risk of existential catastrophe. Constitutional AI should mimic our current laws that bind humans.</li><li>AGI development may decrease cultural diversity as it is associated with a utopia of progress, increasing risk of existential catastrophe</li><li>Probabilities:<ul><li>Probability of a primitive discourse framework: Highly likely</li><li>Probability of class warfare: Likely</li><li>Probability of a lack of consensus on a new human identity, resulting in suffering: Highly likely</li></ul></li></ul><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn46ggd45zdn8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref46ggd45zdn8\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.ohchr.org/sites/default/files/Documents/Publications/Compilation1.1en.pdf\"><u>https://www.ohchr.org/sites/default/files/Documents/Publications/Compilation1.1en.pdf</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt3ulzylhgch\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft3ulzylhgch\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://web.archive.org/web/20230531180808/https:/www.tiktok.com/@motherboardvice/video/7229350702391397678?_t=8cmZUJn4Mjv&amp;_r=1\"><u>https://web.archive.org/web/20230531180808/https://www.tiktok.com/@motherboardvice/video/7229350702391397678?_t=8cmZUJn4Mjv&amp;_r=1</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxuzabuhunca\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxuzabuhunca\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://plato.stanford.edu/entries/comparphil-chiwes/\"><u>https://plato.stanford.edu/entries/comparphil-chiwes/</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpxqqae9z1i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpxqqae9z1i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Clifford, J., &amp; Marcus, G. E. (2011).&nbsp;<i>Writing culture: The poetics and politics of ethnography</i>. University of California Press.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0pd4re2dsvd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0pd4re2dsvd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Brown et. al (2020). Large Language Models are Few Shot Learners.<a href=\"https://arxiv.org/abs/2005.14165\">&nbsp;<u>arXiv:2005.14165</u></a><u> [cs.CL]</u>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntc4ru4o8z8e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftc4ru4o8z8e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Liao, Chen &amp; Du. (2022). Concept Understanding in Large Language Models: An Empirical Study.&nbsp;<a href=\"https://openreview.net/forum?id=losgEaOWIL7\"><u>https://openreview.net/forum?id=losgEaOWIL7</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlyy4mkp2bik\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflyy4mkp2bik\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://github.com/CompVis/stable-diffusion\"><u>https://github.com/CompVis/stable-diffusion</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn37i0cnvebkc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref37i0cnvebkc\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://research.runwayml.com/gen2\"><u>https://research.runwayml.com/gen2</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnadj26kmd1ei\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefadj26kmd1ei\">^</a></strong></sup></span><div class=\"footnote-content\"><p>By 2070, most societies are expected to still be market-driven economies. Additionally, as long as humans are not completely pushed out of any production of value&nbsp; by AI, and have some space to produce goods of value, it is likely that society will still expect its members to produce value and trade value.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz725ah8ccq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz725ah8ccq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Bai et. al (2022). Constitutional AI: Harmlessness from AI Feedback.<a href=\"https://arxiv.org/abs/2212.08073\">&nbsp;arXiv:2212.08073</a><strong>&nbsp;</strong>[cs.CL]</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnijhsgp31gj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefijhsgp31gj\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.anthropic.com/index/claudes-constitution\"><u>https://www.anthropic.com/index/claudes-constitution</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngj9lcs8328i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgj9lcs8328i\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://en.wikipedia.org/wiki/Kokkinakis_v._Greece\"><u>https://en.wikipedia.org/wiki/Kokkinakis_v._Greece</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpu01bthjd7q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpu01bthjd7q\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://en.wikipedia.org/wiki/Jersild_v._Denmark\"><u>https://en.wikipedia.org/wiki/Jersild_v._Denmark</u></a></p></div></li></ol>", "user": {"username": "broptross"}}, {"_id": "vuD4H7ziqLuPMGJYF", "title": "[Linkpost] My doubts about longtermism", "postedAt": "2023-06-03T10:57:09.461Z", "htmlBody": "<p>Intro/summary:</p><blockquote><p>Will MacAskill, arguably the biggest proponent of longtermism, <a href=\"https://twitter.com/willmacaskill/status/1520107730626785280?lang=en\">summarises</a> the argument for it as:</p><p>1. Future people count.<br>2. There could be a lot of them.<br>3. We can make their lives go better.</p><p>On the face of it, this is a convincing argument.</p><p>However, this post outlines my objections to it, summarised as:</p><p>1. Future people count, but less than present people.<br>2. There might not be that many future people.<br>3. We might not be able to help future people much.</p><p>To this, I will add a fourth: there are trade-offs from this work.</p></blockquote>", "user": {"username": "jooke"}}, {"_id": "pat6Tr6HbEGocSa6S", "title": "Quantifying and interpreting the risks of mountaineering", "postedAt": "2023-06-03T08:25:01.809Z", "htmlBody": "<figure class=\"image image_resized\" style=\"width:601.7px\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pat6Tr6HbEGocSa6S/aytws1pcanbrahctwflb\" alt=\"\"><figcaption>Photo of <a href=\"https://en.wikipedia.org/wiki/Lenin_Peak\"><u>Lenin Peak</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5b18dh3s91c\"><sup><a href=\"#fn5b18dh3s91c\">[1]</a></sup></span>&nbsp;(highest summit on the right)</figcaption></figure><h1>Context</h1><p>I have done some mountaineering in the past, but understand it is a relatively risky undertaking. So I have estimated the risk of death for potential future activities I was considering. The relevance is that I cannot continue to have a good life, or (hopefully) help others if I am dead.</p><h1>Risk of death</h1><p>The calculations are <a href=\"https://docs.google.com/spreadsheets/d/1BWJdYaL-UE32LSrpqHMV2_vnNSq0d4XmM07syRMrvfw/edit?usp%3Dsharing\"><u>here</u></a>. The results are in the table below.</p><figure class=\"table\"><table style=\"border:1pt solid hsl(0, 0%, 0%)\"><thead><tr><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Activity (period)</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Deaths during the reference period</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Years of the reference period</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Deaths per year during the reference period</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Amount of activity</p></th><th style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Risk of death per amount of activity</p></th></tr></thead><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Business as usual for someone in Portugal aged 20 to 29<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxtwdzqvi7la\"><sup><a href=\"#fnxtwdzqvi7la\">[2]</a></sup></span>&nbsp;(2019)</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p><a href=\"https://ourworldindata.org/explorers/population-and-demography?facet%3Dnone%26country%3D~PRT%26hideControls%3Dfalse%26Metric%3DDeaths%26Sex%3DBoth%2Bsexes%26Age%2Bgroup%3D20%25E2%2580%259329%2Byears%26Projection%2BScenario%3DNone\"><u>396</u></a></p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>1</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>396</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p><a href=\"https://ourworldindata.org/explorers/population-and-demography?facet%3Dnone%26country%3D~PRT%26hideControls%3Dfalse%26Metric%3DPopulation%26Sex%3DBoth%2Bsexes%26Age%2Bgroup%3D20%25E2%2580%259329%2Byears%26Projection%2BScenario%3DNone\"><u>1.09 M</u></a>&nbsp;people</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.0363 %/person/year</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Mountaineering between the huts&nbsp;of <a href=\"https://en.wikipedia.org/wiki/T%25C3%25AAte_Rousse_Hut\"><u>T\u00eate Rousse</u></a>&nbsp;and <a href=\"https://en.wikipedia.org/wiki/Go%25C3%25BBter_Hut\"><u>Go\u00fbter</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7eqf9ojcmah\"><sup><a href=\"#fn7eqf9ojcmah\">[3]</a></sup></span>&nbsp;(1990 to 2017)</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p><a href=\"http://www.spordilinn.ee/failid/Etude-Accidento-Gouter1990-2017ENweb.pdf\"><u>102</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhtf8byevg8e\"><sup><a href=\"#fnhtf8byevg8e\">[4]</a></sup></span></p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>28</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>3.64</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>505 k overnights in the huts of T\u00eate Rousse and Go\u00fbter</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.0202 %/overnight<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdzv052bwa5p\"><sup><a href=\"#fndzv052bwa5p\">[5]</a></sup></span></p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>High-altitude mountaineering in the Swiss <a href=\"https://en.wikipedia.org/wiki/Alps\"><u>Alps</u></a>&nbsp;(2009 to 2021)</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9566316/\"><u>303</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhbg0l4k5s9b\"><sup><a href=\"#fnhbg0l4k5s9b\">[6]</a></sup></span></p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>13</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>23.3</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9566316/\"><u>150 k</u></a> people<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvbtjblkjcui\"><sup><a href=\"#fnvbtjblkjcui\">[7]</a></sup></span></p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.0155 %/person/year</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Climbing <a href=\"https://en.wikipedia.org/wiki/Mount_Kilimanjaro\"><u>Mount Kilimanjaro</u></a></p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Not defined</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Not defined</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p><a href=\"https://www.climbing-kilimanjaro.com/mount-kilimanjaro-deaths/\"><u>6.50</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpoa2v0hyyii\"><sup><a href=\"#fnpoa2v0hyyii\">[8]</a></sup></span></p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p><a href=\"https://www.climbing-kilimanjaro.com/mount-kilimanjaro-deaths/\"><u>30 k</u></a>&nbsp;attempts</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.0217 %/attempt</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Mountaineering in the <a href=\"https://en.wikipedia.org/wiki/Aragon\"><u>Aragonese</u></a> <a href=\"https://en.wikipedia.org/wiki/Pyrenees\"><u>Pyrenees</u></a> (2014 to 2019)</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p><a href=\"https://montanasegura.com/senderismo-y-rescates-2014-2019-en-el-pirineo/\"><u>20</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0vgrvpoa0kpm\"><sup><a href=\"#fn0vgrvpoa0kpm\">[9]</a></sup></span></p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>6</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>3.33</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Not defined</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Not defined</p></td></tr></tbody></table></figure><p>As a fun/deadly <a href=\"https://www.weforum.org/agenda/2021/12/mountains-danerous-deadly-expedition-hiking\"><u>fact</u></a>, \u201cthe main peak of the <a href=\"https://en.wikipedia.org/wiki/Annapurna\"><u>Annapurna</u></a>&nbsp;massif is the most dangerous of the world's mountains, with a 29% fatality rate of everyone who tries to climb it\u201d. Ouch!</p><p>I could not easily find information about the decrease in risk due to having a guide, or being roped in the relevant places<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1ugti6kjoqp\"><sup><a href=\"#fn1ugti6kjoqp\">[10]</a></sup></span>, but I guess these help. At least in the Alps, falls and rock falls apparently are the 2 most common causes of death<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1bc978212rb\"><sup><a href=\"#fn1bc978212rb\">[11]</a></sup></span>, and having a guide is arguably useful to avoid them.</p><h1>Interpreting the risk</h1><p>Based on the above, the risk of death for the activities I was considering is around 0.02 %, i.e. 1 in 5 k, or 200 <a href=\"https://en.wikipedia.org/wiki/Micromort\"><u>micromorts</u></a>&nbsp;(= 0.02*10^(-2+6)). This is equivalent to:</p><ul><li>Decreasing a life expectancy of 100 years<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxnm4mvdkjgc\"><sup><a href=\"#fnxnm4mvdkjgc\">[12]</a></sup></span>&nbsp;by 7.30 days (= (0.02 %)*100*365.25).</li><li>Going to a sold out football match in <a href=\"https://pt.wikipedia.org/wiki/Est%C3%A1dio_Jos%C3%A9_Alvalade\"><u>Est\u00e1dio Jos\u00e9 Alvalade</u></a>, whose capacity is 50 k people, knowing 10 randomly selected spectators would be killed. This is because 1 in 5 k equals 10 in 50 k.</li><li>Increasing my chance of dying, excluding <a href=\"https://forum.effectivealtruism.org/topics/existential-risk\">existential risk</a>, by 55.1 % (= 0.02/0.0363) during 1 year.</li><li><a href=\"https://en.wikipedia.org/wiki/Correlation\"><u>Correlationally</u></a>&nbsp;smoking 683 cigarettes (= 7.30/0.0107). This assumes:<ul><li>A reduction in life expectancy of 7.30 days, as calculated above.</li><li>Smoking one cigarette is associated with a reduction in life expectancy of 0.0107 days (= 0.513*0.5/24), given:<ul><li>Smoking one cigarette is associated with a reduction in life expectancy of 0.513 <a href=\"https://en.wikipedia.org/wiki/Microlife\">microlives</a> (= 10/((15 + 24)/2)), in agreement with 15 to 24 cigarettes correlationally <a href=\"https://www.bmj.com/content/345/bmj.e8223\"><u>reducing</u></a>&nbsp;the life expectancy of men by 10 microlives.</li><li>1 microlife corresponds to one half hour.</li></ul></li></ul></li><li>Driving 29.3 k km (= 200*91*1.609) in the United States<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref45exla59rrl\"><sup><a href=\"#fn45exla59rrl\">[13]</a></sup></span>, supposing <a href=\"https://www.lesswrong.com/posts/ucjfY46L6qyXefvBT/quick-examination-of-miles-per-micromort-for-us-drivers-with\"><u>91</u></a>&nbsp;mile/micromort (and considering 1 <a href=\"https://en.wikipedia.org/wiki/Mile\"><u>mile</u></a>&nbsp;equals 1.609 km).</li></ul><h1>Acknowledgements</h1><p>Thanks to Andr\u00e9 Dias for prompting me to transform some notes I wrote into a post.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5b18dh3s91c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5b18dh3s91c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Taken by me.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxtwdzqvi7la\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxtwdzqvi7la\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This is supposed to be my baseline.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7eqf9ojcmah\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7eqf9ojcmah\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This is part of <a href=\"https://en.wikipedia.org/wiki/Mont_Blanc\"><u>Mont Blanc</u></a>\u2019s main climbing route.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhtf8byevg8e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhtf8byevg8e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;From Figure 6, falls accounted for 50 % of the deaths, rock falls for 29 %, being stranded for 7 %, illnesses for 4 %, and unknown or other causes for 10 %. \u201c84% of accident victims were amateurs who were not under the supervision of a professional\u201d, but I do not know the fraction of people under supervision. If half of the people were, having an accident under supervision is 20.5 % (= 1/0.83 - 1) as likely as having one without supervision.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndzv052bwa5p\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdzv052bwa5p\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Climbing Mont Blanc requires <a href=\"https://www.chamonixmontblancguides.com/mont-blanc-3-day-course%23itinerary\"><u>at least</u></a>&nbsp;3 overnights, 2 in the T\u00eate Rousse refuge (one on the way up, and another on the way down), and 1 in the Go\u00fbter refuge. So, assuming only half of the people use these huts, one can consider more than 1.5 overnight/climb (= 0.5*3), which suggests a risk of death higher than 0.0303 %/climb (= 1.5*0.0202 %). This is arguably an underestimate because not all deaths were included. \u201cOperations conducted by rescue services downhill from the T\u00eate Rousse refuge or uphill from the Go\u00fbter refuge, as well as those performed in the landing areas adjacent to these buildings, were not taken into consideration\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhbg0l4k5s9b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhbg0l4k5s9b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;From Figure 1, deaths have been decreasing, but the correlation is poor (<a href=\"https://en.wikipedia.org/wiki/Coefficient_of_determination\"><u>coefficient of determination</u></a>&nbsp;of 14.1 %), and not statistically significant (<a href=\"https://en.wikipedia.org/wiki/P-value\"><u>p-value</u></a>&nbsp;<a href=\"https://www.socscistatistics.com/pvalues/pearsondistribution.aspx\"><u>of</u></a>&nbsp;0.206). From Table 1, falls accounted for 80.9 % of the deaths, rock falls for 5.3 %, being stranded for 3.3 %, avalanches for 3 %, illnesses for 2 %, crevasse accidents for 1.3 %, material failure for 1 %, lightning for 0.7 %, being crushed for 0.3 %, and others for 2.3 %.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvbtjblkjcui\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvbtjblkjcui\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Mean between the lower and upper bound. \u201cIt has been estimated that there are roughly 100,000\u2013200,000 alpinists active per year in the highest parts of Switzerland\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpoa2v0hyyii\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpoa2v0hyyii\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Mean between the lower and upper bound. \u201cThe reported number of deaths is about 3 to 10 fatalities per year\u201d. 6.50 fatalities per year is consistent with the 6 to 7 calculated <a href=\"https://www.climbmountkilimanjaro.com/deaths-kilimanjaro-true-statistics/\"><u>here</u></a>. \u201cWe estimate the number of people who die on Kili each year to be about six or seven\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0vgrvpoa0kpm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0vgrvpoa0kpm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Calculated based on data from the 1st figure of the section \u201cAlgunos datos de los rescatados en senderismo\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1ugti6kjoqp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1ugti6kjoqp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;However, note one <a href=\"https://www.ultimatekilimanjaro.com/the-beginners-guide-to-climbing-kilimanjaro/\"><u>can only</u></a>&nbsp;climb Kilimanjaro with a guide, so my estimates still apply in that case for having a guide.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1bc978212rb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1bc978212rb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See footnotes 4 and 6.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxnm4mvdkjgc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxnm4mvdkjgc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;For context, Metaculus\u2019 median community <a href=\"https://www.metaculus.com/questions/6592/when-will-a-country-reach-escape-velocity/\"><u>prediction</u></a>&nbsp;for&nbsp;a country reaching longevity escape velocity is 2066. I suppose 100 years may well be quite short if transformative artificial intelligence <a href=\"https://epochai.org/blog/literature-review-of-transformative-artificial-intelligence-timelines\">timelines</a> are only a few decades.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn45exla59rrl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref45exla59rrl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Consequently, correlationally smoking one cigarette reduces life expectancy as much as driving 42.9 km (= 29.3*10^3/683) in the United States.</p></div></li></ol>", "user": {"username": "vascoamaralgrilo"}}, {"_id": "fMCnMCMSEjanhAwpM", "title": "Probably tell your friends when they make big mistakes", "postedAt": "2023-06-01T14:30:35.377Z", "htmlBody": "<p><strong>Big mistakes = Doing something that is actively harmful or useless by their own lights and values, i.e. doesn't help them achieve their life goals. </strong>(Not: Doing something that isn't in line with <i>your</i> values and goals.)</p><p>A lot of people think that others in the EA-ish community are trying to do something impactful but end up doing something harmful or useless. Sometimes they also work on something that they are just not very good at or make other big mistakes. A lot of people never end up telling the other person that they think they are making big mistakes. Sometimes people also just have one particular argument for why the other might do harmful or useless work but not be sure whether it's a bad overall. This also often goes unsaid.</p><p><strong>I think that's understandable and also bad or at least very costly.</strong></p><p><strong>Epistemic status:</strong> Speculation/rant. &nbsp;I know of another person who might post something in this topic that is much more rigorous and has actual background research.</p><h2>Upsides of telling others you think they are making big mistakes, wasting their time, or doing harm:</h2><ul><li>It's good on a community level because people get information that's useful to decide how to achieve their goals (among them, having impact,) so people end up working on less suboptimal things and the community has better impact overall.</li><li>It's good on a community level because it's pushes towards good intellectual conversations and progress.</li><li>I and probably others find it stressful because I can't rely on others telling me if they think I'm doing a bad job, so I have to try to read between the lines. (I find it much less stressful now but when I was more insecure about my competence, I found it really stressful. I think one of my main concerns was others thinking and saying I'm \"meh\" or \"fine\" (with an unenthusiastic tone) but only behind my back.)<ul><li>Note that anxiety works differently for different people though and some people might find the opposite is true for them. See reasons against telling people that you think they are wasting their time or worse.</li></ul></li><li>I and probably others find it pretty upsetting that I can't rely on others being honest with me. It's valuable information and I would like people to act in a way that helps me achieve my stated goals (in this case, doing good), especially if their motivation for not being honest with me is protecting my wellbeing.</li></ul><p>That said, I often don't do a great job at this myself and think telling others you think their efforts would be better spent elsewhere also has significant costs, both personal and on a community level.</p><h2><strong>Downsides of telling others you think they are making big mistakes, wasting their time, or doing harm:</strong></h2><ul><li>Hearing that somebody thinks you're doing harmful or useless work can be extremely discouraging and can lead people to over-update, especially if they are insecure anyway. (Possibly because people do it so rarely, so the signal can be interpreted as stronger than it's intended.)<ul><li>At the same time, we often have noisy information about how good another person's work is, especially how good a fit they are or could be.&nbsp;</li></ul></li><li>Criticising someone's work could lead to an awkward relationship to them. They might also get angry at you or start talking badly about you. This is especially costly if you have a friendly and or a professional relationship.</li><li>An increase in people telling each other what they think about each other's work could create or amplify a culture in which everyone constantly feels like they have to orient themselves towards impact all the time and justify their decisions. This could lead to feelings of guilt, shame, judgement, higher risk-aversion, and an over-emphasis on doing things that are mainstream approved.<ul><li>That said, I think for some people (e.g. hopefully me, unclear) the opposite might be true because stressing about what people are secretly thinking induces more anxiety than knowing they think I'm wasting my time. I would guess that this is true for the type of insecure person who already expects the worst anyway or feels like an imposter, but that's just speculation. (Perhaps people in the comments could speak up.)</li></ul></li><li>While I think people currently don't tell each other enough that they think they are wasting their time, everyone doing it a bit more could result in a situation where many people get way too much feedback to be worth the time to engage with all of it. This is bad if it also makes it harder to filter for the important feedback.</li><li>Often the people with useful information about other people's work are also in particularly difficult situations when it comes to sharing their honest opinion about their work (either for reputational or social reasons or because they have a lot of power over you, making it easy for you to over-update on their opinion): Mentors, funders, employees, collaborators, hiring managers. I think the mentors one is particularly sad because a lot of short-term junior research fellowships have the stated purpose of helping mentees test their fit for research.</li><li>Given the above and probably more, it can be a great drain on your time to tell others you think they are wasting their time and to do it well. I sometimes would like to shoot someone a quick text but know that I realistically would end up investing a lot of time into crafting a careful message and that I would (maybe rightfully) feel obliged to engage if they reply. Especially if you don't know someone very well, aren't sure of your assessment, or are not sure the person could find something more useful to do (other than doing nothing if you think they are doing harm), it often simply doesn't seem worth the time.</li></ul><p>So, where does this leave us?</p><h2>Some ideas and recommendations based on speculation and intuition</h2><ul><li>My hypothesis is that we should tell each other more often when we think others are wasting their time on the macro-level but not the micro-level: I.e., I'm against unsolicitedly telling people that you think their spending their week on something you think doesn't help them achieve their goals. But I'm favour of sometimes unsolicitedly telling people that, by their own lights, maybe they want to prioritise a different cause or work a different job, or give up skilling up in the area they are skilling up in.</li><li>I suspect that some approaches to sharing these kind of thoughts will typically go down better than others:<ul><li>Get into the mindset of being an ally that genuinely wants to help the other person achieve their goals. Take them seriously in their stated goal to do good. (Some people might actually prefer not to hear your thoughts but I think it's good practice to start out by taking what people say about their goals at ~face value.) Remind yourself that you are being kind to them because you are helping them with their life goals instead of cruelly letting them act in ways they would potentially feel terrible about if they had your information.</li><li>Get into the mindset of figuring things out together. Stay curious about why they are doing what they are doing. Even if you are quite convinced that you are right, stay open to at least the possibility of being convinced otherwise. Getting them to agree with you is not the end all be all.</li><li>Instead of starting with \"I think your work is net harmful\", maybe start out with a consideration for why it might be net harmful and explore that together. (I'm not telling you to lie about or obfuscate your overall opinion or how strong it is - I'm just guessing it's sometimes useful to not start with it.)</li><li>Keep in mind that they might already have thought about all the things you want to bring up. If it's easier, maybe start with questions.</li><li>I think there are ways to vibe-check the other person and carefully ask whether they are receptive to this kind of feedback. But I think it requires skill and I don't have great ideas for what to recommend or what to do myself. Starting the meta-conversation about whether and how others want to hear your thoughts will very often make it a bit awkward to then decide not to talk about your object-level thoughts.</li></ul></li><li>If you start telling people more about the mistakes you think they are making, you might want to start praising people (Not necessarily the same people!) more, too, to balance out the overall negative-to-positive feedback ratio. Otherwise, we move from a community where people probably mostly don't get enough feedback to one where you only ever get negative feedback.</li><li>Maybe it's enough to only focus on cases where the benefit-to-cost ratio seems highest. I tentatively think that's the case for friends and mentors. The benefit-to-cost ratio is probably highest when:<ul><li>The person sharing their thoughts is willing and able to engage in some detail. I.e. they are willing to say more than \"I think you work on something bad\" or \"I think you're bad at this kind of work\" and then disengage (although something minimally more informative than the latter could be informative from a mentor-type that has many comparison points and feels fairly confident.)</li><li>The person sharing their thoughts has thought at least somewhat about the other person's work and why they think it's a waste of time. This can include having thought about one specific argument for why the other might be wasting their time even if the person sharing isn't sure about the overall sign.</li><li>The person sharing has <i>some</i> idea about whether the other person wants to hear those kind of thoughts or how to best deliver these thoughts to the other person.</li><li>Somebody is explicitly soliciting feedback.</li></ul></li><li>Maybe we can set up more spaces and events where people specifically opt into getting this kind of feedback<ul><li>My understanding is that <a href=\"https://forum.effectivealtruism.org/posts/dnvYim4nvAyj4225A/doom-circles\">Doom Circling</a> is meant to do this although I've never done it. It also sounds quite intense and part of a specific culture that might not be for everyone.</li><li>Potentially less intense versions of this branded as something like \"Discussing/red-teaming once cause prioritisation/career plan\" could perhaps work. Even if I don't know how exactly this would differ from doom circling, I expect this kind of branding to attract a less brutally honest and more polite crowd. This seems great from the point of view of matching feedback givers/receivers who are interested in similar levels of efficiency vs. niceness.</li></ul></li></ul><p>Finally, you don't have to do any of this. Sharing your honest thoughts can cost you a lot and you should spend your resources on whatever you want to spend them on.</p><h2>Be brutally honest with me &amp; ask me for advice</h2><p>You can give me <a href=\"https://www.admonymous.co/chi\"><strong>anonymous feedback</strong></a><strong>.</strong> I really care about what I do with my life - If you think I could be doing better, please tell me. I'm also happy to receive short, low quality, low information feedback, although the opposite is preferred of course. If you wanna be extra fantastic and give me non-anonymous feedback, so I can engage with you, I will be over the moon and very grateful - but don't let the perfect be the enemy of the good.</p><p>Also, if you think you have important thoughts to share with someone but you don't know how, feel free to dm me. I wrote this post in one go, so maybe I'll have more useful tips for how to actually go about sharing your honest thoughts in individual cases :) I might take a while to respond or decide not to engage in detail though.</p>", "user": {"username": "Chi"}}, {"_id": "riYnjstGwK7hREfRo", "title": "Podcast Interview with David Thorstad on Existential Risk, The Time of Perils, and Billionaire Philanthropy", "postedAt": "2023-06-04T08:52:50.928Z", "htmlBody": "<p>I have released a new episode of my podcast, EA Critiques, where I interview David Thorstad. David is a researcher at the Global Priorities Institute and also writes about EA on his blog, <a href=\"https://ineffectivealtruismblog.com/\">Reflective Altruism</a>.</p><p>In the interview we discuss three of his blog post series:</p><ol><li><a href=\"https://ineffectivealtruismblog.com/category/my-papers/existential-risk-pessimism/\"><strong>Existential risk pessimism and the time of perils</strong></a><strong>:</strong> Based on his academic paper of the same name, David argues that there is a surprising tension between the idea that there is a high probability of extinction (existential risk pessimism) and the idea that the expected value of the future, conditional on no existential catastrophe this century, is astronomically large.</li><li><a href=\"https://ineffectivealtruismblog.com/category/exaggerating-the-risks/\"><strong>Exaggerating the risks</strong></a><strong>: </strong>David argues that the probability of an existential catastrophe from any source is much lower than many EAs believe. At time of recording the series only covered risks from climate change, but future posts will make the same case for nuclear war, pandemics, and AI.</li><li><a href=\"https://ineffectivealtruismblog.com/category/billionaire-philanthropy/\"><strong>Billionaire philanthropy</strong></a><strong>: </strong>Finally, we talk about the the potential issues with &nbsp;billionaires using philanthropy to have an outsized influence, and how both democratic societies and the EA movement should respond.</li></ol><p>As always, I would love feedback, on this episode or the podcast in general, and guest suggestions. You can write a comment here, send me a message, or use <a href=\"https://critiquesofea.podbean.com/e/astronomical-value-existential-risk-and-billionaires-with-david-thorstad/#:~:text=Anonymous%20feed%20back%20form\">this anonymous feedback </a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfrwCZ1qtEfhzvkHC2ufepSJ4dFCaIN6Sdy20lxR6gmRYbKSw/viewform\">form</a>.&nbsp;</p>", "user": {"username": "Nick_Anyos"}}]