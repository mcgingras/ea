[{"_id": "Mj8hu5kXfh7dtoR3A", "postedAt": "2023-07-25T19:21:45.436Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>This was a difficult post, and my first post for SoGive as the Lead Researcher &amp; Philanthropy Advisor! I hope it can be useful to our discussions on cost-effectiveness.</p><p>I hope my uncertainty comes though. I haven't been thinking about the size of the future for a very long time, but I learned a lot from writing this. As I mentioned at the beginning, please leave feedback on my assumptions, math, and methods, so I can write better posts about thresholds in the future.</p><p>It might be a while, but I'd like to do some writing about cost-effectiveness thresholds for animal advocacy and multipliers as well. Feel free to leave your thoughts about those as a reply to this comment as well.</p>", "parentCommentId": null, "user": {"username": "Spencer Ericson"}}, {"_id": "PAjMvkiyQsniSBD6n", "postedAt": "2023-07-25T19:58:02.259Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Cool! One point from a quick skim - the number of animals wouldn't be lost in many kinds of human extinction events or existential risks. Only a subset would erase the entire biosphere - e.g. a resource-maximising rogue AI, vacuum decay, etc. Presumably with extinction of just humans the animal density of reclaimed land would be higher than current, so the number of animals would rise (assuming it outweighs the end of factory farming).&nbsp;</p><p>The implications of human existential risks for animals is interesting, and I can see some points either way depending on the moral theory (e.g. end of factory farming in human extinction, but rise of wild animal suffering; total number and quality of animal lives in a beyond-Earth humanity; potential of a completely re-wilded Earth in a beyond-Earth humanity; risks of astronomical suffering if a beyond-Earth humanity retains the equivalent of factory farming...)</p>", "parentCommentId": null, "user": {"username": "BenStewart"}}, {"_id": "tioxsARqnnHXJsA7X", "postedAt": "2023-07-25T20:01:49.090Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Thanks Ben! I totally agree. The math in this post was trying to get at upper and lower bounds and a median -- but for setting one's personal thresholds, the nuance you mention is incredibly important. I hope this post, and the Desmos tool I linked, can help people play with these numbers and set their own thresholds!</p>", "parentCommentId": "PAjMvkiyQsniSBD6n", "user": {"username": "Spencer Ericson"}}, {"_id": "WrQhzonbm7AQyEHxC", "postedAt": "2023-07-26T10:32:02.737Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Interesting read, and a tricky topic! A few thoughts:</p><ol><li>What were the reasons for tentatively suggesting using the median estimate of the commenters, rather than being consistent with the SoGive neartermist threshold?</li><li>One reason against using the very high-end of the range is the plausible existence of alien civilisations. If humanity goes extinct, but there are many other potential civilisations and we think they have similar moral value to humans, then preventing human extinction is less valuable.<ol><li>You could try using an adapted version of the <a href=\"https://en.wikipedia.org/wiki/Drake_equation\">Drake equation</a> to estimate how many civilisations there might be (some of the parameters would have to be changed to take into account the different context, i.e. you're not just estimating current civilizations that could currently communicate with us in the Milky Way, but the number there could be in the Local Supercluster)</li></ol></li><li>I'm still not entirely sure what the purpose of the threshold would be.<ol><li>The most obvious reason is to compare longtermist causes with neartermist ones, to understanding the opportunity cost - in which case I think this threshold should be consistent with the other SoGive benchmarks/thresholds (i.e. what you did with your initial calculations).<ol><li>Indeed the lower end estimate (only valuing existing life) would be useful for donors who take a completely neartermist perspective, but who aren't set on supporting (e.g.) health and development charities</li></ol></li><li>If the aim is to be selective amongst longtermist causes so that you're not just funding all (or none) of them, then why not just donate to the most cost-effective causes (starting with the most cost-effective) until your funding runs out?<ol><li>I suppose this is where the giving now vs giving later point comes in. But in this case I'm not sure how you could try to set a threshold <i>a priori</i>?&nbsp;<ol><li>It seems like you need some estimates of cost-effectiveness first. Then (e.g.) choose to fund the top x% of interventions in one year, and use this to inform the threshold in subsequent years. Depending on the apparent distribution of the initial cost-effectiveness estimates, you might decide 'actually, we think there are plenty of interventions out there that are better than all the ones we have seen so far, if only we search a little bit harder'</li></ol></li></ol></li></ol></li><li>Trying to incentivise more robust thinking around the cost-effectiveness of individual longtermist projects seems really valuable! I'd like to see more engagement by those working on such projects. Perhaps SoGive can help enable such engagement :)</li></ol>", "parentCommentId": null, "user": {"username": "Matt_Sharp"}}, {"_id": "hhx962TpNAMG92TeK", "postedAt": "2023-07-26T13:46:44.293Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Nice post, Spencer!</p><blockquote><p>0.01% absolute reduction</p></blockquote><p>Nitpick, an absolute reduction of 10^-4 is often indicated as 0.01 <a href=\"https://en.wikipedia.org/wiki/Percentage_point\">pp</a> (0.01 percentage points).</p><blockquote><p>Furthermore, we might never have enough evidence to say whether an intervention has reduced cumulative x-risk by a certain amount. It might be more manageable to set a threshold based on reduction in per-century x-risk.</p></blockquote><p>I would go a little further, and say that we might never have enough evidence to say whether non-extinction x-risk this century was reduced, as I think the evidence base for non-extinction <a href=\"https://forum.effectivealtruism.org/topics/value-lock-in\">value lock-in</a> is quite poor (related links <a href=\"https://docs.google.com/document/d/1mkLFhxixWdT5peJHq4rfFzq4QbHyfZtANH1nou68q88/edit#heading=h.17f05s8r0u3q\">here</a>). So I believe it is better to focus on extinction risk, or probability of a given population loss, as forecasted in the <a href=\"https://forum.effectivealtruism.org/posts/un42vaZgyX7ch2kaj/announcing-forecasting-existential-risks-evidence-from-a\">Existential-Risk Persuasion Tournament</a>.</p><figure class=\"table\" style=\"height:1733.12px;width:700px\"><table style=\"background-color:rgb(255, 255, 255);border:1px double rgb(179, 179, 179)\"><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Ajeya Cotra</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><a href=\"https://forum.effectivealtruism.org/posts/xb4twLYLCrSgw7eDd/ajeya-cotra-on-worldview-diversification-and-how-big-the\"><u>\u201cAI risk is something that we think has a currently higher cost effectiveness\u201d</u></a></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><p>\u201c$200 trillion per world saved\u201d</p><p>Or&nbsp;<strong>$20 billion per bp</strong></p></td></tr></tbody></table></figure><p>To clarify, the above estimate is a conservative cost-effectiveness bar for Open Philanthropy's longtermist grants. In <a href=\"https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/#last-dollar-project-022528\">this</a> section of <a href=\"https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/\">episode 90</a> of The 80,000 Hours Podcast, Ajeya says it concerns \"meta R&amp;D to make responses to new pathogens faster\", and \"[Open Philanthropy] were aiming for this to be conservative\".</p><blockquote><p>Were these commenters expecting it to be much cheaper to save a life by preventing the loss of potential in an extinction, than to save a life using near-termist interventions?</p></blockquote><p>I guess so.</p><blockquote><p>These estimates are not robust enough to make the most important decisions we face. We recommend conducting a survey of funders, charities, and experts to get a stronger picture of what the standard should be and the cost-effectiveness of different types of work.</p></blockquote><p>I would say better quantitative models would also be needed to more reliably estimate the cost-effectiveness of interventions aiming to decrease extinction risk.</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "dwZAviuqgXnCaftHH", "postedAt": "2023-07-26T15:17:52.163Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Thanks Matt!</p><ol><li>My estimate was just one estimate. I could have included it in the table but when I did the table it seemed like such an outlier, and done with a totally different method as well, perhaps useful for a different purpose... It might be worth adding it into the table? Not sure.</li><li>Interesting consideration! If we expect humanity to at one point technologize the LS, and extinction prevents that, don't we still lose all those lives? It would not eradicate all life if there were aliens, but still the same amount of life in total. (I'm not endorsing any one prediction for how large the future will be.) My formulas here don't quantify how much worse it is to lose 100% of life than 99% of life.</li><li>Sure, you could set your threshold differently depending on your purpose. I could have made this clearer!<ol><li>Exactly as you say, comparing <i>across</i> cause areas, you might want to keep the cost you're willing to pay for an outcome (a life) consistent.</li><li>If you've decided on a worldview diversification strategy that gives you separate buckets for different cause areas (e.g. <a href=\"https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/#worldview-diversification-000845\">by credence instead of by stakes</a>), then you'd want to set your threshold separately for different cause areas, and use each threshold to compare <i>within</i> a cause area. If you set a threshold for what you're willing to pay for a life within longtermist interventions, and fewer funding opportunities live up to that compared to the amount of money you have available, you can save some of your money in that bucket and donate it later, in the hopes that new opportunities that meet your threshold can arise. For an example of giving later based on a threshold, Open Philanthropy wants to give money each year to projects that are more cost-effective than what they will spend their <a href=\"https://www.openphilanthropy.org/research/good-ventures-and-giving-now-vs-later-2016-update/\">\"last dollar\"</a> on.</li></ol></li><li>Thanks, me too!</li></ol>", "parentCommentId": "WrQhzonbm7AQyEHxC", "user": {"username": "Spencer Ericson"}}, {"_id": "qSEgPXfBpPaTqDNNs", "postedAt": "2023-07-26T15:21:13.504Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Thanks Vasco! This helps my understanding.</p>", "parentCommentId": "hhx962TpNAMG92TeK", "user": {"username": "Spencer Ericson"}}, {"_id": "ZiWHhGaGETixLtyHy", "postedAt": "2023-07-26T21:01:42.582Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Re 2 - ah yeah, I was assuming that at least one alien civilisation would aim to 'technologize the Local Supercluster' if humans didn't. If they all just decided to stick to their own solar system or not spread sentience/digital minds, then of course that would be a loss of experiences.</p><p>Thanks for clarifying 1 and 3!</p>", "parentCommentId": "dwZAviuqgXnCaftHH", "user": {"username": "Matt_Sharp"}}, {"_id": "zymwNDexppGyjTwhG", "postedAt": "2023-09-04T16:48:46.979Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Hi Spencer,</p><p>You may be interested in Brian Tomasik's analysis on <a href=\"https://longtermrisk.org/how-the-simulation-argument-dampens-future-fanaticism\">How the Simulation Argument Dampens Future Fanaticism</a>. I think its essence is well captured in <a href=\"https://web.archive.org/web/20160627084201/http://felicifia.org:80/viewtopic.php?t=899\">this</a> short comment by Pablo Stafforini:</p><blockquote><p>According to the simulation argument, either (1) humanity will soon become extinct; (2) posthumanity will never run ancestor simulations; or (3) we are almost certainly living in a simulation. Suppose (1) is true. Then the classical utilitarian case for focusing on existential risk reduction loses much of its force, since we are by assumption doomed to perish quickly anyway. Now suppose (3) is true. Here it seems plausible that the simulators will restart the simulation very quickly after the sims manage to kill themselves. So the case for focusing on existential risk is also weakened considerably. It is only on the second of the three scenarios that extinction is (roughly) as bad as classical utilitarians take it to be. So we can conclude: if you think there is a chance that posthumanity will run ancestor simulations (~2), the prospect of human extinction is much less serious than you thought it was.</p></blockquote><p>If I recall correctly, the argument goes more or less as follows. The larger the future, the greater the likelihood of us being in a short-lived simulation, thus having negligible influence in the far future. The 2 effects cancel out, and therefore the ratio between far future value and near future value does not depend on the size of the future. The ratio is roughly inversely proportional to the fraction of resources going towards simulations, i.e. 2 times as much resources going to simulations means the far future is half as valuable relative to the near term future.</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "5tjWnEmozKxxTFGGy", "postedAt": "2023-09-05T17:24:31.683Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>A couple of nitpicks on the calculations for the size of the moral universe:</p><ul><li>The the average span of a mammalian species seems somewhat a spurious metric to use for humanity, given that we have already largely nullified the natural environmental and evolutionary drivers of such a bound (via technology and civilisation<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkxyntolpngr\"><sup><a href=\"#fnkxyntolpngr\">[1]</a></sup></span>).</li><li>perhaps the length of the <a href=\"https://en.wikipedia.org/wiki/Future_of_an_expanding_universe#The_Stelliferous_Era\">Stelliferous Era</a> (~100T years) is a more natural bound than the lifetime of the Sun (~5B years). It seems unlikely that we would colonise the solar system and last for billions of years without successfully crossing interstellar space.</li><li><a href=\"https://en.wikipedia.org/wiki/Open_individualism\">Open Individualism</a> suggests that we should be counting person-years (or animal-years) rather than individual animals. Intuitively, this seems more natural especially for animals with lower complexity brains who are unlikely to have much of a sense of self (even if they are sentient and can suffer).</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkxyntolpngr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkxyntolpngr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is not to say that we won't go extinct from artificial environmental and evolutionary causes of our own making. Just that bounding to a number (1M years) that originates from natural (nonanthropogenic) processes seems highly arbitrary.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "Sb9ij8KKupBAJQi9K", "postedAt": "2023-09-05T17:30:01.023Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Great that SoGive is setting these benchmarks. I'm of the opinion that we are very much in a time of perils, and indeed the entire future may well hinge on the <a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and\">next few years (or less)</a>, with regard to AI x-risk. I also think that work aimed at <a href=\"https://forum.effectivealtruism.org/topics/slowing-down-ai\">slowing down AI</a> is woefully neglected. Right now, I think such work (<a href=\"https://forum.effectivealtruism.org/posts/zakNChFRxPcYKyieh/which-organisation-is-most-effective-for-the-the-marginal?commentId=xBptYhEnagLfJWkA2\">examples</a>) could easily be as cost effective as $1M per bp of x-risk reduction (this century at least).</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "ArybJopeF2HrHaaAD", "postedAt": "2023-09-05T17:44:36.059Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Note also that if timelines are short, then the cost effectiveness of saving lives via traditional neartermist interventions decreases. IIRC, GiveWell equates a \"life saved\" with ~40 years extra life (or 40 QALYs). So if we only have 5 years left before extinction, then the cost effectiveness estimates worsen by a factor of 8 (40/5). So instead of \u00a35,000 to save a life, neartermist interventions would need to clear a bar of \u00a3625 to save a life. Although I think at this point we might be better off just thinking in terms of \u00a3/QALY. And <a href=\"https://thinkingaboutcharity.blogspot.com/2020/05/how-cost-effective-is-cost-effective.html\">interventions</a> aimed at averting severe depression (\"\u00a3200 to avert one year of severe depression\") or chicken suffering (\"\u00a35 to avert the suffering of one chicken who is living in very poor conditions\") seem more promising.</p>", "parentCommentId": "Sb9ij8KKupBAJQi9K", "user": {"username": "Greg_Colbourn"}}, {"_id": "Y3SCBSWn9dAEBGSCP", "postedAt": "2023-09-05T17:56:00.662Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Also, for this reason I think that x-risk reduction should very much <a href=\"https://forum.effectivealtruism.org/posts/KDjEogAqWNTdddF9g/long-termism-vs-existential-risk\">not be lumped in with longtermism</a>. It is, in fact, very near term, now.</p>", "parentCommentId": "Sb9ij8KKupBAJQi9K", "user": {"username": "Greg_Colbourn"}}, {"_id": "JuCwiCozoYry5AA8a", "postedAt": "2023-09-05T18:25:25.328Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Thank you Vasco! This seems hard to model, but worthwhile. I'll think on it.</p>", "parentCommentId": "zymwNDexppGyjTwhG", "user": {"username": "Spencer Ericson"}}, {"_id": "ctBZYBugkK9tsaJww", "postedAt": "2023-09-05T18:38:49.895Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Interesting point about how any extinction timelines less than the length of a human life change the thresholds we should be using for neartermism as well! Thank you, Greg. I'll read what you linked.</p>", "parentCommentId": "ArybJopeF2HrHaaAD", "user": {"username": "Spencer Ericson"}}, {"_id": "mD9mfrftRWNXaNTfc", "postedAt": "2023-09-06T19:54:09.979Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Interesting. But how soon is \"soon\"? And even if we are a simulation, to all intents and purposes it is real to us. It doesn't seem like much of a consolation that the simulators might restart the simulation after we go extinct (any more than the Many Worlds interpretation of Quantum Mechanics gives solace over many universes still existing nearby in probability space in the multiverse).&nbsp;<br><br>Maybe the simulators will stage an intervention over us reaching the Singularity. I don't think we can rely on this though (indeed, this is part of the <a href=\"https://forum.effectivealtruism.org/posts/THogLaytmj3n8oGbD/p-doom-or-agi-is-high-why-the-default-outcome-of-agi-is-doom#:~:text=This%20is%20my,to%20save%20us!\">exotic scenarios that make up the ~10% chance that I think we aren't doomed from AGI by default</a>).</p>", "parentCommentId": "zymwNDexppGyjTwhG", "user": {"username": "Greg_Colbourn"}}, {"_id": "nqMcjtqCgPPqXodys", "postedAt": "2023-09-07T08:24:22.052Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Thanks for engaging, Greg!</p><blockquote><p>But how soon is \"soon\"?</p></blockquote><p>I seem to remember a comment from Carl Shulman saying the risk of simulation shut-down should not be assumed to be less than 1 in 1 M per year (or maybe it was per century). This suggests there is still a long way before it happens. On the other hand, I would intuitively think the risk to be higher if the time we are in really is special. I do not remember whether the comment was taking that into account.</p><blockquote><p>And even if we are a simulation, to all intents and purposes it is real to us. It doesn't seem like much of a consolation that the simulators might restart the simulation after we go extinct (any more than the Many Worlds interpretation of Quantum Mechanics gives solace over many universes still existing nearby in probability space in the multiverse).</p></blockquote><p>Yes, it is not a consolation. It is an argument for focussing more on interventions which have nearterm benefits, like corporate campaigns for chicken welfare, instead of ones whose benefits may not be realised due to simulation shut-down.</p>", "parentCommentId": "mD9mfrftRWNXaNTfc", "user": {"username": "vascoamaralgrilo"}}, {"_id": "Hh53Cctfm2BZpkjRo", "postedAt": "2023-09-07T10:27:43.427Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<blockquote><p>Yes, it is not a consolation. It is an argument for focussing more on interventions which have nearterm benefits, like corporate campaigns for chicken welfare, instead of ones whose benefits may not be realised due to simulation shut-down</p></blockquote><p>I still don't think this goes through either. I'm saying we should care about our world going extinct just as much as if it were the only world (given we can't causally influence the others).</p>", "parentCommentId": "nqMcjtqCgPPqXodys", "user": {"username": "Greg_Colbourn"}}, {"_id": "GsrK4qE9JQqHJjBDR", "postedAt": "2023-09-12T05:51:26.857Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<blockquote><p>Were these commenters expecting it to be much cheaper to save a life by preventing the loss of potential in an extinction, than to save a life using near-termist interventions?</p></blockquote><p><br>I think that commenters are looking at the cost-effectiveness they could reach with current budget constraints. If we had way more money for longtermism, we could go to a higher cost per basis point. That is different than the&nbsp;<strong>value&nbsp;</strong>of reducing a basis point, which very well could be astronomical, given GiveWell costs for saving a life (though to be consistent, one should try to estimate the long-term impacts of a GiveWell intervention as well).</p>", "parentCommentId": null, "user": {"username": "Denkenberger"}}, {"_id": "sLsPofqEh2tQ7vqu4", "postedAt": "2023-09-18T13:12:19.539Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>If we're looking at upper bounds, even the Stelliferous Era is highly conservative. The Black Hole era could last up to 10&lt;sup&gt;100&lt;/sup&gt; years <a href=\"https://en.wikipedia.org/wiki/The_Five_Ages_of_the_Universe\">https://en.wikipedia.org/wiki/The_Five_Ages_of_the_Universe</a> and it's at least conceivable under known physics that we could farm their rotational <a href=\"https://arxiv.org/abs/0804.1912\">energy</a> or still more speculatively their <a href=\"https://www.youtube.com/watch?v=Qam5BkXIEhQ\">Hawking radiation</a>.&nbsp;</p><p>Usual Pascalian reasoning applies in that this would allow such a ridiculously large number of person-years that even with an implausibly low credence in its possibility the expectation dwarfs the whole stellar era.</p>", "parentCommentId": "5tjWnEmozKxxTFGGy", "user": {"username": "Arepo"}}, {"_id": "q9wcDaTcjKQRWFtxp", "postedAt": "2023-09-18T13:38:02.397Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>I find this argument unconvincing. The vast majority of 'simulations' humans run are very unlike our actual history. The modal simulated entity to date is probably an NPC from World of Warcraft, a zergling from Starcraft or similar. This makes it incredibly speculative to imagine what <i>our</i> supposed simulators might be like, what resources they might have available and what their motivations might be.</p><p>Also the vast majority of 'simulations' focus on 'exciting' moments - pitched Team Fortress battles, epic RPG narratives, or at least active interaction with the simulators. If you and your workmates are just tapping away in your office on your keyboard doing theoretical existential risk research, the probability that someone <i>like us</i> has spent their precious resources to (re)create you seem radically lowered than if you're (say) fighting a pitched battle.</p>", "parentCommentId": "zymwNDexppGyjTwhG", "user": {"username": "Arepo"}}, {"_id": "gLdKmwWzu3jfT7jTC", "postedAt": "2023-09-18T14:11:19.380Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>The problem with this position is that the Black Hole Era\u2014at least, the way the \u201c<a href=\"https://en.wikipedia.org/wiki/The_Five_Ages_of_the_Universe\">Five Ages of the Universe</a>\u201d article you link to defines it\u2014only starts after proton decay has run to (effective) completion,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn4zz46fqydb\"><sup><a href=\"#fnn4zz46fqydb\">[1]</a></sup></span>&nbsp;which means that all matter will be in black holes, which means that conscious beings will not exist to farm black holes for their energy. (If do, however, agree that life is in theory not dependent on luminous stars, and so life could continue beyond the Stelliferous Era and into the Degenerate Era, which adds many years.)</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn4zz46fqydb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn4zz46fqydb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Whether proton decay will actually happen is still a major open question in physics. See, for example, <a href=\"https://www.kavlifoundation.org/news/the-enduring-quest-for-proton-decay\">Hadhazy (2021)</a> or <a href=\"https://www.forbes.com/sites/startswithabang/2020/01/03/how-certain-are-we-that-protons-dont-decay/\">Siegel (2020)</a>.</p><p>(Additionally, if proton decay does happen, there's then the question of \u201ccould a technologically mature civilization stop proton decay?\u201d. My money would be on \u201cno\u201d, but of course our current understanding of particle decay physics could be incorrect, or an advanced civilization might find an ingenious workaround.)</p></div></li></ol>", "parentCommentId": "sLsPofqEh2tQ7vqu4", "user": {"username": "Will Aldred"}}, {"_id": "2Hdf7HmQgXjHLw3av", "postedAt": "2023-09-18T16:43:09.479Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Directionally, I agree with your points. On the last one, I'll note that counting person-years (or animal-years) falls naturally out of empty individualism as well as open individualism, and so the point goes through under the (substantively) weaker claim of \u201ceither open or empty individualism is true\u201d.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhi2xg5i6517\"><sup><a href=\"#fnhi2xg5i6517\">[1]</a></sup></span></p><p>(You may be interested in <a href=\"https://www.quora.com/What-does-David-Pearce-think-of-closed-empty-and-open-individualism\">David Pearce's take on closed, empty, and open individualism</a>.)</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhi2xg5i6517\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhi2xg5i6517\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For the casual reader: The three candidate theories of personal identity are <a href=\"https://manuherran.com/empty-open-and-closed-individualism/\">empty, open, and closed individualism</a>. Closed is the common sense view, but most people who have thought seriously about personal identity\u2014e.g., <a href=\"https://www.stafforini.com/docs/Parfit%20-%20The%20unimportance%20of%20identity.pdf\">Parfit</a>\u2014have concluded that it must be false (tl;dr: because nothing, not memory in particular, can \u201ccarry\u201d identity in the way that's needed for closed individualism to make sense). Of the remaining two candidates, open appears to be the fringe view\u2014supporters include <a href=\"https://link.springer.com/book/10.1007/978-1-4020-3014-7\">Kolak</a>, <a href=\"https://opentheory.net/2018/09/a-new-theory-of-open-individualism/\">Johnson</a>, <a href=\"https://www.smashwords.com/books/view/719903\">Vinding</a>, and <a href=\"https://qualiacomputing.com/2020/06/06/mini-series-on-open-individualism/\">Gomez-Emilsson</a> (although <a href=\"http://phantomself.org/kolak-i-am-you/#comment-19214\">Kolak's response</a> to <a href=\"http://phantomself.org/kolak-i-am-you/\">Cornwall</a> makes it unclear to what extent he is indeed a supporter). Proponents of (what we now call) empty individualism include Parfit, <a href=\"https://en.wikipedia.org/wiki/Philosophical_Explanations\">Nozick</a>, <a href=\"https://www.jstor.org/stable/2216134\">Shoemaker</a>, and <a href=\"https://www.routledge.com/Humes-Philosophy-Of-The-Self/Pitson/p/book/9780415248020\">Hume</a>.</p></div></li></ol>", "parentCommentId": "5tjWnEmozKxxTFGGy", "user": {"username": "Will Aldred"}}, {"_id": "BKLTbsbiWHjwcKezg", "postedAt": "2023-09-19T18:54:35.083Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>As you say, whether proton decay will happen seems to be an open question. If you're feeling highly confident you could knock off another couple of zeroes to represent that credence and still end up with a number that eclipses everything else.</p>", "parentCommentId": "gLdKmwWzu3jfT7jTC", "user": {"username": "Arepo"}}, {"_id": "kh8sMXrGBRz3kcWm7", "postedAt": "2023-09-20T12:24:13.242Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Agree. I find Empty Individualism pretty depressing to think about though. And Open Individualism seems more natural, from (my) subjective experience.&nbsp;</p>", "parentCommentId": "2Hdf7HmQgXjHLw3av", "user": {"username": "Greg_Colbourn"}}, {"_id": "2ga6WsFTtQt5XZRct", "postedAt": "2023-09-20T18:36:49.274Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Thanks for commenting!</p><blockquote><p>I find this argument unconvincing. The vast majority of 'simulations' humans run are very unlike our actual history. The modal simulated entity to date is probably an NPC from World of Warcraft, a zergling from Starcraft or similar. This makes it incredibly speculative to imagine what <i>our</i> supposed simulators might be like, what resources they might have available and what their motivations might be.</p></blockquote><p>Agreed, but could you explain why that would be an objection to Brian's argument?</p><blockquote><p>Also the vast majority of 'simulations' focus on 'exciting' moments - pitched Team Fortress battles, epic RPG narratives, or at least active interaction with the simulators. If you and your workmates are just tapping away in your office on your keyboard doing theoretical existential risk research, the probability that someone <i>like us</i> has spent their precious resources to (re)create you seem radically lowered than if you're (say) fighting a pitched battle.</p></blockquote><p>I do not know, because I agree with your 1st paragraph about it being quite hard to predict future simulated entities based on past history.</p>", "parentCommentId": "q9wcDaTcjKQRWFtxp", "user": {"username": "vascoamaralgrilo"}}, {"_id": "3e3j82cDqbmyXiF2Z", "postedAt": "2023-09-21T15:27:55.795Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Agreed, but if the lifespan of the only world is much shorter due to risk of simulation shut-down, the loss of value due to extinction is smaller. In any case, this argument should be weighted together with many others. I personally still direct 100 % of my donations to the Long-Term Future Fund, which is essentially funding AI safety work. Thanks for your work in this space!</p>", "parentCommentId": "Hh53Cctfm2BZpkjRo", "user": {"username": "vascoamaralgrilo"}}, {"_id": "urf7TZCSo7BzyjGXM", "postedAt": "2023-09-22T04:03:04.688Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Thanks for your donations to the LTFF. I think they need to start <a href=\"https://forum.effectivealtruism.org/posts/ee8Pamunhqabucwjq/long-term-future-fund-ask-us-anything-september-2023?commentId=k6qFNirwiyMxR4CQm\">funding stuff aimed</a> <a href=\"https://forum.effectivealtruism.org/posts/ee8Pamunhqabucwjq/long-term-future-fund-ask-us-anything-september-2023?commentId=b6bfh7dknvr6ixBCx\">at slowing AI down</a> (/pushing for a global moratorium on AGI development). There's not enough time for AI Safety work to bear fruit otherwise.</p>", "parentCommentId": "3e3j82cDqbmyXiF2Z", "user": {"username": "Greg_Colbourn"}}, {"_id": "gDLAdZAYQpPNumc5A", "postedAt": "2023-09-22T10:48:42.464Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>I mainly had in mind Pablo's summary. It's been a long time since I read Brian's essay, and I don't have bandwidth to review it now, so if he says something substantially different there, my argument might not apply. But basically every argument I remember hearing about how the simulation argument implies we should modify our behaviour presupposes that we have some level of inferential knowledge of our simulators (this presupposition being hidden in the assumption that simulations would be primarily ancestor simulations). This presupposition seems basically false to me, because, for example:</p><p>a. A zergling would struggle to gain much inferential knowledge of its simulators' motivations.</p><p>b. A zergling looking around at the scope and complexity of its universe would typically observe that it itself is 2-dimensional (albeit with some quasi-3D properties), and is made from approx <a href=\"https://vsbattles.fandom.com/wiki/User_blog:DerpCity/Size_of_the_Starcraft_Zerg_Units#Unit_Heights\">38x94 'atoms'</a>. Perhaps more advanced simulations would both be more numerous (and hence a higher proportion of simulationspace) and more complex, but it still seems hard to imagine they'll average to anything like the same level of complexity as we see in our universe, or have a consistent difference from it.</p><p>c. If the simulation argument is correct for a single layer of reality, it seems (to the degree permitted by a and b) far more likely that it's correct for multiple, perhaps vast numbers of layers of reality (insert 'spawn more Overlords' joke here). Thus the people whose decisions and motivations a zergling is trying to ultimately guess at is not ours, but someone whose distance from us is approx&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"n(\\textit{|human - zergling|})\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"padding-bottom: 0.314em;\">|</span><span class=\"mjx-charbox MJXc-TeX-main-I\" style=\"padding-bottom: 0.25em;\">human - zergling</span><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"padding-bottom: 0.314em;\">|</span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>, where <i>n</i> is the number of layers. It's hard to imagine the zergling - or us - could make any intelligible assumptions at all about them at that level of removal.&nbsp;</p><p>To show this in Pablo's argument:</p><blockquote><p>Now suppose (3) is true. Here it seems plausible that the simulators will restart the simulation very quickly after the sims manage to kill themselves.&nbsp;</p></blockquote><p>For this to be 'plausible' is to assert that we know our simulators' motivations well enough to know that whatever they hoped to gain by running us will 'plausibly' be motivating enough for them to do it a second time in much the same form, and that <i>their</i> simulators will at least permit it, and so on.&nbsp;</p><p>Another version of the anti-x-risk argument from simulation I've heard (and which I confess with hindsight I was conflating Pablo's with - maybe it's part of Brian's argument?) is that the simulators will likely switch off our universe if it expands beyond a certain size due to resource constraints. Again, this argument implies IMO vastly too high confidence in both their motivation and resource limits.</p>", "parentCommentId": "2ga6WsFTtQt5XZRct", "user": {"username": "Arepo"}}, {"_id": "Hj7idcqwhXjdA8o7q", "postedAt": "2023-10-01T18:30:34.901Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Thanks for explaining that!</p><p>Brian <a href=\"https://longtermrisk.org/how-the-simulation-argument-dampens-future-fanaticism#Simplifying_LS\">concludes</a> that L/S = T*D/F, where:</p><ul><li>L is the cost-effectiveness of longtermist interventions.</li><li>S is the cost-effectiveness of neartermist interventions.</li><li>T \"represent[s] how much more important it is to influence a unit of sentience by the average future digital agent than a present-day biological one for these reasons [\"future, simulated human might have much higher intensity of experience per unit time, and we may have much greater control over the quality of his experience\"]\".</li><li>D is \"a discount representing how much harder it is to actually end up helping a being in the far future than in the near term, due to both uncertainty and the muted effects of our actions now on what happens later on\".</li><li>F is \"the fraction of all computational sent-years spent non-solipsishly simulating almost-space-colonizing ancestral planets (both the most intelligent and also less intelligent creatures on those planets)\". \"A non-solipsish simulation is one in which most or all of the people and animals who seem to exist on Earth are actually being simulated to a non-trivial level of detail\".</li></ul><p>Brian <a href=\"https://longtermrisk.org/how-the-simulation-argument-dampens-future-fanaticism#Plugging_in_parameter_values\">guesses</a> T = 10^4, D = 10^-3, and F = 10^-6, thus concluding L/S = 10^7. I guess you are saying with your comment just above that F should be much lower than 10^-6? For reference, here is Brian's motivation for F = 10^-6:</p><blockquote><p>It's very unclear how many simulations of almost-space-colonizing planets superintelligences would run. The fraction of all computing resources spent on this might be close to 100% or might be below 10<sup>-15</sup>. It's hard to predict resource allocation by advanced civilizations. But I set this parameter based on assuming that ~10<sup>-4</sup> of sent-years will go toward ancestor simulations <i>of some sort</i> (this is probably too high, but it's biased upward in expectation, since, e.g., maybe there's a 0.05% chance that post-humans devote 20% of sent-years to ancestor simulations), and only 1% of those simulations will be of the almost-space-colonizing period (since there might also be many simulations of the origin of life, prehistory, and the early years after a planet's \"singularity\"). If we think that simulations contain more sentience per petaflop of computation than do other number-crunching calculations, then 10<sup>-4</sup> of sent-years devoted to ancestor simulations of some kind may mean less than 10<sup>-4</sup> of all raw petaflops devoted to such simulations.</p></blockquote>", "parentCommentId": "gDLAdZAYQpPNumc5A", "user": {"username": "vascoamaralgrilo"}}, {"_id": "gxYpHSX6hb2cmSDSY", "postedAt": "2023-10-01T18:51:35.705Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>The possibility of us living in a short-lived simulation isn't enough to count much against longtermism, because it's also possible we could live in a long-lived simulation or a long-lived world, and those possibilities will be much higher stakes, so still dominate expected value calculations unless we assign them <i>tiny</i> probability together.</p><p>I think the argument crucially depends on the assumption that simulations will be disproportionately short-lived, and we have acausal influence over agents in other simulations. If for each long-running world (simulated or otherwise) with moral agents and moral patients, there are N short-lived worlds with (moral) agents and moral patients, and our actions are correlated with those of agents across worlds, then we get to decide for more agents in in short-lived worlds than long-lived ones. Basically, acausal influence will boost the expected value of all interventions, but if moral patients are disproportionately in short-lived simulations with agents whose decisions we're correlated with relative to long-run simulations with agents whose decisions we're correlated with (or more skewed towards the short-lived than it seems for our own world), acausal influence will disproportionately boost the expected value of neartermist interventions relative to longtermist ones.</p><p>Also, ~all of the expected value will be acausal if we fully count the value of acausal influence, based on the <a href=\"https://globalprioritiesinstitute.org/the-evidentialists-wager/\">evidentialist's wager</a> and <a href=\"https://globalprioritiesinstitute.org/hayden-wilkinson-can-an-evidentialist-be-risk-averse/\">similar</a>, given the possibility of very large or even infinite numbers of agents with whom we're correlated.</p>", "parentCommentId": "zymwNDexppGyjTwhG", "user": {"username": "MichaelStJules"}}, {"_id": "7rgjadkbo2uiH7D4P", "postedAt": "2023-10-01T19:13:19.673Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Thanks for clarifying, Michael!</p><blockquote><p>I think the argument crucially depends on the assumption that simulations will be disproportionately short-lived</p></blockquote><p>Yes, the argument depends on Brian's parameter F not being super small. F is \"fraction of all computational sent-years spent non-solipsishly simulating almost-space-colonizing ancestral planets (both the most intelligent and also less intelligent creatures on those planets)\". \"A non-solipsish simulation is one in which most or all of the people and animals who seem to exist on Earth are actually being simulated to a non-trivial level of detail\". Brian <a href=\"https://longtermrisk.org/how-the-simulation-argument-dampens-future-fanaticism#Plugging_in_parameter_values\">guessed</a> F = 10^-6, but it feels like it should be much smaller to me. If the value of the future is e.g. 10^30 times the value of this century, it is maybe reasonable to assume that the vast vast majority of computational sent-years are also simulations of the far future, as opposed to simulations of almost-space-colonizing ancestral planets.</p>", "parentCommentId": "gxYpHSX6hb2cmSDSY", "user": {"username": "vascoamaralgrilo"}}, {"_id": "sNKXpCjesHmMruo4L", "postedAt": "2023-10-01T19:36:15.533Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>The informality of that equation makes it hard for me to know how to reason about it. For eg,&nbsp;</p><ul><li>T, D and F seem heavily interdependent.</li><li>I'm just not sure how to parse 'computational sent-years spent non-solipsishly simulating almost-space-colonizing ancestral planets'. What does it mean for a year of sentient life to be spent simulating something? Do you think he means what fraction of experienced years exist in ancestor simulations? I'm still confused by this after reading the last paragraph.</li><li>I'm not sure what the expression's value represents. Are we supposed to multiply some further estimate we have of longtermist work by 10^7? (if so, what estimate is it that's so low that 10^7 isn't enough of a multiplier to make it still eclipse all short termist work?)</li></ul><p>If you feel like you understand it, maybe you could give me a concrete example of how to apply this reasoning?</p><p>For what it's worth, I have much more prosaic reasons for doubting the value of explicitly longtermist work both in practice (the stuff I've discussed with you before that makes me feel like it's misprioritised) and in principle (my instinct is that in situations that reduce to a kind of Pascalian mugging, xP(x) where x is a counterfactual payoff increase and P(x) is the probability of that payoff increase, approaches 0 as x tends to infinity).</p>", "parentCommentId": "Hj7idcqwhXjdA8o7q", "user": {"username": "Arepo"}}, {"_id": "di7kQiwNXgPteBoTz", "postedAt": "2023-10-02T10:28:31.021Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<blockquote><p>T, D and F seem heavily dependent.</p></blockquote><p>I agree.</p><blockquote><p>I'm just not sure how to parse 'computational sent-years spent non-solipsishly simulating almost-space-colonizing ancestral planets'.</p></blockquote><p>I think F = \"sent-years respecting the simulations of the beings in almost-space-colonizing ancestral planets\"/\"all sent-years of the universe\". Brian defines sent-years as follows:</p><blockquote><p>I'll define 1 sent-year as the amount of complexity-weighted experience of one life-year of a typical biological human. That is, consider the sentience over time experienced in a year by the median biological human on Earth right now. Then, a computational process that has 46 times this much subjective experience has 46 sent-years of computation.<a href=\"https://longtermrisk.org/how-the-simulation-argument-dampens-future-fanaticism#link_ajs-fn-id_2-2869\">2</a> Computations with a higher density of sentience may have more sents even if they have fewer FLOPS.</p></blockquote><p>I said Brian <a href=\"https://longtermrisk.org/how-the-simulation-argument-dampens-future-fanaticism#Simplifying_LS\">concluded</a> that L/S = T*D/F, but this was after simplifying L/S = T*D/(E/N + F), where:</p><ul><li>E is \"the amount of sentience on Earth in the near term (say, the next century or two)\".</li><li>\"On average, these civilizations [\"that are about to colonize space\"] will run computations whose sentience is equivalent to that of N human-years\".</li></ul><p>Then Brian says:</p><blockquote><p>Everyone agrees that E/N is very small, perhaps less than 10<sup>-30</sup> or something, because the far future could contain <a href=\"http://www.nickbostrom.com/astronomical/waste.html\">astronomical amounts</a> of sentience [see e.g. Table 1 of <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/Toby-Newberry_How-many-lives-does-the-future-hold.pdf\"><u>Newberry 2021</u></a>]. If F is not nearly as small (and I would guess that it's not), then we can approximate L/S as T * D / F.</p></blockquote><p>The simulation argument dampening future fanaticism comes from Brian assuming that E/N &lt;&lt; F, in which case L/S = T*D/F, and therefore prioritising the future no longer depends on its size. However, for the reasons you mentioned (we are not simulating our ancestors much), I feel like we should a priori expect E/N and F to be similar, and correlated, in which case L/S will still be huge unless it is countered by a very small D (i.e. if the typical low tractability argument against longtermism goes through).</p><blockquote><p>I'm not sure what the expression's value represents. Are we supposed to multiply some further estimate we have of longtermist work by 10^7? (if so, what estimate is it that's so low that 10^7 isn't enough of a multiplier to make it still eclipse all short termist work?)</p></blockquote><p>I think L/S is just supposed to be a heuristic for how much to prioritise longtermist actions relative to neartermist ones. Brian's inputs lead to 10^7, but they were mainly illustrative:</p><blockquote><p>This [L/S = 10^7] happens to be bigger than 1, which suggests that targeting the far future is still ~10 million times better than targeting the short term. But this calculation could have come out as less than 1 using other possible inputs. Combined with general model uncertainty, it seems premature to conclude that far-future-focused actions dominate short-term helping. It's likely that the far future will still dominate after more thorough analysis, but by much less than a naive future fanatic would have thought.</p></blockquote><p>However, it seems to me that, even if one thinks that both E/N and F are super small, L/S could still be smaller than 1 due to super small D. This relates to your point that:</p><blockquote><p>my instinct is that in situations that reduce to a kind of Pascalian mugging, xP(x) where x is a payoff size and P(x) is the probability of that payoff, approaches 0 as x tends to infinity</p></blockquote><p>I share your instinct. I think David Thorstad&nbsp;<a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/David-Thorstad-the-scope-of-longtermism.pdf\"><u>calls</u></a> that rapid diminution.</p><blockquote><p>If you feel like you understand it, maybe you could give me a concrete example of how to apply this reasoning?</p></blockquote><p>I think Brian's reasoning works more or less as follows. Neglecting the simulation argument, if I save one life, I am only saving one life. However, if F = 10^-16<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1miw4hdjysh\"><sup><a href=\"#fn1miw4hdjysh\">[1]</a></sup></span>&nbsp;of sentience-years are spent simulating situation like my own, and the future contains N = 10^30 sentience-years, then me saving a life will imply saving F*N = 10^14 copies of the person I saved. I do not think the argument goes through because I would expect F to be super small in this case, such that F*N is similar to 1.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1miw4hdjysh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1miw4hdjysh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Brian's F = 10^-6 divided by the human population of 10^10.</p></div></li></ol>", "parentCommentId": "sNKXpCjesHmMruo4L", "user": {"username": "vascoamaralgrilo"}}, {"_id": "9FhJHBP6gsENARtuB", "postedAt": "2023-10-07T10:36:20.916Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Appreciate the patient breakdown :)</p><blockquote><p>This [L/S = 10^7] happens to be bigger than 1, which suggests that targeting the far future is still ~10 million times better than targeting the short term. But this calculation could have come out as less than 1 using other possible inputs. Combined with general model uncertainty, it seems premature to conclude that far-future-focused actions dominate short-term helping. It's likely that the far future will still dominate after more thorough analysis, but by much less than a naive future fanatic would have thought.</p></blockquote><p>This is more of a sidenote, but given all the empirical and model uncertainty in <i>any</i> &nbsp;far-future oriented work, it doesn't seem like adding a highly speculative counterargument with its own radical uncertainties should meaningfully shift anyone's priors. It seems like a strong longtermist could accept Brian's views at face value and say 'but the possibility of L/S being vastly bigger than 1 means we should just accept the Pascalian reasoning and plow ahead regardless', while a sceptic could point to rapid diminution and say no simulationy weirdness is necessary to reject these views.</p><p>(Sidesidenote: I wonder whether anyone has investigated the maths of this in any detail? I can imagine there being some possible proof by contradiction of RD, along the lines of 'if there were some minimum amount that it was rational for the muggee to accept, a dishonest mugger could learn that and raise the offer beyond it whereas an honest mugger might not be able to, and therefore, when the mugger's epistemics are taken into account, you should not be willing to accept that amount. Though I can also imagine this might just end up as an awkward integral that you have to choose your values for somewhat arbitrarily)</p><blockquote><p>I think Brian's reasoning works more or less as follows. Neglecting the simulation argument, if I save one life, I am only saving one life. However, if F = 10^-16<a href=\"https://forum.effectivealtruism.org/posts/bmrr8DFufz5Yh8yRC/thresholds-1-what-does-good-look-like-for-longtermism?commentId=di7kQiwNXgPteBoTz#fn1miw4hdjysh\"><sup>[1]</sup></a>&nbsp;of sentience-years are spent simulating situation like my own, and the future contains N = 10^30 sentience-years, then me saving a life will imply saving F*N = 10^14 copies of the person I saved. I do not think the argument goes through because I would expect F to be super small in this case, such that F*N is similar to 1.</p></blockquote><p>For the record, this kind of thing is why I love Brian (aside from him being a wonderful human) - I disagree with him vigorously on almost every point of detail on reflection, but he always come up with some weird take. I had either forgotten or never saw this version of the argument, and was imagining the version closer to Pablo's that talks about the limited value of the far future rather than the increased near-term value.</p><p>That said, I still think I can basically C&amp;P my objection. It's maybe less that I think F is likely to be super small, and more that, given our inability to make any intelligible statements about our purported simulators' nature or intentions it feels basically undefined (or, if you like, any statement whatsoever about its value is ultimately going to be predicated on arbitrary assumptions), making the equation just not parse (or not output any value that could guide our behaviour).</p>", "parentCommentId": "di7kQiwNXgPteBoTz", "user": {"username": "Arepo"}}, {"_id": "xav6DgicuvhqSStNh", "postedAt": "2023-11-16T22:04:56.909Z", "postId": "bmrr8DFufz5Yh8yRC", "htmlBody": "<p>Hi Spencer,</p><p>You mention the following methods to estimate thresholds:</p><blockquote><h2><strong>Using near-termist thresholds as a starting point</strong></h2><h2><strong>Using benchmarks for cost-effectiveness from current longtermist charities</strong></h2><h2><strong>Using the estimates [guesses] of others</strong></h2></blockquote><p>I think the 2nd is the most promising. It is the one employed to establish thresholds for neartermist charities, which are usually benchmarked against GiveWell's <a href=\"https://www.givewell.org/charities/top-charities\">top charities</a>. Open Philanthropy is arguably the largest funder of longtermist projects, so I think it would be valuable to:</p><ul><li>Know what are their marginal longtermist <a href=\"https://www.openphilanthropy.org/grants/\">grants</a> (in theory, the marginal grants in each cause area should be equally cost-effective, but it would be better to pick scalable interventions whose marginal cost-effectiveness would not decrease much for additional resources).</li><li>Then try to estimate how cost-effectively they reduce e.g. extinction risk until 2050. Maybe this can already be done using the quantitative <a href=\"https://forum.effectivealtruism.org/posts/xFsmibHafAu8APgiS/request-for-proposals-help-open-philanthropy-quantify\">models</a> of Open Philanthropy's bio team.</li></ul><p>In addition, I believe it is worth estimating the cost-effectiveness of longtermist charities in terms of a neartermist metric like DALYs averted per $ without accounting for future generations, and then see how they compare with GiveWell's top charities.</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}]